Using TensorFlow backend.
[2019-04-09 14:36:32,750] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v3', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.01, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v3', eval_act_func='part4_v4', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=10000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=1e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=500000, metric_func='part4_v2', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v3-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v2', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=4.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=11, test_env=['Part4-Light-Pit-Test-v5', 'Part4-Light-Pit-Test-v6'], test_mode='Multiple', train_act_func='part4_v4', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=2.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-09 14:36:32,751] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-09 14:36:32.788342: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-09 14:36:48,642] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-09 14:36:48,642] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v3', 'Part4-Light-Pit-Test-v5', 'Part4-Light-Pit-Test-v6'] ...
[2019-04-09 14:36:48,657] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation worker starts!
[2019-04-09 14:36:48,665] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation worker starts!
[2019-04-09 14:36:48,673] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation worker starts!
[2019-04-09 14:36:48,673] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:48,674] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-09 14:36:48,736] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:48,737] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run1
[2019-04-09 14:36:49,675] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:49,678] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-09 14:36:49,748] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:49,749] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run1
[2019-04-09 14:36:50,679] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:50,680] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-09 14:36:50,752] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:50,753] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run1
[2019-04-09 14:36:51,681] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:51,682] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-09 14:36:51,755] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:51,756] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run1
[2019-04-09 14:36:52,683] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:52,684] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-09 14:36:52,757] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:52,758] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run1
[2019-04-09 14:36:53,685] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:53,685] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-09 14:36:53,775] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:53,776] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run1
[2019-04-09 14:36:53,892] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-09 14:36:53,893] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:36:53,893] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:36:53,893] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:53,893] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:36:53,894] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:53,894] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:53,896] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run1
[2019-04-09 14:36:53,904] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run1
[2019-04-09 14:36:53,905] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run1
[2019-04-09 14:36:54,686] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:54,687] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-09 14:36:54,855] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:54,857] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run1
[2019-04-09 14:36:55,688] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:55,689] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-09 14:36:55,791] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:55,793] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run1
[2019-04-09 14:36:56,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.08179764 0.10764114 0.08219421 0.09038598 0.10231986 0.09023292
 0.07471948 0.1012568  0.07749003 0.10158291 0.09037897], sum to 1.0000
[2019-04-09 14:36:56,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7865
[2019-04-09 14:36:56,530] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 20.91375239504256, -0.75, 0.0, 1.0, 40.0, 27.91182655106121], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 0.0000, 
sim time next is 600.0000, 
raw observation next is [1.2, 95.16666666666667, 0.0, 0.0, 19.0, 20.91375239504256, -0.5424445982317676, 0.0, 1.0, 55.0, 52.47635082657239], 
processed observation next is [0.0, 0.0, 0.4958448753462604, 0.9516666666666667, 0.0, 0.0, 0.08333333333333333, 0.2428126995868801, 0.3191851339227441, 0.0, 1.0, 0.8, 0.5247635082657239], 
reward next is 0.4752, 
noisyNet noise sample is [array([0.86143297], dtype=float32), -1.2735325]. 
=============================================
[2019-04-09 14:36:56,690] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:56,691] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-09 14:36:56,792] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:56,794] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run1
[2019-04-09 14:36:57,691] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:57,692] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-09 14:36:57,804] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:57,805] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run1
[2019-04-09 14:36:58,693] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:58,694] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-09 14:36:58,863] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:58,864] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run1
[2019-04-09 14:36:59,695] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:36:59,695] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-09 14:36:59,798] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:36:59,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run1
[2019-04-09 14:37:00,696] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:37:00,697] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-09 14:37:00,805] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:37:00,809] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run1
[2019-04-09 14:37:01,699] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:37:01,701] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-09 14:37:01,803] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:37:01,810] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run1
[2019-04-09 14:37:02,701] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:37:02,703] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-09 14:37:02,845] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:37:02,847] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run1
[2019-04-09 14:37:03,704] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:37:03,705] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-09 14:37:03,808] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:37:03,810] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run1
[2019-04-09 14:37:53,986] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-09 14:37:53,987] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-5.616666666666667, 53.0, 109.0, 659.0, 19.0, 24.02148280259883, 0.1459089356559151, 0.0, 1.0, 20.0, 37.86686324382089]
[2019-04-09 14:37:53,988] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:37:53,989] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.08512241 0.0995215  0.07617257 0.07463349 0.08322341 0.07185552
 0.09219122 0.11819533 0.06588833 0.13432357 0.09887259], sampled 0.9567195251006704
[2019-04-09 14:38:22,649] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5703.4585 278947.4767 2724.2354
[2019-04-09 14:38:22,669] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:38:22,782] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:38:28,695] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5503.6726 299111.0336 2113.9357
[2019-04-09 14:38:28,716] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:38:28,831] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:38:29,335] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5429.9047 306703.5826 1746.3276
[2019-04-09 14:38:29,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:38:29,470] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:38:30,358] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 5503.6725915149245, 299111.03358333505, 2113.9356801481467, 5703.458487339613, 278947.47672741493, 2724.2353795261097, 5429.904677075419, 306703.5825884023, 1746.3276204810109]
[2019-04-09 14:38:30,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.07646798 0.1008053  0.07561256 0.10145476 0.09925191 0.07255669
 0.08129517 0.12094231 0.06846711 0.10390331 0.09924287], sum to 1.0000
[2019-04-09 14:38:30,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4506
[2019-04-09 14:38:30,529] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.8, 95.66666666666667, 0.0, 0.0, 19.0, 21.00797424931885, -0.5151608010092239, 0.0, 1.0, 45.0, 35.8424968343913], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2400.0000, 
sim time next is 3000.0000, 
raw observation next is [6.0, 95.83333333333333, 0.0, 0.0, 19.0, 21.08812616237849, -0.5013952358961518, 0.0, 1.0, 35.0, 28.43961593132224], 
processed observation next is [0.0, 0.0, 0.6288088642659281, 0.9583333333333333, 0.0, 0.0, 0.08333333333333333, 0.25734384686487416, 0.33286825470128273, 0.0, 1.0, 0.4, 0.28439615931322243], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.342618], dtype=float32), 0.6010856]. 
=============================================
[2019-04-09 14:38:32,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[-0.47751415]
 [-0.44312716]
 [-0.3398267 ]
 [-0.26321816]
 [-0.22674337]], R is [[0.28214246]
 [0.92089605]
 [1.58351243]
 [2.26672554]
 [2.90350342]].
[2019-04-09 14:38:32,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.07262217 0.10643199 0.08020633 0.09646549 0.10765174 0.07491197
 0.07727755 0.11436081 0.07539345 0.10095541 0.09372314], sum to 1.0000
[2019-04-09 14:38:32,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4429
[2019-04-09 14:38:32,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.07237332 0.10857299 0.07524583 0.0936094  0.10485592 0.07766355
 0.07437821 0.12049735 0.07791951 0.09879763 0.0960863 ], sum to 1.0000
[2019-04-09 14:38:32,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-09 14:38:32,534] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 95.83333333333333, 0.0, 0.0, 19.0, 21.08812616237849, -0.5013952358961518, 0.0, 1.0, 35.0, 28.43961593132224], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3000.0000, 
sim time next is 3600.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.18196952876107, -0.4929959063938273, 0.0, 1.0, 35.0, 24.04472250157624], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.26516412739675577, 0.3356680312020576, 0.0, 1.0, 0.4, 0.24044722501576238], 
reward next is 0.7596, 
noisyNet noise sample is [array([-0.61437863], dtype=float32), 0.6194006]. 
=============================================
[2019-04-09 14:38:32,631] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 21.44863695997195, -0.4144749707351792, 0.0, 1.0, 30.0, 32.90883770190169], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3600.0000, 
sim time next is 4200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.68169442605958, -0.3958729210227168, 0.0, 1.0, 40.0, 29.84401981999068], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.30680786883829825, 0.36804235965909443, 0.0, 1.0, 0.5, 0.2984401981999068], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.8125008], dtype=float32), -0.47133124]. 
=============================================
[2019-04-09 14:38:32,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.07252096 0.10192695 0.07250653 0.08929437 0.10190737 0.07452902
 0.07946634 0.117791   0.08202619 0.11395764 0.09407363], sum to 1.0000
[2019-04-09 14:38:32,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2978
[2019-04-09 14:38:32,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06974939 0.10236652 0.07645629 0.07789171 0.10131124 0.08176013
 0.07887854 0.110481   0.08156654 0.1178914  0.10164722], sum to 1.0000
[2019-04-09 14:38:32,717] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8159
[2019-04-09 14:38:32,758] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 21.5755844169332, -0.4037403126049228, 0.0, 1.0, 55.0, 52.15928465451772], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4800.0000, 
sim time next is 5400.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.66707518568287, -0.3844145987424778, 0.0, 1.0, 30.0, 38.39161860858441], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.3055895988069057, 0.37186180041917405, 0.0, 1.0, 0.3, 0.3839161860858441], 
reward next is 0.6161, 
noisyNet noise sample is [array([-0.20999326], dtype=float32), 2.241925]. 
=============================================
[2019-04-09 14:38:32,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 21.37928447207765, -0.4120364699629344, 0.0, 1.0, 35.0, 39.29708623821615], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4200.0000, 
sim time next is 4800.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 21.69636895811408, -0.3701873761873108, 0.0, 1.0, 60.0, 59.40263982914195], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.3080307465095065, 0.3766042079375631, 0.0, 1.0, 0.9, 0.5940263982914195], 
reward next is 0.4060, 
noisyNet noise sample is [array([0.87829906], dtype=float32), 0.23283778]. 
=============================================
[2019-04-09 14:38:33,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07320213 0.09976391 0.0738465  0.07460823 0.07919127 0.06350494
 0.09465887 0.11197397 0.07461344 0.15280317 0.10183355], sum to 1.0000
[2019-04-09 14:38:33,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6774
[2019-04-09 14:38:33,311] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.366666666666667, 95.0, 0.0, 0.0, 19.0, 22.85628653111098, -0.1547908452764647, 0.0, 1.0, 55.0, 41.86317196776425], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 12000.0000, 
sim time next is 12600.0000, 
raw observation next is [7.45, 94.5, 0.0, 0.0, 19.0, 23.00631729105661, -0.1405697581847979, 0.0, 1.0, 55.0, 43.95202343307825], 
processed observation next is [0.0, 0.13043478260869565, 0.6689750692520776, 0.945, 0.0, 0.0, 0.08333333333333333, 0.4171931075880509, 0.45314341393840074, 0.0, 1.0, 0.8, 0.4395202343307825], 
reward next is 0.5605, 
noisyNet noise sample is [array([-1.521035], dtype=float32), 0.3875357]. 
=============================================
[2019-04-09 14:38:33,594] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06969413 0.10341404 0.06891759 0.06953271 0.07962556 0.06113004
 0.11486152 0.11033887 0.0681305  0.14676596 0.10758913], sum to 1.0000
[2019-04-09 14:38:33,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-09 14:38:33,702] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 22.458780068765, -0.2164066531792284, 0.0, 1.0, 65.0, 71.27590530269448], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 22.56549749774275, -0.1878605808703704, 0.0, 1.0, 45.0, 44.91043621141554], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.3804581248118959, 0.4373798063765432, 0.0, 1.0, 0.6, 0.44910436211415544], 
reward next is 0.5509, 
noisyNet noise sample is [array([-0.5321972], dtype=float32), 0.4529183]. 
=============================================
[2019-04-09 14:38:33,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0813994  0.10043857 0.0790442  0.06235434 0.08149701 0.06767471
 0.09783284 0.11366496 0.06510208 0.1424585  0.10853339], sum to 1.0000
[2019-04-09 14:38:33,809] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3500
[2019-04-09 14:38:33,906] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 23.62424483960008, -0.02607933609311178, 0.0, 1.0, 25.0, 29.9112366308268], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 22200.0000, 
sim time next is 22800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.56784852903701, -0.0360606094138401, 0.0, 1.0, 35.0, 28.4131302220461], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.46398737741975093, 0.48797979686205334, 0.0, 1.0, 0.4, 0.284131302220461], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.21004869], dtype=float32), 0.043858998]. 
=============================================
[2019-04-09 14:38:33,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.07849768 0.10027134 0.06769185 0.07370174 0.07495748 0.06007088
 0.10777851 0.12226462 0.06320856 0.14449815 0.1070592 ], sum to 1.0000
[2019-04-09 14:38:33,958] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9455
[2019-04-09 14:38:34,061] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 23.66119693149527, 0.02129131095885142, 0.0, 1.0, 25.0, 41.10719843546858], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 24600.0000, 
sim time next is 25200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.70850141158509, 0.03914398718407813, 0.0, 1.0, 65.0, 53.17536009438155], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.47570845096542413, 0.5130479957280261, 0.0, 1.0, 1.0, 0.5317536009438155], 
reward next is 0.4682, 
noisyNet noise sample is [array([1.3481666], dtype=float32), 0.4566632]. 
=============================================
[2019-04-09 14:38:34,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.07083249 0.10687853 0.07322785 0.06981947 0.07548689 0.06481517
 0.10510249 0.11584408 0.06133035 0.14404999 0.11261264], sum to 1.0000
[2019-04-09 14:38:34,128] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-09 14:38:34,146] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 23.25584034326726, -0.02077788682962787, 0.0, 1.0, 45.0, 46.92407713121836], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 26400.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.47363341566563, 0.00829879012545231, 0.0, 1.0, 65.0, 54.20918099052368], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.45613611797213593, 0.5027662633751507, 0.0, 1.0, 1.0, 0.5420918099052368], 
reward next is 0.4579, 
noisyNet noise sample is [array([1.072046], dtype=float32), -1.0816365]. 
=============================================
[2019-04-09 14:38:34,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[-0.62167865]
 [-0.6161527 ]
 [-0.685081  ]
 [-0.6714101 ]
 [-0.74433446]], R is [[-0.28758809]
 [ 0.24604699]
 [ 0.64499849]
 [ 1.16586781]
 [ 1.22751558]].
[2019-04-09 14:38:34,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06997731 0.10710561 0.06302084 0.07310668 0.06963833 0.05290276
 0.11563294 0.11746397 0.06102527 0.15136625 0.11876006], sum to 1.0000
[2019-04-09 14:38:34,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8824
[2019-04-09 14:38:34,286] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 14.0, 0.0, 19.0, 23.90260803281129, 0.0731993721101437, 0.0, 1.0, 25.0, 37.10513443830806], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [7.7, 93.0, 17.5, 0.0, 19.0, 23.98244849067345, 0.09081593850343739, 0.0, 1.0, 65.0, 53.08193905551345], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.058333333333333334, 0.0, 0.08333333333333333, 0.4985373742227874, 0.5302719795011458, 0.0, 1.0, 1.0, 0.5308193905551345], 
reward next is 0.4692, 
noisyNet noise sample is [array([0.47751227], dtype=float32), 0.76898104]. 
=============================================
[2019-04-09 14:38:34,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.69429255]
 [-0.6838156 ]
 [-0.6350412 ]
 [-0.6051087 ]
 [-0.58525497]], R is [[-0.19551572]
 [ 0.43538806]
 [ 0.95604229]
 [ 1.3811605 ]
 [ 2.007411  ]].
[2019-04-09 14:38:34,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06561118 0.11610221 0.0706789  0.07565094 0.08241893 0.06807871
 0.112183   0.10934406 0.06493019 0.12913269 0.10586927], sum to 1.0000
[2019-04-09 14:38:34,673] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7739
[2019-04-09 14:38:34,690] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [7.7, 93.0, 45.33333333333333, 0.0, 19.0, 23.68788431433291, 0.02802588787398817, 0.0, 1.0, 50.0, 49.69801770624564], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 35400.0000, 
sim time next is 36000.0000, 
raw observation next is [7.7, 93.0, 49.0, 0.0, 19.0, 23.60574525949185, 0.04974340539655897, 0.0, 1.0, 60.0, 59.8657557671219], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.16333333333333333, 0.0, 0.08333333333333333, 0.4671454382909876, 0.5165811351321863, 0.0, 1.0, 0.9, 0.598657557671219], 
reward next is 0.4013, 
noisyNet noise sample is [array([-0.14570078], dtype=float32), 0.57414436]. 
=============================================
[2019-04-09 14:38:34,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[-0.66080356]
 [-0.69787955]
 [-0.6541271 ]
 [-0.6645371 ]
 [-0.7180822 ]], R is [[-0.26488471]
 [ 0.24078393]
 [ 1.23837614]
 [ 1.85881269]
 [ 2.44542599]].
[2019-04-09 14:38:34,797] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06777273 0.0948291  0.07185659 0.06826261 0.08097981 0.06980267
 0.10361377 0.11094734 0.06200369 0.14586706 0.12406462], sum to 1.0000
[2019-04-09 14:38:34,798] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4048
[2019-04-09 14:38:34,811] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 70.0, 0.0, 19.0, 24.02682644772232, 0.09026723969261297, 0.0, 1.0, 40.0, 30.26906280397528], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 40200.0000, 
sim time next is 40800.0000, 
raw observation next is [7.7, 93.0, 72.5, 0.0, 19.0, 23.98788759718512, 0.07949541368945617, 0.0, 1.0, 40.0, 28.74532155011726], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.24166666666666667, 0.0, 0.08333333333333333, 0.49899063309876013, 0.5264984712298187, 0.0, 1.0, 0.5, 0.2874532155011726], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.8754714], dtype=float32), 0.655416]. 
=============================================
[2019-04-09 14:38:34,827] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06280313 0.11078078 0.06676499 0.06369949 0.08203524 0.06076489
 0.11697607 0.1132031  0.06375732 0.13364951 0.1255655 ], sum to 1.0000
[2019-04-09 14:38:34,829] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9633
[2019-04-09 14:38:34,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.06799298 0.10903019 0.06655889 0.07594247 0.08554988 0.06600403
 0.11330724 0.11680301 0.06633948 0.12641844 0.1060534 ], sum to 1.0000
[2019-04-09 14:38:34,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-09 14:38:34,978] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 70.0, 0.0, 19.0, 23.87701459714613, 0.1239512422710323, 0.0, 1.0, 60.0, 53.35155072803669], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 40200.0000, 
sim time next is 40800.0000, 
raw observation next is [7.7, 93.0, 72.5, 0.0, 19.0, 23.96217978840276, 0.1688964314394956, 0.0, 1.0, 65.0, 63.22776029814254], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.24166666666666667, 0.0, 0.08333333333333333, 0.4968483157002301, 0.5562988104798319, 0.0, 1.0, 1.0, 0.6322776029814254], 
reward next is 0.3677, 
noisyNet noise sample is [array([-0.4287561], dtype=float32), -0.92606455]. 
=============================================
[2019-04-09 14:38:35,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.7, 93.0, 72.5, 0.0, 19.0, 24.31221845113104, 0.1760492168152356, 0.0, 1.0, 60.0, 46.91817438587929], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 40800.0000, 
sim time next is 41400.0000, 
raw observation next is [7.7, 93.0, 75.0, 0.0, 19.0, 24.33417030832666, 0.1802011488839942, 0.0, 1.0, 55.0, 41.44423129859394], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.25, 0.0, 0.08333333333333333, 0.5278475256938883, 0.5600670496279981, 0.0, 1.0, 0.8, 0.4144423129859394], 
reward next is 0.5856, 
noisyNet noise sample is [array([0.18796298], dtype=float32), -0.38544467]. 
=============================================
[2019-04-09 14:38:35,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06568285 0.10757685 0.06958527 0.06880498 0.07559765 0.0672779
 0.11079095 0.11702223 0.05899806 0.145887   0.11277624], sum to 1.0000
[2019-04-09 14:38:35,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6992
[2019-04-09 14:38:35,114] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.800000000000001, 91.83333333333333, 89.0, 0.0, 19.0, 23.80966284479139, 0.1130001843990162, 0.0, 1.0, 35.0, 37.96959504108267], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 43800.0000, 
sim time next is 44400.0000, 
raw observation next is [7.9, 90.66666666666667, 92.5, 0.0, 19.0, 23.86241918892587, 0.1311831626404272, 0.0, 1.0, 55.0, 40.1919034685565], 
processed observation next is [0.0, 0.5217391304347826, 0.6814404432132966, 0.9066666666666667, 0.30833333333333335, 0.0, 0.08333333333333333, 0.4885349324104891, 0.5437277208801424, 0.0, 1.0, 0.8, 0.40191903468556495], 
reward next is 0.5981, 
noisyNet noise sample is [array([1.3754817], dtype=float32), 1.7310934]. 
=============================================
[2019-04-09 14:38:35,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.07096969 0.11800927 0.0651381  0.07382741 0.08389498 0.06602195
 0.11053977 0.10746485 0.06026327 0.12692088 0.11694985], sum to 1.0000
[2019-04-09 14:38:35,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8097
[2019-04-09 14:38:35,392] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [7.9, 86.0, 85.66666666666666, 0.0, 19.0, 24.61690925920426, 0.2502710314172973, 0.0, 1.0, 25.0, 31.05285000263074], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 49200.0000, 
sim time next is 49800.0000, 
raw observation next is [7.8, 86.0, 84.33333333333334, 0.0, 19.0, 24.60498569428744, 0.2602644139060611, 0.0, 1.0, 60.0, 45.31755871943514], 
processed observation next is [0.0, 0.5652173913043478, 0.6786703601108034, 0.86, 0.28111111111111114, 0.0, 0.08333333333333333, 0.5504154745239532, 0.5867548046353537, 0.0, 1.0, 0.9, 0.4531755871943514], 
reward next is 0.5468, 
noisyNet noise sample is [array([0.6854826], dtype=float32), 0.335752]. 
=============================================
[2019-04-09 14:38:35,428] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07348254 0.10530489 0.06361181 0.07028844 0.08368997 0.0636909
 0.10491688 0.12374062 0.06300171 0.1335504  0.1147218 ], sum to 1.0000
[2019-04-09 14:38:35,429] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06915859 0.10393112 0.06934585 0.07202115 0.07730458 0.06818964
 0.10877351 0.12824577 0.0675024  0.13052227 0.10500509], sum to 1.0000
[2019-04-09 14:38:35,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0265
[2019-04-09 14:38:35,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6543
[2019-04-09 14:38:35,441] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 24.59899364899384, 0.2736632548962009, 0.0, 1.0, 40.0, 36.99665667537036], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 19.0, 24.6513520473689, 0.2050179549031054, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 0.08333333333333333, 0.5542793372807416, 0.5683393183010351, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6827777], dtype=float32), 0.92380464]. 
=============================================
[2019-04-09 14:38:35,445] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.0, 86.0, 87.0, 0.0, 19.0, 24.42857159607681, 0.2175399567344608, 0.0, 1.0, 25.0, 36.6031587657337], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 48600.0000, 
sim time next is 49200.0000, 
raw observation next is [7.9, 86.0, 85.66666666666666, 0.0, 19.0, 24.46219510743905, 0.2154339671419892, 0.0, 1.0, 50.0, 34.61399770583147], 
processed observation next is [0.0, 0.5652173913043478, 0.6814404432132966, 0.86, 0.2855555555555555, 0.0, 0.08333333333333333, 0.5385162589532543, 0.571811322380663, 0.0, 1.0, 0.7, 0.34613997705831473], 
reward next is 0.6539, 
noisyNet noise sample is [array([-0.25772053], dtype=float32), -0.049444534]. 
=============================================
[2019-04-09 14:38:35,599] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.06844494 0.11780971 0.06590493 0.0657124  0.08679237 0.05946418
 0.11248035 0.11818136 0.05239484 0.1286995  0.12411542], sum to 1.0000
[2019-04-09 14:38:35,604] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-09 14:38:35,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06194501 0.10488558 0.06483295 0.07329682 0.07968818 0.06321695
 0.11450966 0.1187574  0.06041019 0.13800745 0.12044988], sum to 1.0000
[2019-04-09 14:38:35,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9928
[2019-04-09 14:38:35,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 19.0, 24.98560381721681, 0.3490125865669383, 0.0, 1.0, 30.0, 37.04141778089171], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 52800.0000, 
sim time next is 53400.0000, 
raw observation next is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 19.0, 25.03242019856331, 0.3599791515206267, 0.0, 1.0, 65.0, 52.15395163751896], 
processed observation next is [0.0, 0.6086956521739131, 0.6643582640812559, 0.86, 0.23111111111111116, 0.0, 0.08333333333333333, 0.5860350165469427, 0.6199930505068756, 0.0, 1.0, 1.0, 0.5215395163751896], 
reward next is 0.4785, 
noisyNet noise sample is [array([1.2759413], dtype=float32), -1.3202909]. 
=============================================
[2019-04-09 14:38:35,801] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 19.0, 24.49392840030476, 0.2245047943204897, 0.0, 1.0, 45.0, 33.47874175117412], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 53400.0000, 
sim time next is 54000.0000, 
raw observation next is [7.2, 86.0, 64.5, 0.0, 19.0, 24.50259972946329, 0.2376990788164609, 0.0, 1.0, 65.0, 62.4276375010682], 
processed observation next is [0.0, 0.6521739130434783, 0.662049861495845, 0.86, 0.215, 0.0, 0.08333333333333333, 0.5418833107886076, 0.5792330262721537, 0.0, 1.0, 1.0, 0.624276375010682], 
reward next is 0.3757, 
noisyNet noise sample is [array([-0.05694424], dtype=float32), -1.2042265]. 
=============================================
[2019-04-09 14:38:35,830] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[-0.7555065 ]
 [-0.64323366]
 [-0.6537951 ]
 [-0.6600139 ]
 [-0.6164232 ]], R is [[-0.24199429]
 [ 0.4256382 ]
 [ 1.00229168]
 [ 1.52486229]
 [ 2.08703375]].
[2019-04-09 14:38:35,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.06543866 0.1085658  0.06194454 0.07951086 0.0816055  0.05602296
 0.11143895 0.12949552 0.06212685 0.12622178 0.11762857], sum to 1.0000
[2019-04-09 14:38:35,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6372
[2019-04-09 14:38:36,004] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.9, 84.0, 50.0, 0.0, 19.0, 24.83151213121745, 0.3392312376470295, 0.0, 1.0, 50.0, 46.37421855095409], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 55800.0000, 
sim time next is 56400.0000, 
raw observation next is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 19.0, 24.94678356134559, 0.3437453800963081, 0.0, 1.0, 50.0, 39.49544570530354], 
processed observation next is [0.0, 0.6521739130434783, 0.6509695290858727, 0.8333333333333335, 0.1488888888888889, 0.0, 0.08333333333333333, 0.5788986301121325, 0.6145817933654361, 0.0, 1.0, 0.7, 0.3949544570530354], 
reward next is 0.6050, 
noisyNet noise sample is [array([-2.7690542], dtype=float32), -0.34974188]. 
=============================================
[2019-04-09 14:38:36,155] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06504028 0.10611074 0.06330325 0.06781538 0.08100223 0.06034848
 0.11359335 0.1238034  0.05725427 0.1349369  0.12679172], sum to 1.0000
[2019-04-09 14:38:36,155] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7147
[2019-04-09 14:38:36,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 19.0, 24.76347266975053, 0.2706909788556898, 0.0, 1.0, 45.0, 25.80535509541827], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 19.0, 24.66576153445266, 0.2742817930389797, 0.0, 1.0, 55.0, 39.98508274164611], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.08333333333333333, 0.5554801278710549, 0.5914272643463265, 0.0, 1.0, 0.8, 0.3998508274164611], 
reward next is 0.6001, 
noisyNet noise sample is [array([0.6786715], dtype=float32), 0.54846156]. 
=============================================
[2019-04-09 14:38:36,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06420844 0.11060511 0.07011902 0.07452248 0.07209723 0.05818892
 0.11759067 0.12405396 0.052444   0.13684338 0.11932682], sum to 1.0000
[2019-04-09 14:38:36,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7382
[2019-04-09 14:38:36,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.06760495 0.10869333 0.06550635 0.07212786 0.07766616 0.06176523
 0.11129475 0.11988932 0.04907044 0.1476174  0.11876427], sum to 1.0000
[2019-04-09 14:38:36,438] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6061
[2019-04-09 14:38:36,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06076621 0.10149624 0.0690596  0.07247651 0.07525751 0.05563433
 0.11950454 0.11946035 0.05529754 0.13325387 0.13779329], sum to 1.0000
[2019-04-09 14:38:36,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4104
[2019-04-09 14:38:36,470] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.300000000000001, 88.5, 0.0, 0.0, 19.0, 25.17680062933047, 0.3539543657761205, 0.0, 1.0, 30.0, 29.38440728802383], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 65400.0000, 
sim time next is 66000.0000, 
raw observation next is [4.200000000000001, 88.0, 0.0, 0.0, 19.0, 25.05955576976914, 0.3446969586453379, 0.0, 1.0, 55.0, 40.02816746568534], 
processed observation next is [0.0, 0.782608695652174, 0.5789473684210527, 0.88, 0.0, 0.0, 0.08333333333333333, 0.5882963141474283, 0.6148989862151126, 0.0, 1.0, 0.8, 0.40028167465685344], 
reward next is 0.5997, 
noisyNet noise sample is [array([-0.711724], dtype=float32), 1.4994032]. 
=============================================
[2019-04-09 14:38:36,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.71999633]
 [-0.69341266]
 [-0.64792466]
 [-0.7020373 ]
 [-0.69103193]], R is [[-0.10558546]
 [ 0.60162634]
 [ 1.28714037]
 [ 1.95079315]
 [ 2.59155154]].
[2019-04-09 14:38:36,477] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 24.62477271478048, 0.2689404340041804, 0.0, 1.0, 60.0, 47.91023491873328], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 24.63225930952211, 0.2737451454447458, 0.0, 1.0, 50.0, 43.85223976564816], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.5526882757935091, 0.5912483818149153, 0.0, 1.0, 0.7, 0.4385223976564816], 
reward next is 0.5615, 
noisyNet noise sample is [array([-0.2753408], dtype=float32), -0.6716186]. 
=============================================
[2019-04-09 14:38:36,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 19.0, 24.85154625603911, 0.3100790863874645, 0.0, 1.0, 50.0, 40.27278946668059], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 19.0, 24.81185675237818, 0.2980138657689925, 0.0, 1.0, 40.0, 31.57007724350936], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5676547293648483, 0.5993379552563308, 0.0, 1.0, 0.5, 0.3157007724350936], 
reward next is 0.6843, 
noisyNet noise sample is [array([-0.01316769], dtype=float32), 0.27283645]. 
=============================================
[2019-04-09 14:38:36,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.7323085 ]
 [-0.741037  ]
 [-0.8232857 ]
 [-0.80831134]
 [-0.81054336]], R is [[-0.18182796]
 [ 0.34088796]
 [ 0.99507451]
 [ 1.54841447]
 [ 2.0490334 ]].
[2019-04-09 14:38:36,492] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0675322  0.10487783 0.07548236 0.07035205 0.08489313 0.06224971
 0.12352059 0.11030388 0.05137587 0.11773136 0.13168113], sum to 1.0000
[2019-04-09 14:38:36,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-09 14:38:36,506] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 87.0, 0.0, 0.0, 19.0, 24.81185675237818, 0.2980138657689925, 0.0, 1.0, 40.0, 31.57007724350936], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 67200.0000, 
sim time next is 67800.0000, 
raw observation next is [3.9, 86.5, 0.0, 0.0, 19.0, 24.75981383907553, 0.2831176648773644, 0.0, 1.0, 30.0, 28.08574829729534], 
processed observation next is [0.0, 0.782608695652174, 0.5706371191135734, 0.865, 0.0, 0.0, 0.08333333333333333, 0.5633178199229608, 0.5943725549591214, 0.0, 1.0, 0.3, 0.2808574829729534], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.01316769], dtype=float32), 0.27283645]. 
=============================================
[2019-04-09 14:38:36,515] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.06686217 0.10469916 0.06473607 0.07701825 0.08331668 0.05649759
 0.11288805 0.12574217 0.05882707 0.13417187 0.11524095], sum to 1.0000
[2019-04-09 14:38:36,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5379
[2019-04-09 14:38:36,534] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.9, 86.5, 0.0, 0.0, 19.0, 24.50342887793131, 0.227770037106176, 0.0, 1.0, 45.0, 28.20597838430512], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 67800.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 19.0, 24.43554171234717, 0.2110166086218327, 0.0, 1.0, 45.0, 26.87234753965729], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 0.08333333333333333, 0.5362951426955975, 0.570338869540611, 0.0, 1.0, 0.6, 0.26872347539657293], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.18662997], dtype=float32), -0.8432519]. 
=============================================
[2019-04-09 14:38:36,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.07155814 0.11510643 0.07217415 0.06759448 0.08882145 0.0642397
 0.11121571 0.10414751 0.05496726 0.12866066 0.12151445], sum to 1.0000
[2019-04-09 14:38:36,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7451
[2019-04-09 14:38:36,718] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.883333333333334, 88.5, 0.0, 0.0, 19.0, 24.78749017444751, 0.3171476815532342, 0.0, 1.0, 60.0, 47.71596244465249], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 71400.0000, 
sim time next is 72000.0000, 
raw observation next is [2.7, 89.0, 0.0, 0.0, 19.0, 24.80596076336752, 0.3181324246129815, 0.0, 1.0, 55.0, 50.67886052049698], 
processed observation next is [0.0, 0.8695652173913043, 0.5373961218836566, 0.89, 0.0, 0.0, 0.08333333333333333, 0.5671633969472932, 0.6060441415376605, 0.0, 1.0, 0.8, 0.5067886052049698], 
reward next is 0.4932, 
noisyNet noise sample is [array([0.97496957], dtype=float32), -0.3544482]. 
=============================================
[2019-04-09 14:38:36,738] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.80664057]
 [-0.7399607 ]
 [-0.96168554]
 [-0.769642  ]
 [-0.750744  ]], R is [[-0.25927758]
 [ 0.26615557]
 [ 0.8764106 ]
 [ 1.25251722]
 [ 2.23999214]].
[2019-04-09 14:38:36,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.07184353 0.10868788 0.06814954 0.06856498 0.07897598 0.05754433
 0.12074316 0.120263   0.04852902 0.12794244 0.12875606], sum to 1.0000
[2019-04-09 14:38:36,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0706
[2019-04-09 14:38:36,830] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.416666666666667, 86.83333333333334, 0.0, 0.0, 19.0, 24.54295596408788, 0.296285430226093, 0.0, 1.0, 65.0, 67.68066238448206], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 76200.0000, 
sim time next is 76800.0000, 
raw observation next is [1.233333333333334, 88.66666666666667, 0.0, 0.0, 19.0, 24.6342907471373, 0.2203957306607493, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.49676823638042483, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.5528575622614417, 0.5734652435535831, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6464843], dtype=float32), -0.83486235]. 
=============================================
[2019-04-09 14:38:37,289] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05258474 0.11438874 0.07516451 0.0665791  0.07035108 0.06108473
 0.12621397 0.09926513 0.05657586 0.12108091 0.1567112 ], sum to 1.0000
[2019-04-09 14:38:37,290] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-09 14:38:37,305] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.4, 92.33333333333334, 0.0, 0.0, 19.0, 25.07248466868045, 0.3489363214366321, 0.0, 1.0, 55.0, 44.26545080260544], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 88800.0000, 
sim time next is 89400.0000, 
raw observation next is [-0.5, 91.66666666666667, 0.0, 0.0, 19.0, 25.08364269426377, 0.3423107694119278, 0.0, 1.0, 20.0, 36.52260326170337], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.9166666666666667, 0.0, 0.0, 0.08333333333333333, 0.5903035578553141, 0.614103589803976, 0.0, 1.0, 0.1, 0.3652260326170337], 
reward next is 0.6348, 
noisyNet noise sample is [array([-0.17704703], dtype=float32), -0.6236103]. 
=============================================
[2019-04-09 14:38:37,327] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0832251  0.10928903 0.06593734 0.06911886 0.07130677 0.05693009
 0.11122385 0.13760377 0.04894955 0.1302862  0.1161295 ], sum to 1.0000
[2019-04-09 14:38:37,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7331
[2019-04-09 14:38:37,343] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.3, 95.0, 0.0, 0.0, 19.0, 24.90053788385037, 0.3238600737636296, 0.0, 1.0, 20.0, 35.35102246688147], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 82800.0000, 
sim time next is 83400.0000, 
raw observation next is [0.25, 95.0, 0.0, 0.0, 19.0, 24.87070315246052, 0.3164369211001614, 0.0, 1.0, 55.0, 40.66566843765123], 
processed observation next is [0.0, 1.0, 0.46952908587257625, 0.95, 0.0, 0.0, 0.08333333333333333, 0.5725585960383768, 0.6054789737000538, 0.0, 1.0, 0.8, 0.40665668437651226], 
reward next is 0.5933, 
noisyNet noise sample is [array([0.86244345], dtype=float32), -0.1910052]. 
=============================================
[2019-04-09 14:38:37,499] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06211188 0.09667022 0.07998846 0.06263077 0.07756449 0.06338453
 0.12797892 0.10915522 0.06103131 0.14183825 0.11764588], sum to 1.0000
[2019-04-09 14:38:37,503] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3100
[2019-04-09 14:38:37,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06590558 0.0989284  0.07543052 0.06343565 0.08395107 0.06603076
 0.12891117 0.1123427  0.06301103 0.11978149 0.12227152], sum to 1.0000
[2019-04-09 14:38:37,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0798
[2019-04-09 14:38:37,519] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 91.0, 0.0, 0.0, 19.0, 24.45891961085883, 0.2412165463472036, 0.0, 1.0, 50.0, 50.49349907970405], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 90000.0000, 
sim time next is 90600.0000, 
raw observation next is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 19.0, 24.53472759510885, 0.2413504828943342, 0.0, 1.0, 35.0, 38.35722677266574], 
processed observation next is [1.0, 0.043478260869565216, 0.44090489381348114, 0.9100000000000001, 0.0, 0.0, 0.08333333333333333, 0.5445606329257376, 0.5804501609647781, 0.0, 1.0, 0.4, 0.3835722677266574], 
reward next is 0.6164, 
noisyNet noise sample is [array([0.25121674], dtype=float32), -0.06295488]. 
=============================================
[2019-04-09 14:38:37,524] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 91.0, 0.0, 0.0, 19.0, 24.97607346722386, 0.3062734696849274, 0.0, 1.0, 60.0, 48.85829202303774], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 90000.0000, 
sim time next is 90600.0000, 
raw observation next is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 19.0, 24.93446126235435, 0.2999978914333046, 0.0, 1.0, 20.0, 43.79195129226179], 
processed observation next is [1.0, 0.043478260869565216, 0.44090489381348114, 0.9100000000000001, 0.0, 0.0, 0.08333333333333333, 0.5778717718628625, 0.5999992971444349, 0.0, 1.0, 0.1, 0.43791951292261794], 
reward next is 0.5621, 
noisyNet noise sample is [array([-0.13182269], dtype=float32), -0.033567917]. 
=============================================
[2019-04-09 14:38:37,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05452326 0.09287376 0.06728002 0.05773012 0.06010856 0.05582431
 0.13738161 0.12317533 0.07524373 0.12837097 0.14748824], sum to 1.0000
[2019-04-09 14:38:37,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1250
[2019-04-09 14:38:37,961] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.3, 80.33333333333334, 0.0, 0.0, 19.0, 23.97265671547456, 0.09412042436919499, 0.0, 1.0, 50.0, 33.95085166917214], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 100200.0000, 
sim time next is 100800.0000, 
raw observation next is [-3.4, 79.0, 0.0, 0.0, 19.0, 23.94685017077969, 0.08185491059555997, 0.0, 1.0, 20.0, 34.81065532157407], 
processed observation next is [1.0, 0.17391304347826086, 0.368421052631579, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4955708475649742, 0.5272849701985199, 0.0, 1.0, 0.1, 0.3481065532157407], 
reward next is 0.6519, 
noisyNet noise sample is [array([-0.5629393], dtype=float32), 0.02717011]. 
=============================================
[2019-04-09 14:38:38,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0537915  0.09585019 0.06651266 0.05406516 0.06211426 0.06087975
 0.1372192  0.11604615 0.08127285 0.10487215 0.16737618], sum to 1.0000
[2019-04-09 14:38:38,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5115
[2019-04-09 14:38:38,429] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05578847 0.09930582 0.06838989 0.05379268 0.06359243 0.05586198
 0.1351905  0.1182187  0.08000026 0.11778587 0.15207338], sum to 1.0000
[2019-04-09 14:38:38,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6229
[2019-04-09 14:38:38,440] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 75.0, 0.0, 0.0, 19.0, 24.22800874538739, 0.135542649238837, 0.0, 1.0, 30.0, 36.73802738843543], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 108000.0000, 
sim time next is 108600.0000, 
raw observation next is [-6.800000000000001, 73.83333333333333, 0.0, 0.0, 19.0, 24.1647274193619, 0.1180780536678925, 0.0, 1.0, 45.0, 35.04319817333718], 
processed observation next is [1.0, 0.2608695652173913, 0.2742382271468144, 0.7383333333333333, 0.0, 0.0, 0.08333333333333333, 0.513727284946825, 0.5393593512226308, 0.0, 1.0, 0.6, 0.3504319817333718], 
reward next is 0.6496, 
noisyNet noise sample is [array([-0.79592025], dtype=float32), -0.38532704]. 
=============================================
[2019-04-09 14:38:38,443] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.9, 72.66666666666667, 0.0, 0.0, 19.0, 23.81899904531825, 0.07629055108807989, 0.0, 1.0, 30.0, 37.46904000759861], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 109200.0000, 
sim time next is 109800.0000, 
raw observation next is [-7.0, 71.5, 0.0, 0.0, 19.0, 23.8053993840376, 0.06453644748025412, 0.0, 1.0, 50.0, 38.15818191333475], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.715, 0.0, 0.0, 0.08333333333333333, 0.4837832820031333, 0.5215121491600847, 0.0, 1.0, 0.7, 0.3815818191333475], 
reward next is 0.6184, 
noisyNet noise sample is [array([-0.51094526], dtype=float32), -0.9049959]. 
=============================================
[2019-04-09 14:38:38,445] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05806789 0.08731178 0.07332341 0.05462369 0.06580982 0.06759226
 0.13182235 0.11645484 0.07753294 0.11820264 0.14925835], sum to 1.0000
[2019-04-09 14:38:38,448] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1362
[2019-04-09 14:38:38,483] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 71.5, 0.0, 0.0, 19.0, 23.84275235986723, 0.07599519677565693, 0.0, 1.0, 65.0, 70.68509631951264], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 109800.0000, 
sim time next is 110400.0000, 
raw observation next is [-7.1, 70.33333333333333, 0.0, 0.0, 19.0, 23.85171439200661, 0.07872815127816835, 0.0, 1.0, 20.0, 52.03930198538097], 
processed observation next is [1.0, 0.2608695652173913, 0.2659279778393352, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.4876428660005508, 0.5262427170927227, 0.0, 1.0, 0.1, 0.5203930198538097], 
reward next is 0.4796, 
noisyNet noise sample is [array([-1.1219902], dtype=float32), -0.05018426]. 
=============================================
[2019-04-09 14:38:38,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05617756 0.11628789 0.06730296 0.07394531 0.06745271 0.05316593
 0.13126147 0.10862733 0.07500216 0.11158744 0.13918929], sum to 1.0000
[2019-04-09 14:38:38,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4726
[2019-04-09 14:38:38,787] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 65.66666666666667, 30.83333333333334, 7.5, 22.5, 23.32734048765388, -0.03338051972069603, 1.0, 1.0, 35.0, 38.44103795075608], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 116400.0000, 
sim time next is 117000.0000, 
raw observation next is [-7.55, 64.5, 37.0, 9.0, 22.5, 23.46680399109615, -0.01736458398657721, 1.0, 1.0, 45.0, 33.8631712193226], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.645, 0.12333333333333334, 0.009944751381215469, 0.375, 0.4555669992580125, 0.49421180533780756, 1.0, 1.0, 0.6, 0.338631712193226], 
reward next is 0.6614, 
noisyNet noise sample is [array([-1.1801025], dtype=float32), -0.029601628]. 
=============================================
[2019-04-09 14:38:38,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[-0.5118928 ]
 [-0.4377607 ]
 [-0.42106348]
 [-0.49965143]
 [-0.46420002]], R is [[0.26800749]
 [0.88091707]
 [1.50498414]
 [2.07442617]
 [2.57943106]].
[2019-04-09 14:38:39,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04362979 0.13983554 0.0629041  0.09743928 0.07212622 0.0517611
 0.11379074 0.10381056 0.09701499 0.07572299 0.14196463], sum to 1.0000
[2019-04-09 14:38:39,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2083
[2019-04-09 14:38:39,570] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 22.5, 24.60746762093473, 0.2142532206018295, 1.0, 1.0, 60.0, 57.51317914621473], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 22.5, 24.66727549595945, 0.2336221203948624, 1.0, 1.0, 25.0, 41.65423463470775], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.375, 0.5556062913299543, 0.5778740401316208, 1.0, 1.0, 0.2, 0.4165423463470775], 
reward next is 0.5835, 
noisyNet noise sample is [array([-0.16645105], dtype=float32), 0.6927773]. 
=============================================
[2019-04-09 14:38:39,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0421703  0.13291176 0.07528943 0.08403147 0.08941757 0.06323354
 0.10780219 0.11147436 0.08904034 0.07737837 0.12725066], sum to 1.0000
[2019-04-09 14:38:39,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0399332  0.15008843 0.07035445 0.10227121 0.06985757 0.05407964
 0.11568229 0.09688368 0.08418671 0.07945222 0.13721061], sum to 1.0000
[2019-04-09 14:38:39,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7478
[2019-04-09 14:38:39,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5457
[2019-04-09 14:38:39,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0492426  0.1337848  0.07404066 0.09414902 0.07673784 0.05834207
 0.10602407 0.10166102 0.09161647 0.08104288 0.13335854], sum to 1.0000
[2019-04-09 14:38:39,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1281
[2019-04-09 14:38:39,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 22.5, 26.25300199977538, 0.5770548539822868, 1.0, 1.0, 40.0, 46.12316329462137], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 140400.0000, 
sim time next is 141000.0000, 
raw observation next is [-6.700000000000001, 61.5, 90.0, 65.33333333333333, 22.5, 26.32167382356701, 0.5813532026277629, 1.0, 1.0, 55.0, 40.44270488641178], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.615, 0.3, 0.0721915285451197, 0.375, 0.6934728186305842, 0.693784400875921, 1.0, 1.0, 0.8, 0.40442704886411784], 
reward next is 0.5956, 
noisyNet noise sample is [array([1.2087889], dtype=float32), 1.8992611]. 
=============================================
[2019-04-09 14:38:39,996] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 61.0, 143.5, 295.0, 22.5, 26.16093585255692, 0.5574701068745297, 1.0, 1.0, 40.0, 43.16797994084443], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 136800.0000, 
sim time next is 137400.0000, 
raw observation next is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 22.5, 26.18183491139914, 0.5565009907302777, 1.0, 1.0, 45.0, 40.74051591457589], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48333333333333334, 0.2563535911602209, 0.375, 0.6818195759499283, 0.6855003302434258, 1.0, 1.0, 0.6, 0.40740515914575887], 
reward next is 0.5926, 
noisyNet noise sample is [array([1.3152127], dtype=float32), 0.6688986]. 
=============================================
[2019-04-09 14:38:40,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.37941518]
 [-0.42555568]
 [-0.38211852]
 [-0.43339065]
 [-0.46215132]], R is [[0.25229177]
 [0.7885372 ]
 [1.22962475]
 [1.56298375]
 [2.10591888]].
[2019-04-09 14:38:40,013] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 22.5, 26.41020514959587, 0.5997581746774933, 1.0, 1.0, 30.0, 35.94705176843127], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 140400.0000, 
sim time next is 141000.0000, 
raw observation next is [-6.700000000000001, 61.5, 90.0, 65.33333333333333, 22.5, 26.38142130644722, 0.4692780751191534, 1.0, 1.0, 20.0, 37.55536808136412], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.615, 0.3, 0.0721915285451197, 0.375, 0.6984517755372682, 0.6564260250397178, 1.0, 1.0, 0.1, 0.3755536808136412], 
reward next is 0.6244, 
noisyNet noise sample is [array([-0.12537184], dtype=float32), -1.6286527]. 
=============================================
[2019-04-09 14:38:40,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-0.4190605 ]
 [-0.4011212 ]
 [-0.47285977]
 [-0.4270356 ]
 [-0.47455016]], R is [[0.17092422]
 [0.80974442]
 [1.41573787]
 [1.99555087]
 [2.58870459]].
[2019-04-09 14:38:40,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04184216 0.1354107  0.07175299 0.08262963 0.07614593 0.06537689
 0.10886162 0.11228597 0.08403346 0.07565357 0.14600709], sum to 1.0000
[2019-04-09 14:38:40,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-09 14:38:40,056] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 64.0, 44.0, 24.0, 22.5, 25.41832579424526, 0.385595445012953, 1.0, 1.0, 30.0, 29.77682476302396], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 144000.0000, 
sim time next is 144600.0000, 
raw observation next is [-6.800000000000001, 65.16666666666667, 38.33333333333333, 17.0, 22.5, 25.44212585864183, 0.3991763393671868, 1.0, 1.0, 65.0, 58.27257795299733], 
processed observation next is [1.0, 0.6956521739130435, 0.2742382271468144, 0.6516666666666667, 0.12777777777777777, 0.01878453038674033, 0.375, 0.6201771548868192, 0.6330587797890622, 1.0, 1.0, 1.0, 0.5827257795299733], 
reward next is 0.4173, 
noisyNet noise sample is [array([-0.10546765], dtype=float32), 2.811178]. 
=============================================
[2019-04-09 14:38:40,068] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03854367 0.12895435 0.07680108 0.09549543 0.07035488 0.06183426
 0.11063111 0.09320121 0.10578646 0.07705624 0.14134127], sum to 1.0000
[2019-04-09 14:38:40,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5053
[2019-04-09 14:38:40,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 62.0, 75.5, 55.16666666666666, 22.5, 25.55583030417756, 0.4888127572846079, 1.0, 1.0, 65.0, 62.24797227743294], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 141600.0000, 
sim time next is 142200.0000, 
raw observation next is [-6.7, 62.5, 61.0, 45.0, 22.5, 24.83477963684544, 0.4285029316142546, 1.0, 1.0, 45.0, 46.05764163206206], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.625, 0.20333333333333334, 0.049723756906077346, 0.375, 0.56956496973712, 0.6428343105380848, 1.0, 1.0, 0.6, 0.4605764163206206], 
reward next is 0.5394, 
noisyNet noise sample is [array([-0.21452548], dtype=float32), 1.0748174]. 
=============================================
[2019-04-09 14:38:40,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03981784 0.12142946 0.07356305 0.08126172 0.06836804 0.05872389
 0.11922469 0.10470243 0.08824505 0.08341098 0.16125293], sum to 1.0000
[2019-04-09 14:38:40,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7349
[2019-04-09 14:38:40,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0460427  0.11205835 0.07695831 0.09278818 0.0700728  0.06186654
 0.11306886 0.09467727 0.09021634 0.08749077 0.15475978], sum to 1.0000
[2019-04-09 14:38:40,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5327
[2019-04-09 14:38:40,318] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 63.5, 49.66666666666667, 31.0, 22.5, 25.85252799606104, 0.4918213659421338, 1.0, 1.0, 35.0, 36.34889514780505], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 143400.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 22.5, 25.92769827014125, 0.4963083325113037, 1.0, 1.0, 45.0, 34.27948378550481], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.375, 0.6606415225117708, 0.6654361108371013, 1.0, 1.0, 0.6, 0.3427948378550481], 
reward next is 0.6572, 
noisyNet noise sample is [array([-0.18910146], dtype=float32), -1.8673738]. 
=============================================
[2019-04-09 14:38:40,320] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 22.5, 25.28429303764892, 0.3252298241673625, 1.0, 1.0, 50.0, 40.38398609033163], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [-7.3, 66.0, 0.0, 0.0, 22.5, 24.22595980269331, 0.2394652665610413, 1.0, 1.0, 55.0, 51.61107855381869], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.66, 0.0, 0.0, 0.375, 0.5188299835577759, 0.5798217555203471, 1.0, 1.0, 0.8, 0.5161107855381869], 
reward next is 0.4839, 
noisyNet noise sample is [array([0.8374632], dtype=float32), -2.7226372]. 
=============================================
[2019-04-09 14:38:40,325] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.39926797]
 [-0.43497694]
 [-0.34397608]
 [-0.46086234]
 [-0.4182338 ]], R is [[0.30520299]
 [0.93866199]
 [1.51675618]
 [2.06548357]
 [2.56983232]].
[2019-04-09 14:38:40,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04179107 0.11030896 0.07524682 0.07384734 0.07833771 0.0635042
 0.12104097 0.1172796  0.09532139 0.08230439 0.1410175 ], sum to 1.0000
[2019-04-09 14:38:40,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1155
[2019-04-09 14:38:40,502] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 22.5, 24.96938179113234, 0.3664044120483501, 1.0, 1.0, 35.0, 39.90957504045544], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 147600.0000, 
sim time next is 148200.0000, 
raw observation next is [-7.3, 69.33333333333334, 0.0, 0.0, 22.5, 25.32853943076545, 0.395096276211568, 1.0, 1.0, 20.0, 31.82000936243144], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6933333333333335, 0.0, 0.0, 0.375, 0.6107116192304541, 0.6316987587371893, 1.0, 1.0, 0.1, 0.3182000936243144], 
reward next is 0.6818, 
noisyNet noise sample is [array([-2.8830845], dtype=float32), -0.23273857]. 
=============================================
[2019-04-09 14:38:40,663] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03444746 0.12795867 0.06825857 0.08975656 0.05905871 0.05215351
 0.12291399 0.10302244 0.1012157  0.08145558 0.15975885], sum to 1.0000
[2019-04-09 14:38:40,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1233
[2019-04-09 14:38:40,684] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.0, 65.33333333333334, 0.0, 0.0, 22.5, 24.59574314987454, 0.2397549415050435, 1.0, 1.0, 65.0, 65.5649846560772], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 22.5, 24.49023740997729, 0.2277111435404565, 1.0, 1.0, 20.0, 52.33153992614149], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.375, 0.5408531174981075, 0.5759037145134854, 1.0, 1.0, 0.1, 0.5233153992614149], 
reward next is 0.4767, 
noisyNet noise sample is [array([-1.1408045], dtype=float32), 0.6027276]. 
=============================================
[2019-04-09 14:38:40,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0416656  0.11698396 0.06669979 0.093744   0.0663395  0.04803899
 0.12262477 0.10522427 0.08797178 0.09101689 0.15969054], sum to 1.0000
[2019-04-09 14:38:40,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3748
[2019-04-09 14:38:40,822] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.9, 64.66666666666667, 0.0, 0.0, 22.5, 24.9375845901541, 0.29618794221216, 1.0, 1.0, 35.0, 36.25707465673848], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [-8.0, 65.33333333333334, 0.0, 0.0, 22.5, 24.83297588686001, 0.2502342913617387, 1.0, 1.0, 35.0, 37.38688612587723], 
processed observation next is [1.0, 0.8260869565217391, 0.24099722991689754, 0.6533333333333334, 0.0, 0.0, 0.375, 0.5694146572383341, 0.5834114304539129, 1.0, 1.0, 0.4, 0.37386886125877233], 
reward next is 0.6261, 
noisyNet noise sample is [array([1.4583453], dtype=float32), -0.8115421]. 
=============================================
[2019-04-09 14:38:40,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.3954816 ]
 [-0.4090235 ]
 [-0.38571802]
 [-0.42267913]
 [-0.4032604 ]], R is [[0.24418372]
 [0.87917113]
 [1.48866129]
 [2.07428837]
 [2.6610477 ]].
[2019-04-09 14:38:41,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04564679 0.10620256 0.07638998 0.06482182 0.06894805 0.05497546
 0.13727003 0.10688043 0.08104072 0.09791683 0.15990739], sum to 1.0000
[2019-04-09 14:38:41,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6824
[2019-04-09 14:38:41,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 19.0, 23.4220746750593, 0.01043748238919824, 0.0, 1.0, 20.0, 33.9304934970496], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 23.40168222474933, 0.00340979666233863, 0.0, 1.0, 55.0, 46.42403015703511], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.45014018539577744, 0.5011365988874462, 0.0, 1.0, 0.8, 0.4642403015703511], 
reward next is 0.5358, 
noisyNet noise sample is [array([-0.03265524], dtype=float32), -0.35947055]. 
=============================================
[2019-04-09 14:38:41,317] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05384988 0.10779428 0.07036467 0.07123324 0.06582403 0.04825256
 0.14331703 0.11563322 0.0618439  0.10628649 0.15560067], sum to 1.0000
[2019-04-09 14:38:41,318] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3470
[2019-04-09 14:38:41,336] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 23.63272392793003, 0.0301683984007135, 0.0, 1.0, 45.0, 67.80456406828762], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 170400.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 23.51863082560109, 0.01916275311730224, 0.0, 1.0, 35.0, 38.97561097654467], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.4598859021334241, 0.506387584372434, 0.0, 1.0, 0.4, 0.3897561097654467], 
reward next is 0.6102, 
noisyNet noise sample is [array([-0.24588947], dtype=float32), 0.08374291]. 
=============================================
[2019-04-09 14:38:41,349] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.68621135]
 [-0.64748645]
 [-0.6634625 ]
 [-0.65290165]
 [-0.75946045]], R is [[-0.07700306]
 [ 0.24572134]
 [ 1.24326408]
 [ 1.71412122]
 [ 2.30071759]].
[2019-04-09 14:38:41,452] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05069207 0.09589218 0.07286085 0.067169   0.05931748 0.05427198
 0.14282495 0.13861129 0.0512539  0.10838304 0.15872324], sum to 1.0000
[2019-04-09 14:38:41,452] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4559
[2019-04-09 14:38:41,465] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 23.09239947622654, -0.07361574438888152, 0.0, 1.0, 55.0, 45.93579591788578], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 170400.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 23.05205102870724, -0.08137792766464207, 0.0, 1.0, 45.0, 42.1535605565189], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.42100425239226996, 0.472874024111786, 0.0, 1.0, 0.6, 0.421535605565189], 
reward next is 0.5785, 
noisyNet noise sample is [array([-0.6836362], dtype=float32), 1.143818]. 
=============================================
[2019-04-09 14:38:41,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[-0.59534574]
 [-0.5409007 ]
 [-0.47870976]
 [-0.49680448]
 [-0.5521141 ]], R is [[0.03810722]
 [0.57836819]
 [1.23950791]
 [1.87868392]
 [2.51979303]].
[2019-04-09 14:38:41,942] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06136179 0.10368068 0.07988621 0.05684172 0.0619884  0.06324263
 0.12580276 0.10608608 0.06920701 0.11303288 0.15886986], sum to 1.0000
[2019-04-09 14:38:41,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-09 14:38:41,964] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.9, 74.66666666666667, 0.0, 0.0, 19.0, 22.2714055905798, -0.2727750151387303, 0.0, 1.0, 40.0, 36.28426822026802], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 180600.0000, 
sim time next is 181200.0000, 
raw observation next is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 19.0, 22.23101314716163, -0.2453303370747709, 0.0, 1.0, 55.0, 51.19358302856558], 
processed observation next is [1.0, 0.08695652173913043, 0.2160664819944598, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.3525844289301358, 0.4182232209750764, 0.0, 1.0, 0.8, 0.5119358302856558], 
reward next is 0.4881, 
noisyNet noise sample is [array([-0.35937274], dtype=float32), -0.09543641]. 
=============================================
[2019-04-09 14:38:41,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06425571 0.0795887  0.08011615 0.05508883 0.05872246 0.05437411
 0.14239374 0.12736432 0.06116467 0.12254574 0.15438554], sum to 1.0000
[2019-04-09 14:38:41,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0879
[2019-04-09 14:38:41,997] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.900000000000002, 74.0, 0.0, 0.0, 19.0, 23.43174174364227, -0.02776211787395684, 0.0, 1.0, 50.0, 40.21119817086118], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 177600.0000, 
sim time next is 178200.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 19.0, 23.33835768363775, -0.05259916137147267, 0.0, 1.0, 20.0, 33.30690534422791], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.08333333333333333, 0.4448631403031458, 0.4824669462095091, 0.0, 1.0, 0.1, 0.33306905344227905], 
reward next is 0.6669, 
noisyNet noise sample is [array([2.4339538], dtype=float32), 0.6552901]. 
=============================================
[2019-04-09 14:38:42,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05824953 0.09477858 0.08382639 0.04481754 0.06914628 0.06003146
 0.1299452  0.11766355 0.07005914 0.12018273 0.15129969], sum to 1.0000
[2019-04-09 14:38:42,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2119
[2019-04-09 14:38:42,259] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 22.60031721371652, -0.2184594359761225, 0.0, 1.0, 25.0, 40.49510165064065], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 187800.0000, 
sim time next is 188400.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 22.58550621140012, -0.234503210581097, 0.0, 1.0, 25.0, 32.30621023797846], 
processed observation next is [1.0, 0.17391304347826086, 0.2160664819944598, 0.78, 0.0, 0.0, 0.08333333333333333, 0.3821255176166766, 0.42183226313963434, 0.0, 1.0, 0.2, 0.3230621023797846], 
reward next is 0.6769, 
noisyNet noise sample is [array([1.1366625], dtype=float32), -0.45231172]. 
=============================================
[2019-04-09 14:38:42,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05932637 0.09488642 0.06964944 0.06216964 0.06021729 0.05828477
 0.14515878 0.10602673 0.08030335 0.11228687 0.15169033], sum to 1.0000
[2019-04-09 14:38:42,377] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5958
[2019-04-09 14:38:42,389] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 22.73869476879665, -0.2324876409615103, 0.0, 1.0, 55.0, 44.75915720457813], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 22.69821911494532, -0.2430326093698082, 0.0, 1.0, 50.0, 40.01507512064268], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.08333333333333333, 0.39151825957877673, 0.4189891302100639, 0.0, 1.0, 0.7, 0.4001507512064268], 
reward next is 0.5998, 
noisyNet noise sample is [array([0.7673179], dtype=float32), -1.6518849]. 
=============================================
[2019-04-09 14:38:42,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05755181 0.10279655 0.07784005 0.05655184 0.06242649 0.05462319
 0.13799265 0.10861994 0.07115129 0.12227048 0.14817572], sum to 1.0000
[2019-04-09 14:38:42,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-09 14:38:42,498] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.18010545173247, -0.115431398737975, 0.0, 1.0, 60.0, 52.27791021039619], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 23.2198961165085, -0.1184867303870722, 0.0, 1.0, 20.0, 46.35411292525177], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 0.08333333333333333, 0.4349913430423751, 0.46050442320430923, 0.0, 1.0, 0.1, 0.46354112925251767], 
reward next is 0.5365, 
noisyNet noise sample is [array([0.02803523], dtype=float32), 0.1094916]. 
=============================================
[2019-04-09 14:38:42,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.6554693]
 [-0.5796984]
 [-0.5622755]
 [-0.4995392]
 [-0.5554492]], R is [[-1.69533491e-03]
 [ 4.75542516e-01]
 [ 1.05570817e+00]
 [ 1.51590312e+00]
 [ 1.87609684e+00]].
[2019-04-09 14:38:42,801] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04983004 0.09463123 0.0694112  0.05218063 0.06134592 0.05283316
 0.15226023 0.10120115 0.08365356 0.11231557 0.17033735], sum to 1.0000
[2019-04-09 14:38:42,801] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9343
[2019-04-09 14:38:42,815] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 22.5, 22.77915832429294, -0.2673767449242522, 0.0, 1.0, 20.0, 35.50001442236471], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 22.5, 22.54458458555985, -0.2767779497493329, 1.0, 1.0, 65.0, 60.85768671411055], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.375, 0.3787153821299875, 0.407740683416889, 1.0, 1.0, 1.0, 0.6085768671411055], 
reward next is 0.3914, 
noisyNet noise sample is [array([1.6724952], dtype=float32), 0.1459657]. 
=============================================
[2019-04-09 14:38:43,094] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06306417 0.08816026 0.06960025 0.06389384 0.05874997 0.06662779
 0.1222911  0.11300153 0.09270481 0.1136639  0.1482424 ], sum to 1.0000
[2019-04-09 14:38:43,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6948
[2019-04-09 14:38:43,155] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 22.5, 22.70651759070048, -0.2207715129994744, 1.0, 1.0, 50.0, 49.87724441280563], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 201600.0000, 
sim time next is 202200.0000, 
raw observation next is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 22.5, 22.91096555724641, -0.1894906743508487, 1.0, 1.0, 55.0, 65.97644855208716], 
processed observation next is [1.0, 0.34782608695652173, 0.21837488457987075, 0.78, 0.07555555555555557, 0.22467771639042353, 0.375, 0.4092471297705342, 0.43683644188305043, 1.0, 1.0, 0.8, 0.6597644855208716], 
reward next is 0.3402, 
noisyNet noise sample is [array([0.7725574], dtype=float32), 0.13268963]. 
=============================================
[2019-04-09 14:38:43,415] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04821368 0.12396623 0.07287487 0.1017153  0.0662759  0.05182685
 0.10564085 0.09817062 0.09721636 0.0866427  0.14745669], sum to 1.0000
[2019-04-09 14:38:43,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-09 14:38:43,425] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 22.5, 24.11128592951563, 0.0063967352432013, 1.0, 1.0, 35.0, 35.42012293654307], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 207600.0000, 
sim time next is 208200.0000, 
raw observation next is [-7.483333333333333, 75.5, 94.0, 0.0, 22.5, 24.13290457119967, -0.06460468872306573, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25530932594644506, 0.755, 0.31333333333333335, 0.0, 0.375, 0.5110753809333058, 0.47846510375897805, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6797574], dtype=float32), -0.91945916]. 
=============================================
[2019-04-09 14:38:43,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04202518 0.13620484 0.07034598 0.09834288 0.08136024 0.05538233
 0.10980492 0.10146756 0.09231333 0.08874407 0.12400859], sum to 1.0000
[2019-04-09 14:38:43,516] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2241
[2019-04-09 14:38:43,534] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 22.5, 24.62035537651637, 0.15533796075849, 1.0, 1.0, 65.0, 68.6583610856982], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 213600.0000, 
sim time next is 214200.0000, 
raw observation next is [-5.6, 68.5, 153.0, 0.0, 22.5, 24.69246831254555, 0.1830479914971605, 1.0, 1.0, 25.0, 52.43696041241419], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.685, 0.51, 0.0, 0.375, 0.5577056927121292, 0.5610159971657201, 1.0, 1.0, 0.2, 0.5243696041241419], 
reward next is 0.4756, 
noisyNet noise sample is [array([1.163555], dtype=float32), -1.6807858]. 
=============================================
[2019-04-09 14:38:44,054] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04870709 0.12787643 0.07299713 0.08933075 0.0688396  0.05515704
 0.10938127 0.10911009 0.09941031 0.08474944 0.13444094], sum to 1.0000
[2019-04-09 14:38:44,055] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1813
[2019-04-09 14:38:44,078] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.2, 61.0, 49.66666666666667, 0.0, 22.5, 24.09515510378658, 0.1150260159525777, 1.0, 1.0, 20.0, 35.76252839012209], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 229200.0000, 
sim time next is 229800.0000, 
raw observation next is [-3.3, 61.5, 43.33333333333334, 0.0, 22.5, 24.49055199343766, 0.1411508881691445, 1.0, 1.0, 25.0, 33.35894104373816], 
processed observation next is [1.0, 0.6521739130434783, 0.37119113573407203, 0.615, 0.1444444444444445, 0.0, 0.375, 0.5408793327864716, 0.5470502960563816, 1.0, 1.0, 0.2, 0.3335894104373816], 
reward next is 0.6664, 
noisyNet noise sample is [array([0.7338746], dtype=float32), -0.69916975]. 
=============================================
[2019-04-09 14:38:44,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04149958 0.1532834  0.06684751 0.09376346 0.07098836 0.04756391
 0.11434595 0.10455552 0.08748661 0.07343756 0.14622812], sum to 1.0000
[2019-04-09 14:38:44,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-09 14:38:44,452] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 22.5, 24.73395410029527, 0.1575354594286006, 1.0, 1.0, 40.0, 33.5568914322368], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 22.5, 24.75165439330848, 0.05387981729394849, 1.0, 1.0, 40.0, 34.79839525149215], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.375, 0.5626378661090401, 0.5179599390979829, 1.0, 1.0, 0.5, 0.34798395251492154], 
reward next is 0.6520, 
noisyNet noise sample is [array([-0.28536573], dtype=float32), -0.5438788]. 
=============================================
[2019-04-09 14:38:44,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03833117 0.14450388 0.0679311  0.09704829 0.06272976 0.0441243
 0.13463399 0.1012856  0.08483736 0.07594892 0.14862557], sum to 1.0000
[2019-04-09 14:38:44,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2576
[2019-04-09 14:38:44,622] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 22.5, 25.29733149274076, 0.1967119943562312, 1.0, 1.0, 50.0, 38.73687805423374], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 22.5, 24.84175051987834, 0.2078465128146005, 1.0, 1.0, 30.0, 34.99430550044658], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.375, 0.5701458766565283, 0.5692821709382002, 1.0, 1.0, 0.3, 0.34994305500446576], 
reward next is 0.6501, 
noisyNet noise sample is [array([-3.5586967], dtype=float32), 0.014787561]. 
=============================================
[2019-04-09 14:38:44,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03803715 0.12103291 0.0773986  0.08852863 0.06952331 0.05380481
 0.12319595 0.11171989 0.09364228 0.07934994 0.14376654], sum to 1.0000
[2019-04-09 14:38:44,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4739
[2019-04-09 14:38:44,680] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 22.5, 25.24457572066691, 0.337209456638722, 1.0, 1.0, 65.0, 63.55240365530549], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 22.5, 25.43313822996093, 0.3632442227246908, 1.0, 1.0, 45.0, 46.86675213889651], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.375, 0.6194281858300776, 0.6210814075748969, 1.0, 1.0, 0.6, 0.46866752138896506], 
reward next is 0.5313, 
noisyNet noise sample is [array([-0.36863098], dtype=float32), -0.14787255]. 
=============================================
[2019-04-09 14:38:44,783] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04116729 0.12422165 0.07248246 0.08554994 0.07117918 0.05619676
 0.1154434  0.11937548 0.07963807 0.08362339 0.15112233], sum to 1.0000
[2019-04-09 14:38:44,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1211
[2019-04-09 14:38:44,803] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 23.85350847332587, -0.01017088241652088, 1.0, 1.0, 30.0, 33.87705657299048], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 238800.0000, 
sim time next is 239400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 23.78922676414197, -0.03968529267223807, 1.0, 1.0, 20.0, 32.62967187448183], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.4824355636784974, 0.486771569109254, 1.0, 1.0, 0.1, 0.3262967187448183], 
reward next is 0.6737, 
noisyNet noise sample is [array([0.5666477], dtype=float32), -2.9550858]. 
=============================================
[2019-04-09 14:38:44,810] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0359134  0.13364494 0.0773216  0.08616983 0.05720482 0.05381238
 0.10911594 0.10320479 0.09916842 0.08221728 0.16222651], sum to 1.0000
[2019-04-09 14:38:44,816] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4478
[2019-04-09 14:38:44,830] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.26403305499199, 0.1168020372558524, 1.0, 1.0, 40.0, 28.3382790747412], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 235800.0000, 
sim time next is 236400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.2807522377088, 0.07774004193167722, 1.0, 1.0, 45.0, 33.02457856450399], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.5233960198090667, 0.525913347310559, 1.0, 1.0, 0.6, 0.3302457856450399], 
reward next is 0.6698, 
noisyNet noise sample is [array([1.2468776], dtype=float32), -1.3560677]. 
=============================================
[2019-04-09 14:38:45,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04576955 0.12566048 0.06990257 0.09914639 0.06618235 0.05462418
 0.12076764 0.10679709 0.09357747 0.07824765 0.13932456], sum to 1.0000
[2019-04-09 14:38:45,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-09 14:38:45,061] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.4273679804846, 0.1121095821608579, 0.0, 1.0, 35.0, 39.70243208478988], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 242400.0000, 
sim time next is 243000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.25175418257108, 0.1012070268046813, 1.0, 1.0, 65.0, 64.41241079063516], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.5209795152142567, 0.5337356756015604, 1.0, 1.0, 1.0, 0.6441241079063517], 
reward next is 0.3559, 
noisyNet noise sample is [array([0.8572882], dtype=float32), 1.2868352]. 
=============================================
[2019-04-09 14:38:45,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.32868275]
 [-0.33502823]
 [-0.38239795]
 [-0.38963947]
 [-0.34335697]], R is [[0.05124611]
 [0.65370929]
 [1.27479625]
 [1.79419267]
 [2.2579782 ]].
[2019-04-09 14:38:45,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04728289 0.13510908 0.07038517 0.08549155 0.06687185 0.06406537
 0.11811594 0.11682207 0.07822736 0.07857865 0.13905011], sum to 1.0000
[2019-04-09 14:38:45,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2525
[2019-04-09 14:38:45,172] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 23.1131912787101, -0.1022770758743317, 0.0, 1.0, 35.0, 29.58739497703801], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 247800.0000, 
sim time next is 248400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 23.02405405337777, -0.1250773683675002, 0.0, 1.0, 25.0, 28.29898082822277], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.4186711711148143, 0.45830754387749995, 0.0, 1.0, 0.2, 0.2829898082822277], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.07143126], dtype=float32), -1.8861607]. 
=============================================
[2019-04-09 14:38:45,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05239131 0.10331021 0.06243135 0.07424716 0.06995816 0.04984685
 0.12219483 0.12331979 0.07783454 0.11347866 0.15098709], sum to 1.0000
[2019-04-09 14:38:45,193] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6126
[2019-04-09 14:38:45,205] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 22.82526718135459, -0.1344316849997909, 0.0, 1.0, 25.0, 38.89308299450246], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 249600.0000, 
sim time next is 250200.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 19.0, 22.89567168908327, -0.1403435321558187, 0.0, 1.0, 50.0, 39.54171935677431], 
processed observation next is [1.0, 0.9130434782608695, 0.3614958448753463, 0.7, 0.0, 0.0, 0.08333333333333333, 0.40797264075693906, 0.4532188226147271, 0.0, 1.0, 0.7, 0.3954171935677431], 
reward next is 0.6046, 
noisyNet noise sample is [array([1.4589578], dtype=float32), -0.8273276]. 
=============================================
[2019-04-09 14:38:45,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0534169  0.10390879 0.06204025 0.07121658 0.07081021 0.05539101
 0.12414242 0.11975563 0.07552382 0.09882279 0.16497163], sum to 1.0000
[2019-04-09 14:38:45,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5042
[2019-04-09 14:38:45,263] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 22.98802371911739, -0.1439854231541582, 0.0, 1.0, 25.0, 34.4552580232474], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 249600.0000, 
sim time next is 250200.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 19.0, 22.94704062217962, -0.1612895094357364, 0.0, 1.0, 45.0, 33.66444483329816], 
processed observation next is [1.0, 0.9130434782608695, 0.3614958448753463, 0.7, 0.0, 0.0, 0.08333333333333333, 0.41225338518163507, 0.4462368301880879, 0.0, 1.0, 0.6, 0.3366444483329816], 
reward next is 0.6634, 
noisyNet noise sample is [array([-1.5278783], dtype=float32), 0.93728864]. 
=============================================
[2019-04-09 14:38:45,903] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03410841 0.12919638 0.06543674 0.09795935 0.07060495 0.0550908
 0.1123156  0.0926718  0.10762466 0.07469927 0.16029209], sum to 1.0000
[2019-04-09 14:38:45,906] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-09 14:38:45,965] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.1, 60.5, 117.0, 0.0, 22.5, 27.26137881410775, 0.5903809898153995, 1.0, 1.0, 45.0, 45.34272456991346], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 225000.0000, 
sim time next is 225600.0000, 
raw observation next is [-3.0, 60.0, 106.8333333333333, 0.0, 22.5, 26.70536349136063, 0.6290357787604614, 1.0, 1.0, 25.0, 36.81587256739004], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.6, 0.356111111111111, 0.0, 0.375, 0.7254469576133857, 0.7096785929201538, 1.0, 1.0, 0.2, 0.3681587256739004], 
reward next is 0.6318, 
noisyNet noise sample is [array([-0.48003796], dtype=float32), -1.205074]. 
=============================================
[2019-04-09 14:38:46,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05983585 0.08610266 0.07347684 0.05046744 0.06018678 0.05227415
 0.13574263 0.12984762 0.07226288 0.1122682  0.16753484], sum to 1.0000
[2019-04-09 14:38:46,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3729
[2019-04-09 14:38:46,068] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 19.0, 22.88106620747726, -0.1919871007285602, 0.0, 1.0, 25.0, 33.49171038485138], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 19.0, 22.81865190585181, -0.2051758438967316, 0.0, 1.0, 40.0, 33.40271747619284], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.08333333333333333, 0.40155432548765074, 0.43160805203442276, 0.0, 1.0, 0.5, 0.3340271747619284], 
reward next is 0.6660, 
noisyNet noise sample is [array([-2.1508813], dtype=float32), 0.2409941]. 
=============================================
[2019-04-09 14:38:46,130] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.06181283 0.0904517  0.07723422 0.05944413 0.06756036 0.04892786
 0.13239458 0.12094476 0.07612555 0.11463125 0.15047269], sum to 1.0000
[2019-04-09 14:38:46,132] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4266
[2019-04-09 14:38:46,151] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 19.0, 23.92789745505283, 0.03970621490875991, 0.0, 1.0, 50.0, 42.33485753384662], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 19.0, 23.87981541808206, 0.02656659466662467, 0.0, 1.0, 45.0, 40.78736715218724], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.08333333333333333, 0.489984618173505, 0.5088555315555415, 0.0, 1.0, 0.6, 0.40787367152187237], 
reward next is 0.5921, 
noisyNet noise sample is [array([-1.2676224], dtype=float32), 0.081315115]. 
=============================================
[2019-04-09 14:38:46,303] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05051832 0.10818223 0.06922065 0.06299164 0.06315807 0.05766213
 0.12495682 0.11782118 0.07621565 0.10313344 0.16613986], sum to 1.0000
[2019-04-09 14:38:46,307] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7045
[2019-04-09 14:38:46,325] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.1, 69.0, 0.0, 0.0, 19.0, 23.64609215518319, 0.002307524033837933, 0.0, 1.0, 50.0, 46.94898401675312], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 268200.0000, 
sim time next is 268800.0000, 
raw observation next is [-8.366666666666667, 68.33333333333333, 0.0, 0.0, 19.0, 23.69867935326302, -0.02448118300185024, 0.0, 1.0, 55.0, 47.41242591668108], 
processed observation next is [1.0, 0.08695652173913043, 0.23084025854108958, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.47488994610525176, 0.4918396056660499, 0.0, 1.0, 0.8, 0.4741242591668108], 
reward next is 0.5259, 
noisyNet noise sample is [array([-1.4108049], dtype=float32), 0.2272424]. 
=============================================
[2019-04-09 14:38:46,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05822885 0.0977505  0.0782377  0.06314181 0.06780773 0.05540703
 0.12335987 0.10731936 0.08418486 0.1125881  0.15197413], sum to 1.0000
[2019-04-09 14:38:46,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5099
[2019-04-09 14:38:46,600] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 70.0, 0.0, 0.0, 19.0, 22.45330861781012, -0.2476272774526992, 0.0, 1.0, 25.0, 49.53623008876538], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 273600.0000, 
sim time next is 274200.0000, 
raw observation next is [-9.683333333333334, 69.5, 0.0, 0.0, 19.0, 22.47152527601443, -0.2526467600682906, 0.0, 1.0, 45.0, 37.93972634049325], 
processed observation next is [1.0, 0.17391304347826086, 0.19436749769159742, 0.695, 0.0, 0.0, 0.08333333333333333, 0.3726271063345357, 0.41578441331056976, 0.0, 1.0, 0.6, 0.3793972634049325], 
reward next is 0.6206, 
noisyNet noise sample is [array([-1.4296837], dtype=float32), 1.375601]. 
=============================================
[2019-04-09 14:38:46,873] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05795357 0.09524123 0.07562428 0.06568769 0.06458019 0.05998561
 0.13851768 0.1057748  0.08333714 0.11332136 0.13997652], sum to 1.0000
[2019-04-09 14:38:46,874] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6442
[2019-04-09 14:38:46,889] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.51666666666667, 69.5, 0.0, 0.0, 19.0, 23.28429329399936, -0.1208593584270892, 0.0, 1.0, 45.0, 42.94200404521407], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 280200.0000, 
sim time next is 280800.0000, 
raw observation next is [-11.7, 70.0, 0.0, 0.0, 19.0, 23.20957371775902, -0.1442157133376617, 0.0, 1.0, 50.0, 40.6766004551637], 
processed observation next is [1.0, 0.2608695652173913, 0.13850415512465375, 0.7, 0.0, 0.0, 0.08333333333333333, 0.4341311431465851, 0.45192809555411273, 0.0, 1.0, 0.7, 0.40676600455163703], 
reward next is 0.5932, 
noisyNet noise sample is [array([0.45476854], dtype=float32), -0.30236515]. 
=============================================
[2019-04-09 14:38:47,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06017516 0.09552435 0.07484761 0.06407838 0.06435237 0.05672547
 0.13482097 0.11039405 0.08164462 0.10926322 0.14817381], sum to 1.0000
[2019-04-09 14:38:47,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3324
[2019-04-09 14:38:47,122] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.51666666666667, 69.5, 0.0, 0.0, 19.0, 22.79095019634098, -0.2065545790502419, 0.0, 1.0, 25.0, 43.96160493445096], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 280200.0000, 
sim time next is 280800.0000, 
raw observation next is [-11.7, 70.0, 0.0, 0.0, 19.0, 22.78809360974236, -0.2219354339145784, 0.0, 1.0, 35.0, 43.4696666559738], 
processed observation next is [1.0, 0.2608695652173913, 0.13850415512465375, 0.7, 0.0, 0.0, 0.08333333333333333, 0.39900780081186343, 0.4260215220284738, 0.0, 1.0, 0.4, 0.43469666655973804], 
reward next is 0.5653, 
noisyNet noise sample is [array([1.2423238], dtype=float32), -0.4165699]. 
=============================================
[2019-04-09 14:38:47,149] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05346493 0.10408456 0.07217696 0.06999683 0.06564929 0.05457627
 0.12206659 0.10120654 0.09013554 0.11302191 0.15362059], sum to 1.0000
[2019-04-09 14:38:47,149] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0175
[2019-04-09 14:38:47,178] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 22.5, 22.50125680941563, -0.2696977938243187, 1.0, 1.0, 50.0, 74.68917723515705], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 22.5, 22.37717744467954, -0.3384871130424021, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.375, 0.36476478705662824, 0.3871709623191993, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1724854], dtype=float32), 0.64900714]. 
=============================================
[2019-04-09 14:38:47,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06045893 0.10559822 0.08111593 0.08583596 0.06987943 0.05398526
 0.11107356 0.10404851 0.08798112 0.1089253  0.13109778], sum to 1.0000
[2019-04-09 14:38:47,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-09 14:38:47,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.71666666666667, 69.5, 9.999999999999998, 145.3333333333333, 22.5, 22.4062969257432, -0.2497911901956851, 1.0, 1.0, 25.0, 50.05339675755405], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 287400.0000, 
sim time next is 288000.0000, 
raw observation next is [-12.8, 70.0, 15.0, 205.5, 22.5, 22.64101712573829, -0.2417522564151776, 1.0, 1.0, 45.0, 44.13838514760708], 
processed observation next is [1.0, 0.34782608695652173, 0.1080332409972299, 0.7, 0.05, 0.22707182320441988, 0.375, 0.38675142714485755, 0.41941591452827415, 1.0, 1.0, 0.6, 0.44138385147607084], 
reward next is 0.5586, 
noisyNet noise sample is [array([-1.1014439], dtype=float32), 0.54794836]. 
=============================================
[2019-04-09 14:38:47,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-0.36473432]
 [-0.41599947]
 [-0.44472247]
 [-0.50974315]
 [-0.4176187 ]], R is [[0.14179501]
 [0.63984311]
 [1.061939  ]
 [1.33935463]
 [1.59134364]].
[2019-04-09 14:38:47,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05285687 0.11307652 0.07464714 0.08472919 0.07401089 0.0582287
 0.11030358 0.10078246 0.07116728 0.117641   0.14255652], sum to 1.0000
[2019-04-09 14:38:47,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2938
[2019-04-09 14:38:47,492] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 22.5, 22.59416167789752, -0.2523405910434857, 1.0, 1.0, 50.0, 40.37789853708262], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 288600.0000, 
sim time next is 289200.0000, 
raw observation next is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 22.5, 22.64274170962712, -0.2227062380045131, 1.0, 1.0, 30.0, 41.31549940693325], 
processed observation next is [1.0, 0.34782608695652173, 0.11265004616805181, 0.69, 0.08333333333333333, 0.3600368324125231, 0.375, 0.3868951424689267, 0.425764587331829, 1.0, 1.0, 0.3, 0.4131549940693325], 
reward next is 0.5868, 
noisyNet noise sample is [array([-0.17163184], dtype=float32), 0.38935104]. 
=============================================
[2019-04-09 14:38:47,550] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7651: loss 21.4654
[2019-04-09 14:38:47,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7652: learning rate 0.0000
[2019-04-09 14:38:47,642] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05300453 0.11099625 0.07405668 0.08262439 0.06614545 0.05338721
 0.11091333 0.09900549 0.11075215 0.10326668 0.13584788], sum to 1.0000
[2019-04-09 14:38:47,642] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3308
[2019-04-09 14:38:47,694] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 22.5, 22.47864786925022, -0.2520265351621835, 1.0, 1.0, 60.0, 59.82310299091456], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 288600.0000, 
sim time next is 289200.0000, 
raw observation next is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 22.5, 22.58543048412094, -0.1966001215383686, 1.0, 1.0, 35.0, 48.88548022370298], 
processed observation next is [1.0, 0.34782608695652173, 0.11265004616805181, 0.69, 0.08333333333333333, 0.3600368324125231, 0.375, 0.38211920701007845, 0.43446662615387716, 1.0, 1.0, 0.4, 0.48885480223702976], 
reward next is 0.5111, 
noisyNet noise sample is [array([-1.5082334], dtype=float32), -0.11213023]. 
=============================================
[2019-04-09 14:38:47,825] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 7732: loss 19.3288
[2019-04-09 14:38:47,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 7732: learning rate 0.0000
[2019-04-09 14:38:47,928] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7769: loss 21.4386
[2019-04-09 14:38:47,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7771: learning rate 0.0000
[2019-04-09 14:38:47,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05361021 0.12316238 0.08130683 0.0892456  0.06555405 0.05939195
 0.1086687  0.10883306 0.10350944 0.08090691 0.12581089], sum to 1.0000
[2019-04-09 14:38:47,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7478
[2019-04-09 14:38:47,971] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-11.7, 63.0, 91.0, 447.5, 22.5, 23.96985882145591, 0.04516387760118337, 1.0, 1.0, 25.0, 54.43324398743679], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 295200.0000, 
sim time next is 295800.0000, 
raw observation next is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 22.5, 24.09722994271882, 0.06232130274186013, 1.0, 1.0, 55.0, 39.08020606175126], 
processed observation next is [1.0, 0.43478260869565216, 0.14358264081255764, 0.625, 0.2988888888888889, 0.518232044198895, 0.375, 0.5081024952265683, 0.52077376758062, 1.0, 1.0, 0.8, 0.3908020606175126], 
reward next is 0.6092, 
noisyNet noise sample is [array([-1.2615249], dtype=float32), -0.645916]. 
=============================================
[2019-04-09 14:38:48,113] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7843: loss 26.7693
[2019-04-09 14:38:48,114] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7843: learning rate 0.0000
[2019-04-09 14:38:48,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04601216 0.12893845 0.07747879 0.11323465 0.07133995 0.05833759
 0.10761783 0.09684386 0.09851824 0.06933325 0.13234526], sum to 1.0000
[2019-04-09 14:38:48,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8364
[2019-04-09 14:38:48,147] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 22.5, 24.21330642013184, 0.1081496230145574, 1.0, 1.0, 60.0, 55.61725421024506], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 22.5, 24.41528368948248, 0.154482363078133, 1.0, 1.0, 60.0, 58.84046149097394], 
processed observation next is [1.0, 0.43478260869565216, 0.1638965835641737, 0.605, 0.3111111111111111, 0.6195211786372007, 0.375, 0.5346069741235399, 0.5514941210260443, 1.0, 1.0, 0.9, 0.5884046149097394], 
reward next is 0.4116, 
noisyNet noise sample is [array([1.0081466], dtype=float32), -1.3018169]. 
=============================================
[2019-04-09 14:38:48,217] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7892: loss 29.7141
[2019-04-09 14:38:48,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7892: learning rate 0.0000
[2019-04-09 14:38:48,233] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7902: loss 20.9310
[2019-04-09 14:38:48,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7902: learning rate 0.0000
[2019-04-09 14:38:48,290] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7927: loss 29.6862
[2019-04-09 14:38:48,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7927: learning rate 0.0000
[2019-04-09 14:38:48,355] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7958: loss 25.4631
[2019-04-09 14:38:48,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7958: learning rate 0.0000
[2019-04-09 14:38:48,357] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7958: loss 22.8413
[2019-04-09 14:38:48,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7958: learning rate 0.0000
[2019-04-09 14:38:48,487] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8021: loss 21.9020
[2019-04-09 14:38:48,488] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8021: learning rate 0.0000
[2019-04-09 14:38:48,500] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04913894 0.12424716 0.08104368 0.08721265 0.0746457  0.06645252
 0.10186244 0.11637089 0.07846288 0.08209635 0.13846672], sum to 1.0000
[2019-04-09 14:38:48,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0696
[2019-04-09 14:38:48,516] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 22.5, 24.84015883766401, 0.2332302683252741, 1.0, 1.0, 20.0, 32.15799540882652], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 312000.0000, 
sim time next is 312600.0000, 
raw observation next is [-9.5, 42.33333333333333, 78.0, 574.3333333333334, 22.5, 24.8489886024588, 0.2203262093932481, 1.0, 1.0, 20.0, 30.69610210262401], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4233333333333333, 0.26, 0.6346224677716391, 0.375, 0.5707490502049, 0.5734420697977494, 1.0, 1.0, 0.1, 0.3069610210262401], 
reward next is 0.6930, 
noisyNet noise sample is [array([0.7364165], dtype=float32), 0.71958286]. 
=============================================
[2019-04-09 14:38:48,572] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8060: loss 26.5964
[2019-04-09 14:38:48,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8060: learning rate 0.0000
[2019-04-09 14:38:48,612] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8079: loss 26.1611
[2019-04-09 14:38:48,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8079: learning rate 0.0000
[2019-04-09 14:38:48,658] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8101: loss 17.1903
[2019-04-09 14:38:48,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8102: learning rate 0.0000
[2019-04-09 14:38:48,683] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8112: loss 21.6082
[2019-04-09 14:38:48,685] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8112: learning rate 0.0000
[2019-04-09 14:38:48,727] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8132: loss 23.5414
[2019-04-09 14:38:48,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8133: learning rate 0.0000
[2019-04-09 14:38:49,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04362104 0.11482037 0.08715823 0.09877622 0.06793974 0.06870347
 0.10712509 0.11216531 0.09794112 0.07763921 0.12411021], sum to 1.0000
[2019-04-09 14:38:49,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2120
[2019-04-09 14:38:49,018] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.5, 42.0, 47.0, 358.5, 22.5, 24.8013209851595, 0.2356311460157248, 1.0, 1.0, 45.0, 33.18119783743341], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 316800.0000, 
sim time next is 317400.0000, 
raw observation next is [-9.683333333333334, 43.16666666666666, 39.33333333333333, 319.0, 22.5, 24.18497572675381, 0.1692281831917709, 1.0, 1.0, 25.0, 30.7172788039394], 
processed observation next is [1.0, 0.6956521739130435, 0.19436749769159742, 0.4316666666666666, 0.1311111111111111, 0.3524861878453039, 0.375, 0.5154146438961508, 0.556409394397257, 1.0, 1.0, 0.2, 0.307172788039394], 
reward next is 0.6928, 
noisyNet noise sample is [array([1.4003696], dtype=float32), -1.0313019]. 
=============================================
[2019-04-09 14:38:49,295] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04034987 0.12185375 0.06851312 0.09089544 0.06226495 0.05970757
 0.12456921 0.11161329 0.08530616 0.08157169 0.15335488], sum to 1.0000
[2019-04-09 14:38:49,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2606
[2019-04-09 14:38:49,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 22.5, 23.92835763857511, 0.04756213247186408, 1.0, 1.0, 20.0, 39.72927318571274], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 328800.0000, 
sim time next is 329400.0000, 
raw observation next is [-12.55, 66.5, 0.0, 0.0, 22.5, 23.8284405857543, 0.01508070722934189, 0.0, 1.0, 30.0, 40.34490863281741], 
processed observation next is [1.0, 0.8260869565217391, 0.11495844875346259, 0.665, 0.0, 0.0, 0.375, 0.48570338214619174, 0.5050269024097807, 0.0, 1.0, 0.3, 0.40344908632817406], 
reward next is 0.5966, 
noisyNet noise sample is [array([-0.40306154], dtype=float32), -1.5443391]. 
=============================================
[2019-04-09 14:38:49,416] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04596175 0.10735288 0.0752034  0.08384535 0.07051632 0.0573475
 0.12268598 0.11770434 0.08727707 0.08386819 0.14823714], sum to 1.0000
[2019-04-09 14:38:49,418] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5995
[2019-04-09 14:38:49,438] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04346703 0.10265308 0.08278669 0.06726327 0.07561193 0.04884288
 0.14232425 0.10646386 0.0685416  0.11046606 0.15157925], sum to 1.0000
[2019-04-09 14:38:49,440] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-12.8, 72.33333333333334, 0.0, 0.0, 19.0, 22.6601830445104, -0.1705351557099084, 0.0, 1.0, 25.0, 36.17100332809365], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 332400.0000, 
sim time next is 333000.0000, 
raw observation next is [-12.8, 73.5, 0.0, 0.0, 19.0, 22.54067120018742, -0.1910044591792915, 0.0, 1.0, 50.0, 42.75471473680897], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.735, 0.0, 0.0, 0.08333333333333333, 0.37838926668228484, 0.4363318469402362, 0.0, 1.0, 0.7, 0.4275471473680897], 
reward next is 0.5725, 
noisyNet noise sample is [array([1.2129987], dtype=float32), 0.67205775]. 
=============================================
[2019-04-09 14:38:49,441] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7599
[2019-04-09 14:38:49,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[-0.3030184 ]
 [-0.37207976]
 [-0.25150543]
 [-0.26371926]
 [-0.329453  ]], R is [[0.34362507]
 [0.97847879]
 [1.36336279]
 [2.34972906]
 [2.89804029]].
[2019-04-09 14:38:49,457] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-13.56666666666667, 78.0, 0.0, 0.0, 19.0, 21.73001963825598, -0.4015740309446573, 0.0, 1.0, 65.0, 61.26937629241313], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 19.0, 21.68907241222693, -0.383706126693259, 0.0, 1.0, 60.0, 76.65407472074361], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.08333333333333333, 0.30742270101891095, 0.37209795776891363, 0.0, 1.0, 0.9, 0.766540747207436], 
reward next is 0.2335, 
noisyNet noise sample is [array([-0.58304673], dtype=float32), -1.154179]. 
=============================================
[2019-04-09 14:38:49,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04535211 0.11276146 0.08278357 0.08699447 0.07266003 0.06609104
 0.10957853 0.12389709 0.08128466 0.08439746 0.13419954], sum to 1.0000
[2019-04-09 14:38:49,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8970
[2019-04-09 14:38:49,688] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 22.5, 25.77235091074086, 0.4993068188133558, 1.0, 1.0, 20.0, 51.82554320902822], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 310200.0000, 
sim time next is 310800.0000, 
raw observation next is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 22.5, 25.46408936652619, 0.4797298436314936, 1.0, 1.0, 65.0, 48.23215661981064], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4333333333333334, 0.28055555555555556, 0.6898710865561695, 0.375, 0.6220074472105157, 0.6599099478771645, 1.0, 1.0, 1.0, 0.4823215661981064], 
reward next is 0.5177, 
noisyNet noise sample is [array([-0.6035526], dtype=float32), -0.5128837]. 
=============================================
[2019-04-09 14:38:49,818] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05112335 0.11966016 0.07631072 0.06154278 0.06309806 0.05291527
 0.12573953 0.11194605 0.06659642 0.10802506 0.16304262], sum to 1.0000
[2019-04-09 14:38:49,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9516
[2019-04-09 14:38:49,841] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.56666666666667, 78.0, 0.0, 0.0, 19.0, 22.90069408658464, -0.18398942835716, 0.0, 1.0, 20.0, 38.49015134901127], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 19.0, 22.81878324396457, -0.1897059460664902, 0.0, 1.0, 65.0, 65.97625797135225], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4015652703303809, 0.43676468464450324, 0.0, 1.0, 1.0, 0.6597625797135226], 
reward next is 0.3402, 
noisyNet noise sample is [array([-0.7780384], dtype=float32), -0.577778]. 
=============================================
[2019-04-09 14:38:49,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03868477 0.11288485 0.08185569 0.07685988 0.06253391 0.06211597
 0.12298987 0.10906211 0.09208182 0.08428456 0.15664656], sum to 1.0000
[2019-04-09 14:38:49,876] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2243
[2019-04-09 14:38:49,891] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-11.15, 53.0, 0.0, 0.0, 22.5, 24.99404420237556, 0.2228702716100015, 1.0, 1.0, 50.0, 43.51672090370761], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 322200.0000, 
sim time next is 322800.0000, 
raw observation next is [-11.33333333333333, 54.33333333333334, 0.0, 0.0, 22.5, 24.83392958082194, 0.197335041118758, 1.0, 1.0, 25.0, 32.63300483235464], 
processed observation next is [1.0, 0.7391304347826086, 0.14866112650046176, 0.5433333333333334, 0.0, 0.0, 0.375, 0.5694941317351617, 0.565778347039586, 1.0, 1.0, 0.2, 0.32633004832354634], 
reward next is 0.6737, 
noisyNet noise sample is [array([-3.6199877], dtype=float32), 1.7073027]. 
=============================================
[2019-04-09 14:38:49,896] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03919405 0.12615179 0.0829805  0.09901793 0.07543206 0.06254527
 0.1191631  0.10439269 0.08683896 0.07263161 0.13165206], sum to 1.0000
[2019-04-09 14:38:49,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0024
[2019-04-09 14:38:49,905] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 22.5, 24.78135340424203, 0.2224061863493898, 1.0, 1.0, 60.0, 53.08572400655307], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [-11.8, 58.0, 0.0, 0.0, 22.5, 24.75860567190553, 0.1352867852931076, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13573407202216065, 0.58, 0.0, 0.0, 0.375, 0.5632171393254609, 0.5450955950977026, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66215914], dtype=float32), -0.41826752]. 
=============================================
[2019-04-09 14:38:50,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04667258 0.13134278 0.0813244  0.0868291  0.07275051 0.07099639
 0.09798279 0.10920595 0.08885596 0.06583474 0.14820473], sum to 1.0000
[2019-04-09 14:38:50,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9219
[2019-04-09 14:38:50,085] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.683333333333334, 43.16666666666666, 39.33333333333333, 319.0, 22.5, 26.37343099092689, 0.1873611627442187, 1.0, 1.0, 30.0, 46.18820891725014], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 317400.0000, 
sim time next is 318000.0000, 
raw observation next is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 22.5, 26.32464717830438, 0.49055198196369, 1.0, 1.0, 65.0, 65.00347707295181], 
processed observation next is [1.0, 0.6956521739130435, 0.18928901200369344, 0.4433333333333334, 0.10555555555555554, 0.30883977900552484, 0.375, 0.6937205981920318, 0.6635173273212299, 1.0, 1.0, 1.0, 0.6500347707295181], 
reward next is 0.3500, 
noisyNet noise sample is [array([0.28075024], dtype=float32), 1.6169884]. 
=============================================
[2019-04-09 14:38:50,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[-0.23839985]
 [-0.2212308 ]
 [-0.33045116]
 [-0.29888386]
 [-0.1998702 ]], R is [[0.18339515]
 [0.71967912]
 [1.09477794]
 [1.73951423]
 [2.33060694]].
[2019-04-09 14:38:50,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05408743 0.12666911 0.06607674 0.08081357 0.07210783 0.04826126
 0.12483336 0.12817442 0.06665086 0.08973325 0.14259227], sum to 1.0000
[2019-04-09 14:38:50,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-09 14:38:50,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03912381 0.10995469 0.06822331 0.08950514 0.06639647 0.06166723
 0.11978842 0.09393677 0.10554494 0.07508229 0.17077689], sum to 1.0000
[2019-04-09 14:38:50,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-09 14:38:50,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-12.8, 70.0, 0.0, 0.0, 22.5, 23.4271630874773, -0.001878013353581694, 0.0, 1.0, 60.0, 62.29129948180967], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 331200.0000, 
sim time next is 331800.0000, 
raw observation next is [-12.8, 71.16666666666667, 0.0, 0.0, 19.0, 23.41137670126723, -0.007844545675751833, 0.0, 1.0, 50.0, 48.28647388398753], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7116666666666667, 0.0, 0.0, 0.08333333333333333, 0.4509480584389358, 0.49738515144141604, 0.0, 1.0, 0.7, 0.48286473883987535], 
reward next is 0.5171, 
noisyNet noise sample is [array([0.01973312], dtype=float32), -0.14515902]. 
=============================================
[2019-04-09 14:38:50,214] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-13.2, 80.33333333333333, 0.0, 0.0, 19.0, 22.86259195793925, -0.1700236856569848, 0.0, 1.0, 20.0, 42.40144803352484], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 337200.0000, 
sim time next is 337800.0000, 
raw observation next is [-13.3, 81.16666666666667, 0.0, 0.0, 19.0, 22.81924558174688, -0.1885632898288872, 0.0, 1.0, 55.0, 49.12333937233655], 
processed observation next is [1.0, 0.9130434782608695, 0.09418282548476452, 0.8116666666666668, 0.0, 0.0, 0.08333333333333333, 0.4016037984789067, 0.43714557005703764, 0.0, 1.0, 0.8, 0.4912333937233655], 
reward next is 0.5088, 
noisyNet noise sample is [array([-0.9778991], dtype=float32), -0.74419826]. 
=============================================
[2019-04-09 14:38:50,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06731097 0.09450341 0.07574261 0.0743997  0.06716094 0.0582911
 0.11682206 0.10071782 0.09047931 0.12043394 0.1341382 ], sum to 1.0000
[2019-04-09 14:38:50,229] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0409984  0.11954629 0.0783272  0.08523995 0.08046366 0.05864533
 0.12200863 0.10916224 0.07578419 0.07653715 0.15328705], sum to 1.0000
[2019-04-09 14:38:50,232] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-09 14:38:50,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2029
[2019-04-09 14:38:50,245] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-15.2, 70.33333333333334, 0.0, 0.0, 19.0, 20.56537691994355, -0.6914719302914168, 0.0, 1.0, 25.0, 48.05931170418641], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 357600.0000, 
sim time next is 358200.0000, 
raw observation next is [-15.3, 71.0, 0.0, 0.0, 19.0, 20.56124796721584, -0.6977405060915008, 0.0, 1.0, 25.0, 35.06163243281355], 
processed observation next is [1.0, 0.13043478260869565, 0.03878116343490302, 0.71, 0.0, 0.0, 0.08333333333333333, 0.2134373306013201, 0.26741983130283303, 0.0, 1.0, 0.2, 0.35061632432813555], 
reward next is 0.6494, 
noisyNet noise sample is [array([-0.18545555], dtype=float32), 1.9739615]. 
=============================================
[2019-04-09 14:38:50,260] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.63333333333333, 67.66666666666667, 0.0, 0.0, 22.5, 23.04245222907944, -0.0282326159496348, 1.0, 1.0, 65.0, 73.96526540337956], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 330000.0000, 
sim time next is 330600.0000, 
raw observation next is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 22.5, 23.18961657910576, -0.01174398954697526, 1.0, 1.0, 35.0, 54.98486303433364], 
processed observation next is [1.0, 0.8260869565217391, 0.1103416435826407, 0.6883333333333332, 0.0, 0.0, 0.375, 0.4324680482588133, 0.4960853368176749, 1.0, 1.0, 0.4, 0.5498486303433364], 
reward next is 0.4502, 
noisyNet noise sample is [array([-0.85294354], dtype=float32), 0.5686367]. 
=============================================
[2019-04-09 14:38:50,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04138287 0.10626675 0.07011735 0.09160598 0.06947591 0.06067109
 0.13678624 0.11063138 0.08107397 0.08624567 0.14574274], sum to 1.0000
[2019-04-09 14:38:50,357] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-09 14:38:50,374] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.8, 73.5, 0.0, 0.0, 19.0, 23.62223353196071, -0.04029670997571137, 0.0, 1.0, 45.0, 39.2545654708896], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 333000.0000, 
sim time next is 333600.0000, 
raw observation next is [-12.8, 74.66666666666666, 0.0, 0.0, 19.0, 23.47401183707699, -0.05312900605263727, 0.0, 1.0, 65.0, 66.79088741919276], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7466666666666666, 0.0, 0.0, 0.08333333333333333, 0.45616765308974916, 0.4822903313157876, 0.0, 1.0, 1.0, 0.6679088741919276], 
reward next is 0.3321, 
noisyNet noise sample is [array([0.8004461], dtype=float32), -1.1199898]. 
=============================================
[2019-04-09 14:38:50,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03605609 0.1087227  0.07425895 0.08761837 0.0680009  0.06565216
 0.11234508 0.10165912 0.09865891 0.074336   0.17269166], sum to 1.0000
[2019-04-09 14:38:50,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2731
[2019-04-09 14:38:50,435] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 22.5, 24.53241559468102, 0.192529895110455, 1.0, 1.0, 65.0, 73.91391625954486], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [-11.8, 58.0, 0.0, 0.0, 22.5, 24.54067157423792, 0.2054347023580827, 1.0, 1.0, 30.0, 47.57591678813417], 
processed observation next is [1.0, 0.782608695652174, 0.13573407202216065, 0.58, 0.0, 0.0, 0.375, 0.5450559645198266, 0.5684782341193609, 1.0, 1.0, 0.3, 0.4757591678813417], 
reward next is 0.5242, 
noisyNet noise sample is [array([0.88865405], dtype=float32), -0.19751737]. 
=============================================
[2019-04-09 14:38:50,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05785605 0.088469   0.07990029 0.06913538 0.06564784 0.06149089
 0.12406304 0.1138963  0.07344399 0.11330087 0.15279633], sum to 1.0000
[2019-04-09 14:38:50,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2952
[2019-04-09 14:38:50,470] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 22.048100684773, -0.3498925057129034, 0.0, 1.0, 60.0, 65.17702053499391], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 22.1137831215612, -0.356764406489086, 0.0, 1.0, 30.0, 49.21440442266321], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.34281526013010016, 0.38107853117030466, 0.0, 1.0, 0.3, 0.49214404422663205], 
reward next is 0.5079, 
noisyNet noise sample is [array([0.02178973], dtype=float32), 0.65337956]. 
=============================================
[2019-04-09 14:38:50,509] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 9101: loss 22.8013
[2019-04-09 14:38:50,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06722748 0.09211684 0.07161895 0.0686652  0.06597374 0.04747834
 0.12739311 0.1269165  0.06012801 0.10782789 0.16465394], sum to 1.0000
[2019-04-09 14:38:50,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 9102: learning rate 0.0000
[2019-04-09 14:38:50,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-09 14:38:50,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-13.9, 66.66666666666666, 0.0, 0.0, 19.0, 22.35257615722598, -0.3163998673529559, 0.0, 1.0, 20.0, 44.44030776717429], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 345000.0000, 
sim time next is 345600.0000, 
raw observation next is [-13.9, 66.0, 0.0, 0.0, 19.0, 22.33189250308652, -0.3372372791614948, 0.0, 1.0, 50.0, 42.27134683139679], 
processed observation next is [1.0, 0.0, 0.07756232686980608, 0.66, 0.0, 0.0, 0.08333333333333333, 0.3609910419238768, 0.3875875736128351, 0.0, 1.0, 0.7, 0.4227134683139679], 
reward next is 0.5773, 
noisyNet noise sample is [array([-1.68107], dtype=float32), -0.088406645]. 
=============================================
[2019-04-09 14:38:50,565] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05265772 0.09032566 0.08117175 0.06577552 0.06867226 0.05593023
 0.13080624 0.12861703 0.07441623 0.11372983 0.13789757], sum to 1.0000
[2019-04-09 14:38:50,568] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-09 14:38:50,594] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-14.58333333333333, 69.0, 0.0, 0.0, 19.0, 21.63864259583297, -0.428708484854198, 0.0, 1.0, 20.0, 50.97336819591732], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 349800.0000, 
sim time next is 350400.0000, 
raw observation next is [-14.66666666666667, 69.0, 0.0, 0.0, 19.0, 21.73349051722328, -0.4238650039636992, 0.0, 1.0, 25.0, 42.16099405426729], 
processed observation next is [1.0, 0.043478260869565216, 0.05632502308402576, 0.69, 0.0, 0.0, 0.08333333333333333, 0.3111242097686067, 0.3587116653454336, 0.0, 1.0, 0.2, 0.4216099405426729], 
reward next is 0.5784, 
noisyNet noise sample is [array([0.40750235], dtype=float32), 0.78748095]. 
=============================================
[2019-04-09 14:38:50,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05827971 0.08541064 0.08861592 0.05888186 0.06111134 0.05892082
 0.13140473 0.10582873 0.08118627 0.11913802 0.151222  ], sum to 1.0000
[2019-04-09 14:38:50,919] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6753
[2019-04-09 14:38:50,937] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 21.77312611870997, -0.4444890175337582, 0.0, 1.0, 20.0, 39.07511804786355], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 21.75978142807299, -0.4701806816499983, 0.0, 1.0, 55.0, 53.34182132445765], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.31331511900608255, 0.3432731061166672, 0.0, 1.0, 0.8, 0.5334182132445765], 
reward next is 0.4666, 
noisyNet noise sample is [array([1.2627648], dtype=float32), 0.6337096]. 
=============================================
[2019-04-09 14:38:51,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.06947196 0.08083608 0.07200292 0.05599328 0.06072903 0.05648915
 0.12331618 0.11739317 0.07808526 0.1308247  0.15485825], sum to 1.0000
[2019-04-09 14:38:51,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8352
[2019-04-09 14:38:51,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 22.03355015628872, -0.4196060378170478, 0.0, 1.0, 20.0, 43.12961629828985], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 355800.0000, 
sim time next is 356400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 21.9524553126109, -0.4263307934201901, 0.0, 1.0, 60.0, 57.58421731630019], 
processed observation next is [1.0, 0.13043478260869565, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.32937127605090843, 0.3578897355266033, 0.0, 1.0, 0.9, 0.5758421731630019], 
reward next is 0.4242, 
noisyNet noise sample is [array([0.40576446], dtype=float32), -0.27971387]. 
=============================================
[2019-04-09 14:38:51,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06979133 0.0941251  0.07566585 0.06601994 0.06851404 0.06364837
 0.10890517 0.11194094 0.08302223 0.12127986 0.13708721], sum to 1.0000
[2019-04-09 14:38:51,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-09 14:38:51,316] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-16.1, 77.16666666666666, 0.0, 0.0, 19.0, 20.63051485480183, -0.8823163059048985, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 19.67980651591112, -0.8172528119961564, 0.0, 1.0, 60.0, 109.0087908171803], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.13998387632592676, 0.2275823960012812, 0.0, 1.0, 0.9, 1.090087908171803], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.8041823], dtype=float32), -1.0563803]. 
=============================================
[2019-04-09 14:38:51,360] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03914039 0.13097137 0.08396855 0.10258293 0.07857089 0.06158148
 0.11155287 0.09595337 0.09491658 0.07380816 0.1269535 ], sum to 1.0000
[2019-04-09 14:38:51,360] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8150
[2019-04-09 14:38:51,397] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-9.5, 42.0, 54.66666666666667, 398.0000000000001, 22.5, 26.11093865898905, 0.5332157217673071, 1.0, 1.0, 65.0, 80.4465150065583], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 316200.0000, 
sim time next is 316800.0000, 
raw observation next is [-9.5, 42.0, 47.0, 358.5, 22.5, 26.18641806232121, 0.5567296641889291, 1.0, 1.0, 60.0, 51.92561634775865], 
processed observation next is [1.0, 0.6956521739130435, 0.1994459833795014, 0.42, 0.15666666666666668, 0.3961325966850829, 0.375, 0.6822015051934341, 0.685576554729643, 1.0, 1.0, 0.9, 0.5192561634775865], 
reward next is 0.4807, 
noisyNet noise sample is [array([0.5483431], dtype=float32), -0.4544287]. 
=============================================
[2019-04-09 14:38:52,091] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03766629 0.11070886 0.07870924 0.08391177 0.06343631 0.05860891
 0.13799535 0.10286137 0.08531091 0.08220973 0.1585813 ], sum to 1.0000
[2019-04-09 14:38:52,092] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0428
[2019-04-09 14:38:52,115] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.3, 63.0, 0.0, 0.0, 22.5, 24.7914202734454, 0.2944627182744748, 1.0, 1.0, 35.0, 53.53303188962857], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 327600.0000, 
sim time next is 328200.0000, 
raw observation next is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 22.5, 24.81144078630384, 0.2811818317472504, 1.0, 1.0, 20.0, 42.4950294171671], 
processed observation next is [1.0, 0.8260869565217391, 0.1195752539242845, 0.6416666666666667, 0.0, 0.0, 0.375, 0.5676200655253201, 0.5937272772490835, 1.0, 1.0, 0.1, 0.424950294171671], 
reward next is 0.5750, 
noisyNet noise sample is [array([1.0257474], dtype=float32), 0.53742135]. 
=============================================
[2019-04-09 14:38:52,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05746231 0.10241586 0.0790526  0.06405219 0.06019054 0.05649163
 0.12998146 0.09742462 0.08064497 0.11007474 0.16220906], sum to 1.0000
[2019-04-09 14:38:52,512] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-09 14:38:52,519] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-16.28333333333333, 78.5, 0.0, 0.0, 22.5, 21.84926152713692, -0.5220807874619368, 0.0, 1.0, 30.0, 42.22415278673877], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 371400.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 22.5, 21.59971532376609, -0.6169381971556133, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 0.375, 0.2999762769805076, 0.2943539342814622, 1.0, 1.0, 0.0, 0.0], 
reward next is 0.7812, 
noisyNet noise sample is [array([1.3294219], dtype=float32), -0.57978106]. 
=============================================
[2019-04-09 14:38:52,546] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[-0.43838727]
 [-0.4299214 ]
 [-0.3778231 ]
 [-0.448614  ]
 [-0.46985018]], R is [[0.3090941 ]
 [0.88376164]
 [1.42766762]
 [1.98092532]
 [2.52876997]].
[2019-04-09 14:38:52,635] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-09 14:38:52,643] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:38:52,643] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:38:52,645] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run2
[2019-04-09 14:38:52,657] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:38:52,659] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:38:52,659] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:38:52,660] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:38:52,663] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run2
[2019-04-09 14:38:52,663] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run2
[2019-04-09 14:39:44,141] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00066325], dtype=float32), 0.0012244043]
[2019-04-09 14:39:44,141] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-5.9, 62.0, 0.0, 0.0, 19.0, 24.73511189989834, 0.2675114687353101, 0.0, 1.0, 40.0, 27.33682901505735]
[2019-04-09 14:39:44,141] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:39:44,142] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.07077549 0.09776834 0.07942419 0.07422875 0.08284814 0.0705609
 0.10118897 0.10540087 0.07150588 0.12557022 0.12072821], sampled 0.09698080627153938
[2019-04-09 14:40:09,238] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00066325], dtype=float32), 0.0012244043]
[2019-04-09 14:40:09,239] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [14.6, 72.0, 0.0, 0.0, 19.0, 28.78647935203653, 1.470150471096462, 0.0, 1.0, 30.0, 15.0557244207572]
[2019-04-09 14:40:09,239] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:40:09,240] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.06281386 0.1030957  0.07863273 0.08460768 0.08043714 0.08027061
 0.09612881 0.09411516 0.0764692  0.12198165 0.12144734], sampled 0.4225190418088953
[2019-04-09 14:40:14,500] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5711.5891 276378.4640 2643.0053
[2019-04-09 14:40:14,521] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:14,521] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:14,636] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:14,636] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:19,893] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5418.8308 307756.1221 1745.2525
[2019-04-09 14:40:19,913] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:19,913] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:20,024] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:20,024] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:20,555] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5498.0829 299743.9500 2158.4409
[2019-04-09 14:40:20,576] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:20,576] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:40:20,688] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:20,688] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:40:21,578] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 10000, evaluation results [10000.0, 5498.08294532352, 299743.9499525491, 2158.440943178917, 5711.5891089280685, 276378.46401294495, 2643.0053336049414, 5418.830780203949, 307756.1220844279, 1745.2525014089376]
[2019-04-09 14:40:22,429] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05629408 0.089734   0.07493964 0.06234924 0.06702773 0.05524305
 0.13700375 0.10930546 0.07330038 0.11880141 0.15600131], sum to 1.0000
[2019-04-09 14:40:22,429] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6241
[2019-04-09 14:40:22,455] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 19.0, 23.27547773325086, -0.1029009590582267, 0.0, 1.0, 50.0, 40.79902107054684], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 351600.0000, 
sim time next is 352200.0000, 
raw observation next is [-14.91666666666667, 69.0, 0.0, 0.0, 19.0, 23.29674856928297, -0.1176430939501604, 0.0, 1.0, 50.0, 41.35864481870387], 
processed observation next is [1.0, 0.043478260869565216, 0.04939981532779307, 0.69, 0.0, 0.0, 0.08333333333333333, 0.441395714106914, 0.4607856353499465, 0.0, 1.0, 0.7, 0.4135864481870387], 
reward next is 0.5864, 
noisyNet noise sample is [array([0.04587699], dtype=float32), 1.0314618]. 
=============================================
[2019-04-09 14:40:23,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04388096 0.11552893 0.09028204 0.09000906 0.0704684  0.06175608
 0.10016635 0.11734474 0.09390765 0.07309481 0.14356105], sum to 1.0000
[2019-04-09 14:40:23,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-09 14:40:23,558] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 22.5, 24.06078804615417, 0.01783497758086576, 1.0, 1.0, 30.0, 27.96925953308346], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 412200.0000, 
sim time next is 412800.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 22.5, 23.89192877605638, -0.0688941805195036, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.375, 0.49099406467136486, 0.4770352731601655, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1150024], dtype=float32), 0.8912077]. 
=============================================
[2019-04-09 14:40:23,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06040128 0.10523275 0.09313148 0.0675485  0.0764332  0.08227312
 0.09439827 0.1218431  0.08875806 0.09106087 0.11891934], sum to 1.0000
[2019-04-09 14:40:23,996] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5372
[2019-04-09 14:40:24,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03648449 0.12286789 0.07227303 0.09363502 0.06458359 0.06122564
 0.12634577 0.09658185 0.09197886 0.08342075 0.1506031 ], sum to 1.0000
[2019-04-09 14:40:24,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7540
[2019-04-09 14:40:24,030] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 22.5, 23.59074661470505, -0.05777959833191484, 1.0, 1.0, 50.0, 40.11750198000235], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 414000.0000, 
sim time next is 414600.0000, 
raw observation next is [-9.583333333333334, 40.33333333333334, 0.0, 0.0, 22.5, 23.47393567700875, -0.06101419184693429, 1.0, 1.0, 55.0, 48.09266102313096], 
processed observation next is [1.0, 0.8260869565217391, 0.1971375807940905, 0.40333333333333343, 0.0, 0.0, 0.375, 0.4561613064173959, 0.4796619360510219, 1.0, 1.0, 0.8, 0.48092661023130956], 
reward next is 0.5191, 
noisyNet noise sample is [array([0.14094055], dtype=float32), 0.7078444]. 
=============================================
[2019-04-09 14:40:24,033] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 42.5, 769.5, 22.5, 25.18664122148061, 0.2415342791569983, 1.0, 1.0, 30.0, 36.84619415596199], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 399600.0000, 
sim time next is 400200.0000, 
raw observation next is [-9.4, 39.66666666666667, 40.66666666666666, 748.6666666666666, 22.5, 24.94145640034607, 0.2221432008922266, 1.0, 1.0, 45.0, 32.74761505008834], 
processed observation next is [1.0, 0.6521739130434783, 0.20221606648199447, 0.3966666666666667, 0.1355555555555555, 0.8272559852670349, 0.375, 0.5784547000288391, 0.5740477336307422, 1.0, 1.0, 0.6, 0.3274761505008834], 
reward next is 0.6725, 
noisyNet noise sample is [array([-0.51348895], dtype=float32), -1.43315]. 
=============================================
[2019-04-09 14:40:24,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0414402  0.10809748 0.07711042 0.09205817 0.06707019 0.06844429
 0.11817316 0.11518654 0.09246729 0.07796936 0.14198281], sum to 1.0000
[2019-04-09 14:40:24,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4386
[2019-04-09 14:40:24,078] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.916666666666668, 41.66666666666666, 0.0, 0.0, 22.5, 23.20088590131269, -0.1097277851288618, 1.0, 1.0, 40.0, 45.41763171934929], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 417000.0000, 
sim time next is 417600.0000, 
raw observation next is [-10.0, 42.0, 0.0, 0.0, 22.5, 23.12188118620626, -0.1313184117398821, 0.0, 1.0, 40.0, 30.52135337283271], 
processed observation next is [1.0, 0.8695652173913043, 0.18559556786703602, 0.42, 0.0, 0.0, 0.375, 0.42682343218385493, 0.456227196086706, 0.0, 1.0, 0.5, 0.3052135337283271], 
reward next is 0.6948, 
noisyNet noise sample is [array([1.012422], dtype=float32), 0.0024893284]. 
=============================================
[2019-04-09 14:40:24,193] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03623144 0.12660529 0.07104465 0.08339333 0.06310562 0.06341462
 0.11599726 0.11165661 0.08235608 0.07772711 0.16846797], sum to 1.0000
[2019-04-09 14:40:24,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5181
[2019-04-09 14:40:24,210] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.75, 41.0, 0.0, 0.0, 22.5, 23.16477319554048, -0.1083310452458615, 1.0, 1.0, 20.0, 41.34120205846389], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 415800.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 22.5, 23.19248559667867, -0.1248988276893473, 1.0, 1.0, 45.0, 41.94288800629577], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.375, 0.4327071330565557, 0.4583670574368843, 1.0, 1.0, 0.6, 0.4194288800629577], 
reward next is 0.5806, 
noisyNet noise sample is [array([-0.16077165], dtype=float32), 0.10108493]. 
=============================================
[2019-04-09 14:40:24,319] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04520791 0.10130068 0.08406996 0.08564577 0.07134032 0.05347601
 0.11414819 0.09537812 0.10312901 0.10346268 0.14284141], sum to 1.0000
[2019-04-09 14:40:24,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7275
[2019-04-09 14:40:24,376] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 22.5, 24.33646835879316, 0.04507081513858833, 1.0, 1.0, 45.0, 47.26122575533144], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 22.5, 24.44515988470818, 0.07320276477758834, 1.0, 1.0, 55.0, 56.50801944869937], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 0.375, 0.5370966570590149, 0.5244009215925295, 1.0, 1.0, 0.8, 0.5650801944869938], 
reward next is 0.4349, 
noisyNet noise sample is [array([0.4475967], dtype=float32), -0.10960233]. 
=============================================
[2019-04-09 14:40:24,546] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03997492 0.1136762  0.08574088 0.08668431 0.07115178 0.06790353
 0.10807885 0.10863496 0.09207615 0.06554819 0.16053028], sum to 1.0000
[2019-04-09 14:40:24,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1315
[2019-04-09 14:40:24,569] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 37.0, 21.0, 403.0, 22.5, 26.01645966659174, 0.4694510951501039, 1.0, 1.0, 65.0, 59.57241321736398], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 22.5, 25.96011737196714, 0.4587806512452405, 1.0, 1.0, 20.0, 48.95923457634888], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3666666666666667, 0.058333333333333334, 0.37421731123388585, 0.375, 0.663343114330595, 0.6529268837484135, 1.0, 1.0, 0.1, 0.48959234576348876], 
reward next is 0.5104, 
noisyNet noise sample is [array([-2.0625622], dtype=float32), -2.023643]. 
=============================================
[2019-04-09 14:40:24,689] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05466479 0.09680261 0.08182582 0.06531568 0.07353979 0.04375839
 0.14842062 0.13074757 0.06982566 0.10198788 0.13311124], sum to 1.0000
[2019-04-09 14:40:24,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6000
[2019-04-09 14:40:24,715] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 22.80839179710531, -0.2357380484146768, 0.0, 1.0, 50.0, 41.18410866698407], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 430200.0000, 
sim time next is 430800.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 22.75908655143204, -0.2504239764566061, 0.0, 1.0, 50.0, 39.05297896628881], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.3965905459526701, 0.4165253411811313, 0.0, 1.0, 0.7, 0.3905297896628881], 
reward next is 0.6095, 
noisyNet noise sample is [array([-2.146999], dtype=float32), -0.88459015]. 
=============================================
[2019-04-09 14:40:24,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05851886 0.09133419 0.07575808 0.06389143 0.06106637 0.05821995
 0.136978   0.11718029 0.09042764 0.1115181  0.13510714], sum to 1.0000
[2019-04-09 14:40:24,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5257
[2019-04-09 14:40:24,774] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 19.0, 21.52438296456345, -0.5147881863423608, 0.0, 1.0, 55.0, 47.89293895784003], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 444000.0000, 
sim time next is 444600.0000, 
raw observation next is [-10.9, 50.5, 0.0, 0.0, 19.0, 21.66812372936403, -0.5233556451852929, 0.0, 1.0, 45.0, 42.82849176083134], 
processed observation next is [1.0, 0.13043478260869565, 0.16066481994459833, 0.505, 0.0, 0.0, 0.08333333333333333, 0.3056769774470025, 0.32554811827156904, 0.0, 1.0, 0.6, 0.4282849176083134], 
reward next is 0.5717, 
noisyNet noise sample is [array([-0.27303872], dtype=float32), -1.4613365]. 
=============================================
[2019-04-09 14:40:24,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0435252  0.10191933 0.0769932  0.06277722 0.05373152 0.04355557
 0.14872745 0.10243058 0.07757195 0.11098017 0.17778772], sum to 1.0000
[2019-04-09 14:40:24,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-09 14:40:24,909] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 22.39747792954163, -0.2788081500475315, 0.0, 1.0, 20.0, 53.39380680897819], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 434400.0000, 
sim time next is 435000.0000, 
raw observation next is [-11.28333333333333, 54.83333333333334, 0.0, 0.0, 19.0, 22.46755773552982, -0.2587458809411752, 0.0, 1.0, 60.0, 55.60541809204004], 
processed observation next is [1.0, 0.0, 0.1500461680517083, 0.5483333333333335, 0.0, 0.0, 0.08333333333333333, 0.3722964779608183, 0.4137513730196083, 0.0, 1.0, 0.9, 0.5560541809204004], 
reward next is 0.4439, 
noisyNet noise sample is [array([0.2836408], dtype=float32), 0.95186543]. 
=============================================
[2019-04-09 14:40:24,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.4375494 ]
 [-0.47912842]
 [-0.4331345 ]
 [-0.4861846 ]
 [-0.49335313]], R is [[-0.00415775]
 [ 0.46194574]
 [ 0.66764647]
 [ 1.05181789]
 [ 1.66212201]].
[2019-04-09 14:40:25,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05812993 0.09133545 0.06911877 0.05739862 0.06069417 0.06390113
 0.13925934 0.11396872 0.08357819 0.11164597 0.1509697 ], sum to 1.0000
[2019-04-09 14:40:25,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1039
[2019-04-09 14:40:25,034] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.7, 52.0, 0.0, 0.0, 19.0, 21.36892308925454, -0.6054067133365834, 0.0, 1.0, 50.0, 44.80065350105567], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 449400.0000, 
sim time next is 450000.0000, 
raw observation next is [-10.6, 52.0, 0.0, 0.0, 19.0, 21.3264887785251, -0.6215850601705449, 0.0, 1.0, 20.0, 37.65132548608956], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.52, 0.0, 0.0, 0.08333333333333333, 0.277207398210425, 0.2928049799431517, 0.0, 1.0, 0.1, 0.3765132548608956], 
reward next is 0.6235, 
noisyNet noise sample is [array([-0.11191539], dtype=float32), 0.056752555]. 
=============================================
[2019-04-09 14:40:25,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.30512622]
 [-0.30364224]
 [-0.28567955]
 [-0.30455446]
 [-0.28161186]], R is [[0.34080899]
 [0.8893944 ]
 [1.44564843]
 [1.93553948]
 [2.39537048]].
[2019-04-09 14:40:25,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05635944 0.1018099  0.07537701 0.07670836 0.07394703 0.06011813
 0.1201344  0.09766293 0.09511065 0.10676251 0.1360096 ], sum to 1.0000
[2019-04-09 14:40:25,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-09 14:40:25,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05325533 0.09695602 0.07183065 0.06466257 0.06694291 0.05925045
 0.12581834 0.12279335 0.08838165 0.10088819 0.14922047], sum to 1.0000
[2019-04-09 14:40:25,690] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2011
[2019-04-09 14:40:25,718] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 19.0, 22.09624609205119, -0.4332479006191112, 0.0, 1.0, 55.0, 47.4806064565754], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 454800.0000, 
sim time next is 455400.0000, 
raw observation next is [-8.95, 43.5, 0.0, 0.0, 19.0, 22.03220307318049, -0.4248882810577768, 0.0, 1.0, 25.0, 42.55613281208285], 
processed observation next is [1.0, 0.2608695652173913, 0.21468144044321333, 0.435, 0.0, 0.0, 0.08333333333333333, 0.33601692276504086, 0.3583705729807411, 0.0, 1.0, 0.2, 0.42556132812082853], 
reward next is 0.5744, 
noisyNet noise sample is [array([2.629326], dtype=float32), -0.30754545]. 
=============================================
[2019-04-09 14:40:25,720] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 22.5, 21.67338810393078, -0.5098485051848443, 1.0, 1.0, 50.0, 90.12798654720663], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 22.5, 21.77212970111645, -0.423786133348762, 1.0, 1.0, 60.0, 83.07279573136779], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.375, 0.3143441417597043, 0.35873795555041266, 1.0, 1.0, 0.9, 0.8307279573136779], 
reward next is 0.1693, 
noisyNet noise sample is [array([-0.5591064], dtype=float32), 0.7003252]. 
=============================================
[2019-04-09 14:40:25,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05862764 0.10111181 0.07426924 0.06809516 0.06697178 0.05730542
 0.12824419 0.10672179 0.09101298 0.09637258 0.15126747], sum to 1.0000
[2019-04-09 14:40:25,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-09 14:40:25,740] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.05, 48.0, 0.0, 0.0, 19.0, 22.1827592372676, -0.4283552150835115, 0.0, 1.0, 40.0, 38.16895180370184], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 451800.0000, 
sim time next is 452400.0000, 
raw observation next is [-9.866666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 22.09609420568083, -0.4531364956233984, 0.0, 1.0, 40.0, 36.3954717753974], 
processed observation next is [1.0, 0.21739130434782608, 0.18928901200369344, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.34134118380673595, 0.34895450145886714, 0.0, 1.0, 0.5, 0.363954717753974], 
reward next is 0.6360, 
noisyNet noise sample is [array([-0.33535722], dtype=float32), 1.4852434]. 
=============================================
[2019-04-09 14:40:25,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06936602 0.09360185 0.0717777  0.05413987 0.06391728 0.0642003
 0.13198422 0.10525149 0.08704031 0.1094396  0.1492814 ], sum to 1.0000
[2019-04-09 14:40:25,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3053
[2019-04-09 14:40:25,834] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 52.0, 0.0, 0.0, 19.0, 21.5297058448565, -0.5076963438139551, 0.0, 1.0, 30.0, 33.61061648504138], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 447600.0000, 
sim time next is 448200.0000, 
raw observation next is [-10.9, 52.0, 0.0, 0.0, 19.0, 21.45411615709734, -0.5158333783203577, 0.0, 1.0, 35.0, 30.52934869457514], 
processed observation next is [1.0, 0.17391304347826086, 0.16066481994459833, 0.52, 0.0, 0.0, 0.08333333333333333, 0.2878430130914449, 0.3280555405598808, 0.0, 1.0, 0.4, 0.3052934869457514], 
reward next is 0.6947, 
noisyNet noise sample is [array([-0.62234926], dtype=float32), 1.359537]. 
=============================================
[2019-04-09 14:40:26,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0572442  0.11042792 0.07444859 0.06229326 0.07176971 0.05510005
 0.13828315 0.10721546 0.06041983 0.09280405 0.16999374], sum to 1.0000
[2019-04-09 14:40:26,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1519
[2019-04-09 14:40:26,080] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 24.24574485233325, 0.06736064097068946, 0.0, 1.0, 50.0, 58.14113705392336], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 428400.0000, 
sim time next is 429000.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 24.03822782577864, 0.04557603880529607, 0.0, 1.0, 20.0, 35.21381948773787], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.5031856521482201, 0.5151920129350988, 0.0, 1.0, 0.1, 0.35213819487737874], 
reward next is 0.6479, 
noisyNet noise sample is [array([-0.26852956], dtype=float32), 0.44828352]. 
=============================================
[2019-04-09 14:40:26,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-0.37159055]
 [-0.35600483]
 [-0.35663146]
 [-0.3312229 ]
 [-0.29257625]], R is [[0.34547132]
 [0.76060522]
 [1.75299919]
 [2.25895739]
 [2.63125968]].
[2019-04-09 14:40:26,423] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04482534 0.12283565 0.08372105 0.08813189 0.06529468 0.05053268
 0.11533417 0.0910794  0.11310665 0.08225158 0.14288697], sum to 1.0000
[2019-04-09 14:40:26,423] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6026
[2019-04-09 14:40:26,447] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04797138 0.12776431 0.07729557 0.09636154 0.06790653 0.05087299
 0.11340474 0.10130825 0.09767912 0.08327996 0.13615565], sum to 1.0000
[2019-04-09 14:40:26,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4148
[2019-04-09 14:40:26,474] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 30.0, 98.0, 0.0, 22.5, 24.45637608637958, -0.04263309225793489, 1.0, 1.0, 20.0, 46.40576756072623], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 22.5, 24.46323602834276, -0.02605859999536976, 1.0, 1.0, 50.0, 45.19023038414434], 
processed observation next is [1.0, 0.43478260869565216, 0.37857802400738694, 0.2933333333333333, 0.34, 0.0, 0.375, 0.5386030023618966, 0.4913138000015434, 1.0, 1.0, 0.7, 0.4519023038414434], 
reward next is 0.5481, 
noisyNet noise sample is [array([0.7062439], dtype=float32), -1.2081419]. 
=============================================
[2019-04-09 14:40:26,476] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.466666666666667, 34.16666666666666, 36.0, 0.0, 22.5, 23.30423673202075, -0.2364273031358435, 1.0, 1.0, 40.0, 35.48369924605174], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 463800.0000, 
sim time next is 464400.0000, 
raw observation next is [-6.2, 33.0, 42.5, 0.0, 22.5, 23.39001958101836, -0.2206125594240323, 1.0, 1.0, 35.0, 31.40972488356002], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.33, 0.14166666666666666, 0.0, 0.375, 0.4491682984181968, 0.42646248019198923, 1.0, 1.0, 0.4, 0.3140972488356002], 
reward next is 0.6859, 
noisyNet noise sample is [array([-0.9138222], dtype=float32), 0.11158146]. 
=============================================
[2019-04-09 14:40:26,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05354563 0.09277614 0.071562   0.0675676  0.05997076 0.05350488
 0.12997599 0.11555097 0.10859638 0.0994588  0.14749093], sum to 1.0000
[2019-04-09 14:40:26,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-09 14:40:26,867] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.9, 50.5, 0.0, 0.0, 19.0, 22.68437033106534, -0.2559692625213665, 0.0, 1.0, 65.0, 74.15972309614773], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 19.0, 22.6747488092641, -0.2462297960459461, 0.0, 1.0, 20.0, 48.3039982771257], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.08333333333333333, 0.3895624007720082, 0.417923401318018, 0.0, 1.0, 0.1, 0.483039982771257], 
reward next is 0.5170, 
noisyNet noise sample is [array([-2.7080386], dtype=float32), -1.7509439]. 
=============================================
[2019-04-09 14:40:27,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04614003 0.13664187 0.08047365 0.09626994 0.07025325 0.05611025
 0.09582274 0.10833514 0.10499565 0.07189046 0.13306695], sum to 1.0000
[2019-04-09 14:40:27,141] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-09 14:40:27,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.066666666666666, 32.33333333333334, 68.0, 0.0, 22.5, 23.38527980713476, -0.2381991677190021, 1.0, 1.0, 65.0, 55.09401884434168], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 466800.0000, 
sim time next is 467400.0000, 
raw observation next is [-4.783333333333333, 32.16666666666666, 74.0, 0.0, 22.5, 23.43106322452009, -0.2010985934506124, 1.0, 1.0, 55.0, 54.39623937039071], 
processed observation next is [1.0, 0.391304347826087, 0.3301015697137581, 0.32166666666666655, 0.24666666666666667, 0.0, 0.375, 0.45258860204334095, 0.43296713551646254, 1.0, 1.0, 0.8, 0.5439623937039071], 
reward next is 0.4560, 
noisyNet noise sample is [array([-2.6304998], dtype=float32), -0.56871223]. 
=============================================
[2019-04-09 14:40:27,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04569801 0.15262613 0.07467858 0.11327203 0.07101817 0.04517147
 0.10334966 0.10575116 0.10945834 0.06864696 0.11032946], sum to 1.0000
[2019-04-09 14:40:27,221] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-09 14:40:27,242] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 22.5, 23.9954989742824, -0.04609151356419103, 1.0, 1.0, 65.0, 67.20629964513853], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 470400.0000, 
sim time next is 471000.0000, 
raw observation next is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 22.5, 24.13722730048946, -0.01050624690014229, 1.0, 1.0, 60.0, 58.57103854614131], 
processed observation next is [1.0, 0.43478260869565216, 0.3887349953831949, 0.28666666666666674, 0.35333333333333333, 0.0, 0.375, 0.5114356083741217, 0.4964979176999526, 1.0, 1.0, 0.9, 0.5857103854614131], 
reward next is 0.4143, 
noisyNet noise sample is [array([-1.0511082], dtype=float32), 0.021021988]. 
=============================================
[2019-04-09 14:40:27,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.17610566]
 [-0.2317328 ]
 [-0.14217594]
 [-0.2257877 ]
 [-0.1452648 ]], R is [[0.24457243]
 [0.57006371]
 [1.11798799]
 [1.72756004]
 [2.19337034]].
[2019-04-09 14:40:27,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04704565 0.14309841 0.08093094 0.08857224 0.06721034 0.05796414
 0.09460499 0.11203366 0.10623389 0.07383054 0.12847519], sum to 1.0000
[2019-04-09 14:40:27,557] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7382
[2019-04-09 14:40:27,586] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.1, 35.5, 32.0, 0.0, 22.5, 24.68268390876126, 0.1139266851776888, 1.0, 1.0, 30.0, 34.76636453059079], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 490200.0000, 
sim time next is 490800.0000, 
raw observation next is [1.1, 37.0, 26.0, 0.0, 22.5, 24.13802684270841, 0.09823343749872504, 1.0, 1.0, 55.0, 45.546638621848], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.37, 0.08666666666666667, 0.0, 0.375, 0.5115022368923675, 0.5327444791662417, 1.0, 1.0, 0.8, 0.45546638621847996], 
reward next is 0.5445, 
noisyNet noise sample is [array([0.20091721], dtype=float32), -1.4129801]. 
=============================================
[2019-04-09 14:40:27,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04251071 0.1183954  0.07900217 0.08773483 0.06854734 0.05520106
 0.1163656  0.09225048 0.08972786 0.09817251 0.15209202], sum to 1.0000
[2019-04-09 14:40:27,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-09 14:40:27,801] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 22.5, 23.39669140708172, -0.12667257618565, 1.0, 1.0, 65.0, 64.28874871412941], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 462000.0000, 
sim time next is 462600.0000, 
raw observation next is [-7.0, 36.5, 23.0, 0.0, 22.5, 23.62832813874468, -0.08771029358804133, 1.0, 1.0, 40.0, 53.14507920915795], 
processed observation next is [1.0, 0.34782608695652173, 0.2686980609418283, 0.365, 0.07666666666666666, 0.0, 0.375, 0.4690273448953901, 0.47076323547065285, 1.0, 1.0, 0.5, 0.5314507920915795], 
reward next is 0.4685, 
noisyNet noise sample is [array([-0.71430796], dtype=float32), 0.38818967]. 
=============================================
[2019-04-09 14:40:27,978] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04073374 0.12581083 0.07631999 0.10157334 0.07554461 0.05388196
 0.10881428 0.11173066 0.10176458 0.07203335 0.1317927 ], sum to 1.0000
[2019-04-09 14:40:27,978] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8802
[2019-04-09 14:40:28,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04465974 0.11395012 0.08705258 0.09268554 0.07274319 0.06105008
 0.10480832 0.12072499 0.09355346 0.07550047 0.13327146], sum to 1.0000
[2019-04-09 14:40:28,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9619
[2019-04-09 14:40:28,078] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.4, 35.66666666666667, 98.16666666666667, 0.0, 22.5, 26.92890678824371, 0.4393026554396895, 1.0, 1.0, 45.0, 60.34271553974422], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 483600.0000, 
sim time next is 484200.0000, 
raw observation next is [-0.3, 36.0, 94.0, 0.0, 22.5, 26.41424958497886, 0.5363844102319578, 1.0, 1.0, 65.0, 53.17797870293957], 
processed observation next is [1.0, 0.6086956521739131, 0.4542936288088643, 0.36, 0.31333333333333335, 0.0, 0.375, 0.7011874654149052, 0.6787948034106526, 1.0, 1.0, 1.0, 0.5317797870293957], 
reward next is 0.4682, 
noisyNet noise sample is [array([-1.7359791], dtype=float32), 0.46435183]. 
=============================================
[2019-04-09 14:40:28,099] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 41.5, 13.33333333333334, 0.0, 22.5, 24.6339328691846, 0.185808714585578, 1.0, 1.0, 40.0, 35.03300840111444], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 492600.0000, 
sim time next is 493200.0000, 
raw observation next is [1.1, 43.0, 10.0, 0.0, 22.5, 25.05072545881248, 0.2483741357129907, 1.0, 1.0, 65.0, 59.77300601816777], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.43, 0.03333333333333333, 0.0, 0.375, 0.5875604549010399, 0.5827913785709969, 1.0, 1.0, 1.0, 0.5977300601816777], 
reward next is 0.4023, 
noisyNet noise sample is [array([-0.14486434], dtype=float32), -0.91526693]. 
=============================================
[2019-04-09 14:40:28,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04048761 0.14240536 0.08059217 0.1092402  0.07658801 0.05301427
 0.11358997 0.10772964 0.08345765 0.07548277 0.11741228], sum to 1.0000
[2019-04-09 14:40:28,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1415
[2019-04-09 14:40:28,171] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.7333333333333334, 35.0, 50.00000000000001, 0.0, 22.5, 25.01353548910809, 0.181118815490572, 1.0, 1.0, 50.0, 33.28192848083753], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 488400.0000, 
sim time next is 489000.0000, 
raw observation next is [0.9166666666666667, 34.5, 44.0, 0.0, 22.5, 25.04462853448339, 0.1904306540791003, 1.0, 1.0, 20.0, 30.43259919306043], 
processed observation next is [1.0, 0.6521739130434783, 0.48799630655586346, 0.345, 0.14666666666666667, 0.0, 0.375, 0.5870523778736159, 0.5634768846930335, 1.0, 1.0, 0.1, 0.3043259919306043], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.69607115], dtype=float32), 0.3382602]. 
=============================================
[2019-04-09 14:40:28,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.26546377]
 [-0.1936028 ]
 [-0.19657673]
 [-0.28710786]
 [-0.22629587]], R is [[0.54404849]
 [1.20578873]
 [1.8514775 ]
 [2.46120381]
 [3.0138042 ]].
[2019-04-09 14:40:28,479] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0488729  0.09754547 0.07589704 0.06653612 0.06274108 0.05894527
 0.1336284  0.10974386 0.09781282 0.09796035 0.15031664], sum to 1.0000
[2019-04-09 14:40:28,481] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-09 14:40:28,496] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-10.05, 48.0, 0.0, 0.0, 19.0, 22.88242856820146, -0.2457843899441846, 0.0, 1.0, 65.0, 64.5146266586399], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 451800.0000, 
sim time next is 452400.0000, 
raw observation next is [-9.866666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 22.81990722063835, -0.3399911195861196, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.18928901200369344, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.4016589350531958, 0.3866696268046268, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30951914], dtype=float32), -1.3377272]. 
=============================================
[2019-04-09 14:40:28,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04136156 0.14797194 0.06984323 0.08725309 0.07801429 0.05348359
 0.11468029 0.11693504 0.09569933 0.07457972 0.12017788], sum to 1.0000
[2019-04-09 14:40:28,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2941
[2019-04-09 14:40:28,644] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 35.66666666666667, 98.16666666666667, 0.0, 22.5, 24.53627467128069, -0.08242849967911416, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 483600.0000, 
sim time next is 484200.0000, 
raw observation next is [-0.3, 36.0, 94.0, 0.0, 22.5, 24.45371179415977, 0.0907142521458388, 1.0, 1.0, 30.0, 51.86765200400803], 
processed observation next is [1.0, 0.6086956521739131, 0.4542936288088643, 0.36, 0.31333333333333335, 0.0, 0.375, 0.5378093161799807, 0.530238084048613, 1.0, 1.0, 0.3, 0.5186765200400802], 
reward next is 0.4813, 
noisyNet noise sample is [array([1.4897003], dtype=float32), -0.58165205]. 
=============================================
[2019-04-09 14:40:28,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03286174 0.14179552 0.07766915 0.10920998 0.06914663 0.05373706
 0.1013056  0.09287731 0.11546813 0.06334729 0.1425816 ], sum to 1.0000
[2019-04-09 14:40:28,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1931
[2019-04-09 14:40:28,732] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 22.5, 25.65540371901028, 0.3400165724320227, 1.0, 1.0, 65.0, 59.98646189110629], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 22.5, 25.74112834548884, 0.3557284980080357, 1.0, 1.0, 25.0, 47.77302551041436], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 0.375, 0.6450940287907366, 0.6185761660026786, 1.0, 1.0, 0.2, 0.4777302551041436], 
reward next is 0.5223, 
noisyNet noise sample is [array([0.03210729], dtype=float32), 0.17077726]. 
=============================================
[2019-04-09 14:40:28,930] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03515484 0.131864   0.0726274  0.09536882 0.06788965 0.04414734
 0.12509516 0.10121281 0.10402638 0.08233199 0.14028156], sum to 1.0000
[2019-04-09 14:40:28,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2587
[2019-04-09 14:40:28,982] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 96.0, 0.0, 0.0, 19.0, 23.56417020673433, -0.04854387476851294, 0.0, 1.0, 45.0, 29.72122466603964], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 505800.0000, 
sim time next is 506400.0000, 
raw observation next is [1.433333333333333, 96.0, 0.0, 0.0, 19.0, 23.47930846408465, -0.06738544117326013, 0.0, 1.0, 30.0, 26.06181701349767], 
processed observation next is [1.0, 0.8695652173913043, 0.502308402585411, 0.96, 0.0, 0.0, 0.08333333333333333, 0.45660903867372077, 0.4775381862755799, 0.0, 1.0, 0.3, 0.2606181701349767], 
reward next is 0.7394, 
noisyNet noise sample is [array([-0.17201552], dtype=float32), -0.51034564]. 
=============================================
[2019-04-09 14:40:29,094] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0403142  0.13486905 0.08131792 0.10325842 0.07233202 0.05216195
 0.10796628 0.09945375 0.10881019 0.07262206 0.12689407], sum to 1.0000
[2019-04-09 14:40:29,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-09 14:40:29,124] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06358659 0.09185214 0.06333446 0.06468019 0.0706442  0.05359019
 0.13040195 0.12844852 0.0576774  0.15151452 0.12426985], sum to 1.0000
[2019-04-09 14:40:29,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1773
[2019-04-09 14:40:29,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05196298 0.12451523 0.08109088 0.09022337 0.07773302 0.06134305
 0.10013166 0.11202867 0.09593459 0.07560514 0.12943134], sum to 1.0000
[2019-04-09 14:40:29,130] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8409
[2019-04-09 14:40:29,151] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.066666666666666, 32.33333333333334, 68.0, 0.0, 22.5, 23.66912847107221, -0.08335801641770078, 1.0, 1.0, 65.0, 71.2623271504081], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 466800.0000, 
sim time next is 467400.0000, 
raw observation next is [-4.783333333333333, 32.16666666666666, 74.0, 0.0, 22.5, 23.89474289112363, -0.03403915469458167, 1.0, 1.0, 60.0, 60.42900327124868], 
processed observation next is [1.0, 0.391304347826087, 0.3301015697137581, 0.32166666666666655, 0.24666666666666667, 0.0, 0.375, 0.4912285742603026, 0.4886536151018061, 1.0, 1.0, 0.9, 0.6042900327124868], 
reward next is 0.3957, 
noisyNet noise sample is [array([-0.58661956], dtype=float32), -1.2497478]. 
=============================================
[2019-04-09 14:40:29,155] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 19.0, 24.44707631409621, 0.1939494247396617, 0.0, 1.0, 60.0, 53.39278859707193], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 24.59560114231865, 0.2039784260093514, 0.0, 1.0, 25.0, 40.35475175783188], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.5496334285265542, 0.5679928086697837, 0.0, 1.0, 0.2, 0.40354751757831875], 
reward next is 0.5965, 
noisyNet noise sample is [array([0.7550693], dtype=float32), 0.13838795]. 
=============================================
[2019-04-09 14:40:29,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.8999999999999999, 31.5, 119.0, 0.0, 22.5, 25.10718928766709, 0.2106089278097892, 1.0, 1.0, 50.0, 43.50160465140456], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 480600.0000, 
sim time next is 481200.0000, 
raw observation next is [-0.8, 32.66666666666666, 114.8333333333333, 0.0, 22.5, 25.30944260551082, 0.2913376348620259, 1.0, 1.0, 55.0, 67.72979923214574], 
processed observation next is [1.0, 0.5652173913043478, 0.4404432132963989, 0.32666666666666655, 0.38277777777777766, 0.0, 0.375, 0.6091202171259017, 0.5971125449540087, 1.0, 1.0, 0.8, 0.6772979923214574], 
reward next is 0.3227, 
noisyNet noise sample is [array([0.9433109], dtype=float32), 0.81166166]. 
=============================================
[2019-04-09 14:40:29,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05077202 0.12038267 0.07043725 0.08448795 0.07602315 0.05004203
 0.12313341 0.11362058 0.07415753 0.10822858 0.12871484], sum to 1.0000
[2019-04-09 14:40:29,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4160
[2019-04-09 14:40:29,301] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 94.0, 0.0, 0.0, 19.0, 24.64928754618796, 0.1989475393466482, 0.0, 1.0, 40.0, 41.80804824450907], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 513000.0000, 
sim time next is 513600.0000, 
raw observation next is [3.1, 94.66666666666667, 0.0, 0.0, 19.0, 24.67091911856138, 0.1919671264791705, 0.0, 1.0, 40.0, 40.50056918791301], 
processed observation next is [1.0, 0.9565217391304348, 0.5484764542936289, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.5559099265467816, 0.5639890421597235, 0.0, 1.0, 0.5, 0.4050056918791301], 
reward next is 0.5950, 
noisyNet noise sample is [array([-0.7168085], dtype=float32), -0.62849617]. 
=============================================
[2019-04-09 14:40:29,354] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06281263 0.08950758 0.06417441 0.06012976 0.06007921 0.05692297
 0.13307509 0.11823751 0.05600081 0.15942106 0.13963898], sum to 1.0000
[2019-04-09 14:40:29,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2625
[2019-04-09 14:40:29,371] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 19.0, 24.46826390002916, 0.1850822015640694, 0.0, 1.0, 25.0, 37.60123967221018], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 19.0, 24.57413944263014, 0.1805105802690431, 0.0, 1.0, 30.0, 35.75819594223284], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 0.08333333333333333, 0.5478449535525117, 0.5601701934230143, 0.0, 1.0, 0.3, 0.3575819594223284], 
reward next is 0.6424, 
noisyNet noise sample is [array([0.7485565], dtype=float32), 0.46235323]. 
=============================================
[2019-04-09 14:40:29,431] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0438179  0.11381203 0.08158386 0.0968583  0.07070482 0.05956726
 0.10846056 0.11211083 0.10612933 0.07465507 0.13230006], sum to 1.0000
[2019-04-09 14:40:29,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7855
[2019-04-09 14:40:29,453] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 22.5, 24.86701441670811, 0.2112960059612966, 1.0, 1.0, 30.0, 36.68763016570539], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 498600.0000, 
sim time next is 499200.0000, 
raw observation next is [0.9000000000000001, 92.0, 0.0, 0.0, 22.5, 24.7925762426445, 0.2093020673468625, 1.0, 1.0, 55.0, 42.33037445677538], 
processed observation next is [1.0, 0.782608695652174, 0.48753462603878117, 0.92, 0.0, 0.0, 0.375, 0.566048020220375, 0.5697673557822874, 1.0, 1.0, 0.8, 0.4233037445677538], 
reward next is 0.5767, 
noisyNet noise sample is [array([1.2827511], dtype=float32), 1.2116845]. 
=============================================
[2019-04-09 14:40:29,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07398555 0.09574185 0.07245149 0.07332189 0.06912474 0.05666407
 0.11036929 0.10840496 0.06679623 0.14267784 0.13046214], sum to 1.0000
[2019-04-09 14:40:29,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-09 14:40:29,513] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 19.0, 25.01785233204737, 0.2755237009439727, 0.0, 1.0, 45.0, 31.04634461088909], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [4.05, 87.0, 0.0, 0.0, 19.0, 24.9429325388494, 0.1986966333745509, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.5747922437673131, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5785777115707834, 0.5662322111248503, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31480646], dtype=float32), -0.58901006]. 
=============================================
[2019-04-09 14:40:29,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07869992 0.09633815 0.07266221 0.06787835 0.07497349 0.06027008
 0.09871043 0.1119474  0.07149286 0.14215288 0.12487426], sum to 1.0000
[2019-04-09 14:40:29,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1370
[2019-04-09 14:40:29,545] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 19.0, 24.9429325388494, 0.1986966333745509, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 19.0, 24.70261472071402, 0.2052226555847771, 0.0, 1.0, 20.0, 45.64095752165863], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 0.08333333333333333, 0.5585512267261684, 0.5684075518615924, 0.0, 1.0, 0.1, 0.45640957521658626], 
reward next is 0.5436, 
noisyNet noise sample is [array([-0.31480646], dtype=float32), -0.58901006]. 
=============================================
[2019-04-09 14:40:29,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[-0.48918176]
 [-0.49032575]
 [-0.5582052 ]
 [-0.44036722]
 [-0.3933343 ]], R is [[0.09339789]
 [1.09246397]
 [1.77107596]
 [2.42694473]
 [3.05864882]].
[2019-04-09 14:40:29,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.07281305 0.08440322 0.06799997 0.06104435 0.07145399 0.05675867
 0.12071696 0.12824969 0.06041882 0.15358904 0.12255235], sum to 1.0000
[2019-04-09 14:40:29,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7813
[2019-04-09 14:40:29,725] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 23.77523911507637, 0.003701832818239575, 0.0, 1.0, 40.0, 28.60956593367428], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 523200.0000, 
sim time next is 523800.0000, 
raw observation next is [4.65, 88.5, 0.0, 0.0, 19.0, 23.77823729471875, 0.009574945323493384, 0.0, 1.0, 60.0, 58.25800692883512], 
processed observation next is [0.0, 0.043478260869565216, 0.5914127423822716, 0.885, 0.0, 0.0, 0.08333333333333333, 0.4815197745598958, 0.5031916484411645, 0.0, 1.0, 0.9, 0.5825800692883513], 
reward next is 0.4174, 
noisyNet noise sample is [array([1.8749119], dtype=float32), -0.51426405]. 
=============================================
[2019-04-09 14:40:30,098] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03905443 0.1292879  0.08229741 0.09978271 0.07240205 0.05251338
 0.1067839  0.0960047  0.11204553 0.07137345 0.13845457], sum to 1.0000
[2019-04-09 14:40:30,098] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5591
[2019-04-09 14:40:30,121] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 22.5, 25.27434855274913, 0.2812495407416488, 1.0, 1.0, 60.0, 47.09589604854013], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 487200.0000, 
sim time next is 487800.0000, 
raw observation next is [0.55, 35.5, 56.0, 0.0, 22.5, 25.39855613177092, 0.2956670423660864, 1.0, 1.0, 45.0, 43.1885331406572], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.355, 0.18666666666666668, 0.0, 0.375, 0.6165463443142434, 0.5985556807886955, 1.0, 1.0, 0.6, 0.431885331406572], 
reward next is 0.5681, 
noisyNet noise sample is [array([-1.1350223], dtype=float32), 0.8671989]. 
=============================================
[2019-04-09 14:40:30,131] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0452675  0.13214035 0.08748764 0.09786875 0.07798804 0.05374466
 0.09554166 0.10186283 0.11492226 0.0659666  0.12720971], sum to 1.0000
[2019-04-09 14:40:30,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1910
[2019-04-09 14:40:30,149] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.55, 35.5, 56.0, 0.0, 22.5, 25.39855613177092, 0.2956670423660864, 1.0, 1.0, 45.0, 43.1885331406572], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 487800.0000, 
sim time next is 488400.0000, 
raw observation next is [0.7333333333333334, 35.0, 50.00000000000001, 0.0, 22.5, 25.47387312631709, 0.2988232827618003, 1.0, 1.0, 25.0, 33.34168252184417], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.35, 0.16666666666666669, 0.0, 0.375, 0.6228227605264243, 0.5996077609206001, 1.0, 1.0, 0.2, 0.3334168252184417], 
reward next is 0.6666, 
noisyNet noise sample is [array([-1.1350223], dtype=float32), 0.8671989]. 
=============================================
[2019-04-09 14:40:30,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.08395559 0.09872416 0.0743387  0.06984504 0.07061676 0.06551936
 0.10457291 0.12689131 0.05090971 0.13644564 0.11818087], sum to 1.0000
[2019-04-09 14:40:30,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9355
[2019-04-09 14:40:30,180] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.3, 88.0, 0.0, 0.0, 19.0, 24.44785255532251, 0.1871743259209278, 0.0, 1.0, 20.0, 48.63332539975071], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 525600.0000, 
sim time next is 526200.0000, 
raw observation next is [4.216666666666667, 87.66666666666667, 0.0, 0.0, 19.0, 24.53674434724302, 0.1999333712586296, 0.0, 1.0, 65.0, 55.2574969104041], 
processed observation next is [0.0, 0.08695652173913043, 0.5794090489381348, 0.8766666666666667, 0.0, 0.0, 0.08333333333333333, 0.544728695603585, 0.5666444570862098, 0.0, 1.0, 1.0, 0.552574969104041], 
reward next is 0.4474, 
noisyNet noise sample is [array([0.0966497], dtype=float32), 0.20194966]. 
=============================================
[2019-04-09 14:40:30,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06635979 0.11303653 0.07313746 0.07168616 0.08747754 0.06758717
 0.09876923 0.10371701 0.07430933 0.12594146 0.11797832], sum to 1.0000
[2019-04-09 14:40:30,715] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5271
[2019-04-09 14:40:30,730] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 87.0, 110.5, 122.0, 19.0, 23.82245907563117, 0.05356530970739933, 0.0, 1.0, 35.0, 30.26211424227408], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 554400.0000, 
sim time next is 555000.0000, 
raw observation next is [-0.6, 86.33333333333333, 99.33333333333331, 128.3333333333333, 19.0, 23.78315840697451, 0.03795243090909102, 0.0, 1.0, 45.0, 28.73760745875372], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8633333333333333, 0.331111111111111, 0.14180478821362794, 0.08333333333333333, 0.48192986724787595, 0.5126508103030304, 0.0, 1.0, 0.6, 0.28737607458753717], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.01692383], dtype=float32), -0.124615185]. 
=============================================
[2019-04-09 14:40:30,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[-0.4601767 ]
 [-0.5069829 ]
 [-0.56196165]
 [-0.45751065]
 [-0.46375936]], R is [[0.23336396]
 [0.92840922]
 [1.60023534]
 [2.24775577]
 [2.87391734]].
[2019-04-09 14:40:30,771] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04290777 0.09749109 0.08634583 0.05816705 0.05564247 0.05258743
 0.12981966 0.10322145 0.07159568 0.11908717 0.18313438], sum to 1.0000
[2019-04-09 14:40:30,772] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-09 14:40:30,784] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 25.35817888683121, 0.3720419798905401, 0.0, 1.0, 65.0, 54.14332491821923], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 19.0, 25.41474991965924, 0.3810757807700786, 0.0, 1.0, 50.0, 49.44878049334296], 
processed observation next is [1.0, 0.9130434782608695, 0.5323176361957526, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.61789582663827, 0.6270252602566929, 0.0, 1.0, 0.7, 0.4944878049334296], 
reward next is 0.5055, 
noisyNet noise sample is [array([-1.2445637], dtype=float32), 0.17279309]. 
=============================================
[2019-04-09 14:40:30,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06834099 0.11744232 0.07203136 0.08025433 0.08413579 0.073446
 0.0973095  0.11109442 0.07472414 0.11751323 0.10370794], sum to 1.0000
[2019-04-09 14:40:30,953] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2418
[2019-04-09 14:40:30,971] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 24.09178308723932, 0.1712507747234467, 0.0, 1.0, 65.0, 63.30431966950514], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 19.0, 24.21297250087459, 0.1945431807585514, 0.0, 1.0, 25.0, 49.40023479260894], 
processed observation next is [0.0, 0.4782608695652174, 0.44136657433056325, 0.8133333333333332, 0.3422222222222223, 0.24530386740331492, 0.08333333333333333, 0.5177477084062158, 0.5648477269195171, 0.0, 1.0, 0.2, 0.49400234792608944], 
reward next is 0.5060, 
noisyNet noise sample is [array([1.8470438], dtype=float32), 0.0037728364]. 
=============================================
[2019-04-09 14:40:30,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[-0.33208096]
 [-0.34157676]
 [-0.3200631 ]
 [-0.39717007]
 [-0.39125937]], R is [[0.12077424]
 [0.4865233 ]
 [0.90930545]
 [1.48114288]
 [1.85155928]].
[2019-04-09 14:40:30,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0683156  0.11212806 0.07500392 0.07022141 0.07736272 0.06945518
 0.10950307 0.10212068 0.07976    0.13371849 0.1024108 ], sum to 1.0000
[2019-04-09 14:40:30,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-09 14:40:30,991] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07290752 0.10300037 0.07486673 0.07919954 0.08006382 0.07260699
 0.09844092 0.11006767 0.07087509 0.13513125 0.10284014], sum to 1.0000
[2019-04-09 14:40:30,991] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-09 14:40:31,002] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.7, 82.0, 0.0, 0.0, 19.0, 25.50942499147744, 0.4025203585941401, 0.0, 1.0, 35.0, 42.4238898762739], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 532800.0000, 
sim time next is 533400.0000, 
raw observation next is [2.516666666666667, 82.50000000000001, 0.0, 0.0, 19.0, 25.50931922512201, 0.3960046510628438, 0.0, 1.0, 55.0, 43.18975048486745], 
processed observation next is [0.0, 0.17391304347826086, 0.5323176361957526, 0.8250000000000002, 0.0, 0.0, 0.08333333333333333, 0.6257766020935008, 0.6320015503542813, 0.0, 1.0, 0.8, 0.4318975048486745], 
reward next is 0.5681, 
noisyNet noise sample is [array([-0.29146776], dtype=float32), -0.9097264]. 
=============================================
[2019-04-09 14:40:31,006] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 19.0, 24.50972079627767, 0.2200309622860584, 0.0, 1.0, 45.0, 33.1213753308593], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 541200.0000, 
sim time next is 541800.0000, 
raw observation next is [0.8, 90.0, 0.0, 0.0, 19.0, 24.66602588100876, 0.230444837228683, 0.0, 1.0, 60.0, 58.70857106586621], 
processed observation next is [0.0, 0.2608695652173913, 0.4847645429362882, 0.9, 0.0, 0.0, 0.08333333333333333, 0.55550215675073, 0.5768149457428943, 0.0, 1.0, 0.9, 0.5870857106586621], 
reward next is 0.4129, 
noisyNet noise sample is [array([1.0211997], dtype=float32), 0.01758455]. 
=============================================
[2019-04-09 14:40:31,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07173965 0.10470794 0.07034075 0.06895252 0.0766843  0.06719563
 0.10528748 0.1075806  0.06572378 0.14374562 0.11804171], sum to 1.0000
[2019-04-09 14:40:31,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0119
[2019-04-09 14:40:31,171] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 23.48999424455259, -0.008111724258474286, 0.0, 1.0, 20.0, 46.29461406602069], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 544800.0000, 
sim time next is 545400.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.54292318863297, -0.08413516119886205, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.46191026571941407, 0.471954946267046, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9860147], dtype=float32), 0.9589172]. 
=============================================
[2019-04-09 14:40:31,487] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07461236 0.10815271 0.06985387 0.07866936 0.08050089 0.07255418
 0.10488816 0.10913737 0.07290898 0.13008907 0.09863299], sum to 1.0000
[2019-04-09 14:40:31,492] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4481
[2019-04-09 14:40:31,495] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05850183 0.10366194 0.07187016 0.06421202 0.06789703 0.05007939
 0.13294761 0.12218953 0.06503418 0.10870364 0.15490277], sum to 1.0000
[2019-04-09 14:40:31,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9260
[2019-04-09 14:40:31,505] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 24.90716759430045, 0.3108608013543987, 0.0, 1.0, 50.0, 51.49245735630811], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 545400.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 24.9780730163813, 0.3105151336076264, 0.0, 1.0, 40.0, 37.33962581105976], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.5815060846984416, 0.6035050445358755, 0.0, 1.0, 0.5, 0.3733962581105976], 
reward next is 0.6266, 
noisyNet noise sample is [array([0.15402368], dtype=float32), 1.9610482]. 
=============================================
[2019-04-09 14:40:31,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06334949 0.11534104 0.07193321 0.07191808 0.08134946 0.06954151
 0.10034236 0.10589767 0.07223736 0.1353055  0.11278439], sum to 1.0000
[2019-04-09 14:40:31,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0711852  0.10290443 0.07070571 0.07286532 0.07239182 0.06810976
 0.10716477 0.11375792 0.06776154 0.14475478 0.10839871], sum to 1.0000
[2019-04-09 14:40:31,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0030
[2019-04-09 14:40:31,512] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.3893898 ]
 [-0.35443643]
 [-0.30948156]
 [-0.40946156]
 [-0.33038458]], R is [[0.1695334 ]
 [0.65291345]
 [1.07430637]
 [1.60112429]
 [1.8599925 ]].
[2019-04-09 14:40:31,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5651
[2019-04-09 14:40:31,515] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.466666666666667, 96.33333333333333, 0.0, 0.0, 19.0, 24.69671213337197, 0.2396724588979284, 0.0, 1.0, 50.0, 49.72470828393296], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 516000.0000, 
sim time next is 516600.0000, 
raw observation next is [3.55, 96.5, 0.0, 0.0, 19.0, 24.72559188335924, 0.2559247260621996, 0.0, 1.0, 65.0, 53.74959614483813], 
processed observation next is [1.0, 1.0, 0.5609418282548477, 0.965, 0.0, 0.0, 0.08333333333333333, 0.5604659902799366, 0.5853082420207332, 0.0, 1.0, 1.0, 0.5374959614483813], 
reward next is 0.4625, 
noisyNet noise sample is [array([0.27454266], dtype=float32), 1.8871518]. 
=============================================
[2019-04-09 14:40:31,519] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 24.40349391582853, 0.1850566346210447, 0.0, 1.0, 65.0, 67.91256217364152], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 545400.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 24.39925703322068, 0.1100222240861021, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.5332714194350568, 0.5366740746953674, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3258392], dtype=float32), -0.85776025]. 
=============================================
[2019-04-09 14:40:31,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.5, 34.0, 104.0, 19.0, 24.39947023975658, 0.1993519956052613, 0.0, 1.0, 60.0, 49.2326320192895], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 549000.0000, 
sim time next is 549600.0000, 
raw observation next is [0.1666666666666667, 91.33333333333334, 52.33333333333333, 103.8333333333333, 19.0, 24.45089494652477, 0.2070172930722529, 0.0, 1.0, 45.0, 44.09546419872402], 
processed observation next is [0.0, 0.34782608695652173, 0.4672206832871654, 0.9133333333333334, 0.17444444444444443, 0.11473296500920807, 0.08333333333333333, 0.5375745788770642, 0.5690057643574177, 0.0, 1.0, 0.6, 0.44095464198724016], 
reward next is 0.5590, 
noisyNet noise sample is [array([-0.44460446], dtype=float32), -0.049072515]. 
=============================================
[2019-04-09 14:40:31,536] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.43948597]
 [-0.4558378 ]
 [-0.50737596]
 [-0.47146982]
 [-0.50814366]], R is [[0.52509344]
 [0.8407169 ]
 [1.41883695]
 [2.11663294]
 [2.79018021]].
[2019-04-09 14:40:31,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06350844 0.1108215  0.07473371 0.08506566 0.08244932 0.06323422
 0.10076489 0.10960998 0.06774521 0.13310772 0.10895944], sum to 1.0000
[2019-04-09 14:40:31,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6836
[2019-04-09 14:40:31,688] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 19.0, 24.35177647472323, 0.189871438330865, 0.0, 1.0, 30.0, 39.08003443030909], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 19.0, 24.3914193528297, 0.1896938335752775, 0.0, 1.0, 50.0, 38.88895949198136], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.08333333333333333, 0.5326182794024751, 0.5632312778584259, 0.0, 1.0, 0.7, 0.3888895949198136], 
reward next is 0.6111, 
noisyNet noise sample is [array([-0.5024594], dtype=float32), -1.0683651]. 
=============================================
[2019-04-09 14:40:31,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04970201 0.09461993 0.07091718 0.07328485 0.06059333 0.0530092
 0.13651949 0.13422057 0.06774245 0.11475255 0.14463842], sum to 1.0000
[2019-04-09 14:40:31,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2949
[2019-04-09 14:40:31,766] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 19.0, 25.77187398038991, 0.4583599421477215, 0.0, 1.0, 45.0, 36.15920822680165], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 508800.0000, 
sim time next is 509400.0000, 
raw observation next is [2.15, 94.0, 0.0, 0.0, 19.0, 25.78062563310444, 0.451075885826932, 0.0, 1.0, 35.0, 28.48487449144029], 
processed observation next is [1.0, 0.9130434782608695, 0.5221606648199446, 0.94, 0.0, 0.0, 0.08333333333333333, 0.6483854694253699, 0.6503586286089773, 0.0, 1.0, 0.4, 0.28484874491440293], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.11909668], dtype=float32), -0.6513126]. 
=============================================
[2019-04-09 14:40:32,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.06616302 0.11683338 0.0694813  0.08205495 0.08563432 0.07028678
 0.10178881 0.1078487  0.06950232 0.12667026 0.10373618], sum to 1.0000
[2019-04-09 14:40:32,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2888
[2019-04-09 14:40:32,167] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 82.0, 89.0, 135.0, 19.0, 25.09795048586616, 0.3617665389131006, 0.0, 1.0, 60.0, 54.03779642386845], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 559800.0000, 
sim time next is 560400.0000, 
raw observation next is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 25.11661304542783, 0.3704790410963886, 0.0, 1.0, 30.0, 42.33202351515339], 
processed observation next is [0.0, 0.4782608695652174, 0.44228993536472766, 0.8166666666666668, 0.3194444444444444, 0.19723756906077347, 0.08333333333333333, 0.5930510871189858, 0.6234930136987962, 0.0, 1.0, 0.3, 0.4233202351515339], 
reward next is 0.5767, 
noisyNet noise sample is [array([1.6581014], dtype=float32), -0.730473]. 
=============================================
[2019-04-09 14:40:32,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06702462 0.09839825 0.07185701 0.06948609 0.0841672  0.06255495
 0.11353891 0.11880919 0.05790248 0.13189745 0.12436382], sum to 1.0000
[2019-04-09 14:40:32,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5459
[2019-04-09 14:40:32,274] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06446953 0.12161487 0.07457075 0.08331406 0.08910706 0.06858484
 0.09628816 0.09660482 0.07416812 0.1214784  0.10979939], sum to 1.0000
[2019-04-09 14:40:32,278] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 86.33333333333333, 0.0, 0.0, 19.0, 23.60935575684017, 0.05626406611068102, 0.0, 1.0, 35.0, 46.45126715988522], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 591000.0000, 
sim time next is 591600.0000, 
raw observation next is [-2.8, 85.66666666666667, 0.0, 0.0, 19.0, 23.68888820378349, 0.0561763216701562, 0.0, 1.0, 45.0, 32.24008249671559], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.47407401698195734, 0.5187254405567188, 0.0, 1.0, 0.6, 0.3224008249671559], 
reward next is 0.6776, 
noisyNet noise sample is [array([-0.16764747], dtype=float32), -0.8320887]. 
=============================================
[2019-04-09 14:40:32,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-09 14:40:32,289] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.2, 83.0, 100.0, 73.0, 19.0, 23.80626980737995, 0.09718922893915251, 0.0, 1.0, 50.0, 61.01648401505277], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 574200.0000, 
sim time next is 574800.0000, 
raw observation next is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 19.0, 23.73685339246846, 0.04889774548914842, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3005555555555556, 0.07550644567219153, 0.08333333333333333, 0.47807111603903846, 0.5162992484963828, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2500623], dtype=float32), 1.1224838]. 
=============================================
[2019-04-09 14:40:32,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07181809 0.10610417 0.07506185 0.08586449 0.08054563 0.07484215
 0.09635181 0.10902013 0.06641211 0.13083246 0.10314704], sum to 1.0000
[2019-04-09 14:40:32,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-09 14:40:32,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06071111 0.10068692 0.06737599 0.08360396 0.0807201  0.05468022
 0.11489246 0.1266436  0.05819735 0.12575103 0.1267373 ], sum to 1.0000
[2019-04-09 14:40:32,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9463
[2019-04-09 14:40:32,451] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 19.0, 24.88298909787117, 0.3471762734577913, 0.0, 1.0, 45.0, 67.20558742013921], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 19.0, 24.81064413453148, 0.3613004584540259, 0.0, 1.0, 30.0, 36.87061399533383], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4555555555555557, 0.6208103130755065, 0.08333333333333333, 0.5675536778776232, 0.620433486151342, 0.0, 1.0, 0.3, 0.3687061399533383], 
reward next is 0.6313, 
noisyNet noise sample is [array([-0.44706747], dtype=float32), -0.046533488]. 
=============================================
[2019-04-09 14:40:32,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 83.0, 0.0, 0.0, 19.0, 23.07869551413407, -0.1956987044725726, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [-3.2, 83.0, 0.0, 0.0, 19.0, 22.873624188437, -0.1689897795147811, 0.0, 1.0, 65.0, 72.35660762517942], 
processed observation next is [0.0, 0.9565217391304348, 0.37396121883656513, 0.83, 0.0, 0.0, 0.08333333333333333, 0.40613534903641657, 0.443670073495073, 0.0, 1.0, 1.0, 0.7235660762517943], 
reward next is 0.2764, 
noisyNet noise sample is [array([-1.2542218], dtype=float32), -0.7196471]. 
=============================================
[2019-04-09 14:40:32,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.42301917]
 [-0.48155296]
 [-0.48116267]
 [-0.40664476]
 [-0.4926324 ]], R is [[-0.22099599]
 [ 0.781214  ]
 [ 1.41994143]
 [ 2.02361941]
 [ 2.733495  ]].
[2019-04-09 14:40:32,465] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15304: loss 26.5173
[2019-04-09 14:40:32,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15305: learning rate 0.0000
[2019-04-09 14:40:32,497] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15328: loss 23.0048
[2019-04-09 14:40:32,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15328: learning rate 0.0000
[2019-04-09 14:40:32,505] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06399113 0.10723627 0.07206365 0.07795175 0.08556496 0.07182526
 0.10460132 0.11256053 0.06959705 0.12279969 0.11180841], sum to 1.0000
[2019-04-09 14:40:32,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2612
[2019-04-09 14:40:32,517] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06708168 0.10386963 0.0726191  0.07570282 0.08249412 0.06469648
 0.10611532 0.12408469 0.05668522 0.12498879 0.12166214], sum to 1.0000
[2019-04-09 14:40:32,518] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 19.0, 24.55195765725655, 0.2509383329047246, 0.0, 1.0, 30.0, 33.7143907263339], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 573600.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 19.0, 24.58964715471086, 0.2453645205692947, 0.0, 1.0, 30.0, 30.33909850146947], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.08333333333333333, 0.5491372628925717, 0.5817881735230982, 0.0, 1.0, 0.3, 0.3033909850146947], 
reward next is 0.6966, 
noisyNet noise sample is [array([1.1221243], dtype=float32), -2.1499157]. 
=============================================
[2019-04-09 14:40:32,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1482
[2019-04-09 14:40:32,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 22.8100688026565, -0.1611287712267145, 0.0, 1.0, 35.0, 28.58816799454111], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 601200.0000, 
sim time next is 601800.0000, 
raw observation next is [-3.4, 83.66666666666667, 0.0, 0.0, 19.0, 22.84521601645534, -0.1703480938982826, 0.0, 1.0, 20.0, 29.84499492556649], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.4037680013712783, 0.4432173020339058, 0.0, 1.0, 0.1, 0.29844994925566487], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.2867214], dtype=float32), -0.8345872]. 
=============================================
[2019-04-09 14:40:32,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06142966 0.10082414 0.06984267 0.07666907 0.09347612 0.06730083
 0.10647863 0.11927136 0.05561947 0.1265822  0.12250585], sum to 1.0000
[2019-04-09 14:40:32,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0848
[2019-04-09 14:40:32,618] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 85.0, 0.0, 0.0, 19.0, 22.76968332266484, -0.1833847007994329, 0.0, 1.0, 65.0, 58.16528418027324], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 603000.0000, 
sim time next is 603600.0000, 
raw observation next is [-3.4, 85.66666666666667, 0.0, 0.0, 19.0, 22.74226753210751, -0.176514735137407, 0.0, 1.0, 20.0, 43.81034698371398], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.395188961008959, 0.4411617549541977, 0.0, 1.0, 0.1, 0.43810346983713977], 
reward next is 0.5619, 
noisyNet noise sample is [array([0.22956116], dtype=float32), -0.2317968]. 
=============================================
[2019-04-09 14:40:32,645] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15417: loss 30.8167
[2019-04-09 14:40:32,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15417: learning rate 0.0000
[2019-04-09 14:40:32,698] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15444: loss 22.2143
[2019-04-09 14:40:32,701] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15445: learning rate 0.0000
[2019-04-09 14:40:32,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.06776776 0.10395678 0.07214174 0.07868416 0.08989742 0.07116899
 0.10171273 0.11747432 0.06213953 0.12378184 0.1112747 ], sum to 1.0000
[2019-04-09 14:40:32,826] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-09 14:40:32,835] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.07436436 0.10877466 0.07759493 0.072693   0.08015685 0.07748404
 0.10318177 0.10033455 0.0757979  0.12166663 0.10795133], sum to 1.0000
[2019-04-09 14:40:32,837] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0499
[2019-04-09 14:40:32,843] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.8, 84.33333333333333, 0.0, 0.0, 19.0, 23.66432239847252, 0.04829150376010641, 0.0, 1.0, 65.0, 63.55519137240458], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 592800.0000, 
sim time next is 593400.0000, 
raw observation next is [-2.8, 83.66666666666667, 0.0, 0.0, 19.0, 23.69771815931578, 0.0760173710777584, 0.0, 1.0, 60.0, 59.62607023917102], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.47480984660964837, 0.5253391236925861, 0.0, 1.0, 0.9, 0.5962607023917103], 
reward next is 0.4037, 
noisyNet noise sample is [array([0.02965988], dtype=float32), -0.620689]. 
=============================================
[2019-04-09 14:40:32,848] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 23.13568973227609, 0.002050891463621203, 0.0, 1.0, 30.0, 40.03628050903027], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 545400.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.34769700060828, -0.0369351022800699, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4456414167173568, 0.4876882992399767, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6393798], dtype=float32), -0.51260805]. 
=============================================
[2019-04-09 14:40:32,872] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[-0.39044052]
 [-0.4957102 ]
 [-0.4510513 ]
 [-0.40221784]
 [-0.46274114]], R is [[0.58044893]
 [1.1742816 ]
 [1.33712423]
 [2.32375288]
 [3.30051541]].
[2019-04-09 14:40:32,993] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15620: loss 27.5850
[2019-04-09 14:40:32,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15620: learning rate 0.0000
[2019-04-09 14:40:33,032] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05632069 0.11322881 0.07176804 0.07351401 0.09119955 0.06822383
 0.10630117 0.11256187 0.07281154 0.11657432 0.11749609], sum to 1.0000
[2019-04-09 14:40:33,034] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-09 14:40:33,051] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 19.0, 24.63517732884577, 0.2238560163211916, 0.0, 1.0, 45.0, 32.91298443394901], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 19.0, 24.60606279999145, 0.2066351270928285, 0.0, 1.0, 30.0, 31.51469625261528], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5505052333326207, 0.5688783756976096, 0.0, 1.0, 0.3, 0.3151469625261528], 
reward next is 0.6849, 
noisyNet noise sample is [array([-0.16981143], dtype=float32), -0.39565742]. 
=============================================
[2019-04-09 14:40:33,070] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.35953546]
 [-0.43737108]
 [-0.45221835]
 [-0.39946017]
 [-0.4404214 ]], R is [[0.24164015]
 [0.9100939 ]
 [1.57805634]
 [2.10398579]
 [2.55703163]].
[2019-04-09 14:40:33,170] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15729: loss 18.1424
[2019-04-09 14:40:33,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15729: learning rate 0.0000
[2019-04-09 14:40:33,304] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0664407  0.11195182 0.07700336 0.07559398 0.08590397 0.06511324
 0.11202334 0.11019572 0.0623388  0.12055773 0.11287732], sum to 1.0000
[2019-04-09 14:40:33,307] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1037
[2019-04-09 14:40:33,322] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 85.66666666666667, 0.0, 0.0, 19.0, 24.27774397128976, 0.160244264976159, 0.0, 1.0, 65.0, 62.1718130145704], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 591600.0000, 
sim time next is 592200.0000, 
raw observation next is [-2.8, 85.0, 0.0, 0.0, 19.0, 24.26531174600038, 0.1722437171446452, 0.0, 1.0, 20.0, 46.82741545089613], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5221093121666982, 0.5574145723815483, 0.0, 1.0, 0.1, 0.4682741545089613], 
reward next is 0.5317, 
noisyNet noise sample is [array([-1.570999], dtype=float32), 1.2463483]. 
=============================================
[2019-04-09 14:40:33,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06623334 0.11274844 0.07439244 0.07679397 0.082269   0.0671459
 0.10460622 0.11644715 0.06767278 0.1280826  0.10360817], sum to 1.0000
[2019-04-09 14:40:33,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0726
[2019-04-09 14:40:33,400] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 81.5, 127.0, 467.0, 19.0, 25.19101265058169, 0.3959404496623801, 0.0, 1.0, 55.0, 37.62485842340445], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 570600.0000, 
sim time next is 571200.0000, 
raw observation next is [-1.2, 82.0, 122.5, 401.3333333333334, 19.0, 25.24218523801933, 0.3884652128786485, 0.0, 1.0, 20.0, 35.50264058041144], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.82, 0.4083333333333333, 0.443462246777164, 0.08333333333333333, 0.603515436501611, 0.6294884042928829, 0.0, 1.0, 0.1, 0.3550264058041144], 
reward next is 0.6450, 
noisyNet noise sample is [array([0.89511204], dtype=float32), -1.4361756]. 
=============================================
[2019-04-09 14:40:33,421] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15870: loss 26.3642
[2019-04-09 14:40:33,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15871: learning rate 0.0000
[2019-04-09 14:40:33,570] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15960: loss 18.7973
[2019-04-09 14:40:33,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15960: learning rate 0.0000
[2019-04-09 14:40:33,597] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15972: loss 25.7563
[2019-04-09 14:40:33,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15972: learning rate 0.0000
[2019-04-09 14:40:33,642] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0657334  0.11362647 0.07069196 0.07346139 0.08331074 0.06761937
 0.10744993 0.1026839  0.06630241 0.1236213  0.12549914], sum to 1.0000
[2019-04-09 14:40:33,643] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5129
[2019-04-09 14:40:33,658] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 83.0, 0.0, 0.0, 19.0, 24.54725304377424, 0.2199068384300142, 0.0, 1.0, 45.0, 43.32110782091716], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 598800.0000, 
sim time next is 599400.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 19.0, 24.62494220951262, 0.2141437433412931, 0.0, 1.0, 45.0, 39.27771417998745], 
processed observation next is [0.0, 0.9565217391304348, 0.37673130193905824, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5520785174593851, 0.5713812477804311, 0.0, 1.0, 0.6, 0.3927771417998745], 
reward next is 0.6072, 
noisyNet noise sample is [array([-0.95576525], dtype=float32), 0.055399302]. 
=============================================
[2019-04-09 14:40:33,706] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 16032: loss 22.5561
[2019-04-09 14:40:33,706] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 16032: learning rate 0.0000
[2019-04-09 14:40:33,716] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16040: loss 26.1832
[2019-04-09 14:40:33,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16041: learning rate 0.0000
[2019-04-09 14:40:33,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.07337794 0.10766572 0.07140298 0.07864044 0.07949424 0.07044014
 0.10189167 0.09035984 0.0846631  0.1365371  0.10552687], sum to 1.0000
[2019-04-09 14:40:33,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3287
[2019-04-09 14:40:33,860] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16125: loss 23.4787
[2019-04-09 14:40:33,873] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16126: learning rate 0.0000
[2019-04-09 14:40:33,875] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 75.0, 0.0, 0.0, 19.0, 23.73661828734371, -0.00525606644682471, 0.0, 1.0, 45.0, 35.1109386813213], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 615600.0000, 
sim time next is 616200.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 19.0, 23.71639977560842, -0.01317970128867129, 0.0, 1.0, 30.0, 33.63416296459683], 
processed observation next is [0.0, 0.13043478260869565, 0.3518005540166205, 0.75, 0.0, 0.0, 0.08333333333333333, 0.4763666479673683, 0.4956067662371096, 0.0, 1.0, 0.3, 0.3363416296459683], 
reward next is 0.6637, 
noisyNet noise sample is [array([-0.97541416], dtype=float32), -1.7418305]. 
=============================================
[2019-04-09 14:40:34,060] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16230: loss 27.7495
[2019-04-09 14:40:34,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16231: learning rate 0.0000
[2019-04-09 14:40:34,122] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0744071  0.11805785 0.06592824 0.08169169 0.09134343 0.07204396
 0.09707738 0.10217448 0.06424566 0.13103668 0.10199349], sum to 1.0000
[2019-04-09 14:40:34,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2537
[2019-04-09 14:40:34,141] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 82.5, 118.0, 335.6666666666667, 19.0, 25.17406893572483, 0.3856222401290344, 0.0, 1.0, 45.0, 42.76051928966719], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 571800.0000, 
sim time next is 572400.0000, 
raw observation next is [-1.2, 83.0, 113.5, 270.0, 19.0, 25.18660823836436, 0.3723394266247306, 0.0, 1.0, 25.0, 37.9653526337319], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.37833333333333335, 0.2983425414364641, 0.08333333333333333, 0.5988840198636968, 0.6241131422082435, 0.0, 1.0, 0.2, 0.37965352633731897], 
reward next is 0.6203, 
noisyNet noise sample is [array([-0.8820593], dtype=float32), -0.117025234]. 
=============================================
[2019-04-09 14:40:34,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07554754 0.09556869 0.07166172 0.07459386 0.08019163 0.08014697
 0.09897268 0.10367125 0.07363994 0.13836281 0.10764299], sum to 1.0000
[2019-04-09 14:40:34,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7938
[2019-04-09 14:40:34,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 66.5, 0.0, 0.0, 19.0, 23.10583765367673, -0.1993436222730768, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 624600.0000, 
sim time next is 625200.0000, 
raw observation next is [-4.5, 66.0, 0.0, 0.0, 19.0, 22.93974578942823, -0.1620081991804734, 0.0, 1.0, 60.0, 75.16817231737265], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.66, 0.0, 0.0, 0.08333333333333333, 0.4116454824523525, 0.4459972669398422, 0.0, 1.0, 0.9, 0.7516817231737265], 
reward next is 0.2483, 
noisyNet noise sample is [array([1.634418], dtype=float32), 1.0055975]. 
=============================================
[2019-04-09 14:40:34,732] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16618: loss 27.5875
[2019-04-09 14:40:34,733] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16619: learning rate 0.0000
[2019-04-09 14:40:35,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.07307408 0.11271914 0.07769985 0.0779029  0.0793901  0.06969635
 0.10183073 0.10911974 0.05867164 0.12271833 0.11717717], sum to 1.0000
[2019-04-09 14:40:35,117] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-09 14:40:35,133] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 84.33333333333333, 0.0, 0.0, 19.0, 24.80652283500535, 0.231135648770401, 0.0, 1.0, 45.0, 60.43356197220277], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 592800.0000, 
sim time next is 593400.0000, 
raw observation next is [-2.8, 83.66666666666667, 0.0, 0.0, 19.0, 24.67797796739727, 0.2202560545230708, 0.0, 1.0, 30.0, 37.98784459013571], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5564981639497724, 0.5734186848410235, 0.0, 1.0, 0.3, 0.3798784459013571], 
reward next is 0.6201, 
noisyNet noise sample is [array([-1.3424772], dtype=float32), -1.1492102]. 
=============================================
[2019-04-09 14:40:35,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07502248 0.10184515 0.07838257 0.07528398 0.08295111 0.0741213
 0.10020516 0.10229282 0.07662161 0.12829027 0.10498353], sum to 1.0000
[2019-04-09 14:40:35,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6517
[2019-04-09 14:40:35,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 72.33333333333334, 57.66666666666666, 17.0, 19.0, 22.29422823851836, -0.3031361201253884, 0.0, 1.0, 50.0, 40.38141011785225], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 636600.0000, 
sim time next is 637200.0000, 
raw observation next is [-3.9, 71.0, 77.0, 25.5, 19.0, 22.29514380444658, -0.294834349513901, 0.0, 1.0, 50.0, 43.43448593795249], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.71, 0.25666666666666665, 0.0281767955801105, 0.08333333333333333, 0.3579286503705485, 0.40172188349536636, 0.0, 1.0, 0.7, 0.4343448593795249], 
reward next is 0.5657, 
noisyNet noise sample is [array([-2.5458632], dtype=float32), -0.53240067]. 
=============================================
[2019-04-09 14:40:35,267] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06305113 0.12299304 0.06859772 0.08174004 0.08930639 0.06528258
 0.10493623 0.11574595 0.06863817 0.10634066 0.11336804], sum to 1.0000
[2019-04-09 14:40:35,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6678
[2019-04-09 14:40:35,291] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 19.0, 22.70605897981964, -0.1964552698259465, 0.0, 1.0, 65.0, 62.52290442649183], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [-0.6, 54.0, 83.0, 38.0, 19.0, 22.77895067705055, -0.1758742996409941, 0.0, 1.0, 50.0, 48.30709961227536], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27666666666666667, 0.041988950276243095, 0.08333333333333333, 0.39824588975421243, 0.44137523345300195, 0.0, 1.0, 0.7, 0.48307099612275356], 
reward next is 0.5169, 
noisyNet noise sample is [array([0.20774315], dtype=float32), 0.6832532]. 
=============================================
[2019-04-09 14:40:35,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06301443 0.11448828 0.06608627 0.08251944 0.08809733 0.06388772
 0.10280289 0.1154202  0.07688782 0.1169864  0.10980924], sum to 1.0000
[2019-04-09 14:40:35,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-09 14:40:35,354] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 19.0, 23.46183117921655, -0.0527790987842601, 0.0, 1.0, 65.0, 68.4466507135509], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [-0.6, 54.0, 83.0, 38.0, 19.0, 23.53105008075363, -0.03502707783034901, 0.0, 1.0, 45.0, 52.6581369206704], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27666666666666667, 0.041988950276243095, 0.08333333333333333, 0.4609208400628025, 0.4883243073898837, 0.0, 1.0, 0.6, 0.5265813692067041], 
reward next is 0.4734, 
noisyNet noise sample is [array([0.26702163], dtype=float32), -0.84247184]. 
=============================================
[2019-04-09 14:40:35,404] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 17009: loss 22.2679
[2019-04-09 14:40:35,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 17009: learning rate 0.0000
[2019-04-09 14:40:35,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0746717  0.114443   0.06779166 0.07818165 0.07480995 0.06593625
 0.1004741  0.12225982 0.07236975 0.11905649 0.11000571], sum to 1.0000
[2019-04-09 14:40:35,516] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-09 14:40:35,536] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 19.0, 22.4013190256721, -0.3520905346592884, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 645600.0000, 
sim time next is 646200.0000, 
raw observation next is [-3.05, 63.0, 89.0, 38.0, 19.0, 22.23679421426662, -0.3299810743232087, 0.0, 1.0, 25.0, 52.86259455599799], 
processed observation next is [0.0, 0.4782608695652174, 0.3781163434903047, 0.63, 0.2966666666666667, 0.041988950276243095, 0.08333333333333333, 0.35306618452221833, 0.3900063085589304, 0.0, 1.0, 0.2, 0.5286259455599799], 
reward next is 0.4714, 
noisyNet noise sample is [array([-0.793117], dtype=float32), -0.019713582]. 
=============================================
[2019-04-09 14:40:35,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0665231  0.1131386  0.07231273 0.08019171 0.07775083 0.07045039
 0.10597775 0.11089927 0.07124881 0.1199889  0.11151789], sum to 1.0000
[2019-04-09 14:40:35,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5041
[2019-04-09 14:40:35,601] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 19.0, 23.11221696354559, -0.08220172574959542, 0.0, 1.0, 50.0, 45.24125959312845], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 654000.0000, 
sim time next is 654600.0000, 
raw observation next is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 19.0, 23.21267195924428, -0.07870058907765629, 0.0, 1.0, 45.0, 33.14846950115601], 
processed observation next is [0.0, 0.5652173913043478, 0.4242843951985227, 0.5983333333333333, 0.4944444444444443, 0.08913443830570902, 0.08333333333333333, 0.4343893299370232, 0.47376647030744795, 0.0, 1.0, 0.6, 0.3314846950115601], 
reward next is 0.6685, 
noisyNet noise sample is [array([0.39353716], dtype=float32), -0.536598]. 
=============================================
[2019-04-09 14:40:35,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07654713 0.10628151 0.0743685  0.06844355 0.08419028 0.07057463
 0.10333589 0.1029604  0.07171526 0.12947492 0.11210795], sum to 1.0000
[2019-04-09 14:40:35,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9288
[2019-04-09 14:40:35,624] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 77.16666666666667, 6.333333333333332, 0.0, 19.0, 22.65900752396202, -0.1867740322293567, 0.0, 1.0, 35.0, 43.2162302712447], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 633000.0000, 
sim time next is 633600.0000, 
raw observation next is [-4.5, 79.0, 9.5, 0.0, 19.0, 22.69434211630924, -0.17152283231032, 0.0, 1.0, 65.0, 57.70562565596365], 
processed observation next is [0.0, 0.34782608695652173, 0.3379501385041552, 0.79, 0.03166666666666667, 0.0, 0.08333333333333333, 0.39119517635910334, 0.44282572256322666, 0.0, 1.0, 1.0, 0.5770562565596365], 
reward next is 0.4229, 
noisyNet noise sample is [array([-0.8026341], dtype=float32), -0.96651566]. 
=============================================
[2019-04-09 14:40:35,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0743233  0.11807412 0.06956719 0.08211651 0.0822226  0.07563493
 0.09380928 0.12160376 0.07454879 0.10876466 0.09933484], sum to 1.0000
[2019-04-09 14:40:35,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6310
[2019-04-09 14:40:35,661] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 19.0, 22.63012341158942, -0.2160771040983047, 0.0, 1.0, 60.0, 56.8477392900235], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 645600.0000, 
sim time next is 646200.0000, 
raw observation next is [-3.05, 63.0, 89.0, 38.0, 19.0, 22.70785988112259, -0.2022563511234859, 0.0, 1.0, 50.0, 48.5115953793874], 
processed observation next is [0.0, 0.4782608695652174, 0.3781163434903047, 0.63, 0.2966666666666667, 0.041988950276243095, 0.08333333333333333, 0.3923216567602159, 0.4325812162921714, 0.0, 1.0, 0.7, 0.485115953793874], 
reward next is 0.5149, 
noisyNet noise sample is [array([-0.17358361], dtype=float32), -0.7098854]. 
=============================================
[2019-04-09 14:40:35,844] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 17262: loss 23.6304
[2019-04-09 14:40:35,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 17263: learning rate 0.0000
[2019-04-09 14:40:36,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.07323645 0.10958203 0.0742619  0.07849175 0.08003367 0.07130635
 0.0984748  0.11804473 0.06588151 0.11927003 0.11141673], sum to 1.0000
[2019-04-09 14:40:36,403] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8990
[2019-04-09 14:40:36,416] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 58.00000000000001, 97.83333333333333, 62.16666666666667, 19.0, 23.86402806531702, 0.05345876403450878, 0.0, 1.0, 20.0, 52.85539207961746], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 656400.0000, 
sim time next is 657000.0000, 
raw observation next is [-0.8999999999999999, 57.0, 81.0, 56.0, 19.0, 23.9477634656271, 0.07084147005824348, 0.0, 1.0, 65.0, 57.50562122204768], 
processed observation next is [0.0, 0.6086956521739131, 0.43767313019390586, 0.57, 0.27, 0.061878453038674036, 0.08333333333333333, 0.4956469554689251, 0.5236138233527479, 0.0, 1.0, 1.0, 0.5750562122204768], 
reward next is 0.4249, 
noisyNet noise sample is [array([1.9576936], dtype=float32), -0.6479554]. 
=============================================
[2019-04-09 14:40:36,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.32090864]
 [-0.2489648 ]
 [-0.3342379 ]
 [-0.26937836]
 [-0.33077788]], R is [[0.13599518]
 [0.60608131]
 [0.96531022]
 [1.4624455 ]
 [1.82382345]].
[2019-04-09 14:40:36,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0700618  0.12011325 0.06813131 0.08256543 0.08364982 0.06019412
 0.10924274 0.12115829 0.06227424 0.10723288 0.11537614], sum to 1.0000
[2019-04-09 14:40:36,471] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-09 14:40:36,493] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 23.53819748039859, -0.06670838399868041, 0.0, 1.0, 60.0, 53.44192776300184], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 684000.0000, 
sim time next is 684600.0000, 
raw observation next is [-3.483333333333333, 69.33333333333333, 0.0, 0.0, 19.0, 23.58085741380317, -0.06859229188981598, 0.0, 1.0, 40.0, 43.8069681844521], 
processed observation next is [0.0, 0.9565217391304348, 0.3661126500461681, 0.6933333333333332, 0.0, 0.0, 0.08333333333333333, 0.4650714511502641, 0.4771359027033946, 0.0, 1.0, 0.5, 0.43806968184452105], 
reward next is 0.5619, 
noisyNet noise sample is [array([-1.043794], dtype=float32), 0.59069175]. 
=============================================
[2019-04-09 14:40:36,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0722929  0.10844073 0.07040833 0.07480989 0.07699664 0.06134398
 0.09979259 0.12246294 0.0681783  0.12625459 0.11901922], sum to 1.0000
[2019-04-09 14:40:36,876] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-09 14:40:36,899] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 22.95275901245107, -0.2882269218348144, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 22.71502701015734, -0.2706216188783658, 0.0, 1.0, 60.0, 71.04461915470267], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.3929189175131116, 0.40979279370721144, 0.0, 1.0, 0.9, 0.7104461915470267], 
reward next is 0.2896, 
noisyNet noise sample is [array([-1.7868383], dtype=float32), -1.968603]. 
=============================================
[2019-04-09 14:40:36,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06340711 0.11854672 0.07354178 0.09848832 0.08100644 0.06530949
 0.10898105 0.1049802  0.05457025 0.10945756 0.12171113], sum to 1.0000
[2019-04-09 14:40:36,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-09 14:40:36,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.07475952 0.11923899 0.06834479 0.08365594 0.0856702  0.06212312
 0.09788215 0.11511336 0.06613176 0.10801231 0.11906787], sum to 1.0000
[2019-04-09 14:40:36,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-09 14:40:36,982] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 71.16666666666667, 0.0, 0.0, 19.0, 22.24911994716525, -0.3364739150012195, 0.0, 1.0, 40.0, 36.39745574319166], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 691800.0000, 
sim time next is 692400.0000, 
raw observation next is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 19.0, 22.387072304505, -0.3378955100822275, 0.0, 1.0, 45.0, 34.98537081416813], 
processed observation next is [1.0, 0.0, 0.35918744228993543, 0.7133333333333334, 0.0, 0.0, 0.08333333333333333, 0.36558935870875003, 0.3873681633059241, 0.0, 1.0, 0.6, 0.34985370814168126], 
reward next is 0.6501, 
noisyNet noise sample is [array([-1.4618149], dtype=float32), 0.7420858]. 
=============================================
[2019-04-09 14:40:36,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.75, 59.5, 0.0, 0.0, 19.0, 23.29853686224119, -0.1119411711923574, 0.0, 1.0, 20.0, 38.21695158707681], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 671400.0000, 
sim time next is 672000.0000, 
raw observation next is [-1.933333333333333, 60.33333333333333, 0.0, 0.0, 19.0, 23.33579302853344, -0.1036137177718043, 0.0, 1.0, 60.0, 51.3357256470847], 
processed observation next is [0.0, 0.782608695652174, 0.40904893813481075, 0.6033333333333333, 0.0, 0.0, 0.08333333333333333, 0.44464941904445343, 0.4654620940760652, 0.0, 1.0, 0.9, 0.513357256470847], 
reward next is 0.4866, 
noisyNet noise sample is [array([-0.2216171], dtype=float32), -1.9430503]. 
=============================================
[2019-04-09 14:40:37,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.28973728]
 [-0.37470567]
 [-0.37668598]
 [-0.33998436]
 [-0.40322006]], R is [[0.14046726]
 [0.75689304]
 [1.23221016]
 [1.67735076]
 [2.07153606]].
[2019-04-09 14:40:37,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06498449 0.08610855 0.089032   0.06465451 0.07852752 0.06756224
 0.12201489 0.11222097 0.06056708 0.12726033 0.1270675 ], sum to 1.0000
[2019-04-09 14:40:37,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3461
[2019-04-09 14:40:37,171] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 23.457680565001, -0.09295115921200485, 0.0, 1.0, 65.0, 57.19844618875662], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 23.52682253441958, -0.08342735679989861, 0.0, 1.0, 35.0, 50.98324420495072], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.460568544534965, 0.47219088106670043, 0.0, 1.0, 0.4, 0.5098324420495072], 
reward next is 0.4902, 
noisyNet noise sample is [array([0.02306174], dtype=float32), 0.6295701]. 
=============================================
[2019-04-09 14:40:37,309] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.07128833 0.11486275 0.07120215 0.07709491 0.08891364 0.07129826
 0.10547305 0.10813784 0.06336476 0.10762907 0.12073524], sum to 1.0000
[2019-04-09 14:40:37,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3001
[2019-04-09 14:40:37,349] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 19.0, 23.37149787393288, -0.1162767097453096, 0.0, 1.0, 60.0, 58.90963878288252], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 679800.0000, 
sim time next is 680400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 23.35213346602055, -0.1118605036215514, 0.0, 1.0, 30.0, 45.95024349921974], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.4460111221683792, 0.4627131654594829, 0.0, 1.0, 0.3, 0.4595024349921974], 
reward next is 0.5405, 
noisyNet noise sample is [array([0.6680569], dtype=float32), -0.030026412]. 
=============================================
[2019-04-09 14:40:37,399] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06078572 0.09344312 0.07711002 0.0591692  0.05956317 0.05329112
 0.13955502 0.10317271 0.09238111 0.11870813 0.14282067], sum to 1.0000
[2019-04-09 14:40:37,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8812
[2019-04-09 14:40:37,415] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 75.0, 0.0, 0.0, 19.0, 23.58128304096845, -0.1222083602026516, 0.0, 1.0, 55.0, 44.13646040633842], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 703800.0000, 
sim time next is 704400.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 19.0, 23.5031154186401, -0.1123966161350955, 0.0, 1.0, 65.0, 67.11090742542711], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.75, 0.0, 0.0, 0.08333333333333333, 0.4585929515533416, 0.4625344612883015, 0.0, 1.0, 1.0, 0.6711090742542711], 
reward next is 0.3289, 
noisyNet noise sample is [array([-0.03189657], dtype=float32), 0.61685926]. 
=============================================
[2019-04-09 14:40:37,864] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06729782 0.12021279 0.07180071 0.07400316 0.08124036 0.06354525
 0.11279154 0.11087875 0.0628743  0.12036382 0.11499146], sum to 1.0000
[2019-04-09 14:40:37,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-09 14:40:37,886] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.899999999999999, 71.0, 0.0, 0.0, 19.0, 23.01746402387751, -0.2269979583815354, 0.0, 1.0, 20.0, 29.35848638725795], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 690000.0000, 
sim time next is 690600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 19.0, 22.9114480597494, -0.2303617629433917, 0.0, 1.0, 65.0, 57.33606676645591], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.40928733831245, 0.42321274568553613, 0.0, 1.0, 1.0, 0.5733606676645591], 
reward next is 0.4266, 
noisyNet noise sample is [array([1.193317], dtype=float32), -0.38375163]. 
=============================================
[2019-04-09 14:40:38,263] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06992181 0.11884213 0.07293416 0.07872202 0.08508289 0.06070516
 0.10568985 0.11712498 0.06735063 0.11365435 0.10997206], sum to 1.0000
[2019-04-09 14:40:38,266] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5623
[2019-04-09 14:40:38,281] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 19.0, 22.41630227523456, -0.3160774642255103, 0.0, 1.0, 55.0, 45.61584096105204], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 19.0, 22.44428744333127, -0.3150783661968332, 0.0, 1.0, 45.0, 41.06033325398938], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.08333333333333333, 0.37035728694427245, 0.39497387793438893, 0.0, 1.0, 0.6, 0.4106033325398938], 
reward next is 0.5894, 
noisyNet noise sample is [array([-0.10878155], dtype=float32), 0.77712256]. 
=============================================
[2019-04-09 14:40:38,463] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05688751 0.09132253 0.07089385 0.06006872 0.05964507 0.05779546
 0.13302599 0.11800089 0.08757333 0.1096778  0.15510882], sum to 1.0000
[2019-04-09 14:40:38,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7726
[2019-04-09 14:40:38,482] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.43366154583959, -0.2874370885188551, 0.0, 1.0, 35.0, 50.31255968868679], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 710400.0000, 
sim time next is 711000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.69722953240995, -0.2630880497706036, 0.0, 1.0, 60.0, 51.79293597042726], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.39143579436749576, 0.4123039834097988, 0.0, 1.0, 0.9, 0.5179293597042726], 
reward next is 0.4821, 
noisyNet noise sample is [array([0.34570992], dtype=float32), -2.4120636]. 
=============================================
[2019-04-09 14:40:38,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[-0.164363  ]
 [-0.36965102]
 [-0.24364106]
 [-0.34561032]
 [-0.34413302]], R is [[0.13032407]
 [0.6258952 ]
 [0.92084002]
 [1.20795131]
 [2.19587183]].
[2019-04-09 14:40:38,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.06233962 0.08816767 0.08039897 0.06243144 0.0662024  0.05503451
 0.12499728 0.10845204 0.09341209 0.11543068 0.14313331], sum to 1.0000
[2019-04-09 14:40:38,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8861
[2019-04-09 14:40:38,671] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.89444583888179, -0.285324991483757, 0.0, 1.0, 45.0, 31.40491693484352], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 711000.0000, 
sim time next is 711600.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.82161920029353, -0.2732602305929652, 0.0, 1.0, 65.0, 59.30988706634808], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4018016000244608, 0.4089132564690116, 0.0, 1.0, 1.0, 0.5930988706634808], 
reward next is 0.4069, 
noisyNet noise sample is [array([0.09428217], dtype=float32), 0.34975412]. 
=============================================
[2019-04-09 14:40:38,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.06429836 0.08416584 0.07441518 0.05764835 0.06720103 0.0619478
 0.12482092 0.12414958 0.08541227 0.12538452 0.13055609], sum to 1.0000
[2019-04-09 14:40:38,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3687
[2019-04-09 14:40:38,837] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 75.5, 0.0, 0.0, 19.0, 22.90654897468084, -0.2137897230054206, 0.0, 1.0, 60.0, 69.27152720553937], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 707400.0000, 
sim time next is 708000.0000, 
raw observation next is [-2.466666666666667, 75.66666666666667, 0.0, 0.0, 19.0, 22.96903840685118, -0.1969817169145672, 0.0, 1.0, 40.0, 46.47777792413112], 
processed observation next is [1.0, 0.17391304347826086, 0.39427516158818104, 0.7566666666666667, 0.0, 0.0, 0.08333333333333333, 0.414086533904265, 0.4343394276951443, 0.0, 1.0, 0.5, 0.4647777792413112], 
reward next is 0.5352, 
noisyNet noise sample is [array([-1.0002608], dtype=float32), 0.5272948]. 
=============================================
[2019-04-09 14:40:38,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.12687027]
 [-0.16097414]
 [-0.17641774]
 [-0.13499771]
 [-0.10121159]], R is [[0.41556397]
 [0.71869308]
 [1.26666427]
 [1.96526849]
 [2.64067769]].
[2019-04-09 14:40:38,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04068659 0.13228288 0.07870095 0.10499761 0.06230085 0.05378299
 0.10831295 0.09292385 0.1180251  0.07425098 0.13373527], sum to 1.0000
[2019-04-09 14:40:38,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3919
[2019-04-09 14:40:38,979] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 22.5, 24.41798921469163, 0.04431611282364795, 1.0, 1.0, 60.0, 53.70764951040645], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 727200.0000, 
sim time next is 727800.0000, 
raw observation next is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 22.5, 24.44959722632058, 0.06054116069273791, 1.0, 1.0, 35.0, 41.00511197531344], 
processed observation next is [1.0, 0.43478260869565216, 0.4205909510618652, 0.6766666666666667, 0.421111111111111, 0.06813996316758747, 0.375, 0.537466435526715, 0.5201803868975793, 1.0, 1.0, 0.4, 0.4100511197531344], 
reward next is 0.5899, 
noisyNet noise sample is [array([0.58519834], dtype=float32), -0.1436164]. 
=============================================
[2019-04-09 14:40:39,157] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.07127772 0.08138335 0.08264442 0.06620197 0.08232923 0.07330027
 0.11767795 0.11247742 0.06691752 0.1197632  0.12602688], sum to 1.0000
[2019-04-09 14:40:39,160] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-09 14:40:39,174] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 22.91738509272179, -0.2291061011930352, 0.0, 1.0, 60.0, 50.85712592149823], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 22.95538076461979, -0.3077030465770627, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.41294839705164915, 0.39743231780764576, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25544795], dtype=float32), -0.5659813]. 
=============================================
[2019-04-09 14:40:39,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04569187 0.13924299 0.08378179 0.09334047 0.07401459 0.05939991
 0.10667561 0.11124788 0.09906475 0.07228545 0.11525464], sum to 1.0000
[2019-04-09 14:40:39,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6698
[2019-04-09 14:40:39,347] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 22.5, 25.55887496052739, 0.3597698765507758, 1.0, 1.0, 30.0, 30.28487256859404], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 22.5, 25.58670133792226, 0.3558950193716944, 1.0, 1.0, 50.0, 32.75818911752978], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.375, 0.6322251114935217, 0.6186316731238981, 1.0, 1.0, 0.7, 0.3275818911752978], 
reward next is 0.6724, 
noisyNet noise sample is [array([-1.0815703], dtype=float32), 2.0319943]. 
=============================================
[2019-04-09 14:40:39,910] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06163988 0.09651218 0.07151143 0.07034932 0.06369012 0.05973335
 0.13673118 0.11353094 0.08323198 0.10795901 0.13511057], sum to 1.0000
[2019-04-09 14:40:39,910] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1534
[2019-04-09 14:40:39,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 23.22084978178026, -0.1834504280740853, 0.0, 1.0, 55.0, 43.76269390192456], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 711600.0000, 
sim time next is 712200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 23.27519654454263, -0.1827399745389672, 0.0, 1.0, 20.0, 37.87576108542852], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.43959971204521925, 0.43908667515367755, 0.0, 1.0, 0.1, 0.3787576108542852], 
reward next is 0.6212, 
noisyNet noise sample is [array([1.9952173], dtype=float32), 0.4719107]. 
=============================================
[2019-04-09 14:40:40,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07153152 0.08346722 0.08588984 0.0506819  0.07696795 0.07167222
 0.11782873 0.1109444  0.07137661 0.12571959 0.13391995], sum to 1.0000
[2019-04-09 14:40:40,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5654
[2019-04-09 14:40:40,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0469144  0.12673108 0.08423032 0.09874231 0.07262341 0.06961398
 0.09455311 0.11943867 0.09157324 0.06847093 0.12710851], sum to 1.0000
[2019-04-09 14:40:40,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1349
[2019-04-09 14:40:40,380] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 74.5, 0.0, 0.0, 19.0, 23.34331549118641, -0.1268947471438623, 0.0, 1.0, 25.0, 48.19001583748636], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 697800.0000, 
sim time next is 698400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 23.37322533070071, -0.1322812338835141, 0.0, 1.0, 50.0, 32.98289289804225], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.44776877755839245, 0.45590625537216195, 0.0, 1.0, 0.7, 0.32982892898042254], 
reward next is 0.6702, 
noisyNet noise sample is [array([0.56535673], dtype=float32), -0.8004521]. 
=============================================
[2019-04-09 14:40:40,392] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.25, 46.0, 80.0, 714.0, 22.5, 25.60893542802305, 0.5556880652032423, 1.0, 1.0, 30.0, 25.95745260878816], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [0.1666666666666667, 46.33333333333334, 80.83333333333334, 600.1666666666666, 22.5, 26.20300403826015, 0.5933262458603248, 1.0, 1.0, 20.0, 31.86700981036074], 
processed observation next is [1.0, 0.6086956521739131, 0.4672206832871654, 0.46333333333333343, 0.2694444444444445, 0.6631675874769797, 0.375, 0.6835836698550125, 0.6977754152867749, 1.0, 1.0, 0.1, 0.3186700981036074], 
reward next is 0.6813, 
noisyNet noise sample is [array([1.6386234], dtype=float32), 0.51732767]. 
=============================================
[2019-04-09 14:40:40,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.05840369]
 [-0.02845456]
 [-0.07689507]
 [-0.08796757]
 [-0.07604689]], R is [[0.53626895]
 [1.27133179]
 [1.91521573]
 [2.45622492]
 [3.06228232]].
[2019-04-09 14:40:40,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02706864 0.12893514 0.07298954 0.09973726 0.07048059 0.05084448
 0.1222551  0.10094788 0.10732242 0.07418918 0.14522973], sum to 1.0000
[2019-04-09 14:40:40,455] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0800
[2019-04-09 14:40:40,483] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.166666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 26.82394600380939, 0.6223389515113656, 1.0, 1.0, 65.0, 46.88065053389147], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 753600.0000, 
sim time next is 754200.0000, 
raw observation next is [-3.35, 55.0, 0.0, 0.0, 22.5, 26.67968439145707, 0.6109993453092137, 1.0, 1.0, 20.0, 43.84652728838743], 
processed observation next is [1.0, 0.7391304347826086, 0.3698060941828255, 0.55, 0.0, 0.0, 0.375, 0.7233070326214225, 0.7036664484364046, 1.0, 1.0, 0.1, 0.43846527288387427], 
reward next is 0.5615, 
noisyNet noise sample is [array([0.1628224], dtype=float32), -0.97767466]. 
=============================================
[2019-04-09 14:40:40,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04423347 0.13994442 0.08548342 0.10267851 0.07454602 0.06582657
 0.088355   0.11097147 0.09038708 0.06812558 0.12944853], sum to 1.0000
[2019-04-09 14:40:40,544] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8314
[2019-04-09 14:40:40,559] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 22.5, 26.62571818980686, 0.6469124771431706, 1.0, 1.0, 55.0, 35.50356358958814], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [0.25, 46.0, 80.0, 714.0, 22.5, 26.70444112519335, 0.6631495273620889, 1.0, 1.0, 60.0, 38.47752563984858], 
processed observation next is [1.0, 0.6086956521739131, 0.46952908587257625, 0.46, 0.26666666666666666, 0.7889502762430939, 0.375, 0.7253700937661124, 0.7210498424540296, 1.0, 1.0, 0.9, 0.3847752563984858], 
reward next is 0.6152, 
noisyNet noise sample is [array([0.67076635], dtype=float32), 1.6879919]. 
=============================================
[2019-04-09 14:40:40,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06360216 0.09338503 0.07719264 0.06594513 0.06422514 0.06378619
 0.1220919  0.11210948 0.08639385 0.11813649 0.13313204], sum to 1.0000
[2019-04-09 14:40:40,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2323
[2019-04-09 14:40:40,661] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 23.33292506736048, -0.1632610987357908, 0.0, 1.0, 60.0, 50.99420957152035], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 712800.0000, 
sim time next is 713400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 23.39867794319209, -0.1664422662886952, 0.0, 1.0, 25.0, 45.46395108101404], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4498898285993409, 0.4445192445704349, 0.0, 1.0, 0.2, 0.45463951081014037], 
reward next is 0.5454, 
noisyNet noise sample is [array([-0.9936954], dtype=float32), -1.3994234]. 
=============================================
[2019-04-09 14:40:40,687] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-09 14:40:40,689] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:40:40,689] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:40:40,689] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:40:40,690] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:40:40,690] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:40:40,691] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:40:40,695] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run3
[2019-04-09 14:40:40,714] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run3
[2019-04-09 14:40:40,739] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run3
[2019-04-09 14:40:58,666] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00191011], dtype=float32), 0.003194208]
[2019-04-09 14:40:58,666] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-6.183333333333334, 48.16666666666667, 166.3333333333333, 446.3333333333333, 22.5, 22.2256483892327, -0.4342534604991955, 1.0, 1.0, 20.0, 24.26172635893404]
[2019-04-09 14:40:58,666] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:40:58,667] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.05784077 0.13560921 0.08118823 0.09389786 0.0867692  0.06815853
 0.0956887  0.10493588 0.08729911 0.08306899 0.1055435 ], sampled 0.8232632559523178
[2019-04-09 14:41:12,081] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00191011], dtype=float32), 0.003194208]
[2019-04-09 14:41:12,082] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [7.366666666666667, 75.33333333333333, 0.0, 0.0, 19.0, 27.81328912718238, 1.135685333767668, 0.0, 1.0, 60.0, 42.04569882985609]
[2019-04-09 14:41:12,082] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:41:12,082] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.04614048 0.12279879 0.07333907 0.09455618 0.08733067 0.06377572
 0.1153262  0.10414629 0.07188026 0.10784213 0.11286424], sampled 0.1659580143553283
[2019-04-09 14:41:39,379] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00191011], dtype=float32), 0.003194208]
[2019-04-09 14:41:39,379] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-4.0, 69.0, 0.0, 0.0, 22.5, 27.30113559221749, 0.9550515121727097, 1.0, 1.0, 20.0, 30.91328628959826]
[2019-04-09 14:41:39,379] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:41:39,380] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.05510888 0.11840955 0.07777084 0.09799583 0.08342206 0.07741073
 0.10625372 0.10725021 0.06776761 0.0964935  0.11211713], sampled 0.15795331053522366
[2019-04-09 14:41:51,827] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00191011], dtype=float32), 0.003194208]
[2019-04-09 14:41:51,827] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-1.166666666666667, 22.33333333333334, 74.33333333333333, 602.6666666666666, 22.5, 27.62622179567373, 0.9082607644500812, 1.0, 1.0, 35.0, 20.39046877669297]
[2019-04-09 14:41:51,828] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:41:51,828] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.05762073 0.13028234 0.08656172 0.08983905 0.07363877 0.09105746
 0.097576   0.12044279 0.06900613 0.07399179 0.10998324], sampled 0.5949474086348143
[2019-04-09 14:41:52,075] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00191011], dtype=float32), 0.003194208]
[2019-04-09 14:41:52,075] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-10.5921601, 53.33892167, 0.0, 0.0, 19.0, 24.42250039772733, 0.2257934149065, 0.0, 1.0, 20.0, 25.92318107671988]
[2019-04-09 14:41:52,075] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:41:52,076] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.07502951 0.09900524 0.08128131 0.07753099 0.08291481 0.0793562
 0.11225649 0.08931317 0.05973426 0.11468475 0.12889333], sampled 0.30422211507650987
[2019-04-09 14:42:00,744] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5729.1455 276495.3633 2638.5674
[2019-04-09 14:42:00,765] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:00,765] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:00,765] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:00,883] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:00,883] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:00,883] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,092] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5507.3068 298923.3246 2149.9321
[2019-04-09 14:42:08,112] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,112] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,112] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,226] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,226] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,226] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,759] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5430.8952 306536.4334 1754.5643
[2019-04-09 14:42:08,780] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,780] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,780] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:42:08,893] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,893] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:08,893] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:42:09,782] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 20000, evaluation results [20000.0, 5507.306822991223, 298923.3246384132, 2149.9320951427094, 5729.145520233603, 276495.3633396914, 2638.5674038553084, 5430.895202505475, 306536.4334199852, 1754.5643167345238]
[2019-04-09 14:42:09,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05203428 0.10223258 0.07474744 0.06931659 0.0746714  0.05476478
 0.13025324 0.12759064 0.06694911 0.09992234 0.14751765], sum to 1.0000
[2019-04-09 14:42:10,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-09 14:42:10,034] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 24.2023481212365, 0.1279602308251675, 0.0, 1.0, 50.0, 38.766497219926], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 774600.0000, 
sim time next is 775200.0000, 
raw observation next is [-6.9, 68.33333333333334, 0.0, 0.0, 19.0, 24.14354517755909, 0.1091112825928226, 0.0, 1.0, 20.0, 31.28615100506363], 
processed observation next is [1.0, 1.0, 0.27146814404432135, 0.6833333333333335, 0.0, 0.0, 0.08333333333333333, 0.5119620981299242, 0.5363704275309409, 0.0, 1.0, 0.1, 0.3128615100506363], 
reward next is 0.6871, 
noisyNet noise sample is [array([-0.20202704], dtype=float32), 0.00085260224]. 
=============================================
[2019-04-09 14:42:10,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04300125 0.10737436 0.07196389 0.07846019 0.077487   0.05055854
 0.1258626  0.11782816 0.08512283 0.087152   0.15518929], sum to 1.0000
[2019-04-09 14:42:10,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8277
[2019-04-09 14:42:10,208] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.6, 61.0, 0.0, 0.0, 19.0, 24.65860816901562, 0.2581708369090831, 0.0, 1.0, 55.0, 43.05026416223151], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 766800.0000, 
sim time next is 767400.0000, 
raw observation next is [-5.7, 61.5, 0.0, 0.0, 19.0, 24.59560685761976, 0.2676415831826998, 0.0, 1.0, 60.0, 52.66224776414421], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.615, 0.0, 0.0, 0.08333333333333333, 0.5496339048016466, 0.5892138610608999, 0.0, 1.0, 0.9, 0.5266224776414421], 
reward next is 0.4734, 
noisyNet noise sample is [array([-0.50112414], dtype=float32), 0.30857328]. 
=============================================
[2019-04-09 14:42:10,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06464515 0.09120071 0.07250669 0.06012868 0.06236099 0.05960422
 0.12190939 0.11180598 0.09666198 0.11290535 0.1462709 ], sum to 1.0000
[2019-04-09 14:42:10,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7227
[2019-04-09 14:42:10,733] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.466666666666667, 74.66666666666667, 0.0, 0.0, 19.0, 23.67816400188205, 0.00532151743789785, 0.0, 1.0, 45.0, 33.49728712048207], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 790800.0000, 
sim time next is 791400.0000, 
raw observation next is [-7.383333333333333, 74.83333333333333, 0.0, 0.0, 19.0, 23.65764817986907, 0.01005760888050307, 0.0, 1.0, 55.0, 43.155062581399], 
processed observation next is [1.0, 0.13043478260869565, 0.25807940904893817, 0.7483333333333333, 0.0, 0.0, 0.08333333333333333, 0.4714706816557559, 0.5033525362935011, 0.0, 1.0, 0.8, 0.43155062581399], 
reward next is 0.5684, 
noisyNet noise sample is [array([0.1980117], dtype=float32), -0.51807165]. 
=============================================
[2019-04-09 14:42:11,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05293515 0.10819597 0.07755298 0.06179519 0.05771552 0.05236874
 0.13602495 0.09856797 0.09594976 0.10701532 0.15187846], sum to 1.0000
[2019-04-09 14:42:11,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4144
[2019-04-09 14:42:11,177] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 24.57536616922989, 0.1541173696796467, 0.0, 1.0, 50.0, 38.85001741156439], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 799800.0000, 
sim time next is 800400.0000, 
raw observation next is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 19.0, 24.49923436012229, 0.1360252691315957, 0.0, 1.0, 45.0, 37.02126062577399], 
processed observation next is [1.0, 0.2608695652173913, 0.26592797783933514, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.5416028633435243, 0.5453417563771986, 0.0, 1.0, 0.6, 0.3702126062577399], 
reward next is 0.6298, 
noisyNet noise sample is [array([-0.26229364], dtype=float32), 0.3386853]. 
=============================================
[2019-04-09 14:42:11,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05521531 0.09681702 0.07803794 0.07634334 0.07972492 0.05063091
 0.126737   0.13028146 0.07249591 0.10895611 0.12476011], sum to 1.0000
[2019-04-09 14:42:11,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1489
[2019-04-09 14:42:11,278] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 24.65881429656396, 0.2667104777148154, 0.0, 1.0, 65.0, 59.91199343723996], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 774000.0000, 
sim time next is 774600.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 24.69975859903833, 0.2732851481553387, 0.0, 1.0, 20.0, 50.49846951231309], 
processed observation next is [1.0, 1.0, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5583132165865274, 0.5910950493851129, 0.0, 1.0, 0.1, 0.5049846951231309], 
reward next is 0.4950, 
noisyNet noise sample is [array([0.64481235], dtype=float32), 0.35270542]. 
=============================================
[2019-04-09 14:42:11,306] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06488577 0.08098056 0.08096786 0.05770114 0.0586114  0.05632081
 0.13829622 0.10875079 0.07708202 0.11760723 0.15879616], sum to 1.0000
[2019-04-09 14:42:11,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3742
[2019-04-09 14:42:11,330] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 19.0, 23.51114343000851, -0.07204057085468084, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 19.0, 23.31688868843474, -0.06674082491315632, 0.0, 1.0, 40.0, 40.62780132274825], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 0.08333333333333333, 0.44307405736956174, 0.47775305836228127, 0.0, 1.0, 0.5, 0.40627801322748247], 
reward next is 0.5937, 
noisyNet noise sample is [array([-1.1474241], dtype=float32), -1.0498935]. 
=============================================
[2019-04-09 14:42:11,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03804136 0.11734042 0.08395218 0.09220156 0.06007522 0.05840021
 0.11682718 0.10136276 0.10635781 0.07672139 0.14871998], sum to 1.0000
[2019-04-09 14:42:11,336] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0303
[2019-04-09 14:42:11,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06638989 0.08626262 0.07353413 0.05409634 0.06222105 0.05883136
 0.13591406 0.11302778 0.07558003 0.12507275 0.14906998], sum to 1.0000
[2019-04-09 14:42:11,345] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.899999999999999, 54.0, 0.0, 0.0, 22.5, 25.52003442658002, 0.405184259473426, 1.0, 1.0, 25.0, 32.61038420439692], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 758400.0000, 
sim time next is 759000.0000, 
raw observation next is [-3.9, 53.5, 0.0, 0.0, 22.5, 25.44200940992808, 0.3037699690521118, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.535, 0.0, 0.0, 0.375, 0.6201674508273399, 0.6012566563507039, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6112743], dtype=float32), 0.96068966]. 
=============================================
[2019-04-09 14:42:11,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7647
[2019-04-09 14:42:11,349] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.29973233]
 [-0.4028565 ]
 [-0.24112654]
 [-0.43187344]
 [-0.33922428]], R is [[0.20885405]
 [1.20676553]
 [1.84946895]
 [2.55138302]
 [3.23565745]].
[2019-04-09 14:42:11,359] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[ 0.0557646 ]
 [ 0.00566506]
 [ 0.00795169]
 [ 0.10725652]
 [-0.00645259]], R is [[0.97823048]
 [1.64234424]
 [2.28663015]
 [2.89459944]
 [3.42273998]].
[2019-04-09 14:42:11,362] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 23.03221818992814, -0.1639475759879004, 0.0, 1.0, 35.0, 35.59771476631031], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 802200.0000, 
sim time next is 802800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 19.0, 22.98812687110739, -0.1545732252910712, 0.0, 1.0, 60.0, 50.97938867429602], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.67, 0.0, 0.0, 0.08333333333333333, 0.41567723925894917, 0.44847559156964295, 0.0, 1.0, 0.9, 0.5097938867429602], 
reward next is 0.4902, 
noisyNet noise sample is [array([1.0893362], dtype=float32), 1.3190674]. 
=============================================
[2019-04-09 14:42:11,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06278624 0.09827267 0.0731297  0.06585261 0.06382649 0.05709616
 0.13192572 0.10350271 0.09942441 0.11172858 0.13245466], sum to 1.0000
[2019-04-09 14:42:11,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5379
[2019-04-09 14:42:11,689] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05922372 0.09043173 0.08387928 0.06467409 0.06399657 0.06500216
 0.11992767 0.10557861 0.10112242 0.10744271 0.13872112], sum to 1.0000
[2019-04-09 14:42:11,689] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4477
[2019-04-09 14:42:11,701] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 23.26380355825469, -0.1053057121790928, 0.0, 1.0, 50.0, 47.16703489771461], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [-7.299999999999999, 71.0, 0.0, 0.0, 19.0, 23.23199875306319, -0.1161194895659995, 0.0, 1.0, 45.0, 32.16502027279482], 
processed observation next is [1.0, 0.21739130434782608, 0.2603878116343491, 0.71, 0.0, 0.0, 0.08333333333333333, 0.4359998960885993, 0.46129350347800013, 0.0, 1.0, 0.6, 0.3216502027279482], 
reward next is 0.6783, 
noisyNet noise sample is [array([-0.61203206], dtype=float32), 1.0435072]. 
=============================================
[2019-04-09 14:42:11,702] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 74.0, 0.0, 0.0, 19.0, 23.21015585485047, -0.09481790106131271, 0.0, 1.0, 45.0, 32.44875157289599], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 788400.0000, 
sim time next is 789000.0000, 
raw observation next is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 19.0, 23.25010798797571, -0.1054144877362055, 0.0, 1.0, 45.0, 28.87827728942358], 
processed observation next is [1.0, 0.13043478260869565, 0.24884579870729456, 0.7416666666666667, 0.0, 0.0, 0.08333333333333333, 0.43750899899797585, 0.4648618374212648, 0.0, 1.0, 0.6, 0.2887827728942358], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.28497767], dtype=float32), 0.10757218]. 
=============================================
[2019-04-09 14:42:11,713] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[-0.23984765]
 [-0.2791707 ]
 [-0.36043882]
 [-0.26688278]
 [-0.3328066 ]], R is [[0.44052783]
 [1.11163509]
 [1.72592211]
 [2.3629756 ]
 [2.78678966]].
[2019-04-09 14:42:11,716] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04521189 0.125877   0.08542319 0.09810003 0.07242877 0.05285496
 0.09705786 0.10573588 0.11471983 0.07553364 0.12705699], sum to 1.0000
[2019-04-09 14:42:11,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4532
[2019-04-09 14:42:11,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 22.5, 24.38239763876492, 0.08853567995480721, 1.0, 1.0, 30.0, 45.24358460852473], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 812400.0000, 
sim time next is 813000.0000, 
raw observation next is [-6.2, 75.0, 69.66666666666666, 0.0, 22.5, 24.5088342358623, 0.1040342855567324, 1.0, 1.0, 40.0, 30.71674543449174], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.2322222222222222, 0.0, 0.375, 0.5424028529885249, 0.5346780951855775, 1.0, 1.0, 0.5, 0.30716745434491743], 
reward next is 0.6928, 
noisyNet noise sample is [array([0.20578381], dtype=float32), -0.9778477]. 
=============================================
[2019-04-09 14:42:11,776] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.11193764]
 [-0.1411188 ]
 [-0.13713978]
 [-0.1671048 ]
 [-0.15255204]], R is [[0.60459375]
 [1.14611197]
 [1.52181435]
 [2.14392304]
 [2.81589484]].
[2019-04-09 14:42:11,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03975189 0.12135544 0.07653432 0.10850608 0.06729502 0.04621049
 0.10587652 0.0888442  0.11796048 0.08737203 0.14029351], sum to 1.0000
[2019-04-09 14:42:11,866] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1363
[2019-04-09 14:42:11,877] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.35, 73.0, 87.0, 0.0, 22.5, 25.28838513308513, 0.2412059638609854, 1.0, 1.0, 20.0, 36.17642306381956], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 815400.0000, 
sim time next is 816000.0000, 
raw observation next is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.5, 25.25416239944312, 0.2376417435051953, 1.0, 1.0, 20.0, 34.1840280233726], 
processed observation next is [1.0, 0.43478260869565216, 0.32225300092336107, 0.7233333333333333, 0.30277777777777776, 0.0, 0.375, 0.6045135332869268, 0.5792139145017318, 1.0, 1.0, 0.1, 0.341840280233726], 
reward next is 0.6582, 
noisyNet noise sample is [array([0.41089946], dtype=float32), -0.13147418]. 
=============================================
[2019-04-09 14:42:11,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.09081365]
 [-0.13837425]
 [-0.06156597]
 [-0.12345856]
 [-0.08142371]], R is [[0.51133329]
 [1.14445567]
 [1.74897218]
 [2.32614183]
 [2.8822093 ]].
[2019-04-09 14:42:12,279] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05812366 0.0917317  0.0780157  0.06894159 0.06514704 0.05543712
 0.12518063 0.11086562 0.09696729 0.11217036 0.13741931], sum to 1.0000
[2019-04-09 14:42:12,281] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5756
[2019-04-09 14:42:12,303] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 23.31455125758099, -0.1239617635534359, 0.0, 1.0, 45.0, 35.40746395953168], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 799800.0000, 
sim time next is 800400.0000, 
raw observation next is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 19.0, 23.26903002886308, -0.137924649635308, 0.0, 1.0, 50.0, 35.12530531181953], 
processed observation next is [1.0, 0.2608695652173913, 0.26592797783933514, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.4390858357385901, 0.4540251167882307, 0.0, 1.0, 0.7, 0.3512530531181953], 
reward next is 0.6487, 
noisyNet noise sample is [array([-0.70290405], dtype=float32), -0.06970739]. 
=============================================
[2019-04-09 14:42:12,554] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04931454 0.1065961  0.07188582 0.08260712 0.06346885 0.05568253
 0.12403058 0.09522355 0.09948517 0.1011868  0.15051897], sum to 1.0000
[2019-04-09 14:42:12,556] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8460
[2019-04-09 14:42:12,589] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.7, 75.0, 16.0, 0.0, 22.5, 22.70625877740088, -0.2055703123916627, 1.0, 1.0, 65.0, 60.56374778631935], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 22.5, 22.84929517429392, -0.1690518703002875, 1.0, 1.0, 60.0, 59.11451162961104], 
processed observation next is [1.0, 0.34782608695652173, 0.2793167128347184, 0.75, 0.07111111111111112, 0.0, 0.375, 0.4041079311911601, 0.44364937656657083, 1.0, 1.0, 0.9, 0.5911451162961103], 
reward next is 0.4089, 
noisyNet noise sample is [array([1.6610265], dtype=float32), -0.5528117]. 
=============================================
[2019-04-09 14:42:12,602] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[-0.09281708]
 [-0.08122671]
 [-0.1473366 ]
 [-0.1681864 ]
 [-0.19725204]], R is [[0.35060668]
 [0.74146312]
 [1.34075058]
 [1.84646559]
 [2.4527843 ]].
[2019-04-09 14:42:12,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03655682 0.13551556 0.07665161 0.09012004 0.06661869 0.04923933
 0.10836089 0.09547281 0.11126121 0.07553548 0.15466756], sum to 1.0000
[2019-04-09 14:42:12,648] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1640
[2019-04-09 14:42:12,678] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 22.5, 25.81966075750176, 0.4403540059192655, 1.0, 1.0, 30.0, 36.65047536652133], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 831000.0000, 
sim time next is 831600.0000, 
raw observation next is [-3.9, 86.0, 54.0, 0.0, 22.5, 25.97043104808365, 0.3448987988259587, 1.0, 1.0, 20.0, 42.47226835731796], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.86, 0.18, 0.0, 0.375, 0.6642025873403042, 0.6149662662753196, 1.0, 1.0, 0.1, 0.4247226835731796], 
reward next is 0.5753, 
noisyNet noise sample is [array([-0.4706257], dtype=float32), 1.6071084]. 
=============================================
[2019-04-09 14:42:12,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05997363 0.09405778 0.07136707 0.06256501 0.06203648 0.05441758
 0.14320184 0.1053663  0.08247136 0.11952877 0.14501409], sum to 1.0000
[2019-04-09 14:42:12,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04414471 0.12447498 0.07908797 0.09646241 0.07332122 0.04807604
 0.11201825 0.0985883  0.10470726 0.09442872 0.12469018], sum to 1.0000
[2019-04-09 14:42:12,764] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3864
[2019-04-09 14:42:12,773] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5876
[2019-04-09 14:42:12,782] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 19.0, 23.96348196516601, 0.05286472517731159, 0.0, 1.0, 55.0, 41.73569498329125], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 789000.0000, 
sim time next is 789600.0000, 
raw observation next is [-7.633333333333333, 74.33333333333334, 0.0, 0.0, 19.0, 23.92473672750719, 0.04366401289421362, 0.0, 1.0, 40.0, 37.683691175808], 
processed observation next is [1.0, 0.13043478260869565, 0.2511542012927055, 0.7433333333333334, 0.0, 0.0, 0.08333333333333333, 0.4937280606255993, 0.5145546709647378, 0.0, 1.0, 0.5, 0.37683691175808], 
reward next is 0.6232, 
noisyNet noise sample is [array([-0.53367656], dtype=float32), 1.4395239]. 
=============================================
[2019-04-09 14:42:12,798] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 81.33333333333334, 64.33333333333333, 0.0, 22.5, 25.36122887505999, 0.2048594625992484, 1.0, 1.0, 35.0, 34.56762447879808], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 829200.0000, 
sim time next is 829800.0000, 
raw observation next is [-3.9, 82.5, 59.0, 0.0, 22.5, 24.94210405098713, 0.246295803693993, 1.0, 1.0, 50.0, 35.13116533815877], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.825, 0.19666666666666666, 0.0, 0.375, 0.5785086709155941, 0.5820986012313311, 1.0, 1.0, 0.7, 0.3513116533815877], 
reward next is 0.6487, 
noisyNet noise sample is [array([0.55819964], dtype=float32), 0.6217022]. 
=============================================
[2019-04-09 14:42:13,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04369593 0.11670085 0.08350569 0.0875246  0.06686704 0.05077091
 0.1130617  0.10072746 0.1168806  0.083942   0.13632321], sum to 1.0000
[2019-04-09 14:42:13,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1930
[2019-04-09 14:42:13,218] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.2, 75.0, 61.0, 0.0, 22.5, 25.27265959127693, 0.2641980964049052, 1.0, 1.0, 60.0, 48.5367717851335], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 811800.0000, 
sim time next is 812400.0000, 
raw observation next is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 22.5, 25.33921806120488, 0.2778776373828045, 1.0, 1.0, 25.0, 43.09407135002591], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.21777777777777776, 0.0, 0.375, 0.6116015051004066, 0.5926258791276015, 1.0, 1.0, 0.2, 0.4309407135002591], 
reward next is 0.5691, 
noisyNet noise sample is [array([-0.06952041], dtype=float32), -0.5657865]. 
=============================================
[2019-04-09 14:42:13,553] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04277905 0.11599042 0.08056022 0.09163287 0.07142745 0.05046491
 0.10795333 0.10669614 0.11137675 0.08497243 0.1361465 ], sum to 1.0000
[2019-04-09 14:42:13,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4869
[2019-04-09 14:42:13,577] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 22.5, 25.19398796564752, 0.3765226838436379, 1.0, 1.0, 65.0, 56.79481807797987], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 22.5, 25.44917921249969, 0.4107122023985968, 1.0, 1.0, 45.0, 43.44144820594831], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8266666666666667, 0.14111111111111113, 0.0, 0.375, 0.6207649343749742, 0.6369040674661989, 1.0, 1.0, 0.6, 0.43441448205948313], 
reward next is 0.5656, 
noisyNet noise sample is [array([0.50641143], dtype=float32), -0.7126334]. 
=============================================
[2019-04-09 14:42:13,773] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03450665 0.12236959 0.07642162 0.08933362 0.06186951 0.04588524
 0.11237568 0.09613587 0.11311301 0.08705142 0.16093777], sum to 1.0000
[2019-04-09 14:42:13,779] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0145
[2019-04-09 14:42:13,799] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.9, 85.33333333333334, 52.33333333333333, 0.0, 22.5, 25.77851612875771, 0.2845987903509835, 1.0, 1.0, 25.0, 38.12505322274219], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 832200.0000, 
sim time next is 832800.0000, 
raw observation next is [-3.9, 84.66666666666667, 50.66666666666666, 0.0, 22.5, 25.13310454607431, 0.2292878227160147, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8466666666666667, 0.16888888888888887, 0.0, 0.375, 0.5944253788395258, 0.5764292742386715, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8190634], dtype=float32), 0.18734084]. 
=============================================
[2019-04-09 14:42:14,303] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04657371 0.13349202 0.07830513 0.10171866 0.07968286 0.05165765
 0.1075974  0.10537149 0.10549499 0.07115985 0.11894625], sum to 1.0000
[2019-04-09 14:42:14,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2601
[2019-04-09 14:42:14,323] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 104.5, 0.0, 22.5, 25.20726410862865, 0.3095639639368354, 1.0, 1.0, 50.0, 41.68126078508654], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 820800.0000, 
sim time next is 821400.0000, 
raw observation next is [-4.5, 72.33333333333334, 102.6666666666667, 0.0, 22.5, 24.71650665734559, 0.2544949472502333, 1.0, 1.0, 30.0, 35.53504411493403], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7233333333333334, 0.3422222222222223, 0.0, 0.375, 0.5597088881121325, 0.5848316490834111, 1.0, 1.0, 0.3, 0.35535044114934033], 
reward next is 0.6446, 
noisyNet noise sample is [array([-0.9563896], dtype=float32), -0.0880222]. 
=============================================
[2019-04-09 14:42:14,407] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03517298 0.12197518 0.07492191 0.08557346 0.06405066 0.04628334
 0.11498573 0.10220667 0.10554013 0.09078556 0.15850434], sum to 1.0000
[2019-04-09 14:42:14,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9962
[2019-04-09 14:42:14,435] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.66666666666667, 35.66666666666666, 0.0, 22.5, 25.4530406247411, 0.4167334566440449, 1.0, 1.0, 45.0, 42.53566600660768], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 835800.0000, 
sim time next is 836400.0000, 
raw observation next is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 22.5, 25.69304373191778, 0.4507230053039166, 1.0, 1.0, 65.0, 59.65033588020436], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8333333333333335, 0.10777777777777776, 0.0, 0.375, 0.6410869776598149, 0.6502410017679722, 1.0, 1.0, 1.0, 0.5965033588020436], 
reward next is 0.4035, 
noisyNet noise sample is [array([0.54038686], dtype=float32), -0.67623407]. 
=============================================
[2019-04-09 14:42:14,558] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05142618 0.10199749 0.08481304 0.08918367 0.07216142 0.05814186
 0.10870867 0.09665924 0.09765983 0.10075673 0.13849191], sum to 1.0000
[2019-04-09 14:42:14,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-09 14:42:14,590] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 22.5, 23.18977476035025, -0.1201171324207858, 1.0, 1.0, 65.0, 68.04130963834089], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 807000.0000, 
sim time next is 807600.0000, 
raw observation next is [-6.533333333333334, 75.0, 26.66666666666667, 0.0, 22.5, 23.28403962047273, -0.04501812097770264, 1.0, 1.0, 45.0, 50.53435916428666], 
processed observation next is [1.0, 0.34782608695652173, 0.28162511542012925, 0.75, 0.0888888888888889, 0.0, 0.375, 0.4403366350393941, 0.4849939596740991, 1.0, 1.0, 0.6, 0.5053435916428666], 
reward next is 0.4947, 
noisyNet noise sample is [array([0.10848461], dtype=float32), -0.25610635]. 
=============================================
[2019-04-09 14:42:14,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05262266 0.09038655 0.07401292 0.06007932 0.06642001 0.05137062
 0.14154619 0.11997736 0.0705116  0.11797825 0.15509461], sum to 1.0000
[2019-04-09 14:42:14,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1968
[2019-04-09 14:42:14,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05463571 0.10597948 0.07923787 0.08918004 0.07585421 0.06172404
 0.10483729 0.10679519 0.09040771 0.08835117 0.14299734], sum to 1.0000
[2019-04-09 14:42:14,652] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 19.0, 23.95036211208476, 0.05742070813512393, 0.0, 1.0, 65.0, 55.39677169548995], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 867000.0000, 
sim time next is 867600.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 23.88115328238612, 0.06169169306676737, 0.0, 1.0, 25.0, 42.74637325107739], 
processed observation next is [1.0, 0.043478260869565216, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.4900961068655099, 0.5205638976889225, 0.0, 1.0, 0.2, 0.42746373251077385], 
reward next is 0.5725, 
noisyNet noise sample is [array([0.900335], dtype=float32), 1.7405978]. 
=============================================
[2019-04-09 14:42:14,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-09 14:42:14,688] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 22.5, 24.07121077352398, 0.0474902544570833, 1.0, 1.0, 45.0, 45.25592413821359], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 22.5, 24.42510955680684, 0.08268857428649344, 1.0, 1.0, 65.0, 57.26382905525931], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.375, 0.5354257964005701, 0.5275628580954977, 1.0, 1.0, 1.0, 0.5726382905525931], 
reward next is 0.4274, 
noisyNet noise sample is [array([0.10848461], dtype=float32), -0.25610635]. 
=============================================
[2019-04-09 14:42:15,343] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05323122 0.08948357 0.0725803  0.05561689 0.07120398 0.05010182
 0.13302188 0.13256468 0.07233819 0.10964932 0.16020815], sum to 1.0000
[2019-04-09 14:42:15,343] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4270
[2019-04-09 14:42:15,357] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 79.5, 0.0, 0.0, 19.0, 23.82138544822799, 0.07981558778867627, 0.0, 1.0, 45.0, 44.03683768878666], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 19.0, 23.85803296544304, 0.07968393413883439, 0.0, 1.0, 45.0, 36.86437456005931], 
processed observation next is [1.0, 1.0, 0.39427516158818104, 0.7966666666666665, 0.0, 0.0, 0.08333333333333333, 0.4881694137869201, 0.5265613113796115, 0.0, 1.0, 0.6, 0.3686437456005931], 
reward next is 0.6314, 
noisyNet noise sample is [array([1.864316], dtype=float32), -0.029355295]. 
=============================================
[2019-04-09 14:42:15,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06206337 0.08895763 0.08043315 0.06739024 0.07399166 0.05662477
 0.12657322 0.11190098 0.07337335 0.11176984 0.14692174], sum to 1.0000
[2019-04-09 14:42:15,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-09 14:42:15,593] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 24.5525794718711, 0.2485541750650082, 0.0, 1.0, 60.0, 49.35213086329395], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [-1.7, 79.00000000000001, 0.0, 0.0, 19.0, 24.7210216924556, 0.1785470815591479, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.7900000000000001, 0.0, 0.0, 0.08333333333333333, 0.5600851410379667, 0.5595156938530493, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1991706], dtype=float32), 0.7861803]. 
=============================================
[2019-04-09 14:42:15,705] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23287: loss 27.0435
[2019-04-09 14:42:15,706] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23287: learning rate 0.0000
[2019-04-09 14:42:15,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05788779 0.07963797 0.06811909 0.05982787 0.06494459 0.05527313
 0.13521826 0.10433473 0.09605234 0.12049954 0.15820466], sum to 1.0000
[2019-04-09 14:42:15,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-09 14:42:15,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05766483 0.08572731 0.06851102 0.05704164 0.05553774 0.0592679
 0.14230928 0.11119386 0.0883113  0.11778104 0.15665406], sum to 1.0000
[2019-04-09 14:42:15,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5146
[2019-04-09 14:42:15,796] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 19.0, 24.67329723306432, 0.1253564038827723, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 879600.0000, 
sim time next is 880200.0000, 
raw observation next is [-0.8999999999999999, 74.0, 0.0, 0.0, 19.0, 24.34592167411058, 0.1373248974612569, 0.0, 1.0, 20.0, 47.53386371774867], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.74, 0.0, 0.0, 0.08333333333333333, 0.5288268061758817, 0.545774965820419, 0.0, 1.0, 0.1, 0.47533863717748676], 
reward next is 0.5247, 
noisyNet noise sample is [array([-0.35740542], dtype=float32), 0.640869]. 
=============================================
[2019-04-09 14:42:15,802] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 19.0, 24.57002555756424, 0.1862083532734626, 0.0, 1.0, 45.0, 32.42992404440005], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 19.0, 24.55677742392641, 0.1720557931549179, 0.0, 1.0, 45.0, 30.78535234893183], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.08333333333333333, 0.5463981186605341, 0.5573519310516393, 0.0, 1.0, 0.6, 0.3078535234893183], 
reward next is 0.6921, 
noisyNet noise sample is [array([0.17776439], dtype=float32), -0.031282682]. 
=============================================
[2019-04-09 14:42:15,983] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23444: loss 21.8010
[2019-04-09 14:42:15,986] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23445: learning rate 0.0000
[2019-04-09 14:42:16,001] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23454: loss 35.7607
[2019-04-09 14:42:16,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23454: learning rate 0.0000
[2019-04-09 14:42:16,077] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23499: loss 23.4583
[2019-04-09 14:42:16,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23500: learning rate 0.0000
[2019-04-09 14:42:16,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0460665  0.10919788 0.07432702 0.08540561 0.06771828 0.04699275
 0.10652889 0.10472882 0.10668716 0.10354587 0.14880127], sum to 1.0000
[2019-04-09 14:42:16,140] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4665
[2019-04-09 14:42:16,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 22.5, 25.6731629260278, 0.3594565776872833, 1.0, 1.0, 25.0, 36.17860835669089], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [1.1, 82.0, 48.0, 0.0, 22.5, 25.65219852719471, 0.367456970407156, 1.0, 1.0, 45.0, 35.63761822771752], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.82, 0.16, 0.0, 0.375, 0.6376832105995591, 0.6224856568023853, 1.0, 1.0, 0.6, 0.35637618227717516], 
reward next is 0.6436, 
noisyNet noise sample is [array([0.48019612], dtype=float32), -0.37229565]. 
=============================================
[2019-04-09 14:42:16,296] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23609: loss 24.0746
[2019-04-09 14:42:16,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23613: learning rate 0.0000
[2019-04-09 14:42:16,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03752872 0.1378868  0.07009644 0.09157131 0.06853143 0.04549409
 0.11010411 0.10398063 0.12108153 0.08637784 0.12734705], sum to 1.0000
[2019-04-09 14:42:16,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8162
[2019-04-09 14:42:16,429] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.25, 95.0, 104.0, 0.0, 22.5, 25.47740980596277, 0.3772694639710517, 1.0, 1.0, 65.0, 58.92949490210285], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 909000.0000, 
sim time next is 909600.0000, 
raw observation next is [3.433333333333334, 94.33333333333334, 102.6666666666667, 0.0, 22.5, 25.55828436635079, 0.4101789822580058, 1.0, 1.0, 60.0, 53.66682932686525], 
processed observation next is [1.0, 0.5217391304347826, 0.5577100646352725, 0.9433333333333335, 0.3422222222222223, 0.0, 0.375, 0.6298570305292325, 0.6367263274193352, 1.0, 1.0, 0.9, 0.5366682932686525], 
reward next is 0.4633, 
noisyNet noise sample is [array([-0.56452864], dtype=float32), -0.012520098]. 
=============================================
[2019-04-09 14:42:16,610] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23789: loss 38.1905
[2019-04-09 14:42:16,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23790: learning rate 0.0000
[2019-04-09 14:42:16,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23834: loss 32.6156
[2019-04-09 14:42:16,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23834: learning rate 0.0000
[2019-04-09 14:42:16,774] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23878: loss 27.6135
[2019-04-09 14:42:16,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23878: learning rate 0.0000
[2019-04-09 14:42:16,788] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 23887: loss 22.1662
[2019-04-09 14:42:16,790] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 23887: learning rate 0.0000
[2019-04-09 14:42:16,819] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23907: loss 35.3328
[2019-04-09 14:42:16,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23907: learning rate 0.0000
[2019-04-09 14:42:16,889] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02945757 0.12393889 0.0749901  0.0925663  0.06632286 0.0487808
 0.10626771 0.10405887 0.11724554 0.08634551 0.15002592], sum to 1.0000
[2019-04-09 14:42:16,893] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5895
[2019-04-09 14:42:16,905] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.4, 93.0, 60.0, 0.0, 22.5, 26.55345170486537, 0.6121630957468216, 1.0, 1.0, 30.0, 29.98409667121805], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 919200.0000, 
sim time next is 919800.0000, 
raw observation next is [4.4, 93.0, 54.0, 0.0, 22.5, 26.60478731427872, 0.6159165863577022, 1.0, 1.0, 50.0, 30.31687982105403], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.18, 0.0, 0.375, 0.7170656095232267, 0.7053055287859008, 1.0, 1.0, 0.7, 0.3031687982105403], 
reward next is 0.6968, 
noisyNet noise sample is [array([-1.0990185], dtype=float32), -1.2078629]. 
=============================================
[2019-04-09 14:42:16,956] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06265954 0.0878173  0.08657426 0.06560043 0.07278362 0.04892177
 0.11983928 0.13313654 0.06688549 0.10635737 0.14942443], sum to 1.0000
[2019-04-09 14:42:16,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9789
[2019-04-09 14:42:16,978] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23991: loss 28.5920
[2019-04-09 14:42:16,980] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23992: learning rate 0.0000
[2019-04-09 14:42:16,984] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 24.67108827247454, 0.263570211633448, 0.0, 1.0, 50.0, 47.96094659416001], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [-2.55, 79.5, 0.0, 0.0, 19.0, 24.71530798218286, 0.267972378480321, 0.0, 1.0, 65.0, 54.64890458059491], 
processed observation next is [1.0, 1.0, 0.3919667590027701, 0.795, 0.0, 0.0, 0.08333333333333333, 0.5596089985152384, 0.589324126160107, 0.0, 1.0, 1.0, 0.546489045805949], 
reward next is 0.4535, 
noisyNet noise sample is [array([0.04312476], dtype=float32), -1.1425257]. 
=============================================
[2019-04-09 14:42:17,150] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24081: loss 24.5372
[2019-04-09 14:42:17,152] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24081: learning rate 0.0000
[2019-04-09 14:42:17,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02760291 0.1293627  0.07026236 0.10081337 0.06029397 0.0465058
 0.11658861 0.09711059 0.1128251  0.08110147 0.15753314], sum to 1.0000
[2019-04-09 14:42:17,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8417
[2019-04-09 14:42:17,559] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.9, 92.66666666666667, 0.0, 0.0, 22.5, 27.02784456951778, 0.7468776348384253, 1.0, 1.0, 60.0, 36.3269498019051], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 925800.0000, 
sim time next is 926400.0000, 
raw observation next is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 22.5, 27.085304927027, 0.6081746650630876, 1.0, 1.0, 60.0, 60.30359823412302], 
processed observation next is [1.0, 0.7391304347826086, 0.5955678670360112, 0.9333333333333335, 0.0, 0.0, 0.375, 0.7571087439189167, 0.7027248883543625, 1.0, 1.0, 0.9, 0.6030359823412302], 
reward next is 0.3970, 
noisyNet noise sample is [array([-1.9614567], dtype=float32), -0.28369963]. 
=============================================
[2019-04-09 14:42:17,612] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24333: loss 29.5583
[2019-04-09 14:42:17,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24333: learning rate 0.0000
[2019-04-09 14:42:17,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04390448 0.10786234 0.0698157  0.07609864 0.06205633 0.04961487
 0.12362835 0.13229917 0.09012115 0.10412095 0.14047804], sum to 1.0000
[2019-04-09 14:42:17,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2249
[2019-04-09 14:42:17,717] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 19.0, 25.55993327382122, 0.490966597337085, 0.0, 1.0, 20.0, 28.35158068989512], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 19.0, 25.47098706020451, 0.4711802787986403, 0.0, 1.0, 30.0, 27.08529731188923], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6225822550170426, 0.6570600929328801, 0.0, 1.0, 0.3, 0.27085297311889234], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.7032436], dtype=float32), 1.2223939]. 
=============================================
[2019-04-09 14:42:17,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04444276 0.10112791 0.07417598 0.07727019 0.06637616 0.04836012
 0.12903656 0.11956196 0.09121662 0.09829069 0.15014105], sum to 1.0000
[2019-04-09 14:42:17,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4504
[2019-04-09 14:42:17,771] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 19.0, 26.04551730298224, 0.6194807713787489, 0.0, 1.0, 60.0, 44.68448589621947], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 939600.0000, 
sim time next is 940200.0000, 
raw observation next is [5.0, 99.33333333333334, 0.0, 0.0, 19.0, 26.01618070238166, 0.6188635144676633, 0.0, 1.0, 45.0, 41.20485936242515], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9933333333333334, 0.0, 0.0, 0.08333333333333333, 0.6680150585318051, 0.7062878381558878, 0.0, 1.0, 0.6, 0.41204859362425145], 
reward next is 0.5880, 
noisyNet noise sample is [array([1.1109536], dtype=float32), 0.2901619]. 
=============================================
[2019-04-09 14:42:18,146] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24634: loss 28.0929
[2019-04-09 14:42:18,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24634: learning rate 0.0000
[2019-04-09 14:42:18,623] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04585114 0.10638217 0.0756102  0.08140421 0.06129406 0.05398702
 0.11380703 0.09802977 0.11350422 0.09200225 0.15812789], sum to 1.0000
[2019-04-09 14:42:18,629] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8273
[2019-04-09 14:42:18,663] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 22.5, 25.18854520454989, 0.2603119495991364, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 895200.0000, 
sim time next is 895800.0000, 
raw observation next is [0.9166666666666667, 78.66666666666667, 35.33333333333333, 0.0, 22.5, 25.26854540313871, 0.3103488460191843, 1.0, 1.0, 60.0, 64.3520074132207], 
processed observation next is [1.0, 0.34782608695652173, 0.48799630655586346, 0.7866666666666667, 0.11777777777777776, 0.0, 0.375, 0.6057121169282258, 0.6034496153397281, 1.0, 1.0, 0.9, 0.643520074132207], 
reward next is 0.3565, 
noisyNet noise sample is [array([0.19180064], dtype=float32), -0.69458276]. 
=============================================
[2019-04-09 14:42:18,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03945315 0.12496118 0.0784912  0.09221072 0.06954552 0.05084544
 0.10338134 0.104798   0.10610449 0.10210577 0.12810329], sum to 1.0000
[2019-04-09 14:42:18,749] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1144
[2019-04-09 14:42:18,775] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.8, 93.0, 94.0, 0.0, 22.5, 26.63628541098651, 0.5144355030097963, 1.0, 1.0, 35.0, 31.71860586820985], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 913800.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 22.5, 26.04514265968001, 0.5721638892907449, 1.0, 1.0, 20.0, 28.92594017302071], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 0.375, 0.6704285549733342, 0.6907212964302483, 1.0, 1.0, 0.1, 0.2892594017302071], 
reward next is 0.7107, 
noisyNet noise sample is [array([0.5447024], dtype=float32), -0.24101698]. 
=============================================
[2019-04-09 14:42:18,881] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 25052: loss 33.0583
[2019-04-09 14:42:18,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 25053: learning rate 0.0000
[2019-04-09 14:42:18,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03281702 0.1276951  0.07681063 0.08755586 0.06622697 0.04649375
 0.13234763 0.1030973  0.10829692 0.07660306 0.14205572], sum to 1.0000
[2019-04-09 14:42:18,914] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2565
[2019-04-09 14:42:18,941] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.2, 93.0, 84.0, 0.0, 22.5, 26.60790404718399, 0.6526888605343447, 1.0, 1.0, 45.0, 27.60039280956922], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 916800.0000, 
sim time next is 917400.0000, 
raw observation next is [4.3, 93.0, 78.0, 0.0, 22.5, 26.70638990835056, 0.6707622005111776, 1.0, 1.0, 30.0, 29.6250453952588], 
processed observation next is [1.0, 0.6086956521739131, 0.5817174515235458, 0.93, 0.26, 0.0, 0.375, 0.7255324923625466, 0.7235874001703926, 1.0, 1.0, 0.3, 0.29625045395258803], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.19427978], dtype=float32), 0.9841892]. 
=============================================
[2019-04-09 14:42:19,027] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 25139: loss 29.5051
[2019-04-09 14:42:19,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 25140: learning rate 0.0000
[2019-04-09 14:42:19,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03464615 0.12875319 0.08128228 0.09667615 0.0684169  0.04881127
 0.10895371 0.10914113 0.09833638 0.08380703 0.14117578], sum to 1.0000
[2019-04-09 14:42:19,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8369
[2019-04-09 14:42:19,198] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03739843 0.1378611  0.07169335 0.0813248  0.07033534 0.04948846
 0.12281805 0.1193056  0.09071139 0.0912229  0.12784052], sum to 1.0000
[2019-04-09 14:42:19,201] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.4, 99.33333333333334, 0.0, 0.0, 22.5, 26.50809644242813, 0.7129347278191472, 1.0, 1.0, 55.0, 33.18411144900696], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 931800.0000, 
sim time next is 932400.0000, 
raw observation next is [4.4, 100.0, 0.0, 0.0, 22.5, 26.52019605858961, 0.7146355612271517, 1.0, 1.0, 60.0, 41.06388183789255], 
processed observation next is [1.0, 0.8260869565217391, 0.5844875346260389, 1.0, 0.0, 0.0, 0.375, 0.710016338215801, 0.738211853742384, 1.0, 1.0, 0.9, 0.4106388183789255], 
reward next is 0.5894, 
noisyNet noise sample is [array([-0.45923668], dtype=float32), -0.68811387]. 
=============================================
[2019-04-09 14:42:19,202] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-09 14:42:19,226] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 98.66666666666667, 0.0, 0.0, 19.0, 26.04913246541079, 0.6099962511612406, 0.0, 1.0, 30.0, 31.22808694890423], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 940800.0000, 
sim time next is 941400.0000, 
raw observation next is [5.0, 98.0, 0.0, 0.0, 19.0, 25.98707573000933, 0.5935858235415584, 0.0, 1.0, 40.0, 29.66899470345169], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.98, 0.0, 0.0, 0.08333333333333333, 0.6655896441674441, 0.6978619411805195, 0.0, 1.0, 0.5, 0.2966899470345169], 
reward next is 0.7033, 
noisyNet noise sample is [array([0.30197227], dtype=float32), 0.48736262]. 
=============================================
[2019-04-09 14:42:19,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.06444281 0.08709214 0.07633981 0.0604708  0.06768814 0.0536206
 0.12786072 0.12052857 0.08078136 0.12895833 0.13221666], sum to 1.0000
[2019-04-09 14:42:19,700] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6073
[2019-04-09 14:42:19,719] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.883333333333334, 83.0, 0.0, 0.0, 19.0, 26.30609487655707, 0.6579610493329529, 0.0, 1.0, 30.0, 42.37947834215233], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 965400.0000, 
sim time next is 966000.0000, 
raw observation next is [8.066666666666666, 83.0, 0.0, 0.0, 19.0, 26.30779066892464, 0.65945203822429, 0.0, 1.0, 50.0, 34.52589683143231], 
processed observation next is [1.0, 0.17391304347826086, 0.6860572483841183, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6923158890770532, 0.7198173460747633, 0.0, 1.0, 0.7, 0.3452589683143231], 
reward next is 0.6547, 
noisyNet noise sample is [array([-0.74597967], dtype=float32), -1.6076806]. 
=============================================
[2019-04-09 14:42:19,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[-0.19921777]
 [-0.15447368]
 [-0.16256994]
 [-0.06774843]
 [-0.10069541]], R is [[0.51757276]
 [1.0886023 ]
 [1.54472589]
 [2.19213724]
 [2.80153608]].
[2019-04-09 14:42:19,746] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04961808 0.09311035 0.06883977 0.06547484 0.06049228 0.04814713
 0.12940863 0.11283859 0.10145453 0.12056116 0.15005466], sum to 1.0000
[2019-04-09 14:42:19,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7525
[2019-04-09 14:42:19,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05093283 0.09428488 0.07813929 0.07318576 0.06904947 0.05389398
 0.12663853 0.10592566 0.09218547 0.11308027 0.14268392], sum to 1.0000
[2019-04-09 14:42:19,775] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.25, 83.0, 0.0, 0.0, 19.0, 25.6258680076125, 0.5023206192463561, 0.0, 1.0, 50.0, 32.2618960104564], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 966600.0000, 
sim time next is 967200.0000, 
raw observation next is [8.433333333333334, 83.0, 0.0, 0.0, 19.0, 25.60485372725799, 0.5194674261985491, 0.0, 1.0, 60.0, 57.80737022479911], 
processed observation next is [1.0, 0.17391304347826086, 0.6962142197599263, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6337378106048325, 0.6731558087328496, 0.0, 1.0, 0.9, 0.578073702247991], 
reward next is 0.4219, 
noisyNet noise sample is [array([1.8158526], dtype=float32), -0.4411002]. 
=============================================
[2019-04-09 14:42:19,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-09 14:42:19,796] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.4, 83.0, 0.0, 0.0, 19.0, 26.60113879675591, 0.7066818697209887, 0.0, 1.0, 65.0, 44.26631726397514], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 973800.0000, 
sim time next is 974400.0000, 
raw observation next is [9.6, 83.0, 0.0, 0.0, 19.0, 26.60846373797181, 0.7064966724115966, 0.0, 1.0, 20.0, 39.06324484459676], 
processed observation next is [1.0, 0.2608695652173913, 0.7285318559556787, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7173719781643175, 0.7354988908038655, 0.0, 1.0, 0.1, 0.3906324484459676], 
reward next is 0.6094, 
noisyNet noise sample is [array([1.2361658], dtype=float32), -0.44323733]. 
=============================================
[2019-04-09 14:42:19,802] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05424325 0.1054032  0.07878279 0.06720548 0.06409389 0.05129292
 0.11832009 0.09749015 0.09407651 0.12086742 0.14822435], sum to 1.0000
[2019-04-09 14:42:19,802] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-09 14:42:19,812] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.7, 80.0, 0.0, 0.0, 19.0, 26.46939975148314, 0.6989380784212239, 0.0, 1.0, 45.0, 38.1854473404796], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 961200.0000, 
sim time next is 961800.0000, 
raw observation next is [7.7, 80.5, 0.0, 0.0, 19.0, 26.50162345474639, 0.6228850038335791, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.805, 0.0, 0.0, 0.08333333333333333, 0.7084686212288659, 0.707628334611193, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1605068], dtype=float32), -0.88958603]. 
=============================================
[2019-04-09 14:42:20,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05891626 0.09597512 0.07513874 0.0561512  0.0574537  0.04662516
 0.13163485 0.11979276 0.08499905 0.13144724 0.1418659 ], sum to 1.0000
[2019-04-09 14:42:20,167] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1631
[2019-04-09 14:42:20,184] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.7, 81.5, 0.0, 0.0, 19.0, 26.49560944985808, 0.6824675546075231, 0.0, 1.0, 20.0, 33.68685515123769], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 963000.0000, 
sim time next is 963600.0000, 
raw observation next is [7.699999999999999, 82.0, 0.0, 0.0, 19.0, 26.45793963409718, 0.6834591412314305, 0.0, 1.0, 25.0, 35.43121966737782], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7048283028414316, 0.7278197137438102, 0.0, 1.0, 0.2, 0.35431219667377817], 
reward next is 0.6457, 
noisyNet noise sample is [array([0.8985639], dtype=float32), 0.8335752]. 
=============================================
[2019-04-09 14:42:20,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05191635 0.10953184 0.06886064 0.07747623 0.06271335 0.05379109
 0.1115698  0.11112262 0.09230648 0.11112911 0.14958242], sum to 1.0000
[2019-04-09 14:42:20,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2452
[2019-04-09 14:42:20,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03643422 0.12601933 0.07719807 0.09875056 0.06962796 0.04915425
 0.10674414 0.10321778 0.10738429 0.0905422  0.13492717], sum to 1.0000
[2019-04-09 14:42:20,443] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0878
[2019-04-09 14:42:20,444] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 22.5, 26.91662562252698, 0.7890947900775095, 1.0, 1.0, 40.0, 24.46003835381007], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [9.8, 92.33333333333333, 32.5, 0.0, 22.5, 27.11261956595011, 0.8126944934141607, 1.0, 1.0, 30.0, 29.6416837755457], 
processed observation next is [1.0, 0.34782608695652173, 0.7340720221606649, 0.9233333333333333, 0.10833333333333334, 0.0, 0.375, 0.7593849638291758, 0.7708981644713869, 1.0, 1.0, 0.3, 0.296416837755457], 
reward next is 0.7036, 
noisyNet noise sample is [array([0.58355606], dtype=float32), 2.2775586]. 
=============================================
[2019-04-09 14:42:20,460] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.600000000000001, 100.0, 0.0, 0.0, 22.5, 25.42136944762573, 0.5249753080416368, 1.0, 1.0, 65.0, 90.80858045836602], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 933600.0000, 
sim time next is 934200.0000, 
raw observation next is [4.7, 100.0, 0.0, 0.0, 22.5, 25.40037513363949, 0.5469008349714705, 0.0, 1.0, 20.0, 45.95809721026558], 
processed observation next is [1.0, 0.8260869565217391, 0.592797783933518, 1.0, 0.0, 0.0, 0.375, 0.6166979278032908, 0.6823002783238236, 0.0, 1.0, 0.1, 0.4595809721026558], 
reward next is 0.5404, 
noisyNet noise sample is [array([-0.37361103], dtype=float32), -0.13671996]. 
=============================================
[2019-04-09 14:42:20,644] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03822098 0.12102412 0.07770309 0.08444749 0.06833287 0.04715085
 0.11359138 0.11188015 0.08515094 0.1021968  0.15030137], sum to 1.0000
[2019-04-09 14:42:20,645] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8046
[2019-04-09 14:42:20,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04801186 0.11245233 0.06544619 0.07186258 0.06674822 0.0517953
 0.11548879 0.10663357 0.10714514 0.09802394 0.15639207], sum to 1.0000
[2019-04-09 14:42:20,651] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0914
[2019-04-09 14:42:20,662] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.9, 100.0, 0.0, 0.0, 22.5, 25.48373732167906, 0.4893232710940864, 1.0, 1.0, 35.0, 32.69536644038158], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 935400.0000, 
sim time next is 936000.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 22.5, 25.47587515089468, 0.4961914315161012, 1.0, 1.0, 60.0, 45.14401592375793], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.375, 0.62298959590789, 0.6653971438387004, 1.0, 1.0, 0.9, 0.45144015923757924], 
reward next is 0.5486, 
noisyNet noise sample is [array([0.42647004], dtype=float32), 0.024708692]. 
=============================================
[2019-04-09 14:42:20,664] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 22.5, 26.3936876295256, 0.7377789932580964, 1.0, 1.0, 55.0, 40.82701428374067], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [9.8, 92.33333333333333, 32.5, 0.0, 22.5, 26.70597365213074, 0.7679713302044688, 1.0, 1.0, 20.0, 28.25923262410812], 
processed observation next is [1.0, 0.34782608695652173, 0.7340720221606649, 0.9233333333333333, 0.10833333333333334, 0.0, 0.375, 0.7254978043442284, 0.7559904434014896, 1.0, 1.0, 0.1, 0.2825923262410812], 
reward next is 0.7174, 
noisyNet noise sample is [array([-1.1316512], dtype=float32), 0.38325438]. 
=============================================
[2019-04-09 14:42:20,678] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.09235036]
 [0.00847983]
 [0.14100885]
 [0.07306197]
 [0.18564898]], R is [[0.69348985]
 [1.35960126]
 [1.98879409]
 [2.39121127]
 [3.36729908]].
[2019-04-09 14:42:20,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02653997 0.12705965 0.07531761 0.10005673 0.06632128 0.04370805
 0.11096845 0.09134751 0.10375527 0.08785166 0.1670739 ], sum to 1.0000
[2019-04-09 14:42:20,745] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0592
[2019-04-09 14:42:20,757] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.95, 78.0, 57.0, 0.0, 22.5, 28.16854188470371, 1.059585469084371, 1.0, 1.0, 60.0, 18.92117040192583], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1006200.0000, 
sim time next is 1006800.0000, 
raw observation next is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 22.5, 28.22958453271337, 1.066063875820702, 1.0, 1.0, 20.0, 14.84238996014432], 
processed observation next is [1.0, 0.6521739130434783, 0.8818097876269622, 0.77, 0.1722222222222222, 0.0, 0.375, 0.8524653777261143, 0.8553546252735673, 1.0, 1.0, 0.1, 0.1484238996014432], 
reward next is 0.8516, 
noisyNet noise sample is [array([1.394238], dtype=float32), 0.8384458]. 
=============================================
[2019-04-09 14:42:20,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04582274 0.11170379 0.07264657 0.07748966 0.07491668 0.05183055
 0.11971571 0.11073573 0.10500708 0.09443945 0.135692  ], sum to 1.0000
[2019-04-09 14:42:20,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9461
[2019-04-09 14:42:20,982] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.25, 92.5, 60.0, 0.0, 22.5, 27.53489940428776, 0.8965488653726751, 1.0, 1.0, 30.0, 24.00976923139319], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 984600.0000, 
sim time next is 985200.0000, 
raw observation next is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 22.5, 27.59497468827143, 0.9002901856705606, 1.0, 1.0, 65.0, 25.78388665199346], 
processed observation next is [1.0, 0.391304347826087, 0.7488457987072946, 0.9266666666666667, 0.22, 0.0, 0.375, 0.7995812240226193, 0.8000967285568535, 1.0, 1.0, 1.0, 0.2578388665199346], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.48886275], dtype=float32), -0.12535603]. 
=============================================
[2019-04-09 14:42:21,033] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04532961 0.09006472 0.08221461 0.05992967 0.05987442 0.05297792
 0.15395509 0.12189031 0.07583196 0.11312563 0.14480615], sum to 1.0000
[2019-04-09 14:42:21,034] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5476
[2019-04-09 14:42:21,056] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 25.85455132471802, 0.6011408133630451, 0.0, 1.0, 60.0, 47.68897795637221], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 945600.0000, 
sim time next is 946200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 25.89938592187603, 0.6038132466259489, 0.0, 1.0, 50.0, 39.62484123478523], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.6582821601563357, 0.7012710822086495, 0.0, 1.0, 0.7, 0.3962484123478523], 
reward next is 0.6038, 
noisyNet noise sample is [array([-1.4793203], dtype=float32), -0.21830234]. 
=============================================
[2019-04-09 14:42:21,501] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03795083 0.11176482 0.07831435 0.08910651 0.07349362 0.04344747
 0.11551103 0.11617848 0.10451635 0.09307589 0.13664064], sum to 1.0000
[2019-04-09 14:42:21,502] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-09 14:42:21,517] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.45, 86.0, 128.0, 0.0, 22.5, 28.05325548045496, 0.9979165933308277, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [12.53333333333333, 86.0, 126.5, 0.0, 22.5, 27.89512487667631, 1.03357327795995, 1.0, 1.0, 65.0, 57.72010981145038], 
processed observation next is [1.0, 0.5217391304347826, 0.8097876269621421, 0.86, 0.4216666666666667, 0.0, 0.375, 0.8245937397230257, 0.8445244259866499, 1.0, 1.0, 1.0, 0.5772010981145038], 
reward next is 0.4228, 
noisyNet noise sample is [array([0.13671601], dtype=float32), 2.3962085]. 
=============================================
[2019-04-09 14:42:21,529] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.08813937]
 [0.01717852]
 [0.12880857]
 [0.00334175]
 [0.11669149]], R is [[0.44283384]
 [1.43840551]
 [2.20606518]
 [2.97478127]
 [3.75492334]].
[2019-04-09 14:42:22,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03801852 0.12398323 0.07761295 0.08584602 0.0720422  0.05090787
 0.10323727 0.11585173 0.11276335 0.07947364 0.14026326], sum to 1.0000
[2019-04-09 14:42:22,261] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-09 14:42:22,282] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.4, 79.0, 0.0, 0.0, 22.5, 27.67026619838599, 1.02182847678123, 0.0, 1.0, 45.0, 26.94361191162941], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1020600.0000, 
sim time next is 1021200.0000, 
raw observation next is [14.4, 78.33333333333333, 0.0, 0.0, 22.5, 27.59454034909977, 1.017591767872166, 1.0, 1.0, 50.0, 26.70633325298198], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7833333333333333, 0.0, 0.0, 0.375, 0.7995450290916475, 0.8391972559573887, 1.0, 1.0, 0.7, 0.2670633325298198], 
reward next is 0.7329, 
noisyNet noise sample is [array([0.8660854], dtype=float32), 0.1480309]. 
=============================================
[2019-04-09 14:42:22,294] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03146084 0.13868654 0.08054402 0.10028243 0.06161786 0.04847345
 0.10983138 0.09872096 0.1127479  0.08600751 0.13162711], sum to 1.0000
[2019-04-09 14:42:22,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4354
[2019-04-09 14:42:22,313] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 22.5, 28.40816054611033, 1.101650873816823, 1.0, 1.0, 65.0, 12.36097478587106], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1006800.0000, 
sim time next is 1007400.0000, 
raw observation next is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 22.5, 28.43428311541989, 1.12411466748438, 1.0, 1.0, 40.0, 21.55067424843474], 
processed observation next is [1.0, 0.6521739130434783, 0.8868882733148662, 0.76, 0.15444444444444447, 0.0, 0.375, 0.8695235929516576, 0.87470488916146, 1.0, 1.0, 0.5, 0.2155067424843474], 
reward next is 0.7845, 
noisyNet noise sample is [array([1.7416118], dtype=float32), -0.009846265]. 
=============================================
[2019-04-09 14:42:22,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04978512 0.08915382 0.06888905 0.06056399 0.07077566 0.05796923
 0.11866742 0.10955937 0.10120221 0.11423664 0.15919752], sum to 1.0000
[2019-04-09 14:42:22,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1434
[2019-04-09 14:42:22,727] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.7018996303059, 1.035055272723407, 0.0, 1.0, 65.0, 23.91357782313606], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.76268541844304, 1.034852828187211, 0.0, 1.0, 25.0, 25.34418764924769], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8135571182035868, 0.8449509427290703, 0.0, 1.0, 0.2, 0.2534418764924769], 
reward next is 0.7466, 
noisyNet noise sample is [array([-1.6547413], dtype=float32), 1.9380736]. 
=============================================
[2019-04-09 14:42:23,072] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03020861 0.14484726 0.07712728 0.09666402 0.07224967 0.05349501
 0.11731552 0.10186674 0.09101527 0.08485388 0.13035679], sum to 1.0000
[2019-04-09 14:42:23,076] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2160
[2019-04-09 14:42:23,097] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [15.5, 76.0, 30.33333333333334, 0.0, 22.5, 26.71161675314063, 0.9899480793336478, 1.0, 1.0, 60.0, 0.1037883291792881], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1009200.0000, 
sim time next is 1009800.0000, 
raw observation next is [15.5, 76.5, 25.0, 0.0, 22.5, 28.16959576525553, 1.087456553147776, 1.0, 1.0, 25.0, 8.620562658771137], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.765, 0.08333333333333333, 0.0, 0.375, 0.8474663137712941, 0.8624855177159253, 1.0, 1.0, 0.2, 0.08620562658771137], 
reward next is 0.9138, 
noisyNet noise sample is [array([-0.2725474], dtype=float32), -0.9982616]. 
=============================================
[2019-04-09 14:42:23,102] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03552899 0.12112442 0.07376529 0.09365707 0.06776237 0.04638208
 0.10477477 0.10251433 0.12298699 0.09544189 0.13606174], sum to 1.0000
[2019-04-09 14:42:23,105] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5577
[2019-04-09 14:42:23,116] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [11.8, 86.0, 116.0, 0.0, 22.5, 27.85021006258879, 0.9871240641137184, 1.0, 1.0, 40.0, 21.18627651100407], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 991200.0000, 
sim time next is 991800.0000, 
raw observation next is [11.9, 86.0, 120.0, 0.0, 22.5, 27.88431434864226, 0.9913560510868352, 1.0, 1.0, 50.0, 19.4186951098683], 
processed observation next is [1.0, 0.4782608695652174, 0.7922437673130196, 0.86, 0.4, 0.0, 0.375, 0.823692862386855, 0.8304520170289451, 1.0, 1.0, 0.7, 0.194186951098683], 
reward next is 0.8058, 
noisyNet noise sample is [array([0.36220032], dtype=float32), 0.7108199]. 
=============================================
[2019-04-09 14:42:23,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04712642 0.09315816 0.07755482 0.07772727 0.0656329  0.05436939
 0.13421956 0.10308704 0.09236266 0.11302821 0.14173356], sum to 1.0000
[2019-04-09 14:42:23,333] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7258
[2019-04-09 14:42:23,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04645275 0.08782176 0.07824585 0.06179605 0.07436994 0.06438898
 0.12345397 0.11310319 0.07338218 0.1230117  0.1539736 ], sum to 1.0000
[2019-04-09 14:42:23,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9941
[2019-04-09 14:42:23,348] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 19.0, 27.650624003438, 1.03547447836976, 0.0, 1.0, 65.0, 25.22113005386493], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 27.68337904280884, 1.034550203721812, 0.0, 1.0, 55.0, 22.78770867263801], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.8069482535674034, 0.8448500679072707, 0.0, 1.0, 0.8, 0.2278770867263801], 
reward next is 0.7721, 
noisyNet noise sample is [array([-0.35715678], dtype=float32), -0.7907769]. 
=============================================
[2019-04-09 14:42:23,354] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.9, 77.5, 0.0, 0.0, 19.0, 27.60647928590947, 1.013615899670122, 0.0, 1.0, 45.0, 25.32528194811321], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1043400.0000, 
sim time next is 1044000.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 19.0, 27.59996742966882, 1.019496745884805, 0.0, 1.0, 65.0, 27.13574828246862], 
processed observation next is [1.0, 0.08695652173913043, 0.844875346260388, 0.78, 0.0, 0.0, 0.08333333333333333, 0.7999972858057349, 0.8398322486282682, 0.0, 1.0, 1.0, 0.2713574828246862], 
reward next is 0.7286, 
noisyNet noise sample is [array([0.4073391], dtype=float32), 1.0057057]. 
=============================================
[2019-04-09 14:42:23,380] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.14829423]
 [-0.1270108 ]
 [-0.17833838]
 [-0.25614128]
 [-0.21053809]], R is [[0.53667474]
 [1.27805519]
 [2.02531505]
 [2.76944852]
 [3.53514457]].
[2019-04-09 14:42:23,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04996895 0.10119548 0.07004865 0.0690047  0.0658269  0.0578556
 0.12405737 0.10558109 0.093221   0.10944016 0.15380003], sum to 1.0000
[2019-04-09 14:42:23,404] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5993
[2019-04-09 14:42:23,419] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.56666666666667, 82.0, 0.0, 0.0, 22.5, 27.71439136263573, 1.028574927962127, 1.0, 1.0, 20.0, 23.61922783824385], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1064400.0000, 
sim time next is 1065000.0000, 
raw observation next is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 22.5, 27.67739838622918, 1.042767669810732, 1.0, 1.0, 50.0, 24.86917454418434], 
processed observation next is [1.0, 0.30434782608695654, 0.8056325023084026, 0.825, 0.026666666666666665, 0.030570902394106807, 0.375, 0.8064498655190983, 0.8475892232702439, 1.0, 1.0, 0.7, 0.2486917454418434], 
reward next is 0.7513, 
noisyNet noise sample is [array([-0.8015156], dtype=float32), 2.6462016]. 
=============================================
[2019-04-09 14:42:23,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[ 0.12008838]
 [-0.06260241]
 [ 0.04423264]
 [ 0.05808783]
 [ 0.01163128]], R is [[0.81405187]
 [1.56971908]
 [2.36821294]
 [3.10532999]
 [3.79316378]].
[2019-04-09 14:42:23,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03852176 0.13405222 0.08139147 0.09731105 0.06314313 0.0529053
 0.10468941 0.09448202 0.12873839 0.08069944 0.12406575], sum to 1.0000
[2019-04-09 14:42:23,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3967
[2019-04-09 14:42:23,492] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.11666666666667, 80.5, 104.6666666666667, 156.0, 22.5, 28.47128427790194, 1.208562206944419, 1.0, 1.0, 20.0, 10.37368537612433], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [13.3, 80.0, 107.0, 117.0, 22.5, 28.56892257204692, 1.21815511034021, 1.0, 1.0, 35.0, 8.70960386403722], 
processed observation next is [1.0, 0.43478260869565216, 0.8310249307479226, 0.8, 0.3566666666666667, 0.1292817679558011, 0.375, 0.8807435476705766, 0.9060517034467367, 1.0, 1.0, 0.4, 0.0870960386403722], 
reward next is 0.9129, 
noisyNet noise sample is [array([0.64360523], dtype=float32), 1.1337017]. 
=============================================
[2019-04-09 14:42:24,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03380829 0.14961731 0.06933369 0.10940378 0.07078606 0.05661237
 0.10702112 0.10108565 0.09665928 0.07719892 0.1284736 ], sum to 1.0000
[2019-04-09 14:42:24,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2090
[2019-04-09 14:42:24,069] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.55, 55.0, 171.0, 0.0, 22.5, 29.18253462402289, 1.315060929262028, 1.0, 0.0, 65.0, 55.01369597368764], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1085400.0000, 
sim time next is 1086000.0000, 
raw observation next is [18.63333333333333, 54.66666666666667, 165.8333333333333, 0.0, 22.5, 28.83038217540047, 1.43994458219214, 1.0, 0.0, 25.0, 5.443315692930989], 
processed observation next is [1.0, 0.5652173913043478, 0.9787626962142197, 0.5466666666666667, 0.5527777777777776, 0.0, 0.375, 0.9025318479500392, 0.97998152739738, 1.0, 0.0, 0.2, 0.05443315692930989], 
reward next is 0.9456, 
noisyNet noise sample is [array([0.80598927], dtype=float32), -0.8018784]. 
=============================================
[2019-04-09 14:42:24,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[ 0.17889895]
 [-0.00024448]
 [ 0.09390898]
 [ 0.09439649]
 [ 0.17632909]], R is [[0.97012901]
 [1.41029084]
 [2.34732628]
 [3.32385302]
 [4.23648977]].
[2019-04-09 14:42:24,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04960838 0.08758198 0.0797022  0.0740395  0.06698361 0.06163185
 0.12488246 0.09973282 0.10152967 0.11162756 0.14268   ], sum to 1.0000
[2019-04-09 14:42:24,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2362
[2019-04-09 14:42:24,122] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 19.0, 27.55767482009563, 1.052999538861773, 0.0, 1.0, 60.0, 29.38920177088858], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1053000.0000, 
sim time next is 1053600.0000, 
raw observation next is [14.0, 77.66666666666667, 0.0, 0.0, 19.0, 27.62151235158587, 1.045750125803686, 0.0, 1.0, 30.0, 26.00191732624041], 
processed observation next is [1.0, 0.17391304347826086, 0.8504155124653741, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8017926959654892, 0.8485833752678954, 0.0, 1.0, 0.3, 0.2600191732624041], 
reward next is 0.7400, 
noisyNet noise sample is [array([1.424189], dtype=float32), 0.476942]. 
=============================================
[2019-04-09 14:42:24,335] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02961752 0.1373336  0.08127225 0.08763573 0.07116601 0.04534644
 0.11963876 0.08035    0.10600894 0.08131359 0.16031715], sum to 1.0000
[2019-04-09 14:42:24,339] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8362
[2019-04-09 14:42:24,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [19.2, 50.66666666666667, 132.0, 0.0, 22.5, 29.60007737727977, 1.464285020579194, 1.0, 0.0, 60.0, 1.344440115740649], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1089600.0000, 
sim time next is 1090200.0000, 
raw observation next is [19.3, 49.83333333333334, 124.0, 0.0, 22.5, 29.633916126303, 1.47061292404929, 1.0, 0.0, 60.0, 0.9278393248166747], 
processed observation next is [1.0, 0.6086956521739131, 0.9972299168975071, 0.4983333333333334, 0.41333333333333333, 0.0, 0.375, 0.9694930105252499, 0.99020430801643, 1.0, 0.0, 0.9, 0.009278393248166747], 
reward next is 0.9907, 
noisyNet noise sample is [array([-0.17610921], dtype=float32), -0.9714259]. 
=============================================
[2019-04-09 14:42:24,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05288837 0.09981452 0.08020999 0.07317824 0.06907559 0.05928373
 0.12128218 0.09633457 0.09544849 0.11326588 0.13921842], sum to 1.0000
[2019-04-09 14:42:24,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-09 14:42:24,511] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 22.5, 27.51563618511994, 1.061840243626527, 1.0, 1.0, 65.0, 25.36404596980874], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1063200.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 22.5, 27.66361210400704, 1.054272264025476, 1.0, 1.0, 25.0, 18.24240881477998], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.375, 0.8053010086672533, 0.8514240880084921, 1.0, 1.0, 0.2, 0.1824240881477998], 
reward next is 0.8176, 
noisyNet noise sample is [array([1.1910161], dtype=float32), 1.3802071]. 
=============================================
[2019-04-09 14:42:24,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03682426 0.12450802 0.07836614 0.08109525 0.06936361 0.05086916
 0.11316691 0.1278341  0.08854416 0.08954997 0.1398784 ], sum to 1.0000
[2019-04-09 14:42:24,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4459
[2019-04-09 14:42:24,606] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 22.5, 27.45515197119951, 1.010321401047821, 0.0, 1.0, 20.0, 27.87328396497597], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1022400.0000, 
sim time next is 1023000.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.43848624552205, 1.011722189218281, 1.0, 1.0, 45.0, 25.6908698800892], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7865405204601709, 0.837240729739427, 1.0, 1.0, 0.6, 0.256908698800892], 
reward next is 0.7431, 
noisyNet noise sample is [array([-1.0449128], dtype=float32), 0.14408739]. 
=============================================
[2019-04-09 14:42:24,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.19066687]
 [0.2073137 ]
 [0.2197649 ]
 [0.22588389]
 [0.24975006]], R is [[1.00110829]
 [1.71236444]
 [2.38416648]
 [2.89201832]
 [3.59065437]].
[2019-04-09 14:42:24,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03300299 0.15596375 0.08106295 0.11089868 0.06860582 0.04417316
 0.10998502 0.09666261 0.11153313 0.06586131 0.12225058], sum to 1.0000
[2019-04-09 14:42:24,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0426
[2019-04-09 14:42:24,749] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.6, 65.0, 217.5, 266.0, 22.5, 28.31902183105671, 1.417343820821835, 1.0, 0.0, 60.0, 15.1346232248912], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1080000.0000, 
sim time next is 1080600.0000, 
raw observation next is [16.88333333333334, 63.5, 205.3333333333333, 283.0, 22.5, 27.6651166073448, 1.308128639179575, 1.0, 0.0, 20.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9302862419205914, 0.635, 0.6844444444444443, 0.312707182320442, 0.375, 0.8054263839454, 0.936042879726525, 1.0, 0.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1554377], dtype=float32), -0.35457575]. 
=============================================
[2019-04-09 14:42:24,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03564064 0.1626123  0.07141049 0.1259539  0.07532421 0.04388977
 0.10302877 0.08931617 0.11085227 0.06428478 0.11768673], sum to 1.0000
[2019-04-09 14:42:24,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2304
[2019-04-09 14:42:24,913] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.55, 55.0, 171.0, 0.0, 22.5, 29.1548824716095, 1.387956157919181, 1.0, 0.0, 65.0, 1.670446020921908], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1085400.0000, 
sim time next is 1086000.0000, 
raw observation next is [18.63333333333333, 54.66666666666667, 165.8333333333333, 0.0, 22.5, 29.26238267164425, 1.401466429878492, 1.0, 0.0, 25.0, 1.772251938489453], 
processed observation next is [1.0, 0.5652173913043478, 0.9787626962142197, 0.5466666666666667, 0.5527777777777776, 0.0, 0.375, 0.9385318893036875, 0.967155476626164, 1.0, 0.0, 0.2, 0.017722519384894532], 
reward next is 0.9823, 
noisyNet noise sample is [array([-0.8659771], dtype=float32), 0.56543404]. 
=============================================
[2019-04-09 14:42:24,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.20680054]
 [0.10962133]
 [0.22126807]
 [0.09089993]
 [0.25088316]], R is [[1.29929066]
 [2.26959324]
 [3.2345593 ]
 [4.17473173]
 [5.10130024]].
[2019-04-09 14:42:25,300] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05930084 0.09339132 0.07430579 0.06263725 0.06812774 0.05933773
 0.12578966 0.13331765 0.07541589 0.11123233 0.13714372], sum to 1.0000
[2019-04-09 14:42:25,305] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8188
[2019-04-09 14:42:25,322] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 19.0, 27.57981047527559, 0.9716105753755456, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1059600.0000, 
sim time next is 1060200.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 19.0, 27.44395993905093, 1.012834234206466, 0.0, 1.0, 60.0, 45.63543935263765], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.08333333333333333, 0.7869966615875775, 0.8376114114021553, 0.0, 1.0, 0.9, 0.45635439352637647], 
reward next is 0.5436, 
noisyNet noise sample is [array([1.8139124], dtype=float32), -1.0312505]. 
=============================================
[2019-04-09 14:42:25,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04566742 0.09660957 0.08286596 0.06176078 0.06875771 0.06107808
 0.12617189 0.11325311 0.07472373 0.12019883 0.14891288], sum to 1.0000
[2019-04-09 14:42:25,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5793
[2019-04-09 14:42:25,417] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.3, 75.5, 0.0, 0.0, 19.0, 27.52595098553076, 1.029762180903256, 0.0, 1.0, 65.0, 38.64209117997061], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1041000.0000, 
sim time next is 1041600.0000, 
raw observation next is [14.2, 76.0, 0.0, 0.0, 19.0, 27.57969851846945, 1.030713059798964, 0.0, 1.0, 50.0, 29.23100274783629], 
processed observation next is [1.0, 0.043478260869565216, 0.8559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.7983082098724541, 0.843571019932988, 0.0, 1.0, 0.7, 0.2923100274783629], 
reward next is 0.7077, 
noisyNet noise sample is [array([0.4749228], dtype=float32), -0.076180294]. 
=============================================
[2019-04-09 14:42:25,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03542164 0.1477278  0.07695819 0.09497547 0.0774582  0.04361775
 0.11262087 0.10711511 0.09763309 0.08388293 0.12258895], sum to 1.0000
[2019-04-09 14:42:25,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-09 14:42:25,463] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 22.5, 29.45434352691683, 1.427961706684118, 1.0, 0.0, 20.0, 1.518714742759354], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [18.8, 54.0, 155.5, 0.0, 22.5, 29.4954344705892, 1.436204184053401, 1.0, 0.0, 50.0, 0.9427941563345276], 
processed observation next is [1.0, 0.6086956521739131, 0.9833795013850417, 0.54, 0.5183333333333333, 0.0, 0.375, 0.9579528725490999, 0.9787347280178004, 1.0, 0.0, 0.7, 0.009427941563345275], 
reward next is 0.9906, 
noisyNet noise sample is [array([0.9574737], dtype=float32), -1.3252225]. 
=============================================
[2019-04-09 14:42:25,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04561552 0.11063991 0.08033565 0.07592779 0.07519282 0.05530267
 0.11198787 0.09009711 0.10367057 0.11469298 0.13653705], sum to 1.0000
[2019-04-09 14:42:25,593] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-09 14:42:25,605] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.2, 83.0, 15.0, 48.33333333333334, 22.5, 27.70669890327759, 1.050787607555364, 1.0, 1.0, 40.0, 22.63669437506147], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1066200.0000, 
sim time next is 1066800.0000, 
raw observation next is [12.2, 83.0, 18.5, 58.66666666666666, 22.5, 27.73164680636217, 1.079834635127778, 1.0, 1.0, 35.0, 19.73254500527538], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.06166666666666667, 0.06482504604051564, 0.375, 0.8109705671968476, 0.8599448783759259, 1.0, 1.0, 0.4, 0.19732545005275381], 
reward next is 0.8027, 
noisyNet noise sample is [array([-1.1932709], dtype=float32), 0.1483176]. 
=============================================
[2019-04-09 14:42:25,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05160445 0.09849348 0.08049282 0.0601306  0.06786006 0.05235106
 0.13258426 0.10413372 0.09178096 0.12857622 0.13199235], sum to 1.0000
[2019-04-09 14:42:25,786] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2271
[2019-04-09 14:42:25,800] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.3, 77.16666666666667, 0.0, 0.0, 19.0, 27.68378793434214, 1.050573165151082, 0.0, 1.0, 55.0, 21.05299249382931], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1051800.0000, 
sim time next is 1052400.0000, 
raw observation next is [14.2, 77.33333333333334, 0.0, 0.0, 19.0, 27.65824173024432, 1.045644779212068, 0.0, 1.0, 25.0, 24.74330151132343], 
processed observation next is [1.0, 0.17391304347826086, 0.8559556786703602, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8048534775203601, 0.848548259737356, 0.0, 1.0, 0.2, 0.24743301511323432], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.06219134], dtype=float32), -0.61034334]. 
=============================================
[2019-04-09 14:42:25,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03274591 0.1330904  0.07540783 0.08772358 0.06903581 0.04751055
 0.10068861 0.10747378 0.11271632 0.08174919 0.15185799], sum to 1.0000
[2019-04-09 14:42:25,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7635
[2019-04-09 14:42:25,850] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [19.3, 49.83333333333334, 124.0, 0.0, 22.5, 29.6839855524237, 1.481414745231168, 1.0, 0.0, 60.0, 0.4335611246486018], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1090200.0000, 
sim time next is 1090800.0000, 
raw observation next is [19.4, 49.0, 116.0, 0.0, 22.5, 29.70527710538631, 1.475626425366191, 1.0, 0.0, 55.0, 1.302014830823811], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.38666666666666666, 0.0, 0.375, 0.9754397587821924, 0.9918754751220638, 1.0, 0.0, 0.8, 0.013020148308238111], 
reward next is 0.9870, 
noisyNet noise sample is [array([-2.7007022], dtype=float32), 0.8197997]. 
=============================================
[2019-04-09 14:42:26,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.07407857 0.09955069 0.06944739 0.08186103 0.08845505 0.06551905
 0.09996311 0.12381948 0.05551248 0.13034005 0.11145309], sum to 1.0000
[2019-04-09 14:42:26,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6977
[2019-04-09 14:42:26,095] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [10.18333333333333, 78.66666666666667, 0.0, 0.0, 19.0, 27.71307107016506, 1.136845428309287, 0.0, 1.0, 50.0, 25.09472843936455], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1131000.0000, 
sim time next is 1131600.0000, 
raw observation next is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 19.0, 27.70701935824064, 1.134878107800557, 0.0, 1.0, 50.0, 24.85896041598397], 
processed observation next is [0.0, 0.08695652173913043, 0.7497691597414591, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.8089182798533866, 0.8782927026001857, 0.0, 1.0, 0.7, 0.2485896041598397], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.40638363], dtype=float32), 1.2090185]. 
=============================================
[2019-04-09 14:42:26,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06708218 0.10032064 0.07444923 0.08015455 0.08839528 0.08760525
 0.10045793 0.09182481 0.06874324 0.12422317 0.11674383], sum to 1.0000
[2019-04-09 14:42:26,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8797
[2019-04-09 14:42:26,259] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.69854085348572, 1.126345427155961, 0.0, 1.0, 30.0, 24.82229658417832], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1135200.0000, 
sim time next is 1135800.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 27.70371588521734, 1.12422613346198, 0.0, 1.0, 60.0, 28.40543605989435], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8086429904347782, 0.8747420444873267, 0.0, 1.0, 0.9, 0.2840543605989435], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.42620474], dtype=float32), -1.1714488]. 
=============================================
[2019-04-09 14:42:26,328] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04024991 0.09885792 0.08147323 0.07192818 0.07129517 0.05193844
 0.13986361 0.12677471 0.06879154 0.09556843 0.15325885], sum to 1.0000
[2019-04-09 14:42:26,332] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5146
[2019-04-09 14:42:26,341] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.9, 68.5, 0.0, 0.0, 19.0, 27.89215857848308, 1.179683046006361, 0.0, 1.0, 60.0, 20.82521907083449], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1121400.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 19.0, 27.8851132664928, 1.178532801592175, 0.0, 1.0, 45.0, 22.54132813552582], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 0.08333333333333333, 0.8237594388744002, 0.8928442671973916, 0.0, 1.0, 0.6, 0.2254132813552582], 
reward next is 0.7746, 
noisyNet noise sample is [array([-1.2735761], dtype=float32), 2.384731]. 
=============================================
[2019-04-09 14:42:26,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.22962633]
 [0.01907927]
 [0.28008685]
 [0.1578359 ]
 [0.20169258]], R is [[0.82778049]
 [1.61125052]
 [2.3482573 ]
 [3.06978178]
 [3.82075763]].
[2019-04-09 14:42:26,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07127576 0.11693689 0.07600049 0.0848276  0.08394776 0.07701602
 0.09575017 0.0924021  0.07243982 0.12267281 0.10673051], sum to 1.0000
[2019-04-09 14:42:26,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8025
[2019-04-09 14:42:26,515] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [11.6, 82.0, 0.0, 0.0, 19.0, 27.68292578332623, 1.094049814841371, 0.0, 1.0, 25.0, 28.29015378767487], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1144200.0000, 
sim time next is 1144800.0000, 
raw observation next is [11.6, 83.0, 0.0, 0.0, 19.0, 27.67154839945786, 1.092210270537876, 0.0, 1.0, 60.0, 31.48031373714826], 
processed observation next is [0.0, 0.2608695652173913, 0.7839335180055402, 0.83, 0.0, 0.0, 0.08333333333333333, 0.8059623666214882, 0.864070090179292, 0.0, 1.0, 0.9, 0.31480313737148263], 
reward next is 0.6852, 
noisyNet noise sample is [array([0.7004632], dtype=float32), 0.23734868]. 
=============================================
[2019-04-09 14:42:26,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06089487 0.11130928 0.07638863 0.07918765 0.08855556 0.08273052
 0.1060643  0.09718032 0.07683907 0.11149134 0.10935851], sum to 1.0000
[2019-04-09 14:42:26,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8384
[2019-04-09 14:42:26,551] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 19.0, 27.61252975631039, 1.105070319784167, 0.0, 1.0, 60.0, 30.10632881208964], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [11.6, 80.0, 0.0, 0.0, 19.0, 27.63117119849439, 1.104074575070659, 0.0, 1.0, 60.0, 29.11655377044786], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.8, 0.0, 0.0, 0.08333333333333333, 0.8025975998745324, 0.8680248583568865, 0.0, 1.0, 0.9, 0.2911655377044786], 
reward next is 0.7088, 
noisyNet noise sample is [array([-1.2423657], dtype=float32), 0.23141947]. 
=============================================
[2019-04-09 14:42:26,570] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.1451214 ]
 [-0.17561688]
 [-0.20081705]
 [-0.27561945]
 [-0.16919895]], R is [[0.57194716]
 [1.26516438]
 [1.83023083]
 [2.81192851]
 [3.35849142]].
[2019-04-09 14:42:26,773] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0342425  0.13715744 0.0779702  0.09397859 0.07462155 0.04462027
 0.09577417 0.10073724 0.127688   0.07786317 0.13534693], sum to 1.0000
[2019-04-09 14:42:26,781] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9782
[2019-04-09 14:42:26,795] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.68333333333333, 69.16666666666667, 207.3333333333333, 143.3333333333333, 22.5, 28.8474650476069, 1.323873259454848, 1.0, 1.0, 35.0, 6.237138236105198], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1077000.0000, 
sim time next is 1077600.0000, 
raw observation next is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 22.5, 28.9212700377758, 1.347560543617734, 1.0, 1.0, 65.0, 5.267541784497134], 
processed observation next is [1.0, 0.4782608695652174, 0.9021237303785783, 0.6833333333333335, 0.7688888888888891, 0.19797421731123394, 0.375, 0.9101058364813165, 0.949186847872578, 1.0, 1.0, 1.0, 0.052675417844971345], 
reward next is 0.9473, 
noisyNet noise sample is [array([1.7227678], dtype=float32), 0.3855552]. 
=============================================
[2019-04-09 14:42:26,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06335809 0.11287863 0.07162844 0.07442585 0.08305923 0.07411882
 0.09443589 0.10134511 0.07713707 0.13199994 0.11561298], sum to 1.0000
[2019-04-09 14:42:26,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4969
[2019-04-09 14:42:26,854] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 19.0, 27.67908125318837, 1.095265939605167, 0.0, 1.0, 25.0, 24.11685238513918], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 19.0, 27.68793614580447, 1.114284581249155, 0.0, 1.0, 45.0, 23.40420439464245], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.08333333333333333, 0.8073280121503726, 0.8714281937497184, 0.0, 1.0, 0.6, 0.23404204394642447], 
reward next is 0.7660, 
noisyNet noise sample is [array([0.19431429], dtype=float32), 0.13337451]. 
=============================================
[2019-04-09 14:42:26,884] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-09 14:42:26,885] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:42:26,885] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:42:26,886] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:42:26,886] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:42:26,887] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:42:26,887] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:42:26,896] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run4
[2019-04-09 14:42:26,908] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run4
[2019-04-09 14:42:26,924] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run4
[2019-04-09 14:42:34,735] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00324841], dtype=float32), 0.0051612738]
[2019-04-09 14:42:34,736] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-8.218421501333333, 77.66427306666667, 0.0, 0.0, 19.0, 22.41312517048655, -0.2866438354000727, 0.0, 1.0, 15.0, 0.0]
[2019-04-09 14:42:34,737] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:42:34,738] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.05876108 0.10237598 0.07933537 0.07049712 0.07241099 0.0649659
 0.12292562 0.09443194 0.07327875 0.12186015 0.1391571 ], sampled 0.0860819748365913
[2019-04-09 14:42:36,062] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00324841], dtype=float32), 0.0051612738]
[2019-04-09 14:42:36,062] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-9.683333333333334, 69.5, 0.0, 0.0, 19.0, 22.34602579101892, -0.3308936195080361, 0.0, 1.0, 35.0, 57.76537766444778]
[2019-04-09 14:42:36,062] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:42:36,064] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.06527404 0.10684314 0.08052129 0.08510114 0.07688573 0.06833808
 0.10413539 0.08821496 0.09803871 0.11717289 0.10947455], sampled 0.9302425682219438
[2019-04-09 14:42:40,922] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00324841], dtype=float32), 0.0051612738]
[2019-04-09 14:42:40,922] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-10.3, 61.83333333333334, 152.6666666666667, 428.6666666666666, 19.0, 22.30442847390493, -0.3267746818658708, 0.0, 1.0, 40.0, 37.15082203936083]
[2019-04-09 14:42:40,922] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:42:40,923] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.0707867  0.11102743 0.08003348 0.08858149 0.08268534 0.06981073
 0.09263416 0.1023927  0.08595269 0.11361236 0.10248291], sampled 0.7056331619118275
[2019-04-09 14:43:34,633] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00324841], dtype=float32), 0.0051612738]
[2019-04-09 14:43:34,633] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [3.507212450333333, 25.00443165, 173.9610935666666, 798.6842886666666, 22.5, 27.84400992685116, 1.02874269822367, 1.0, 1.0, 35.0, 20.78262860866872]
[2019-04-09 14:43:34,633] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:43:34,634] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.05756621 0.12505496 0.09372234 0.09941739 0.07660855 0.07990936
 0.09404702 0.1105414  0.07685189 0.07803912 0.10824177], sampled 0.06899797775952465
[2019-04-09 14:43:36,600] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00324841], dtype=float32), 0.0051612738]
[2019-04-09 14:43:36,600] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [6.7, 56.66666666666666, 0.0, 0.0, 19.0, 27.21143639069957, 0.8640438649474119, 0.0, 1.0, 25.0, 22.4912568574691]
[2019-04-09 14:43:36,600] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:43:36,601] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.06533301 0.11392723 0.07173941 0.08091178 0.08230229 0.07272392
 0.10196193 0.1021531  0.07818002 0.12049402 0.11027325], sampled 0.5297334404259998
[2019-04-09 14:43:47,187] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5728.0181 276717.2925 2610.6685
[2019-04-09 14:43:47,207] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:47,207] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:47,207] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:47,207] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:47,317] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:47,317] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:47,317] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:47,317] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:53,395] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5417.5470 307668.0047 1675.0192
[2019-04-09 14:43:53,415] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:53,415] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:53,415] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:53,415] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:53,528] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:53,528] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:53,528] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:53,528] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:55,203] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5507.8563 298482.7389 2114.3695
[2019-04-09 14:43:55,223] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:55,223] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:55,223] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:55,223] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:43:55,339] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:55,339] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:55,339] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:55,339] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:43:56,226] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 30000, evaluation results [30000.0, 5507.856275398075, 298482.7388539056, 2114.3694657426868, 5728.018085166184, 276717.29245973664, 2610.6684528301525, 5417.546991693806, 307668.00467815856, 1675.0191769491262]
[2019-04-09 14:43:56,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05990776 0.09233008 0.06780349 0.06579585 0.08911624 0.05487228
 0.1241444  0.11601532 0.05719076 0.12436432 0.14845946], sum to 1.0000
[2019-04-09 14:43:56,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7696
[2019-04-09 14:43:56,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 27.78767429629495, 1.151649814456595, 0.0, 1.0, 60.0, 27.62354667481898], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 19.0, 27.77348609206096, 1.149043925326946, 0.0, 1.0, 60.0, 24.12763314429088], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8144571743384134, 0.8830146417756487, 0.0, 1.0, 0.9, 0.2412763314429088], 
reward next is 0.7587, 
noisyNet noise sample is [array([0.45105407], dtype=float32), -1.2150639]. 
=============================================
[2019-04-09 14:43:56,506] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.3408698 ]
 [-0.2076597 ]
 [-0.17069171]
 [-0.23047674]
 [-0.1197136 ]], R is [[0.52483183]
 [1.24334812]
 [1.91918957]
 [2.66183805]
 [3.41644335]].
[2019-04-09 14:43:56,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06257361 0.11535264 0.06713673 0.09288414 0.09353161 0.08473599
 0.098417   0.11069492 0.07350933 0.10806716 0.09309683], sum to 1.0000
[2019-04-09 14:43:56,653] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2122
[2019-04-09 14:43:56,666] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 19.0, 27.92408250219282, 1.159439336063868, 0.0, 0.0, 50.0, 20.22324747391911], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1159800.0000, 
sim time next is 1160400.0000, 
raw observation next is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 19.0, 27.93775300026983, 1.164170885811026, 0.0, 0.0, 25.0, 20.00019464725791], 
processed observation next is [0.0, 0.43478260869565216, 0.9492151431209604, 0.6633333333333334, 0.4072222222222223, 0.0, 0.08333333333333333, 0.8281460833558191, 0.8880569619370086, 0.0, 0.0, 0.2, 0.2000019464725791], 
reward next is 0.8000, 
noisyNet noise sample is [array([-0.09181283], dtype=float32), 0.13550761]. 
=============================================
[2019-04-09 14:43:56,754] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.06097095 0.11424586 0.07685768 0.09245714 0.09114919 0.0790373
 0.09839223 0.09390344 0.07787726 0.11790171 0.09720717], sum to 1.0000
[2019-04-09 14:43:56,758] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9341
[2019-04-09 14:43:56,772] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.6, 80.0, 0.0, 0.0, 19.0, 27.66156609302891, 1.11340212026627, 0.0, 1.0, 60.0, 29.85118577964165], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1143000.0000, 
sim time next is 1143600.0000, 
raw observation next is [11.6, 81.0, 0.0, 0.0, 19.0, 27.66770352706617, 1.113004461716961, 0.0, 1.0, 65.0, 29.54698559020817], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.81, 0.0, 0.0, 0.08333333333333333, 0.8056419605888475, 0.871001487238987, 0.0, 1.0, 1.0, 0.2954698559020817], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.9380532], dtype=float32), -1.4910145]. 
=============================================
[2019-04-09 14:43:57,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05865068 0.12307626 0.06564613 0.10219332 0.09152934 0.0710178
 0.09953837 0.11391918 0.06975773 0.10919829 0.09547285], sum to 1.0000
[2019-04-09 14:43:57,019] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9824
[2019-04-09 14:43:57,030] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 19.0, 27.85696788045128, 1.15378927588872, 0.0, 0.0, 20.0, 32.29623987068364], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1158600.0000, 
sim time next is 1159200.0000, 
raw observation next is [17.2, 67.0, 106.5, 0.0, 19.0, 27.86854166728996, 1.16446913424922, 0.0, 0.0, 65.0, 25.36202414584186], 
processed observation next is [0.0, 0.43478260869565216, 0.9390581717451525, 0.67, 0.355, 0.0, 0.08333333333333333, 0.8223784722741634, 0.8881563780830734, 0.0, 0.0, 1.0, 0.2536202414584186], 
reward next is 0.7464, 
noisyNet noise sample is [array([-1.7728689], dtype=float32), -0.8772006]. 
=============================================
[2019-04-09 14:43:57,044] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06750636 0.11578884 0.06918333 0.08372552 0.08863584 0.08507773
 0.09975071 0.09683184 0.06947062 0.12201279 0.10201638], sum to 1.0000
[2019-04-09 14:43:57,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4580
[2019-04-09 14:43:57,057] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.51666666666667, 80.5, 0.0, 0.0, 19.0, 27.6825064283069, 1.112058781835273, 0.0, 1.0, 55.0, 28.57322482723397], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1147800.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 19.0, 27.68432830694766, 1.112185794576821, 0.0, 1.0, 40.0, 25.52642742218173], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 0.08333333333333333, 0.8070273589123049, 0.8707285981922737, 0.0, 1.0, 0.5, 0.2552642742218173], 
reward next is 0.7447, 
noisyNet noise sample is [array([-0.9761819], dtype=float32), -0.6596024]. 
=============================================
[2019-04-09 14:43:57,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06600216 0.12292101 0.07010724 0.08614366 0.08619133 0.0775606
 0.09799784 0.09774538 0.07491611 0.12091018 0.0995044 ], sum to 1.0000
[2019-04-09 14:43:57,076] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8710
[2019-04-09 14:43:57,096] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [12.7, 80.0, 0.0, 0.0, 19.0, 27.68432830694766, 1.112185794576821, 0.0, 1.0, 40.0, 25.52642742218173], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1148400.0000, 
sim time next is 1149000.0000, 
raw observation next is [12.7, 80.66666666666667, 0.0, 0.0, 19.0, 27.68712198866218, 1.111766828926526, 0.0, 1.0, 60.0, 28.22511252009594], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8066666666666668, 0.0, 0.0, 0.08333333333333333, 0.8072601657218484, 0.8705889429755086, 0.0, 1.0, 0.9, 0.2822511252009594], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.9761819], dtype=float32), -0.6596024]. 
=============================================
[2019-04-09 14:43:57,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.18613362]
 [-0.18573347]
 [-0.21902132]
 [-0.2384584 ]
 [-0.2206671 ]], R is [[0.55109459]
 [1.29031944]
 [1.99168396]
 [2.72531366]
 [3.44860792]].
[2019-04-09 14:43:57,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0620411  0.12793352 0.07135064 0.10147736 0.0828499  0.06834098
 0.0927192  0.11609191 0.06105545 0.1109783  0.10516169], sum to 1.0000
[2019-04-09 14:43:57,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7765
[2019-04-09 14:43:57,344] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [18.71666666666667, 63.33333333333333, 63.00000000000001, 0.0, 19.0, 28.30969050224336, 1.253549462483368, 0.0, 0.0, 30.0, 15.64599174947131], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1180200.0000, 
sim time next is 1180800.0000, 
raw observation next is [18.8, 63.0, 54.5, 0.0, 19.0, 28.32100216895903, 1.251853551377835, 0.0, 0.0, 55.0, 15.73834072213184], 
processed observation next is [0.0, 0.6956521739130435, 0.9833795013850417, 0.63, 0.18166666666666667, 0.0, 0.08333333333333333, 0.8600835140799191, 0.917284517125945, 0.0, 0.0, 0.8, 0.15738340722131838], 
reward next is 0.8426, 
noisyNet noise sample is [array([-1.046268], dtype=float32), -0.5213401]. 
=============================================
[2019-04-09 14:43:57,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03343098 0.1338658  0.07943697 0.08308613 0.06848103 0.05077018
 0.11296102 0.10122716 0.10007048 0.08663704 0.15003324], sum to 1.0000
[2019-04-09 14:43:57,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4147
[2019-04-09 14:43:57,463] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 22.5, 28.66370132456401, 1.325762386754084, 1.0, 0.0, 35.0, 15.57580507929983], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1104000.0000, 
sim time next is 1104600.0000, 
raw observation next is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 22.5, 28.64752842764554, 1.313465351565518, 0.0, 0.0, 40.0, 11.98101049177715], 
processed observation next is [1.0, 0.782608695652174, 0.8831948291782087, 0.5633333333333332, 0.0, 0.0, 0.375, 0.8872940356371283, 0.9378217838551727, 0.0, 0.0, 0.5, 0.1198101049177715], 
reward next is 0.8802, 
noisyNet noise sample is [array([0.7333314], dtype=float32), -0.45861584]. 
=============================================
[2019-04-09 14:43:57,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05939231 0.12114746 0.07495166 0.10246036 0.09083133 0.07260899
 0.08999267 0.10198818 0.0697097  0.1109024  0.106015  ], sum to 1.0000
[2019-04-09 14:43:57,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6621
[2019-04-09 14:43:57,534] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.8, 63.0, 54.5, 0.0, 19.0, 28.35229414117551, 1.262114789220334, 0.0, 0.0, 50.0, 16.30847559109228], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1180800.0000, 
sim time next is 1181400.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 19.0, 28.34077553555633, 1.261806390918453, 0.0, 0.0, 50.0, 16.04864562072795], 
processed observation next is [0.0, 0.6956521739130435, 0.981071098799631, 0.6333333333333333, 0.15333333333333332, 0.0, 0.08333333333333333, 0.8617312946296941, 0.920602130306151, 0.0, 0.0, 0.7, 0.1604864562072795], 
reward next is 0.8395, 
noisyNet noise sample is [array([-0.11666318], dtype=float32), 0.28816986]. 
=============================================
[2019-04-09 14:43:57,587] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05733481 0.13324295 0.06737781 0.10518099 0.09574516 0.07165536
 0.09346981 0.10198261 0.07527807 0.10019862 0.09853372], sum to 1.0000
[2019-04-09 14:43:57,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4550
[2019-04-09 14:43:57,602] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.8, 63.0, 165.5, 0.0, 19.0, 28.13480220743425, 1.20924136603739, 0.0, 0.0, 65.0, 33.83312368433329], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1166400.0000, 
sim time next is 1167000.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 167.3333333333333, 0.0, 19.0, 28.14621548995285, 1.213129366011434, 0.0, 0.0, 35.0, 22.47225952645521], 
processed observation next is [0.0, 0.5217391304347826, 0.981071098799631, 0.6333333333333333, 0.5577777777777776, 0.0, 0.08333333333333333, 0.845517957496071, 0.9043764553371446, 0.0, 0.0, 0.4, 0.2247225952645521], 
reward next is 0.7753, 
noisyNet noise sample is [array([0.24458317], dtype=float32), -0.6520944]. 
=============================================
[2019-04-09 14:43:57,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[-0.06374834]
 [-0.10524705]
 [-0.0878987 ]
 [-0.16166493]
 [-0.17243075]], R is [[0.68901008]
 [1.34378874]
 [2.16109467]
 [2.96163821]
 [3.75236845]].
[2019-04-09 14:43:57,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05268926 0.09110541 0.07639241 0.06111732 0.06305646 0.05867194
 0.12486341 0.12266812 0.0873007  0.10785721 0.1542777 ], sum to 1.0000
[2019-04-09 14:43:57,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-09 14:43:57,884] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 19.0, 27.90916574189912, 1.167419419119127, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1117200.0000, 
sim time next is 1117800.0000, 
raw observation next is [12.45, 65.0, 0.0, 0.0, 19.0, 27.8436857273201, 1.18716167007779, 0.0, 1.0, 65.0, 47.91357492204308], 
processed observation next is [1.0, 0.9565217391304348, 0.8074792243767314, 0.65, 0.0, 0.0, 0.08333333333333333, 0.8203071439433417, 0.8957205566925968, 0.0, 1.0, 1.0, 0.47913574922043084], 
reward next is 0.5209, 
noisyNet noise sample is [array([-0.03490055], dtype=float32), -0.24512132]. 
=============================================
[2019-04-09 14:43:57,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03741126 0.10944784 0.07327601 0.07337325 0.0749508  0.04935899
 0.12635428 0.1232314  0.0825801  0.08789652 0.16211958], sum to 1.0000
[2019-04-09 14:43:57,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1634
[2019-04-09 14:43:57,911] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.8, 63.66666666666666, 0.0, 0.0, 19.0, 27.99090946143026, 1.214248828959494, 0.0, 1.0, 55.0, 18.69007337138671], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 19.0, 27.97759085745869, 1.210897469146719, 0.0, 1.0, 25.0, 21.471238420909], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 0.08333333333333333, 0.8314659047882241, 0.903632489715573, 0.0, 1.0, 0.2, 0.21471238420909], 
reward next is 0.7853, 
noisyNet noise sample is [array([-1.913625], dtype=float32), -0.6872643]. 
=============================================
[2019-04-09 14:43:57,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.34156996]
 [0.36851805]
 [0.39492315]
 [0.47300917]
 [0.5106247 ]], R is [[1.18634021]
 [1.98757613]
 [2.71701145]
 [3.44866943]
 [4.2058506 ]].
[2019-04-09 14:43:58,089] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31192: loss 37.9551
[2019-04-09 14:43:58,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31192: learning rate 0.0000
[2019-04-09 14:43:58,179] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06798036 0.12270864 0.07219031 0.08199666 0.08515362 0.07374538
 0.09872805 0.10133217 0.07688508 0.11723167 0.10204798], sum to 1.0000
[2019-04-09 14:43:58,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-09 14:43:58,196] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [11.6, 82.0, 0.0, 0.0, 19.0, 27.71023793198358, 1.106970832717242, 0.0, 1.0, 30.0, 25.00499596448241], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1144200.0000, 
sim time next is 1144800.0000, 
raw observation next is [11.6, 83.0, 0.0, 0.0, 19.0, 27.69818139437451, 1.105099668011718, 0.0, 1.0, 60.0, 28.93452366651854], 
processed observation next is [0.0, 0.2608695652173913, 0.7839335180055402, 0.83, 0.0, 0.0, 0.08333333333333333, 0.8081817828645427, 0.868366556003906, 0.0, 1.0, 0.9, 0.2893452366651854], 
reward next is 0.7107, 
noisyNet noise sample is [array([0.7078607], dtype=float32), -1.0562103]. 
=============================================
[2019-04-09 14:43:58,229] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31282: loss 36.5746
[2019-04-09 14:43:58,232] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31283: learning rate 0.0000
[2019-04-09 14:43:58,256] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31297: loss 36.1813
[2019-04-09 14:43:58,257] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31297: learning rate 0.0000
[2019-04-09 14:43:58,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0631878  0.13304743 0.07438029 0.09855016 0.09312832 0.07756259
 0.09199207 0.1011239  0.07378557 0.10243887 0.09080305], sum to 1.0000
[2019-04-09 14:43:58,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1577
[2019-04-09 14:43:58,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 19.0, 28.14767972671652, 1.219189932460273, 0.0, 0.0, 20.0, 21.73672333430816], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1167600.0000, 
sim time next is 1168200.0000, 
raw observation next is [18.55, 64.0, 171.0, 0.0, 19.0, 28.16134972157138, 1.225825786437354, 0.0, 0.0, 20.0, 14.48469120588413], 
processed observation next is [0.0, 0.5217391304347826, 0.976454293628809, 0.64, 0.57, 0.0, 0.08333333333333333, 0.8467791434642816, 0.908608595479118, 0.0, 0.0, 0.1, 0.14484691205884132], 
reward next is 0.8552, 
noisyNet noise sample is [array([-0.09600167], dtype=float32), -2.8456466]. 
=============================================
[2019-04-09 14:43:58,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05339104 0.10004315 0.05875481 0.07068852 0.06846345 0.06328207
 0.12973635 0.13200864 0.05447176 0.12739418 0.14176601], sum to 1.0000
[2019-04-09 14:43:58,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4620
[2019-04-09 14:43:58,436] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 77.0, 0.0, 0.0, 19.0, 27.79146975777813, 1.154808557844353, 0.0, 1.0, 55.0, 23.94081707748063], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1126800.0000, 
sim time next is 1127400.0000, 
raw observation next is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 27.7760871194712, 1.15156556318118, 0.0, 1.0, 45.0, 24.0065866784159], 
processed observation next is [0.0, 0.043478260869565216, 0.7511542012927056, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8146739266226, 0.8838551877270601, 0.0, 1.0, 0.6, 0.240065866784159], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.60284334], dtype=float32), -1.619122]. 
=============================================
[2019-04-09 14:43:58,495] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0554562  0.12390295 0.07007798 0.09049928 0.08344086 0.07145791
 0.1060375  0.09848226 0.06644259 0.09644001 0.1377625 ], sum to 1.0000
[2019-04-09 14:43:58,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8546
[2019-04-09 14:43:58,516] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 19.0, 28.23024375480243, 1.237928333243767, 0.0, 0.0, 65.0, 20.03334726324685], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 19.0, 28.22357123234421, 1.236084781937595, 0.0, 0.0, 60.0, 21.71683778580082], 
processed observation next is [0.0, 1.0, 0.922437673130194, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8519642693620174, 0.9120282606458651, 0.0, 0.0, 0.9, 0.2171683778580082], 
reward next is 0.7828, 
noisyNet noise sample is [array([-0.62746], dtype=float32), 2.5213354]. 
=============================================
[2019-04-09 14:43:58,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[ 0.02862379]
 [-0.08320552]
 [-0.14922228]
 [-0.14093958]
 [-0.11155078]], R is [[0.65600079]
 [1.44910729]
 [2.24828696]
 [3.03855753]
 [3.8247242 ]].
[2019-04-09 14:43:58,539] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31477: loss 33.9777
[2019-04-09 14:43:58,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06585145 0.10984011 0.07312059 0.07914237 0.09178926 0.07523473
 0.09484303 0.10186762 0.07576784 0.12053415 0.11200888], sum to 1.0000
[2019-04-09 14:43:58,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31480: learning rate 0.0000
[2019-04-09 14:43:58,546] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4751
[2019-04-09 14:43:58,555] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.7, 84.0, 16.0, 0.5, 19.0, 27.7270032702984, 1.105450779484183, 0.0, 1.0, 45.0, 24.24199418132847], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1152000.0000, 
sim time next is 1152600.0000, 
raw observation next is [13.16666666666667, 82.5, 21.0, 0.3333333333333333, 19.0, 27.71815794220061, 1.106927700922127, 0.0, 1.0, 40.0, 24.05074536753349], 
processed observation next is [0.0, 0.34782608695652173, 0.8273314866112651, 0.825, 0.07, 0.00036832412523020257, 0.08333333333333333, 0.809846495183384, 0.8689759003073757, 0.0, 1.0, 0.5, 0.2405074536753349], 
reward next is 0.7595, 
noisyNet noise sample is [array([-1.4212008], dtype=float32), -0.9564124]. 
=============================================
[2019-04-09 14:43:58,664] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31571: loss 39.8221
[2019-04-09 14:43:58,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31572: learning rate 0.0000
[2019-04-09 14:43:58,743] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05983498 0.11426527 0.07556802 0.08928116 0.08619737 0.07417479
 0.09645922 0.10297868 0.06815585 0.11806573 0.11501882], sum to 1.0000
[2019-04-09 14:43:58,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-09 14:43:58,758] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [18.1, 64.33333333333334, 0.0, 0.0, 19.0, 28.17157501635503, 1.246798804954239, 0.0, 0.0, 25.0, 18.40189020914027], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [18.0, 65.0, 0.0, 0.0, 19.0, 28.22987641636442, 1.24586175154421, 0.0, 0.0, 60.0, 18.43540252378497], 
processed observation next is [0.0, 0.782608695652174, 0.9612188365650972, 0.65, 0.0, 0.0, 0.08333333333333333, 0.8524897013637016, 0.9152872505147367, 0.0, 0.0, 0.9, 0.1843540252378497], 
reward next is 0.8156, 
noisyNet noise sample is [array([1.0540094], dtype=float32), 0.36216134]. 
=============================================
[2019-04-09 14:43:58,800] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06516649 0.12146603 0.07367969 0.09545796 0.08951692 0.07920996
 0.09457544 0.09713881 0.08015549 0.1056384  0.0979948 ], sum to 1.0000
[2019-04-09 14:43:58,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1728
[2019-04-09 14:43:58,815] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.63333333333333, 69.66666666666667, 90.83333333333333, 0.0, 19.0, 27.92194345533408, 1.148205459682997, 0.0, 0.0, 60.0, 24.25847108365206], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1158000.0000, 
sim time next is 1158600.0000, 
raw observation next is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 19.0, 27.92200340809605, 1.153010702968563, 0.0, 0.0, 20.0, 21.12976995616328], 
processed observation next is [0.0, 0.391304347826087, 0.9312096029547556, 0.6833333333333332, 0.32888888888888884, 0.0, 0.08333333333333333, 0.8268336173413374, 0.884336900989521, 0.0, 0.0, 0.1, 0.2112976995616328], 
reward next is 0.7887, 
noisyNet noise sample is [array([-0.9764447], dtype=float32), -2.2733202]. 
=============================================
[2019-04-09 14:43:58,853] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31692: loss 37.7292
[2019-04-09 14:43:58,854] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31693: learning rate 0.0000
[2019-04-09 14:43:58,963] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 31767: loss 38.0093
[2019-04-09 14:43:58,964] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 31767: learning rate 0.0000
[2019-04-09 14:43:59,001] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31786: loss 39.8579
[2019-04-09 14:43:59,004] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31788: learning rate 0.0000
[2019-04-09 14:43:59,123] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31863: loss 37.0286
[2019-04-09 14:43:59,126] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31864: learning rate 0.0000
[2019-04-09 14:43:59,182] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31899: loss 37.0303
[2019-04-09 14:43:59,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31902: learning rate 0.0000
[2019-04-09 14:43:59,457] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32080: loss 38.8039
[2019-04-09 14:43:59,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32080: learning rate 0.0000
[2019-04-09 14:43:59,553] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32143: loss 34.0355
[2019-04-09 14:43:59,554] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32144: learning rate 0.0000
[2019-04-09 14:43:59,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05819723 0.11598604 0.06839085 0.09904899 0.09128578 0.07048514
 0.1024237  0.11143498 0.059362   0.10787827 0.11550699], sum to 1.0000
[2019-04-09 14:43:59,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5311
[2019-04-09 14:43:59,569] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 19.0, 28.25718519959179, 1.240794597306778, 0.0, 0.0, 35.0, 18.42789978527944], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1200000.0000, 
sim time next is 1200600.0000, 
raw observation next is [17.15, 71.0, 0.0, 0.0, 19.0, 28.24571712811075, 1.248627452486302, 0.0, 0.0, 45.0, 18.08356031038167], 
processed observation next is [0.0, 0.9130434782608695, 0.9376731301939059, 0.71, 0.0, 0.0, 0.08333333333333333, 0.853809760675896, 0.9162091508287672, 0.0, 0.0, 0.6, 0.18083560310381672], 
reward next is 0.8192, 
noisyNet noise sample is [array([-1.8216071], dtype=float32), -2.0134463]. 
=============================================
[2019-04-09 14:43:59,768] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32285: loss 39.5625
[2019-04-09 14:43:59,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32285: learning rate 0.0000
[2019-04-09 14:43:59,829] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06675716 0.1192094  0.0714412  0.09146452 0.08982981 0.08081707
 0.09069283 0.1127479  0.06377946 0.10782687 0.10543378], sum to 1.0000
[2019-04-09 14:43:59,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-09 14:43:59,842] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 19.0, 27.97877582260629, 1.180572882964591, 0.0, 0.0, 20.0, 19.23725399297962], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1161600.0000, 
sim time next is 1162200.0000, 
raw observation next is [18.11666666666667, 65.33333333333334, 140.0, 0.0, 19.0, 28.03596372888953, 1.182955070032697, 0.0, 0.0, 60.0, 19.91805659239631], 
processed observation next is [0.0, 0.43478260869565216, 0.9644506001846724, 0.6533333333333334, 0.4666666666666667, 0.0, 0.08333333333333333, 0.8363303107407942, 0.8943183566775655, 0.0, 0.0, 0.9, 0.1991805659239631], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.6382547], dtype=float32), 0.33162066]. 
=============================================
[2019-04-09 14:43:59,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04713842 0.12978716 0.06772824 0.09951237 0.09084556 0.06136688
 0.10062177 0.09767715 0.07113534 0.11717667 0.11701045], sum to 1.0000
[2019-04-09 14:43:59,874] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6619
[2019-04-09 14:43:59,885] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.3, 65.0, 14.5, 0.0, 19.0, 28.319404020097, 1.257225179695498, 0.0, 0.0, 55.0, 16.3478493389693], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1184400.0000, 
sim time next is 1185000.0000, 
raw observation next is [18.3, 64.66666666666667, 9.666666666666664, 0.0, 19.0, 28.31454913912756, 1.256049830770835, 0.0, 0.0, 50.0, 16.3752679373341], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6466666666666667, 0.032222222222222215, 0.0, 0.08333333333333333, 0.8595457615939633, 0.9186832769236117, 0.0, 0.0, 0.7, 0.163752679373341], 
reward next is 0.8362, 
noisyNet noise sample is [array([-0.8740743], dtype=float32), -0.11233164]. 
=============================================
[2019-04-09 14:43:59,899] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[0.18703601]
 [0.04668018]
 [0.07972541]
 [0.11314639]
 [0.1926215 ]], R is [[0.9589237 ]
 [1.78585601]
 [2.62096834]
 [3.40800953]
 [4.18690729]].
[2019-04-09 14:44:00,198] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05617522 0.12194785 0.070705   0.10145254 0.08078207 0.07400425
 0.09825423 0.10675777 0.05956535 0.10276631 0.12758933], sum to 1.0000
[2019-04-09 14:44:00,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5533
[2019-04-09 14:44:00,211] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 78.0, 0.0, 0.0, 19.0, 28.19213245118359, 1.233889115708858, 0.0, 0.0, 40.0, 18.85538782493387], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [16.1, 78.33333333333333, 0.0, 0.0, 19.0, 28.18168379144777, 1.232942262338064, 0.0, 0.0, 45.0, 18.86262241133165], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7833333333333333, 0.0, 0.0, 0.08333333333333333, 0.8484736492873143, 0.9109807541126879, 0.0, 0.0, 0.6, 0.1886262241133165], 
reward next is 0.8114, 
noisyNet noise sample is [array([0.33600414], dtype=float32), 1.522514]. 
=============================================
[2019-04-09 14:44:00,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05782667 0.11670907 0.08559629 0.0785922  0.08366712 0.08396637
 0.08521383 0.09724093 0.07135504 0.12451796 0.11531455], sum to 1.0000
[2019-04-09 14:44:00,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2410
[2019-04-09 14:44:00,357] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.12474883621227, 1.223409621421125, 0.0, 0.0, 35.0, 19.28046658664152], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1220400.0000, 
sim time next is 1221000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.11963458027425, 1.227058025552002, 0.0, 0.0, 25.0, 19.79226410703777], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.843302881689521, 0.9090193418506672, 0.0, 0.0, 0.2, 0.19792264107037771], 
reward next is 0.8021, 
noisyNet noise sample is [array([1.4718089], dtype=float32), 0.39435452]. 
=============================================
[2019-04-09 14:44:00,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-0.01628809]
 [-0.19603974]
 [-0.19113624]
 [-0.14217009]
 [-0.20879908]], R is [[0.70557261]
 [1.50571227]
 [2.29360962]
 [3.07334661]
 [3.83911276]].
[2019-04-09 14:44:00,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0497122  0.11331566 0.07707189 0.08768518 0.09468074 0.06940862
 0.108941   0.092093   0.07557915 0.11071062 0.12080192], sum to 1.0000
[2019-04-09 14:44:00,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0478
[2019-04-09 14:44:00,410] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.4, 97.33333333333334, 96.0, 0.0, 19.0, 28.13976859426308, 1.267241632983267, 0.0, 1.0, 45.0, 16.91204306999922], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [14.4, 96.66666666666666, 97.0, 0.0, 19.0, 28.14099086646668, 1.241783435202248, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9666666666666666, 0.3233333333333333, 0.0, 0.08333333333333333, 0.8450825722055567, 0.9139278117340828, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.7238352], dtype=float32), 0.3728972]. 
=============================================
[2019-04-09 14:44:00,416] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05677418 0.12354531 0.08092613 0.09347661 0.08601854 0.07108305
 0.09906492 0.10359666 0.06897523 0.10232057 0.11421876], sum to 1.0000
[2019-04-09 14:44:00,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0706
[2019-04-09 14:44:00,434] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 19.0, 28.31785950343966, 1.259980090328028, 0.0, 0.0, 60.0, 18.32606422725572], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 19.0, 28.3128173367806, 1.266532103311169, 0.0, 0.0, 20.0, 17.82480007811502], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.08333333333333333, 0.8594014447317168, 0.9221773677703897, 0.0, 0.0, 0.1, 0.17824800078115022], 
reward next is 0.8218, 
noisyNet noise sample is [array([0.08047865], dtype=float32), -1.7986784]. 
=============================================
[2019-04-09 14:44:00,482] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32753: loss 38.2234
[2019-04-09 14:44:00,483] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32753: learning rate 0.0000
[2019-04-09 14:44:00,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0666969  0.10714477 0.07599847 0.08929233 0.0830416  0.06503408
 0.09434166 0.10102231 0.08041611 0.12508722 0.11192463], sum to 1.0000
[2019-04-09 14:44:00,573] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3582
[2019-04-09 14:44:00,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.5, 100.0, 83.66666666666667, 0.0, 19.0, 28.11658817963011, 1.230627281015112, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1248600.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 19.0, 28.07041228744617, 1.252289539433258, 0.0, 1.0, 35.0, 29.34663701213483], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.08333333333333333, 0.8392010239538475, 0.9174298464777527, 0.0, 1.0, 0.4, 0.2934663701213483], 
reward next is 0.7065, 
noisyNet noise sample is [array([0.55166674], dtype=float32), 0.30221996]. 
=============================================
[2019-04-09 14:44:00,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06560069 0.11129966 0.08090898 0.0774449  0.08834022 0.07775126
 0.10023643 0.09959321 0.07299808 0.11399131 0.11183521], sum to 1.0000
[2019-04-09 14:44:00,700] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5426
[2019-04-09 14:44:00,711] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.07557367269204, 1.220335800877543, 0.0, 0.0, 20.0, 25.2047040296455], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1228800.0000, 
sim time next is 1229400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.071842206732, 1.225403274238773, 0.0, 0.0, 45.0, 17.00507677113766], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8393201838943334, 0.908467758079591, 0.0, 0.0, 0.6, 0.1700507677113766], 
reward next is 0.8299, 
noisyNet noise sample is [array([0.8241898], dtype=float32), -0.10432017]. 
=============================================
[2019-04-09 14:44:01,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06423923 0.10617574 0.07360878 0.07137159 0.08640029 0.07176296
 0.09450538 0.10779587 0.06770538 0.13664827 0.11978646], sum to 1.0000
[2019-04-09 14:44:01,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-09 14:44:01,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.6, 100.0, 80.83333333333333, 0.0, 19.0, 28.10045949472752, 1.229347829983514, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [14.5, 100.0, 83.66666666666667, 0.0, 19.0, 28.05013498712488, 1.251079444992096, 0.0, 1.0, 25.0, 30.0028690203224], 
processed observation next is [0.0, 0.43478260869565216, 0.8642659279778394, 1.0, 0.2788888888888889, 0.0, 0.08333333333333333, 0.8375112489270734, 0.917026481664032, 0.0, 1.0, 0.2, 0.300028690203224], 
reward next is 0.7000, 
noisyNet noise sample is [array([0.39709282], dtype=float32), -2.1852093]. 
=============================================
[2019-04-09 14:44:01,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0481988  0.10942807 0.06900733 0.09259935 0.08795021 0.06927062
 0.10278514 0.10745792 0.07975366 0.12046368 0.11308517], sum to 1.0000
[2019-04-09 14:44:01,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6791
[2019-04-09 14:44:01,321] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [13.8, 100.0, 98.0, 0.0, 19.0, 28.1031786427199, 1.269225137136294, 0.0, 1.0, 50.0, 17.86836187348975], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1256400.0000, 
sim time next is 1257000.0000, 
raw observation next is [13.8, 100.0, 97.0, 0.0, 19.0, 28.10968824162473, 1.270815920444026, 0.0, 1.0, 50.0, 17.76784368551149], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.3233333333333333, 0.0, 0.08333333333333333, 0.8424740201353943, 0.9236053068146752, 0.0, 1.0, 0.7, 0.1776784368551149], 
reward next is 0.8223, 
noisyNet noise sample is [array([-0.9846235], dtype=float32), -0.3233708]. 
=============================================
[2019-04-09 14:44:01,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[ 0.02044755]
 [-0.03249578]
 [-0.15878022]
 [-0.17876625]
 [-0.00907938]], R is [[0.717677  ]
 [1.5318166 ]
 [2.33813906]
 [3.14438248]
 [3.92038774]].
[2019-04-09 14:44:01,342] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 33316: loss 36.2201
[2019-04-09 14:44:01,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 33316: learning rate 0.0000
[2019-04-09 14:44:01,347] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05088035 0.12075362 0.07283521 0.08790386 0.08568848 0.0634503
 0.10956816 0.1078909  0.0694212  0.11535924 0.1162487 ], sum to 1.0000
[2019-04-09 14:44:01,348] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2463
[2019-04-09 14:44:01,358] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 19.0, 28.13127934647861, 1.27307384046662, 0.0, 1.0, 65.0, 23.03544905190086], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [13.0, 100.0, 19.0, 0.0, 19.0, 28.12164608021168, 1.270218381615045, 0.0, 1.0, 30.0, 23.54213820374192], 
processed observation next is [0.0, 0.6956521739130435, 0.8227146814404434, 1.0, 0.06333333333333334, 0.0, 0.08333333333333333, 0.8434705066843066, 0.923406127205015, 0.0, 1.0, 0.3, 0.23542138203741922], 
reward next is 0.7646, 
noisyNet noise sample is [array([-0.03431486], dtype=float32), -0.008770053]. 
=============================================
[2019-04-09 14:44:01,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.09479085]
 [-0.08021211]
 [-0.04938712]
 [-0.15950984]
 [-0.08340219]], R is [[0.67760104]
 [1.44047058]
 [2.24559355]
 [3.0435791 ]
 [3.83336782]].
[2019-04-09 14:44:01,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05716468 0.10676153 0.07584704 0.07310658 0.08839745 0.06587743
 0.10896008 0.11152738 0.06074159 0.12995169 0.12166446], sum to 1.0000
[2019-04-09 14:44:01,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0439
[2019-04-09 14:44:01,464] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.016666666666667, 96.0, 0.0, 0.0, 19.0, 27.73896807416134, 1.195190908924633, 0.0, 1.0, 20.0, 22.73376077486065], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1278600.0000, 
sim time next is 1279200.0000, 
raw observation next is [6.833333333333334, 96.0, 0.0, 0.0, 19.0, 27.71994242553065, 1.147666674406608, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.651892890120037, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8099952021275542, 0.882555558135536, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49681747], dtype=float32), 0.064646974]. 
=============================================
[2019-04-09 14:44:01,476] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 33400: loss 37.7135
[2019-04-09 14:44:01,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 33403: learning rate 0.0000
[2019-04-09 14:44:01,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05558704 0.11844366 0.06536599 0.09165437 0.09025967 0.06991916
 0.09951124 0.11162055 0.06590205 0.1222831  0.10945314], sum to 1.0000
[2019-04-09 14:44:01,493] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0762
[2019-04-09 14:44:01,503] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 100.0, 97.0, 0.0, 19.0, 28.14280728998296, 1.276583076203667, 0.0, 1.0, 45.0, 16.34884710300219], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1257000.0000, 
sim time next is 1257600.0000, 
raw observation next is [13.8, 100.0, 96.0, 0.0, 19.0, 28.14210482743114, 1.277387712633168, 0.0, 1.0, 20.0, 17.51435475250308], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32, 0.0, 0.08333333333333333, 0.8451754022859284, 0.9257959042110561, 0.0, 1.0, 0.1, 0.1751435475250308], 
reward next is 0.8249, 
noisyNet noise sample is [array([-1.1428769], dtype=float32), 0.61624813]. 
=============================================
[2019-04-09 14:44:01,875] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06316879 0.10910251 0.08737567 0.08261652 0.08625819 0.08033152
 0.09676445 0.09249732 0.07705063 0.11784942 0.106985  ], sum to 1.0000
[2019-04-09 14:44:01,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2927
[2019-04-09 14:44:01,892] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.0997517781621, 1.225817895592135, 0.0, 0.0, 55.0, 18.65322761889815], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1233000.0000, 
sim time next is 1233600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.09644796543718, 1.225093227765475, 0.0, 0.0, 30.0, 19.74819358543792], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8413706637864315, 0.9083644092551584, 0.0, 0.0, 0.3, 0.1974819358543792], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.08276691], dtype=float32), -0.42163998]. 
=============================================
[2019-04-09 14:44:02,005] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06546685 0.11315865 0.0776085  0.07644064 0.08786764 0.0675598
 0.11891045 0.10772038 0.05921195 0.1101232  0.11593192], sum to 1.0000
[2019-04-09 14:44:02,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5214
[2019-04-09 14:44:02,027] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 19.0, 27.51436981939844, 1.109306356773483, 0.0, 1.0, 40.0, 33.72305857542354], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 19.0, 27.49550965219888, 1.104781176395887, 0.0, 1.0, 45.0, 27.6315023503367], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.08333333333333333, 0.7912924710165733, 0.8682603921319624, 0.0, 1.0, 0.6, 0.276315023503367], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.38773948], dtype=float32), 1.3093809]. 
=============================================
[2019-04-09 14:44:02,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05593027 0.13203542 0.06855685 0.08351412 0.07941565 0.06608625
 0.10774094 0.11838315 0.06718055 0.11483003 0.10632682], sum to 1.0000
[2019-04-09 14:44:02,053] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9371
[2019-04-09 14:44:02,064] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 100.0, 77.0, 0.0, 19.0, 28.14655481669053, 1.284509456034052, 0.0, 1.0, 20.0, 17.73579334581639], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [13.8, 100.0, 72.66666666666667, 0.0, 19.0, 28.18071282880465, 1.282488338787641, 0.0, 1.0, 65.0, 18.68668053500669], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.24222222222222223, 0.0, 0.08333333333333333, 0.848392735733721, 0.9274961129292136, 0.0, 1.0, 1.0, 0.18686680535006692], 
reward next is 0.8131, 
noisyNet noise sample is [array([-1.223726], dtype=float32), 0.22021423]. 
=============================================
[2019-04-09 14:44:02,134] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05738686 0.11635692 0.07291126 0.08414804 0.08979359 0.06100569
 0.11053154 0.11662196 0.06509847 0.11659737 0.10954833], sum to 1.0000
[2019-04-09 14:44:02,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3199
[2019-04-09 14:44:02,155] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 19.0, 27.60416187037604, 1.156732796288942, 0.0, 1.0, 40.0, 25.62753950906607], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1284600.0000, 
sim time next is 1285200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.58951605744519, 1.153232822344129, 0.0, 1.0, 30.0, 28.55704616267539], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7991263381204327, 0.8844109407813763, 0.0, 1.0, 0.3, 0.2855704616267539], 
reward next is 0.7144, 
noisyNet noise sample is [array([-1.1603472], dtype=float32), 0.41775393]. 
=============================================
[2019-04-09 14:44:02,596] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06962293 0.10997532 0.07752435 0.08929525 0.08881042 0.07892934
 0.09073019 0.0957496  0.07708237 0.10694175 0.11533842], sum to 1.0000
[2019-04-09 14:44:02,597] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7748
[2019-04-09 14:44:02,608] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.10842893424296, 1.224801503048819, 0.0, 0.0, 60.0, 21.47113874630176], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1230000.0000, 
sim time next is 1230600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.10491541752945, 1.223638928033906, 0.0, 0.0, 55.0, 21.50338923292778], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8420762847941209, 0.9078796426779686, 0.0, 0.0, 0.8, 0.2150338923292778], 
reward next is 0.7850, 
noisyNet noise sample is [array([0.4604978], dtype=float32), 0.18400156]. 
=============================================
[2019-04-09 14:44:02,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0478961  0.09009647 0.07630299 0.06196088 0.05137185 0.06139427
 0.11939812 0.0981345  0.10089567 0.1235524  0.16899672], sum to 1.0000
[2019-04-09 14:44:02,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9112
[2019-04-09 14:44:02,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05741962 0.12052535 0.08400767 0.08336666 0.09575377 0.06622413
 0.10201949 0.10392265 0.06612133 0.10906448 0.11157485], sum to 1.0000
[2019-04-09 14:44:02,712] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.8, 92.0, 0.0, 0.0, 19.0, 27.20405486861845, 1.027552930859519, 0.0, 1.0, 25.0, 24.64752040142551], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1306200.0000, 
sim time next is 1306800.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 19.0, 27.25284510056737, 1.034319749714806, 0.0, 1.0, 65.0, 52.4939364829685], 
processed observation next is [1.0, 0.13043478260869565, 0.5373961218836566, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7710704250472808, 0.8447732499049353, 0.0, 1.0, 1.0, 0.524939364829685], 
reward next is 0.4751, 
noisyNet noise sample is [array([1.127176], dtype=float32), 0.17236498]. 
=============================================
[2019-04-09 14:44:02,714] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3302
[2019-04-09 14:44:02,725] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 19.0, 27.50491542788927, 1.130401906787927, 0.0, 1.0, 60.0, 33.19009392471351], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.50326224256304, 1.13434747787735, 0.0, 1.0, 30.0, 32.28280550593996], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7919385202135866, 0.8781158259591167, 0.0, 1.0, 0.3, 0.3228280550593996], 
reward next is 0.6772, 
noisyNet noise sample is [array([-0.33357853], dtype=float32), 0.5861989]. 
=============================================
[2019-04-09 14:44:02,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[ 0.04254186]
 [-0.07503022]
 [-0.05739127]
 [-0.03874476]
 [-0.07316196]], R is [[0.55057442]
 [1.21316767]
 [1.86394644]
 [2.47668481]
 [3.16189384]].
[2019-04-09 14:44:02,844] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04907341 0.08884373 0.07697553 0.06393204 0.06096762 0.05773187
 0.12865925 0.09654702 0.10302518 0.11426865 0.15997577], sum to 1.0000
[2019-04-09 14:44:02,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2875
[2019-04-09 14:44:02,862] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 19.0, 27.24183634827146, 1.010146503108049, 0.0, 1.0, 20.0, 38.56572934130966], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1312200.0000, 
sim time next is 1312800.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 19.0, 27.2960434463555, 1.001892068032598, 0.0, 1.0, 35.0, 28.73133709083825], 
processed observation next is [1.0, 0.17391304347826086, 0.5124653739612189, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7746702871962917, 0.8339640226775327, 0.0, 1.0, 0.4, 0.2873133709083825], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.7300492], dtype=float32), 0.10040558]. 
=============================================
[2019-04-09 14:44:02,931] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06067892 0.10507944 0.07042356 0.08585686 0.09917668 0.07516524
 0.10352971 0.11118917 0.05931229 0.11784387 0.11174423], sum to 1.0000
[2019-04-09 14:44:02,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7871
[2019-04-09 14:44:02,944] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 19.0, 27.51466524045293, 1.148882867866478, 0.0, 1.0, 55.0, 28.48782136351547], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1284600.0000, 
sim time next is 1285200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.5312510801607, 1.147809578178155, 0.0, 1.0, 60.0, 33.35984295175329], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.794270923346725, 0.8826031927260516, 0.0, 1.0, 0.9, 0.33359842951753294], 
reward next is 0.6664, 
noisyNet noise sample is [array([0.9263108], dtype=float32), 1.9474313]. 
=============================================
[2019-04-09 14:44:03,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04379391 0.10290453 0.07618003 0.08747873 0.09436405 0.07152557
 0.11871572 0.09394111 0.05901406 0.12312174 0.12896055], sum to 1.0000
[2019-04-09 14:44:03,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1492
[2019-04-09 14:44:03,124] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 19.0, 27.39219094448623, 1.096802492173278, 0.0, 1.0, 60.0, 36.32724625010982], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 19.0, 27.45363480632279, 1.093932441313798, 0.0, 1.0, 25.0, 32.84052648286897], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 0.08333333333333333, 0.787802900526899, 0.8646441471045994, 0.0, 1.0, 0.2, 0.32840526482868965], 
reward next is 0.6716, 
noisyNet noise sample is [array([-0.75399643], dtype=float32), -1.2110187]. 
=============================================
[2019-04-09 14:44:03,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05441663 0.09688544 0.07917278 0.07253343 0.06633034 0.06412832
 0.11939772 0.09530216 0.07952034 0.1273753  0.14493753], sum to 1.0000
[2019-04-09 14:44:03,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6167
[2019-04-09 14:44:03,425] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04814521 0.08701313 0.08129831 0.06779712 0.06367128 0.05831055
 0.12593235 0.09174818 0.11144022 0.11509994 0.14954361], sum to 1.0000
[2019-04-09 14:44:03,428] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8969
[2019-04-09 14:44:03,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 19.0, 27.36169437848576, 1.056907609601852, 0.0, 1.0, 25.0, 30.39979906245581], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 19.0, 27.3512397715819, 1.055799446697321, 0.0, 1.0, 50.0, 31.97286117266414], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7792699809651582, 0.8519331488991071, 0.0, 1.0, 0.7, 0.3197286117266414], 
reward next is 0.6803, 
noisyNet noise sample is [array([-0.9937888], dtype=float32), 0.32242343]. 
=============================================
[2019-04-09 14:44:03,443] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.616666666666667, 92.0, 0.0, 0.0, 19.0, 27.44374581462509, 1.050975841416066, 0.0, 1.0, 40.0, 30.13971619543583], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1307400.0000, 
sim time next is 1308000.0000, 
raw observation next is [2.533333333333334, 92.0, 0.0, 0.0, 19.0, 27.41893596549394, 1.044357196144254, 0.0, 1.0, 65.0, 51.31730450002308], 
processed observation next is [1.0, 0.13043478260869565, 0.5327793167128348, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7849113304578283, 0.848119065381418, 0.0, 1.0, 1.0, 0.5131730450002308], 
reward next is 0.4868, 
noisyNet noise sample is [array([0.45487916], dtype=float32), -0.5466579]. 
=============================================
[2019-04-09 14:44:03,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04815069 0.09244394 0.08276315 0.05454746 0.06038015 0.05491752
 0.12393759 0.09226515 0.09847517 0.13001202 0.16210723], sum to 1.0000
[2019-04-09 14:44:03,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[ 0.0448525 ]
 [ 0.05469999]
 [-0.02444075]
 [ 0.07459193]
 [ 0.03271273]], R is [[0.66403252]
 [1.35339427]
 [2.04365444]
 [2.67990208]
 [3.07494235]].
[2019-04-09 14:44:03,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4761
[2019-04-09 14:44:03,466] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.8, 92.0, 0.0, 0.0, 19.0, 27.4028606827096, 0.9956915394075794, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1306200.0000, 
sim time next is 1306800.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 19.0, 27.28598503328178, 1.037054986545833, 0.0, 1.0, 45.0, 53.37660384568197], 
processed observation next is [1.0, 0.13043478260869565, 0.5373961218836566, 0.92, 0.0, 0.0, 0.08333333333333333, 0.773832086106815, 0.8456849955152776, 0.0, 1.0, 0.6, 0.5337660384568197], 
reward next is 0.4662, 
noisyNet noise sample is [array([0.37186894], dtype=float32), -2.2144237]. 
=============================================
[2019-04-09 14:44:03,468] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.1215543 ]
 [0.09674703]
 [0.09265868]
 [0.10233764]
 [0.05998515]], R is [[0.5959425 ]
 [1.2885859 ]
 [1.97431827]
 [2.6485703 ]
 [3.34641051]].
[2019-04-09 14:44:03,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04500526 0.08674906 0.07197239 0.06216831 0.05563023 0.05384767
 0.13506572 0.09722871 0.11073909 0.11570007 0.16589355], sum to 1.0000
[2019-04-09 14:44:03,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7621
[2019-04-09 14:44:03,609] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.533333333333334, 92.0, 0.0, 0.0, 19.0, 27.41698541692892, 1.043774941985823, 0.0, 1.0, 20.0, 32.58373447447775], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1308000.0000, 
sim time next is 1308600.0000, 
raw observation next is [2.45, 92.0, 0.0, 0.0, 19.0, 27.4028667439711, 1.050201285579961, 0.0, 1.0, 60.0, 38.28302095672284], 
processed observation next is [1.0, 0.13043478260869565, 0.5304709141274239, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7835722286642582, 0.8500670951933204, 0.0, 1.0, 0.9, 0.38283020956722835], 
reward next is 0.6172, 
noisyNet noise sample is [array([-0.18082345], dtype=float32), 1.8867338]. 
=============================================
[2019-04-09 14:44:03,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05043777 0.09089299 0.07814905 0.07648912 0.06010489 0.05151425
 0.12508771 0.09515392 0.09996224 0.11490677 0.1573013 ], sum to 1.0000
[2019-04-09 14:44:03,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04771454 0.10272421 0.07813618 0.06970666 0.05743471 0.05418138
 0.11867999 0.09562615 0.10970592 0.10982996 0.15626025], sum to 1.0000
[2019-04-09 14:44:03,632] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0345
[2019-04-09 14:44:03,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-09 14:44:03,639] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [2.0, 92.0, 0.0, 0.0, 19.0, 27.21291929903408, 1.008620844264746, 0.0, 1.0, 50.0, 34.94894563766754], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1311600.0000, 
sim time next is 1312200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 19.0, 27.26755200238997, 0.9488811778789022, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.515235457063712, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7722960001991641, 0.8162937259596341, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9653881], dtype=float32), -1.0109752]. 
=============================================
[2019-04-09 14:44:03,651] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333334, 92.0, 0.0, 0.0, 19.0, 27.31559381202149, 0.9883155220569435, 0.0, 1.0, 60.0, 39.60537961245598], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 19.0, 27.26778741731772, 0.9865675881959132, 0.0, 1.0, 45.0, 37.90503239776192], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 0.08333333333333333, 0.77231561810981, 0.8288558627319711, 0.0, 1.0, 0.6, 0.37905032397761923], 
reward next is 0.6209, 
noisyNet noise sample is [array([1.6167494], dtype=float32), 0.92021817]. 
=============================================
[2019-04-09 14:44:03,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05074398 0.09414004 0.0754526  0.05956273 0.05827812 0.05485133
 0.12944122 0.09368519 0.10410326 0.12198292 0.1577586 ], sum to 1.0000
[2019-04-09 14:44:03,680] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-09 14:44:03,695] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 19.0, 27.17871800733133, 0.961158872791096, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 19.0, 27.07624899528791, 1.000903924645689, 0.0, 1.0, 50.0, 56.34261865480102], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7563540829406591, 0.833634641548563, 0.0, 1.0, 0.7, 0.5634261865480102], 
reward next is 0.4366, 
noisyNet noise sample is [array([-0.18082345], dtype=float32), 1.8867338]. 
=============================================
[2019-04-09 14:44:03,702] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[0.1367312 ]
 [0.13883281]
 [0.17089748]
 [0.18777049]
 [0.22022653]], R is [[0.59531832]
 [1.58936512]
 [2.01457596]
 [2.99443007]
 [3.58165574]].
[2019-04-09 14:44:03,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03798608 0.10903037 0.07975961 0.08259206 0.06266677 0.05042378
 0.10198397 0.09426945 0.12423667 0.10298203 0.15406911], sum to 1.0000
[2019-04-09 14:44:03,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05596211 0.11351064 0.06722707 0.08497812 0.08432142 0.07126066
 0.09625454 0.11209746 0.07651822 0.12411639 0.11375339], sum to 1.0000
[2019-04-09 14:44:03,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-09 14:44:03,746] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7789
[2019-04-09 14:44:03,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 40.5, 0.0, 22.5, 27.64897009005363, 1.037354135269919, 1.0, 1.0, 35.0, 29.51742850551941], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1329600.0000, 
sim time next is 1330200.0000, 
raw observation next is [0.5, 92.0, 45.0, 0.0, 22.5, 27.76013645367995, 1.044001643558276, 1.0, 1.0, 30.0, 21.80419769312287], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.15, 0.0, 0.375, 0.8133447044733293, 0.8480005478527586, 1.0, 1.0, 0.3, 0.2180419769312287], 
reward next is 0.7820, 
noisyNet noise sample is [array([0.14396195], dtype=float32), -1.1248568]. 
=============================================
[2019-04-09 14:44:03,765] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [13.8, 100.0, 83.0, 0.0, 19.0, 28.15295842530267, 1.280092471626976, 0.0, 1.0, 50.0, 17.59257041347809], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1260600.0000, 
sim time next is 1261200.0000, 
raw observation next is [13.8, 100.0, 80.0, 0.0, 19.0, 28.14990201666649, 1.280118499291821, 0.0, 1.0, 60.0, 20.28198103572746], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.26666666666666666, 0.0, 0.08333333333333333, 0.845825168055541, 0.926706166430607, 0.0, 1.0, 0.9, 0.2028198103572746], 
reward next is 0.7972, 
noisyNet noise sample is [array([-0.90669715], dtype=float32), 0.89811206]. 
=============================================
[2019-04-09 14:44:03,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03095752 0.1394874  0.08024076 0.09652835 0.05802688 0.04512877
 0.09531707 0.09741245 0.14227633 0.07703222 0.13759229], sum to 1.0000
[2019-04-09 14:44:03,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0004
[2019-04-09 14:44:03,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04479424 0.09727601 0.07848325 0.06318225 0.05895878 0.04785037
 0.13186327 0.09736209 0.09953967 0.10732099 0.17336908], sum to 1.0000
[2019-04-09 14:44:03,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-09 14:44:03,921] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 27.45862497885058, 0.9911139183206212, 0.0, 1.0, 65.0, 40.3351517223636], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1317600.0000, 
sim time next is 1318200.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 19.0, 27.36586170017812, 0.9878286299830736, 0.0, 1.0, 20.0, 36.2196313736545], 
processed observation next is [1.0, 0.2608695652173913, 0.5046168051708219, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7804884750148432, 0.8292762099943579, 0.0, 1.0, 0.1, 0.362196313736545], 
reward next is 0.6378, 
noisyNet noise sample is [array([1.2083346], dtype=float32), 0.15970679]. 
=============================================
[2019-04-09 14:44:03,922] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 22.5, 27.98229661965581, 1.081424487146427, 1.0, 1.0, 50.0, 23.71758301985733], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 22.5, 28.01556491537195, 1.053132686663619, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.375, 0.8346304096143292, 0.851044228887873, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94939363], dtype=float32), -0.32386822]. 
=============================================
[2019-04-09 14:44:03,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[0.42581862]
 [0.3754676 ]
 [0.3093514 ]
 [0.31416547]
 [0.32175332]], R is [[1.35538638]
 [2.1046567 ]
 [2.84432507]
 [3.58846807]
 [4.31267691]].
[2019-04-09 14:44:04,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02999837 0.11171805 0.08181285 0.09998465 0.06033147 0.04199502
 0.10031673 0.09376811 0.13849212 0.0953027  0.14627989], sum to 1.0000
[2019-04-09 14:44:04,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8819
[2019-04-09 14:44:04,042] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 122.8333333333333, 0.0, 22.5, 28.0545170097537, 1.102233053760817, 1.0, 1.0, 60.0, 27.68458883712838], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1336800.0000, 
sim time next is 1337400.0000, 
raw observation next is [1.1, 92.0, 127.0, 0.0, 22.5, 28.07275802771833, 1.105635509412732, 1.0, 1.0, 65.0, 25.40998645558967], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.42333333333333334, 0.0, 0.375, 0.8393965023098607, 0.868545169804244, 1.0, 1.0, 1.0, 0.2540998645558967], 
reward next is 0.7459, 
noisyNet noise sample is [array([1.523894], dtype=float32), -0.12782404]. 
=============================================
[2019-04-09 14:44:04,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02917838 0.12726769 0.08101648 0.09432068 0.05954625 0.04699068
 0.10325582 0.09258725 0.1322154  0.08623939 0.147382  ], sum to 1.0000
[2019-04-09 14:44:04,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5638
[2019-04-09 14:44:04,081] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 124.6666666666667, 0.0, 22.5, 28.03659107175647, 1.093353298256389, 1.0, 1.0, 45.0, 24.28772141090166], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [1.1, 92.0, 122.3333333333333, 0.0, 22.5, 28.01187462354771, 1.100828950913256, 1.0, 1.0, 50.0, 24.0270144887682], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4077777777777777, 0.0, 0.375, 0.8343228852956424, 0.866942983637752, 1.0, 1.0, 0.7, 0.24027014488768197], 
reward next is 0.7597, 
noisyNet noise sample is [array([0.69474673], dtype=float32), 0.41015354]. 
=============================================
[2019-04-09 14:44:04,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05175289 0.0811131  0.07700721 0.05410152 0.05974291 0.05748696
 0.14187583 0.1058317  0.08716464 0.1219473  0.16197594], sum to 1.0000
[2019-04-09 14:44:04,201] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7440
[2019-04-09 14:44:04,222] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 19.0, 27.28881449005537, 1.012486879229953, 0.0, 1.0, 20.0, 34.87335810018338], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1313400.0000, 
sim time next is 1314000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 27.33731685199533, 1.024280261529362, 0.0, 1.0, 60.0, 35.28074427863091], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7781097376662774, 0.8414267538431206, 0.0, 1.0, 0.9, 0.3528074427863091], 
reward next is 0.6472, 
noisyNet noise sample is [array([-0.6151319], dtype=float32), -2.4404762]. 
=============================================
[2019-04-09 14:44:04,231] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[-0.00919639]
 [ 0.08425815]
 [ 0.05563386]
 [-0.0427473 ]
 [-0.08862885]], R is [[0.73090851]
 [1.37486577]
 [2.0448494 ]
 [2.70076323]
 [3.35880542]].
[2019-04-09 14:44:04,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04808487 0.1014021  0.07902557 0.08262936 0.0634536  0.05152026
 0.10648264 0.09311984 0.11340757 0.09614404 0.1647301 ], sum to 1.0000
[2019-04-09 14:44:04,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9973
[2019-04-09 14:44:04,302] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 22.5, 27.41834595428328, 1.003200957696425, 1.0, 1.0, 45.0, 25.75185695598424], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 22.5, 27.64585673656364, 1.011316500237735, 1.0, 1.0, 65.0, 26.71391252719749], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.375, 0.8038213947136367, 0.8371055000792449, 1.0, 1.0, 1.0, 0.2671391252719749], 
reward next is 0.7329, 
noisyNet noise sample is [array([0.09922697], dtype=float32), -0.2646149]. 
=============================================
[2019-04-09 14:44:04,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02982565 0.12285152 0.08538602 0.08621101 0.06326991 0.04378083
 0.1099839  0.08861835 0.1421323  0.09044732 0.13749315], sum to 1.0000
[2019-04-09 14:44:04,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1203
[2019-04-09 14:44:04,469] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 94.33333333333333, 0.0, 22.5, 28.04978109671804, 1.126587263260822, 1.0, 1.0, 45.0, 22.07660846312182], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1345800.0000, 
sim time next is 1346400.0000, 
raw observation next is [1.1, 92.0, 88.5, 0.0, 22.5, 28.09366177048553, 1.129116705620619, 1.0, 1.0, 20.0, 21.41602661556726], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.295, 0.0, 0.375, 0.8411384808737941, 0.876372235206873, 1.0, 1.0, 0.1, 0.21416026615567263], 
reward next is 0.7858, 
noisyNet noise sample is [array([1.915716], dtype=float32), 0.090755865]. 
=============================================
[2019-04-09 14:44:04,700] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03041131 0.12245037 0.08078098 0.09012043 0.06027769 0.04643372
 0.09796361 0.0918313  0.14018913 0.09001247 0.14952901], sum to 1.0000
[2019-04-09 14:44:04,702] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-09 14:44:04,725] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 114.5, 0.0, 22.5, 28.0360811521252, 1.09469737780436, 1.0, 1.0, 30.0, 21.78108527684857], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1335600.0000, 
sim time next is 1336200.0000, 
raw observation next is [1.1, 92.0, 118.6666666666667, 0.0, 22.5, 28.06521396254318, 1.098951447507476, 1.0, 1.0, 65.0, 42.07693119340443], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.39555555555555566, 0.0, 0.375, 0.8387678302119316, 0.8663171491691587, 1.0, 1.0, 1.0, 0.42076931193404427], 
reward next is 0.5792, 
noisyNet noise sample is [array([0.43991196], dtype=float32), -0.23077083]. 
=============================================
[2019-04-09 14:44:05,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03076524 0.1160676  0.08410025 0.09368762 0.06206544 0.0392966
 0.11478287 0.09926994 0.12453176 0.081378   0.15405472], sum to 1.0000
[2019-04-09 14:44:05,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0333
[2019-04-09 14:44:05,072] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 122.8333333333333, 0.0, 22.5, 28.12055368418652, 1.075336464978123, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1336800.0000, 
sim time next is 1337400.0000, 
raw observation next is [1.1, 92.0, 127.0, 0.0, 22.5, 28.0696579128172, 1.101526277802425, 1.0, 1.0, 20.0, 36.61002911057442], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.42333333333333334, 0.0, 0.375, 0.8391381594014332, 0.8671754259341418, 1.0, 1.0, 0.1, 0.3661002911057442], 
reward next is 0.6339, 
noisyNet noise sample is [array([-0.87819767], dtype=float32), 0.12501235]. 
=============================================
[2019-04-09 14:44:05,250] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03076033 0.10912982 0.07580969 0.08760519 0.05667621 0.04120919
 0.10861687 0.10315255 0.12402648 0.08561926 0.17739448], sum to 1.0000
[2019-04-09 14:44:05,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0801
[2019-04-09 14:44:05,272] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 96.0, 5.999999999999998, 0.0, 22.5, 26.00686407882126, 0.9499025252095276, 1.0, 1.0, 40.0, 13.46900360850802], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1357800.0000, 
sim time next is 1358400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.51638151238156, 1.007939566903736, 1.0, 1.0, 20.0, 35.86898205553201], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7930317926984634, 0.8359798556345787, 1.0, 1.0, 0.1, 0.35868982055532006], 
reward next is 0.6413, 
noisyNet noise sample is [array([0.46793243], dtype=float32), -1.7014731]. 
=============================================
[2019-04-09 14:44:05,318] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02689323 0.12189182 0.07483054 0.078554   0.05638136 0.04024179
 0.11247166 0.09817165 0.12747079 0.09290106 0.17019206], sum to 1.0000
[2019-04-09 14:44:05,322] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9406
[2019-04-09 14:44:05,334] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 62.0, 0.0, 22.5, 28.1138984866779, 1.119763646083408, 1.0, 1.0, 20.0, 22.47924827107261], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [1.1, 92.0, 57.5, 0.0, 22.5, 28.03971996101789, 1.123881367463457, 1.0, 1.0, 65.0, 45.72161286067026], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19166666666666668, 0.0, 0.375, 0.8366433300848243, 0.8746271224878189, 1.0, 1.0, 1.0, 0.45721612860670263], 
reward next is 0.5428, 
noisyNet noise sample is [array([0.1178766], dtype=float32), 1.6833825]. 
=============================================
[2019-04-09 14:44:05,340] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.5069874 ]
 [0.47318548]
 [0.46357876]
 [0.48570275]
 [0.4577846 ]], R is [[1.02350926]
 [1.78848171]
 [2.55802059]
 [3.30784488]
 [4.02066612]].
[2019-04-09 14:44:05,697] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02610914 0.12988096 0.07796057 0.09963541 0.06108074 0.03467849
 0.11099839 0.09548522 0.12926751 0.08966018 0.14524332], sum to 1.0000
[2019-04-09 14:44:05,701] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9401
[2019-04-09 14:44:05,714] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 76.83333333333334, 0.0, 22.5, 28.14646303646171, 1.132111740680259, 1.0, 1.0, 65.0, 25.43508170911774], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1347600.0000, 
sim time next is 1348200.0000, 
raw observation next is [1.1, 92.0, 71.0, 0.0, 22.5, 28.15322618280858, 1.1339521138077, 1.0, 1.0, 65.0, 25.69433943076277], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.23666666666666666, 0.0, 0.375, 0.8461021819007151, 0.8779840379359, 1.0, 1.0, 1.0, 0.2569433943076277], 
reward next is 0.7431, 
noisyNet noise sample is [array([-1.285993], dtype=float32), -0.076190636]. 
=============================================
[2019-04-09 14:44:05,740] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03788505 0.10758192 0.0753672  0.09170494 0.06510417 0.04696467
 0.11568823 0.09232247 0.1249031  0.09039418 0.15208401], sum to 1.0000
[2019-04-09 14:44:05,747] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4298
[2019-04-09 14:44:05,772] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6, 92.0, 27.0, 0.0, 22.5, 27.68315221671556, 1.011626324404185, 1.0, 1.0, 55.0, 33.29751856626698], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1327800.0000, 
sim time next is 1328400.0000, 
raw observation next is [0.5, 92.0, 31.5, 0.0, 22.5, 27.72470078625189, 1.016580336301748, 1.0, 1.0, 45.0, 23.850304664738], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.105, 0.0, 0.375, 0.8103917321876576, 0.8388601121005826, 1.0, 1.0, 0.6, 0.23850304664738], 
reward next is 0.7615, 
noisyNet noise sample is [array([1.6870731], dtype=float32), 1.0145748]. 
=============================================
[2019-04-09 14:44:05,994] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03001075 0.11255445 0.08083536 0.08399053 0.06785996 0.04636769
 0.10252268 0.11125521 0.11692641 0.08235699 0.16531996], sum to 1.0000
[2019-04-09 14:44:05,994] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8922
[2019-04-09 14:44:06,014] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 26.77187763719303, 0.9731784681244865, 1.0, 1.0, 45.0, 30.9946991151865], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 26.93740150386308, 0.9957190701284292, 1.0, 1.0, 65.0, 44.43093643708468], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7447834586552565, 0.8319063567094763, 1.0, 1.0, 1.0, 0.44430936437084684], 
reward next is 0.5557, 
noisyNet noise sample is [array([-0.21572177], dtype=float32), 0.6108085]. 
=============================================
[2019-04-09 14:44:06,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03034721 0.13039781 0.08104556 0.09283067 0.06698804 0.0420776
 0.10093283 0.09171498 0.13085894 0.08212148 0.15068485], sum to 1.0000
[2019-04-09 14:44:06,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-09 14:44:06,047] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8, 92.0, 102.0, 0.0, 22.5, 27.98514764346033, 1.072972106176402, 1.0, 1.0, 65.0, 35.35317671784853], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 22.5, 28.00354392305612, 1.079582017420965, 1.0, 1.0, 30.0, 28.62018016984732], 
processed observation next is [1.0, 0.43478260869565216, 0.48753462603878117, 0.92, 0.353888888888889, 0.0, 0.375, 0.8336286602546767, 0.8598606724736549, 1.0, 1.0, 0.3, 0.2862018016984732], 
reward next is 0.7138, 
noisyNet noise sample is [array([-1.7084458], dtype=float32), -0.49928683]. 
=============================================
[2019-04-09 14:44:06,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02815138 0.1093802  0.07915533 0.0900185  0.06151104 0.03906253
 0.13467023 0.10768653 0.09828939 0.08704574 0.16502908], sum to 1.0000
[2019-04-09 14:44:06,100] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6183
[2019-04-09 14:44:06,103] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02965806 0.12775913 0.07217264 0.10332289 0.0612145  0.04026816
 0.11190096 0.08514086 0.14591725 0.08679485 0.1358507 ], sum to 1.0000
[2019-04-09 14:44:06,103] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7369
[2019-04-09 14:44:06,118] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.15098285604348, 0.9996689894437942, 1.0, 1.0, 65.0, 38.79648610792184], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.12977391136412, 0.9959093702634799, 1.0, 1.0, 50.0, 34.82103301984755], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7608144926136765, 0.8319697900878267, 1.0, 1.0, 0.7, 0.34821033019847547], 
reward next is 0.6518, 
noisyNet noise sample is [array([-1.7009096], dtype=float32), 0.109262705]. 
=============================================
[2019-04-09 14:44:06,119] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 92.0, 110.3333333333333, 0.0, 22.5, 28.02675238922428, 1.085223759324485, 1.0, 1.0, 45.0, 19.856331772736], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [1.1, 92.0, 114.5, 0.0, 22.5, 28.05123208203707, 1.089934919594716, 1.0, 1.0, 55.0, 22.67902071574116], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.38166666666666665, 0.0, 0.375, 0.8376026735030893, 0.8633116398649053, 1.0, 1.0, 0.8, 0.2267902071574116], 
reward next is 0.7732, 
noisyNet noise sample is [array([0.11145142], dtype=float32), -1.6141291]. 
=============================================
[2019-04-09 14:44:06,124] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.5037368 ]
 [0.59193337]
 [0.58024275]
 [0.46621954]
 [0.59262973]], R is [[1.19443893]
 [1.79452968]
 [2.42920923]
 [3.01629758]
 [3.64620018]].
[2019-04-09 14:44:06,177] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02632125 0.11748258 0.08690043 0.08599218 0.06283946 0.05034212
 0.11620752 0.09842457 0.10493704 0.08273506 0.16781779], sum to 1.0000
[2019-04-09 14:44:06,177] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3252
[2019-04-09 14:44:06,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.29777402107322, 0.993382840396419, 1.0, 1.0, 55.0, 30.37259569198959], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1365000.0000, 
sim time next is 1365600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.35644027041081, 0.9838582984939159, 0.0, 1.0, 35.0, 29.2317000221411], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7797033558675676, 0.8279527661646386, 0.0, 1.0, 0.4, 0.292317000221411], 
reward next is 0.7077, 
noisyNet noise sample is [array([0.9452566], dtype=float32), 0.041616015]. 
=============================================
[2019-04-09 14:44:06,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03423636 0.12982543 0.08391488 0.09148391 0.07289545 0.044213
 0.12082885 0.10360962 0.09295432 0.07380737 0.15223077], sum to 1.0000
[2019-04-09 14:44:06,239] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5471
[2019-04-09 14:44:06,256] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.21958710957956, 0.9698157302623397, 0.0, 1.0, 65.0, 39.02273185801031], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1366800.0000, 
sim time next is 1367400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.17201687276612, 0.9672983881820549, 1.0, 1.0, 45.0, 37.46894455159048], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7643347393971768, 0.822432796060685, 1.0, 1.0, 0.6, 0.37468944551590483], 
reward next is 0.6253, 
noisyNet noise sample is [array([0.9452566], dtype=float32), 0.041616015]. 
=============================================
[2019-04-09 14:44:06,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02747348 0.1293858  0.07599783 0.09289286 0.07437592 0.04319407
 0.11283483 0.09470867 0.10408753 0.08811811 0.1569309 ], sum to 1.0000
[2019-04-09 14:44:06,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1329
[2019-04-09 14:44:06,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03055601 0.11146138 0.07393304 0.06639466 0.06131708 0.03894453
 0.14043166 0.10336909 0.0967208  0.10415319 0.1727185 ], sum to 1.0000
[2019-04-09 14:44:06,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2137
[2019-04-09 14:44:06,456] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.6, 95.5, 12.0, 0.0, 22.5, 27.97119506581967, 0.9489692820217926, 1.0, 1.0, 60.0, 62.10832098317221], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1356600.0000, 
sim time next is 1357200.0000, 
raw observation next is [0.5, 96.0, 9.0, 0.0, 22.5, 27.22824945926251, 1.086248131626149, 1.0, 1.0, 20.0, 42.43474155972847], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.03, 0.0, 0.375, 0.7690207882718759, 0.8620827105420497, 1.0, 1.0, 0.1, 0.4243474155972847], 
reward next is 0.5757, 
noisyNet noise sample is [array([0.3128977], dtype=float32), -0.12677109]. 
=============================================
[2019-04-09 14:44:06,472] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 27.04261841989634, 0.9320003975959538, 0.0, 1.0, 40.0, 36.57112652823461], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1374600.0000, 
sim time next is 1375200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 27.03021135310049, 0.9305576117794571, 0.0, 1.0, 25.0, 31.74449724542015], 
processed observation next is [1.0, 0.9565217391304348, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7525176127583743, 0.8101858705931524, 0.0, 1.0, 0.2, 0.3174449724542015], 
reward next is 0.6826, 
noisyNet noise sample is [array([0.7324023], dtype=float32), -1.8217471]. 
=============================================
[2019-04-09 14:44:07,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03408645 0.11140083 0.08216513 0.08201907 0.06345664 0.04815795
 0.09597839 0.08587684 0.15366197 0.09551287 0.14768392], sum to 1.0000
[2019-04-09 14:44:07,985] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4966
[2019-04-09 14:44:07,994] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999998, 95.83333333333334, 54.66666666666666, 0.0, 22.5, 27.56969777075639, 0.9148727528654863, 1.0, 1.0, 50.0, 25.87514496463616], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1417800.0000, 
sim time next is 1418400.0000, 
raw observation next is [0.0, 95.0, 59.0, 0.0, 22.5, 27.60481321185105, 0.9219080904440262, 1.0, 1.0, 45.0, 23.8252913453716], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.19666666666666666, 0.0, 0.375, 0.8004011009875874, 0.8073026968146754, 1.0, 1.0, 0.6, 0.238252913453716], 
reward next is 0.7617, 
noisyNet noise sample is [array([0.52929646], dtype=float32), 0.7105721]. 
=============================================
[2019-04-09 14:44:08,290] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03950457 0.09809387 0.08257415 0.06541586 0.06456688 0.05000508
 0.1254439  0.11175479 0.07986514 0.12314635 0.15962942], sum to 1.0000
[2019-04-09 14:44:08,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-09 14:44:08,305] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.81644082430407, 0.903272283713588, 0.0, 1.0, 45.0, 35.67625739287607], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1383600.0000, 
sim time next is 1384200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.93189725938391, 0.8960928285970827, 0.0, 1.0, 65.0, 50.96326778199795], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7443247716153257, 0.7986976095323609, 0.0, 1.0, 1.0, 0.5096326778199795], 
reward next is 0.4904, 
noisyNet noise sample is [array([-1.1884178], dtype=float32), -1.4394675]. 
=============================================
[2019-04-09 14:44:08,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02945004 0.12885357 0.08352476 0.09530266 0.06534375 0.04153333
 0.10892043 0.11034904 0.11949955 0.0846681  0.13255475], sum to 1.0000
[2019-04-09 14:44:08,504] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1721
[2019-04-09 14:44:08,522] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 22.5, 27.81114246841975, 0.9876693759985633, 1.0, 1.0, 45.0, 22.02622009216586], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 22.5, 27.84090436680271, 0.9718574511312571, 1.0, 1.0, 25.0, 28.98138116893194], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.375, 0.820075363900226, 0.8239524837104191, 1.0, 1.0, 0.2, 0.2898138116893194], 
reward next is 0.7102, 
noisyNet noise sample is [array([-2.7367182], dtype=float32), 0.1855781]. 
=============================================
[2019-04-09 14:44:08,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.5324568 ]
 [0.54483986]
 [0.50567824]
 [0.49114472]
 [0.49322176]], R is [[1.26273012]
 [2.02984071]
 [2.75294566]
 [3.43068862]
 [4.20641136]].
[2019-04-09 14:44:09,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02893604 0.12652786 0.08471702 0.09642062 0.06088998 0.04213457
 0.11753081 0.10088664 0.10466649 0.08379875 0.15349115], sum to 1.0000
[2019-04-09 14:44:09,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0388
[2019-04-09 14:44:09,063] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 50.33333333333333, 0.0, 22.5, 26.28727039310804, 0.8600454400603068, 1.0, 1.0, 55.0, 9.318583715758793], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1437600.0000, 
sim time next is 1438200.0000, 
raw observation next is [1.1, 92.0, 46.0, 0.0, 22.5, 27.44648306390357, 0.9354902792333085, 1.0, 1.0, 20.0, 22.1431874400126], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.15333333333333332, 0.0, 0.375, 0.7872069219919643, 0.8118300930777695, 1.0, 1.0, 0.1, 0.221431874400126], 
reward next is 0.7786, 
noisyNet noise sample is [array([0.8707683], dtype=float32), 0.28157616]. 
=============================================
[2019-04-09 14:44:09,371] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03046285 0.14666352 0.08304451 0.09096645 0.06755645 0.03976938
 0.11517101 0.09495371 0.10163541 0.07901634 0.15076047], sum to 1.0000
[2019-04-09 14:44:09,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6883
[2019-04-09 14:44:09,399] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.06319673923263, 0.9039295757948341, 0.0, 1.0, 65.0, 37.24867075711108], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1453200.0000, 
sim time next is 1453800.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.0212000049994, 0.9018352868820472, 1.0, 1.0, 55.0, 36.04618823191115], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7517666670832833, 0.8006117622940158, 1.0, 1.0, 0.8, 0.3604618823191115], 
reward next is 0.6395, 
noisyNet noise sample is [array([0.59709716], dtype=float32), -0.16438608]. 
=============================================
[2019-04-09 14:44:09,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03058505 0.13182871 0.0787429  0.10064188 0.06734066 0.04017693
 0.10133012 0.09110517 0.12066066 0.08242336 0.1551646 ], sum to 1.0000
[2019-04-09 14:44:09,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4222
[2019-04-09 14:44:09,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 81.0, 0.0, 22.5, 27.7419797868373, 0.973774468470971, 1.0, 1.0, 40.0, 28.25229875840083], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1432800.0000, 
sim time next is 1433400.0000, 
raw observation next is [1.1, 92.0, 78.0, 0.0, 22.5, 27.71352863014575, 0.9793562325681694, 1.0, 1.0, 40.0, 21.96289355846636], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.26, 0.0, 0.375, 0.8094607191788125, 0.8264520775227231, 1.0, 1.0, 0.5, 0.2196289355846636], 
reward next is 0.7804, 
noisyNet noise sample is [array([-0.59611934], dtype=float32), -1.0201293]. 
=============================================
[2019-04-09 14:44:09,793] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02640937 0.12991366 0.07795778 0.07828688 0.0586145  0.04697828
 0.12833688 0.09602775 0.11136065 0.07967344 0.16644083], sum to 1.0000
[2019-04-09 14:44:09,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3492
[2019-04-09 14:44:09,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 26.98560623607764, 0.875919996600616, 1.0, 1.0, 65.0, 36.72318943395244], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 19.0, 26.97747636457053, 0.8736866584819479, 0.0, 1.0, 35.0, 35.90162847313984], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 0.08333333333333333, 0.7481230303808776, 0.7912288861606492, 0.0, 1.0, 0.4, 0.35901628473139835], 
reward next is 0.6410, 
noisyNet noise sample is [array([-0.4712862], dtype=float32), 0.2535485]. 
=============================================
[2019-04-09 14:44:09,834] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[0.61625606]
 [0.5780866 ]
 [0.64090955]
 [0.6239074 ]
 [0.5653184 ]], R is [[1.10320795]
 [1.72494388]
 [2.38835526]
 [3.04284954]
 [3.71627593]].
[2019-04-09 14:44:09,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02552872 0.1139444  0.08132709 0.08362954 0.06564816 0.04587495
 0.11537772 0.09442518 0.12117134 0.08105548 0.17201748], sum to 1.0000
[2019-04-09 14:44:09,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-09 14:44:09,929] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.18822577976699, 0.9250527746430022, 1.0, 1.0, 35.0, 30.27820087097691], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1451400.0000, 
sim time next is 1452000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.18193992932212, 0.917926410546821, 1.0, 1.0, 30.0, 28.63552483557747], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7651616607768433, 0.8059754701822737, 1.0, 1.0, 0.3, 0.2863552483557747], 
reward next is 0.7136, 
noisyNet noise sample is [array([0.08356346], dtype=float32), -0.29422295]. 
=============================================
[2019-04-09 14:44:09,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.56767464]
 [0.5414031 ]
 [0.56038475]
 [0.44496864]
 [0.54414874]], R is [[1.19374442]
 [1.87902498]
 [2.56906438]
 [3.27798724]
 [3.93392801]].
[2019-04-09 14:44:10,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02284272 0.1299044  0.08051114 0.09485567 0.0600635  0.03756677
 0.11282673 0.07939713 0.13927165 0.08068729 0.162073  ], sum to 1.0000
[2019-04-09 14:44:10,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9889
[2019-04-09 14:44:10,235] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 84.0, 0.0, 22.5, 27.65329870531261, 0.9549479079278815, 1.0, 1.0, 65.0, 29.3940597597643], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1422600.0000, 
sim time next is 1423200.0000, 
raw observation next is [0.0, 95.0, 87.0, 0.0, 22.5, 27.63350623137957, 0.9527251118272558, 1.0, 1.0, 65.0, 29.36737211517596], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.29, 0.0, 0.375, 0.8027921859482975, 0.817575037275752, 1.0, 1.0, 1.0, 0.2936737211517596], 
reward next is 0.7063, 
noisyNet noise sample is [array([-0.540609], dtype=float32), 0.8064888]. 
=============================================
[2019-04-09 14:44:10,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0484685  0.08736832 0.08383464 0.05922535 0.06449459 0.04631476
 0.13156226 0.10874815 0.08556996 0.11229608 0.17211738], sum to 1.0000
[2019-04-09 14:44:10,828] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9220
[2019-04-09 14:44:10,841] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 19.0, 26.90420464963813, 0.8414240763637394, 0.0, 1.0, 65.0, 38.19421649664537], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1473000.0000, 
sim time next is 1473600.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 19.0, 26.97009762894352, 0.8305506318902355, 0.0, 1.0, 65.0, 32.79389836561598], 
processed observation next is [1.0, 0.043478260869565216, 0.5124653739612189, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7475081357452934, 0.7768502106300784, 0.0, 1.0, 1.0, 0.3279389836561598], 
reward next is 0.6721, 
noisyNet noise sample is [array([-1.4825554], dtype=float32), -0.40949672]. 
=============================================
[2019-04-09 14:44:11,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04763562 0.09008346 0.07680966 0.0571482  0.05850026 0.0522099
 0.13880979 0.10303769 0.09218553 0.12721029 0.15636961], sum to 1.0000
[2019-04-09 14:44:11,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9912
[2019-04-09 14:44:11,311] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.66838962839256, 0.7666452617692467, 0.0, 1.0, 25.0, 35.05392796519375], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.72332968815301, 0.7732694936075752, 0.0, 1.0, 65.0, 40.41320885601318], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7269441406794176, 0.7577564978691917, 0.0, 1.0, 1.0, 0.4041320885601318], 
reward next is 0.5959, 
noisyNet noise sample is [array([0.60045356], dtype=float32), -0.33106253]. 
=============================================
[2019-04-09 14:44:11,494] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39385: loss 28.7899
[2019-04-09 14:44:11,495] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39385: loss 27.9145
[2019-04-09 14:44:11,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39385: learning rate 0.0000
[2019-04-09 14:44:11,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39386: learning rate 0.0000
[2019-04-09 14:44:11,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04446531 0.0910467  0.08388752 0.06335204 0.05612523 0.04810089
 0.13013966 0.09682377 0.10309187 0.11638194 0.16658506], sum to 1.0000
[2019-04-09 14:44:11,503] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39390: loss 31.0500
[2019-04-09 14:44:11,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5392
[2019-04-09 14:44:11,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39392: learning rate 0.0000
[2019-04-09 14:44:11,522] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.78410906845916, 0.7857506020564394, 0.0, 1.0, 25.0, 31.56310881440189], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1487400.0000, 
sim time next is 1488000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.74915052263981, 0.7920453008789671, 0.0, 1.0, 50.0, 32.55949134006688], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7290958768866508, 0.764015100292989, 0.0, 1.0, 0.7, 0.32559491340066876], 
reward next is 0.6744, 
noisyNet noise sample is [array([0.3227666], dtype=float32), 0.15488078]. 
=============================================
[2019-04-09 14:44:11,544] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.25068563]
 [0.18895197]
 [0.32117403]
 [0.26412567]
 [0.21799088]], R is [[0.96590167]
 [1.64061165]
 [2.25577545]
 [2.82319665]
 [3.45894265]].
[2019-04-09 14:44:11,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0292716  0.12619013 0.07898084 0.08631466 0.06057061 0.04216596
 0.10738084 0.09182353 0.12440905 0.08563077 0.167262  ], sum to 1.0000
[2019-04-09 14:44:11,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4509
[2019-04-09 14:44:11,690] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 51.33333333333334, 0.0, 22.5, 27.63293212671199, 0.9086743733774402, 1.0, 1.0, 20.0, 20.52214207660925], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [2.1, 100.0, 55.66666666666666, 0.0, 22.5, 27.6773966094017, 0.9154032235258445, 1.0, 1.0, 45.0, 22.35143511786947], 
processed observation next is [1.0, 0.391304347826087, 0.5207756232686982, 1.0, 0.18555555555555553, 0.0, 0.375, 0.8064497174501417, 0.8051344078419481, 1.0, 1.0, 0.6, 0.2235143511786947], 
reward next is 0.7765, 
noisyNet noise sample is [array([-0.6120918], dtype=float32), 2.660942]. 
=============================================
[2019-04-09 14:44:11,702] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03664136 0.08677577 0.07760593 0.06116054 0.05410285 0.044639
 0.1335981  0.10835356 0.08141658 0.13552243 0.18018387], sum to 1.0000
[2019-04-09 14:44:11,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9649
[2019-04-09 14:44:11,723] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.93950747840109, 0.8249064414021139, 0.0, 1.0, 35.0, 31.62314981854557], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.94803543274885, 0.8425485830445845, 0.0, 1.0, 65.0, 46.83139531268915], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7456696193957374, 0.7808495276815282, 0.0, 1.0, 1.0, 0.46831395312689156], 
reward next is 0.5317, 
noisyNet noise sample is [array([2.0834115], dtype=float32), 0.051232975]. 
=============================================
[2019-04-09 14:44:11,792] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39565: loss 29.2978
[2019-04-09 14:44:11,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39574: learning rate 0.0000
[2019-04-09 14:44:11,809] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39575: loss 30.3766
[2019-04-09 14:44:11,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39576: learning rate 0.0000
[2019-04-09 14:44:11,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04162507 0.10157131 0.08253628 0.06697436 0.05836039 0.05030999
 0.12881652 0.09147209 0.10281643 0.10828046 0.16723709], sum to 1.0000
[2019-04-09 14:44:11,854] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1112
[2019-04-09 14:44:11,874] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 19.0, 26.6479416435316, 0.7565440194916199, 0.0, 1.0, 65.0, 66.95154736500014], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 22.5, 26.56470436726387, 0.7660883559458599, 0.0, 1.0, 20.0, 41.78844564342449], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.375, 0.7137253639386559, 0.7553627853152866, 0.0, 1.0, 0.1, 0.4178844564342449], 
reward next is 0.5821, 
noisyNet noise sample is [array([0.30657429], dtype=float32), 0.8760845]. 
=============================================
[2019-04-09 14:44:11,987] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 39677: loss 30.4883
[2019-04-09 14:44:11,988] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 39678: learning rate 0.0000
[2019-04-09 14:44:12,073] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39731: loss 32.7634
[2019-04-09 14:44:12,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39731: learning rate 0.0000
[2019-04-09 14:44:12,226] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39822: loss 32.9173
[2019-04-09 14:44:12,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39822: learning rate 0.0000
[2019-04-09 14:44:12,319] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39870: loss 30.1094
[2019-04-09 14:44:12,320] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39870: learning rate 0.0000
[2019-04-09 14:44:12,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02760792 0.11237751 0.07849964 0.08893094 0.06340145 0.04030095
 0.10862169 0.09957716 0.14720336 0.09319188 0.14028752], sum to 1.0000
[2019-04-09 14:44:12,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5242
[2019-04-09 14:44:12,388] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39907: loss 30.7538
[2019-04-09 14:44:12,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39907: learning rate 0.0000
[2019-04-09 14:44:12,391] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.75, 98.0, 73.0, 0.0, 22.5, 27.62364751928573, 0.9178942623054459, 1.0, 1.0, 35.0, 18.37863926785175], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1506600.0000, 
sim time next is 1507200.0000, 
raw observation next is [2.933333333333333, 97.33333333333334, 75.5, 118.0, 22.5, 27.64740223537089, 0.9315569550122054, 1.0, 1.0, 50.0, 23.75317797197305], 
processed observation next is [1.0, 0.43478260869565216, 0.543859649122807, 0.9733333333333334, 0.25166666666666665, 0.13038674033149172, 0.375, 0.8039501862809075, 0.8105189850040685, 1.0, 1.0, 0.7, 0.2375317797197305], 
reward next is 0.7625, 
noisyNet noise sample is [array([0.98104703], dtype=float32), -1.0470067]. 
=============================================
[2019-04-09 14:44:12,535] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39994: loss 33.3246
[2019-04-09 14:44:12,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39997: learning rate 0.0000
[2019-04-09 14:44:12,547] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-09 14:44:12,548] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:44:12,549] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:44:12,549] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:44:12,550] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:44:12,550] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:44:12,551] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:44:12,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run5
[2019-04-09 14:44:12,580] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run5
[2019-04-09 14:44:12,592] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run5
[2019-04-09 14:44:28,348] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00463252], dtype=float32), 0.00713978]
[2019-04-09 14:44:28,349] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.1, 96.0, 0.0, 0.0, 22.5, 23.99890376120468, 0.04461773321863316, 0.0, 1.0, 30.0, 46.20006659659079]
[2019-04-09 14:44:28,349] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:44:28,349] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.04297086 0.14128344 0.07855458 0.10325363 0.0837516  0.04866852
 0.10239299 0.10146344 0.09363145 0.08420862 0.11982091], sampled 0.5251052982331421
[2019-04-09 14:45:33,579] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5709.1707 277897.0014 2668.1342
[2019-04-09 14:45:33,599] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:33,599] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:33,599] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:33,599] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:33,599] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:33,715] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:33,715] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:33,715] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:33,715] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:33,715] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:34,556] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00463252], dtype=float32), 0.00713978]
[2019-04-09 14:45:34,557] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [3.0423791825, 59.33198681833333, 121.6250860633333, 0.0, 22.5, 27.33223868971944, 0.9313861261346985, 1.0, 1.0, 25.0, 30.15603521681829]
[2019-04-09 14:45:34,558] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:45:34,559] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.03764012 0.15524313 0.07439005 0.11389066 0.07930702 0.05367554
 0.11032984 0.09220087 0.09054385 0.07980416 0.11297479], sampled 0.7588536021609796
[2019-04-09 14:45:41,265] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5493.2730 300282.9344 2164.5881
[2019-04-09 14:45:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:41,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:41,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:41,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:41,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:41,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:42,515] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5421.3384 307324.4968 1792.2348
[2019-04-09 14:45:42,536] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:42,536] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:42,536] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:42,536] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:42,536] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:45:42,651] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:42,651] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:42,651] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:42,651] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:42,651] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:45:43,538] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 40000, evaluation results [40000.0, 5493.273043903125, 300282.9343848785, 2164.5881306109363, 5709.170731151642, 277897.00141581194, 2668.1342043143845, 5421.338358311811, 307324.4968215273, 1792.2347540344406]
[2019-04-09 14:45:43,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03313344 0.1247261  0.0735808  0.07786021 0.05998579 0.04005606
 0.12997669 0.12089819 0.08829933 0.10020049 0.15128283], sum to 1.0000
[2019-04-09 14:45:43,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6116
[2019-04-09 14:45:43,647] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 19.0, 26.97063241363875, 0.8842177090296747, 0.0, 1.0, 20.0, 31.92390875278717], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1458600.0000, 
sim time next is 1459200.0000, 
raw observation next is [1.433333333333334, 90.0, 0.0, 0.0, 19.0, 26.96832861978098, 0.8883813053854189, 0.0, 1.0, 50.0, 30.32670657698936], 
processed observation next is [1.0, 0.9130434782608695, 0.502308402585411, 0.9, 0.0, 0.0, 0.08333333333333333, 0.7473607183150817, 0.7961271017951397, 0.0, 1.0, 0.7, 0.3032670657698936], 
reward next is 0.6967, 
noisyNet noise sample is [array([0.51236403], dtype=float32), -0.20405921]. 
=============================================
[2019-04-09 14:45:43,675] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40074: loss 31.0898
[2019-04-09 14:45:43,676] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40074: learning rate 0.0000
[2019-04-09 14:45:43,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04992456 0.08317527 0.07974605 0.06865784 0.06115118 0.05169415
 0.13002631 0.11335278 0.103801   0.10483789 0.15363295], sum to 1.0000
[2019-04-09 14:45:43,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-09 14:45:43,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0431696  0.11391547 0.07981297 0.08510647 0.06304634 0.04800671
 0.10578167 0.09310294 0.11990954 0.10766523 0.140483  ], sum to 1.0000
[2019-04-09 14:45:43,696] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2513
[2019-04-09 14:45:43,703] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 19.0, 26.97029725268553, 0.8130280579869406, 0.0, 1.0, 50.0, 35.92261748760363], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1492800.0000, 
sim time next is 1493400.0000, 
raw observation next is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 19.0, 27.04867415331911, 0.7919466208918386, 0.0, 1.0, 60.0, 39.6517552333675], 
processed observation next is [1.0, 0.2608695652173913, 0.4981532779316713, 0.9933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7540561794432591, 0.7639822069639463, 0.0, 1.0, 0.9, 0.396517552333675], 
reward next is 0.6035, 
noisyNet noise sample is [array([1.2332745], dtype=float32), 1.2070348]. 
=============================================
[2019-04-09 14:45:43,723] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 22.5, 27.02726000485611, 0.7986306546302234, 1.0, 1.0, 50.0, 29.10346495625927], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1500000.0000, 
sim time next is 1500600.0000, 
raw observation next is [1.516666666666667, 100.0, 27.66666666666666, 0.0, 22.5, 27.07045463286095, 0.8119043694315305, 1.0, 1.0, 30.0, 27.41023659691514], 
processed observation next is [1.0, 0.34782608695652173, 0.5046168051708219, 1.0, 0.0922222222222222, 0.0, 0.375, 0.7558712194050793, 0.7706347898105101, 1.0, 1.0, 0.3, 0.2741023659691514], 
reward next is 0.7259, 
noisyNet noise sample is [array([1.3812562], dtype=float32), 0.96999514]. 
=============================================
[2019-04-09 14:45:43,771] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03130369 0.12060607 0.08517459 0.09172007 0.06260991 0.04014866
 0.10237413 0.08824776 0.15475231 0.09153472 0.13152812], sum to 1.0000
[2019-04-09 14:45:43,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0902
[2019-04-09 14:45:43,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 22.5, 27.62131014214571, 0.9182726046449415, 1.0, 1.0, 45.0, 25.83907362954078], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1505400.0000, 
sim time next is 1506000.0000, 
raw observation next is [2.566666666666667, 98.66666666666667, 68.66666666666667, 0.0, 22.5, 27.66407345927934, 0.9294963053374491, 1.0, 1.0, 20.0, 21.67299569102686], 
processed observation next is [1.0, 0.43478260869565216, 0.5337026777469991, 0.9866666666666667, 0.2288888888888889, 0.0, 0.375, 0.805339454939945, 0.8098321017791497, 1.0, 1.0, 0.1, 0.21672995691026858], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.49365106], dtype=float32), -0.15257247]. 
=============================================
[2019-04-09 14:45:43,789] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03398047 0.11712755 0.08490334 0.0773669  0.06758061 0.04182372
 0.12834302 0.11414938 0.08786534 0.09523808 0.1516216 ], sum to 1.0000
[2019-04-09 14:45:43,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.48878744]
 [0.44572815]
 [0.45081905]
 [0.44394985]
 [0.41982278]], R is [[1.26382577]
 [1.9927969 ]
 [2.75101137]
 [3.51795411]
 [4.22735214]].
[2019-04-09 14:45:43,793] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1095
[2019-04-09 14:45:43,814] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.183333333333333, 92.0, 0.0, 0.0, 19.0, 26.99184259012116, 0.8388534474002927, 0.0, 1.0, 65.0, 41.97095035883259], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1462200.0000, 
sim time next is 1462800.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 19.0, 26.96392126501366, 0.842811935833342, 0.0, 1.0, 20.0, 38.21973527788858], 
processed observation next is [1.0, 0.9565217391304348, 0.4976915974145891, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7469934387511383, 0.7809373119444474, 0.0, 1.0, 0.1, 0.38219735277888583], 
reward next is 0.6178, 
noisyNet noise sample is [array([-0.16220729], dtype=float32), -0.53027683]. 
=============================================
[2019-04-09 14:45:43,995] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40261: loss 23.8885
[2019-04-09 14:45:43,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40261: learning rate 0.0000
[2019-04-09 14:45:44,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02748405 0.142141   0.08875896 0.09972653 0.06207443 0.03350233
 0.10337669 0.08703679 0.12186719 0.07729974 0.15673226], sum to 1.0000
[2019-04-09 14:45:44,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8196
[2019-04-09 14:45:44,118] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 22.5, 28.07664249911853, 0.8819889220732599, 1.0, 1.0, 55.0, 42.51372331252046], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [5.800000000000001, 83.0, 100.0, 700.0, 22.5, 27.96402047220795, 1.120217217856714, 1.0, 1.0, 60.0, 21.05336633701543], 
processed observation next is [1.0, 0.5217391304347826, 0.6232686980609419, 0.83, 0.3333333333333333, 0.7734806629834254, 0.375, 0.8303350393506624, 0.8734057392855714, 1.0, 1.0, 0.9, 0.2105336633701543], 
reward next is 0.7895, 
noisyNet noise sample is [array([1.0475137], dtype=float32), 0.043831542]. 
=============================================
[2019-04-09 14:45:44,255] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02862305 0.13548495 0.08576225 0.10330548 0.06082379 0.04953113
 0.09260953 0.08590279 0.12379462 0.07525891 0.15890345], sum to 1.0000
[2019-04-09 14:45:44,255] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2033
[2019-04-09 14:45:44,266] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 22.5, 28.47542967775718, 1.179390987135731, 1.0, 1.0, 20.0, 3.329026396688572], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1518600.0000, 
sim time next is 1519200.0000, 
raw observation next is [10.0, 63.0, 80.0, 682.0, 22.5, 28.59457264258711, 1.20318991381087, 1.0, 1.0, 25.0, 7.457286239496252], 
processed observation next is [1.0, 0.6086956521739131, 0.739612188365651, 0.63, 0.26666666666666666, 0.7535911602209945, 0.375, 0.8828810535489259, 0.9010633046036233, 1.0, 1.0, 0.2, 0.07457286239496252], 
reward next is 0.9254, 
noisyNet noise sample is [array([-0.40041503], dtype=float32), 0.6124337]. 
=============================================
[2019-04-09 14:45:44,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05803565 0.07259281 0.09331659 0.05682923 0.06063295 0.05046682
 0.13761455 0.12218504 0.07944512 0.12880284 0.14007846], sum to 1.0000
[2019-04-09 14:45:44,324] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9765
[2019-04-09 14:45:44,343] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 19.0, 26.97780119350584, 0.8240953405217447, 0.0, 1.0, 20.0, 28.31872095461046], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1473600.0000, 
sim time next is 1474200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 19.0, 26.97251258134352, 0.8237363072079574, 0.0, 1.0, 65.0, 38.55046790990293], 
processed observation next is [1.0, 0.043478260869565216, 0.515235457063712, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7477093817786266, 0.7745787690693192, 0.0, 1.0, 1.0, 0.3855046790990293], 
reward next is 0.6145, 
noisyNet noise sample is [array([0.99117446], dtype=float32), -0.12429334]. 
=============================================
[2019-04-09 14:45:44,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03275746 0.12064207 0.07947385 0.07019167 0.0632021  0.04524179
 0.12760219 0.09914484 0.09670253 0.0985186  0.16652289], sum to 1.0000
[2019-04-09 14:45:44,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03442398 0.11007187 0.09316493 0.06787949 0.06481411 0.04269288
 0.13073829 0.11093396 0.10154726 0.09239938 0.15133391], sum to 1.0000
[2019-04-09 14:45:44,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5559
[2019-04-09 14:45:44,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4272
[2019-04-09 14:45:44,618] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.15, 75.0, 0.0, 0.0, 19.0, 27.82961321584138, 1.124370173893797, 0.0, 1.0, 45.0, 23.1669184597841], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1546200.0000, 
sim time next is 1546800.0000, 
raw observation next is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 19.0, 27.80483400262811, 1.118041280167905, 0.0, 1.0, 50.0, 20.74368429492609], 
processed observation next is [1.0, 0.9130434782608695, 0.6555863342566944, 0.7533333333333333, 0.0, 0.0, 0.08333333333333333, 0.817069500219009, 0.8726804267226349, 0.0, 1.0, 0.7, 0.2074368429492609], 
reward next is 0.7926, 
noisyNet noise sample is [array([0.57942075], dtype=float32), 0.8211954]. 
=============================================
[2019-04-09 14:45:44,622] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333334, 74.66666666666667, 0.0, 0.0, 19.0, 27.8859118856908, 1.127775262434361, 0.0, 1.0, 60.0, 22.24038357341484], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1545600.0000, 
sim time next is 1546200.0000, 
raw observation next is [7.15, 75.0, 0.0, 0.0, 19.0, 27.83832682838427, 1.126178430249529, 0.0, 1.0, 45.0, 22.54054837429738], 
processed observation next is [1.0, 0.9130434782608695, 0.6606648199445985, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8198605690320226, 0.8753928100831763, 0.0, 1.0, 0.6, 0.2254054837429738], 
reward next is 0.7746, 
noisyNet noise sample is [array([0.9930339], dtype=float32), -0.8137654]. 
=============================================
[2019-04-09 14:45:44,902] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40798: loss 28.0199
[2019-04-09 14:45:44,903] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40798: learning rate 0.0000
[2019-04-09 14:45:45,157] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04876187 0.09522147 0.07672339 0.05893727 0.06254972 0.05126097
 0.130955   0.10527521 0.09949744 0.11089679 0.1599209 ], sum to 1.0000
[2019-04-09 14:45:45,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5389
[2019-04-09 14:45:45,177] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.65446054801669, 0.7720293589091353, 0.0, 1.0, 30.0, 34.65834201791216], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1490400.0000, 
sim time next is 1491000.0000, 
raw observation next is [2.016666666666667, 96.66666666666666, 0.0, 0.0, 19.0, 26.71545765881783, 0.7860288245828785, 0.0, 1.0, 50.0, 35.88404073657983], 
processed observation next is [1.0, 0.2608695652173913, 0.5184672206832872, 0.9666666666666666, 0.0, 0.0, 0.08333333333333333, 0.7262881382348191, 0.7620096081942928, 0.0, 1.0, 0.7, 0.35884040736579825], 
reward next is 0.6412, 
noisyNet noise sample is [array([0.7732071], dtype=float32), -0.60616785]. 
=============================================
[2019-04-09 14:45:45,191] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[0.23667155]
 [0.32364148]
 [0.3296548 ]
 [0.25186592]
 [0.30713958]], R is [[0.9617179 ]
 [1.60551739]
 [1.95187187]
 [2.93235302]
 [3.55668044]].
[2019-04-09 14:45:45,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03112892 0.13150914 0.08698469 0.07141809 0.05663676 0.04213922
 0.12304664 0.10575224 0.09716965 0.07685955 0.17735508], sum to 1.0000
[2019-04-09 14:45:45,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-09 14:45:45,376] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 73.5, 0.0, 0.0, 19.0, 27.83419363989439, 1.127406321492586, 0.0, 1.0, 30.0, 21.61647749852401], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 19.0, 27.80540479636834, 1.123809800656631, 0.0, 1.0, 45.0, 21.29253444994945], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7366666666666666, 0.0, 0.0, 0.08333333333333333, 0.8171170663640283, 0.8746032668855438, 0.0, 1.0, 0.6, 0.2129253444994945], 
reward next is 0.7871, 
noisyNet noise sample is [array([-1.1717008], dtype=float32), -0.9672555]. 
=============================================
[2019-04-09 14:45:45,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0341995  0.1286947  0.07923398 0.06729545 0.06146341 0.04930355
 0.14342791 0.11455876 0.07897618 0.093082   0.1497645 ], sum to 1.0000
[2019-04-09 14:45:45,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5009
[2019-04-09 14:45:45,523] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 76.0, 0.0, 0.0, 19.0, 27.72260399674171, 1.097345424723147, 0.0, 1.0, 65.0, 26.47555416629986], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1548000.0000, 
sim time next is 1548600.0000, 
raw observation next is [6.416666666666667, 77.0, 0.0, 0.0, 19.0, 27.69429377487281, 1.049235127938978, 0.0, 1.0, 45.0, 30.70218118717677], 
processed observation next is [1.0, 0.9565217391304348, 0.6403508771929826, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8078578145727343, 0.849745042646326, 0.0, 1.0, 0.6, 0.3070218118717677], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.9582743], dtype=float32), 1.4125668]. 
=============================================
[2019-04-09 14:45:45,579] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 41212: loss 29.1755
[2019-04-09 14:45:45,580] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 41213: learning rate 0.0000
[2019-04-09 14:45:45,745] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 41305: loss 31.5080
[2019-04-09 14:45:45,747] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 41305: learning rate 0.0000
[2019-04-09 14:45:45,950] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03777076 0.10578872 0.07822822 0.07221051 0.05827001 0.04704468
 0.12167024 0.09011095 0.10478671 0.11145963 0.17265958], sum to 1.0000
[2019-04-09 14:45:45,951] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3413
[2019-04-09 14:45:45,968] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.566666666666666, 85.16666666666667, 0.0, 0.0, 19.0, 27.37280283833362, 0.9683257679863434, 0.0, 1.0, 30.0, 27.05895886231671], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1569000.0000, 
sim time next is 1569600.0000, 
raw observation next is [4.6, 85.0, 0.0, 0.0, 19.0, 27.39758801486435, 0.9693920318174042, 0.0, 1.0, 35.0, 27.5053839831709], 
processed observation next is [1.0, 0.17391304347826086, 0.590027700831025, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7831323345720292, 0.8231306772724681, 0.0, 1.0, 0.4, 0.275053839831709], 
reward next is 0.7249, 
noisyNet noise sample is [array([0.01456047], dtype=float32), 2.1492882]. 
=============================================
[2019-04-09 14:45:45,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04252677 0.09232946 0.08764279 0.06522982 0.06253897 0.04251653
 0.12729168 0.11893678 0.08593721 0.10967859 0.16537139], sum to 1.0000
[2019-04-09 14:45:45,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3821
[2019-04-09 14:45:45,996] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.37531297555944, 1.004091796941871, 0.0, 1.0, 30.0, 32.23105961408265], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1555200.0000, 
sim time next is 1555800.0000, 
raw observation next is [5.0, 82.00000000000001, 0.0, 0.0, 19.0, 27.34712301856105, 0.9994408514578365, 0.0, 1.0, 40.0, 28.99423785740843], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.8200000000000002, 0.0, 0.0, 0.08333333333333333, 0.7789269182134207, 0.8331469504859456, 0.0, 1.0, 0.5, 0.2899423785740843], 
reward next is 0.7101, 
noisyNet noise sample is [array([1.5328517], dtype=float32), 0.23763244]. 
=============================================
[2019-04-09 14:45:46,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0421886  0.09736517 0.09094228 0.05423469 0.06487485 0.0488264
 0.13836896 0.12596685 0.06784922 0.10145293 0.16793013], sum to 1.0000
[2019-04-09 14:45:46,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5915
[2019-04-09 14:45:46,026] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.416666666666667, 82.00000000000001, 0.0, 0.0, 19.0, 27.50802295992813, 1.045004866173073, 0.0, 1.0, 35.0, 30.47700253587595], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1552200.0000, 
sim time next is 1552800.0000, 
raw observation next is [5.333333333333334, 82.0, 0.0, 0.0, 19.0, 27.54690474112084, 1.032374159370226, 0.0, 1.0, 50.0, 23.56717126831621], 
processed observation next is [1.0, 1.0, 0.6103416435826409, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7955753950934034, 0.8441247197900754, 0.0, 1.0, 0.7, 0.2356717126831621], 
reward next is 0.7643, 
noisyNet noise sample is [array([1.0989245], dtype=float32), -1.245593]. 
=============================================
[2019-04-09 14:45:46,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.047435   0.09288801 0.08679244 0.0701668  0.06104575 0.05284895
 0.12183309 0.10234118 0.10340963 0.11490918 0.14633001], sum to 1.0000
[2019-04-09 14:45:46,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9167
[2019-04-09 14:45:46,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.05017181 0.09549318 0.07874545 0.07179237 0.06307477 0.04937441
 0.1284144  0.09879067 0.10313527 0.10857795 0.15242974], sum to 1.0000
[2019-04-09 14:45:46,425] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.25, 80.5, 0.0, 0.0, 22.5, 27.36642890419883, 0.8782182004733694, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1582200.0000, 
sim time next is 1582800.0000, 
raw observation next is [5.166666666666666, 81.0, 0.0, 0.0, 22.5, 27.23986600688131, 0.9103475487362983, 1.0, 1.0, 65.0, 55.86552622248111], 
processed observation next is [1.0, 0.30434782608695654, 0.6057248384118191, 0.81, 0.0, 0.0, 0.375, 0.7699888339067759, 0.8034491829120994, 1.0, 1.0, 1.0, 0.5586552622248111], 
reward next is 0.4413, 
noisyNet noise sample is [array([0.72458404], dtype=float32), 0.7804043]. 
=============================================
[2019-04-09 14:45:46,431] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7511
[2019-04-09 14:45:46,441] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.95, 82.33333333333334, 0.0, 0.0, 19.0, 27.21857463386268, 0.9269672125741031, 0.0, 1.0, 35.0, 27.82332292606086], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1576200.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.32007302605253, 0.9392461099973949, 0.0, 1.0, 60.0, 26.01663449463179], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7766727521710441, 0.8130820366657984, 0.0, 1.0, 0.9, 0.2601663449463179], 
reward next is 0.7398, 
noisyNet noise sample is [array([-0.10742308], dtype=float32), 1.6082517]. 
=============================================
[2019-04-09 14:45:46,585] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04281173 0.10072391 0.07124171 0.06546467 0.05892345 0.04947577
 0.12856087 0.09871981 0.10086706 0.11408952 0.16912147], sum to 1.0000
[2019-04-09 14:45:46,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7517
[2019-04-09 14:45:46,603] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.5, 85.5, 0.0, 0.0, 19.0, 27.39528354287701, 0.9572204410350805, 0.0, 1.0, 50.0, 26.70859376716844], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1567800.0000, 
sim time next is 1568400.0000, 
raw observation next is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 19.0, 27.34776830863656, 0.9637642279494821, 0.0, 1.0, 60.0, 35.31131905000755], 
processed observation next is [1.0, 0.13043478260869565, 0.5881809787626964, 0.8533333333333334, 0.0, 0.0, 0.08333333333333333, 0.77898069238638, 0.8212547426498275, 0.0, 1.0, 0.9, 0.3531131905000755], 
reward next is 0.6469, 
noisyNet noise sample is [array([-1.1053772], dtype=float32), 0.83616674]. 
=============================================
[2019-04-09 14:45:46,628] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03556562 0.09978502 0.07818084 0.06038838 0.06083047 0.04515434
 0.13419183 0.11352164 0.08250674 0.10438113 0.18549404], sum to 1.0000
[2019-04-09 14:45:46,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8741
[2019-04-09 14:45:46,650] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.3176004454054, 1.001114150251761, 0.0, 1.0, 45.0, 28.32110841234463], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1557600.0000, 
sim time next is 1558200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.35715169122448, 1.009464789372606, 0.0, 1.0, 65.0, 33.04414403632289], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7797626409353734, 0.836488263124202, 0.0, 1.0, 1.0, 0.3304414403632289], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.758539], dtype=float32), -1.8658589]. 
=============================================
[2019-04-09 14:45:46,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02944341 0.14711729 0.08522294 0.10448841 0.06438721 0.03965084
 0.10268965 0.08394041 0.13291937 0.08277419 0.12736624], sum to 1.0000
[2019-04-09 14:45:46,888] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2557
[2019-04-09 14:45:46,898] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 22.5, 28.44337812589018, 1.151051500343145, 1.0, 1.0, 40.0, 15.97963596692601], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 22.5, 28.42436341692234, 1.169175897800606, 1.0, 1.0, 25.0, 15.51962458857645], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.375, 0.868696951410195, 0.8897252992668686, 1.0, 1.0, 0.2, 0.1551962458857645], 
reward next is 0.8448, 
noisyNet noise sample is [array([-0.38893008], dtype=float32), -0.29045922]. 
=============================================
[2019-04-09 14:45:47,283] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0210517  0.14851815 0.08971266 0.0885831  0.05777769 0.04499508
 0.11601376 0.10980427 0.10572191 0.07041071 0.147411  ], sum to 1.0000
[2019-04-09 14:45:47,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1319
[2019-04-09 14:45:47,294] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 22.5, 28.95133172027966, 1.271071666301539, 1.0, 1.0, 40.0, 8.737122471563346], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 22.5, 28.92489633997794, 1.277615363555583, 1.0, 1.0, 20.0, 10.28180358396902], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 0.375, 0.9104080283314951, 0.925871787851861, 1.0, 1.0, 0.1, 0.10281803583969021], 
reward next is 0.8972, 
noisyNet noise sample is [array([-0.6835154], dtype=float32), -0.866897]. 
=============================================
[2019-04-09 14:45:47,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02864722 0.14353876 0.08137096 0.09238641 0.06677318 0.04149363
 0.10127883 0.10662508 0.12014694 0.06082718 0.15691188], sum to 1.0000
[2019-04-09 14:45:47,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7670
[2019-04-09 14:45:47,408] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 22.5, 28.98047544618784, 1.288591779392038, 1.0, 1.0, 30.0, 8.107993627354002], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [13.55, 50.0, 78.0, 0.0, 22.5, 29.04522759324256, 1.289736212079997, 1.0, 1.0, 55.0, 6.515377626678504], 
processed observation next is [1.0, 0.6521739130434783, 0.8379501385041552, 0.5, 0.26, 0.0, 0.375, 0.9204356327702135, 0.9299120706933323, 1.0, 1.0, 0.8, 0.06515377626678504], 
reward next is 0.9348, 
noisyNet noise sample is [array([-0.20274048], dtype=float32), -0.7059877]. 
=============================================
[2019-04-09 14:45:47,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.63528216]
 [0.6882762 ]
 [0.68659675]
 [0.61378026]
 [0.74863183]], R is [[1.62884164]
 [2.53147316]
 [3.41211987]
 [4.27841091]
 [5.12797546]].
[2019-04-09 14:45:47,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03426662 0.1136071  0.07956562 0.08662618 0.07115525 0.04649135
 0.10986051 0.08709943 0.12445738 0.09486417 0.15200634], sum to 1.0000
[2019-04-09 14:45:47,454] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7510
[2019-04-09 14:45:47,465] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 22.5, 27.77473125212307, 1.013611281891929, 1.0, 1.0, 55.0, 27.82078528366463], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1588200.0000, 
sim time next is 1588800.0000, 
raw observation next is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 22.5, 27.85031207016928, 1.035598478428881, 1.0, 1.0, 55.0, 16.39093776401665], 
processed observation next is [1.0, 0.391304347826087, 0.6555863342566944, 0.7333333333333334, 0.34, 0.13167587476979745, 0.375, 0.8208593391807734, 0.8451994928096269, 1.0, 1.0, 0.8, 0.16390937764016647], 
reward next is 0.8361, 
noisyNet noise sample is [array([-1.4495404], dtype=float32), -2.033825]. 
=============================================
[2019-04-09 14:45:47,515] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02495471 0.16023687 0.08473035 0.08408817 0.0599333  0.04020547
 0.09596692 0.09277479 0.13791318 0.06908822 0.15010805], sum to 1.0000
[2019-04-09 14:45:47,524] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9451
[2019-04-09 14:45:47,544] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 49.0, 155.3333333333333, 0.0, 22.5, 28.42336868143446, 1.29061678856746, 1.0, 1.0, 40.0, 11.3317651732924], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1606200.0000, 
sim time next is 1606800.0000, 
raw observation next is [13.8, 49.0, 150.1666666666667, 0.0, 22.5, 28.12849856287317, 1.226482655639138, 1.0, 1.0, 65.0, 2.571034235321387], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5005555555555558, 0.0, 0.375, 0.8440415469060975, 0.9088275518797126, 1.0, 1.0, 1.0, 0.02571034235321387], 
reward next is 0.9743, 
noisyNet noise sample is [array([2.0114713], dtype=float32), 1.2901906]. 
=============================================
[2019-04-09 14:45:47,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0373769  0.12897357 0.08327235 0.07756534 0.06153547 0.04227269
 0.12061178 0.0891943  0.11429276 0.09945703 0.14544773], sum to 1.0000
[2019-04-09 14:45:47,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9594
[2019-04-09 14:45:47,838] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 19.0, 20.0, 22.5, 27.19631816312004, 0.9143122698746886, 1.0, 1.0, 45.0, 29.12049397898165], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1584000.0000, 
sim time next is 1584600.0000, 
raw observation next is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 22.5, 27.19112798894986, 0.9249188397968298, 1.0, 1.0, 65.0, 35.89648656912784], 
processed observation next is [1.0, 0.34782608695652173, 0.6084949215143122, 0.8100000000000002, 0.08333333333333333, 0.027624309392265192, 0.375, 0.7659273324124882, 0.8083062799322766, 1.0, 1.0, 1.0, 0.3589648656912784], 
reward next is 0.6410, 
noisyNet noise sample is [array([-0.75003475], dtype=float32), 0.20029695]. 
=============================================
[2019-04-09 14:45:47,902] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02418435 0.13552298 0.08086885 0.09393733 0.05863613 0.0406323
 0.10590988 0.09692707 0.13725969 0.08588618 0.14023523], sum to 1.0000
[2019-04-09 14:45:47,904] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2821
[2019-04-09 14:45:47,911] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.75, 52.5, 50.0, 37.0, 22.5, 28.73344430555056, 1.265810448290731, 1.0, 1.0, 55.0, 7.466127967739527], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1614600.0000, 
sim time next is 1615200.0000, 
raw observation next is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 22.5, 29.00192259424621, 1.279811218512573, 1.0, 1.0, 30.0, 4.602147201201975], 
processed observation next is [1.0, 0.6956521739130435, 0.8107109879963068, 0.53, 0.13944444444444448, 0.03406998158379374, 0.375, 0.9168268828538508, 0.926603739504191, 1.0, 1.0, 0.3, 0.04602147201201975], 
reward next is 0.9540, 
noisyNet noise sample is [array([-0.14274694], dtype=float32), -0.69563687]. 
=============================================
[2019-04-09 14:45:48,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02375533 0.12156124 0.10303757 0.08750214 0.06477421 0.03729934
 0.10509871 0.10781562 0.12726103 0.07330392 0.14859085], sum to 1.0000
[2019-04-09 14:45:48,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9937
[2019-04-09 14:45:48,014] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 22.5, 29.13933839125609, 1.294357328071412, 1.0, 1.0, 45.0, 7.617801181413143], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1613400.0000, 
sim time next is 1614000.0000, 
raw observation next is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 22.5, 29.1568522265165, 1.298103736139787, 1.0, 1.0, 65.0, 8.748457848270927], 
processed observation next is [1.0, 0.6956521739130435, 0.8208679593721148, 0.52, 0.18222222222222223, 0.03406998158379374, 0.375, 0.9297376855430416, 0.9327012453799289, 1.0, 1.0, 1.0, 0.08748457848270927], 
reward next is 0.9125, 
noisyNet noise sample is [array([-1.3063889], dtype=float32), -0.7556605]. 
=============================================
[2019-04-09 14:45:48,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.84291196]
 [0.7736773 ]
 [0.7948556 ]
 [0.7793362 ]
 [0.8471942 ]], R is [[1.68187618]
 [2.58887935]
 [3.49016428]
 [4.39005518]
 [5.23984957]].
[2019-04-09 14:45:48,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02885886 0.11555833 0.084457   0.07436348 0.06479251 0.03611593
 0.1369603  0.10822496 0.09585394 0.09132018 0.1634945 ], sum to 1.0000
[2019-04-09 14:45:48,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5842
[2019-04-09 14:45:48,055] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 22.5, 27.97924147202199, 1.170400677287338, 1.0, 1.0, 20.0, 19.83777172777901], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 22.5, 27.96332276934254, 1.164071522944458, 1.0, 1.0, 50.0, 21.01654729136064], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.375, 0.8302768974452116, 0.8880238409814861, 1.0, 1.0, 0.7, 0.2101654729136064], 
reward next is 0.7898, 
noisyNet noise sample is [array([0.72345966], dtype=float32), -0.7854883]. 
=============================================
[2019-04-09 14:45:48,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03415992 0.10143152 0.0783878  0.06414231 0.06173526 0.04609701
 0.13551259 0.12053448 0.08026402 0.10667768 0.17105748], sum to 1.0000
[2019-04-09 14:45:48,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-09 14:45:48,120] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 82.0, 0.0, 0.0, 19.0, 27.40519199756094, 1.013268705184864, 0.0, 1.0, 55.0, 29.7614497272839], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1554600.0000, 
sim time next is 1555200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.36816104054985, 1.007072331628911, 0.0, 1.0, 45.0, 30.0608380611015], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7806800867124876, 0.835690777209637, 0.0, 1.0, 0.6, 0.30060838061101497], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.740918], dtype=float32), -0.21574591]. 
=============================================
[2019-04-09 14:45:48,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0317607  0.11134951 0.08286221 0.07020187 0.07381043 0.04281459
 0.12268219 0.12301198 0.09042774 0.08162472 0.16945398], sum to 1.0000
[2019-04-09 14:45:48,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3641
[2019-04-09 14:45:48,233] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 76.0, 0.0, 0.0, 19.0, 27.81888156352463, 1.139504892402941, 0.0, 1.0, 55.0, 26.45959917198922], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1630800.0000, 
sim time next is 1631400.0000, 
raw observation next is [7.1, 77.66666666666667, 0.0, 0.0, 19.0, 27.804091428758, 1.13536896672888, 0.0, 1.0, 35.0, 20.99290001136935], 
processed observation next is [1.0, 0.9130434782608695, 0.6592797783933518, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8170076190631667, 0.8784563222429599, 0.0, 1.0, 0.4, 0.2099290001136935], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.23358795], dtype=float32), -0.1811254]. 
=============================================
[2019-04-09 14:45:48,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02913513 0.11983429 0.09201819 0.06918692 0.06374982 0.04410918
 0.11607422 0.09590846 0.11658999 0.08291242 0.17048143], sum to 1.0000
[2019-04-09 14:45:48,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7538
[2019-04-09 14:45:48,365] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 22.5, 27.96856872826513, 1.166683718008982, 1.0, 1.0, 50.0, 19.65187997648661], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 22.5, 27.95918786120467, 1.16078941809615, 1.0, 1.0, 60.0, 23.11911727016907], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.375, 0.8299323217670557, 0.88692980603205, 1.0, 1.0, 0.9, 0.2311911727016907], 
reward next is 0.7688, 
noisyNet noise sample is [array([-0.67009604], dtype=float32), 0.28365934]. 
=============================================
[2019-04-09 14:45:48,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02800963 0.11581037 0.08313327 0.07350208 0.06825882 0.04393982
 0.12709403 0.10759138 0.10104387 0.09108242 0.16053429], sum to 1.0000
[2019-04-09 14:45:48,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2393
[2019-04-09 14:45:48,749] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.9, 81.0, 0.0, 0.0, 19.0, 27.76200457799535, 1.138032412352439, 0.0, 1.0, 40.0, 23.70457780288366], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1632600.0000, 
sim time next is 1633200.0000, 
raw observation next is [6.8, 82.66666666666666, 0.0, 0.0, 19.0, 27.76719294160033, 1.133958850543322, 0.0, 1.0, 30.0, 22.9547125299035], 
processed observation next is [1.0, 0.9130434782608695, 0.6509695290858727, 0.8266666666666665, 0.0, 0.0, 0.08333333333333333, 0.813932745133361, 0.8779862835144406, 0.0, 1.0, 0.3, 0.22954712529903498], 
reward next is 0.7705, 
noisyNet noise sample is [array([-0.28288618], dtype=float32), 0.19146837]. 
=============================================
[2019-04-09 14:45:49,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03070756 0.14098158 0.08625907 0.09864468 0.05690141 0.04285471
 0.0974381  0.10768751 0.12458316 0.07165757 0.14228463], sum to 1.0000
[2019-04-09 14:45:49,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6253
[2019-04-09 14:45:49,026] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 22.5, 28.54396195037781, 1.193889246763637, 1.0, 1.0, 35.0, 11.9578396231882], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1596000.0000, 
sim time next is 1596600.0000, 
raw observation next is [10.5, 59.0, 216.0, 249.0, 22.5, 28.62612184668955, 1.214054426481471, 1.0, 1.0, 50.0, 11.56803474741371], 
processed observation next is [1.0, 0.4782608695652174, 0.7534626038781165, 0.59, 0.72, 0.2751381215469613, 0.375, 0.8855101538907958, 0.904684808827157, 1.0, 1.0, 0.7, 0.11568034747413711], 
reward next is 0.8843, 
noisyNet noise sample is [array([0.51879364], dtype=float32), 2.1929865]. 
=============================================
[2019-04-09 14:45:49,268] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04465158 0.08817736 0.08797333 0.06051005 0.06211917 0.05888386
 0.1321631  0.10786037 0.08258663 0.10671429 0.1683603 ], sum to 1.0000
[2019-04-09 14:45:49,270] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0980
[2019-04-09 14:45:49,281] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.9, 94.5, 0.0, 0.0, 19.0, 27.56803843889491, 1.050281105888568, 0.0, 1.0, 35.0, 26.94800906254419], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1647000.0000, 
sim time next is 1647600.0000, 
raw observation next is [7.0, 95.0, 0.0, 0.0, 19.0, 27.64257932783372, 1.055069413416089, 0.0, 1.0, 20.0, 23.03059152736678], 
processed observation next is [1.0, 0.043478260869565216, 0.6565096952908588, 0.95, 0.0, 0.0, 0.08333333333333333, 0.8035482773194765, 0.8516898044720297, 0.0, 1.0, 0.1, 0.2303059152736678], 
reward next is 0.7697, 
noisyNet noise sample is [array([0.85687006], dtype=float32), -0.86817056]. 
=============================================
[2019-04-09 14:45:49,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04159685 0.10086022 0.08359782 0.06566    0.06366876 0.05261137
 0.12188339 0.09809583 0.10247245 0.1232766  0.1462768 ], sum to 1.0000
[2019-04-09 14:45:49,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3817
[2019-04-09 14:45:49,306] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.47412484631009, 1.020304725212255, 0.0, 1.0, 65.0, 32.13422533031665], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1656600.0000, 
sim time next is 1657200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.52183286680089, 1.023793411471785, 0.0, 1.0, 40.0, 30.84179651440208], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7934860722334074, 0.841264470490595, 0.0, 1.0, 0.5, 0.3084179651440208], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.29812154], dtype=float32), -1.2419775]. 
=============================================
[2019-04-09 14:45:49,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03788261 0.09853391 0.08980764 0.06625509 0.06601299 0.05225797
 0.12967618 0.11309303 0.08903792 0.10411847 0.15332423], sum to 1.0000
[2019-04-09 14:45:49,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5785
[2019-04-09 14:45:49,704] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.200000000000001, 83.33333333333334, 0.0, 0.0, 19.0, 27.61276149750951, 1.064893153499452, 0.0, 1.0, 45.0, 30.84697130729554], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1639200.0000, 
sim time next is 1639800.0000, 
raw observation next is [7.2, 84.0, 0.0, 0.0, 19.0, 27.58866162474571, 1.057312811544001, 0.0, 1.0, 50.0, 26.74058125284262], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.84, 0.0, 0.0, 0.08333333333333333, 0.7990551353954759, 0.8524376038480003, 0.0, 1.0, 0.7, 0.2674058125284262], 
reward next is 0.7326, 
noisyNet noise sample is [array([2.407853], dtype=float32), -0.4002348]. 
=============================================
[2019-04-09 14:45:49,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04187156 0.09457853 0.08698353 0.06537905 0.0590096  0.05061976
 0.11649758 0.09336829 0.10896785 0.12219858 0.16052572], sum to 1.0000
[2019-04-09 14:45:49,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6817
[2019-04-09 14:45:49,785] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.51588635818989, 1.01511792556927, 0.0, 1.0, 65.0, 30.15761042299682], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1659600.0000, 
sim time next is 1660200.0000, 
raw observation next is [6.416666666666667, 97.0, 0.0, 0.0, 19.0, 27.53294985427032, 1.022501235030528, 0.0, 1.0, 55.0, 28.06640853504406], 
processed observation next is [1.0, 0.21739130434782608, 0.6403508771929826, 0.97, 0.0, 0.0, 0.08333333333333333, 0.79441248785586, 0.840833745010176, 0.0, 1.0, 0.8, 0.28066408535044063], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.11779786], dtype=float32), -1.994114]. 
=============================================
[2019-04-09 14:45:49,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04093508 0.07620048 0.08519117 0.0528403  0.05729721 0.05153363
 0.13120644 0.10015458 0.09323347 0.12238253 0.18902512], sum to 1.0000
[2019-04-09 14:45:49,800] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5278
[2019-04-09 14:45:49,810] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 27.65191609193511, 1.05863978705509, 0.0, 1.0, 35.0, 28.81877437438927], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1648800.0000, 
sim time next is 1649400.0000, 
raw observation next is [7.1, 96.16666666666666, 0.0, 0.0, 19.0, 27.64452017768044, 1.051001112354263, 0.0, 1.0, 50.0, 24.81357030023333], 
processed observation next is [1.0, 0.08695652173913043, 0.6592797783933518, 0.9616666666666666, 0.0, 0.0, 0.08333333333333333, 0.8037100148067035, 0.8503337041180877, 0.0, 1.0, 0.7, 0.2481357030023333], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.2538744], dtype=float32), -0.33914822]. 
=============================================
[2019-04-09 14:45:50,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03366016 0.10768681 0.09687156 0.06601499 0.06342951 0.04788364
 0.13280715 0.10658138 0.07742805 0.09934661 0.16829012], sum to 1.0000
[2019-04-09 14:45:50,093] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-09 14:45:50,112] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.0, 83.33333333333334, 0.0, 0.0, 19.0, 27.61480093185851, 1.082775367522738, 0.0, 1.0, 35.0, 25.53628907649037], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1636800.0000, 
sim time next is 1637400.0000, 
raw observation next is [7.1, 82.66666666666667, 0.0, 0.0, 19.0, 27.64175152820359, 1.07199005917417, 0.0, 1.0, 50.0, 26.56895464618805], 
processed observation next is [1.0, 0.9565217391304348, 0.6592797783933518, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.8034792940169657, 0.8573300197247233, 0.0, 1.0, 0.7, 0.2656895464618805], 
reward next is 0.7343, 
noisyNet noise sample is [array([-0.88282335], dtype=float32), 0.9491462]. 
=============================================
[2019-04-09 14:45:50,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03832054 0.08860391 0.08123863 0.06466435 0.05916653 0.05390001
 0.11847986 0.08846196 0.11130852 0.11110733 0.18474841], sum to 1.0000
[2019-04-09 14:45:50,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6736
[2019-04-09 14:45:50,211] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.40921751829384, 1.005845852252071, 0.0, 1.0, 50.0, 27.28234868854665], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1656600.0000, 
sim time next is 1657200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.48933788376259, 1.011769337191659, 0.0, 1.0, 55.0, 26.57335874686872], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7907781569802159, 0.8372564457305529, 0.0, 1.0, 0.8, 0.2657335874686872], 
reward next is 0.7343, 
noisyNet noise sample is [array([1.5453783], dtype=float32), -1.0880185]. 
=============================================
[2019-04-09 14:45:50,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03366847 0.09389774 0.08966483 0.06109784 0.05836476 0.0460138
 0.12698713 0.11980996 0.08171713 0.10555677 0.18322158], sum to 1.0000
[2019-04-09 14:45:50,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6773
[2019-04-09 14:45:50,329] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.2, 86.0, 0.0, 0.0, 19.0, 27.57980436802524, 1.053599730087442, 0.0, 1.0, 55.0, 33.69682962628774], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1641600.0000, 
sim time next is 1642200.0000, 
raw observation next is [7.1, 87.16666666666667, 0.0, 0.0, 19.0, 27.55318722334805, 1.048923178177639, 0.0, 1.0, 55.0, 31.64759495176686], 
processed observation next is [1.0, 0.0, 0.6592797783933518, 0.8716666666666667, 0.0, 0.0, 0.08333333333333333, 0.7960989352790042, 0.8496410593925464, 0.0, 1.0, 0.8, 0.3164759495176686], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.50246763], dtype=float32), 0.19979905]. 
=============================================
[2019-04-09 14:45:50,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03502313 0.12089349 0.08293135 0.08019157 0.05908621 0.04448691
 0.11269091 0.09836383 0.1053892  0.09592299 0.16502044], sum to 1.0000
[2019-04-09 14:45:50,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8678
[2019-04-09 14:45:50,363] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 22.5, 27.55092163294456, 0.9968492194200529, 1.0, 1.0, 20.0, 22.00365875038903], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 22.5, 27.62981930017541, 1.01664075833821, 1.0, 1.0, 65.0, 33.28432498576007], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.375, 0.8024849416812841, 0.8388802527794033, 1.0, 1.0, 1.0, 0.3328432498576007], 
reward next is 0.6672, 
noisyNet noise sample is [array([0.15566212], dtype=float32), -0.6390541]. 
=============================================
[2019-04-09 14:45:50,404] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04136147 0.09215708 0.08461522 0.0651589  0.06571788 0.05774212
 0.12360274 0.10331249 0.10939629 0.11368811 0.14324774], sum to 1.0000
[2019-04-09 14:45:50,408] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4221
[2019-04-09 14:45:50,421] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.53764325796063, 1.028264388298066, 0.0, 1.0, 50.0, 28.87435308755988], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.55366479007751, 1.020239436102812, 0.0, 1.0, 45.0, 25.78874860239133], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7961387325064591, 0.8400798120342706, 0.0, 1.0, 0.6, 0.2578874860239133], 
reward next is 0.7421, 
noisyNet noise sample is [array([0.03889315], dtype=float32), -0.58785766]. 
=============================================
[2019-04-09 14:45:50,479] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03119489 0.12529477 0.08618589 0.10153691 0.06309942 0.03903151
 0.09701639 0.08791374 0.14131542 0.08407182 0.14333916], sum to 1.0000
[2019-04-09 14:45:50,479] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7929
[2019-04-09 14:45:50,497] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 22.5, 27.9891290765012, 1.055360791640729, 1.0, 1.0, 35.0, 24.14264887975682], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1676400.0000, 
sim time next is 1677000.0000, 
raw observation next is [1.616666666666667, 92.0, 57.33333333333333, 0.0, 22.5, 28.03573516492262, 1.060730912985658, 1.0, 1.0, 60.0, 27.77908830203552], 
processed observation next is [1.0, 0.391304347826087, 0.5073868882733149, 0.92, 0.1911111111111111, 0.0, 0.375, 0.8363112637435517, 0.8535769709952193, 1.0, 1.0, 0.9, 0.2777908830203552], 
reward next is 0.7222, 
noisyNet noise sample is [array([-1.3347918], dtype=float32), 2.2127898]. 
=============================================
[2019-04-09 14:45:50,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.6884477 ]
 [0.53870845]
 [0.61436653]
 [0.521037  ]
 [0.64950156]], R is [[1.28053987]
 [2.02630806]
 [2.77591896]
 [3.52178669]
 [4.1982646 ]].
[2019-04-09 14:45:50,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0311371  0.12776972 0.08535255 0.0907836  0.05971872 0.04473958
 0.09790986 0.08804037 0.14479825 0.07685035 0.15289989], sum to 1.0000
[2019-04-09 14:45:50,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-09 14:45:50,570] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 22.5, 28.06700915360253, 1.063744844494967, 1.0, 1.0, 50.0, 25.80065704197976], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 22.5, 28.08319943548222, 1.060668241551477, 1.0, 1.0, 55.0, 24.3882334733733], 
processed observation next is [1.0, 0.43478260869565216, 0.502308402585411, 0.92, 0.20555555555555557, 0.0, 0.375, 0.8402666196235185, 0.853556080517159, 1.0, 1.0, 0.8, 0.243882334733733], 
reward next is 0.7561, 
noisyNet noise sample is [array([1.20467], dtype=float32), 0.4226175]. 
=============================================
[2019-04-09 14:45:50,641] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02853989 0.12474588 0.08499745 0.08167315 0.06459934 0.03548983
 0.10022898 0.10698827 0.127112   0.07471925 0.17090598], sum to 1.0000
[2019-04-09 14:45:50,641] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-09 14:45:50,655] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 22.5, 29.17122652307136, 1.30441544357749, 1.0, 1.0, 35.0, 7.845002926303681], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 22.5, 29.17869514271063, 1.294188204468422, 1.0, 1.0, 55.0, 7.878845338327117], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.375, 0.9315579285592192, 0.9313960681561406, 1.0, 1.0, 0.8, 0.07878845338327117], 
reward next is 0.9212, 
noisyNet noise sample is [array([0.430787], dtype=float32), -0.64903]. 
=============================================
[2019-04-09 14:45:50,763] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02526383 0.13465716 0.09329579 0.09110711 0.05647799 0.04239828
 0.11440934 0.10145196 0.11552412 0.07380329 0.1516111 ], sum to 1.0000
[2019-04-09 14:45:50,764] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1436
[2019-04-09 14:45:50,774] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.63333333333333, 56.33333333333334, 0.0, 0.0, 22.5, 28.13896723399134, 1.192097835166783, 1.0, 1.0, 55.0, 9.18536083160717], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1617600.0000, 
sim time next is 1618200.0000, 
raw observation next is [11.35, 57.5, 0.0, 0.0, 22.5, 28.45082076344031, 1.207682804741008, 1.0, 1.0, 20.0, 11.94658906356512], 
processed observation next is [1.0, 0.7391304347826086, 0.7770083102493075, 0.575, 0.0, 0.0, 0.375, 0.8709017302866924, 0.9025609349136694, 1.0, 1.0, 0.1, 0.1194658906356512], 
reward next is 0.8805, 
noisyNet noise sample is [array([0.71173465], dtype=float32), -0.15976614]. 
=============================================
[2019-04-09 14:45:50,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02398595 0.12522133 0.08626497 0.09669268 0.06819364 0.04061819
 0.10868583 0.08676483 0.13924123 0.08691398 0.13741729], sum to 1.0000
[2019-04-09 14:45:50,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4649
[2019-04-09 14:45:50,927] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 85.33333333333334, 103.0, 0.0, 22.5, 28.11251067813458, 1.083177970134882, 1.0, 1.0, 65.0, 28.00204360127866], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 22.5, 28.03699298147451, 1.078589624253687, 1.0, 1.0, 45.0, 30.95122433047626], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.375, 0.8364160817895424, 0.859529874751229, 1.0, 1.0, 0.6, 0.3095122433047626], 
reward next is 0.6905, 
noisyNet noise sample is [array([-0.48526657], dtype=float32), -0.07732808]. 
=============================================
[2019-04-09 14:45:50,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.03905884 0.1181485  0.09446898 0.08770612 0.0628699  0.05100504
 0.09527779 0.08674961 0.1124885  0.09870974 0.15351695], sum to 1.0000
[2019-04-09 14:45:50,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3959
[2019-04-09 14:45:50,970] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 92.0, 41.5, 0.0, 22.5, 27.72000721573147, 0.9876787143985406, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1674000.0000, 
sim time next is 1674600.0000, 
raw observation next is [2.083333333333333, 92.0, 45.33333333333334, 0.0, 22.5, 27.69324594298206, 1.023502512926369, 1.0, 1.0, 35.0, 39.44987781692475], 
processed observation next is [1.0, 0.391304347826087, 0.5203139427516159, 0.92, 0.15111111111111114, 0.0, 0.375, 0.807770495248505, 0.8411675043087897, 1.0, 1.0, 0.4, 0.3944987781692475], 
reward next is 0.6055, 
noisyNet noise sample is [array([1.1431494], dtype=float32), 0.8706768]. 
=============================================
[2019-04-09 14:45:51,069] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02139241 0.13767838 0.08157619 0.10074574 0.06406552 0.03961017
 0.11585374 0.08423044 0.12022334 0.08332501 0.15129915], sum to 1.0000
[2019-04-09 14:45:51,072] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1837
[2019-04-09 14:45:51,086] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 88.0, 100.0, 0.0, 22.5, 27.95661748237745, 1.085476877552297, 1.0, 1.0, 65.0, 25.46508129169798], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1690200.0000, 
sim time next is 1690800.0000, 
raw observation next is [1.1, 88.0, 96.66666666666667, 0.0, 22.5, 28.01063355483126, 1.095097899424977, 1.0, 1.0, 25.0, 27.44258037473461], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.32222222222222224, 0.0, 0.375, 0.8342194629026048, 0.865032633141659, 1.0, 1.0, 0.2, 0.2744258037473461], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.12964949], dtype=float32), 0.13858798]. 
=============================================
[2019-04-09 14:45:51,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02072137 0.13231386 0.08582588 0.08493808 0.05502182 0.03578897
 0.11635527 0.08180618 0.13462284 0.08945476 0.16315098], sum to 1.0000
[2019-04-09 14:45:51,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4966
[2019-04-09 14:45:51,227] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 93.33333333333333, 0.0, 22.5, 27.3467048316219, 1.091359136007203, 1.0, 1.0, 45.0, 33.13947956820051], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1691400.0000, 
sim time next is 1692000.0000, 
raw observation next is [1.1, 88.0, 90.0, 0.0, 22.5, 26.76351053899265, 0.9733754903865064, 1.0, 1.0, 45.0, 13.12548159738487], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.3, 0.0, 0.375, 0.7302925449160543, 0.8244584967955021, 1.0, 1.0, 0.6, 0.1312548159738487], 
reward next is 0.8687, 
noisyNet noise sample is [array([2.3293455], dtype=float32), 0.13551085]. 
=============================================
[2019-04-09 14:45:51,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.8493241 ]
 [0.83532405]
 [0.9384388 ]
 [0.76495135]
 [0.82196295]], R is [[1.71487236]
 [2.36632872]
 [2.64418817]
 [3.4012866 ]
 [4.08220482]].
[2019-04-09 14:45:51,413] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0219248  0.12523644 0.07875501 0.07717617 0.06339069 0.03467985
 0.10035247 0.09881156 0.13616976 0.0792282  0.184275  ], sum to 1.0000
[2019-04-09 14:45:51,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7462
[2019-04-09 14:45:51,458] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.516666666666667, 82.16666666666667, 37.66666666666667, 0.0, 22.5, 28.04765987179758, 1.058312523472735, 1.0, 1.0, 35.0, 23.16896245564172], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 28.06913112384791, 0.9024682324739857, 1.0, 1.0, 60.0, 66.25749340385728], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.375, 0.8390942603206591, 0.8008227441579953, 1.0, 1.0, 0.9, 0.6625749340385728], 
reward next is 0.3374, 
noisyNet noise sample is [array([0.5703494], dtype=float32), -0.21642044]. 
=============================================
[2019-04-09 14:45:51,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04765172 0.09579837 0.08241283 0.06890062 0.06527036 0.04905234
 0.12468196 0.09571378 0.1123254  0.11354293 0.14464973], sum to 1.0000
[2019-04-09 14:45:51,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7671
[2019-04-09 14:45:51,528] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.5364101226555, 1.027884709710873, 0.0, 1.0, 30.0, 28.78271501776902], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.55248209236025, 1.019871038158278, 0.0, 1.0, 60.0, 28.80884852740811], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7960401743633542, 0.839957012719426, 0.0, 1.0, 0.9, 0.2880884852740811], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.50586194], dtype=float32), 0.10730394]. 
=============================================
[2019-04-09 14:45:52,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02152392 0.13404132 0.08358143 0.0779314  0.06252272 0.03953472
 0.11384236 0.086484   0.10905527 0.08494403 0.1865388 ], sum to 1.0000
[2019-04-09 14:45:52,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6074
[2019-04-09 14:45:52,091] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 22.5, 27.22801046871096, 0.9561251253552537, 1.0, 1.0, 25.0, 33.85373762539801], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 22.5, 27.18752121288065, 0.9547541054704678, 1.0, 1.0, 50.0, 33.95870113925711], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.375, 0.7656267677400542, 0.8182513684901559, 1.0, 1.0, 0.7, 0.3395870113925711], 
reward next is 0.6604, 
noisyNet noise sample is [array([1.2709272], dtype=float32), -0.60984534]. 
=============================================
[2019-04-09 14:45:52,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02777395 0.12102237 0.08288928 0.09145501 0.06208033 0.0356678
 0.10060398 0.09288227 0.14610143 0.0868665  0.15265708], sum to 1.0000
[2019-04-09 14:45:52,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1509
[2019-04-09 14:45:52,266] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 22.5, 27.95179818341693, 1.050968496973063, 1.0, 1.0, 45.0, 27.07575062787156], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1698600.0000, 
sim time next is 1699200.0000, 
raw observation next is [1.6, 81.0, 41.5, 0.0, 22.5, 28.00623066477346, 1.058854896818752, 1.0, 1.0, 25.0, 23.45327104416302], 
processed observation next is [1.0, 0.6956521739130435, 0.5069252077562327, 0.81, 0.13833333333333334, 0.0, 0.375, 0.8338525553977885, 0.8529516322729173, 1.0, 1.0, 0.2, 0.2345327104416302], 
reward next is 0.7655, 
noisyNet noise sample is [array([1.1205586], dtype=float32), -1.1113808]. 
=============================================
[2019-04-09 14:45:52,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02814797 0.10937572 0.08785383 0.06910867 0.0606451  0.04427319
 0.12946598 0.09966721 0.08765756 0.10132943 0.18247539], sum to 1.0000
[2019-04-09 14:45:52,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0292
[2019-04-09 14:45:52,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04856903 0.09098503 0.08440513 0.06796399 0.06056711 0.04996205
 0.12919684 0.10482482 0.10359913 0.11150116 0.14842568], sum to 1.0000
[2019-04-09 14:45:52,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0012
[2019-04-09 14:45:52,617] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.51186870037826, 1.008410533666694, 0.0, 1.0, 30.0, 25.97627088359275], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1659600.0000, 
sim time next is 1660200.0000, 
raw observation next is [6.416666666666667, 97.0, 0.0, 0.0, 19.0, 27.52920568537316, 0.9731001278299645, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.6403508771929826, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7941004737810967, 0.8243667092766548, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26653644], dtype=float32), -1.0892255]. 
=============================================
[2019-04-09 14:45:52,619] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.67194960144438, 0.8237706496670305, 0.0, 1.0, 65.0, 78.52125468152434], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1724400.0000, 
sim time next is 1725000.0000, 
raw observation next is [0.08333333333333333, 94.5, 0.0, 0.0, 19.0, 26.59286951437717, 0.8317482064075215, 0.0, 1.0, 20.0, 46.35426105725268], 
processed observation next is [1.0, 1.0, 0.4649122807017544, 0.945, 0.0, 0.0, 0.08333333333333333, 0.7160724595314308, 0.7772494021358405, 0.0, 1.0, 0.1, 0.46354261057252677], 
reward next is 0.5365, 
noisyNet noise sample is [array([1.0302508], dtype=float32), -1.171195]. 
=============================================
[2019-04-09 14:45:52,628] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03750145 0.09422483 0.08868136 0.06056331 0.05186756 0.04579959
 0.13081732 0.09814038 0.08976031 0.10801069 0.19463316], sum to 1.0000
[2019-04-09 14:45:52,630] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[0.57238996]
 [0.5649853 ]
 [0.55526257]
 [0.6098875 ]
 [0.6614032 ]], R is [[1.01406074]
 [1.21870768]
 [2.20652056]
 [2.79168224]
 [3.30443883]].
[2019-04-09 14:45:52,632] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0720
[2019-04-09 14:45:52,650] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.3333333333333333, 93.0, 0.0, 0.0, 19.0, 26.60130593071379, 0.7862312298457245, 0.0, 1.0, 20.0, 30.00853724264419], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 19.0, 26.51605039099022, 0.7646063911899782, 0.0, 1.0, 20.0, 28.52582975096602], 
processed observation next is [1.0, 1.0, 0.47414589104339805, 0.925, 0.0, 0.0, 0.08333333333333333, 0.7096708659158516, 0.7548687970633261, 0.0, 1.0, 0.1, 0.2852582975096602], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.54229516], dtype=float32), -1.106562]. 
=============================================
[2019-04-09 14:45:52,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03127161 0.09934134 0.08836516 0.06461389 0.06055499 0.04682288
 0.13299344 0.11128201 0.08860014 0.10632213 0.16983235], sum to 1.0000
[2019-04-09 14:45:52,726] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2934
[2019-04-09 14:45:52,765] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.4166666666666667, 92.5, 0.0, 0.0, 19.0, 26.52181662849393, 0.7884756510579033, 0.0, 1.0, 25.0, 64.6077470626676], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1727400.0000, 
sim time next is 1728000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.45002086379905, 0.7903416848283652, 0.0, 1.0, 35.0, 37.58219822165894], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7041684053165875, 0.7634472282761218, 0.0, 1.0, 0.4, 0.3758219822165894], 
reward next is 0.6242, 
noisyNet noise sample is [array([-0.05172509], dtype=float32), -1.9963535]. 
=============================================
[2019-04-09 14:45:52,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[0.47371322]
 [0.41597968]
 [0.4929492 ]
 [0.3497209 ]
 [0.3977732 ]], R is [[0.85714906]
 [1.2025001 ]
 [2.19047499]
 [2.70069075]
 [3.29510069]].
[2019-04-09 14:45:52,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03731854 0.10231247 0.08110458 0.05882595 0.05477859 0.04373448
 0.1477192  0.11010832 0.07225866 0.10426129 0.18757789], sum to 1.0000
[2019-04-09 14:45:52,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-09 14:45:52,843] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.3333333333333333, 93.0, 0.0, 0.0, 19.0, 26.69495952534409, 0.8145790008763155, 0.0, 1.0, 20.0, 33.0791608346802], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 19.0, 26.64378745926398, 0.7968234878149122, 0.0, 1.0, 20.0, 32.0161271329066], 
processed observation next is [1.0, 1.0, 0.47414589104339805, 0.925, 0.0, 0.0, 0.08333333333333333, 0.7203156216053316, 0.7656078292716374, 0.0, 1.0, 0.1, 0.32016127132906597], 
reward next is 0.6798, 
noisyNet noise sample is [array([-2.1215239], dtype=float32), 0.5235641]. 
=============================================
[2019-04-09 14:45:52,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02742526 0.10425409 0.08885529 0.06769659 0.05442836 0.03762683
 0.14149794 0.10925432 0.10263103 0.08838641 0.17794389], sum to 1.0000
[2019-04-09 14:45:52,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04349944 0.10925065 0.08243717 0.0670239  0.05954826 0.04661296
 0.1317306  0.10022558 0.0972129  0.11512359 0.14733495], sum to 1.0000
[2019-04-09 14:45:52,896] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-09 14:45:52,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1192
[2019-04-09 14:45:52,908] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 19.0, 27.00185486673027, 0.9139060684497758, 0.0, 1.0, 65.0, 39.78504447591888], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1716000.0000, 
sim time next is 1716600.0000, 
raw observation next is [0.6, 91.33333333333334, 0.0, 0.0, 19.0, 27.02054627917013, 0.914348882435171, 0.0, 1.0, 55.0, 38.29695019309383], 
processed observation next is [1.0, 0.8695652173913043, 0.479224376731302, 0.9133333333333334, 0.0, 0.0, 0.08333333333333333, 0.7517121899308442, 0.8047829608117237, 0.0, 1.0, 0.8, 0.38296950193093826], 
reward next is 0.6170, 
noisyNet noise sample is [array([-2.3621888], dtype=float32), 1.1549358]. 
=============================================
[2019-04-09 14:45:52,928] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 22.5, 27.33641141087696, 0.9632462078583721, 0.0, 1.0, 60.0, 56.01946305956474], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 22.5, 27.3273090245862, 0.9670217388476471, 1.0, 1.0, 20.0, 31.71472992923018], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.375, 0.7772757520488499, 0.8223405796158824, 1.0, 1.0, 0.1, 0.3171472992923018], 
reward next is 0.6829, 
noisyNet noise sample is [array([0.70746285], dtype=float32), 0.39903358]. 
=============================================
[2019-04-09 14:45:52,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.4384568 ]
 [0.33658805]
 [0.4944072 ]
 [0.40309748]
 [0.50791657]], R is [[0.99600303]
 [1.42584836]
 [2.41158986]
 [3.11255741]
 [3.77776456]].
[2019-04-09 14:45:53,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05035616 0.0949694  0.0772908  0.07000911 0.06801153 0.04902712
 0.12213736 0.11574017 0.07712884 0.12871338 0.14661616], sum to 1.0000
[2019-04-09 14:45:53,315] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7707
[2019-04-09 14:45:53,335] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.61654252801493, 0.7777572835026759, 0.0, 1.0, 45.0, 35.03678339299861], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1731600.0000, 
sim time next is 1732200.0000, 
raw observation next is [0.45, 91.83333333333334, 0.0, 0.0, 19.0, 26.57526363373356, 0.778523427322526, 0.0, 1.0, 60.0, 48.45751575389534], 
processed observation next is [0.0, 0.043478260869565216, 0.47506925207756234, 0.9183333333333334, 0.0, 0.0, 0.08333333333333333, 0.71460530281113, 0.7595078091075087, 0.0, 1.0, 0.9, 0.4845751575389534], 
reward next is 0.5154, 
noisyNet noise sample is [array([-0.48154435], dtype=float32), 0.13749509]. 
=============================================
[2019-04-09 14:45:53,724] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03191045 0.1240677  0.08984043 0.09481864 0.05975447 0.03719135
 0.10302719 0.08873911 0.12260108 0.07896593 0.16908367], sum to 1.0000
[2019-04-09 14:45:53,725] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5575
[2019-04-09 14:45:53,742] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 22.5, 28.0549763051171, 1.078036469598903, 1.0, 1.0, 20.0, 24.74840621965459], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1681800.0000, 
sim time next is 1682400.0000, 
raw observation next is [1.1, 89.33333333333334, 80.16666666666667, 0.0, 22.5, 28.05182513939892, 1.082375442704437, 1.0, 1.0, 50.0, 26.36632004245786], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.8933333333333334, 0.26722222222222225, 0.0, 0.375, 0.8376520949499101, 0.8607918142348123, 1.0, 1.0, 0.7, 0.26366320042457864], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.30554956], dtype=float32), -0.9616353]. 
=============================================
[2019-04-09 14:45:53,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03815237 0.08789371 0.08098923 0.06048992 0.05403519 0.04108334
 0.13748504 0.11389395 0.10426155 0.11357605 0.16813967], sum to 1.0000
[2019-04-09 14:45:53,817] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6668
[2019-04-09 14:45:53,835] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.72556317858029, 0.8308157423769534, 0.0, 1.0, 20.0, 39.00698132919508], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1728600.0000, 
sim time next is 1729200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.7235898812932, 0.8255068165765743, 0.0, 1.0, 55.0, 43.80672344499183], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7269658234411001, 0.7751689388588581, 0.0, 1.0, 0.8, 0.4380672344499183], 
reward next is 0.5619, 
noisyNet noise sample is [array([0.93200845], dtype=float32), 0.6136903]. 
=============================================
[2019-04-09 14:45:53,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06358469 0.10714677 0.08357326 0.08871295 0.07730866 0.07562766
 0.0967075  0.08448702 0.07686808 0.12887885 0.11710455], sum to 1.0000
[2019-04-09 14:45:53,844] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7275
[2019-04-09 14:45:53,871] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 19.0, 26.37532828145932, 0.7252138799512527, 0.0, 1.0, 35.0, 37.51484891557126], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 19.0, 26.42832397401281, 0.7164544523183983, 0.0, 1.0, 55.0, 43.53515899751054], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7023603311677341, 0.7388181507727994, 0.0, 1.0, 0.8, 0.4353515899751054], 
reward next is 0.5646, 
noisyNet noise sample is [array([0.90374625], dtype=float32), 0.2640699]. 
=============================================
[2019-04-09 14:45:53,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04220176 0.09126785 0.06690585 0.07350859 0.06418243 0.05368966
 0.12943585 0.10895367 0.08097416 0.13098149 0.15789874], sum to 1.0000
[2019-04-09 14:45:53,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4373
[2019-04-09 14:45:53,933] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.341287407927, 0.737450307728036, 0.0, 1.0, 50.0, 39.16516258926686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1731600.0000, 
sim time next is 1732200.0000, 
raw observation next is [0.45, 91.83333333333334, 0.0, 0.0, 19.0, 26.35586606522433, 0.7272949217020378, 0.0, 1.0, 45.0, 33.94835240654574], 
processed observation next is [0.0, 0.043478260869565216, 0.47506925207756234, 0.9183333333333334, 0.0, 0.0, 0.08333333333333333, 0.6963221721020275, 0.742431640567346, 0.0, 1.0, 0.6, 0.3394835240654574], 
reward next is 0.6605, 
noisyNet noise sample is [array([0.8286249], dtype=float32), -0.58449864]. 
=============================================
[2019-04-09 14:45:54,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05342215 0.11838522 0.07729829 0.09076696 0.08779494 0.07679401
 0.10494111 0.08684207 0.07864117 0.11516176 0.10995229], sum to 1.0000
[2019-04-09 14:45:54,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6102
[2019-04-09 14:45:54,113] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 19.0, 26.34046136409464, 0.7058470124439387, 0.0, 1.0, 25.0, 45.78455109517611], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 19.0, 26.32081275920254, 0.6947938660991838, 0.0, 1.0, 50.0, 36.44292888949796], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6934010632668782, 0.7315979553663946, 0.0, 1.0, 0.7, 0.3644292888949796], 
reward next is 0.6356, 
noisyNet noise sample is [array([-0.2010013], dtype=float32), 1.9081646]. 
=============================================
[2019-04-09 14:45:54,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05798747 0.11549285 0.08573088 0.09040514 0.08714288 0.06908539
 0.10619342 0.08116491 0.08580617 0.10822033 0.1127706 ], sum to 1.0000
[2019-04-09 14:45:54,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9058
[2019-04-09 14:45:54,171] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.616666666666667, 87.0, 0.0, 0.0, 19.0, 26.17308205060837, 0.6790276450135493, 0.0, 1.0, 60.0, 49.60804296328826], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1752600.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 26.19585010055133, 0.6865980661888931, 0.0, 1.0, 65.0, 55.52849879612909], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6829875083792775, 0.7288660220629644, 0.0, 1.0, 1.0, 0.5552849879612909], 
reward next is 0.4447, 
noisyNet noise sample is [array([1.160627], dtype=float32), 0.32850143]. 
=============================================
[2019-04-09 14:45:54,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05786143 0.10839701 0.07804286 0.08861499 0.08330055 0.07720026
 0.10462116 0.08782047 0.09054577 0.1092232  0.11437229], sum to 1.0000
[2019-04-09 14:45:54,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-09 14:45:54,305] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.0, 0.0, 0.0, 19.0, 26.35181601831758, 0.7232693320670901, 0.0, 1.0, 60.0, 53.77330928113635], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1744200.0000, 
sim time next is 1744800.0000, 
raw observation next is [-0.6, 84.33333333333333, 0.0, 0.0, 19.0, 26.34105476724867, 0.7257263536558232, 0.0, 1.0, 40.0, 45.56227752524158], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.6950878972707226, 0.7419087845519411, 0.0, 1.0, 0.5, 0.45562277525241585], 
reward next is 0.5444, 
noisyNet noise sample is [array([-1.0947975], dtype=float32), -0.78748006]. 
=============================================
[2019-04-09 14:45:54,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05150533 0.12356974 0.08259264 0.09350966 0.08838508 0.06952222
 0.10055768 0.08709197 0.08380722 0.10430951 0.11514894], sum to 1.0000
[2019-04-09 14:45:54,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1958
[2019-04-09 14:45:54,553] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.7, 83.0, 45.5, 0.0, 19.0, 26.37385564870182, 0.6941717786198335, 0.0, 1.0, 65.0, 53.29247125448276], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [-1.8, 83.66666666666667, 52.0, 0.0, 19.0, 26.38057366118601, 0.7000415193366839, 0.0, 1.0, 25.0, 48.82691076927634], 
processed observation next is [0.0, 0.391304347826087, 0.41274238227146814, 0.8366666666666667, 0.17333333333333334, 0.0, 0.08333333333333333, 0.6983811384321674, 0.733347173112228, 0.0, 1.0, 0.2, 0.4882691076927634], 
reward next is 0.5117, 
noisyNet noise sample is [array([-0.3352836], dtype=float32), 0.11582306]. 
=============================================
[2019-04-09 14:45:54,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[0.24133402]
 [0.18893066]
 [0.18952465]
 [0.22771609]
 [0.19331628]], R is [[0.7331382 ]
 [1.19288206]
 [1.70161891]
 [2.13631845]
 [2.61370611]].
[2019-04-09 14:45:54,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0588788  0.1074229  0.07788086 0.08539704 0.08637966 0.07837599
 0.10520158 0.08140773 0.0871938  0.10937306 0.12248854], sum to 1.0000
[2019-04-09 14:45:54,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7406
[2019-04-09 14:45:54,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.7, 85.0, 26.0, 0.0, 19.0, 26.10489342345895, 0.6375651630116771, 0.0, 1.0, 50.0, 41.09852849774757], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1758600.0000, 
sim time next is 1759200.0000, 
raw observation next is [-1.7, 84.33333333333333, 32.5, 0.0, 19.0, 26.12540896353905, 0.6340264607690246, 0.0, 1.0, 55.0, 39.1206194701714], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.8433333333333333, 0.10833333333333334, 0.0, 0.08333333333333333, 0.6771174136282543, 0.7113421535896749, 0.0, 1.0, 0.8, 0.39120619470171397], 
reward next is 0.6088, 
noisyNet noise sample is [array([1.6301874], dtype=float32), 0.4746789]. 
=============================================
[2019-04-09 14:45:54,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05556451 0.11472208 0.08446133 0.08866508 0.08445308 0.07011351
 0.09812196 0.09089795 0.08601832 0.11363133 0.11335085], sum to 1.0000
[2019-04-09 14:45:54,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-09 14:45:54,799] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05860334 0.12321211 0.08008026 0.094481   0.08003519 0.06893374
 0.09666233 0.08727035 0.08664254 0.11239058 0.11168865], sum to 1.0000
[2019-04-09 14:45:54,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4836
[2019-04-09 14:45:54,810] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.2, 87.0, 0.0, 0.0, 19.0, 25.76262674044196, 0.6232277316478513, 0.0, 1.0, 50.0, 46.48669114524166], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1749600.0000, 
sim time next is 1750200.0000, 
raw observation next is [-1.283333333333333, 87.0, 0.0, 0.0, 19.0, 25.77521638104595, 0.6167398362135434, 0.0, 1.0, 50.0, 39.5185284387031], 
processed observation next is [0.0, 0.2608695652173913, 0.4270544783010157, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6479346984204959, 0.7055799454045145, 0.0, 1.0, 0.7, 0.395185284387031], 
reward next is 0.6048, 
noisyNet noise sample is [array([-0.7879323], dtype=float32), 1.1541809]. 
=============================================
[2019-04-09 14:45:54,815] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.45, 87.0, 0.0, 0.0, 19.0, 26.36491761703306, 0.6916236935430194, 0.0, 1.0, 50.0, 34.67482056759991], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1751400.0000, 
sim time next is 1752000.0000, 
raw observation next is [-1.533333333333333, 87.0, 0.0, 0.0, 19.0, 26.28696956716373, 0.6701720886700481, 0.0, 1.0, 40.0, 33.00430936414172], 
processed observation next is [0.0, 0.2608695652173913, 0.42012927054478305, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6905807972636442, 0.7233906962233494, 0.0, 1.0, 0.5, 0.3300430936414172], 
reward next is 0.6700, 
noisyNet noise sample is [array([-0.8990181], dtype=float32), -2.2587044]. 
=============================================
[2019-04-09 14:45:54,833] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[ 0.16508545]
 [-0.02102725]
 [ 0.08060212]
 [ 0.10926835]
 [ 0.25426096]], R is [[0.74816161]
 [1.39393175]
 [2.01406217]
 [2.60272884]
 [3.17429733]].
[2019-04-09 14:45:54,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05559636 0.11995625 0.08516214 0.10858663 0.08717979 0.07239419
 0.09884863 0.08479768 0.07854421 0.10254534 0.10638872], sum to 1.0000
[2019-04-09 14:45:54,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9518
[2019-04-09 14:45:54,978] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 87.0, 91.66666666666667, 0.0, 19.0, 25.65808355522043, 0.5437163932427125, 0.0, 1.0, 65.0, 60.29511649026089], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1765200.0000, 
sim time next is 1765800.0000, 
raw observation next is [-2.3, 87.0, 97.0, 0.0, 19.0, 25.6332280885192, 0.5673502082835495, 0.0, 1.0, 65.0, 76.8108295259501], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3233333333333333, 0.0, 0.08333333333333333, 0.6361023407099333, 0.6891167360945166, 0.0, 1.0, 1.0, 0.768108295259501], 
reward next is 0.2319, 
noisyNet noise sample is [array([-0.2751616], dtype=float32), 1.8697184]. 
=============================================
[2019-04-09 14:45:55,129] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05566436 0.11435796 0.07708596 0.0861828  0.08423823 0.06887047
 0.09691365 0.08970447 0.08499557 0.11864924 0.12333725], sum to 1.0000
[2019-04-09 14:45:55,129] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3955
[2019-04-09 14:45:55,144] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 86.33333333333333, 17.66666666666667, 0.0, 19.0, 25.80485087545003, 0.5708834799656941, 0.0, 1.0, 40.0, 31.54491199737088], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1757400.0000, 
sim time next is 1758000.0000, 
raw observation next is [-1.7, 85.66666666666667, 21.83333333333334, 0.0, 19.0, 25.76415457384425, 0.5544280927858874, 0.0, 1.0, 35.0, 32.57798016840416], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.8566666666666667, 0.0727777777777778, 0.0, 0.08333333333333333, 0.6470128811536876, 0.6848093642619625, 0.0, 1.0, 0.4, 0.3257798016840416], 
reward next is 0.6742, 
noisyNet noise sample is [array([-0.33735818], dtype=float32), -0.29735318]. 
=============================================
[2019-04-09 14:45:55,152] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[0.178966  ]
 [0.10805474]
 [0.1742313 ]
 [0.08709423]
 [0.11519368]], R is [[0.74290973]
 [1.42003155]
 [1.9402647 ]
 [2.32594943]
 [2.98665762]].
[2019-04-09 14:45:55,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05231521 0.12635548 0.07360994 0.09125465 0.08797149 0.06857396
 0.10365579 0.0977701  0.07753371 0.10068709 0.12027255], sum to 1.0000
[2019-04-09 14:45:55,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3496
[2019-04-09 14:45:55,328] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 115.6666666666667, 0.0, 19.0, 24.9757309620908, 0.4321581466122045, 0.0, 1.0, 45.0, 34.7900578678522], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1777200.0000, 
sim time next is 1777800.0000, 
raw observation next is [-2.8, 83.0, 112.3333333333333, 0.0, 19.0, 25.01143673350298, 0.4257274119465364, 0.0, 1.0, 35.0, 28.75860879645461], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.37444444444444436, 0.0, 0.08333333333333333, 0.5842863944585815, 0.6419091373155121, 0.0, 1.0, 0.4, 0.2875860879645461], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.51295704], dtype=float32), 0.41643658]. 
=============================================
[2019-04-09 14:45:55,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04875847 0.12973562 0.07374571 0.09130693 0.08915716 0.0619809
 0.10698304 0.1000218  0.07717139 0.10934758 0.11179132], sum to 1.0000
[2019-04-09 14:45:55,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2021
[2019-04-09 14:45:55,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 19.0, 25.33597215314975, 0.5691552101341548, 0.0, 1.0, 60.0, 59.98305019575081], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1768800.0000, 
sim time next is 1769400.0000, 
raw observation next is [-2.3, 85.0, 119.0, 0.0, 19.0, 25.58035097795601, 0.5881874624443064, 0.0, 1.0, 25.0, 44.14620295525292], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.85, 0.39666666666666667, 0.0, 0.08333333333333333, 0.6316959148296674, 0.6960624874814355, 0.0, 1.0, 0.2, 0.44146202955252917], 
reward next is 0.5585, 
noisyNet noise sample is [array([-0.40681234], dtype=float32), -0.99331737]. 
=============================================
[2019-04-09 14:45:55,858] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05395271 0.11621253 0.08169835 0.08480397 0.09295405 0.0707149
 0.10491702 0.08956894 0.08028089 0.10600674 0.11888997], sum to 1.0000
[2019-04-09 14:45:55,859] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-09 14:45:55,874] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.3, 87.0, 53.33333333333334, 0.0, 19.0, 25.66560275801214, 0.5300170274582046, 0.0, 1.0, 20.0, 38.58895164300933], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1785000.0000, 
sim time next is 1785600.0000, 
raw observation next is [-3.4, 87.0, 47.0, 0.0, 19.0, 25.67394945994145, 0.5343981538350892, 0.0, 1.0, 65.0, 66.27330236921068], 
processed observation next is [0.0, 0.6956521739130435, 0.368421052631579, 0.87, 0.15666666666666668, 0.0, 0.08333333333333333, 0.6394957883284542, 0.6781327179450297, 0.0, 1.0, 1.0, 0.6627330236921068], 
reward next is 0.3373, 
noisyNet noise sample is [array([0.8444247], dtype=float32), 0.7986323]. 
=============================================
[2019-04-09 14:45:56,180] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05165301 0.13224547 0.07020251 0.09468506 0.08686036 0.06797072
 0.10369001 0.0953559  0.08142015 0.10479945 0.11111739], sum to 1.0000
[2019-04-09 14:45:56,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8718
[2019-04-09 14:45:56,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.8, 83.0, 120.1666666666667, 0.0, 19.0, 25.89927523528431, 0.5972550687309487, 0.0, 1.0, 40.0, 39.48438873719362], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1776000.0000, 
sim time next is 1776600.0000, 
raw observation next is [-2.8, 83.0, 119.0, 0.0, 19.0, 25.91826296922629, 0.5988905216426016, 0.0, 1.0, 60.0, 51.53392413382929], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.39666666666666667, 0.0, 0.08333333333333333, 0.6598552474355243, 0.6996301738808671, 0.0, 1.0, 0.9, 0.5153392413382929], 
reward next is 0.4847, 
noisyNet noise sample is [array([-0.14130645], dtype=float32), 0.4174261]. 
=============================================
[2019-04-09 14:45:56,209] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47420: loss 25.3933
[2019-04-09 14:45:56,213] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47421: learning rate 0.0000
[2019-04-09 14:45:56,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05140899 0.11932084 0.07342882 0.08249106 0.07378913 0.06571589
 0.11111593 0.08721831 0.09342705 0.11589526 0.12618865], sum to 1.0000
[2019-04-09 14:45:56,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2279
[2019-04-09 14:45:56,262] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.03333333333333333, 91.0, 0.0, 0.0, 19.0, 26.68882235719922, 0.7822926420225805, 0.0, 1.0, 45.0, 38.00951612329333], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1738200.0000, 
sim time next is 1738800.0000, 
raw observation next is [0.0, 91.0, 0.0, 0.0, 19.0, 26.6759267063096, 0.7769859432673588, 0.0, 1.0, 25.0, 39.32901227416933], 
processed observation next is [0.0, 0.13043478260869565, 0.46260387811634357, 0.91, 0.0, 0.0, 0.08333333333333333, 0.7229938921924667, 0.7589953144224529, 0.0, 1.0, 0.2, 0.3932901227416933], 
reward next is 0.6067, 
noisyNet noise sample is [array([0.24920672], dtype=float32), 0.9494691]. 
=============================================
[2019-04-09 14:45:56,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05404272 0.12762025 0.07401809 0.0889079  0.08047248 0.06844286
 0.09510776 0.10364843 0.0830323  0.1089384  0.11576883], sum to 1.0000
[2019-04-09 14:45:56,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9590
[2019-04-09 14:45:56,298] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 85.0, 99.0, 0.0, 19.0, 24.94934049826669, 0.3879990488524709, 0.0, 1.0, 20.0, 27.64763397881267], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1780200.0000, 
sim time next is 1780800.0000, 
raw observation next is [-2.8, 85.66666666666667, 93.5, 0.0, 19.0, 24.86348531333401, 0.3856171538911688, 0.0, 1.0, 65.0, 55.70468981589683], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8566666666666667, 0.31166666666666665, 0.0, 0.08333333333333333, 0.5719571094445008, 0.6285390512970562, 0.0, 1.0, 1.0, 0.5570468981589682], 
reward next is 0.4430, 
noisyNet noise sample is [array([-1.9037896], dtype=float32), -1.5106424]. 
=============================================
[2019-04-09 14:45:56,390] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47515: loss 18.0692
[2019-04-09 14:45:56,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47516: learning rate 0.0000
[2019-04-09 14:45:56,393] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47516: loss 20.8063
[2019-04-09 14:45:56,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47518: learning rate 0.0000
[2019-04-09 14:45:56,415] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47532: loss 25.2774
[2019-04-09 14:45:56,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47532: learning rate 0.0000
[2019-04-09 14:45:56,469] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47556: loss 25.1530
[2019-04-09 14:45:56,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47556: learning rate 0.0000
[2019-04-09 14:45:56,471] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05037773 0.11913213 0.07670541 0.09060182 0.09471942 0.06429064
 0.10296495 0.10495871 0.07220498 0.10981805 0.11422613], sum to 1.0000
[2019-04-09 14:45:56,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4680
[2019-04-09 14:45:56,487] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 19.0, 25.50434385543977, 0.4728398263868006, 0.0, 1.0, 50.0, 44.79668123743378], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1801200.0000, 
sim time next is 1801800.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 19.0, 25.51968326273209, 0.4615589935657978, 0.0, 1.0, 40.0, 41.35646833086623], 
processed observation next is [0.0, 0.8695652173913043, 0.3310249307479225, 0.845, 0.0, 0.0, 0.08333333333333333, 0.6266402718943409, 0.653852997855266, 0.0, 1.0, 0.5, 0.4135646833086623], 
reward next is 0.5864, 
noisyNet noise sample is [array([1.4649175], dtype=float32), -0.22636363]. 
=============================================
[2019-04-09 14:45:56,524] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 47593: loss 24.9374
[2019-04-09 14:45:56,530] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 47595: learning rate 0.0000
[2019-04-09 14:45:56,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47674: loss 22.6458
[2019-04-09 14:45:56,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47674: learning rate 0.0000
[2019-04-09 14:45:57,040] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47881: loss 22.5291
[2019-04-09 14:45:57,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47881: learning rate 0.0000
[2019-04-09 14:45:57,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04721342 0.12372998 0.07532138 0.0745004  0.0864559  0.0600165
 0.11116794 0.10620362 0.07434487 0.11404173 0.12700425], sum to 1.0000
[2019-04-09 14:45:57,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1330
[2019-04-09 14:45:57,069] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 19.0, 25.66388078914701, 0.5055601976091806, 0.0, 1.0, 25.0, 37.64429608113788], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 19.0, 25.64531457892793, 0.4888282698961064, 0.0, 1.0, 20.0, 38.39789741080055], 
processed observation next is [0.0, 0.8695652173913043, 0.3356417359187443, 0.835, 0.0, 0.0, 0.08333333333333333, 0.6371095482439942, 0.6629427566320355, 0.0, 1.0, 0.1, 0.3839789741080055], 
reward next is 0.6160, 
noisyNet noise sample is [array([-0.97014594], dtype=float32), 1.2278986]. 
=============================================
[2019-04-09 14:45:57,088] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05729326 0.11623681 0.08437982 0.07360471 0.08918604 0.06162345
 0.11546884 0.11314161 0.06838726 0.10454646 0.1161318 ], sum to 1.0000
[2019-04-09 14:45:57,095] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8123
[2019-04-09 14:45:57,109] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.0, 81.0, 0.0, 0.0, 19.0, 23.88077138060199, 0.1145044500086816, 0.0, 1.0, 45.0, 34.51339824771079], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1812000.0000, 
sim time next is 1812600.0000, 
raw observation next is [-5.0, 80.5, 0.0, 0.0, 19.0, 23.79945554291834, 0.1161545575876776, 0.0, 1.0, 60.0, 60.01586594160538], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.805, 0.0, 0.0, 0.08333333333333333, 0.4832879619098618, 0.5387181858625593, 0.0, 1.0, 0.9, 0.6001586594160537], 
reward next is 0.3998, 
noisyNet noise sample is [array([-0.31198725], dtype=float32), -2.1384566]. 
=============================================
[2019-04-09 14:45:57,121] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47927: loss 19.7340
[2019-04-09 14:45:57,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47927: learning rate 0.0000
[2019-04-09 14:45:57,190] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47961: loss 23.0670
[2019-04-09 14:45:57,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47962: learning rate 0.0000
[2019-04-09 14:45:57,238] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47989: loss 23.5972
[2019-04-09 14:45:57,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47989: learning rate 0.0000
[2019-04-09 14:45:57,292] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48023: loss 17.4610
[2019-04-09 14:45:57,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48024: learning rate 0.0000
[2019-04-09 14:45:57,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05573273 0.10800134 0.08039188 0.07781883 0.08373746 0.07216919
 0.10169009 0.10159907 0.06953247 0.1194457  0.12988128], sum to 1.0000
[2019-04-09 14:45:57,369] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-09 14:45:57,386] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.666666666666667, 78.83333333333333, 0.0, 0.0, 19.0, 24.13815398820725, 0.152074413012502, 0.0, 1.0, 60.0, 57.71890772314714], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1818600.0000, 
sim time next is 1819200.0000, 
raw observation next is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 24.0350524967455, 0.1632814384110696, 0.0, 1.0, 55.0, 55.67405508973651], 
processed observation next is [0.0, 0.043478260869565216, 0.30378578024007385, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5029210413954583, 0.5544271461370233, 0.0, 1.0, 0.8, 0.5567405508973651], 
reward next is 0.4433, 
noisyNet noise sample is [array([-1.3621299], dtype=float32), -0.24438106]. 
=============================================
[2019-04-09 14:45:57,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05146936 0.11040034 0.08204259 0.09045082 0.08926488 0.0668914
 0.10401885 0.10293166 0.07771346 0.11510941 0.10970728], sum to 1.0000
[2019-04-09 14:45:57,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6124
[2019-04-09 14:45:57,405] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 85.0, 65.0, 0.0, 19.0, 25.33391106529215, 0.4741066714428848, 0.0, 1.0, 45.0, 53.68341842227946], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 19.0, 25.22080763919435, 0.480062465350041, 0.0, 1.0, 50.0, 41.64479196017701], 
processed observation next is [0.0, 0.391304347826087, 0.404432132963989, 0.8566666666666667, 0.23444444444444448, 0.0, 0.08333333333333333, 0.6017339699328627, 0.660020821783347, 0.0, 1.0, 0.7, 0.4164479196017701], 
reward next is 0.5836, 
noisyNet noise sample is [array([-1.1012154], dtype=float32), 0.46964914]. 
=============================================
[2019-04-09 14:45:57,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05587034 0.11037567 0.08608264 0.07959472 0.0865787  0.06198746
 0.10602871 0.10730898 0.07445604 0.10732815 0.12438863], sum to 1.0000
[2019-04-09 14:45:57,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4268
[2019-04-09 14:45:57,705] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 81.0, 0.0, 0.0, 19.0, 24.47602912826811, 0.2257010871141748, 0.0, 1.0, 25.0, 45.83283070908891], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1812000.0000, 
sim time next is 1812600.0000, 
raw observation next is [-5.0, 80.5, 0.0, 0.0, 19.0, 24.43304862378393, 0.2120864848360055, 0.0, 1.0, 40.0, 30.84077032074898], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.805, 0.0, 0.0, 0.08333333333333333, 0.5360873853153274, 0.5706954949453352, 0.0, 1.0, 0.5, 0.3084077032074898], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.29513377], dtype=float32), -0.29065093]. 
=============================================
[2019-04-09 14:45:57,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04441077 0.12339053 0.08799282 0.08849024 0.0840717  0.06310514
 0.119776   0.09145791 0.07720495 0.09354071 0.12655923], sum to 1.0000
[2019-04-09 14:45:57,722] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6523
[2019-04-09 14:45:57,743] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 79.5, 0.0, 0.0, 19.0, 25.46801114869728, 0.4332645435871985, 0.0, 1.0, 35.0, 39.30189867245844], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1813800.0000, 
sim time next is 1814400.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 19.0, 25.42917950280176, 0.4148665677550571, 0.0, 1.0, 50.0, 38.48799452319876], 
processed observation next is [0.0, 0.0, 0.32409972299168976, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6190982919001465, 0.6382888559183524, 0.0, 1.0, 0.7, 0.3848799452319876], 
reward next is 0.6151, 
noisyNet noise sample is [array([-0.63092595], dtype=float32), 1.4128252]. 
=============================================
[2019-04-09 14:45:57,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06867994 0.10802529 0.07975143 0.08713508 0.09154309 0.08345548
 0.10038387 0.08242871 0.08237923 0.11333568 0.10288225], sum to 1.0000
[2019-04-09 14:45:57,748] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0564
[2019-04-09 14:45:57,755] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.2, 85.66666666666667, 0.0, 0.0, 19.0, 23.76182639095129, 0.09717534719663264, 0.0, 1.0, 55.0, 48.21028346148834], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1826400.0000, 
sim time next is 1827000.0000, 
raw observation next is [-6.2, 85.0, 0.0, 0.0, 19.0, 23.82510248679066, 0.01542511294187018, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.85, 0.0, 0.0, 0.08333333333333333, 0.4854252072325549, 0.5051417043139567, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7669567], dtype=float32), 0.036995426]. 
=============================================
[2019-04-09 14:45:57,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0572023  0.1194656  0.08387803 0.08381585 0.08643208 0.06064973
 0.10877782 0.10088457 0.0779203  0.10319731 0.11777647], sum to 1.0000
[2019-04-09 14:45:57,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1631
[2019-04-09 14:45:57,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[ 0.00598346]
 [-0.04070814]
 [ 0.06261195]
 [ 0.09812693]
 [ 0.09362604]], R is [[0.97890365]
 [1.48701179]
 [2.104877  ]
 [2.46570206]
 [3.44104505]].
[2019-04-09 14:45:57,791] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 81.50000000000001, 0.0, 0.0, 19.0, 24.91454899111808, 0.2386997690084627, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1811400.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 24.68544820154991, 0.280606249563189, 0.0, 1.0, 20.0, 70.46794147934807], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.5571206834624925, 0.593535416521063, 0.0, 1.0, 0.1, 0.7046794147934807], 
reward next is 0.2953, 
noisyNet noise sample is [array([-1.8675188], dtype=float32), 0.25359958]. 
=============================================
[2019-04-09 14:45:57,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.1411456 ]
 [0.22321655]
 [0.23240916]
 [0.28461617]
 [0.26010412]], R is [[0.54835927]
 [1.54287577]
 [1.89088964]
 [2.47845697]
 [2.9055872 ]].
[2019-04-09 14:45:57,878] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48359: loss 16.9096
[2019-04-09 14:45:57,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48359: learning rate 0.0000
[2019-04-09 14:45:57,931] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.06638803 0.10184786 0.07739375 0.09099428 0.08197333 0.07720752
 0.10771513 0.08194114 0.09187827 0.10261401 0.12004666], sum to 1.0000
[2019-04-09 14:45:57,940] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3807
[2019-04-09 14:45:57,956] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 79.66666666666667, 0.0, 0.0, 19.0, 23.58928466513663, 0.01033591633009547, 0.0, 1.0, 30.0, 40.61784872397656], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1831800.0000, 
sim time next is 1832400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 19.0, 23.54908544697548, -0.004614932934398758, 0.0, 1.0, 35.0, 36.8409678618365], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4624237872479566, 0.4984616890218671, 0.0, 1.0, 0.4, 0.36840967861836504], 
reward next is 0.6316, 
noisyNet noise sample is [array([-0.10881315], dtype=float32), -0.658723]. 
=============================================
[2019-04-09 14:45:58,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05082529 0.10188144 0.07954948 0.09594931 0.08458039 0.06252147
 0.09913373 0.09864491 0.07725114 0.12805004 0.12161268], sum to 1.0000
[2019-04-09 14:45:58,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-09 14:45:58,050] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 82.16666666666667, 0.0, 0.0, 19.0, 24.30471130873331, 0.2556877966363276, 0.0, 1.0, 35.0, 62.19476998951977], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1793400.0000, 
sim time next is 1794000.0000, 
raw observation next is [-4.1, 82.33333333333334, 0.0, 0.0, 19.0, 24.28289412133282, 0.2577159569403566, 0.0, 1.0, 45.0, 34.65878897561181], 
processed observation next is [0.0, 0.782608695652174, 0.3490304709141275, 0.8233333333333335, 0.0, 0.0, 0.08333333333333333, 0.5235745101110684, 0.5859053189801189, 0.0, 1.0, 0.6, 0.34658788975611815], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.56380224], dtype=float32), -0.087993026]. 
=============================================
[2019-04-09 14:45:58,053] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04584732 0.11643463 0.0802979  0.09125733 0.08317713 0.06278525
 0.11789478 0.09171253 0.07403713 0.09921227 0.13734381], sum to 1.0000
[2019-04-09 14:45:58,055] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7880
[2019-04-09 14:45:58,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05351597 0.12058689 0.0794103  0.09679235 0.08691868 0.07258607
 0.09946502 0.09337051 0.08262927 0.10200911 0.11271587], sum to 1.0000
[2019-04-09 14:45:58,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8906
[2019-04-09 14:45:58,063] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[0.17897081]
 [0.22803044]
 [0.18598446]
 [0.19002134]
 [0.17150733]], R is [[0.81638032]
 [1.18626881]
 [2.17440605]
 [2.50337648]
 [3.06791449]].
[2019-04-09 14:45:58,070] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.2, 78.66666666666667, 0.0, 0.0, 19.0, 25.25202890028404, 0.3673312980281709, 0.0, 1.0, 55.0, 45.76621519863298], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1815600.0000, 
sim time next is 1816200.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 19.0, 25.16545796231155, 0.3676616531937745, 0.0, 1.0, 60.0, 55.15628003824118], 
processed observation next is [0.0, 0.0, 0.31578947368421056, 0.785, 0.0, 0.0, 0.08333333333333333, 0.5971214968592958, 0.6225538843979248, 0.0, 1.0, 0.9, 0.5515628003824118], 
reward next is 0.4484, 
noisyNet noise sample is [array([0.9704379], dtype=float32), 2.507353]. 
=============================================
[2019-04-09 14:45:58,073] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 84.33333333333333, 120.1666666666667, 0.0, 19.0, 25.57493543723767, 0.5325869188973423, 0.0, 1.0, 55.0, 46.65784402971887], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1770000.0000, 
sim time next is 1770600.0000, 
raw observation next is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 19.0, 25.53318309674088, 0.5500465474819615, 0.0, 1.0, 65.0, 69.9492970949102], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8366666666666667, 0.40444444444444433, 0.0, 0.08333333333333333, 0.6277652580617401, 0.6833488491606539, 0.0, 1.0, 1.0, 0.6994929709491019], 
reward next is 0.3005, 
noisyNet noise sample is [array([0.7402848], dtype=float32), 0.49359053]. 
=============================================
[2019-04-09 14:45:58,083] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05405153 0.12166709 0.07466223 0.08003248 0.08328044 0.06015488
 0.10527993 0.10290819 0.08461019 0.10688668 0.12646635], sum to 1.0000
[2019-04-09 14:45:58,083] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1832
[2019-04-09 14:45:58,099] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.1, 82.33333333333334, 0.0, 0.0, 19.0, 24.28289412133282, 0.2577159569403566, 0.0, 1.0, 45.0, 34.65878897561181], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1794000.0000, 
sim time next is 1794600.0000, 
raw observation next is [-4.2, 82.5, 0.0, 0.0, 19.0, 24.31741624578517, 0.2519425627780437, 0.0, 1.0, 25.0, 34.77736371885656], 
processed observation next is [0.0, 0.782608695652174, 0.34626038781163443, 0.825, 0.0, 0.0, 0.08333333333333333, 0.526451353815431, 0.583980854259348, 0.0, 1.0, 0.2, 0.34777363718856563], 
reward next is 0.6522, 
noisyNet noise sample is [array([-0.16424386], dtype=float32), 0.029015845]. 
=============================================
[2019-04-09 14:45:58,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0651117  0.10571273 0.08169461 0.08683509 0.08344703 0.07447983
 0.10083754 0.09023682 0.08472331 0.11574657 0.1111748 ], sum to 1.0000
[2019-04-09 14:45:58,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-09 14:45:58,133] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 78.5, 0.0, 0.0, 19.0, 23.96681255952156, 0.06658407483879676, 0.0, 1.0, 45.0, 32.72594733806487], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1837800.0000, 
sim time next is 1838400.0000, 
raw observation next is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 19.0, 23.86553304279183, 0.04191167932198079, 0.0, 1.0, 40.0, 31.20366098673727], 
processed observation next is [0.0, 0.2608695652173913, 0.2816251154201293, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.4887944202326526, 0.5139705597739935, 0.0, 1.0, 0.5, 0.3120366098673727], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.70207113], dtype=float32), 0.40709096]. 
=============================================
[2019-04-09 14:45:58,344] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48626: loss 20.9773
[2019-04-09 14:45:58,346] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48626: learning rate 0.0000
[2019-04-09 14:45:58,432] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05377629 0.12467183 0.08281365 0.08563834 0.09244549 0.05568468
 0.10336353 0.10816938 0.07878724 0.09918381 0.1154657 ], sum to 1.0000
[2019-04-09 14:45:58,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3828
[2019-04-09 14:45:58,446] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 19.0, 24.28553157642944, 0.1999575006889476, 0.0, 1.0, 55.0, 47.16324605032982], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1801800.0000, 
sim time next is 1802400.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 19.0, 24.23936143399679, 0.1960907244951309, 0.0, 1.0, 35.0, 43.88963282901366], 
processed observation next is [0.0, 0.8695652173913043, 0.3287165281625116, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5199467861663992, 0.5653635748317103, 0.0, 1.0, 0.4, 0.4388963282901366], 
reward next is 0.5611, 
noisyNet noise sample is [array([-0.10442654], dtype=float32), 2.511909]. 
=============================================
[2019-04-09 14:45:59,140] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 49076: loss 20.0930
[2019-04-09 14:45:59,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 49077: learning rate 0.0000
[2019-04-09 14:45:59,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04682436 0.13160089 0.07463102 0.1026934  0.07956037 0.05940584
 0.10581594 0.09691529 0.09583986 0.09650702 0.11020611], sum to 1.0000
[2019-04-09 14:45:59,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9175
[2019-04-09 14:45:59,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 71.0, 145.0, 20.0, 19.0, 23.46832351850546, -0.05067510084038526, 0.0, 1.0, 45.0, 35.8945580676943], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1861200.0000, 
sim time next is 1861800.0000, 
raw observation next is [-4.5, 71.0, 153.3333333333333, 26.66666666666667, 19.0, 23.43788640057911, -0.04179733743022906, 0.0, 1.0, 65.0, 63.42366158997129], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.511111111111111, 0.02946593001841621, 0.08333333333333333, 0.45315720004825916, 0.4860675541899237, 0.0, 1.0, 1.0, 0.6342366158997129], 
reward next is 0.3658, 
noisyNet noise sample is [array([0.7786773], dtype=float32), -1.7838017]. 
=============================================
[2019-04-09 14:45:59,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05815087 0.10910436 0.08121826 0.08891077 0.08166872 0.06802932
 0.10399564 0.08953277 0.09077217 0.11422985 0.11438721], sum to 1.0000
[2019-04-09 14:45:59,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1339
[2019-04-09 14:45:59,278] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 23.49735012816046, -0.03701528722218894, 0.0, 1.0, 30.0, 31.57893917068009], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [-6.7, 78.0, 9.666666666666664, 0.0, 19.0, 23.39771589143933, -0.05966096964131044, 0.0, 1.0, 20.0, 30.01233753474552], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.032222222222222215, 0.0, 0.08333333333333333, 0.4498096576199441, 0.4801130101195632, 0.0, 1.0, 0.1, 0.3001233753474552], 
reward next is 0.6999, 
noisyNet noise sample is [array([1.1811519], dtype=float32), 0.23302262]. 
=============================================
[2019-04-09 14:45:59,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0562909  0.11974061 0.07240606 0.09751623 0.08806686 0.06445304
 0.10689986 0.10379376 0.08022647 0.09776534 0.11284078], sum to 1.0000
[2019-04-09 14:45:59,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-09 14:45:59,355] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 172.6666666666667, 47.33333333333333, 19.0, 22.94328912939983, -0.1053752538654666, 0.0, 1.0, 45.0, 43.05644878853144], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1863600.0000, 
sim time next is 1864200.0000, 
raw observation next is [-4.5, 71.0, 175.3333333333333, 54.66666666666666, 19.0, 22.97877809791633, -0.1013643447911539, 0.0, 1.0, 30.0, 28.73219135733019], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5844444444444443, 0.06040515653775321, 0.08333333333333333, 0.41489817482636077, 0.46621188506961536, 0.0, 1.0, 0.3, 0.2873219135733019], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.78169906], dtype=float32), -0.36974132]. 
=============================================
[2019-04-09 14:45:59,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.06034959 0.1194231  0.08372097 0.10278723 0.08810893 0.07043505
 0.09773201 0.08017787 0.0898874  0.09732009 0.11005774], sum to 1.0000
[2019-04-09 14:45:59,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2072
[2019-04-09 14:45:59,416] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 19.0, 23.9185989558771, 0.08781547528789042, 0.0, 1.0, 60.0, 82.15454057640267], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 19.0, 23.86989934924456, 0.1000619853108229, 0.0, 1.0, 25.0, 47.42423176520331], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.08333333333333333, 0.48915827910371323, 0.5333539951036076, 0.0, 1.0, 0.2, 0.4742423176520331], 
reward next is 0.5258, 
noisyNet noise sample is [array([-1.5169843], dtype=float32), -0.9820762]. 
=============================================
[2019-04-09 14:45:59,418] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05199246 0.11831745 0.08035517 0.08448236 0.08290169 0.06772329
 0.10602283 0.09804142 0.09326365 0.09823462 0.11866499], sum to 1.0000
[2019-04-09 14:45:59,419] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 49231: loss 24.9654
[2019-04-09 14:45:59,421] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 49232: learning rate 0.0000
[2019-04-09 14:45:59,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6241
[2019-04-09 14:45:59,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.2, 79.00000000000001, 0.0, 0.0, 19.0, 23.61960480460147, 0.03831968160613397, 0.0, 1.0, 65.0, 60.91733638109692], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1833000.0000, 
sim time next is 1833600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 19.0, 23.58799482290311, 0.06294184637775486, 0.0, 1.0, 50.0, 47.97232289966549], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4656662352419258, 0.5209806154592517, 0.0, 1.0, 0.7, 0.4797232289966549], 
reward next is 0.5203, 
noisyNet noise sample is [array([0.30053204], dtype=float32), 0.6297159]. 
=============================================
[2019-04-09 14:45:59,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06142482 0.11366507 0.07835567 0.08748474 0.08528803 0.06639215
 0.10436007 0.09671141 0.08164417 0.10036455 0.12430937], sum to 1.0000
[2019-04-09 14:45:59,512] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-09 14:45:59,540] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 19.0, 23.31511017446595, 0.02293945865998107, 0.0, 1.0, 30.0, 46.35299598157344], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 19.0, 23.47046623156577, 0.03309078735060899, 0.0, 1.0, 45.0, 37.65817342502697], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.08333333333333333, 0.4558721859638141, 0.511030262450203, 0.0, 1.0, 0.6, 0.37658173425026975], 
reward next is 0.6234, 
noisyNet noise sample is [array([-0.5889707], dtype=float32), -0.8849629]. 
=============================================
[2019-04-09 14:45:59,631] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05007473 0.12723076 0.07514642 0.0988901  0.07907368 0.06503353
 0.09850077 0.09689775 0.09116662 0.09810409 0.11988153], sum to 1.0000
[2019-04-09 14:45:59,634] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1465
[2019-04-09 14:45:59,646] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 19.0, 24.39486685538633, 0.178610039399445, 0.0, 1.0, 40.0, 39.65343066492221], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 19.0, 24.39654549880835, 0.1675397391303146, 0.0, 1.0, 20.0, 36.30602750876169], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.08333333333333333, 0.5330454582340293, 0.5558465797101049, 0.0, 1.0, 0.1, 0.36306027508761685], 
reward next is 0.6369, 
noisyNet noise sample is [array([2.4723096], dtype=float32), -0.26080877]. 
=============================================
[2019-04-09 14:45:59,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06272858 0.1201669  0.07459664 0.10211346 0.0879123  0.07108416
 0.09735355 0.09702774 0.07564622 0.10758245 0.10378806], sum to 1.0000
[2019-04-09 14:45:59,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6857
[2019-04-09 14:45:59,763] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.4, 73.66666666666667, 173.3333333333333, 76.0, 19.0, 23.68169438445323, -0.01186185105124417, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1855200.0000, 
sim time next is 1855800.0000, 
raw observation next is [-5.3, 73.0, 184.0, 81.0, 19.0, 23.49928974518776, 0.02398501478138425, 0.0, 1.0, 50.0, 63.22994104775216], 
processed observation next is [0.0, 0.4782608695652174, 0.31578947368421056, 0.73, 0.6133333333333333, 0.08950276243093923, 0.08333333333333333, 0.4582741454323133, 0.5079950049271281, 0.0, 1.0, 0.7, 0.6322994104775216], 
reward next is 0.3677, 
noisyNet noise sample is [array([0.40668347], dtype=float32), 0.4994259]. 
=============================================
[2019-04-09 14:45:59,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05900792 0.12922098 0.07473744 0.0943726  0.08599944 0.06291964
 0.09966338 0.10753349 0.0746182  0.09929772 0.11262917], sum to 1.0000
[2019-04-09 14:45:59,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-09 14:45:59,805] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 81.0, 148.0, 56.00000000000001, 19.0, 23.43143455787586, -0.04972416347455617, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1867800.0000, 
sim time next is 1868400.0000, 
raw observation next is [-4.5, 83.0, 129.0, 42.0, 19.0, 23.41131047790995, 0.003735563848906912, 0.0, 1.0, 20.0, 62.27344870542544], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.83, 0.43, 0.04640883977900553, 0.08333333333333333, 0.45094253982582916, 0.5012451879496357, 0.0, 1.0, 0.1, 0.6227344870542544], 
reward next is 0.3773, 
noisyNet noise sample is [array([0.05967994], dtype=float32), 0.39536753]. 
=============================================
[2019-04-09 14:45:59,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05128052 0.13730922 0.07022505 0.10157532 0.08763041 0.06148393
 0.10113864 0.08960416 0.08988392 0.09804131 0.11182758], sum to 1.0000
[2019-04-09 14:45:59,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7244
[2019-04-09 14:45:59,955] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 19.0, 23.5612641657438, 0.05517415434403259, 0.0, 1.0, 50.0, 35.36172116974507], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 19.0, 23.63771485729935, 0.06271330980714133, 0.0, 1.0, 55.0, 46.65830557128396], 
processed observation next is [0.0, 0.4782608695652174, 0.3213296398891967, 0.7166666666666667, 0.5422222222222224, 0.059668508287292824, 0.08333333333333333, 0.46980957144161256, 0.5209044366023804, 0.0, 1.0, 0.8, 0.4665830557128396], 
reward next is 0.5334, 
noisyNet noise sample is [array([-1.2314072], dtype=float32), -1.8818035]. 
=============================================
[2019-04-09 14:45:59,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.1471262 ]
 [0.16565701]
 [0.14485094]
 [0.18554947]
 [0.09922346]], R is [[0.7377066 ]
 [1.37671232]
 [1.86720467]
 [2.24520993]
 [2.64920926]].
[2019-04-09 14:46:00,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04435013 0.12081928 0.07397609 0.08961373 0.08837526 0.05786404
 0.1075295  0.09641273 0.08446697 0.10592471 0.13066757], sum to 1.0000
[2019-04-09 14:46:00,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-09 14:46:00,156] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 19.0, 23.61572862509583, 0.02415715754334161, 0.0, 1.0, 55.0, 53.60966901639745], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 19.0, 23.70231621379205, 0.03731036545002576, 0.0, 1.0, 60.0, 53.21652945934065], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 0.08333333333333333, 0.47519301781600404, 0.512436788483342, 0.0, 1.0, 0.9, 0.5321652945934064], 
reward next is 0.4678, 
noisyNet noise sample is [array([2.291519], dtype=float32), -0.6687087]. 
=============================================
[2019-04-09 14:46:00,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05226366 0.12270489 0.07586677 0.09443314 0.08579188 0.06089528
 0.10453016 0.10004456 0.08274505 0.10425318 0.11647146], sum to 1.0000
[2019-04-09 14:46:00,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2484
[2019-04-09 14:46:00,310] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 73.0, 180.6666666666667, 69.33333333333334, 19.0, 24.37329521032049, 0.1922571485235451, 0.0, 1.0, 30.0, 39.17062461772108], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1865400.0000, 
sim time next is 1866000.0000, 
raw observation next is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 19.0, 24.37956782009998, 0.1861747887451825, 0.0, 1.0, 25.0, 34.26612793478939], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.75, 0.6111111111111109, 0.0847145488029466, 0.08333333333333333, 0.5316306516749982, 0.5620582629150609, 0.0, 1.0, 0.2, 0.34266127934789387], 
reward next is 0.6573, 
noisyNet noise sample is [array([-0.9905261], dtype=float32), 1.2840618]. 
=============================================
[2019-04-09 14:46:00,315] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[0.16797148]
 [0.26063973]
 [0.10557489]
 [0.23887677]
 [0.16337548]], R is [[0.97411764]
 [1.57267022]
 [2.0819931 ]
 [2.60924292]
 [3.19771457]].
[2019-04-09 14:46:00,749] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-09 14:46:00,750] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:46:00,751] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:46:00,751] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:46:00,751] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:46:00,752] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:46:00,752] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:46:00,760] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run6
[2019-04-09 14:46:00,777] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run6
[2019-04-09 14:46:00,789] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run6
[2019-04-09 14:46:13,377] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:46:13,377] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-12.15405408, 81.24389426, 0.0, 0.0, 19.0, 23.73345863571329, 0.000701750948718452, 0.0, 1.0, 20.0, 77.34161905361132]
[2019-04-09 14:46:13,377] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:46:13,379] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.04569875 0.12777816 0.07774848 0.08561126 0.0814897  0.05495873
 0.12278011 0.09779738 0.0772289  0.09150554 0.137403  ], sampled 0.28493625785550425
[2019-04-09 14:46:25,963] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:46:25,963] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-1.033333333333333, 68.0, 0.0, 0.0, 19.0, 25.6792129922283, 0.4581498332926134, 0.0, 1.0, 40.0, 32.67772967596721]
[2019-04-09 14:46:25,963] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:46:25,964] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.05265509 0.12310626 0.07647033 0.08709744 0.08605443 0.06445694
 0.100032   0.08594269 0.09913512 0.10970327 0.11534643], sampled 0.84900827183835
[2019-04-09 14:46:39,255] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:46:39,256] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-3.245501091, 73.71324794333333, 237.1066141166667, 244.5399777333333, 22.5, 24.49899891206619, 0.1355951869616282, 1.0, 1.0, 55.0, 45.18296201761427]
[2019-04-09 14:46:39,257] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:46:39,258] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.03588662 0.1353935  0.08606725 0.11654302 0.07837589 0.04684088
 0.10247596 0.1003326  0.1071888  0.08556881 0.10532667], sampled 0.4575144631250766
[2019-04-09 14:46:45,487] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:46:45,488] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-1.404477419, 50.04582828, 0.0, 0.0, 22.5, 25.78386262190691, 0.5034390644607395, 1.0, 1.0, 60.0, 42.94393375660462]
[2019-04-09 14:46:45,488] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:46:45,488] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.03023458 0.13803321 0.09723638 0.10330629 0.0772163  0.04326353
 0.10732374 0.08783467 0.10283107 0.07971954 0.13300075], sampled 0.09704135870042285
[2019-04-09 14:46:56,442] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:46:56,442] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-5.367113944666666, 74.67786165, 23.02251716666666, 104.65180175, 19.0, 23.45846930363035, 0.03514225724094348, 0.0, 1.0, 65.0, 64.79302002505638]
[2019-04-09 14:46:56,442] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:46:56,443] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.05384616 0.11662474 0.07843539 0.08702867 0.08678069 0.06762292
 0.10083558 0.09200442 0.08814459 0.11255721 0.1161197 ], sampled 0.02022752654645177
[2019-04-09 14:47:08,836] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00575365], dtype=float32), 0.009143037]
[2019-04-09 14:47:08,836] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-4.966666666666667, 77.0, 0.0, 0.0, 19.0, 25.84918651767715, 0.5481361835709655, 0.0, 1.0, 55.0, 39.26646232588823]
[2019-04-09 14:47:08,836] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:47:08,837] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.05495682 0.10777552 0.0830493  0.10449801 0.09022888 0.06687842
 0.10860538 0.08475631 0.08711641 0.0937283  0.11840668], sampled 0.6487506238067582
[2019-04-09 14:47:20,993] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5711.4146 277798.4964 2627.1927
[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:21,135] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,514] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5408.9788 308611.6141 1762.0587
[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,534] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:29,649] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,376] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5498.2559 299760.4853 2191.6783
[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:47:31,398] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 50000, evaluation results [50000.0, 5498.2559255633805, 299760.48533047, 2191.6783316712354, 5711.414561861951, 277798.4963684179, 2627.1926778672323, 5408.978825172403, 308611.6141320907, 1762.0586736200157]
[2019-04-09 14:47:31,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05252811 0.12789743 0.0744404  0.0866332  0.08713838 0.05542323
 0.09864669 0.10260033 0.08781731 0.10732773 0.11954726], sum to 1.0000
[2019-04-09 14:47:31,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8245
[2019-04-09 14:47:31,591] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 19.0, 23.67905312582031, -0.02533773717149652, 0.0, 1.0, 40.0, 32.86085291222659], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1881000.0000, 
sim time next is 1881600.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 19.0, 23.6455977324653, -0.022520857319604, 0.0, 1.0, 60.0, 60.6668478390932], 
processed observation next is [0.0, 0.782608695652174, 0.3333333333333333, 0.84, 0.0, 0.0, 0.08333333333333333, 0.47046647770544175, 0.49249304756013196, 0.0, 1.0, 0.9, 0.606668478390932], 
reward next is 0.3933, 
noisyNet noise sample is [array([0.85324067], dtype=float32), 1.5646352]. 
=============================================
[2019-04-09 14:47:31,613] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04757249 0.11755585 0.07875349 0.09254877 0.08549862 0.05783226
 0.09716354 0.09836215 0.09655275 0.10997258 0.11818746], sum to 1.0000
[2019-04-09 14:47:31,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2156
[2019-04-09 14:47:31,637] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 19.0, 24.11844460211166, 0.1004267880306197, 0.0, 1.0, 65.0, 60.59597048140314], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [-4.916666666666667, 85.5, 0.0, 0.0, 19.0, 24.08975056576836, 0.1037911476592488, 0.0, 1.0, 25.0, 47.51177935185605], 
processed observation next is [0.0, 0.7391304347826086, 0.32640812557710064, 0.855, 0.0, 0.0, 0.08333333333333333, 0.5074792138140299, 0.5345970492197496, 0.0, 1.0, 0.2, 0.4751177935185605], 
reward next is 0.5249, 
noisyNet noise sample is [array([-0.8739415], dtype=float32), -0.24476573]. 
=============================================
[2019-04-09 14:47:31,663] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05078271 0.11225409 0.08848727 0.09007935 0.08877394 0.05550214
 0.11090771 0.10001921 0.08085403 0.09921537 0.12312421], sum to 1.0000
[2019-04-09 14:47:31,663] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-09 14:47:31,687] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.3, 79.50000000000001, 0.0, 0.0, 19.0, 23.37943062629042, -0.05879264775283719, 0.0, 1.0, 55.0, 54.71062556035677], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1897800.0000, 
sim time next is 1898400.0000, 
raw observation next is [-7.300000000000001, 80.0, 0.0, 0.0, 19.0, 23.40329373211532, -0.0611958233867494, 0.0, 1.0, 25.0, 39.10540030396508], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.8, 0.0, 0.0, 0.08333333333333333, 0.4502744776762766, 0.4796013922044169, 0.0, 1.0, 0.2, 0.39105400303965077], 
reward next is 0.6089, 
noisyNet noise sample is [array([-0.14621866], dtype=float32), 0.2671818]. 
=============================================
[2019-04-09 14:47:31,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.06941457 0.10118786 0.085237   0.08474217 0.09090506 0.07038863
 0.10115355 0.08725766 0.07574031 0.10966901 0.12430415], sum to 1.0000
[2019-04-09 14:47:31,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9048
[2019-04-09 14:47:31,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04974202 0.11558316 0.09055085 0.07315107 0.08574259 0.05759996
 0.11091824 0.10512524 0.07166025 0.12255953 0.11736713], sum to 1.0000
[2019-04-09 14:47:31,712] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6686
[2019-04-09 14:47:31,718] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.199999999999999, 79.0, 0.0, 0.0, 19.0, 24.11590978771464, 0.1068770153940062, 0.0, 1.0, 30.0, 42.44653919103382], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1834800.0000, 
sim time next is 1835400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 19.0, 24.11467721802136, 0.1096092458742475, 0.0, 1.0, 65.0, 69.87105348887539], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.5095564348351133, 0.5365364152914158, 0.0, 1.0, 1.0, 0.6987105348887539], 
reward next is 0.3013, 
noisyNet noise sample is [array([-1.1700546], dtype=float32), 0.37054235]. 
=============================================
[2019-04-09 14:47:31,727] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.05, 84.5, 0.0, 0.0, 19.0, 23.69825828774321, 0.0409166269034936, 0.0, 1.0, 35.0, 51.83846352591213], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1884600.0000, 
sim time next is 1885200.0000, 
raw observation next is [-5.233333333333333, 85.0, 0.0, 0.0, 19.0, 23.79834611733805, 0.04174378064549183, 0.0, 1.0, 50.0, 36.84461737577963], 
processed observation next is [0.0, 0.8260869565217391, 0.31763619575253926, 0.85, 0.0, 0.0, 0.08333333333333333, 0.4831955097781708, 0.5139145935484973, 0.0, 1.0, 0.7, 0.36844617375779626], 
reward next is 0.6316, 
noisyNet noise sample is [array([1.9645909], dtype=float32), 0.037785724]. 
=============================================
[2019-04-09 14:47:31,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04922209 0.11640478 0.08139505 0.0833939  0.09655584 0.06156758
 0.10815381 0.10184735 0.07589567 0.09733798 0.12822586], sum to 1.0000
[2019-04-09 14:47:31,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6370
[2019-04-09 14:47:31,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05857828 0.10721619 0.07976615 0.09633097 0.08916721 0.06733635
 0.10518906 0.08860912 0.08472033 0.10290883 0.12017751], sum to 1.0000
[2019-04-09 14:47:31,807] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0651
[2019-04-09 14:47:31,819] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 19.0, 23.97244206539246, 0.03091467171225743, 0.0, 1.0, 60.0, 56.09102474187842], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 19.0, 23.94728797606287, 0.02394278694464803, 0.0, 1.0, 50.0, 45.81571214309224], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.08333333333333333, 0.49560733133857254, 0.5079809289815493, 0.0, 1.0, 0.7, 0.4581571214309224], 
reward next is 0.5418, 
noisyNet noise sample is [array([-1.6259162], dtype=float32), 0.7794166]. 
=============================================
[2019-04-09 14:47:31,824] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 19.0, 24.1101839735781, 0.1054965141611103, 0.0, 1.0, 30.0, 40.62970480128395], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 19.0, 24.09120477332503, 0.08966019634495132, 0.0, 1.0, 20.0, 41.0085959242411], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.5076003977770857, 0.5298867321149837, 0.0, 1.0, 0.1, 0.410085959242411], 
reward next is 0.5899, 
noisyNet noise sample is [array([-0.5976418], dtype=float32), 0.15532696]. 
=============================================
[2019-04-09 14:47:31,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[0.27468836]
 [0.38524926]
 [0.34808683]
 [0.30158973]
 [0.38450634]], R is [[0.86068046]
 [1.29116344]
 [1.7506572 ]
 [2.29774451]
 [2.8288579 ]].
[2019-04-09 14:47:31,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04578088 0.1215457  0.08010064 0.08939541 0.08698105 0.05593454
 0.11537342 0.09675682 0.0786146  0.10843232 0.12108462], sum to 1.0000
[2019-04-09 14:47:31,977] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4166
[2019-04-09 14:47:32,002] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 80.83333333333334, 0.0, 0.0, 19.0, 23.33214658103426, -0.0806691408186505, 0.0, 1.0, 55.0, 55.53795863666353], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1901400.0000, 
sim time next is 1902000.0000, 
raw observation next is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 23.3815155968655, -0.07530653121671753, 0.0, 1.0, 35.0, 40.00753965738623], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.4484596330721251, 0.4748978229277608, 0.0, 1.0, 0.4, 0.4000753965738623], 
reward next is 0.5999, 
noisyNet noise sample is [array([0.82709074], dtype=float32), -1.2002532]. 
=============================================
[2019-04-09 14:47:32,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[0.09650794]
 [0.3087678 ]
 [0.22539404]
 [0.22941557]
 [0.20820624]], R is [[0.91179824]
 [1.34730065]
 [1.64957869]
 [2.23023748]
 [2.75149155]].
[2019-04-09 14:47:32,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04323965 0.09587267 0.08530498 0.07664794 0.06367528 0.04465736
 0.13414796 0.08608679 0.10725671 0.11499426 0.14811638], sum to 1.0000
[2019-04-09 14:47:32,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3537
[2019-04-09 14:47:32,477] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 22.92450343105234, -0.2099659686004695, 0.0, 1.0, 45.0, 44.34861117896686], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1916400.0000, 
sim time next is 1917000.0000, 
raw observation next is [-8.65, 80.0, 0.0, 0.0, 19.0, 22.99757339712459, -0.2147308427369809, 0.0, 1.0, 35.0, 43.79061078488867], 
processed observation next is [1.0, 0.17391304347826086, 0.22299168975069253, 0.8, 0.0, 0.0, 0.08333333333333333, 0.4164644497603825, 0.42842305242100637, 0.0, 1.0, 0.4, 0.4379061078488867], 
reward next is 0.5621, 
noisyNet noise sample is [array([-0.38624945], dtype=float32), 0.026037073]. 
=============================================
[2019-04-09 14:47:32,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.3220756 ]
 [0.35173798]
 [0.33833265]
 [0.35153025]
 [0.35800624]], R is [[0.92939484]
 [1.47661483]
 [1.92541575]
 [2.21797132]
 [2.53290844]].
[2019-04-09 14:47:32,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04265802 0.09191546 0.08487526 0.0672361  0.05767938 0.04658517
 0.12968543 0.09968402 0.09422691 0.11355349 0.17190084], sum to 1.0000
[2019-04-09 14:47:32,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6786
[2019-04-09 14:47:32,756] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-9.0, 83.50000000000001, 0.0, 0.0, 19.0, 23.00927241713521, -0.2402786136665757, 0.0, 1.0, 50.0, 38.86979077096969], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1923000.0000, 
sim time next is 1923600.0000, 
raw observation next is [-9.100000000000001, 85.0, 0.0, 0.0, 19.0, 22.95340912863966, -0.2572524741097196, 0.0, 1.0, 55.0, 45.85080783423079], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.85, 0.0, 0.0, 0.08333333333333333, 0.41278409405330496, 0.41424917529676014, 0.0, 1.0, 0.8, 0.45850807834230795], 
reward next is 0.5415, 
noisyNet noise sample is [array([-1.4757297], dtype=float32), -0.95793486]. 
=============================================
[2019-04-09 14:47:32,903] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04732703 0.08631133 0.08273619 0.0618392  0.05926576 0.04625282
 0.1426803  0.10458546 0.10455774 0.1125108  0.15193336], sum to 1.0000
[2019-04-09 14:47:32,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-09 14:47:32,919] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 22.5, 22.91442257968189, -0.2227553958333756, 1.0, 1.0, 45.0, 37.78501518256253], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1927200.0000, 
sim time next is 1927800.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 22.5, 22.98815315863578, -0.2243087018093588, 1.0, 1.0, 45.0, 37.14477397644114], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.375, 0.4156794298863149, 0.4252304327302137, 1.0, 1.0, 0.6, 0.37144773976441137], 
reward next is 0.6286, 
noisyNet noise sample is [array([-0.8102606], dtype=float32), -0.8369471]. 
=============================================
[2019-04-09 14:47:33,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04937732 0.09350029 0.08525286 0.07579022 0.06394462 0.05150354
 0.11977128 0.09587292 0.09647886 0.10979265 0.15871532], sum to 1.0000
[2019-04-09 14:47:33,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4711
[2019-04-09 14:47:33,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04276024 0.09642813 0.09107299 0.06680925 0.05816369 0.04821955
 0.14189996 0.09867869 0.09936266 0.11282112 0.14378373], sum to 1.0000
[2019-04-09 14:47:33,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9532
[2019-04-09 14:47:33,199] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 78.0, 0.0, 0.0, 19.0, 22.53408798562343, -0.284941941985267, 0.0, 1.0, 20.0, 42.27148137367252], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1909800.0000, 
sim time next is 1910400.0000, 
raw observation next is [-8.2, 78.0, 0.0, 0.0, 19.0, 22.59610833425075, -0.2760690148781739, 0.0, 1.0, 45.0, 34.61854713021376], 
processed observation next is [1.0, 0.08695652173913043, 0.23545706371191139, 0.78, 0.0, 0.0, 0.08333333333333333, 0.38300902785422924, 0.4079769950406087, 0.0, 1.0, 0.6, 0.3461854713021376], 
reward next is 0.6538, 
noisyNet noise sample is [array([-0.82227373], dtype=float32), -0.047923055]. 
=============================================
[2019-04-09 14:47:33,204] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 23.58320513928305, -0.0657843727128748, 0.0, 1.0, 65.0, 64.79065595352567], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1914000.0000, 
sim time next is 1914600.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 19.0, 23.65229116894874, -0.04992562696305791, 0.0, 1.0, 65.0, 63.58786081833634], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.08333333333333333, 0.4710242640790616, 0.48335812434564734, 0.0, 1.0, 1.0, 0.6358786081833634], 
reward next is 0.3641, 
noisyNet noise sample is [array([-0.10600361], dtype=float32), -1.1850855]. 
=============================================
[2019-04-09 14:47:33,429] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05032964 0.12872697 0.09240372 0.07355321 0.08437923 0.06047014
 0.1143653  0.08668406 0.06899437 0.1050117  0.13508159], sum to 1.0000
[2019-04-09 14:47:33,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1725
[2019-04-09 14:47:33,441] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 78.5, 0.0, 0.0, 19.0, 23.5906404348173, -0.0410911332798066, 0.0, 1.0, 35.0, 47.15949217138155], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1902600.0000, 
sim time next is 1903200.0000, 
raw observation next is [-7.3, 77.33333333333334, 0.0, 0.0, 19.0, 23.63410397817093, -0.05017047111714029, 0.0, 1.0, 20.0, 39.1902648668743], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.46950866484757753, 0.4832765096276199, 0.0, 1.0, 0.1, 0.391902648668743], 
reward next is 0.6081, 
noisyNet noise sample is [array([-1.2577208], dtype=float32), -0.89822096]. 
=============================================
[2019-04-09 14:47:33,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02847583 0.14260304 0.10490561 0.08896769 0.05948885 0.04131084
 0.08969469 0.10453761 0.12480902 0.07135829 0.14384843], sum to 1.0000
[2019-04-09 14:47:33,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5190
[2019-04-09 14:47:33,714] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.733333333333333, 77.66666666666667, 156.6666666666667, 288.1666666666667, 22.5, 25.01943202723736, 0.2388623113276559, 1.0, 1.0, 30.0, 34.12621418099667], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1938000.0000, 
sim time next is 1938600.0000, 
raw observation next is [-6.449999999999999, 77.0, 171.0, 236.0, 22.5, 25.13173080262132, 0.2403574594891592, 1.0, 1.0, 45.0, 35.03760861767199], 
processed observation next is [1.0, 0.43478260869565216, 0.28393351800554023, 0.77, 0.57, 0.26077348066298345, 0.375, 0.5943109002184433, 0.5801191531630531, 1.0, 1.0, 0.6, 0.35037608617671995], 
reward next is 0.6496, 
noisyNet noise sample is [array([-0.694549], dtype=float32), 1.3838762]. 
=============================================
[2019-04-09 14:47:33,761] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04305487 0.09763395 0.07777256 0.07694005 0.05432694 0.04849425
 0.12129056 0.09317293 0.10550751 0.11056632 0.17124008], sum to 1.0000
[2019-04-09 14:47:33,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2903
[2019-04-09 14:47:33,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02008542 0.15600345 0.07878122 0.09308194 0.06876691 0.03120809
 0.09008553 0.07850701 0.1303919  0.07746744 0.1756211 ], sum to 1.0000
[2019-04-09 14:47:33,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9099
[2019-04-09 14:47:33,787] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.0, 83.50000000000001, 0.0, 0.0, 19.0, 23.49885871541937, -0.1107340864075373, 0.0, 1.0, 45.0, 41.76040298939681], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1923000.0000, 
sim time next is 1923600.0000, 
raw observation next is [-9.100000000000001, 85.0, 0.0, 0.0, 19.0, 23.47053526390172, -0.1307998041351564, 0.0, 1.0, 30.0, 39.46765432604502], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.85, 0.0, 0.0, 0.08333333333333333, 0.45587793865847664, 0.4564000652882812, 0.0, 1.0, 0.3, 0.3946765432604502], 
reward next is 0.6053, 
noisyNet noise sample is [array([0.3717291], dtype=float32), -0.25805482]. 
=============================================
[2019-04-09 14:47:33,794] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 65.0, 182.0, 2.0, 22.5, 24.9707996738056, 0.1900494599415621, 1.0, 1.0, 25.0, 35.869849134415], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1947600.0000, 
sim time next is 1948200.0000, 
raw observation next is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 22.5, 24.88859769207521, 0.1961376578378159, 1.0, 1.0, 65.0, 60.92989219728511], 
processed observation next is [1.0, 0.5652173913043478, 0.3568790397045245, 0.645, 0.5566666666666666, 0.00147329650092081, 0.375, 0.574049807672934, 0.565379219279272, 1.0, 1.0, 1.0, 0.6092989219728512], 
reward next is 0.3907, 
noisyNet noise sample is [array([-1.8381596], dtype=float32), -0.38941202]. 
=============================================
[2019-04-09 14:47:34,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03277623 0.12567632 0.09018284 0.08864703 0.05622291 0.04772601
 0.1040535  0.08697844 0.12815145 0.08212717 0.15745813], sum to 1.0000
[2019-04-09 14:47:34,098] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9449
[2019-04-09 14:47:34,129] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 86.0, 59.0, 285.0, 22.5, 24.22769888810531, 0.04986296880811433, 1.0, 1.0, 60.0, 50.51238375772838], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1933200.0000, 
sim time next is 1933800.0000, 
raw observation next is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 22.5, 24.3458232389624, 0.1009118588373383, 1.0, 1.0, 65.0, 63.21923153504249], 
processed observation next is [1.0, 0.391304347826087, 0.22345337026777473, 0.8483333333333334, 0.22555555555555556, 0.4121546961325968, 0.375, 0.5288186032468666, 0.5336372862791128, 1.0, 1.0, 1.0, 0.6321923153504249], 
reward next is 0.3678, 
noisyNet noise sample is [array([-0.6918717], dtype=float32), -1.1441069]. 
=============================================
[2019-04-09 14:47:34,191] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03494845 0.10206275 0.09118726 0.07436471 0.05408046 0.04599091
 0.11226179 0.08749275 0.11343779 0.09976974 0.18440333], sum to 1.0000
[2019-04-09 14:47:34,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0335
[2019-04-09 14:47:34,206] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-9.2, 88.5, 33.0, 21.0, 22.5, 23.25878065705595, -0.2833674382731291, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1931400.0000, 
sim time next is 1932000.0000, 
raw observation next is [-9.100000000000001, 87.66666666666666, 41.66666666666666, 109.0, 22.5, 23.0140252885549, -0.3253465702176241, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.21052631578947364, 0.8766666666666666, 0.13888888888888887, 0.12044198895027625, 0.375, 0.4178354407129084, 0.39155114326079193, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8556906], dtype=float32), 2.1987443]. 
=============================================
[2019-04-09 14:47:34,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.6540422 ]
 [0.5641479 ]
 [0.5544303 ]
 [0.46003562]
 [0.5010002 ]], R is [[1.47994637]
 [2.46514702]
 [3.01274562]
 [3.56653023]
 [4.10730028]].
[2019-04-09 14:47:34,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01968082 0.13951641 0.09364983 0.08805034 0.07177111 0.0315095
 0.1179245  0.08308301 0.12820868 0.07787565 0.14873017], sum to 1.0000
[2019-04-09 14:47:34,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-09 14:47:34,447] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 62.0, 99.33333333333333, 0.0, 22.5, 25.74340497147836, 0.4253207813867905, 1.0, 1.0, 55.0, 42.33365144567702], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1954200.0000, 
sim time next is 1954800.0000, 
raw observation next is [-2.8, 62.0, 93.0, 0.0, 22.5, 25.9618203236422, 0.440046505261468, 1.0, 1.0, 35.0, 38.5949438273766], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.31, 0.0, 0.375, 0.6634850269701834, 0.6466821684204893, 1.0, 1.0, 0.4, 0.38594943827376604], 
reward next is 0.6141, 
noisyNet noise sample is [array([1.5524799], dtype=float32), -0.02162908]. 
=============================================
[2019-04-09 14:47:34,575] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01969575 0.13296512 0.0972661  0.08563592 0.05782472 0.03152597
 0.09570825 0.09567685 0.12646434 0.08458921 0.1726477 ], sum to 1.0000
[2019-04-09 14:47:34,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0790
[2019-04-09 14:47:34,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04327876 0.09592892 0.08002245 0.0686579  0.05747356 0.04207598
 0.1350782  0.10711677 0.09331808 0.11194003 0.16510938], sum to 1.0000
[2019-04-09 14:47:34,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 62.0, 80.33333333333334, 0.0, 22.5, 26.49877338792764, 0.5469245642134757, 1.0, 1.0, 20.0, 36.88515081575955], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [-2.8, 62.0, 74.0, 0.0, 22.5, 26.5356852388694, 0.5404542656824477, 1.0, 1.0, 20.0, 36.80003180919481], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.24666666666666667, 0.0, 0.375, 0.7113071032391168, 0.6801514218941492, 1.0, 1.0, 0.1, 0.3680003180919481], 
reward next is 0.6320, 
noisyNet noise sample is [array([0.49366435], dtype=float32), -0.43784544]. 
=============================================
[2019-04-09 14:47:34,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4822
[2019-04-09 14:47:34,604] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 19.0, 23.42602197385796, -0.1128887575466991, 0.0, 1.0, 55.0, 54.27510339692081], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1918800.0000, 
sim time next is 1919400.0000, 
raw observation next is [-8.9, 82.00000000000001, 0.0, 0.0, 19.0, 23.44807269322838, -0.1194806284143521, 0.0, 1.0, 45.0, 41.13696329880946], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.8200000000000002, 0.0, 0.0, 0.08333333333333333, 0.45400605776903163, 0.46017312386188264, 0.0, 1.0, 0.6, 0.41136963298809465], 
reward next is 0.5886, 
noisyNet noise sample is [array([-0.3216497], dtype=float32), -0.59217674]. 
=============================================
[2019-04-09 14:47:34,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02254578 0.11602166 0.10347442 0.07792694 0.05956504 0.04007049
 0.12087536 0.09556444 0.11435452 0.07883456 0.17076677], sum to 1.0000
[2019-04-09 14:47:34,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1975
[2019-04-09 14:47:34,865] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.833333333333334, 76.33333333333334, 0.0, 0.0, 22.5, 25.78377729273494, 0.4629133060345418, 1.0, 1.0, 55.0, 43.94842242629917], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1966800.0000, 
sim time next is 1967400.0000, 
raw observation next is [-4.75, 75.0, 0.0, 0.0, 22.5, 25.64578742040653, 0.4455516073914538, 1.0, 1.0, 25.0, 37.19396705517244], 
processed observation next is [1.0, 0.782608695652174, 0.3310249307479225, 0.75, 0.0, 0.0, 0.375, 0.6371489517005443, 0.6485172024638179, 1.0, 1.0, 0.2, 0.37193967055172444], 
reward next is 0.6281, 
noisyNet noise sample is [array([-0.4321266], dtype=float32), 2.2262459]. 
=============================================
[2019-04-09 14:47:34,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02357134 0.12695381 0.0985178  0.094436   0.06035045 0.03351571
 0.10951361 0.09514069 0.12553193 0.07670642 0.15576226], sum to 1.0000
[2019-04-09 14:47:34,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1355
[2019-04-09 14:47:34,900] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 22.5, 25.1689701513747, 0.3574372476535869, 1.0, 1.0, 60.0, 49.04558027526946], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 22.5, 25.15212909431374, 0.348588765821569, 1.0, 1.0, 20.0, 44.42262716102012], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.375, 0.5960107578594783, 0.6161962552738564, 1.0, 1.0, 0.1, 0.4442262716102012], 
reward next is 0.5558, 
noisyNet noise sample is [array([0.17116609], dtype=float32), 0.058413327]. 
=============================================
[2019-04-09 14:47:35,070] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04638467 0.09406921 0.09143831 0.06265194 0.05731978 0.04920754
 0.11394845 0.09984606 0.09746336 0.11475791 0.17291285], sum to 1.0000
[2019-04-09 14:47:35,075] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4208
[2019-04-09 14:47:35,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02431078 0.1287071  0.0806067  0.08476053 0.05268812 0.0348401
 0.13225392 0.10102184 0.11143305 0.06692795 0.18244992], sum to 1.0000
[2019-04-09 14:47:35,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4995
[2019-04-09 14:47:35,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02339551 0.11793243 0.08085594 0.07444189 0.0522809  0.03295481
 0.12483463 0.10952348 0.10838128 0.08243135 0.19296777], sum to 1.0000
[2019-04-09 14:47:35,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9798
[2019-04-09 14:47:35,097] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.3, 78.0, 0.0, 0.0, 19.0, 22.90629004383607, -0.2163807518068966, 0.0, 1.0, 25.0, 28.92792324598092], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1911000.0000, 
sim time next is 1911600.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 19.0, 22.92648598528103, -0.2549864818949579, 0.0, 1.0, 45.0, 34.53857113768468], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.08333333333333333, 0.41054049877341914, 0.41500450603501404, 0.0, 1.0, 0.6, 0.34538571137684676], 
reward next is 0.6546, 
noisyNet noise sample is [array([1.4193918], dtype=float32), 0.16818433]. 
=============================================
[2019-04-09 14:47:35,107] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.416666666666667, 81.0, 0.0, 0.0, 22.5, 25.23529759971772, 0.4124767147769699, 0.0, 1.0, 65.0, 64.00824265455769], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1972200.0000, 
sim time next is 1972800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 22.5, 25.18605685057177, 0.4122904627454778, 0.0, 1.0, 45.0, 50.17349194664038], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.375, 0.5988380708809808, 0.6374301542484926, 0.0, 1.0, 0.6, 0.5017349194664038], 
reward next is 0.4983, 
noisyNet noise sample is [array([1.0237451], dtype=float32), -0.06472088]. 
=============================================
[2019-04-09 14:47:35,111] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 80.5, 0.0, 0.0, 19.0, 24.74684643753401, 0.2819014727478699, 0.0, 1.0, 55.0, 42.91937948243793], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 19.0, 24.68227536353336, 0.2888389723526285, 0.0, 1.0, 65.0, 67.13350806602031], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5568562802944467, 0.5962796574508762, 0.0, 1.0, 1.0, 0.6713350806602031], 
reward next is 0.3287, 
noisyNet noise sample is [array([-0.6270163], dtype=float32), 1.2102286]. 
=============================================
[2019-04-09 14:47:35,171] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02372583 0.13446088 0.08849382 0.0846419  0.0523663  0.03296833
 0.12089866 0.0999454  0.10489138 0.08355065 0.17405681], sum to 1.0000
[2019-04-09 14:47:35,171] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0828
[2019-04-09 14:47:35,190] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 19.0, 24.68180789738919, 0.2948080020565569, 0.0, 1.0, 55.0, 43.58967545014994], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1976400.0000, 
sim time next is 1977000.0000, 
raw observation next is [-5.7, 78.83333333333333, 0.0, 0.0, 19.0, 24.69914105423983, 0.2856651579910851, 0.0, 1.0, 20.0, 38.18670813918784], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.7883333333333333, 0.0, 0.0, 0.08333333333333333, 0.5582617545199859, 0.5952217193303617, 0.0, 1.0, 0.1, 0.3818670813918784], 
reward next is 0.6181, 
noisyNet noise sample is [array([-0.6270163], dtype=float32), 1.2102286]. 
=============================================
[2019-04-09 14:47:35,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.9349788 ]
 [0.78955704]
 [0.8847552 ]
 [0.8782931 ]
 [0.94881505]], R is [[1.52431536]
 [2.07317543]
 [2.54911733]
 [2.85229111]
 [3.3945744 ]].
[2019-04-09 14:47:35,591] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0363515  0.08638597 0.08664151 0.0640237  0.05677545 0.04251532
 0.14039963 0.11448865 0.09012029 0.10243011 0.17986782], sum to 1.0000
[2019-04-09 14:47:35,591] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7218
[2019-04-09 14:47:35,616] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.39581432722866, 0.1891618016714471, 0.0, 1.0, 45.0, 44.20194920338443], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1985400.0000, 
sim time next is 1986000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.37035997026909, 0.1840325818033612, 0.0, 1.0, 60.0, 49.87656076010208], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5308633308557574, 0.5613441939344538, 0.0, 1.0, 0.9, 0.4987656076010208], 
reward next is 0.5012, 
noisyNet noise sample is [array([0.16170077], dtype=float32), -0.92638355]. 
=============================================
[2019-04-09 14:47:35,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.6121912]
 [0.6836127]
 [0.4789389]
 [0.5253997]
 [0.6632309]], R is [[1.15456676]
 [1.70100164]
 [2.18290138]
 [2.64344668]
 [2.96778011]].
[2019-04-09 14:47:35,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03625334 0.08159041 0.09366328 0.05555582 0.05578014 0.04141935
 0.13826293 0.10654757 0.08863603 0.10992007 0.19237109], sum to 1.0000
[2019-04-09 14:47:35,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1029
[2019-04-09 14:47:35,839] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 19.0, 24.18416751155159, 0.1294521362484116, 0.0, 1.0, 20.0, 35.34322303531033], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 24.16060580371829, 0.1095078688728427, 0.0, 1.0, 20.0, 33.73932884400394], 
processed observation next is [1.0, 0.043478260869565216, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5133838169765242, 0.5365026229576143, 0.0, 1.0, 0.1, 0.3373932884400394], 
reward next is 0.6626, 
noisyNet noise sample is [array([0.15628122], dtype=float32), 0.5856215]. 
=============================================
[2019-04-09 14:47:36,207] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04176343 0.07767007 0.08328758 0.06161521 0.0585972  0.0464519
 0.14702986 0.09136905 0.09288609 0.12708864 0.17224097], sum to 1.0000
[2019-04-09 14:47:36,210] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-09 14:47:36,227] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 24.33630695721316, 0.1488548885492887, 0.0, 1.0, 25.0, 44.50022458321472], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.3272190996793, 0.149592225488165, 0.0, 1.0, 30.0, 34.28283541572637], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5272682583066084, 0.5498640751627216, 0.0, 1.0, 0.3, 0.34282835415726365], 
reward next is 0.6572, 
noisyNet noise sample is [array([0.84859014], dtype=float32), 1.6808553]. 
=============================================
[2019-04-09 14:47:36,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03735973 0.09046925 0.08905821 0.07259671 0.05796758 0.04834227
 0.13508637 0.0801651  0.11397851 0.10505286 0.16992344], sum to 1.0000
[2019-04-09 14:47:36,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9100
[2019-04-09 14:47:36,264] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.50366173455348, 0.1608927793597617, 0.0, 1.0, 45.0, 35.75221946666655], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1999200.0000, 
sim time next is 1999800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.49330133786658, 0.1586889802410283, 0.0, 1.0, 65.0, 63.34606150207431], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.541108444822215, 0.5528963267470094, 0.0, 1.0, 1.0, 0.6334606150207431], 
reward next is 0.3665, 
noisyNet noise sample is [array([-1.4483831], dtype=float32), -0.7843687]. 
=============================================
[2019-04-09 14:47:36,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01958949 0.12456951 0.09465561 0.0801013  0.05646255 0.03626044
 0.101114   0.0965538  0.14134979 0.07898495 0.17035858], sum to 1.0000
[2019-04-09 14:47:36,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2767
[2019-04-09 14:47:36,310] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 62.0, 93.0, 0.0, 22.5, 26.02196637124233, 0.4574454139470488, 1.0, 1.0, 60.0, 46.07332799019412], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1954800.0000, 
sim time next is 1955400.0000, 
raw observation next is [-2.8, 62.0, 86.66666666666666, 0.0, 22.5, 26.03101785854751, 0.4702266509993351, 1.0, 1.0, 30.0, 44.38077762648768], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.28888888888888886, 0.0, 0.375, 0.6692514882122925, 0.6567422169997784, 1.0, 1.0, 0.3, 0.4438077762648768], 
reward next is 0.5562, 
noisyNet noise sample is [array([0.13758266], dtype=float32), -0.81753993]. 
=============================================
[2019-04-09 14:47:36,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04231945 0.08300994 0.09511834 0.05855111 0.06107435 0.0545562
 0.13641214 0.09605067 0.08902551 0.11775335 0.16612889], sum to 1.0000
[2019-04-09 14:47:36,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9068
[2019-04-09 14:47:36,418] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 22.99290453676753, -0.1347895460110449, 0.0, 1.0, 25.0, 25.52723567196718], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 22.97512657640905, -0.1308880705174495, 0.0, 1.0, 65.0, 53.64890273824329], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4145938813674208, 0.45637064316085013, 0.0, 1.0, 1.0, 0.5364890273824329], 
reward next is 0.4635, 
noisyNet noise sample is [array([-0.0803486], dtype=float32), 0.6469343]. 
=============================================
[2019-04-09 14:47:36,919] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03647518 0.10203228 0.08679072 0.08931455 0.05829759 0.0421247
 0.11504275 0.08083338 0.13464019 0.10171414 0.15273453], sum to 1.0000
[2019-04-09 14:47:36,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-09 14:47:36,938] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 87.0, 10.33333333333333, 0.0, 22.5, 24.53902275933696, 0.1319832991403598, 1.0, 1.0, 55.0, 39.61852867487801], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2015400.0000, 
sim time next is 2016000.0000, 
raw observation next is [-6.2, 87.0, 15.0, 0.0, 22.5, 24.45279493497794, 0.1210468021933243, 1.0, 1.0, 20.0, 35.9048469085069], 
processed observation next is [1.0, 0.34782608695652173, 0.2908587257617729, 0.87, 0.05, 0.0, 0.375, 0.5377329112481618, 0.5403489340644414, 1.0, 1.0, 0.1, 0.359048469085069], 
reward next is 0.6410, 
noisyNet noise sample is [array([-0.70783496], dtype=float32), -0.015126383]. 
=============================================
[2019-04-09 14:47:36,950] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04658107 0.08834604 0.0865338  0.0711771  0.06434536 0.04561974
 0.13625202 0.09266973 0.09742402 0.10624215 0.16480894], sum to 1.0000
[2019-04-09 14:47:36,952] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2813
[2019-04-09 14:47:36,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.55827564]
 [0.5848178 ]
 [0.60647035]
 [0.5903328 ]
 [0.5739035 ]], R is [[1.20531607]
 [1.79707766]
 [2.40633297]
 [2.99127102]
 [3.5629735 ]].
[2019-04-09 14:47:36,965] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 23.87157786040628, 0.09326430743082886, 0.0, 1.0, 20.0, 50.5564907199372], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2012400.0000, 
sim time next is 2013000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 22.5, 24.21168158507972, 0.1045677196359932, 0.0, 1.0, 30.0, 41.99913291265889], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.5176401320899767, 0.5348559065453311, 0.0, 1.0, 0.3, 0.4199913291265889], 
reward next is 0.5800, 
noisyNet noise sample is [array([-2.002592], dtype=float32), -1.7494253]. 
=============================================
[2019-04-09 14:47:36,973] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.40008393]
 [0.39196476]
 [0.49111173]
 [0.3587576 ]
 [0.3828692 ]], R is [[1.02969313]
 [1.51383126]
 [1.82659137]
 [2.11236167]
 [2.47497535]].
[2019-04-09 14:47:37,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03981464 0.11165566 0.0767765  0.07503277 0.05903698 0.04716858
 0.12258887 0.08961211 0.11703774 0.0895028  0.17177334], sum to 1.0000
[2019-04-09 14:47:37,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2546
[2019-04-09 14:47:37,057] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 23.59972697200275, -0.04741317574241249, 0.0, 1.0, 60.0, 49.10282235020729], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2010600.0000, 
sim time next is 2011200.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 19.0, 23.49690092012254, -0.04462693463809896, 0.0, 1.0, 30.0, 45.70799371585858], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.4580750766768782, 0.4851243551206337, 0.0, 1.0, 0.3, 0.4570799371585858], 
reward next is 0.5429, 
noisyNet noise sample is [array([-0.4478891], dtype=float32), -0.72089887]. 
=============================================
[2019-04-09 14:47:37,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03766923 0.07801865 0.08777463 0.0636705  0.06315418 0.0400194
 0.13615997 0.11538143 0.08547705 0.11883352 0.1738414 ], sum to 1.0000
[2019-04-09 14:47:37,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0265
[2019-04-09 14:47:37,089] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 19.0, 24.20004225621218, 0.1414035793138081, 0.0, 1.0, 45.0, 38.19003158362328], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 24.20594674731574, 0.03926566062673807, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5171622289429783, 0.513088553542246, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45354646], dtype=float32), 0.9377351]. 
=============================================
[2019-04-09 14:47:37,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03259952 0.10445785 0.08329095 0.08635007 0.0566999  0.0443541
 0.11720662 0.09028693 0.12318989 0.08712268 0.1744414 ], sum to 1.0000
[2019-04-09 14:47:37,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3451
[2019-04-09 14:47:37,127] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 22.5, 24.87679737200735, 0.2587690328535122, 1.0, 1.0, 55.0, 47.5570850159764], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2018400.0000, 
sim time next is 2019000.0000, 
raw observation next is [-6.033333333333333, 86.16666666666667, 42.33333333333333, 0.0, 22.5, 25.17707111253225, 0.2787351069474295, 1.0, 1.0, 35.0, 37.53914376656994], 
processed observation next is [1.0, 0.34782608695652173, 0.29547553093259465, 0.8616666666666667, 0.1411111111111111, 0.0, 0.375, 0.5980892593776875, 0.5929117023158098, 1.0, 1.0, 0.4, 0.37539143766569943], 
reward next is 0.6246, 
noisyNet noise sample is [array([0.17959581], dtype=float32), -0.6498914]. 
=============================================
[2019-04-09 14:47:37,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.60824114]
 [0.6795641 ]
 [0.6510716 ]
 [0.60629433]
 [0.4979444 ]], R is [[1.25401902]
 [1.765908  ]
 [2.11365652]
 [2.69993353]
 [3.18419623]].
[2019-04-09 14:47:37,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02906601 0.10857065 0.09188177 0.08900022 0.05477988 0.03947164
 0.1154721  0.08087933 0.13326612 0.08457497 0.17303728], sum to 1.0000
[2019-04-09 14:47:37,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6695
[2019-04-09 14:47:37,297] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.033333333333333, 86.16666666666667, 42.33333333333333, 0.0, 22.5, 25.18161008398233, 0.2598134818213677, 1.0, 1.0, 55.0, 42.39705487308596], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2019000.0000, 
sim time next is 2019600.0000, 
raw observation next is [-6.0, 86.0, 49.0, 0.0, 22.5, 25.19164414811838, 0.2603347858206395, 1.0, 1.0, 55.0, 44.27379644748589], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.86, 0.16333333333333333, 0.0, 0.375, 0.5993036790098651, 0.5867782619402132, 1.0, 1.0, 0.8, 0.4427379644748589], 
reward next is 0.5573, 
noisyNet noise sample is [array([0.7032777], dtype=float32), -0.19654897]. 
=============================================
[2019-04-09 14:47:37,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.03362071 0.09682363 0.09787811 0.07057872 0.05422514 0.03887269
 0.1159091  0.07700066 0.12634626 0.10614669 0.18259832], sum to 1.0000
[2019-04-09 14:47:37,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7595
[2019-04-09 14:47:37,391] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 22.5, 24.00575556391507, 0.0704809834627196, 1.0, 1.0, 30.0, 40.48234212334207], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2017200.0000, 
sim time next is 2017800.0000, 
raw observation next is [-6.1, 86.5, 29.0, 0.0, 22.5, 24.26978353520518, 0.0998069337037199, 1.0, 1.0, 60.0, 49.04054489207238], 
processed observation next is [1.0, 0.34782608695652173, 0.29362880886426596, 0.865, 0.09666666666666666, 0.0, 0.375, 0.5224819612670982, 0.53326897790124, 1.0, 1.0, 0.9, 0.49040544892072385], 
reward next is 0.5096, 
noisyNet noise sample is [array([-0.33786333], dtype=float32), -0.44281837]. 
=============================================
[2019-04-09 14:47:37,743] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02012742 0.14205775 0.09362621 0.09503277 0.06004925 0.03770876
 0.09410527 0.0792611  0.13887174 0.08303975 0.15612006], sum to 1.0000
[2019-04-09 14:47:37,747] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9388
[2019-04-09 14:47:37,762] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.533333333333333, 70.66666666666667, 25.83333333333333, 0.3333333333333333, 22.5, 25.39601113832204, 0.3385254453146493, 1.0, 1.0, 50.0, 36.07214784710717], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1960800.0000, 
sim time next is 1961400.0000, 
raw observation next is [-3.716666666666666, 72.83333333333333, 21.66666666666667, 0.6666666666666665, 22.5, 25.45750408331681, 0.3584279575257632, 1.0, 1.0, 65.0, 61.51217316102377], 
processed observation next is [1.0, 0.6956521739130435, 0.3596491228070176, 0.7283333333333333, 0.07222222222222224, 0.000736648250460405, 0.375, 0.6214586736097342, 0.6194759858419211, 1.0, 1.0, 1.0, 0.6151217316102376], 
reward next is 0.3849, 
noisyNet noise sample is [array([-0.4105581], dtype=float32), 0.820012]. 
=============================================
[2019-04-09 14:47:38,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.022982   0.12249278 0.10063612 0.08921965 0.06397729 0.03138373
 0.09574898 0.10617996 0.12373608 0.08593365 0.15770978], sum to 1.0000
[2019-04-09 14:47:38,595] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4724
[2019-04-09 14:47:38,614] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 22.5, 24.71306324216783, 0.3767180743631039, 1.0, 1.0, 40.0, 34.21561375748832], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 22.5, 25.26875316786525, 0.4225807885396988, 1.0, 1.0, 45.0, 32.43155553235513], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.375, 0.6057294306554374, 0.6408602628465663, 1.0, 1.0, 0.6, 0.3243155553235513], 
reward next is 0.6757, 
noisyNet noise sample is [array([1.2034185], dtype=float32), -0.26911333]. 
=============================================
[2019-04-09 14:47:38,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02170332 0.12234944 0.09292139 0.08813084 0.06283883 0.03474145
 0.10983932 0.08630996 0.13971178 0.08042469 0.16102901], sum to 1.0000
[2019-04-09 14:47:38,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3444
[2019-04-09 14:47:38,999] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 22.5, 25.62884498790404, 0.3795920974056599, 1.0, 1.0, 55.0, 42.51300429464652], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2029200.0000, 
sim time next is 2029800.0000, 
raw observation next is [-4.683333333333333, 76.33333333333333, 150.0, 0.0, 22.5, 25.64222485235331, 0.3877185274039219, 1.0, 1.0, 35.0, 38.17320964005999], 
processed observation next is [1.0, 0.4782608695652174, 0.3328716528162512, 0.7633333333333333, 0.5, 0.0, 0.375, 0.6368520710294424, 0.6292395091346407, 1.0, 1.0, 0.4, 0.38173209640059985], 
reward next is 0.6183, 
noisyNet noise sample is [array([0.4348367], dtype=float32), -0.706115]. 
=============================================
[2019-04-09 14:47:39,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02361296 0.11697301 0.08517857 0.08127149 0.05699399 0.03285108
 0.10980064 0.09434795 0.11945538 0.08443405 0.19508086], sum to 1.0000
[2019-04-09 14:47:39,060] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5520
[2019-04-09 14:47:39,083] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 22.5, 25.64081066649972, 0.5010233798773019, 1.0, 1.0, 45.0, 39.96222827141761], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2058600.0000, 
sim time next is 2059200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 22.5, 25.57351811633414, 0.4900920319247246, 0.0, 1.0, 50.0, 38.42938606154176], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.375, 0.6311265096945116, 0.6633640106415749, 0.0, 1.0, 0.7, 0.38429386061541765], 
reward next is 0.6157, 
noisyNet noise sample is [array([-0.58395326], dtype=float32), -0.51341856]. 
=============================================
[2019-04-09 14:47:39,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02585752 0.11910486 0.09252163 0.05620332 0.05659736 0.03955707
 0.1284747  0.09829798 0.08523702 0.09022249 0.20792612], sum to 1.0000
[2019-04-09 14:47:39,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9274
[2019-04-09 14:47:39,281] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 19.0, 25.26904768029866, 0.4647966426439427, 0.0, 1.0, 65.0, 61.26929955837853], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2062800.0000, 
sim time next is 2063400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 19.0, 25.30925768352623, 0.4742354516613176, 0.0, 1.0, 65.0, 54.87474068669083], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.08333333333333333, 0.6091048069605192, 0.6580784838871059, 0.0, 1.0, 1.0, 0.5487474068669083], 
reward next is 0.4513, 
noisyNet noise sample is [array([0.00813536], dtype=float32), -1.3171518]. 
=============================================
[2019-04-09 14:47:39,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02326642 0.1345206  0.09036127 0.06660668 0.05931664 0.03163639
 0.13030797 0.09928288 0.09747589 0.08324438 0.18398082], sum to 1.0000
[2019-04-09 14:47:39,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2095
[2019-04-09 14:47:39,318] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 19.0, 25.30925768352623, 0.4742354516613176, 0.0, 1.0, 65.0, 54.87474068669083], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2063400.0000, 
sim time next is 2064000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 19.0, 25.38524643971776, 0.48083388100213, 0.0, 1.0, 25.0, 44.63971667906596], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6154372033098134, 0.6602779603340433, 0.0, 1.0, 0.2, 0.4463971667906596], 
reward next is 0.5536, 
noisyNet noise sample is [array([0.00813536], dtype=float32), -1.3171518]. 
=============================================
[2019-04-09 14:47:39,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.97041404]
 [0.9529221 ]
 [1.0089927 ]
 [1.0061346 ]
 [0.9380983 ]], R is [[1.597458  ]
 [2.03273606]
 [2.39971566]
 [2.89161253]
 [3.52293587]].
[2019-04-09 14:47:39,373] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02412601 0.12592071 0.09151056 0.08880682 0.06717708 0.03670892
 0.11827685 0.09395123 0.1183693  0.08697686 0.1481756 ], sum to 1.0000
[2019-04-09 14:47:39,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1611
[2019-04-09 14:47:39,395] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 83.0, 102.0, 0.0, 22.5, 25.70972883840033, 0.4166851915289412, 1.0, 1.0, 55.0, 48.38935263032641], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [-5.6, 83.0, 109.5, 0.0, 22.5, 25.77937032797375, 0.4205483247782683, 1.0, 1.0, 25.0, 35.82529781362521], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.365, 0.0, 0.375, 0.6482808606644793, 0.6401827749260894, 1.0, 1.0, 0.2, 0.3582529781362521], 
reward next is 0.6417, 
noisyNet noise sample is [array([0.7505544], dtype=float32), 1.423095]. 
=============================================
[2019-04-09 14:47:39,538] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02824992 0.09186561 0.09228728 0.06124717 0.0510534  0.03298758
 0.12317726 0.10764617 0.08794317 0.11609387 0.20744857], sum to 1.0000
[2019-04-09 14:47:39,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4979
[2019-04-09 14:47:39,562] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.1, 83.33333333333334, 0.0, 0.0, 19.0, 25.1406684892305, 0.3727858946678776, 0.0, 1.0, 25.0, 42.23543296627614], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2067600.0000, 
sim time next is 2068200.0000, 
raw observation next is [-4.2, 84.0, 0.0, 0.0, 19.0, 25.11017757831154, 0.3550381488055021, 0.0, 1.0, 50.0, 36.96586297324725], 
processed observation next is [1.0, 0.9565217391304348, 0.34626038781163443, 0.84, 0.0, 0.0, 0.08333333333333333, 0.5925147981926283, 0.618346049601834, 0.0, 1.0, 0.7, 0.3696586297324725], 
reward next is 0.6303, 
noisyNet noise sample is [array([-0.3859065], dtype=float32), -0.783498]. 
=============================================
[2019-04-09 14:47:39,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02076602 0.1309675  0.10189803 0.10113641 0.05384028 0.03180655
 0.10777983 0.07994147 0.13423236 0.08274586 0.15488568], sum to 1.0000
[2019-04-09 14:47:39,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7104
[2019-04-09 14:47:39,631] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 82.66666666666667, 58.0, 0.0, 22.5, 26.12698035114974, 0.5056971402435665, 1.0, 1.0, 45.0, 31.48590127555902], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [-3.9, 82.0, 51.5, 0.0, 22.5, 26.11558247298735, 0.3937952487258727, 1.0, 1.0, 55.0, 48.44682126496389], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.17166666666666666, 0.0, 0.375, 0.6762985394156124, 0.6312650829086243, 1.0, 1.0, 0.8, 0.48446821264963885], 
reward next is 0.5155, 
noisyNet noise sample is [array([-1.6431142], dtype=float32), 1.0149484]. 
=============================================
[2019-04-09 14:47:39,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02914944 0.08475022 0.08961181 0.05623423 0.04507309 0.03802846
 0.1343092  0.09796504 0.10295651 0.12352252 0.19839945], sum to 1.0000
[2019-04-09 14:47:39,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-09 14:47:39,949] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.0, 24.75947402635708, 0.2962614554691745, 0.0, 1.0, 25.0, 36.46834795995196], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2076600.0000, 
sim time next is 2077200.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.0, 24.74924536371377, 0.2813560650819636, 0.0, 1.0, 30.0, 34.90898051259162], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.91, 0.0, 0.0, 0.08333333333333333, 0.562437113642814, 0.5937853550273212, 0.0, 1.0, 0.3, 0.3490898051259162], 
reward next is 0.6509, 
noisyNet noise sample is [array([-1.5808997], dtype=float32), 0.5274051]. 
=============================================
[2019-04-09 14:47:40,079] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02492001 0.12020941 0.08475476 0.07698318 0.04970979 0.03833463
 0.11900732 0.0861779  0.10590138 0.09474496 0.19925666], sum to 1.0000
[2019-04-09 14:47:40,080] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2615
[2019-04-09 14:47:40,102] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 22.5, 25.43764547831901, 0.347240800189866, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2053200.0000, 
sim time next is 2053800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 22.5, 25.09555116876625, 0.3638032855731198, 1.0, 1.0, 30.0, 44.80082646865286], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.375, 0.5912959307305208, 0.6212677618577066, 1.0, 1.0, 0.3, 0.4480082646865286], 
reward next is 0.5520, 
noisyNet noise sample is [array([-2.3668718], dtype=float32), -0.04799388]. 
=============================================
[2019-04-09 14:47:40,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03882375 0.10598571 0.08794651 0.07582048 0.06326137 0.03928073
 0.13163401 0.08767103 0.10931671 0.11315052 0.14710917], sum to 1.0000
[2019-04-09 14:47:40,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3407
[2019-04-09 14:47:40,200] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 24.83272926848366, 0.2043435426874884, 1.0, 1.0, 50.0, 41.79196674498452], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2014200.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 24.77797337097383, 0.1853904531095104, 1.0, 1.0, 35.0, 38.12063326483216], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.5648311142478191, 0.5617968177031701, 1.0, 1.0, 0.4, 0.3812063326483216], 
reward next is 0.6188, 
noisyNet noise sample is [array([0.15652612], dtype=float32), -0.55996835]. 
=============================================
[2019-04-09 14:47:40,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01626359 0.13180402 0.08927324 0.08855339 0.06202658 0.02972474
 0.10637755 0.09020734 0.13770105 0.0835303  0.16453817], sum to 1.0000
[2019-04-09 14:47:40,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6697
[2019-04-09 14:47:40,403] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.9, 82.0, 51.5, 0.0, 22.5, 26.28686882726838, 0.5575484845719049, 1.0, 1.0, 60.0, 70.0927872699689], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2044800.0000, 
sim time next is 2045400.0000, 
raw observation next is [-3.9, 82.00000000000001, 45.0, 0.0, 22.5, 26.24668402324883, 0.4475098571193816, 1.0, 1.0, 55.0, 49.67086745088482], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8200000000000002, 0.15, 0.0, 0.375, 0.687223668604069, 0.6491699523731272, 1.0, 1.0, 0.8, 0.4967086745088482], 
reward next is 0.5033, 
noisyNet noise sample is [array([-1.2644551], dtype=float32), 0.012129353]. 
=============================================
[2019-04-09 14:47:40,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02465127 0.11907057 0.08299845 0.09575411 0.05035003 0.0367598
 0.10380292 0.08034371 0.15408197 0.07286093 0.17932625], sum to 1.0000
[2019-04-09 14:47:40,607] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9039
[2019-04-09 14:47:40,621] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 85.5, 0.0, 22.5, 25.47465267280514, 0.3163249658866557, 1.0, 1.0, 25.0, 31.80753207563222], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2023200.0000, 
sim time next is 2023800.0000, 
raw observation next is [-5.600000000000001, 83.0, 91.00000000000001, 0.0, 22.5, 25.43840931197476, 0.3323066336809735, 1.0, 1.0, 65.0, 60.51939763434621], 
processed observation next is [1.0, 0.43478260869565216, 0.3074792243767313, 0.83, 0.3033333333333334, 0.0, 0.375, 0.6198674426645633, 0.6107688778936579, 1.0, 1.0, 1.0, 0.6051939763434622], 
reward next is 0.3948, 
noisyNet noise sample is [array([-0.45084625], dtype=float32), 0.7040827]. 
=============================================
[2019-04-09 14:47:41,049] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55321: loss 18.2812
[2019-04-09 14:47:41,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55322: learning rate 0.0000
[2019-04-09 14:47:41,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55326: loss 27.0439
[2019-04-09 14:47:41,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55326: learning rate 0.0000
[2019-04-09 14:47:41,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03167321 0.11515624 0.0868522  0.08573794 0.05479734 0.03890756
 0.11819731 0.07640605 0.13216628 0.09759355 0.16251224], sum to 1.0000
[2019-04-09 14:47:41,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3938
[2019-04-09 14:47:41,168] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 22.5, 24.17190540787744, 0.1752471682233917, 1.0, 1.0, 65.0, 63.22032478998116], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2101800.0000, 
sim time next is 2102400.0000, 
raw observation next is [-7.3, 79.0, 36.5, 18.5, 22.5, 24.25746977542638, 0.2130397758062487, 1.0, 1.0, 40.0, 48.92073623434346], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.12166666666666667, 0.020441988950276244, 0.375, 0.5214558146188649, 0.571013258602083, 1.0, 1.0, 0.5, 0.4892073623434346], 
reward next is 0.5108, 
noisyNet noise sample is [array([-0.2037716], dtype=float32), -0.44998875]. 
=============================================
[2019-04-09 14:47:41,174] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55393: loss 14.5019
[2019-04-09 14:47:41,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55393: learning rate 0.0000
[2019-04-09 14:47:41,258] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 55440: loss 20.3266
[2019-04-09 14:47:41,259] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 55440: learning rate 0.0000
[2019-04-09 14:47:41,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55544: loss 39.2491
[2019-04-09 14:47:41,451] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55545: learning rate 0.0000
[2019-04-09 14:47:41,489] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03318382 0.10437129 0.0925331  0.06480505 0.05039988 0.03475429
 0.14094722 0.10180812 0.07525022 0.10549673 0.19645032], sum to 1.0000
[2019-04-09 14:47:41,492] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-09 14:47:41,508] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.1, 83.33333333333334, 0.0, 0.0, 19.0, 25.55319142582945, 0.4308331376529421, 0.0, 1.0, 20.0, 42.23481850176114], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2067600.0000, 
sim time next is 2068200.0000, 
raw observation next is [-4.2, 84.0, 0.0, 0.0, 19.0, 25.41475291203383, 0.4177795759414613, 0.0, 1.0, 60.0, 48.4634109820249], 
processed observation next is [1.0, 0.9565217391304348, 0.34626038781163443, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6178960760028192, 0.6392598586471537, 0.0, 1.0, 0.9, 0.48463410982024896], 
reward next is 0.5154, 
noisyNet noise sample is [array([1.0043328], dtype=float32), 0.006601834]. 
=============================================
[2019-04-09 14:47:41,551] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55598: loss 25.5490
[2019-04-09 14:47:41,552] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55599: learning rate 0.0000
[2019-04-09 14:47:41,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02469162 0.13174936 0.08995662 0.0884935  0.05720944 0.03658633
 0.09707312 0.08770169 0.14242364 0.07139848 0.17271616], sum to 1.0000
[2019-04-09 14:47:41,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2331
[2019-04-09 14:47:41,669] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.799999999999999, 82.0, 179.6666666666667, 108.3333333333333, 22.5, 25.6419861866019, 0.4195610784340492, 1.0, 1.0, 40.0, 31.67912604823803], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2108400.0000, 
sim time next is 2109000.0000, 
raw observation next is [-7.8, 82.0, 185.3333333333333, 98.66666666666667, 22.5, 25.6887345473923, 0.4263999378567897, 1.0, 1.0, 25.0, 33.00597627447244], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.6177777777777776, 0.10902394106813997, 0.375, 0.6407278789493583, 0.6421333126189299, 1.0, 1.0, 0.2, 0.33005976274472437], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.6521499], dtype=float32), -1.4031138]. 
=============================================
[2019-04-09 14:47:41,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.8089341 ]
 [0.61407864]
 [0.7534531 ]
 [0.6545911 ]
 [0.73149234]], R is [[1.45891023]
 [2.12752986]
 [2.69190335]
 [3.19546914]
 [3.80989885]].
[2019-04-09 14:47:41,715] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55689: loss 25.2884
[2019-04-09 14:47:41,718] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55691: learning rate 0.0000
[2019-04-09 14:47:41,723] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02661243 0.10024534 0.08714686 0.07414982 0.05132227 0.03231861
 0.14729811 0.10181709 0.10138524 0.1003281  0.17737614], sum to 1.0000
[2019-04-09 14:47:41,728] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1452
[2019-04-09 14:47:41,748] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.1, 83.33333333333334, 0.0, 0.0, 19.0, 25.14957097703589, 0.3601627971286348, 0.0, 1.0, 35.0, 42.85089193416538], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2067600.0000, 
sim time next is 2068200.0000, 
raw observation next is [-4.2, 84.0, 0.0, 0.0, 19.0, 25.08240782793375, 0.3403684347867586, 0.0, 1.0, 25.0, 38.96872280004286], 
processed observation next is [1.0, 0.9565217391304348, 0.34626038781163443, 0.84, 0.0, 0.0, 0.08333333333333333, 0.5902006523278125, 0.6134561449289195, 0.0, 1.0, 0.2, 0.3896872280004286], 
reward next is 0.6103, 
noisyNet noise sample is [array([-0.652296], dtype=float32), -0.7952808]. 
=============================================
[2019-04-09 14:47:41,751] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03756949 0.10249698 0.08625164 0.07078207 0.05849041 0.03764763
 0.13977839 0.08514881 0.11073558 0.10409489 0.16700415], sum to 1.0000
[2019-04-09 14:47:41,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02457384 0.14260292 0.08625887 0.11017586 0.07656387 0.03345995
 0.08766857 0.09113373 0.14146864 0.08675453 0.11933924], sum to 1.0000
[2019-04-09 14:47:41,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7206
[2019-04-09 14:47:41,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4977
[2019-04-09 14:47:41,780] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 22.5, 26.01650336406469, 0.5156905645451918, 1.0, 1.0, 45.0, 35.50461393830832], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 22.5, 26.04014924145883, 0.5264411698297711, 1.0, 1.0, 45.0, 34.4715745472901], 
processed observation next is [1.0, 0.4782608695652174, 0.26315789473684215, 0.7316666666666667, 0.8822222222222219, 0.09650092081031308, 0.375, 0.670012436788236, 0.675480389943257, 1.0, 1.0, 0.6, 0.344715745472901], 
reward next is 0.6553, 
noisyNet noise sample is [array([-0.41341916], dtype=float32), -0.0757348]. 
=============================================
[2019-04-09 14:47:41,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 19.0, 24.78234274198074, 0.2810872674617251, 0.0, 1.0, 55.0, 44.59045251140431], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2085000.0000, 
sim time next is 2085600.0000, 
raw observation next is [-5.2, 87.66666666666667, 0.0, 0.0, 19.0, 24.8008721550611, 0.2709254631471326, 0.0, 1.0, 55.0, 46.16711012527066], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.8766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5667393462550917, 0.5903084877157109, 0.0, 1.0, 0.8, 0.4616711012527066], 
reward next is 0.5383, 
noisyNet noise sample is [array([0.39058158], dtype=float32), -0.35282764]. 
=============================================
[2019-04-09 14:47:41,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02092942 0.1328307  0.08694386 0.09571873 0.06925907 0.03381938
 0.09672533 0.09875508 0.13162497 0.0940644  0.13932899], sum to 1.0000
[2019-04-09 14:47:41,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1193
[2019-04-09 14:47:41,817] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 22.5, 26.04014924145883, 0.5264411698297711, 1.0, 1.0, 45.0, 34.4715745472901], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2113800.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 22.5, 26.07104148961114, 0.5358313128901006, 1.0, 1.0, 20.0, 31.93870606433601], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 0.375, 0.6725867908009283, 0.6786104376300335, 1.0, 1.0, 0.1, 0.3193870606433601], 
reward next is 0.6806, 
noisyNet noise sample is [array([-0.41341916], dtype=float32), -0.0757348]. 
=============================================
[2019-04-09 14:47:41,944] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02040484 0.12543698 0.08860471 0.10848295 0.06109312 0.03198556
 0.10847542 0.07768776 0.14136042 0.08094963 0.15551865], sum to 1.0000
[2019-04-09 14:47:41,946] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4793
[2019-04-09 14:47:41,974] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 22.5, 26.12728770647086, 0.5233256437562278, 1.0, 1.0, 60.0, 44.40445166442886], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2113800.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 22.5, 26.16040802862239, 0.5386177067323017, 1.0, 1.0, 45.0, 38.76599677507989], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 0.375, 0.6800340023851991, 0.6795392355774338, 1.0, 1.0, 0.6, 0.3876599677507989], 
reward next is 0.6123, 
noisyNet noise sample is [array([0.06908587], dtype=float32), 2.8305743]. 
=============================================
[2019-04-09 14:47:42,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02315167 0.11117677 0.08555617 0.0813847  0.05735739 0.03550622
 0.12587899 0.0913011  0.12987176 0.09892061 0.15989459], sum to 1.0000
[2019-04-09 14:47:42,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9586
[2019-04-09 14:47:42,107] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55900: loss 24.7476
[2019-04-09 14:47:42,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55901: learning rate 0.0000
[2019-04-09 14:47:42,115] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 22.5, 25.926529751103, 0.5090058530776972, 1.0, 1.0, 45.0, 38.9422242825897], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2053200.0000, 
sim time next is 2053800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 22.5, 25.81082249633825, 0.515210644733732, 1.0, 1.0, 65.0, 64.18684389147477], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.375, 0.6509018746948542, 0.6717368815779107, 1.0, 1.0, 1.0, 0.6418684389147477], 
reward next is 0.3581, 
noisyNet noise sample is [array([0.2334493], dtype=float32), 1.0625881]. 
=============================================
[2019-04-09 14:47:42,223] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55968: loss 17.5217
[2019-04-09 14:47:42,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55968: learning rate 0.0000
[2019-04-09 14:47:42,257] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55985: loss 22.6118
[2019-04-09 14:47:42,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55986: learning rate 0.0000
[2019-04-09 14:47:42,313] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04099651 0.09093794 0.08549204 0.0660172  0.05122732 0.04484711
 0.13282861 0.09596188 0.10401601 0.11191952 0.17575583], sum to 1.0000
[2019-04-09 14:47:42,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0164
[2019-04-09 14:47:42,335] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 19.0, 24.65363257234241, 0.2142277756948329, 0.0, 1.0, 20.0, 38.82930036496656], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2095200.0000, 
sim time next is 2095800.0000, 
raw observation next is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 19.0, 24.60303565708636, 0.1057822321304948, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8216666666666668, 0.0, 0.0, 0.08333333333333333, 0.5502529714238632, 0.5352607440434983, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2237031], dtype=float32), -0.48369473]. 
=============================================
[2019-04-09 14:47:42,338] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04226817 0.08386131 0.09142195 0.07409654 0.05986457 0.04575637
 0.12910512 0.09168848 0.1052001  0.10788818 0.16884926], sum to 1.0000
[2019-04-09 14:47:42,340] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4300
[2019-04-09 14:47:42,362] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 19.0, 24.21897659767226, 0.2145476411469573, 0.0, 1.0, 65.0, 67.57431287630075], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2080800.0000, 
sim time next is 2081400.0000, 
raw observation next is [-4.583333333333333, 86.0, 0.0, 0.0, 19.0, 24.37795232428434, 0.2405587413482466, 0.0, 1.0, 45.0, 50.66658580115389], 
processed observation next is [1.0, 0.08695652173913043, 0.3356417359187443, 0.86, 0.0, 0.0, 0.08333333333333333, 0.531496027023695, 0.5801862471160822, 0.0, 1.0, 0.6, 0.506665858011539], 
reward next is 0.4933, 
noisyNet noise sample is [array([-2.096009], dtype=float32), 2.2943616]. 
=============================================
[2019-04-09 14:47:42,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02584631 0.1161492  0.09540282 0.10780655 0.0670049  0.03913439
 0.10042955 0.07782856 0.14598863 0.09175867 0.13265042], sum to 1.0000
[2019-04-09 14:47:42,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-09 14:47:42,455] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 82.0, 157.0, 104.5, 22.5, 25.21581225644461, 0.3307746913315623, 1.0, 1.0, 50.0, 32.63438596493523], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2107200.0000, 
sim time next is 2107800.0000, 
raw observation next is [-7.8, 82.0, 174.0, 118.0, 22.5, 25.29644620837141, 0.3448362605083613, 1.0, 1.0, 45.0, 30.99865555702475], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.58, 0.13038674033149172, 0.375, 0.6080371840309509, 0.6149454201694537, 1.0, 1.0, 0.6, 0.3099865555702475], 
reward next is 0.6900, 
noisyNet noise sample is [array([0.3817392], dtype=float32), -0.7664392]. 
=============================================
[2019-04-09 14:47:42,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56107: loss 23.5514
[2019-04-09 14:47:42,488] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56107: learning rate 0.0000
[2019-04-09 14:47:42,565] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56150: loss 22.1668
[2019-04-09 14:47:42,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56151: learning rate 0.0000
[2019-04-09 14:47:42,814] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03203482 0.11341479 0.08531793 0.08700276 0.0626063  0.04615126
 0.11041827 0.09147661 0.13199681 0.08628568 0.15329479], sum to 1.0000
[2019-04-09 14:47:42,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9674
[2019-04-09 14:47:42,836] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 22.5, 25.02219578940699, 0.2685161612682211, 1.0, 1.0, 20.0, 38.42248527168211], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 22.5, 24.94963345523903, 0.3121058965716059, 1.0, 1.0, 65.0, 62.09969900128785], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.375, 0.5791361212699192, 0.604035298857202, 1.0, 1.0, 1.0, 0.6209969900128786], 
reward next is 0.3790, 
noisyNet noise sample is [array([-0.35857135], dtype=float32), 1.4218601]. 
=============================================
[2019-04-09 14:47:42,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01752043 0.13723311 0.08329121 0.10162657 0.05983596 0.03129175
 0.09726951 0.09013289 0.14634912 0.07969972 0.15574972], sum to 1.0000
[2019-04-09 14:47:42,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1043
[2019-04-09 14:47:42,977] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02728222 0.13219732 0.07635688 0.10126539 0.06319146 0.04619417
 0.10329179 0.0777114  0.14409727 0.08001966 0.1483925 ], sum to 1.0000
[2019-04-09 14:47:42,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8521
[2019-04-09 14:47:42,992] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.366666666666667, 64.0, 150.6666666666667, 111.6666666666667, 22.5, 25.16042602497053, 0.45788844587691, 1.0, 1.0, 55.0, 45.9231819153351], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2119200.0000, 
sim time next is 2119800.0000, 
raw observation next is [-6.283333333333333, 64.0, 150.3333333333333, 89.33333333333334, 22.5, 24.789415591678, 0.4120850190054521, 1.0, 1.0, 20.0, 37.44691549561946], 
processed observation next is [1.0, 0.5217391304347826, 0.288550323176362, 0.64, 0.501111111111111, 0.0987108655616943, 0.375, 0.5657846326398334, 0.6373616730018173, 1.0, 1.0, 0.1, 0.3744691549561946], 
reward next is 0.6255, 
noisyNet noise sample is [array([1.6045581], dtype=float32), 1.415385]. 
=============================================
[2019-04-09 14:47:43,003] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 82.00000000000001, 140.0, 91.0, 22.5, 25.37102171320827, 0.350070107237768, 1.0, 1.0, 50.0, 58.45038284640762], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2106600.0000, 
sim time next is 2107200.0000, 
raw observation next is [-7.8, 82.0, 157.0, 104.5, 22.5, 25.36214685809572, 0.4050577830364874, 1.0, 1.0, 65.0, 56.91914502744071], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.5233333333333333, 0.11546961325966851, 0.375, 0.6135122381746433, 0.6350192610121624, 1.0, 1.0, 1.0, 0.5691914502744071], 
reward next is 0.4308, 
noisyNet noise sample is [array([0.41966218], dtype=float32), -0.56602013]. 
=============================================
[2019-04-09 14:47:43,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01942004 0.12470593 0.10056873 0.07962545 0.05580316 0.03605698
 0.11256801 0.08352811 0.12886186 0.09012602 0.16873573], sum to 1.0000
[2019-04-09 14:47:43,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6863
[2019-04-09 14:47:43,160] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56444: loss 20.4797
[2019-04-09 14:47:43,164] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.0, 71.5, 0.0, 0.0, 22.5, 25.24481453030443, 0.449976994140336, 1.0, 1.0, 50.0, 38.71318196113228], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2139000.0000, 
sim time next is 2139600.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 22.5, 25.37912019525763, 0.4544974676010137, 1.0, 1.0, 55.0, 42.47221357736353], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.72, 0.0, 0.0, 0.375, 0.6149266829381359, 0.6514991558670046, 1.0, 1.0, 0.8, 0.4247221357736353], 
reward next is 0.5753, 
noisyNet noise sample is [array([-0.9780424], dtype=float32), -1.4600389]. 
=============================================
[2019-04-09 14:47:43,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56444: learning rate 0.0000
[2019-04-09 14:47:43,299] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56522: loss 21.7191
[2019-04-09 14:47:43,300] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56523: learning rate 0.0000
[2019-04-09 14:47:43,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02697027 0.13255283 0.08692297 0.1055261  0.06801155 0.03708256
 0.10558993 0.07894633 0.13848348 0.08484326 0.13507064], sum to 1.0000
[2019-04-09 14:47:43,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0726
[2019-04-09 14:47:43,611] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 82.0, 157.0, 104.5, 22.5, 25.31019996669705, 0.3893604388330625, 1.0, 1.0, 65.0, 53.84582379923627], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2107200.0000, 
sim time next is 2107800.0000, 
raw observation next is [-7.8, 82.0, 174.0, 118.0, 22.5, 25.5800187329601, 0.4273176176086014, 1.0, 1.0, 20.0, 41.18274135049367], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.58, 0.13038674033149172, 0.375, 0.6316682277466749, 0.6424392058695337, 1.0, 1.0, 0.1, 0.41182741350493673], 
reward next is 0.5882, 
noisyNet noise sample is [array([0.7769341], dtype=float32), -0.76779705]. 
=============================================
[2019-04-09 14:47:43,864] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02022627 0.14550385 0.08903971 0.09545051 0.07313961 0.03124385
 0.10340181 0.07829721 0.13547221 0.08232271 0.14590223], sum to 1.0000
[2019-04-09 14:47:43,867] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2211
[2019-04-09 14:47:43,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02175278 0.1281194  0.10008386 0.07795846 0.05198549 0.03914892
 0.10835719 0.09728414 0.11078601 0.09096731 0.17355642], sum to 1.0000
[2019-04-09 14:47:43,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-09 14:47:43,887] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 22.5, 25.95439681456863, 0.4811202091001252, 1.0, 1.0, 65.0, 48.15008258454557], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 22.5, 25.9850907361724, 0.498860048997515, 1.0, 1.0, 25.0, 46.04686745705955], 
processed observation next is [1.0, 0.4782608695652174, 0.26315789473684215, 0.7316666666666667, 0.8822222222222219, 0.09650092081031308, 0.375, 0.6654242280143666, 0.6662866829991717, 1.0, 1.0, 0.2, 0.4604686745705955], 
reward next is 0.5395, 
noisyNet noise sample is [array([0.11319496], dtype=float32), 1.5393531]. 
=============================================
[2019-04-09 14:47:43,907] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.75, 69.5, 0.0, 0.0, 22.5, 26.28573119053179, 0.4229069459559924, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2136600.0000, 
sim time next is 2137200.0000, 
raw observation next is [-4.833333333333333, 70.0, 0.0, 0.0, 22.5, 25.68100350766912, 0.5113107140051057, 1.0, 1.0, 65.0, 83.50103172837983], 
processed observation next is [1.0, 0.7391304347826086, 0.3287165281625116, 0.7, 0.0, 0.0, 0.375, 0.6400836256390935, 0.6704369046683686, 1.0, 1.0, 1.0, 0.8350103172837983], 
reward next is 0.1650, 
noisyNet noise sample is [array([-1.0448589], dtype=float32), -0.60552275]. 
=============================================
[2019-04-09 14:47:44,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0410492  0.09286507 0.08559209 0.06262922 0.05439775 0.04522669
 0.13549565 0.0959443  0.10752895 0.10003363 0.17923743], sum to 1.0000
[2019-04-09 14:47:44,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3641
[2019-04-09 14:47:44,119] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 19.0, 24.75991377583592, 0.2139894772981984, 0.0, 1.0, 25.0, 39.19040090766556], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 19.0, 24.64871163799753, 0.1975817403099788, 0.0, 1.0, 40.0, 37.22676162791647], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5540593031664608, 0.5658605801033263, 0.0, 1.0, 0.5, 0.37226761627916466], 
reward next is 0.6277, 
noisyNet noise sample is [array([-0.24003455], dtype=float32), -0.007263372]. 
=============================================
[2019-04-09 14:47:44,220] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02959264 0.07964765 0.09579733 0.04975338 0.05286131 0.03963121
 0.13987747 0.10387728 0.08012334 0.12646472 0.20237371], sum to 1.0000
[2019-04-09 14:47:44,223] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8526
[2019-04-09 14:47:44,251] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 19.0, 23.60703721922398, 0.08645492873224501, 0.0, 1.0, 20.0, 54.05533596920716], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2161800.0000, 
sim time next is 2162400.0000, 
raw observation next is [-7.3, 80.0, 0.0, 0.0, 19.0, 23.73389405949878, 0.1028897604902593, 0.0, 1.0, 65.0, 58.59558779351956], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8, 0.0, 0.0, 0.08333333333333333, 0.4778245049582317, 0.5342965868300864, 0.0, 1.0, 1.0, 0.5859558779351957], 
reward next is 0.4140, 
noisyNet noise sample is [array([0.77923083], dtype=float32), -0.6699554]. 
=============================================
[2019-04-09 14:47:44,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0197367  0.12856938 0.09416471 0.0772557  0.06011463 0.03171228
 0.09136571 0.08862116 0.1476261  0.08698539 0.17384829], sum to 1.0000
[2019-04-09 14:47:44,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5576
[2019-04-09 14:47:44,314] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 57083: loss 26.2733
[2019-04-09 14:47:44,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 57083: learning rate 0.0000
[2019-04-09 14:47:44,320] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.7, 67.33333333333334, 141.0, 0.0, 22.5, 25.96607130842782, 0.5429639849326046, 1.0, 1.0, 20.0, 34.00520664585788], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [-5.6, 68.0, 137.0, 0.0, 22.5, 26.15129341928963, 0.5590735535678633, 1.0, 1.0, 50.0, 37.44667288432012], 
processed observation next is [1.0, 0.6086956521739131, 0.30747922437673136, 0.68, 0.45666666666666667, 0.0, 0.375, 0.6792744516074691, 0.6863578511892877, 1.0, 1.0, 0.7, 0.3744667288432012], 
reward next is 0.6255, 
noisyNet noise sample is [array([-1.86643], dtype=float32), 0.21457423]. 
=============================================
[2019-04-09 14:47:44,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.177484 ]
 [1.254048 ]
 [1.0527906]
 [1.1359357]
 [1.1191266]], R is [[1.80445433]
 [2.44635773]
 [2.9909029 ]
 [3.59924245]
 [4.10808372]].
[2019-04-09 14:47:44,404] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 57139: loss 18.2512
[2019-04-09 14:47:44,406] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 57140: learning rate 0.0000
[2019-04-09 14:47:44,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0221523  0.10854818 0.09461226 0.08319668 0.05479831 0.03867507
 0.12539604 0.09272581 0.12787636 0.08530641 0.16671257], sum to 1.0000
[2019-04-09 14:47:44,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2179
[2019-04-09 14:47:44,601] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.5, 81.5, 0.0, 0.0, 22.5, 24.99147375609443, 0.3781443494101839, 1.0, 1.0, 65.0, 54.50221257884565], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2145000.0000, 
sim time next is 2145600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 22.5, 24.88319326711087, 0.3740678911093058, 0.0, 1.0, 25.0, 40.62487719617393], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.375, 0.5735994389259057, 0.6246892970364353, 0.0, 1.0, 0.2, 0.40624877196173925], 
reward next is 0.5938, 
noisyNet noise sample is [array([1.1132867], dtype=float32), 0.06578278]. 
=============================================
[2019-04-09 14:47:44,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02360557 0.11438859 0.09303276 0.07636945 0.05473172 0.03490873
 0.12958214 0.10268921 0.12076138 0.08044425 0.16948627], sum to 1.0000
[2019-04-09 14:47:44,617] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0602
[2019-04-09 14:47:44,630] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 24.88319326711087, 0.3740678911093058, 0.0, 1.0, 25.0, 40.62487719617393], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2145600.0000, 
sim time next is 2146200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 24.84836922948867, 0.3641268978339942, 0.0, 1.0, 20.0, 26.18198489870494], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5706974357907226, 0.6213756326113314, 0.0, 1.0, 0.1, 0.2618198489870494], 
reward next is 0.7382, 
noisyNet noise sample is [array([1.1132867], dtype=float32), 0.06578278]. 
=============================================
[2019-04-09 14:47:44,719] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03721924 0.08982626 0.08830404 0.06837485 0.05260267 0.04232884
 0.12864216 0.09247036 0.10837571 0.10701556 0.18484034], sum to 1.0000
[2019-04-09 14:47:44,723] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4515
[2019-04-09 14:47:44,746] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 24.6032883463269, 0.2021295415099525, 0.0, 1.0, 45.0, 44.29712525331131], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2172600.0000, 
sim time next is 2173200.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 24.54651502926992, 0.1918861037676211, 0.0, 1.0, 45.0, 35.91827121351185], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5455429191058266, 0.563962034589207, 0.0, 1.0, 0.6, 0.3591827121351185], 
reward next is 0.6408, 
noisyNet noise sample is [array([0.20010334], dtype=float32), 1.3556873]. 
=============================================
[2019-04-09 14:47:44,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01912275 0.13575056 0.0925046  0.09299871 0.06132674 0.03202577
 0.11067867 0.08921403 0.12318194 0.0733908  0.16980542], sum to 1.0000
[2019-04-09 14:47:44,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0807
[2019-04-09 14:47:44,864] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 67.0, 22.0, 0.0, 22.5, 26.40069724520275, 0.4733547124789496, 1.0, 1.0, 45.0, 38.20156285729946], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2133600.0000, 
sim time next is 2134200.0000, 
raw observation next is [-4.5, 67.5, 18.0, 0.0, 22.5, 25.7737250787278, 0.4968416678584506, 1.0, 1.0, 20.0, 33.20513321394722], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.675, 0.06, 0.0, 0.375, 0.6478104232273166, 0.6656138892861502, 1.0, 1.0, 0.1, 0.3320513321394722], 
reward next is 0.6679, 
noisyNet noise sample is [array([0.2743448], dtype=float32), 0.9754178]. 
=============================================
[2019-04-09 14:47:44,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01725559 0.11672289 0.10171076 0.07132114 0.04606226 0.03444643
 0.12323744 0.08400394 0.11108224 0.08288645 0.2112709 ], sum to 1.0000
[2019-04-09 14:47:44,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3994
[2019-04-09 14:47:44,912] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 25.40893006073716, 0.4849480336044738, 0.0, 1.0, 20.0, 37.35048088067635], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2145600.0000, 
sim time next is 2146200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 25.42957268764952, 0.4785786730278876, 0.0, 1.0, 45.0, 36.62805681499003], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6191310573041268, 0.6595262243426292, 0.0, 1.0, 0.6, 0.36628056814990034], 
reward next is 0.6337, 
noisyNet noise sample is [array([-0.34011835], dtype=float32), 0.9294775]. 
=============================================
[2019-04-09 14:47:45,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03488426 0.107416   0.08310204 0.07914528 0.05771637 0.03962494
 0.13144763 0.09491303 0.10652492 0.10079277 0.16443282], sum to 1.0000
[2019-04-09 14:47:45,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6758
[2019-04-09 14:47:45,228] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 24.19140694421024, 0.1596238961661134, 0.0, 1.0, 60.0, 50.3994376703538], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2174400.0000, 
sim time next is 2175000.0000, 
raw observation next is [-6.616666666666667, 77.5, 0.0, 0.0, 19.0, 24.27581742122866, 0.1632964850015887, 0.0, 1.0, 55.0, 52.70319580186896], 
processed observation next is [1.0, 0.17391304347826086, 0.2793167128347184, 0.775, 0.0, 0.0, 0.08333333333333333, 0.5229847851023882, 0.5544321616671962, 0.0, 1.0, 0.8, 0.5270319580186896], 
reward next is 0.4730, 
noisyNet noise sample is [array([-0.22710279], dtype=float32), 1.0865836]. 
=============================================
[2019-04-09 14:47:45,236] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[0.6046581 ]
 [0.5984456 ]
 [0.62314683]
 [0.55202556]
 [0.6953914 ]], R is [[1.15695739]
 [1.64139342]
 [2.11972809]
 [2.46505451]
 [3.10306048]].
[2019-04-09 14:47:45,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03491064 0.08430341 0.09859623 0.07380121 0.05643212 0.0420387
 0.14587548 0.08253633 0.09160307 0.09819271 0.1917101 ], sum to 1.0000
[2019-04-09 14:47:45,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9882
[2019-04-09 14:47:45,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 19.0, 24.76219393130328, 0.2890545878507685, 0.0, 1.0, 65.0, 57.89617140186886], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 24.77401077546002, 0.3065320308866883, 0.0, 1.0, 65.0, 63.08041434459307], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.5645008979550017, 0.6021773436288961, 0.0, 1.0, 1.0, 0.6308041434459307], 
reward next is 0.3692, 
noisyNet noise sample is [array([-1.5635486], dtype=float32), -0.87490594]. 
=============================================
[2019-04-09 14:47:45,523] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.84455216]
 [0.8227476 ]
 [0.8463124 ]
 [0.747898  ]
 [0.8257234 ]], R is [[1.05373299]
 [1.46423399]
 [2.06438208]
 [2.61524391]
 [3.07488775]].
[2019-04-09 14:47:45,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03338057 0.08016403 0.0925302  0.05781138 0.05540559 0.04149719
 0.14175911 0.08779197 0.08995739 0.10917088 0.21053171], sum to 1.0000
[2019-04-09 14:47:45,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-09 14:47:45,835] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 19.0, 25.01647798584012, 0.3118805113934384, 0.0, 1.0, 55.0, 39.64236215667414], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2164800.0000, 
sim time next is 2165400.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 19.0, 24.96887463739211, 0.2959922369628558, 0.0, 1.0, 50.0, 37.72943067063199], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.08333333333333333, 0.580739553116009, 0.5986640789876186, 0.0, 1.0, 0.7, 0.3772943067063199], 
reward next is 0.6227, 
noisyNet noise sample is [array([0.6338873], dtype=float32), -1.4295459]. 
=============================================
[2019-04-09 14:47:46,007] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03505101 0.09270088 0.09842882 0.08317519 0.06543885 0.04368344
 0.12445884 0.09146658 0.11204477 0.09793136 0.15562017], sum to 1.0000
[2019-04-09 14:47:46,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7410
[2019-04-09 14:47:46,026] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.6, 75.0, 34.50000000000001, 218.3333333333333, 22.5, 24.2286096111552, 0.1939802069802134, 1.0, 1.0, 65.0, 64.86963561292472], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2190000.0000, 
sim time next is 2190600.0000, 
raw observation next is [-5.6, 75.0, 41.0, 262.0, 22.5, 24.54862016935727, 0.2493549170788519, 1.0, 1.0, 55.0, 48.16199515265026], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.13666666666666666, 0.28950276243093925, 0.375, 0.5457183474464392, 0.5831183056929506, 1.0, 1.0, 0.8, 0.4816199515265026], 
reward next is 0.5184, 
noisyNet noise sample is [array([-0.1446645], dtype=float32), -0.634039]. 
=============================================
[2019-04-09 14:47:46,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02064869 0.11700209 0.10559114 0.08450919 0.05680233 0.03971941
 0.11429113 0.08812027 0.13419688 0.08899324 0.15012558], sum to 1.0000
[2019-04-09 14:47:46,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4766
[2019-04-09 14:47:46,329] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 22.5, 25.81773560784985, 0.5458892720279128, 1.0, 1.0, 25.0, 34.96300491181845], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2138400.0000, 
sim time next is 2139000.0000, 
raw observation next is [-5.0, 71.5, 0.0, 0.0, 22.5, 25.90745749990517, 0.5382956225206178, 1.0, 1.0, 45.0, 29.59488999830233], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.715, 0.0, 0.0, 0.375, 0.6589547916587642, 0.6794318741735393, 1.0, 1.0, 0.6, 0.2959488999830233], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.23220766], dtype=float32), 0.28275672]. 
=============================================
[2019-04-09 14:47:46,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[1.2070656]
 [1.360826 ]
 [1.3005958]
 [1.2620683]
 [1.3363236]], R is [[2.03630471]
 [2.6663115 ]
 [3.20251107]
 [3.82573771]
 [4.40323925]].
[2019-04-09 14:47:46,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01734392 0.12415983 0.09933185 0.08819146 0.04839853 0.02625289
 0.10123234 0.09642147 0.1428007  0.07945975 0.17640722], sum to 1.0000
[2019-04-09 14:47:46,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9619
[2019-04-09 14:47:46,430] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 22.5, 25.55900999685718, 0.4048099183202176, 1.0, 1.0, 30.0, 34.83894778765996], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2208000.0000, 
sim time next is 2208600.0000, 
raw observation next is [-3.65, 68.0, 144.0, 0.0, 22.5, 25.5795561137268, 0.4508057686133502, 1.0, 1.0, 65.0, 60.11893036249644], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.68, 0.48, 0.0, 0.375, 0.6316296761438999, 0.6502685895377834, 1.0, 1.0, 1.0, 0.6011893036249645], 
reward next is 0.3988, 
noisyNet noise sample is [array([-0.4055481], dtype=float32), 0.9550499]. 
=============================================
[2019-04-09 14:47:46,450] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03356254 0.09379272 0.09210668 0.06583311 0.0626983  0.0415289
 0.12780157 0.08867653 0.0936697  0.103244   0.19708598], sum to 1.0000
[2019-04-09 14:47:46,452] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7483
[2019-04-09 14:47:46,469] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 19.0, 24.78158783554475, 0.2938137207196441, 0.0, 1.0, 65.0, 64.27866730196597], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 19.0, 24.77952827223645, 0.3017327030223653, 0.0, 1.0, 60.0, 57.30853249000916], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.5649606893530376, 0.6005775676741217, 0.0, 1.0, 0.9, 0.5730853249000917], 
reward next is 0.4269, 
noisyNet noise sample is [array([-0.15851818], dtype=float32), -0.8142804]. 
=============================================
[2019-04-09 14:47:46,489] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0168285  0.10366144 0.10673758 0.07937816 0.05094756 0.03257804
 0.12907833 0.08613329 0.13558799 0.07082829 0.18824084], sum to 1.0000
[2019-04-09 14:47:46,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2004
[2019-04-09 14:47:46,514] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 22.5, 25.73972511410968, 0.5433848291247586, 1.0, 1.0, 55.0, 36.41959818593425], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2142000.0000, 
sim time next is 2142600.0000, 
raw observation next is [-5.100000000000001, 75.5, 0.0, 0.0, 22.5, 25.83967509713385, 0.541763659804186, 1.0, 1.0, 25.0, 33.40831321791183], 
processed observation next is [1.0, 0.8260869565217391, 0.32132963988919666, 0.755, 0.0, 0.0, 0.375, 0.6533062580944874, 0.6805878866013954, 1.0, 1.0, 0.2, 0.33408313217911834], 
reward next is 0.6659, 
noisyNet noise sample is [array([-0.37095433], dtype=float32), -1.2428273]. 
=============================================
[2019-04-09 14:47:46,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02324457 0.11979749 0.10176493 0.09224315 0.06021456 0.03441684
 0.09798049 0.09860762 0.13098775 0.08916599 0.15157664], sum to 1.0000
[2019-04-09 14:47:46,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7720
[2019-04-09 14:47:46,583] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.1, 69.0, 140.5, 0.0, 22.5, 25.71689184173363, 0.4407835180782713, 1.0, 1.0, 60.0, 51.25677355389561], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2202000.0000, 
sim time next is 2202600.0000, 
raw observation next is [-4.0, 68.5, 138.0, 0.0, 22.5, 25.82226518578641, 0.4519599362326348, 1.0, 1.0, 60.0, 51.06785399262315], 
processed observation next is [1.0, 0.4782608695652174, 0.3518005540166205, 0.685, 0.46, 0.0, 0.375, 0.6518554321488675, 0.650653312077545, 1.0, 1.0, 0.9, 0.5106785399262315], 
reward next is 0.4893, 
noisyNet noise sample is [array([1.5151956], dtype=float32), 1.0953591]. 
=============================================
[2019-04-09 14:47:46,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02157016 0.11381248 0.10508651 0.08826015 0.05968032 0.03305515
 0.10571952 0.09654631 0.13038908 0.08254079 0.16333957], sum to 1.0000
[2019-04-09 14:47:46,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5724
[2019-04-09 14:47:46,606] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 68.5, 138.0, 0.0, 22.5, 25.82226518578641, 0.4519599362326348, 1.0, 1.0, 60.0, 51.06785399262315], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2202600.0000, 
sim time next is 2203200.0000, 
raw observation next is [-3.9, 68.0, 135.5, 0.0, 22.5, 25.8576981706885, 0.4634758260570111, 1.0, 1.0, 50.0, 43.34991002448748], 
processed observation next is [1.0, 0.5217391304347826, 0.3545706371191136, 0.68, 0.45166666666666666, 0.0, 0.375, 0.6548081808907084, 0.6544919420190037, 1.0, 1.0, 0.7, 0.43349910024487476], 
reward next is 0.5665, 
noisyNet noise sample is [array([1.5151956], dtype=float32), 1.0953591]. 
=============================================
[2019-04-09 14:47:46,720] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03425828 0.10381403 0.09471761 0.07570946 0.06018941 0.03910113
 0.11567381 0.07983521 0.12454224 0.09395526 0.17820354], sum to 1.0000
[2019-04-09 14:47:46,722] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7246
[2019-04-09 14:47:46,747] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.6, 75.0, 41.0, 262.0, 22.5, 24.53464322104737, 0.2133652268274558, 1.0, 1.0, 35.0, 36.26597494174295], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2190600.0000, 
sim time next is 2191200.0000, 
raw observation next is [-5.6, 75.0, 51.16666666666667, 293.5, 22.5, 24.79227863595004, 0.2399318414425111, 1.0, 1.0, 55.0, 41.32873423073941], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.17055555555555557, 0.3243093922651934, 0.375, 0.5660232196625034, 0.5799772804808371, 1.0, 1.0, 0.8, 0.41328734230739406], 
reward next is 0.5867, 
noisyNet noise sample is [array([-1.8737568], dtype=float32), 0.6683759]. 
=============================================
[2019-04-09 14:47:46,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03392357 0.10343227 0.09229858 0.06844802 0.05249518 0.03978585
 0.13415062 0.09105217 0.11004753 0.10044012 0.17392612], sum to 1.0000
[2019-04-09 14:47:46,764] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4591
[2019-04-09 14:47:46,780] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 22.5, 23.45711867813674, 0.005235303347308551, 1.0, 1.0, 60.0, 56.85676020277913], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 22.5, 23.51333639816366, 0.02095502091835518, 1.0, 1.0, 45.0, 45.40807705598515], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.375, 0.4594446998469716, 0.506985006972785, 1.0, 1.0, 0.6, 0.4540807705598515], 
reward next is 0.5459, 
noisyNet noise sample is [array([0.06873957], dtype=float32), 0.80149233]. 
=============================================
[2019-04-09 14:47:46,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.7056943 ]
 [0.68216914]
 [0.701319  ]
 [0.62401164]
 [0.6853732 ]], R is [[1.22841978]
 [1.64756799]
 [2.17287278]
 [2.78202677]
 [3.35287213]].
[2019-04-09 14:47:47,009] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03306283 0.11907285 0.09562786 0.0876936  0.06224997 0.03968878
 0.12580036 0.08094341 0.11195901 0.09264243 0.15125893], sum to 1.0000
[2019-04-09 14:47:47,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4036
[2019-04-09 14:47:47,039] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 51.16666666666667, 293.5, 22.5, 24.77798063801801, 0.256070987669042, 1.0, 1.0, 65.0, 63.63708755980227], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2191200.0000, 
sim time next is 2191800.0000, 
raw observation next is [-5.6, 75.0, 61.33333333333333, 325.0, 22.5, 24.96734785299348, 0.2947406176426967, 1.0, 1.0, 40.0, 50.19406828728218], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.20444444444444443, 0.35911602209944754, 0.375, 0.58061232108279, 0.5982468725475656, 1.0, 1.0, 0.5, 0.5019406828728218], 
reward next is 0.4981, 
noisyNet noise sample is [array([-0.18828355], dtype=float32), 0.9562338]. 
=============================================
[2019-04-09 14:47:47,290] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02137021 0.12865524 0.10326315 0.08580016 0.06230948 0.03263238
 0.10203029 0.08947422 0.12776078 0.08480655 0.16189754], sum to 1.0000
[2019-04-09 14:47:47,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6573
[2019-04-09 14:47:47,317] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.816666666666666, 67.5, 133.0, 0.0, 22.5, 25.92569590277585, 0.481509674495818, 1.0, 1.0, 45.0, 44.43807000544398], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2203800.0000, 
sim time next is 2204400.0000, 
raw observation next is [-3.733333333333333, 67.0, 130.5, 0.0, 22.5, 25.99292008293923, 0.4691991585507977, 1.0, 1.0, 55.0, 44.34484907950486], 
processed observation next is [1.0, 0.5217391304347826, 0.35918744228993543, 0.67, 0.435, 0.0, 0.375, 0.666076673578269, 0.6563997195169325, 1.0, 1.0, 0.8, 0.4434484907950486], 
reward next is 0.5566, 
noisyNet noise sample is [array([1.8117528], dtype=float32), 0.19940239]. 
=============================================
[2019-04-09 14:47:47,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02959379 0.09248149 0.09977522 0.0721362  0.05578498 0.03777625
 0.12857954 0.08774249 0.11892681 0.10847992 0.16872334], sum to 1.0000
[2019-04-09 14:47:47,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1451
[2019-04-09 14:47:47,613] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 75.0, 41.0, 262.0, 22.5, 24.61596103904222, 0.238543914955504, 1.0, 1.0, 20.0, 38.91842296050584], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2190600.0000, 
sim time next is 2191200.0000, 
raw observation next is [-5.6, 75.0, 51.16666666666667, 293.5, 22.5, 24.91835289423314, 0.2611953610121171, 1.0, 1.0, 25.0, 34.15043929814828], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.17055555555555557, 0.3243093922651934, 0.375, 0.5765294078527617, 0.5870651203373723, 1.0, 1.0, 0.2, 0.3415043929814828], 
reward next is 0.6585, 
noisyNet noise sample is [array([-0.6459721], dtype=float32), 1.3910285]. 
=============================================
[2019-04-09 14:47:48,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0257403  0.09766486 0.09205928 0.06873069 0.05172282 0.04021205
 0.14668918 0.0932534  0.08786417 0.09509502 0.20096825], sum to 1.0000
[2019-04-09 14:47:48,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-09 14:47:48,396] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.45, 76.5, 0.0, 0.0, 19.0, 24.73610287418973, 0.2971494600961608, 0.0, 1.0, 55.0, 40.02051701701817], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [-6.533333333333333, 77.0, 0.0, 0.0, 19.0, 24.72441577761538, 0.2985621626302674, 0.0, 1.0, 65.0, 57.43357175289781], 
processed observation next is [1.0, 1.0, 0.2816251154201293, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5603679814679484, 0.5995207208767558, 0.0, 1.0, 1.0, 0.5743357175289782], 
reward next is 0.4257, 
noisyNet noise sample is [array([-2.3836527], dtype=float32), -0.5861792]. 
=============================================
[2019-04-09 14:47:48,505] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02422691 0.09915376 0.08462711 0.06584463 0.04891814 0.03851814
 0.14530239 0.0949146  0.0965405  0.10567085 0.19628304], sum to 1.0000
[2019-04-09 14:47:48,506] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-09 14:47:48,524] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.700000000000001, 77.5, 0.0, 0.0, 19.0, 24.29400082849978, 0.1998993893472522, 0.0, 1.0, 45.0, 34.71782834101543], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2247000.0000, 
sim time next is 2247600.0000, 
raw observation next is [-6.700000000000001, 77.0, 0.0, 0.0, 19.0, 24.28565232439007, 0.1894536882001865, 0.0, 1.0, 50.0, 35.53358804019608], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5238043603658392, 0.5631512294000621, 0.0, 1.0, 0.7, 0.3553358804019608], 
reward next is 0.6447, 
noisyNet noise sample is [array([-0.09879124], dtype=float32), -0.22895403]. 
=============================================
[2019-04-09 14:47:48,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03556641 0.08703829 0.09047691 0.06374055 0.05364218 0.04148505
 0.12767045 0.07961426 0.10521759 0.11834625 0.19720207], sum to 1.0000
[2019-04-09 14:47:48,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0685
[2019-04-09 14:47:48,919] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 86.0, 0.0, 0.0, 19.0, 24.60344182675317, 0.2373802565176472, 0.0, 1.0, 30.0, 40.24479025866946], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2257200.0000, 
sim time next is 2257800.0000, 
raw observation next is [-7.9, 86.16666666666667, 0.0, 0.0, 19.0, 24.64186012712917, 0.2337856557668833, 0.0, 1.0, 20.0, 39.88413769212418], 
processed observation next is [1.0, 0.13043478260869565, 0.24376731301939059, 0.8616666666666667, 0.0, 0.0, 0.08333333333333333, 0.5534883439274308, 0.5779285519222944, 0.0, 1.0, 0.1, 0.3988413769212418], 
reward next is 0.6012, 
noisyNet noise sample is [array([-2.3542154], dtype=float32), -1.4239696]. 
=============================================
[2019-04-09 14:47:48,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03641983 0.07628505 0.09435704 0.05961533 0.05552271 0.04640394
 0.15493521 0.10668675 0.08427851 0.10339432 0.18210122], sum to 1.0000
[2019-04-09 14:47:48,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9831
[2019-04-09 14:47:48,988] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.9, 77.33333333333334, 0.0, 0.0, 19.0, 24.43832862608274, 0.2464062795908931, 0.0, 1.0, 25.0, 51.66012801651813], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2251200.0000, 
sim time next is 2251800.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 19.0, 24.51133398552512, 0.2593986832947572, 0.0, 1.0, 65.0, 57.95976513466969], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.08333333333333333, 0.5426111654604266, 0.5864662277649191, 0.0, 1.0, 1.0, 0.5795976513466968], 
reward next is 0.4204, 
noisyNet noise sample is [array([0.4311151], dtype=float32), 0.013707029]. 
=============================================
[2019-04-09 14:47:49,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03430349 0.09156803 0.08954675 0.07497264 0.05060967 0.04143972
 0.1352662  0.08231007 0.11526058 0.10924561 0.17547722], sum to 1.0000
[2019-04-09 14:47:49,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-09 14:47:49,191] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 24.54068657786629, 0.1874252890550251, 0.0, 1.0, 40.0, 42.96727367855998], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2263200.0000, 
sim time next is 2263800.0000, 
raw observation next is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 19.0, 24.51058235906142, 0.170058507803496, 0.0, 1.0, 55.0, 40.48285159077864], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.9033333333333334, 0.0, 0.0, 0.08333333333333333, 0.542548529921785, 0.556686169267832, 0.0, 1.0, 0.8, 0.40482851590778635], 
reward next is 0.5952, 
noisyNet noise sample is [array([1.5493863], dtype=float32), -0.45558333]. 
=============================================
[2019-04-09 14:47:49,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03512173 0.09385622 0.09980207 0.06136678 0.05833919 0.05041095
 0.14479308 0.09771705 0.08440325 0.09341677 0.18077292], sum to 1.0000
[2019-04-09 14:47:49,332] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9863
[2019-04-09 14:47:49,353] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.9, 77.33333333333334, 0.0, 0.0, 19.0, 25.0080001557908, 0.3382324304702403, 0.0, 1.0, 45.0, 42.95049074274335], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2251200.0000, 
sim time next is 2251800.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 19.0, 25.02277939943976, 0.3421123102037781, 0.0, 1.0, 65.0, 57.17203104649905], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.08333333333333333, 0.5852316166199799, 0.6140374367345927, 0.0, 1.0, 1.0, 0.5717203104649905], 
reward next is 0.4283, 
noisyNet noise sample is [array([-1.3287722], dtype=float32), 0.47212744]. 
=============================================
[2019-04-09 14:47:49,672] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-09 14:47:49,673] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:47:49,673] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:47:49,675] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:47:49,674] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:47:49,674] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:47:49,675] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:47:49,680] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run7
[2019-04-09 14:47:49,706] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run7
[2019-04-09 14:47:49,719] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run7
[2019-04-09 14:48:23,219] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00724494], dtype=float32), 0.011077062]
[2019-04-09 14:48:23,219] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [0.5, 92.0, 0.0, 0.0, 19.0, 27.00196745627354, 0.8896666804785854, 0.0, 1.0, 50.0, 31.89472725324604]
[2019-04-09 14:48:23,219] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:48:23,220] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.03169318 0.11737908 0.08619194 0.08323608 0.07512841 0.04413899
 0.12769835 0.08981454 0.09385858 0.09814587 0.15271495], sampled 0.16177432573882156
[2019-04-09 14:48:35,412] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00724494], dtype=float32), 0.011077062]
[2019-04-09 14:48:35,413] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-4.15, 73.5, 59.0, 107.0, 22.5, 23.90703572652586, 0.09430162305589629, 1.0, 1.0, 30.0, 30.48771104003596]
[2019-04-09 14:48:35,413] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:48:35,415] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.04138611 0.12008836 0.08336888 0.08969453 0.07940848 0.05539386
 0.10784674 0.0870544  0.09366196 0.1068316  0.13526501], sampled 0.2669850035762743
[2019-04-09 14:49:08,251] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00724494], dtype=float32), 0.011077062]
[2019-04-09 14:49:08,251] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [3.733333333333333, 37.66666666666667, 107.3333333333333, 622.3333333333334, 19.0, 27.75798708305336, 1.107399291766184, 0.0, 1.0, 25.0, 20.03287096746475]
[2019-04-09 14:49:08,251] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:49:08,252] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.0412403  0.13373482 0.08856831 0.10576376 0.08187895 0.06554135
 0.10914818 0.08827591 0.07353928 0.08978697 0.12252215], sampled 0.4782537522803003
[2019-04-09 14:49:11,026] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5713.0615 278152.2903 2700.2338
[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,047] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:11,163] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,056] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5383.2046 311323.5889 1806.9754
[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:16,183] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,427] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5474.6566 302186.0059 2249.5605
[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:18,561] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:49:19,450] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 60000, evaluation results [60000.0, 5474.656601879317, 302186.0059036709, 2249.56053177152, 5713.061457002711, 278152.29027874937, 2700.2338457474957, 5383.204626133558, 311323.5888821283, 1806.975430436117]
[2019-04-09 14:49:20,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02516617 0.12963036 0.08733799 0.09765813 0.06952322 0.03492383
 0.10748471 0.08985615 0.13562615 0.08169286 0.14110045], sum to 1.0000
[2019-04-09 14:49:20,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-09 14:49:20,046] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.0, 68.0, 169.5, 80.0, 22.5, 25.56975351791828, 0.378845537381843, 1.0, 1.0, 55.0, 46.15035017739262], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2286000.0000, 
sim time next is 2286600.0000, 
raw observation next is [-4.7, 66.33333333333334, 166.6666666666667, 90.0, 22.5, 25.69302790415662, 0.399027096318362, 1.0, 1.0, 60.0, 46.59375575911555], 
processed observation next is [1.0, 0.4782608695652174, 0.332409972299169, 0.6633333333333334, 0.5555555555555557, 0.09944751381215469, 0.375, 0.6410856586797182, 0.6330090321061207, 1.0, 1.0, 0.9, 0.4659375575911555], 
reward next is 0.5341, 
noisyNet noise sample is [array([-0.66002786], dtype=float32), -1.1409082]. 
=============================================
[2019-04-09 14:49:20,086] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04475544 0.0889844  0.09535616 0.07229614 0.05527684 0.05311956
 0.13636327 0.08881342 0.08798153 0.10847095 0.16858222], sum to 1.0000
[2019-04-09 14:49:20,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01638802 0.13941963 0.08767226 0.09914209 0.05825783 0.02864858
 0.08753244 0.08985218 0.15768522 0.06047502 0.17492674], sum to 1.0000
[2019-04-09 14:49:20,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4060
[2019-04-09 14:49:20,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7788
[2019-04-09 14:49:20,111] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.0, 68.0, 169.5, 80.0, 22.5, 25.86040263072213, 0.4331365925854292, 1.0, 1.0, 50.0, 35.2513013372992], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2286000.0000, 
sim time next is 2286600.0000, 
raw observation next is [-4.7, 66.33333333333334, 166.6666666666667, 90.0, 22.5, 25.89410507067068, 0.447662904832006, 1.0, 1.0, 60.0, 44.71903381470915], 
processed observation next is [1.0, 0.4782608695652174, 0.332409972299169, 0.6633333333333334, 0.5555555555555557, 0.09944751381215469, 0.375, 0.6578420892225566, 0.6492209682773353, 1.0, 1.0, 0.9, 0.44719033814709147], 
reward next is 0.5528, 
noisyNet noise sample is [array([0.39880228], dtype=float32), -0.27580118]. 
=============================================
[2019-04-09 14:49:20,114] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 19.0, 24.10275345818375, 0.05869259134931584, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2254800.0000, 
sim time next is 2255400.0000, 
raw observation next is [-7.55, 84.0, 0.0, 0.0, 19.0, 23.98741760178114, 0.07513360798032914, 0.0, 1.0, 40.0, 52.12977062801254], 
processed observation next is [1.0, 0.08695652173913043, 0.25346260387811637, 0.84, 0.0, 0.0, 0.08333333333333333, 0.49895146681509495, 0.525044535993443, 0.0, 1.0, 0.5, 0.5212977062801254], 
reward next is 0.4787, 
noisyNet noise sample is [array([-1.5001233], dtype=float32), -0.35820445]. 
=============================================
[2019-04-09 14:49:20,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01986503 0.12292301 0.09491249 0.09989839 0.0643187  0.03506254
 0.08882385 0.09849856 0.14032334 0.08571512 0.14965905], sum to 1.0000
[2019-04-09 14:49:20,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-09 14:49:20,274] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.5, 59.66666666666666, 194.6666666666667, 98.0, 22.5, 25.95126558679173, 0.4591897978791181, 1.0, 1.0, 45.0, 32.41979830368193], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2289000.0000, 
sim time next is 2289600.0000, 
raw observation next is [-3.2, 58.0, 211.5, 92.0, 22.5, 25.92066703286331, 0.4691108743298387, 1.0, 1.0, 55.0, 39.5109406624363], 
processed observation next is [1.0, 0.5217391304347826, 0.37396121883656513, 0.58, 0.705, 0.10165745856353592, 0.375, 0.6600555860719425, 0.6563702914432795, 1.0, 1.0, 0.8, 0.395109406624363], 
reward next is 0.6049, 
noisyNet noise sample is [array([1.3358116], dtype=float32), 0.4130458]. 
=============================================
[2019-04-09 14:49:20,735] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01891651 0.13520072 0.09892734 0.0905816  0.06888506 0.03074661
 0.08542471 0.09448231 0.1413259  0.07961209 0.15589713], sum to 1.0000
[2019-04-09 14:49:20,738] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8636
[2019-04-09 14:49:20,768] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 22.5, 25.95789180908386, 0.5498659806854694, 1.0, 1.0, 25.0, 35.95858267755646], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 22.5, 25.35940036777773, 0.5266574427685636, 1.0, 1.0, 65.0, 40.67119347744065], 
processed observation next is [1.0, 0.5652173913043478, 0.44090489381348114, 0.46, 0.6255555555555558, 0.07292817679558011, 0.375, 0.6132833639814775, 0.6755524809228546, 1.0, 1.0, 1.0, 0.40671193477440654], 
reward next is 0.5933, 
noisyNet noise sample is [array([1.1643901], dtype=float32), 0.19988132]. 
=============================================
[2019-04-09 14:49:20,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03387132 0.08935731 0.09273605 0.06917013 0.05306131 0.04059673
 0.13523284 0.08458815 0.11117885 0.11566464 0.17454267], sum to 1.0000
[2019-04-09 14:49:20,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8844
[2019-04-09 14:49:20,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 22.5, 22.99105683442557, -0.1364777015728789, 1.0, 1.0, 55.0, 54.85319066820062], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2272800.0000, 
sim time next is 2273400.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 22.5, 22.91313825613527, -0.08791188100625343, 1.0, 1.0, 65.0, 65.5531268607389], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.375, 0.40942818801127245, 0.47069603966458223, 1.0, 1.0, 1.0, 0.655531268607389], 
reward next is 0.3445, 
noisyNet noise sample is [array([-0.8974134], dtype=float32), -0.72799075]. 
=============================================
[2019-04-09 14:49:20,926] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03340408 0.09652354 0.08646709 0.06968715 0.05286121 0.04076165
 0.12604058 0.0877059  0.13680317 0.10415091 0.16559468], sum to 1.0000
[2019-04-09 14:49:20,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7493
[2019-04-09 14:49:20,951] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.566666666666666, 88.33333333333334, 0.0, 0.0, 19.0, 24.69269807044276, 0.2350800581301193, 0.0, 1.0, 65.0, 57.13986260846706], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2262000.0000, 
sim time next is 2262600.0000, 
raw observation next is [-8.65, 89.0, 0.0, 0.0, 19.0, 24.71036614342762, 0.2344442224695642, 0.0, 1.0, 60.0, 55.57651747287392], 
processed observation next is [1.0, 0.17391304347826086, 0.22299168975069253, 0.89, 0.0, 0.0, 0.08333333333333333, 0.5591971786189683, 0.5781480741565214, 0.0, 1.0, 0.9, 0.5557651747287392], 
reward next is 0.4442, 
noisyNet noise sample is [array([-0.18781938], dtype=float32), 0.07919435]. 
=============================================
[2019-04-09 14:49:20,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0389071  0.0965959  0.0883129  0.07995968 0.05265196 0.03829967
 0.12845385 0.09200056 0.11406463 0.10505986 0.16569395], sum to 1.0000
[2019-04-09 14:49:20,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9184
[2019-04-09 14:49:21,002] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 24.29247930197206, 0.1452690445909733, 0.0, 1.0, 45.0, 39.06287843899251], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2263200.0000, 
sim time next is 2263800.0000, 
raw observation next is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 19.0, 24.26199526540981, 0.1459699398069182, 0.0, 1.0, 65.0, 56.86667615007288], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.9033333333333334, 0.0, 0.0, 0.08333333333333333, 0.5218329387841507, 0.5486566466023061, 0.0, 1.0, 1.0, 0.5686667615007288], 
reward next is 0.4313, 
noisyNet noise sample is [array([0.45916188], dtype=float32), 0.011534492]. 
=============================================
[2019-04-09 14:49:21,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01813322 0.14919277 0.1013004  0.07725489 0.0589997  0.02975135
 0.09993137 0.08130326 0.13981515 0.06798459 0.1763333 ], sum to 1.0000
[2019-04-09 14:49:21,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3176
[2019-04-09 14:49:21,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02804022 0.09364527 0.08681384 0.06427872 0.04918225 0.03310006
 0.15943941 0.10016498 0.08590699 0.09738135 0.20204684], sum to 1.0000
[2019-04-09 14:49:21,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4655
[2019-04-09 14:49:21,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 22.5, 26.98839788340211, 0.6689839710208538, 1.0, 1.0, 60.0, 35.91770134395318], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2305800.0000, 
sim time next is 2306400.0000, 
raw observation next is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 22.5, 26.95706090423247, 0.5081832294473355, 1.0, 1.0, 30.0, 48.93624317347091], 
processed observation next is [1.0, 0.6956521739130435, 0.45152354570637127, 0.4733333333333333, 0.1166666666666667, 0.0, 0.375, 0.7464217420193725, 0.6693944098157786, 1.0, 1.0, 0.3, 0.4893624317347091], 
reward next is 0.5106, 
noisyNet noise sample is [array([-0.937301], dtype=float32), 0.0326912]. 
=============================================
[2019-04-09 14:49:21,188] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 74.33333333333333, 0.0, 0.0, 19.0, 25.24458818093611, 0.3852693893317892, 0.0, 1.0, 35.0, 51.31242656229124], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2242200.0000, 
sim time next is 2242800.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 19.0, 25.21982155490191, 0.3694442818267127, 0.0, 1.0, 45.0, 41.31109449800634], 
processed observation next is [1.0, 1.0, 0.2908587257617729, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6016517962418257, 0.6231480939422376, 0.0, 1.0, 0.6, 0.4131109449800634], 
reward next is 0.5869, 
noisyNet noise sample is [array([-0.7439105], dtype=float32), 2.8176637]. 
=============================================
[2019-04-09 14:49:21,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02240095 0.13664615 0.0932912  0.08760205 0.05534446 0.03501401
 0.105423   0.08463943 0.1418771  0.08996721 0.14779441], sum to 1.0000
[2019-04-09 14:49:21,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0346
[2019-04-09 14:49:21,367] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.85, 73.0, 178.0, 50.0, 22.5, 25.37782510476173, 0.3119332936277958, 1.0, 1.0, 25.0, 31.89669527032765], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2284200.0000, 
sim time next is 2284800.0000, 
raw observation next is [-5.566666666666666, 71.33333333333333, 175.1666666666667, 60.0, 22.5, 25.37937360759716, 0.3142744306450031, 1.0, 1.0, 20.0, 30.13461256411723], 
processed observation next is [1.0, 0.43478260869565216, 0.3084025854108957, 0.7133333333333333, 0.583888888888889, 0.06629834254143646, 0.375, 0.6149478006330966, 0.6047581435483343, 1.0, 1.0, 0.1, 0.3013461256411723], 
reward next is 0.6987, 
noisyNet noise sample is [array([-0.75052214], dtype=float32), -2.5388005]. 
=============================================
[2019-04-09 14:49:21,448] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02029329 0.12142272 0.09535953 0.0826903  0.04769158 0.0363108
 0.12219129 0.08675121 0.12644166 0.07736775 0.18347986], sum to 1.0000
[2019-04-09 14:49:21,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9150
[2019-04-09 14:49:21,465] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 22.5, 26.77976598494958, 0.6823904264288747, 1.0, 1.0, 50.0, 37.91054640868602], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 22.5, 26.77570141275872, 0.6705999915185576, 1.0, 1.0, 65.0, 37.33488481412648], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.375, 0.7313084510632267, 0.7235333305061858, 1.0, 1.0, 1.0, 0.37334884814126484], 
reward next is 0.6267, 
noisyNet noise sample is [array([0.8668142], dtype=float32), -0.7633044]. 
=============================================
[2019-04-09 14:49:21,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01639466 0.10138884 0.11160535 0.06363773 0.04773525 0.03447007
 0.11888622 0.09253123 0.10325897 0.07155441 0.23853716], sum to 1.0000
[2019-04-09 14:49:21,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1992
[2019-04-09 14:49:21,984] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 26.10808784458777, 0.5806399759847106, 0.0, 1.0, 25.0, 31.90826113209839], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2317200.0000, 
sim time next is 2317800.0000, 
raw observation next is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 22.5, 25.99678687957012, 0.5646004421935058, 1.0, 1.0, 25.0, 29.76847537306418], 
processed observation next is [1.0, 0.8260869565217391, 0.4178208679593721, 0.5566666666666668, 0.0, 0.0, 0.375, 0.6663989066308433, 0.6882001473978353, 1.0, 1.0, 0.2, 0.2976847537306418], 
reward next is 0.7023, 
noisyNet noise sample is [array([-0.8337567], dtype=float32), -1.63986]. 
=============================================
[2019-04-09 14:49:22,054] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02313391 0.10838686 0.09847552 0.06519219 0.05323686 0.03273953
 0.1391708  0.09315536 0.10782345 0.10344128 0.17524421], sum to 1.0000
[2019-04-09 14:49:22,055] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3031
[2019-04-09 14:49:22,071] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 19.0, 25.61553402616114, 0.5369408459030457, 0.0, 1.0, 55.0, 45.07996531779845], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 19.0, 25.65130820828747, 0.5389682023103379, 0.0, 1.0, 35.0, 31.58467287640735], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.08333333333333333, 0.6376090173572893, 0.6796560674367793, 0.0, 1.0, 0.4, 0.3158467287640735], 
reward next is 0.6842, 
noisyNet noise sample is [array([-0.9392527], dtype=float32), 0.17481233]. 
=============================================
[2019-04-09 14:49:22,078] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.0865794]
 [1.2721366]
 [1.2728504]
 [1.1480565]
 [1.2197021]], R is [[1.83989429]
 [2.37069583]
 [2.80107379]
 [3.50726557]
 [4.19168472]].
[2019-04-09 14:49:22,103] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0179807  0.10893232 0.10356514 0.08509322 0.05244738 0.03437699
 0.12486185 0.08914538 0.13213438 0.07927661 0.17218603], sum to 1.0000
[2019-04-09 14:49:22,104] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0304
[2019-04-09 14:49:22,117] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 22.5, 26.68133695850417, 0.5995191747158009, 1.0, 1.0, 35.0, 32.34026627893332], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2305800.0000, 
sim time next is 2306400.0000, 
raw observation next is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 22.5, 26.65939788189222, 0.6048198467656963, 1.0, 1.0, 20.0, 33.40765024005166], 
processed observation next is [1.0, 0.6956521739130435, 0.45152354570637127, 0.4733333333333333, 0.1166666666666667, 0.0, 0.375, 0.7216164901576851, 0.7016066155885654, 1.0, 1.0, 0.1, 0.33407650240051656], 
reward next is 0.6659, 
noisyNet noise sample is [array([2.0086257], dtype=float32), -0.19881593]. 
=============================================
[2019-04-09 14:49:22,298] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0268572  0.09859931 0.09728359 0.05694505 0.04998986 0.03339949
 0.13857843 0.10386429 0.08595727 0.10331161 0.20521392], sum to 1.0000
[2019-04-09 14:49:22,303] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6245
[2019-04-09 14:49:22,319] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.3, 60.0, 0.0, 0.0, 19.0, 25.76687699284097, 0.4942272743683076, 0.0, 1.0, 50.0, 33.81050894729466], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2329800.0000, 
sim time next is 2330400.0000, 
raw observation next is [-2.3, 61.0, 0.0, 0.0, 19.0, 25.70506422219671, 0.4778438855425668, 0.0, 1.0, 55.0, 38.72761097629243], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.61, 0.0, 0.0, 0.08333333333333333, 0.6420886851830593, 0.6592812951808557, 0.0, 1.0, 0.8, 0.3872761097629243], 
reward next is 0.6127, 
noisyNet noise sample is [array([0.5808959], dtype=float32), -0.9613845]. 
=============================================
[2019-04-09 14:49:22,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.029452   0.11561828 0.08878298 0.07089431 0.05895129 0.04337309
 0.13102305 0.09777986 0.09760979 0.09606577 0.17044955], sum to 1.0000
[2019-04-09 14:49:22,358] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5496
[2019-04-09 14:49:22,376] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 64.5, 0.0, 0.0, 19.0, 25.97598996413569, 0.5357019307890069, 0.0, 1.0, 65.0, 46.18220335601508], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2333400.0000, 
sim time next is 2334000.0000, 
raw observation next is [-2.3, 64.0, 0.0, 0.0, 19.0, 25.96230376313067, 0.5318874687069408, 0.0, 1.0, 50.0, 46.09877583089531], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6635253135942225, 0.6772958229023137, 0.0, 1.0, 0.7, 0.4609877583089531], 
reward next is 0.5390, 
noisyNet noise sample is [array([-0.46791816], dtype=float32), 0.31827354]. 
=============================================
[2019-04-09 14:49:22,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.95054996]
 [0.91996837]
 [1.1619003 ]
 [1.0382713 ]
 [1.1050125 ]], R is [[1.46202028]
 [1.98557806]
 [2.57754922]
 [3.10293651]
 [3.7048347 ]].
[2019-04-09 14:49:22,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02446451 0.11398397 0.07911345 0.08917778 0.0539372  0.03661163
 0.14377818 0.08930434 0.11027561 0.06966515 0.18968818], sum to 1.0000
[2019-04-09 14:49:22,505] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7629
[2019-04-09 14:49:22,518] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 19.0, 26.0786849249807, 0.5879780705819876, 0.0, 1.0, 25.0, 42.12107728999584], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 19.0, 26.10334970322477, 0.5876063390838612, 0.0, 1.0, 60.0, 40.61045363213288], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.08333333333333333, 0.6752791419353974, 0.6958687796946204, 0.0, 1.0, 0.9, 0.4061045363213288], 
reward next is 0.5939, 
noisyNet noise sample is [array([-1.3597356], dtype=float32), 0.5802339]. 
=============================================
[2019-04-09 14:49:22,846] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0256561  0.11395485 0.09328739 0.0868267  0.05619675 0.03056153
 0.14404212 0.09588335 0.11739501 0.06668863 0.1695075 ], sum to 1.0000
[2019-04-09 14:49:22,850] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1017
[2019-04-09 14:49:22,865] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 54.33333333333333, 0.0, 0.0, 19.0, 25.93446889675284, 0.555883218059162, 0.0, 1.0, 65.0, 45.74606946094229], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2322600.0000, 
sim time next is 2323200.0000, 
raw observation next is [-1.7, 54.66666666666667, 0.0, 0.0, 19.0, 25.94535090122844, 0.5579630540079594, 0.0, 1.0, 20.0, 41.67872781110317], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6621125751023701, 0.6859876846693198, 0.0, 1.0, 0.1, 0.4167872781110317], 
reward next is 0.5832, 
noisyNet noise sample is [array([0.7734869], dtype=float32), -0.77029467]. 
=============================================
[2019-04-09 14:49:23,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03816869 0.08200664 0.08027381 0.05804308 0.06250532 0.04653614
 0.12776263 0.11130306 0.09479507 0.12198269 0.17662288], sum to 1.0000
[2019-04-09 14:49:23,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3589
[2019-04-09 14:49:23,335] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 63.5, 0.0, 0.0, 19.0, 25.22963498918357, 0.3790472418218878, 0.0, 1.0, 30.0, 34.75135153733776], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2334600.0000, 
sim time next is 2335200.0000, 
raw observation next is [-2.3, 63.0, 0.0, 0.0, 19.0, 25.1914334923277, 0.3634032835081671, 0.0, 1.0, 40.0, 29.84254453275686], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.63, 0.0, 0.0, 0.08333333333333333, 0.5992861243606417, 0.6211344278360557, 0.0, 1.0, 0.5, 0.2984254453275686], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.14079899], dtype=float32), 0.2342874]. 
=============================================
[2019-04-09 14:49:23,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01868472 0.14034806 0.09392055 0.08775987 0.06036059 0.02998366
 0.11723121 0.08896901 0.13114321 0.08598544 0.14561363], sum to 1.0000
[2019-04-09 14:49:23,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1631
[2019-04-09 14:49:23,458] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.95, 56.83333333333334, 228.3333333333333, 86.0, 22.5, 25.6788223601467, 0.4083286632047999, 1.0, 1.0, 25.0, 29.96748147984762], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 22.5, 25.69152814675461, 0.4121785695819096, 1.0, 1.0, 25.0, 28.30905636646362], 
processed observation next is [1.0, 0.5217391304347826, 0.38781163434903054, 0.5566666666666668, 0.8172222222222224, 0.08839779005524862, 0.375, 0.6409606788962176, 0.6373928565273032, 1.0, 1.0, 0.2, 0.2830905636646362], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.02901766], dtype=float32), -1.2546201]. 
=============================================
[2019-04-09 14:49:23,938] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04595115 0.12231384 0.08346635 0.09294767 0.08923274 0.05660228
 0.1119016  0.09105837 0.08594036 0.10629074 0.11429485], sum to 1.0000
[2019-04-09 14:49:23,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-09 14:49:23,954] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 24.10299005639785, 0.1461981641260579, 0.0, 1.0, 60.0, 49.50440522543908], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2359200.0000, 
sim time next is 2359800.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 24.14468709099127, 0.1735424211554731, 0.0, 1.0, 65.0, 75.72091933827893], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5120572575826058, 0.557847473718491, 0.0, 1.0, 1.0, 0.7572091933827894], 
reward next is 0.2428, 
noisyNet noise sample is [array([0.7643779], dtype=float32), 0.4009114]. 
=============================================
[2019-04-09 14:49:23,979] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02032383 0.11187796 0.09941169 0.06735628 0.05211276 0.02828594
 0.16893786 0.09089296 0.0946115  0.09979651 0.16639273], sum to 1.0000
[2019-04-09 14:49:23,981] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7594
[2019-04-09 14:49:23,995] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.8, 56.5, 0.0, 0.0, 19.0, 25.89974995052756, 0.5313023059432734, 0.0, 1.0, 30.0, 46.33032411125376], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2326200.0000, 
sim time next is 2326800.0000, 
raw observation next is [-1.9, 57.0, 0.0, 0.0, 19.0, 25.97408003098733, 0.5381393718054989, 0.0, 1.0, 60.0, 49.96370031625995], 
processed observation next is [1.0, 0.9565217391304348, 0.4099722991689751, 0.57, 0.0, 0.0, 0.08333333333333333, 0.6645066692489442, 0.679379790601833, 0.0, 1.0, 0.9, 0.4996370031625995], 
reward next is 0.5004, 
noisyNet noise sample is [array([-0.3634864], dtype=float32), 0.1625036]. 
=============================================
[2019-04-09 14:49:24,091] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.04626354 0.12907195 0.09196424 0.0956893  0.08107828 0.06335972
 0.10471079 0.09779587 0.0824743  0.1039953  0.1035967 ], sum to 1.0000
[2019-04-09 14:49:24,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-09 14:49:24,111] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 65.0, 130.0, 405.0, 19.0, 24.62483450123389, 0.2505512065868086, 0.0, 1.0, 25.0, 48.38998616919374], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2368800.0000, 
sim time next is 2369400.0000, 
raw observation next is [-2.716666666666667, 64.5, 133.0, 420.0, 19.0, 24.51936153833785, 0.2490396489377822, 0.0, 1.0, 20.0, 32.98396278289367], 
processed observation next is [0.0, 0.43478260869565216, 0.3873499538319483, 0.645, 0.44333333333333336, 0.46408839779005523, 0.08333333333333333, 0.5432801281948209, 0.5830132163125941, 0.0, 1.0, 0.1, 0.3298396278289367], 
reward next is 0.6702, 
noisyNet noise sample is [array([-1.6517597], dtype=float32), 0.10258786]. 
=============================================
[2019-04-09 14:49:24,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04589328 0.12658608 0.09170405 0.11756372 0.08480239 0.0627673
 0.09580114 0.07797424 0.09241112 0.09976937 0.10472735], sum to 1.0000
[2019-04-09 14:49:24,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-09 14:49:24,520] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.55, 63.5, 139.0, 450.0, 19.0, 24.56209561647328, 0.2467536707258066, 0.0, 1.0, 50.0, 34.8942273802611], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2370600.0000, 
sim time next is 2371200.0000, 
raw observation next is [-2.466666666666667, 63.0, 143.6666666666667, 426.0, 19.0, 24.54506151264121, 0.2419610700116646, 0.0, 1.0, 25.0, 30.20536853974541], 
processed observation next is [0.0, 0.43478260869565216, 0.39427516158818104, 0.63, 0.47888888888888903, 0.4707182320441989, 0.08333333333333333, 0.5454217927201009, 0.5806536900038882, 0.0, 1.0, 0.2, 0.3020536853974541], 
reward next is 0.6979, 
noisyNet noise sample is [array([-1.3958453], dtype=float32), 0.56168973]. 
=============================================
[2019-04-09 14:49:24,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0514333  0.12227993 0.08578091 0.08795279 0.08791865 0.06763869
 0.1002794  0.08483924 0.09321777 0.0950312  0.12362819], sum to 1.0000
[2019-04-09 14:49:24,654] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-09 14:49:24,668] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 19.0, 24.55768069139539, 0.225240984848605, 0.0, 1.0, 30.0, 38.51959321452026], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 19.0, 24.51923053318944, 0.2152510642076553, 0.0, 1.0, 40.0, 30.32092321146316], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.54326921109912, 0.5717503547358851, 0.0, 1.0, 0.5, 0.3032092321146316], 
reward next is 0.6968, 
noisyNet noise sample is [array([0.48665264], dtype=float32), -0.09687058]. 
=============================================
[2019-04-09 14:49:24,841] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04944354 0.1229215  0.08459402 0.10513726 0.0901233  0.06563833
 0.09551251 0.07670091 0.09942476 0.0979884  0.11251537], sum to 1.0000
[2019-04-09 14:49:24,844] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8264
[2019-04-09 14:49:24,866] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 19.0, 25.34945424276145, 0.3775698560689774, 0.0, 1.0, 50.0, 36.51321623663181], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2345400.0000, 
sim time next is 2346000.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 19.0, 25.31197496382655, 0.3709968562505208, 0.0, 1.0, 60.0, 47.20603085822603], 
processed observation next is [0.0, 0.13043478260869565, 0.38965835641735924, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6093312469855459, 0.6236656187501736, 0.0, 1.0, 0.9, 0.4720603085822603], 
reward next is 0.5279, 
noisyNet noise sample is [array([-0.6573077], dtype=float32), -0.8011585]. 
=============================================
[2019-04-09 14:49:24,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04986509 0.12538597 0.0880328  0.10416677 0.0883444  0.0655593
 0.09631661 0.08416138 0.09157099 0.09487134 0.11172536], sum to 1.0000
[2019-04-09 14:49:24,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2271
[2019-04-09 14:49:24,878] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[0.36561286]
 [0.4342335 ]
 [0.3975541 ]
 [0.27589417]
 [0.4339779 ]], R is [[0.95810729]
 [1.58339405]
 [2.18912888]
 [2.76281643]
 [3.20241284]].
[2019-04-09 14:49:24,887] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 65.0, 130.0, 405.0, 19.0, 24.1130715800633, 0.2030162593124771, 0.0, 1.0, 35.0, 45.36358463637073], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2368800.0000, 
sim time next is 2369400.0000, 
raw observation next is [-2.716666666666667, 64.5, 133.0, 420.0, 19.0, 24.21738361838122, 0.2143806806662975, 0.0, 1.0, 25.0, 32.38687903887331], 
processed observation next is [0.0, 0.43478260869565216, 0.3873499538319483, 0.645, 0.44333333333333336, 0.46408839779005523, 0.08333333333333333, 0.5181153015317683, 0.5714602268887659, 0.0, 1.0, 0.2, 0.3238687903887331], 
reward next is 0.6761, 
noisyNet noise sample is [array([1.5245334], dtype=float32), 0.9728984]. 
=============================================
[2019-04-09 14:49:25,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02411959 0.11109143 0.08358865 0.06164588 0.05277241 0.02971765
 0.153503   0.10051309 0.0964896  0.08067409 0.20588455], sum to 1.0000
[2019-04-09 14:49:25,308] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4246
[2019-04-09 14:49:25,325] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.8, 56.5, 0.0, 0.0, 19.0, 25.78705816434485, 0.493779044130062, 0.0, 1.0, 65.0, 55.66622788907608], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2326200.0000, 
sim time next is 2326800.0000, 
raw observation next is [-1.9, 57.0, 0.0, 0.0, 19.0, 25.79186267443459, 0.5015555769637304, 0.0, 1.0, 45.0, 45.16286520163462], 
processed observation next is [1.0, 0.9565217391304348, 0.4099722991689751, 0.57, 0.0, 0.0, 0.08333333333333333, 0.6493218895362158, 0.6671851923212434, 0.0, 1.0, 0.6, 0.45162865201634617], 
reward next is 0.5484, 
noisyNet noise sample is [array([0.768809], dtype=float32), 2.5028791]. 
=============================================
[2019-04-09 14:49:25,362] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63305: loss 24.1123
[2019-04-09 14:49:25,366] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63307: learning rate 0.0000
[2019-04-09 14:49:25,498] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63379: loss 19.2442
[2019-04-09 14:49:25,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63381: learning rate 0.0000
[2019-04-09 14:49:25,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03763363 0.13220698 0.08012699 0.09855081 0.08107191 0.05267461
 0.1153125  0.09141222 0.08660196 0.09500273 0.1294057 ], sum to 1.0000
[2019-04-09 14:49:25,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3258
[2019-04-09 14:49:25,553] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7833333333333333, 44.83333333333334, 30.33333333333333, 30.0, 19.0, 25.1741189323624, 0.3589329236804259, 0.0, 1.0, 30.0, 29.13591573751453], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2394600.0000, 
sim time next is 2395200.0000, 
raw observation next is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 19.0, 25.17588622350488, 0.3448909518143193, 0.0, 1.0, 30.0, 29.00069476521068], 
processed observation next is [0.0, 0.7391304347826086, 0.43582640812557716, 0.4466666666666667, 0.060555555555555536, 0.02541436464088398, 0.08333333333333333, 0.5979905186254065, 0.6149636506047731, 0.0, 1.0, 0.3, 0.29000694765210683], 
reward next is 0.7100, 
noisyNet noise sample is [array([1.0295048], dtype=float32), 0.94723845]. 
=============================================
[2019-04-09 14:49:25,583] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 63427: loss 21.5133
[2019-04-09 14:49:25,589] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 63427: learning rate 0.0000
[2019-04-09 14:49:25,622] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63453: loss 27.4977
[2019-04-09 14:49:25,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63454: learning rate 0.0000
[2019-04-09 14:49:25,661] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63476: loss 24.4189
[2019-04-09 14:49:25,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63476: learning rate 0.0000
[2019-04-09 14:49:25,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03649826 0.08931343 0.0745239  0.06371799 0.06483664 0.04905863
 0.13311566 0.11387517 0.08240395 0.11504875 0.17760761], sum to 1.0000
[2019-04-09 14:49:25,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5900
[2019-04-09 14:49:25,783] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 24.55854485719579, 0.2333507267629074, 0.0, 1.0, 30.0, 23.01877132119732], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2336400.0000, 
sim time next is 2337000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 24.52211721979206, 0.2243688438434054, 0.0, 1.0, 50.0, 33.16276321187885], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.5435097683160052, 0.5747896146144685, 0.0, 1.0, 0.7, 0.3316276321187885], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.469112], dtype=float32), -0.39130095]. 
=============================================
[2019-04-09 14:49:25,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.5988992 ]
 [0.71263105]
 [0.6555093 ]
 [0.71268874]
 [0.83945173]], R is [[1.16523588]
 [1.92339587]
 [2.66368246]
 [3.38580179]
 [4.09205818]].
[2019-04-09 14:49:25,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63558: loss 19.2479
[2019-04-09 14:49:25,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63558: learning rate 0.0000
[2019-04-09 14:49:25,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04369638 0.13032328 0.0887556  0.103527   0.08708333 0.05866877
 0.1082135  0.08982713 0.08590052 0.09374399 0.11026045], sum to 1.0000
[2019-04-09 14:49:25,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2908
[2019-04-09 14:49:25,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04197883 0.11461639 0.09486609 0.09709162 0.09021319 0.05054358
 0.12672254 0.08278216 0.08195668 0.08979608 0.1294328 ], sum to 1.0000
[2019-04-09 14:49:25,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9941
[2019-04-09 14:49:25,893] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.116666666666667, 59.50000000000001, 157.6666666666667, 354.0, 19.0, 24.18630045244687, 0.2531882364425053, 0.0, 1.0, 55.0, 51.59829275870948], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2373000.0000, 
sim time next is 2373600.0000, 
raw observation next is [-1.933333333333333, 57.0, 162.3333333333333, 330.0, 19.0, 24.37209516087422, 0.2694500600872925, 0.0, 1.0, 30.0, 34.91721854298915], 
processed observation next is [0.0, 0.4782608695652174, 0.40904893813481075, 0.57, 0.541111111111111, 0.36464088397790057, 0.08333333333333333, 0.5310079300728517, 0.5898166866957641, 0.0, 1.0, 0.3, 0.3491721854298915], 
reward next is 0.6508, 
noisyNet noise sample is [array([-0.24161075], dtype=float32), 0.23378767]. 
=============================================
[2019-04-09 14:49:25,905] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 19.0, 24.76787939491328, 0.2463940576906491, 0.0, 1.0, 65.0, 66.05521025078431], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2411400.0000, 
sim time next is 2412000.0000, 
raw observation next is [-4.5, 44.0, 0.0, 0.0, 19.0, 24.78695790192279, 0.2928864697869327, 0.0, 1.0, 65.0, 69.06677259952048], 
processed observation next is [0.0, 0.9565217391304348, 0.3379501385041552, 0.44, 0.0, 0.0, 0.08333333333333333, 0.5655798251602325, 0.5976288232623109, 0.0, 1.0, 1.0, 0.6906677259952048], 
reward next is 0.3093, 
noisyNet noise sample is [array([0.5913978], dtype=float32), -0.270751]. 
=============================================
[2019-04-09 14:49:25,908] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.60708094]
 [0.64599895]
 [0.6422297 ]
 [0.58544004]
 [0.4837053 ]], R is [[0.86498201]
 [1.19578004]
 [1.82647336]
 [2.32422304]
 [2.55316448]].
[2019-04-09 14:49:25,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04142017 0.13385834 0.08444336 0.10031952 0.07974426 0.05434139
 0.10617951 0.09698822 0.08783288 0.09552747 0.11934475], sum to 1.0000
[2019-04-09 14:49:25,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3335
[2019-04-09 14:49:25,934] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3, 53.0, 191.0, 0.0, 19.0, 24.16703839234492, 0.1830727038038271, 0.0, 1.0, 55.0, 48.30335460641727], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2381400.0000, 
sim time next is 2382000.0000, 
raw observation next is [-0.2, 52.66666666666667, 185.8333333333333, 0.0, 19.0, 24.18458371196826, 0.1948991368994367, 0.0, 1.0, 30.0, 36.06245697817577], 
processed observation next is [0.0, 0.5652173913043478, 0.4570637119113574, 0.5266666666666667, 0.6194444444444442, 0.0, 0.08333333333333333, 0.5153819759973549, 0.5649663789664788, 0.0, 1.0, 0.3, 0.36062456978175766], 
reward next is 0.6394, 
noisyNet noise sample is [array([-0.7203176], dtype=float32), 1.5919966]. 
=============================================
[2019-04-09 14:49:25,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.61196434]
 [0.612294  ]
 [0.59514844]
 [0.52107906]
 [0.6184812 ]], R is [[1.13669467]
 [1.64229417]
 [2.18074465]
 [3.15893722]
 [3.74456167]].
[2019-04-09 14:49:25,991] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63661: loss 25.0677
[2019-04-09 14:49:25,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63661: learning rate 0.0000
[2019-04-09 14:49:26,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03642263 0.11358397 0.08558565 0.08987745 0.08918323 0.0550308
 0.13221289 0.08451167 0.07658763 0.09808534 0.13891873], sum to 1.0000
[2019-04-09 14:49:26,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3676
[2019-04-09 14:49:26,246] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 43.0, 0.0, 0.0, 19.0, 24.52100806284734, 0.2046724103520416, 0.0, 1.0, 65.0, 57.17384438287378], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2419200.0000, 
sim time next is 2419800.0000, 
raw observation next is [-5.7, 43.83333333333334, 0.0, 0.0, 19.0, 24.60351015310141, 0.2142435959434383, 0.0, 1.0, 35.0, 52.32042655722066], 
processed observation next is [0.0, 0.0, 0.30470914127423826, 0.4383333333333334, 0.0, 0.0, 0.08333333333333333, 0.5502925127584509, 0.5714145319811461, 0.0, 1.0, 0.4, 0.5232042655722066], 
reward next is 0.4768, 
noisyNet noise sample is [array([-0.1019301], dtype=float32), -1.6937687]. 
=============================================
[2019-04-09 14:49:26,334] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63855: loss 34.9305
[2019-04-09 14:49:26,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63855: learning rate 0.0000
[2019-04-09 14:49:26,459] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63928: loss 31.9297
[2019-04-09 14:49:26,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63928: learning rate 0.0000
[2019-04-09 14:49:26,614] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64016: loss 26.9636
[2019-04-09 14:49:26,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64016: learning rate 0.0000
[2019-04-09 14:49:26,620] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03646392 0.1368637  0.07830758 0.09755618 0.08264136 0.05654411
 0.10124488 0.09074501 0.08513806 0.1059949  0.12850033], sum to 1.0000
[2019-04-09 14:49:26,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7792
[2019-04-09 14:49:26,643] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.0, 47.0, 83.66666666666667, 246.6666666666667, 19.0, 25.18801386259763, 0.3580272932777132, 0.0, 1.0, 45.0, 28.04458486173329], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [0.0, 47.0, 82.5, 199.5, 19.0, 25.13906387628779, 0.3668374045409928, 0.0, 1.0, 60.0, 55.34722390080536], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.47, 0.275, 0.22044198895027625, 0.08333333333333333, 0.5949219896906491, 0.6222791348469976, 0.0, 1.0, 0.9, 0.5534722390080536], 
reward next is 0.4465, 
noisyNet noise sample is [array([0.6956878], dtype=float32), -0.35604945]. 
=============================================
[2019-04-09 14:49:26,764] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64105: loss 23.2672
[2019-04-09 14:49:26,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64107: learning rate 0.0000
[2019-04-09 14:49:26,909] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04332546 0.12762749 0.09398048 0.08618139 0.08832243 0.0521859
 0.12065952 0.0899938  0.07832623 0.09251877 0.12687854], sum to 1.0000
[2019-04-09 14:49:26,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1409
[2019-04-09 14:49:26,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 44.0, 0.0, 0.0, 19.0, 24.62959570053407, 0.2137900518974545, 0.0, 1.0, 30.0, 46.39548911734858], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [-4.583333333333333, 43.50000000000001, 0.0, 0.0, 19.0, 24.66344774750852, 0.2064513435044073, 0.0, 1.0, 20.0, 31.31591018825515], 
processed observation next is [0.0, 0.9565217391304348, 0.3356417359187443, 0.43500000000000005, 0.0, 0.0, 0.08333333333333333, 0.5552873122923767, 0.5688171145014691, 0.0, 1.0, 0.1, 0.31315910188255147], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.26302123], dtype=float32), -1.7579463]. 
=============================================
[2019-04-09 14:49:27,100] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64303: loss 24.2747
[2019-04-09 14:49:27,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64304: learning rate 0.0000
[2019-04-09 14:49:27,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05022649 0.10542843 0.09069533 0.07094922 0.08591988 0.06465274
 0.12356222 0.09699398 0.07309274 0.10650545 0.13197348], sum to 1.0000
[2019-04-09 14:49:27,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8574
[2019-04-09 14:49:27,268] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.75, 50.5, 0.0, 0.0, 19.0, 24.35588576337147, 0.09974445057116593, 0.0, 1.0, 45.0, 32.60567961188642], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2424600.0000, 
sim time next is 2425200.0000, 
raw observation next is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 19.0, 24.26294290380744, 0.092548871110803, 0.0, 1.0, 60.0, 60.35824379226203], 
processed observation next is [0.0, 0.043478260869565216, 0.270544783010157, 0.5133333333333333, 0.0, 0.0, 0.08333333333333333, 0.5219119086506199, 0.530849623703601, 0.0, 1.0, 0.9, 0.6035824379226203], 
reward next is 0.3964, 
noisyNet noise sample is [array([-0.15139641], dtype=float32), 0.81249785]. 
=============================================
[2019-04-09 14:49:27,323] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64430: loss 24.5925
[2019-04-09 14:49:27,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64432: learning rate 0.0000
[2019-04-09 14:49:27,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04542874 0.12437476 0.09008951 0.08335026 0.09737806 0.05506378
 0.11708682 0.09416911 0.07044851 0.08897838 0.133632  ], sum to 1.0000
[2019-04-09 14:49:27,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7911
[2019-04-09 14:49:27,349] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64446: loss 24.3186
[2019-04-09 14:49:27,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64447: learning rate 0.0000
[2019-04-09 14:49:27,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 44.0, 0.0, 0.0, 19.0, 24.21235311369487, 0.101329679254603, 0.0, 1.0, 20.0, 33.42093692644613], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [-4.583333333333333, 43.50000000000001, 0.0, 0.0, 19.0, 24.22536395943703, 0.1068273044761092, 0.0, 1.0, 60.0, 58.77238917517381], 
processed observation next is [0.0, 0.9565217391304348, 0.3356417359187443, 0.43500000000000005, 0.0, 0.0, 0.08333333333333333, 0.5187803299530858, 0.5356091014920364, 0.0, 1.0, 0.9, 0.5877238917517381], 
reward next is 0.4123, 
noisyNet noise sample is [array([-1.0536349], dtype=float32), 1.4956512]. 
=============================================
[2019-04-09 14:49:27,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04980453 0.12245034 0.08447424 0.11064784 0.08453849 0.07210008
 0.10683057 0.07669741 0.09022527 0.08685441 0.11537676], sum to 1.0000
[2019-04-09 14:49:27,856] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6404
[2019-04-09 14:49:27,875] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.1, 58.0, 0.0, 0.0, 19.0, 23.26985929742961, -0.08555712756679872, 0.0, 1.0, 65.0, 64.78217291473011], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2431800.0000, 
sim time next is 2432400.0000, 
raw observation next is [-8.2, 59.0, 0.0, 0.0, 19.0, 23.2964132271342, -0.07780822161003227, 0.0, 1.0, 50.0, 51.73631780005868], 
processed observation next is [0.0, 0.13043478260869565, 0.23545706371191139, 0.59, 0.0, 0.0, 0.08333333333333333, 0.4413677689278499, 0.47406392612998927, 0.0, 1.0, 0.7, 0.5173631780005867], 
reward next is 0.4826, 
noisyNet noise sample is [array([-2.3408134], dtype=float32), 0.7016063]. 
=============================================
[2019-04-09 14:49:28,144] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03867751 0.11254453 0.08844249 0.09778509 0.09119579 0.06186928
 0.13213906 0.09415606 0.07232621 0.08570526 0.1251587 ], sum to 1.0000
[2019-04-09 14:49:28,150] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9964
[2019-04-09 14:49:28,168] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.2, 41.66666666666667, 0.0, 0.0, 19.0, 24.0664781296805, 0.06942097804014669, 0.0, 1.0, 65.0, 58.17490074385252], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2416800.0000, 
sim time next is 2417400.0000, 
raw observation next is [-5.3, 42.0, 0.0, 0.0, 19.0, 24.01895990690429, 0.09218566508787018, 0.0, 1.0, 65.0, 74.69364400551817], 
processed observation next is [0.0, 1.0, 0.31578947368421056, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5015799922420241, 0.5307285550292901, 0.0, 1.0, 1.0, 0.7469364400551817], 
reward next is 0.2531, 
noisyNet noise sample is [array([1.5976199], dtype=float32), 0.7352861]. 
=============================================
[2019-04-09 14:49:28,385] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05818291 0.10864004 0.0831902  0.11083783 0.08838994 0.07608502
 0.11029785 0.08186516 0.08350632 0.08778177 0.11122289], sum to 1.0000
[2019-04-09 14:49:28,387] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-09 14:49:28,404] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 60.16666666666666, 0.0, 0.0, 19.0, 22.32048089051707, -0.313507664404804, 0.0, 1.0, 50.0, 41.86975231531178], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2441400.0000, 
sim time next is 2442000.0000, 
raw observation next is [-9.100000000000001, 60.33333333333334, 0.0, 0.0, 19.0, 22.36147689180291, -0.3228233759004694, 0.0, 1.0, 45.0, 35.65108308355213], 
processed observation next is [0.0, 0.2608695652173913, 0.21052631578947364, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.3634564076502424, 0.3923922080331768, 0.0, 1.0, 0.6, 0.3565108308355213], 
reward next is 0.6435, 
noisyNet noise sample is [array([-0.18321127], dtype=float32), 0.27274713]. 
=============================================
[2019-04-09 14:49:28,409] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04592103 0.11314154 0.08358268 0.10263128 0.08580497 0.06104867
 0.10826211 0.07475067 0.10085053 0.09065459 0.13335204], sum to 1.0000
[2019-04-09 14:49:28,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1766
[2019-04-09 14:49:28,419] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.29775423]
 [0.3927254 ]
 [0.30430943]
 [0.4393536 ]
 [0.26061845]], R is [[0.96163034]
 [1.53331649]
 [2.03805208]
 [2.51611137]
 [2.92834568]].
[2019-04-09 14:49:28,425] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.4, 60.83333333333334, 0.0, 0.0, 19.0, 23.68020787320746, -0.06579542754483651, 0.0, 1.0, 50.0, 38.62937779969305], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2443800.0000, 
sim time next is 2444400.0000, 
raw observation next is [-9.5, 61.0, 0.0, 0.0, 19.0, 23.61532241393894, -0.08775983112931397, 0.0, 1.0, 25.0, 36.91658063290534], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.61, 0.0, 0.0, 0.08333333333333333, 0.4679435344949117, 0.47074672295689535, 0.0, 1.0, 0.2, 0.3691658063290534], 
reward next is 0.6308, 
noisyNet noise sample is [array([-0.6879521], dtype=float32), 0.83024335]. 
=============================================
[2019-04-09 14:49:28,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04013097 0.11227912 0.08054831 0.08130358 0.09118097 0.05890257
 0.11866567 0.08252466 0.08975253 0.10016286 0.14454883], sum to 1.0000
[2019-04-09 14:49:28,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2618
[2019-04-09 14:49:28,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 48.0, 0.0, 0.0, 19.0, 24.07767133771512, 0.07754781708458637, 0.0, 1.0, 30.0, 47.20271508847526], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 19.0, 24.05828104618699, 0.06628332103467026, 0.0, 1.0, 30.0, 32.66751238265716], 
processed observation next is [0.0, 0.043478260869565216, 0.28578024007386893, 0.4883333333333333, 0.0, 0.0, 0.08333333333333333, 0.5048567538489159, 0.52209444034489, 0.0, 1.0, 0.3, 0.3266751238265716], 
reward next is 0.6733, 
noisyNet noise sample is [array([-0.61467564], dtype=float32), 0.98224944]. 
=============================================
[2019-04-09 14:49:28,769] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 65250: loss 23.8803
[2019-04-09 14:49:28,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 65251: learning rate 0.0000
[2019-04-09 14:49:28,821] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 65269: loss 27.2876
[2019-04-09 14:49:28,823] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 65271: learning rate 0.0000
[2019-04-09 14:49:28,885] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05164944 0.1295762  0.09280635 0.09226819 0.07360893 0.06857175
 0.1002484  0.10615143 0.07213582 0.08778111 0.12520234], sum to 1.0000
[2019-04-09 14:49:28,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6777
[2019-04-09 14:49:28,900] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.116666666666667, 27.0, 72.66666666666667, 749.3333333333334, 19.0, 24.71827860975155, 0.1757953626722989, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2472600.0000, 
sim time next is 2473200.0000, 
raw observation next is [3.3, 27.0, 70.0, 732.0, 19.0, 24.59285510621548, 0.2005817217526133, 0.0, 1.0, 50.0, 44.82466075756773], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.27, 0.23333333333333334, 0.8088397790055248, 0.08333333333333333, 0.5494045921846235, 0.5668605739175377, 0.0, 1.0, 0.7, 0.44824660757567725], 
reward next is 0.5518, 
noisyNet noise sample is [array([-1.470583], dtype=float32), -0.5442421]. 
=============================================
[2019-04-09 14:49:28,953] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04439989 0.12974617 0.09885986 0.09899142 0.07088923 0.06705025
 0.09442312 0.0906767  0.08877926 0.0838244  0.1323597 ], sum to 1.0000
[2019-04-09 14:49:28,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0172
[2019-04-09 14:49:28,965] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 19.0, 24.97947173801629, 0.2849034619194341, 0.0, 1.0, 40.0, 25.22385132593692], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [1.9, 27.5, 87.0, 832.0, 19.0, 24.95883457295355, 0.2321157211806887, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.275, 0.29, 0.9193370165745857, 0.08333333333333333, 0.5799028810794624, 0.5773719070602296, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16153458], dtype=float32), 0.011448527]. 
=============================================
[2019-04-09 14:49:29,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03565101 0.1251951  0.08762772 0.10499842 0.08780447 0.05089891
 0.1245138  0.07781467 0.08217429 0.09638573 0.12693581], sum to 1.0000
[2019-04-09 14:49:29,341] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9584
[2019-04-09 14:49:29,359] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 19.0, 25.29230666811338, 0.3291630784477977, 0.0, 1.0, 55.0, 40.81313540823891], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2410800.0000, 
sim time next is 2411400.0000, 
raw observation next is [-4.316666666666666, 43.66666666666667, 0.0, 0.0, 19.0, 25.25160213458111, 0.3319066070418457, 0.0, 1.0, 65.0, 64.35869910249535], 
processed observation next is [0.0, 0.9130434782608695, 0.34302862419205915, 0.4366666666666667, 0.0, 0.0, 0.08333333333333333, 0.604300177881759, 0.6106355356806152, 0.0, 1.0, 1.0, 0.6435869910249535], 
reward next is 0.3564, 
noisyNet noise sample is [array([-0.00614967], dtype=float32), -1.0215002]. 
=============================================
[2019-04-09 14:49:29,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0484409  0.13191593 0.09815645 0.09989693 0.08081116 0.07458346
 0.09535448 0.08842573 0.07356998 0.08320301 0.12564197], sum to 1.0000
[2019-04-09 14:49:29,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8598
[2019-04-09 14:49:29,436] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 19.0, 24.08406480116232, 0.1214262834091883, 0.0, 1.0, 40.0, 27.96313533387925], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2466600.0000, 
sim time next is 2467200.0000, 
raw observation next is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 19.0, 24.1543480890335, 0.145145939492684, 0.0, 1.0, 60.0, 56.04346261585528], 
processed observation next is [0.0, 0.5652173913043478, 0.5124653739612189, 0.2766666666666667, 0.2916666666666667, 0.921731123388582, 0.08333333333333333, 0.5128623407527918, 0.5483819798308947, 0.0, 1.0, 0.9, 0.5604346261585529], 
reward next is 0.4396, 
noisyNet noise sample is [array([0.12113292], dtype=float32), 0.27515545]. 
=============================================
[2019-04-09 14:49:29,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04743395 0.12958407 0.09423915 0.10163248 0.08258165 0.07044458
 0.09879547 0.0967638  0.07369747 0.08281831 0.12200913], sum to 1.0000
[2019-04-09 14:49:29,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-09 14:49:29,615] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.383333333333333, 27.0, 81.0, 800.0, 19.0, 24.40232866996049, 0.1691984809037945, 0.0, 1.0, 25.0, 42.45142869550308], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2470200.0000, 
sim time next is 2470800.0000, 
raw observation next is [2.566666666666667, 27.0, 79.5, 792.0, 19.0, 24.38711900579793, 0.1774478636909727, 0.0, 1.0, 20.0, 26.34946188795234], 
processed observation next is [0.0, 0.6086956521739131, 0.5337026777469991, 0.27, 0.265, 0.8751381215469614, 0.08333333333333333, 0.5322599171498276, 0.5591492878969909, 0.0, 1.0, 0.1, 0.2634946188795234], 
reward next is 0.7365, 
noisyNet noise sample is [array([1.3363136], dtype=float32), -1.6008741]. 
=============================================
[2019-04-09 14:49:29,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05372252 0.11539998 0.09316014 0.09229702 0.08587137 0.0642466
 0.11209421 0.09630081 0.0753568  0.09431809 0.1172324 ], sum to 1.0000
[2019-04-09 14:49:29,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1749
[2019-04-09 14:49:29,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04120686 0.11575889 0.08544485 0.09185635 0.09727593 0.06489348
 0.11919729 0.08460311 0.0709546  0.08774958 0.14105909], sum to 1.0000
[2019-04-09 14:49:29,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8972
[2019-04-09 14:49:29,813] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 19.0, 24.5811639007988, 0.2462187777104891, 0.0, 1.0, 45.0, 46.30786438046093], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2464800.0000, 
sim time next is 2465400.0000, 
raw observation next is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 19.0, 24.8009214465721, 0.2674076173975061, 0.0, 1.0, 25.0, 37.3610082102423], 
processed observation next is [0.0, 0.5217391304347826, 0.49676823638042483, 0.285, 0.2966666666666667, 0.9289134438305708, 0.08333333333333333, 0.5667434538810084, 0.5891358724658354, 0.0, 1.0, 0.2, 0.37361008210242297], 
reward next is 0.6264, 
noisyNet noise sample is [array([0.27006587], dtype=float32), 0.7731728]. 
=============================================
[2019-04-09 14:49:29,827] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 19.0, 25.437342321885, 0.347096808616258, 0.0, 1.0, 50.0, 42.6443113911165], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2485200.0000, 
sim time next is 2485800.0000, 
raw observation next is [0.55, 29.0, 0.0, 0.0, 19.0, 25.43155777019631, 0.3592135195179031, 0.0, 1.0, 65.0, 57.23375261795007], 
processed observation next is [0.0, 0.782608695652174, 0.4778393351800555, 0.29, 0.0, 0.0, 0.08333333333333333, 0.6192964808496925, 0.6197378398393011, 0.0, 1.0, 1.0, 0.5723375261795006], 
reward next is 0.4277, 
noisyNet noise sample is [array([-0.48404226], dtype=float32), -2.717932]. 
=============================================
[2019-04-09 14:49:29,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03607791 0.13510622 0.07756316 0.08333082 0.09806027 0.04711855
 0.12414533 0.09115168 0.07744578 0.08764161 0.14235859], sum to 1.0000
[2019-04-09 14:49:29,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3620
[2019-04-09 14:49:29,855] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 33.0, 0.0, 0.0, 19.0, 25.38125332030515, 0.3371256026523675, 0.0, 1.0, 40.0, 49.25583020894921], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [-1.1, 33.33333333333334, 0.0, 0.0, 19.0, 25.42046496456328, 0.3347982627768498, 0.0, 1.0, 35.0, 34.72307734202676], 
processed observation next is [0.0, 0.9565217391304348, 0.4321329639889197, 0.3333333333333334, 0.0, 0.0, 0.08333333333333333, 0.6183720803802734, 0.6115994209256166, 0.0, 1.0, 0.4, 0.34723077342026765], 
reward next is 0.6528, 
noisyNet noise sample is [array([-0.19278398], dtype=float32), 0.9594058]. 
=============================================
[2019-04-09 14:49:29,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.6312442 ]
 [0.6502366 ]
 [0.6712804 ]
 [0.62241656]
 [0.56996894]], R is [[1.30039239]
 [1.79483008]
 [2.12859726]
 [2.68226194]
 [3.24856281]].
[2019-04-09 14:49:29,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03943018 0.12267874 0.08266281 0.09386802 0.09590418 0.05918491
 0.12648381 0.08470979 0.08796151 0.08546309 0.121653  ], sum to 1.0000
[2019-04-09 14:49:29,953] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2912
[2019-04-09 14:49:29,965] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 19.0, 25.07420385949036, 0.2161816606791159, 0.0, 1.0, 20.0, 29.72431403084375], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 19.0, 25.0022353046285, 0.2042445824022414, 0.0, 1.0, 30.0, 28.36003763409046], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.08333333333333333, 0.5835196087190416, 0.5680815274674138, 0.0, 1.0, 0.3, 0.2836003763409046], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.6067167], dtype=float32), -0.60997885]. 
=============================================
[2019-04-09 14:49:30,076] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03961381 0.14235033 0.09908586 0.11110067 0.07745682 0.05329194
 0.09479996 0.08640408 0.0825345  0.08430897 0.12905307], sum to 1.0000
[2019-04-09 14:49:30,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3856
[2019-04-09 14:49:30,093] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.75, 27.0, 78.0, 784.0, 19.0, 25.01188662072625, 0.3372484743382619, 0.0, 1.0, 60.0, 44.25941327274687], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2471400.0000, 
sim time next is 2472000.0000, 
raw observation next is [2.933333333333333, 27.0, 75.33333333333333, 766.6666666666667, 19.0, 25.12187289500445, 0.3557758165877571, 0.0, 1.0, 30.0, 39.82261320553285], 
processed observation next is [0.0, 0.6086956521739131, 0.543859649122807, 0.27, 0.2511111111111111, 0.847145488029466, 0.08333333333333333, 0.5934894079170375, 0.6185919388625857, 0.0, 1.0, 0.3, 0.39822613205532853], 
reward next is 0.6018, 
noisyNet noise sample is [array([0.55040234], dtype=float32), -0.70323145]. 
=============================================
[2019-04-09 14:49:30,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.6690639 ]
 [0.6421385 ]
 [0.61121404]
 [0.42822158]
 [0.61984485]], R is [[1.27118123]
 [1.81587529]
 [2.39539456]
 [2.89933276]
 [3.46818638]].
[2019-04-09 14:49:30,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03954756 0.11539256 0.09656046 0.10675713 0.0798213  0.06231
 0.0970924  0.08813559 0.08299384 0.09621424 0.13517493], sum to 1.0000
[2019-04-09 14:49:30,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7655
[2019-04-09 14:49:30,185] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 19.0, 24.93322928889597, 0.2780455363346079, 0.0, 1.0, 25.0, 32.99808105611431], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 19.0, 24.93564552997482, 0.2831943341306426, 0.0, 1.0, 55.0, 39.04814139249899], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.08333333333333333, 0.577970460831235, 0.5943981113768809, 0.0, 1.0, 0.8, 0.3904814139249899], 
reward next is 0.6095, 
noisyNet noise sample is [array([0.38429585], dtype=float32), 0.93967557]. 
=============================================
[2019-04-09 14:49:30,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0318589  0.10099022 0.0863039  0.07563898 0.05642437 0.03568396
 0.1409115  0.10038648 0.10170465 0.08909314 0.1810039 ], sum to 1.0000
[2019-04-09 14:49:30,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-09 14:49:30,888] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 19.0, 24.35875083322522, 0.1054554497261955, 0.0, 1.0, 45.0, 45.33393722175572], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2516400.0000, 
sim time next is 2517000.0000, 
raw observation next is [-1.7, 44.83333333333334, 0.0, 0.0, 19.0, 24.43303100179904, 0.1199397642597131, 0.0, 1.0, 55.0, 41.05263432622779], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4483333333333334, 0.0, 0.0, 0.08333333333333333, 0.5360859168165867, 0.5399799214199044, 0.0, 1.0, 0.8, 0.4105263432622779], 
reward next is 0.5895, 
noisyNet noise sample is [array([-0.4861755], dtype=float32), 0.4030396]. 
=============================================
[2019-04-09 14:49:30,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.000942  ]
 [0.9886908 ]
 [0.8940963 ]
 [0.8120369 ]
 [0.79781777]], R is [[1.43226743]
 [1.96460533]
 [2.38935995]
 [2.95595026]
 [3.38784719]].
[2019-04-09 14:49:30,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04885521 0.11897749 0.0776386  0.085844   0.08721865 0.06064234
 0.10265906 0.09079361 0.09731598 0.11016907 0.11988605], sum to 1.0000
[2019-04-09 14:49:30,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3575
[2019-04-09 14:49:30,971] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.4, 60.83333333333334, 0.0, 0.0, 19.0, 23.35098963545786, -0.1312043474782129, 0.0, 1.0, 25.0, 34.1076744059528], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2443800.0000, 
sim time next is 2444400.0000, 
raw observation next is [-9.5, 61.0, 0.0, 0.0, 19.0, 23.27183179491383, -0.1545403290773713, 0.0, 1.0, 35.0, 32.45006774395481], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.61, 0.0, 0.0, 0.08333333333333333, 0.4393193162428191, 0.4484865569742096, 0.0, 1.0, 0.4, 0.32450067743954814], 
reward next is 0.6755, 
noisyNet noise sample is [array([1.0788604], dtype=float32), 1.1549804]. 
=============================================
[2019-04-09 14:49:31,129] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03281429 0.09817988 0.10302293 0.08267326 0.05803062 0.0416947
 0.13451272 0.09684534 0.10199742 0.08513182 0.16509697], sum to 1.0000
[2019-04-09 14:49:31,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9238
[2019-04-09 14:49:31,147] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 24.5610480996917, 0.1204628245564179, 0.0, 1.0, 50.0, 34.05467191877482], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2522400.0000, 
sim time next is 2523000.0000, 
raw observation next is [-2.2, 55.66666666666666, 0.0, 0.0, 19.0, 24.58465070311103, 0.1295858700154754, 0.0, 1.0, 65.0, 61.32469123873371], 
processed observation next is [1.0, 0.17391304347826086, 0.4016620498614959, 0.5566666666666665, 0.0, 0.0, 0.08333333333333333, 0.5487208919259192, 0.5431952900051584, 0.0, 1.0, 1.0, 0.6132469123873371], 
reward next is 0.3868, 
noisyNet noise sample is [array([-0.7346822], dtype=float32), -1.4894692]. 
=============================================
[2019-04-09 14:49:31,163] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.83298993]
 [0.77778924]
 [0.7741543 ]
 [0.696517  ]
 [0.8274803 ]], R is [[1.2470758 ]
 [1.89405835]
 [2.3980813 ]
 [2.8728404 ]
 [3.53888988]].
[2019-04-09 14:49:31,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04711457 0.12460823 0.10201157 0.09690113 0.08371536 0.07222169
 0.0979726  0.08509095 0.07986157 0.09770218 0.11280005], sum to 1.0000
[2019-04-09 14:49:31,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6451
[2019-04-09 14:49:31,623] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 19.0, 24.06125670173675, 0.08480337560732672, 0.0, 1.0, 50.0, 34.6020769006592], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 24.03508291186436, 0.08746323849581548, 0.0, 1.0, 50.0, 35.63866954045796], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.08333333333333333, 0.5029235759886967, 0.5291544128319384, 0.0, 1.0, 0.7, 0.3563866954045796], 
reward next is 0.6436, 
noisyNet noise sample is [array([1.972529], dtype=float32), 1.7917135]. 
=============================================
[2019-04-09 14:49:31,635] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.59266055]
 [0.6054369 ]
 [0.65436476]
 [0.65584385]
 [0.47205377]], R is [[1.31892085]
 [1.95971084]
 [2.61698675]
 [3.24641466]
 [3.85249209]].
[2019-04-09 14:49:31,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0395773  0.12984087 0.0817004  0.08777909 0.09437086 0.05829955
 0.13058698 0.08475459 0.07478593 0.08249187 0.13581257], sum to 1.0000
[2019-04-09 14:49:31,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5792
[2019-04-09 14:49:31,754] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 35.0, 0.0, 0.0, 19.0, 25.24962904252641, 0.2715270161035153, 0.0, 1.0, 50.0, 31.2340080409216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2502000.0000, 
sim time next is 2502600.0000, 
raw observation next is [-0.7833333333333333, 35.5, 0.0, 0.0, 19.0, 25.19630001206152, 0.2560006294288226, 0.0, 1.0, 45.0, 29.55525400640328], 
processed observation next is [0.0, 1.0, 0.44090489381348114, 0.355, 0.0, 0.0, 0.08333333333333333, 0.5996916676717934, 0.5853335431429408, 0.0, 1.0, 0.6, 0.2955525400640328], 
reward next is 0.7044, 
noisyNet noise sample is [array([-0.52152485], dtype=float32), 0.50381786]. 
=============================================
[2019-04-09 14:49:32,062] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01679276 0.14439948 0.10728287 0.0943603  0.05110309 0.02663194
 0.0997055  0.08765174 0.13566382 0.07086843 0.1655401 ], sum to 1.0000
[2019-04-09 14:49:32,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7139
[2019-04-09 14:49:32,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 22.5, 26.06207890059244, 0.4271893408181422, 1.0, 1.0, 50.0, 33.15240965637626], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [-0.6, 47.0, 182.5, 58.0, 22.5, 26.1030702267088, 0.4539288014476688, 1.0, 1.0, 55.0, 40.66113908403512], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.47, 0.6083333333333333, 0.06408839779005525, 0.375, 0.6752558522257335, 0.6513096004825563, 1.0, 1.0, 0.8, 0.4066113908403512], 
reward next is 0.5934, 
noisyNet noise sample is [array([-0.1154045], dtype=float32), 0.99759513]. 
=============================================
[2019-04-09 14:49:32,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01898637 0.13506041 0.09083752 0.10430869 0.05270276 0.03558376
 0.11202437 0.07094055 0.14163448 0.06314762 0.17477348], sum to 1.0000
[2019-04-09 14:49:32,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0197
[2019-04-09 14:49:32,340] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.1, 48.66666666666667, 134.0, 41.0, 22.5, 26.83219096133275, 0.554590872972171, 1.0, 1.0, 20.0, 29.02273250813274], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 22.5, 26.8395637775702, 0.5642632191510039, 1.0, 1.0, 20.0, 32.93798941110274], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.375, 0.7366303147975165, 0.6880877397170013, 1.0, 1.0, 0.1, 0.3293798941110274], 
reward next is 0.6706, 
noisyNet noise sample is [array([1.5371591], dtype=float32), 0.3182479]. 
=============================================
[2019-04-09 14:49:32,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04214315 0.08650517 0.09311125 0.07751374 0.06151771 0.05276165
 0.14611466 0.08575691 0.08512031 0.08860693 0.1808484 ], sum to 1.0000
[2019-04-09 14:49:32,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1142
[2019-04-09 14:49:32,603] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 19.0, 25.16803263327986, 0.2582125953892444, 0.0, 1.0, 65.0, 58.97652690398452], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 19.0, 25.10499415058424, 0.2775026675566636, 0.0, 1.0, 20.0, 43.05273019136497], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.08333333333333333, 0.59208284588202, 0.5925008891855545, 0.0, 1.0, 0.1, 0.43052730191364974], 
reward next is 0.5695, 
noisyNet noise sample is [array([-0.33871076], dtype=float32), -0.99140525]. 
=============================================
[2019-04-09 14:49:32,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[0.98742  ]
 [1.0341014]
 [0.9798264]
 [1.0884988]
 [0.8020977]], R is [[1.59253633]
 [1.98684561]
 [2.65749526]
 [3.33195853]
 [3.84979296]].
[2019-04-09 14:49:32,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01684387 0.14084372 0.10161435 0.07319089 0.0482351  0.02851057
 0.11345427 0.08973457 0.1395183  0.08648612 0.16156833], sum to 1.0000
[2019-04-09 14:49:32,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1138
[2019-04-09 14:49:32,890] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.8999999999999999, 48.0, 133.0, 45.0, 22.5, 26.49823150126264, 0.5096058852771495, 1.0, 1.0, 30.0, 27.50254627758759], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2543400.0000, 
sim time next is 2544000.0000, 
raw observation next is [-0.8, 47.66666666666667, 149.5, 49.33333333333333, 22.5, 26.55384858815004, 0.5201017684550281, 1.0, 1.0, 20.0, 29.16093951897181], 
processed observation next is [1.0, 0.43478260869565216, 0.4404432132963989, 0.47666666666666674, 0.49833333333333335, 0.054511970534069976, 0.375, 0.7128207156791699, 0.673367256151676, 1.0, 1.0, 0.1, 0.29160939518971807], 
reward next is 0.7084, 
noisyNet noise sample is [array([1.2600896], dtype=float32), -0.2149783]. 
=============================================
[2019-04-09 14:49:32,906] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.6274567]
 [1.3783145]
 [1.557441 ]
 [1.345361 ]
 [1.4535286]], R is [[2.24036384]
 [2.94293451]
 [3.50627303]
 [3.9336772 ]
 [4.61531878]].
[2019-04-09 14:49:33,024] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03530465 0.09998758 0.08432773 0.07689463 0.05738081 0.04664521
 0.13073902 0.09205829 0.11330076 0.09788356 0.16547772], sum to 1.0000
[2019-04-09 14:49:33,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7899
[2019-04-09 14:49:33,038] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.0, 53.0, 0.0, 0.0, 19.0, 25.43659001037452, 0.2855790917071071, 0.0, 1.0, 25.0, 34.04749650727101], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2521800.0000, 
sim time next is 2522400.0000, 
raw observation next is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 25.38898822766713, 0.2833686010840548, 0.0, 1.0, 60.0, 45.49339452298875], 
processed observation next is [1.0, 0.17391304347826086, 0.404432132963989, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.6157490189722609, 0.5944562003613516, 0.0, 1.0, 0.9, 0.4549339452298875], 
reward next is 0.5451, 
noisyNet noise sample is [array([0.88125336], dtype=float32), 0.56627166]. 
=============================================
[2019-04-09 14:49:33,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01620365 0.13953038 0.08661854 0.08842858 0.05242163 0.03956212
 0.13782465 0.07182176 0.11802778 0.07831563 0.1712452 ], sum to 1.0000
[2019-04-09 14:49:33,067] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0590
[2019-04-09 14:49:33,083] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.333333333333334, 30.0, 28.0, 57.33333333333332, 22.5, 27.63239888233058, 0.8210831250310272, 1.0, 1.0, 25.0, 20.02843675969054], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2567400.0000, 
sim time next is 2568000.0000, 
raw observation next is [1.966666666666667, 31.0, 17.5, 31.16666666666666, 22.5, 27.64842044099184, 0.7897892150144257, 1.0, 1.0, 20.0, 33.58676361176138], 
processed observation next is [1.0, 0.7391304347826086, 0.5170821791320407, 0.31, 0.058333333333333334, 0.03443830570902393, 0.375, 0.8040350367493199, 0.7632630716714752, 1.0, 1.0, 0.1, 0.3358676361176138], 
reward next is 0.6641, 
noisyNet noise sample is [array([-0.9594946], dtype=float32), 1.9398324]. 
=============================================
[2019-04-09 14:49:33,091] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[1.5807431]
 [1.5995474]
 [1.4582856]
 [1.6648302]
 [1.5095713]], R is [[2.25734472]
 [3.03448677]
 [3.70791125]
 [4.2445755 ]
 [4.99010658]].
[2019-04-09 14:49:33,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01817471 0.15508117 0.0845125  0.10775651 0.05599016 0.03294143
 0.09340857 0.06934899 0.15800287 0.06141528 0.16336784], sum to 1.0000
[2019-04-09 14:49:33,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4267
[2019-04-09 14:49:33,152] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.1, 48.66666666666667, 134.0, 41.0, 22.5, 26.91516951911635, 0.5815230396392286, 1.0, 1.0, 25.0, 30.11015159407629], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 22.5, 26.91810607710695, 0.5890265440410445, 1.0, 1.0, 40.0, 30.5466706826701], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.375, 0.7431755064255791, 0.6963421813470149, 1.0, 1.0, 0.5, 0.305466706826701], 
reward next is 0.6945, 
noisyNet noise sample is [array([1.2957454], dtype=float32), -1.1379572]. 
=============================================
[2019-04-09 14:49:33,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01498114 0.13070303 0.09639423 0.07950258 0.0531237  0.0300156
 0.14231887 0.08961413 0.1329     0.07384866 0.15659803], sum to 1.0000
[2019-04-09 14:49:33,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8818
[2019-04-09 14:49:33,378] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.466666666666667, 29.33333333333333, 186.6666666666667, 328.6666666666667, 22.5, 26.92005163973971, 0.6732147957835176, 1.0, 1.0, 45.0, 36.23417719147647], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2553000.0000, 
sim time next is 2553600.0000, 
raw observation next is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 22.5, 26.92650937207708, 0.7061349848129862, 1.0, 1.0, 65.0, 26.8978790322506], 
processed observation next is [1.0, 0.5652173913043478, 0.538319482917821, 0.28666666666666674, 0.5961111111111109, 0.44788213627992646, 0.375, 0.7438757810064235, 0.7353783282709955, 1.0, 1.0, 1.0, 0.268978790322506], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.2281856], dtype=float32), -0.48367065]. 
=============================================
[2019-04-09 14:49:33,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01368518 0.12229533 0.10549293 0.06935509 0.04750921 0.03011738
 0.12478527 0.0896479  0.1364262  0.06760009 0.19308545], sum to 1.0000
[2019-04-09 14:49:33,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7895
[2019-04-09 14:49:33,550] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 22.5, 26.99163580890636, 0.7387240346969285, 1.0, 1.0, 65.0, 35.96949996987325], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2571000.0000, 
sim time next is 2571600.0000, 
raw observation next is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 22.5, 27.02706291739865, 0.7328217268048247, 1.0, 1.0, 60.0, 35.85934315116037], 
processed observation next is [1.0, 0.782608695652174, 0.46629732225300097, 0.35333333333333344, 0.0, 0.0, 0.375, 0.752255243116554, 0.7442739089349416, 1.0, 1.0, 0.9, 0.35859343151160367], 
reward next is 0.6414, 
noisyNet noise sample is [array([0.638281], dtype=float32), 0.10503391]. 
=============================================
[2019-04-09 14:49:33,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02011317 0.118625   0.10771266 0.0765665  0.04608617 0.03332584
 0.13169134 0.08445051 0.09998384 0.0623889  0.21905605], sum to 1.0000
[2019-04-09 14:49:33,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0041
[2019-04-09 14:49:33,807] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 22.5, 26.65563939460524, 0.6863658035263301, 0.0, 1.0, 25.0, 35.69794127205845], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2577600.0000, 
sim time next is 2578200.0000, 
raw observation next is [-1.883333333333333, 46.0, 0.0, 0.0, 19.0, 26.61841022727724, 0.6142230870562652, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4104339796860573, 0.46, 0.0, 0.0, 0.08333333333333333, 0.7182008522731035, 0.7047410290187551, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2056943], dtype=float32), -1.3612891]. 
=============================================
[2019-04-09 14:49:33,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03183788 0.10091897 0.10001651 0.06974901 0.08541366 0.04894295
 0.15253219 0.07883562 0.08082333 0.09170715 0.15922272], sum to 1.0000
[2019-04-09 14:49:33,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4919
[2019-04-09 14:49:33,938] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 19.0, 24.9536061906828, 0.1943563575529732, 0.0, 1.0, 30.0, 26.64182354639734], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2508600.0000, 
sim time next is 2509200.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 19.0, 24.84922023740408, 0.1705146379194268, 0.0, 1.0, 45.0, 25.43737017839741], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.5707683531170066, 0.5568382126398089, 0.0, 1.0, 0.6, 0.25437370178397406], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.12006655], dtype=float32), -0.001792708]. 
=============================================
[2019-04-09 14:49:34,075] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02081549 0.09873798 0.09962694 0.06282867 0.04652658 0.02788521
 0.15468349 0.08398938 0.08979946 0.07153061 0.24357615], sum to 1.0000
[2019-04-09 14:49:34,076] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7594
[2019-04-09 14:49:34,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02258606 0.11308821 0.10429353 0.07071824 0.0530063  0.0363452
 0.13596146 0.08699724 0.11009876 0.07155347 0.19535154], sum to 1.0000
[2019-04-09 14:49:34,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6094
[2019-04-09 14:49:34,098] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.13502308508477, 0.621652907559825, 0.0, 1.0, 65.0, 79.85032451753781], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.08291917260975, 0.6341051035278051, 0.0, 1.0, 65.0, 59.85065061404546], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.08333333333333333, 0.6735765977174791, 0.7113683678426016, 0.0, 1.0, 1.0, 0.5985065061404546], 
reward next is 0.4015, 
noisyNet noise sample is [array([0.21807256], dtype=float32), -0.9479325]. 
=============================================
[2019-04-09 14:49:34,104] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.883333333333333, 46.0, 0.0, 0.0, 19.0, 26.74875970476129, 0.7229738148222871, 0.0, 1.0, 45.0, 36.07768341631822], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2578200.0000, 
sim time next is 2578800.0000, 
raw observation next is [-2.066666666666667, 48.0, 0.0, 0.0, 19.0, 26.71310874509978, 0.7162929925847017, 0.0, 1.0, 50.0, 34.17207109577542], 
processed observation next is [1.0, 0.8695652173913043, 0.40535549399815335, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7260923954249817, 0.7387643308615672, 0.0, 1.0, 0.7, 0.34172071095775425], 
reward next is 0.6583, 
noisyNet noise sample is [array([-0.28998354], dtype=float32), 0.13900535]. 
=============================================
[2019-04-09 14:49:34,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01815847 0.13178343 0.11821482 0.0758973  0.05025616 0.03847719
 0.12760112 0.08518989 0.10966755 0.07183948 0.17291453], sum to 1.0000
[2019-04-09 14:49:34,328] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-09 14:49:34,343] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 22.5, 26.95284424764534, 0.7574375674660164, 1.0, 1.0, 60.0, 38.02328953612892], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [-0.6, 36.0, 0.0, 0.0, 22.5, 26.92291980655472, 0.7584622366225974, 1.0, 1.0, 45.0, 36.07832478237086], 
processed observation next is [1.0, 0.8260869565217391, 0.44598337950138506, 0.36, 0.0, 0.0, 0.375, 0.7435766505462267, 0.7528207455408658, 1.0, 1.0, 0.6, 0.36078324782370863], 
reward next is 0.6392, 
noisyNet noise sample is [array([1.2514538], dtype=float32), 0.618409]. 
=============================================
[2019-04-09 14:49:34,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.6296632]
 [1.6248525]
 [1.6749811]
 [1.6477994]
 [1.6355187]], R is [[2.36004734]
 [2.95621419]
 [3.53646183]
 [4.1333766 ]
 [4.79057932]].
[2019-04-09 14:49:34,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02131034 0.10405962 0.10598742 0.06592907 0.04980459 0.02726373
 0.1579423  0.08016978 0.09194833 0.08743906 0.2081457 ], sum to 1.0000
[2019-04-09 14:49:34,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2012
[2019-04-09 14:49:34,399] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.67734197791113, 0.7073906137120968, 0.0, 1.0, 45.0, 35.53456877906793], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.67567952228924, 0.7022040164984579, 0.0, 1.0, 25.0, 35.22746879264076], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.08333333333333333, 0.7229732935241033, 0.7340680054994859, 0.0, 1.0, 0.2, 0.3522746879264076], 
reward next is 0.6477, 
noisyNet noise sample is [array([0.03213744], dtype=float32), -0.58477503]. 
=============================================
[2019-04-09 14:49:34,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01508935 0.12899344 0.11467307 0.07161158 0.05780393 0.02695497
 0.10686634 0.08105157 0.13380964 0.06109332 0.20205279], sum to 1.0000
[2019-04-09 14:49:34,612] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2930
[2019-04-09 14:49:34,628] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.15, 40.0, 0.0, 0.0, 22.5, 26.75747491274712, 0.6236410468110055, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2575800.0000, 
sim time next is 2576400.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 22.5, 26.47739787741559, 0.6645472666934585, 0.0, 1.0, 30.0, 66.37362279336544], 
processed observation next is [1.0, 0.8260869565217391, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.375, 0.7064498231179659, 0.7215157555644862, 0.0, 1.0, 0.3, 0.6637362279336544], 
reward next is 0.3363, 
noisyNet noise sample is [array([0.8028468], dtype=float32), 2.253957]. 
=============================================
[2019-04-09 14:49:34,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03124694 0.09278303 0.08494309 0.07204847 0.05103268 0.04684021
 0.16582851 0.08140253 0.09410571 0.09050854 0.18926026], sum to 1.0000
[2019-04-09 14:49:34,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1661
[2019-04-09 14:49:34,787] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.01698064039836, 0.4883749380191684, 0.0, 1.0, 65.0, 59.42472197967623], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 25.87903333368791, 0.4794711732678512, 0.0, 1.0, 25.0, 46.35930317535377], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6565861111406592, 0.659823724422617, 0.0, 1.0, 0.2, 0.4635930317535377], 
reward next is 0.5364, 
noisyNet noise sample is [array([0.30579847], dtype=float32), -0.9679694]. 
=============================================
[2019-04-09 14:49:35,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03629982 0.09697201 0.09367265 0.06487502 0.05476793 0.03759551
 0.14763604 0.08454848 0.09546855 0.10546094 0.18270306], sum to 1.0000
[2019-04-09 14:49:35,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5696
[2019-04-09 14:49:35,733] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.2, 75.33333333333334, 0.0, 0.0, 19.0, 25.80879045267815, 0.4516214502534058, 0.0, 1.0, 20.0, 37.33872141661686], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2604000.0000, 
sim time next is 2604600.0000, 
raw observation next is [-5.3, 76.0, 0.0, 0.0, 19.0, 25.73528399563053, 0.4240846646316964, 0.0, 1.0, 50.0, 35.68907898756285], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6446069996358776, 0.6413615548772321, 0.0, 1.0, 0.7, 0.3568907898756285], 
reward next is 0.6431, 
noisyNet noise sample is [array([0.9314075], dtype=float32), 1.1501112]. 
=============================================
[2019-04-09 14:49:35,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01677698 0.1234854  0.11254001 0.07835574 0.0503988  0.02845033
 0.12752877 0.07899515 0.10558999 0.07985641 0.19802253], sum to 1.0000
[2019-04-09 14:49:35,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3087
[2019-04-09 14:49:35,915] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.066666666666667, 48.0, 0.0, 0.0, 19.0, 26.6208814051715, 0.6839653787132168, 0.0, 1.0, 40.0, 37.06530624894916], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2578800.0000, 
sim time next is 2579400.0000, 
raw observation next is [-2.25, 50.0, 0.0, 0.0, 19.0, 26.60595245678218, 0.6809856666919534, 0.0, 1.0, 30.0, 35.60264390978436], 
processed observation next is [1.0, 0.8695652173913043, 0.40027700831024937, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7171627047318484, 0.7269952222306512, 0.0, 1.0, 0.3, 0.3560264390978436], 
reward next is 0.6440, 
noisyNet noise sample is [array([0.05418409], dtype=float32), -0.38102156]. 
=============================================
[2019-04-09 14:49:36,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01589698 0.13274951 0.12027769 0.08851182 0.06433585 0.03374604
 0.10603438 0.06828552 0.12041192 0.07141615 0.17833415], sum to 1.0000
[2019-04-09 14:49:36,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7090
[2019-04-09 14:49:36,328] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.3, 29.0, 99.33333333333334, 288.0, 22.5, 26.41456728969061, 0.7423594902737237, 1.0, 1.0, 65.0, 11.02914828524992], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2562600.0000, 
sim time next is 2563200.0000, 
raw observation next is [3.3, 29.0, 92.0, 256.5, 22.5, 27.50366412624305, 0.8335349928336601, 1.0, 1.0, 55.0, 25.26267093412762], 
processed observation next is [1.0, 0.6956521739130435, 0.554016620498615, 0.29, 0.30666666666666664, 0.28342541436464086, 0.375, 0.791972010520254, 0.77784499761122, 1.0, 1.0, 0.8, 0.2526267093412762], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.10506599], dtype=float32), -0.47122344]. 
=============================================
[2019-04-09 14:49:36,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02204286 0.12393548 0.08900113 0.09809913 0.0644963  0.03709513
 0.10602212 0.0709897  0.13735628 0.07893879 0.17202309], sum to 1.0000
[2019-04-09 14:49:36,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-09 14:49:36,404] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.85, 70.0, 93.0, 91.0, 22.5, 25.77233403789067, 0.4317305529925148, 1.0, 1.0, 55.0, 45.22461769621001], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2626200.0000, 
sim time next is 2626800.0000, 
raw observation next is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 22.5, 25.90535838163083, 0.4487771464973865, 1.0, 1.0, 30.0, 39.94492436070896], 
processed observation next is [1.0, 0.391304347826087, 0.3084025854108957, 0.6833333333333332, 0.3427777777777777, 0.134438305709024, 0.375, 0.6587798651359025, 0.6495923821657955, 1.0, 1.0, 0.3, 0.3994492436070896], 
reward next is 0.6006, 
noisyNet noise sample is [array([0.72033954], dtype=float32), -0.30943188]. 
=============================================
[2019-04-09 14:49:36,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01628322 0.13390543 0.09939564 0.09356724 0.05535432 0.03115215
 0.10883308 0.0728918  0.14797178 0.07791843 0.16272688], sum to 1.0000
[2019-04-09 14:49:36,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9642
[2019-04-09 14:49:36,581] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01454848 0.13445474 0.10317425 0.08091155 0.05275958 0.03091535
 0.1356619  0.07923324 0.12531714 0.06205833 0.18096544], sum to 1.0000
[2019-04-09 14:49:36,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6222
[2019-04-09 14:49:36,586] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.566666666666666, 55.33333333333333, 231.0, 163.0, 22.5, 26.41604279437687, 0.5860916718535261, 1.0, 1.0, 20.0, 30.54539523508194], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [-2.3, 54.0, 234.5, 159.0, 22.5, 26.45681714319394, 0.6027182661691389, 1.0, 1.0, 65.0, 56.05259518198123], 
processed observation next is [1.0, 0.5217391304347826, 0.3988919667590028, 0.54, 0.7816666666666666, 0.17569060773480663, 0.375, 0.7047347619328285, 0.7009060887230464, 1.0, 1.0, 1.0, 0.5605259518198124], 
reward next is 0.4395, 
noisyNet noise sample is [array([1.0908567], dtype=float32), -0.8683933]. 
=============================================
[2019-04-09 14:49:36,605] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.6, 32.0, 0.0, 0.0, 22.5, 26.801998875304, 0.7264741735564924, 1.0, 1.0, 45.0, 38.57756850595635], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2568600.0000, 
sim time next is 2569200.0000, 
raw observation next is [1.233333333333333, 33.0, 0.0, 0.0, 22.5, 26.97927594937298, 0.7855787557060315, 1.0, 1.0, 50.0, 23.25398073866605], 
processed observation next is [1.0, 0.7391304347826086, 0.49676823638042483, 0.33, 0.0, 0.0, 0.375, 0.7482729957810816, 0.7618595852353439, 1.0, 1.0, 0.7, 0.23253980738666052], 
reward next is 0.7675, 
noisyNet noise sample is [array([0.5890294], dtype=float32), 0.08462691]. 
=============================================
[2019-04-09 14:49:37,082] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-09 14:49:37,083] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:49:37,083] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:49:37,083] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:49:37,083] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:49:37,084] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:49:37,084] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:49:37,087] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run8
[2019-04-09 14:49:37,104] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run8
[2019-04-09 14:49:37,120] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run8
[2019-04-09 14:49:37,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02197734 0.12180167 0.11282392 0.08049636 0.04474237 0.04087329
 0.10295723 0.08811316 0.13759932 0.07669878 0.17191668], sum to 1.0000
[2019-04-09 14:49:37,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6549
[2019-04-09 14:49:37,149] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.0, 65.0, 122.5, 183.0, 22.5, 26.2382305127191, 0.5239933892211294, 1.0, 1.0, 35.0, 32.76528692809529], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2628000.0000, 
sim time next is 2628600.0000, 
raw observation next is [-4.816666666666666, 64.5, 132.3333333333333, 213.6666666666667, 22.5, 26.26044905491883, 0.539805072886344, 1.0, 1.0, 55.0, 42.8260564144416], 
processed observation next is [1.0, 0.43478260869565216, 0.32917820867959374, 0.645, 0.44111111111111095, 0.2360957642725599, 0.375, 0.6883707545765692, 0.6799350242954479, 1.0, 1.0, 0.8, 0.42826056414441604], 
reward next is 0.5717, 
noisyNet noise sample is [array([-1.4828448], dtype=float32), 1.8257153]. 
=============================================
[2019-04-09 14:50:15,395] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00866033], dtype=float32), 0.013033661]
[2019-04-09 14:50:15,395] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-7.3, 79.0, 128.0, 392.5, 22.5, 24.4008803140097, 0.08708512379189283, 1.0, 1.0, 20.0, 29.381642111772]
[2019-04-09 14:50:15,395] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:50:15,396] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.03291064 0.1268668  0.107645   0.0956385  0.06607228 0.05381927
 0.09341124 0.0759381  0.11650857 0.0850352  0.14615442], sampled 0.5865578670095578
[2019-04-09 14:50:19,350] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.00866033], dtype=float32), 0.013033661]
[2019-04-09 14:50:19,350] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-1.55, 61.5, 216.0, 252.0, 19.0, 25.6374274089235, 0.5421275171191554, 0.0, 1.0, 65.0, 59.94556762597047]
[2019-04-09 14:50:19,350] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:50:19,351] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.03438939 0.13507129 0.08649506 0.10841446 0.08190893 0.05174745
 0.10805973 0.07882387 0.09931288 0.09103847 0.12473848], sampled 0.38971865460515676
[2019-04-09 14:50:25,771] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00866033], dtype=float32), 0.013033661]
[2019-04-09 14:50:25,771] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [2.016666666666667, 31.5, 202.3333333333333, 175.3333333333333, 22.5, 27.2579162921563, 0.7158285690105335, 1.0, 1.0, 30.0, 31.25692120210501]
[2019-04-09 14:50:25,771] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:50:25,772] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.02037634 0.17438553 0.08643208 0.11095291 0.07462766 0.03311637
 0.10790958 0.06998947 0.11694658 0.08193802 0.1233255 ], sampled 0.8004451596707935
[2019-04-09 14:50:57,583] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5686.0447 280444.5737 2714.1083
[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,610] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:50:57,732] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,462] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5388.4451 310748.6657 1788.9451
[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,483] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:02,598] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,750] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5472.0769 302418.3712 2270.7529
[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:06,888] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:51:07,774] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 70000, evaluation results [70000.0, 5472.076866008205, 302418.37124348554, 2270.7529075639363, 5686.044746111645, 280444.5737446043, 2714.108300266246, 5388.445127636636, 310748.66567193245, 1788.9450915346274]
[2019-04-09 14:51:07,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01813866 0.14501789 0.10314693 0.0864585  0.0487791  0.02988845
 0.11884641 0.06685165 0.1424559  0.07642768 0.1639888 ], sum to 1.0000
[2019-04-09 14:51:07,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-09 14:51:07,866] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 50.0, 88.33333333333333, 137.6666666666667, 22.5, 27.46294018260294, 0.6698460131806123, 1.0, 1.0, 65.0, 71.88650496974434], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2650800.0000, 
sim time next is 2651400.0000, 
raw observation next is [0.5, 50.0, 75.0, 124.0, 22.5, 26.79532410187922, 0.7981484044754676, 1.0, 1.0, 20.0, 36.13330601775989], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.25, 0.13701657458563535, 0.375, 0.7329436751566017, 0.7660494681584892, 1.0, 1.0, 0.1, 0.3613330601775989], 
reward next is 0.6387, 
noisyNet noise sample is [array([-1.7507926], dtype=float32), -0.07539287]. 
=============================================
[2019-04-09 14:51:08,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02003431 0.16410023 0.09338068 0.09316753 0.05954036 0.02991063
 0.10631339 0.0593711  0.15596557 0.07064303 0.14757322], sum to 1.0000
[2019-04-09 14:51:08,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-09 14:51:08,103] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.566666666666666, 55.33333333333333, 231.0, 163.0, 22.5, 26.62004591019107, 0.6560208019615258, 1.0, 1.0, 55.0, 40.21721068610699], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [-2.3, 54.0, 234.5, 159.0, 22.5, 26.71480853910004, 0.666081496062653, 1.0, 1.0, 45.0, 35.54434497564336], 
processed observation next is [1.0, 0.5217391304347826, 0.3988919667590028, 0.54, 0.7816666666666666, 0.17569060773480663, 0.375, 0.7262340449250034, 0.7220271653542176, 1.0, 1.0, 0.6, 0.3554434497564336], 
reward next is 0.6446, 
noisyNet noise sample is [array([1.5720946], dtype=float32), -0.7090295]. 
=============================================
[2019-04-09 14:51:08,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01298758 0.12642382 0.10451879 0.06235115 0.04221818 0.0299772
 0.12440334 0.07277013 0.11732861 0.0790351  0.22798614], sum to 1.0000
[2019-04-09 14:51:08,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3468
[2019-04-09 14:51:08,199] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 54.0, 0.0, 0.0, 22.5, 25.73900625067023, 0.6658227561447859, 1.0, 1.0, 45.0, 26.26580987353157], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2656800.0000, 
sim time next is 2657400.0000, 
raw observation next is [-0.7, 55.0, 0.0, 0.0, 22.5, 26.51592253427557, 0.7091003148677718, 1.0, 1.0, 35.0, 30.69100423668618], 
processed observation next is [1.0, 0.782608695652174, 0.443213296398892, 0.55, 0.0, 0.0, 0.375, 0.709660211189631, 0.7363667716225906, 1.0, 1.0, 0.4, 0.3069100423668618], 
reward next is 0.6931, 
noisyNet noise sample is [array([1.4979104], dtype=float32), 0.31592804]. 
=============================================
[2019-04-09 14:51:08,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01499651 0.14310665 0.09119778 0.08672394 0.04785124 0.02869117
 0.12217572 0.0717755  0.13569134 0.06914281 0.18864733], sum to 1.0000
[2019-04-09 14:51:08,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0959
[2019-04-09 14:51:08,411] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 61.5, 0.0, 0.0, 22.5, 26.52330415105384, 0.6794987913582325, 0.0, 1.0, 50.0, 35.61011152862611], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2662200.0000, 
sim time next is 2662800.0000, 
raw observation next is [-1.2, 62.0, 0.0, 0.0, 22.5, 26.41797166249531, 0.6691115254626645, 1.0, 1.0, 20.0, 31.92615370306448], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.62, 0.0, 0.0, 0.375, 0.701497638541276, 0.7230371751542215, 1.0, 1.0, 0.1, 0.3192615370306448], 
reward next is 0.6807, 
noisyNet noise sample is [array([1.1980201], dtype=float32), 0.093653746]. 
=============================================
[2019-04-09 14:51:08,440] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02364766 0.0954797  0.1039896  0.07157218 0.04466229 0.03104092
 0.14603072 0.07583059 0.11792394 0.09636541 0.19345692], sum to 1.0000
[2019-04-09 14:51:08,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2420
[2019-04-09 14:51:08,465] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 78.33333333333334, 53.66666666666667, 2.666666666666666, 22.5, 25.30959194479112, 0.3519988548311309, 1.0, 1.0, 60.0, 51.00135669521848], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2621400.0000, 
sim time next is 2622000.0000, 
raw observation next is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 22.5, 25.28705215911307, 0.381034941251967, 1.0, 1.0, 30.0, 45.69038400348338], 
processed observation next is [1.0, 0.34782608695652173, 0.26592797783933514, 0.7766666666666667, 0.21777777777777782, 0.00147329650092081, 0.375, 0.6072543465927559, 0.627011647083989, 1.0, 1.0, 0.3, 0.4569038400348338], 
reward next is 0.5431, 
noisyNet noise sample is [array([-0.9778033], dtype=float32), -0.454677]. 
=============================================
[2019-04-09 14:51:08,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.3042321]
 [1.1562891]
 [1.257647 ]
 [1.0966077]
 [1.1229233]], R is [[1.74561679]
 [2.21814704]
 [2.80914688]
 [3.39954495]
 [3.83032107]].
[2019-04-09 14:51:08,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02759257 0.10107089 0.09371573 0.08657762 0.0510272  0.03485595
 0.1226792  0.09009722 0.11633865 0.09172664 0.18431835], sum to 1.0000
[2019-04-09 14:51:08,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6825
[2019-04-09 14:51:08,544] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.299999999999999, 79.0, 30.33333333333333, 5.333333333333334, 22.5, 24.37913073746592, 0.1700854647592571, 1.0, 1.0, 65.0, 64.70442968275448], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2620200.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 42.0, 4.0, 22.5, 24.40607163774532, 0.1866327539722275, 1.0, 1.0, 55.0, 55.26730582092902], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.14, 0.004419889502762431, 0.375, 0.5338393031454434, 0.5622109179907425, 1.0, 1.0, 0.8, 0.5526730582092901], 
reward next is 0.4473, 
noisyNet noise sample is [array([2.5807734], dtype=float32), -0.37173718]. 
=============================================
[2019-04-09 14:51:08,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.029933   0.08594878 0.1055865  0.0678857  0.05540025 0.04490349
 0.1481754  0.09224062 0.08538668 0.09354519 0.19099438], sum to 1.0000
[2019-04-09 14:51:08,699] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9304
[2019-04-09 14:51:08,717] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 19.0, 26.38217030460692, 0.5920546833063423, 0.0, 1.0, 45.0, 46.43932610550315], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.46350567208194, 0.580577575486383, 0.0, 1.0, 65.0, 49.08567444379316], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7052921393401617, 0.693525858495461, 0.0, 1.0, 1.0, 0.4908567444379316], 
reward next is 0.5091, 
noisyNet noise sample is [array([-0.5923578], dtype=float32), -0.16678889]. 
=============================================
[2019-04-09 14:51:08,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0142053  0.11484765 0.091268   0.07381232 0.04580656 0.02362971
 0.11905965 0.06386093 0.1223464  0.06543216 0.26573136], sum to 1.0000
[2019-04-09 14:51:08,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6339
[2019-04-09 14:51:08,799] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.5, 50.0, 88.33333333333333, 137.6666666666667, 22.5, 27.42142923046691, 0.6391773938440494, 1.0, 1.0, 30.0, 54.02700300566431], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2650800.0000, 
sim time next is 2651400.0000, 
raw observation next is [0.5, 50.0, 75.0, 124.0, 22.5, 26.70183707057094, 0.7569499177931752, 1.0, 1.0, 55.0, 42.62669074646491], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.25, 0.13701657458563535, 0.375, 0.7251530892142449, 0.7523166392643917, 1.0, 1.0, 0.8, 0.42626690746464907], 
reward next is 0.5737, 
noisyNet noise sample is [array([-0.13662481], dtype=float32), 0.5369014]. 
=============================================
[2019-04-09 14:51:08,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02187669 0.10284429 0.11301795 0.06960663 0.05678299 0.03673417
 0.14788714 0.08809604 0.08968715 0.09791514 0.17555182], sum to 1.0000
[2019-04-09 14:51:08,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7233
[2019-04-09 14:51:08,932] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.366666666666666, 69.0, 0.0, 0.0, 19.0, 25.53367696367632, 0.4786825613902717, 0.0, 1.0, 55.0, 45.5708115065616], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2673600.0000, 
sim time next is 2674200.0000, 
raw observation next is [-4.683333333333334, 69.0, 0.0, 0.0, 19.0, 25.50868225471501, 0.4694288285357118, 0.0, 1.0, 55.0, 47.37504807561268], 
processed observation next is [1.0, 0.9565217391304348, 0.3328716528162512, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6257235212262507, 0.6564762761785706, 0.0, 1.0, 0.8, 0.4737504807561268], 
reward next is 0.5262, 
noisyNet noise sample is [array([-1.2143985], dtype=float32), -0.39596722]. 
=============================================
[2019-04-09 14:51:09,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01429647 0.13828625 0.10841629 0.07666086 0.06462006 0.02577925
 0.11720206 0.07369323 0.11720816 0.07895146 0.18488595], sum to 1.0000
[2019-04-09 14:51:09,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8659
[2019-04-09 14:51:09,047] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 46.33333333333334, 195.6666666666667, 155.3333333333333, 22.5, 26.07084292711118, 0.6985232526707804, 1.0, 1.0, 30.0, 23.61350745986134], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2645400.0000, 
sim time next is 2646000.0000, 
raw observation next is [0.5, 47.0, 185.5, 168.0, 22.5, 27.18563142466211, 0.7951724713568843, 1.0, 1.0, 65.0, 38.8855545095582], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.47, 0.6183333333333333, 0.1856353591160221, 0.375, 0.7654692853885091, 0.7650574904522948, 1.0, 1.0, 1.0, 0.38885554509558196], 
reward next is 0.6111, 
noisyNet noise sample is [array([-0.8961267], dtype=float32), 0.387302]. 
=============================================
[2019-04-09 14:51:09,050] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02004162 0.10808416 0.0882015  0.05915718 0.05375797 0.02576256
 0.1490897  0.08583079 0.09740356 0.08198665 0.23068437], sum to 1.0000
[2019-04-09 14:51:09,053] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-09 14:51:09,055] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[1.8889385]
 [1.7811255]
 [1.8629588]
 [1.874252 ]
 [1.9499422]], R is [[2.46604085]
 [3.20524549]
 [3.76816559]
 [4.11748552]
 [4.81576109]].
[2019-04-09 14:51:09,080] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.783333333333333, 68.33333333333333, 0.0, 0.0, 19.0, 26.5095421747399, 0.6793628275314975, 0.0, 1.0, 50.0, 37.34422662997334], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2670600.0000, 
sim time next is 2671200.0000, 
raw observation next is [-3.1, 69.0, 0.0, 0.0, 19.0, 26.52852914397149, 0.6702795335437268, 0.0, 1.0, 20.0, 37.12753202372176], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7107107619976242, 0.7234265111812422, 0.0, 1.0, 0.1, 0.37127532023721765], 
reward next is 0.6287, 
noisyNet noise sample is [array([1.0009936], dtype=float32), 2.0856738]. 
=============================================
[2019-04-09 14:51:09,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03264175 0.10854463 0.09184557 0.07211892 0.05031532 0.04074177
 0.14007424 0.07704778 0.12632823 0.09030021 0.17004158], sum to 1.0000
[2019-04-09 14:51:09,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4026
[2019-04-09 14:51:09,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 77.33333333333333, 0.0, 0.0, 19.0, 25.92918339039111, 0.4999194538230542, 0.0, 1.0, 65.0, 52.39127127355012], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2605800.0000, 
sim time next is 2606400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 19.0, 25.94106621677794, 0.4924859343282668, 0.0, 1.0, 45.0, 48.42086944476899], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6617555180648284, 0.6641619781094222, 0.0, 1.0, 0.6, 0.4842086944476899], 
reward next is 0.5158, 
noisyNet noise sample is [array([-0.24444242], dtype=float32), -1.3698444]. 
=============================================
[2019-04-09 14:51:09,235] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0193654  0.08758817 0.10631412 0.06230424 0.04471624 0.03507484
 0.16654652 0.08631331 0.07929098 0.09600587 0.21648031], sum to 1.0000
[2019-04-09 14:51:09,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3518
[2019-04-09 14:51:09,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.683333333333334, 69.0, 0.0, 0.0, 19.0, 25.88917574533869, 0.5642739648085154, 0.0, 1.0, 55.0, 49.30713391072346], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 25.91703854058521, 0.5584749737117848, 0.0, 1.0, 40.0, 40.91727765212482], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6597532117154342, 0.686158324570595, 0.0, 1.0, 0.5, 0.40917277652124817], 
reward next is 0.5908, 
noisyNet noise sample is [array([0.89535886], dtype=float32), 0.5258557]. 
=============================================
[2019-04-09 14:51:09,260] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02343445 0.09015623 0.09356531 0.06461482 0.04606558 0.03162456
 0.16461962 0.08667982 0.08871733 0.08640818 0.2241141 ], sum to 1.0000
[2019-04-09 14:51:09,260] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3781
[2019-04-09 14:51:09,277] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 69.5, 0.0, 0.0, 19.0, 25.96966717610474, 0.5472090613541528, 0.0, 1.0, 35.0, 35.91332012878804], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2675400.0000, 
sim time next is 2676000.0000, 
raw observation next is [-5.666666666666667, 70.0, 0.0, 0.0, 19.0, 25.91061125314077, 0.5245048380786486, 0.0, 1.0, 45.0, 36.65198308017692], 
processed observation next is [1.0, 1.0, 0.30563250230840255, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6592176044283976, 0.6748349460262162, 0.0, 1.0, 0.6, 0.3665198308017692], 
reward next is 0.6335, 
noisyNet noise sample is [array([0.36256948], dtype=float32), -1.1799101]. 
=============================================
[2019-04-09 14:51:09,289] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.3030465]
 [1.347434 ]
 [1.3848658]
 [1.356998 ]
 [1.3927358]], R is [[2.09297299]
 [2.71290994]
 [3.1736474 ]
 [3.4981966 ]
 [4.09600592]].
[2019-04-09 14:51:09,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01382972 0.13070944 0.08371618 0.08641399 0.05968072 0.02634814
 0.11479373 0.07248279 0.15708478 0.07420935 0.18073122], sum to 1.0000
[2019-04-09 14:51:09,507] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2272
[2019-04-09 14:51:09,520] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 43.66666666666667, 198.6666666666667, 157.0, 22.5, 27.12744389555727, 0.7639591462442543, 1.0, 1.0, 55.0, 38.95823699542799], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2643000.0000, 
sim time next is 2643600.0000, 
raw observation next is [0.5, 44.33333333333334, 207.3333333333333, 143.5, 22.5, 27.18430282959388, 0.7716759971163326, 1.0, 1.0, 25.0, 35.20199843736041], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.4433333333333334, 0.6911111111111109, 0.15856353591160222, 0.375, 0.7653585691328232, 0.7572253323721109, 1.0, 1.0, 0.2, 0.3520199843736041], 
reward next is 0.6480, 
noisyNet noise sample is [array([-1.5748881], dtype=float32), 0.14304322]. 
=============================================
[2019-04-09 14:51:09,597] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0268658  0.09069096 0.09392017 0.05785515 0.0515107  0.03410628
 0.15344125 0.09843298 0.09748443 0.09945456 0.19623767], sum to 1.0000
[2019-04-09 14:51:09,598] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5442
[2019-04-09 14:51:09,618] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.0, 69.0, 0.0, 0.0, 19.0, 25.75202468119622, 0.4888192725410406, 0.0, 1.0, 35.0, 38.09029322003157], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2682000.0000, 
sim time next is 2682600.0000, 
raw observation next is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 19.0, 25.7358493172888, 0.4511070754961038, 0.0, 1.0, 50.0, 39.50275749975279], 
processed observation next is [1.0, 0.043478260869565216, 0.20406278855032317, 0.7016666666666667, 0.0, 0.0, 0.08333333333333333, 0.6446541097740667, 0.6503690251653679, 0.0, 1.0, 0.7, 0.3950275749975279], 
reward next is 0.6050, 
noisyNet noise sample is [array([1.0041342], dtype=float32), 0.39721417]. 
=============================================
[2019-04-09 14:51:09,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02757376 0.09558836 0.10178965 0.06512963 0.0491748  0.03950303
 0.13706936 0.08075178 0.11628988 0.08342458 0.20370513], sum to 1.0000
[2019-04-09 14:51:09,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7175
[2019-04-09 14:51:09,861] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-11.95, 79.5, 0.0, 0.0, 19.0, 25.39231546662199, 0.4436947856540059, 0.0, 1.0, 65.0, 63.36894856600768], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2687400.0000, 
sim time next is 2688000.0000, 
raw observation next is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 19.0, 25.45361476541335, 0.4350966375457038, 0.0, 1.0, 60.0, 47.07736500548629], 
processed observation next is [1.0, 0.08695652173913043, 0.12280701754385957, 0.8066666666666665, 0.0, 0.0, 0.08333333333333333, 0.6211345637844458, 0.6450322125152347, 0.0, 1.0, 0.9, 0.47077365005486294], 
reward next is 0.5292, 
noisyNet noise sample is [array([0.11451807], dtype=float32), -0.10456721]. 
=============================================
[2019-04-09 14:51:09,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.0674493]
 [1.106675 ]
 [1.0086849]
 [1.2061803]
 [1.1507764]], R is [[1.63430405]
 [1.98427153]
 [2.09575534]
 [3.07479787]
 [3.47868967]].
[2019-04-09 14:51:10,146] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71258: loss 18.2606
[2019-04-09 14:51:10,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71260: learning rate 0.0000
[2019-04-09 14:51:10,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01014271 0.1340331  0.12242544 0.08228052 0.0549226  0.02585777
 0.12436981 0.0817047  0.11622581 0.07220516 0.17583238], sum to 1.0000
[2019-04-09 14:51:10,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2573
[2019-04-09 14:51:10,171] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 61.5, 0.0, 0.0, 22.5, 26.7110525346855, 0.7148157749593701, 0.0, 1.0, 65.0, 58.6712325621227], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2662200.0000, 
sim time next is 2662800.0000, 
raw observation next is [-1.2, 62.0, 0.0, 0.0, 22.5, 26.63358311251299, 0.715199830717404, 1.0, 1.0, 25.0, 44.798964552993], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.62, 0.0, 0.0, 0.375, 0.7194652593760825, 0.738399943572468, 1.0, 1.0, 0.2, 0.44798964552993], 
reward next is 0.5520, 
noisyNet noise sample is [array([-1.6088935], dtype=float32), 0.98392147]. 
=============================================
[2019-04-09 14:51:10,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01345192 0.15684304 0.10727823 0.06891535 0.04947601 0.02573639
 0.11668243 0.07357839 0.11993694 0.07889596 0.1892053 ], sum to 1.0000
[2019-04-09 14:51:10,199] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8019
[2019-04-09 14:51:10,214] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.2333333333333333, 52.66666666666667, 0.0, 0.0, 22.5, 27.22428738651761, 0.8130573504119291, 1.0, 1.0, 35.0, 31.81052587855662], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2655600.0000, 
sim time next is 2656200.0000, 
raw observation next is [-0.4166666666666667, 53.33333333333333, 0.0, 0.0, 22.5, 27.19127039952636, 0.8066452030132423, 1.0, 1.0, 60.0, 32.36680796357827], 
processed observation next is [1.0, 0.7391304347826086, 0.45106186518928904, 0.5333333333333333, 0.0, 0.0, 0.375, 0.76593919996053, 0.7688817343377474, 1.0, 1.0, 0.9, 0.3236680796357827], 
reward next is 0.6763, 
noisyNet noise sample is [array([-0.42024785], dtype=float32), 1.3526849]. 
=============================================
[2019-04-09 14:51:10,375] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71385: loss 22.2182
[2019-04-09 14:51:10,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71385: learning rate 0.0000
[2019-04-09 14:51:10,416] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71409: loss 20.1783
[2019-04-09 14:51:10,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71409: learning rate 0.0000
[2019-04-09 14:51:10,449] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 71425: loss 21.7532
[2019-04-09 14:51:10,451] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 71427: learning rate 0.0000
[2019-04-09 14:51:10,494] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01315683 0.15447442 0.10006301 0.08914918 0.06192564 0.02018892
 0.12516616 0.07798461 0.12641582 0.08214775 0.1493277 ], sum to 1.0000
[2019-04-09 14:51:10,496] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-09 14:51:10,510] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 54.0, 234.5, 159.0, 22.5, 27.0160202711149, 0.7128029906623955, 1.0, 1.0, 45.0, 30.35878289204843], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2635200.0000, 
sim time next is 2635800.0000, 
raw observation next is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 22.5, 27.03512559328314, 0.7202181509198943, 1.0, 1.0, 30.0, 30.11856628715499], 
processed observation next is [1.0, 0.5217391304347826, 0.4067405355493998, 0.5283333333333334, 0.7933333333333333, 0.1712707182320442, 0.375, 0.752927132773595, 0.7400727169732981, 1.0, 1.0, 0.3, 0.3011856628715499], 
reward next is 0.6988, 
noisyNet noise sample is [array([0.01229569], dtype=float32), -0.542179]. 
=============================================
[2019-04-09 14:51:10,563] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71496: loss 20.7603
[2019-04-09 14:51:10,564] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71496: learning rate 0.0000
[2019-04-09 14:51:10,983] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71724: loss 16.0512
[2019-04-09 14:51:10,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71724: learning rate 0.0000
[2019-04-09 14:51:10,990] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71727: loss 22.1739
[2019-04-09 14:51:10,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71729: learning rate 0.0000
[2019-04-09 14:51:11,044] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71759: loss 21.3360
[2019-04-09 14:51:11,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71760: learning rate 0.0000
[2019-04-09 14:51:11,083] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01948076 0.13968667 0.13137308 0.09183727 0.04911495 0.02783048
 0.09702446 0.07364066 0.13807134 0.08112306 0.1508173 ], sum to 1.0000
[2019-04-09 14:51:11,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5105
[2019-04-09 14:51:11,108] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 76.0, 107.0, 643.0, 22.5, 25.94664562063486, 0.5003474974652664, 1.0, 1.0, 30.0, 40.09604484567569], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2714400.0000, 
sim time next is 2715000.0000, 
raw observation next is [-11.5, 74.0, 110.3333333333333, 653.3333333333333, 22.5, 26.16279255997842, 0.5280672375000199, 1.0, 1.0, 45.0, 38.85235763058176], 
processed observation next is [1.0, 0.43478260869565216, 0.1440443213296399, 0.74, 0.36777777777777765, 0.721915285451197, 0.375, 0.680232713331535, 0.6760224125000066, 1.0, 1.0, 0.6, 0.3885235763058176], 
reward next is 0.6115, 
noisyNet noise sample is [array([-0.68658394], dtype=float32), 0.26054808]. 
=============================================
[2019-04-09 14:51:11,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.4327518]
 [1.4085914]
 [1.3079137]
 [1.1963232]
 [1.2755467]], R is [[2.01933765]
 [2.59818387]
 [3.0601387 ]
 [3.48360753]
 [4.05123234]].
[2019-04-09 14:51:11,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0224358  0.12245154 0.11021249 0.09287281 0.053631   0.03664289
 0.09548383 0.098627   0.12997597 0.07195338 0.16571334], sum to 1.0000
[2019-04-09 14:51:11,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0468
[2019-04-09 14:51:11,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02112474 0.11229686 0.1025648  0.09672126 0.05775193 0.03082371
 0.09350433 0.0758135  0.16277729 0.09567349 0.15094814], sum to 1.0000
[2019-04-09 14:51:11,168] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7285
[2019-04-09 14:51:11,194] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 22.5, 25.69382824668103, 0.4155800838990487, 1.0, 1.0, 20.0, 34.25470236997349], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [-10.5, 70.0, 117.0, 674.0, 22.5, 25.67117373605019, 0.4250076429991294, 1.0, 1.0, 20.0, 31.49166861397051], 
processed observation next is [1.0, 0.43478260869565216, 0.17174515235457063, 0.7, 0.39, 0.7447513812154696, 0.375, 0.6392644780041824, 0.6416692143330431, 1.0, 1.0, 0.1, 0.3149166861397051], 
reward next is 0.6851, 
noisyNet noise sample is [array([0.10056843], dtype=float32), 0.6159245]. 
=============================================
[2019-04-09 14:51:11,196] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-13.0, 83.5, 97.0, 612.0, 22.5, 25.30969398511691, 0.3757690849782627, 1.0, 1.0, 55.0, 41.07284233032724], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2712600.0000, 
sim time next is 2713200.0000, 
raw observation next is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 22.5, 25.41776185618151, 0.3906014657071968, 1.0, 1.0, 55.0, 41.4133231535529], 
processed observation next is [1.0, 0.391304347826087, 0.11172668513388727, 0.81, 0.3344444444444443, 0.6876611418047881, 0.375, 0.6181468213484592, 0.6302004885690656, 1.0, 1.0, 0.8, 0.414133231535529], 
reward next is 0.5859, 
noisyNet noise sample is [array([0.20142834], dtype=float32), -0.2877461]. 
=============================================
[2019-04-09 14:51:11,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02245978 0.12459999 0.10444826 0.09542584 0.0576415  0.03211228
 0.08870284 0.09187435 0.11493431 0.08201058 0.18579032], sum to 1.0000
[2019-04-09 14:51:11,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7028
[2019-04-09 14:51:11,314] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 22.5, 25.55184655029822, 0.5177476827053733, 1.0, 1.0, 45.0, 32.04334715886865], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 22.5, 25.98531142984499, 0.5573272784255002, 1.0, 1.0, 55.0, 36.09984047593102], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 0.375, 0.6654426191537492, 0.6857757594751668, 1.0, 1.0, 0.8, 0.36099840475931017], 
reward next is 0.6390, 
noisyNet noise sample is [array([0.01031605], dtype=float32), 0.6169222]. 
=============================================
[2019-04-09 14:51:11,368] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71928: loss 21.6510
[2019-04-09 14:51:11,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71928: learning rate 0.0000
[2019-04-09 14:51:11,386] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71938: loss 27.8382
[2019-04-09 14:51:11,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71939: learning rate 0.0000
[2019-04-09 14:51:11,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02440479 0.0820107  0.09861434 0.05607533 0.0534078  0.03260621
 0.1511402  0.09368905 0.09498165 0.09435914 0.21871078], sum to 1.0000
[2019-04-09 14:51:11,407] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7701
[2019-04-09 14:51:11,433] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.333333333333334, 70.0, 0.0, 0.0, 19.0, 25.38622833534568, 0.4166325888648288, 0.0, 1.0, 45.0, 63.64326996833836], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2680800.0000, 
sim time next is 2681400.0000, 
raw observation next is [-8.666666666666666, 69.5, 0.0, 0.0, 19.0, 25.27517798175234, 0.4187434945109589, 0.0, 1.0, 60.0, 53.38853374676562], 
processed observation next is [1.0, 0.0, 0.22253000923361038, 0.695, 0.0, 0.0, 0.08333333333333333, 0.606264831812695, 0.6395811648369863, 0.0, 1.0, 0.9, 0.5338853374676562], 
reward next is 0.4661, 
noisyNet noise sample is [array([0.3606457], dtype=float32), 0.30531752]. 
=============================================
[2019-04-09 14:51:11,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02313047 0.09639221 0.09728789 0.059147   0.04945571 0.03306429
 0.16990313 0.08958244 0.07570035 0.09920704 0.20712952], sum to 1.0000
[2019-04-09 14:51:11,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4664
[2019-04-09 14:51:11,486] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 71.5, 0.0, 0.0, 19.0, 25.8174531875104, 0.5255819314111007, 0.0, 1.0, 40.0, 38.24428113109013], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2677800.0000, 
sim time next is 2678400.0000, 
raw observation next is [-7.0, 72.0, 0.0, 0.0, 19.0, 25.77652914804025, 0.5078685653061444, 0.0, 1.0, 45.0, 38.77758329156631], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6480440956700209, 0.6692895217687148, 0.0, 1.0, 0.6, 0.3877758329156631], 
reward next is 0.6122, 
noisyNet noise sample is [array([0.29838476], dtype=float32), -2.003731]. 
=============================================
[2019-04-09 14:51:11,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01451652 0.1530466  0.0985939  0.08730806 0.04541039 0.03072541
 0.10464686 0.08102506 0.14177136 0.07245716 0.17049871], sum to 1.0000
[2019-04-09 14:51:11,527] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6667
[2019-04-09 14:51:11,542] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01593731 0.1253135  0.12708732 0.08426882 0.04137812 0.0300515
 0.11237575 0.0753417  0.15249857 0.06351791 0.17222954], sum to 1.0000
[2019-04-09 14:51:11,547] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-09 14:51:11,549] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 22.5, 25.70800837236213, 0.176689592161353, 1.0, 1.0, 25.0, 30.97705935668926], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 22.5, 25.80138625441101, 0.4929247404498208, 1.0, 1.0, 55.0, 42.18968546863131], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.375, 0.6501155212009175, 0.6643082468166069, 1.0, 1.0, 0.8, 0.4218968546863131], 
reward next is 0.5781, 
noisyNet noise sample is [array([1.0808604], dtype=float32), -0.02770046]. 
=============================================
[2019-04-09 14:51:11,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02546692 0.08922222 0.09206962 0.05337554 0.03752271 0.03385094
 0.17224322 0.07163046 0.10710628 0.11555172 0.20196043], sum to 1.0000
[2019-04-09 14:51:11,564] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2329
[2019-04-09 14:51:11,574] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 22.5, 26.26197568481928, 0.6273260677194263, 1.0, 1.0, 65.0, 49.36878748848365], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 22.5, 25.96660661502223, 0.6015991719523135, 1.0, 1.0, 55.0, 25.66569337709369], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.375, 0.6638838845851858, 0.7005330573174379, 1.0, 1.0, 0.8, 0.2566569337709369], 
reward next is 0.7433, 
noisyNet noise sample is [array([-1.9003178], dtype=float32), -0.31062073]. 
=============================================
[2019-04-09 14:51:11,576] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 19.0, 23.93623481442508, 0.1093286951425029, 0.0, 1.0, 55.0, 39.44013232111703], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 19.0, 23.94739624036433, 0.0918181261070783, 0.0, 1.0, 30.0, 38.02455093807176], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4956163533636942, 0.5306060420356927, 0.0, 1.0, 0.3, 0.3802455093807176], 
reward next is 0.6198, 
noisyNet noise sample is [array([-1.7579068], dtype=float32), -0.64715636]. 
=============================================
[2019-04-09 14:51:11,664] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72080: loss 18.5310
[2019-04-09 14:51:11,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72080: learning rate 0.0000
[2019-04-09 14:51:11,665] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01682796 0.10481762 0.14478017 0.08096434 0.05254911 0.03557976
 0.10119538 0.07903313 0.13191241 0.07677694 0.17556305], sum to 1.0000
[2019-04-09 14:51:11,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7096
[2019-04-09 14:51:11,692] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 22.5, 26.0091682980605, 0.2358295729185918, 1.0, 1.0, 45.0, 28.131624820639], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [-6.333333333333333, 59.83333333333334, 111.6666666666667, 795.3333333333334, 22.5, 25.94807507157039, 0.5415751247036947, 1.0, 1.0, 55.0, 43.2012916921045], 
processed observation next is [1.0, 0.5217391304347826, 0.28716528162511545, 0.5983333333333334, 0.37222222222222234, 0.8788213627992634, 0.375, 0.6623395892975324, 0.6805250415678983, 1.0, 1.0, 0.8, 0.432012916921045], 
reward next is 0.5680, 
noisyNet noise sample is [array([0.19551535], dtype=float32), -0.38124368]. 
=============================================
[2019-04-09 14:51:11,911] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01679545 0.12379318 0.12519363 0.08353182 0.06046354 0.03381589
 0.11137145 0.09215009 0.11561453 0.06908526 0.16818511], sum to 1.0000
[2019-04-09 14:51:11,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9920
[2019-04-09 14:51:11,933] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 22.5, 25.71381973707775, 0.6353135582102105, 1.0, 1.0, 55.0, 20.28914495794436], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 22.5, 26.59117114169549, 0.7019639479021246, 1.0, 1.0, 65.0, 26.79580319934213], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 0.375, 0.7159309284746241, 0.7339879826340415, 1.0, 1.0, 1.0, 0.26795803199342133], 
reward next is 0.7320, 
noisyNet noise sample is [array([0.34850156], dtype=float32), -0.60156333]. 
=============================================
[2019-04-09 14:51:11,992] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72244: loss 17.8208
[2019-04-09 14:51:11,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72247: learning rate 0.0000
[2019-04-09 14:51:12,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03290777 0.09251387 0.10018186 0.07102884 0.05161012 0.04184547
 0.14436488 0.08687542 0.10650863 0.09431433 0.17784879], sum to 1.0000
[2019-04-09 14:51:12,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2666
[2019-04-09 14:51:12,066] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.6, 85.66666666666667, 0.0, 0.0, 19.0, 24.37181163647441, 0.2000166747168838, 0.0, 1.0, 30.0, 33.87089593597694], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2690400.0000, 
sim time next is 2691000.0000, 
raw observation next is [-13.95, 87.0, 0.0, 0.0, 19.0, 24.29621565270792, 0.1698994788982691, 0.0, 1.0, 30.0, 32.16896087124198], 
processed observation next is [1.0, 0.13043478260869565, 0.07617728531855956, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5246846377256601, 0.5566331596327564, 0.0, 1.0, 0.3, 0.3216896087124198], 
reward next is 0.6783, 
noisyNet noise sample is [array([1.0843639], dtype=float32), 0.18489556]. 
=============================================
[2019-04-09 14:51:12,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[0.9193258]
 [1.0649319]
 [1.1568109]
 [1.0481592]
 [1.1089698]], R is [[1.68449795]
 [2.32894397]
 [2.9513154 ]
 [3.57504106]
 [4.04220724]].
[2019-04-09 14:51:12,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01826148 0.12992387 0.12940422 0.06679147 0.04597578 0.03751739
 0.09516265 0.06839032 0.09731226 0.0789824  0.2322782 ], sum to 1.0000
[2019-04-09 14:51:12,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2588
[2019-04-09 14:51:12,252] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72370: loss 25.4680
[2019-04-09 14:51:12,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72370: learning rate 0.0000
[2019-04-09 14:51:12,263] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.5, 52.0, 86.0, 614.0, 22.5, 26.66465959755777, 0.8002598342788009, 1.0, 1.0, 50.0, 31.34833688188467], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2734200.0000, 
sim time next is 2734800.0000, 
raw observation next is [-3.333333333333333, 51.33333333333333, 80.66666666666667, 587.3333333333334, 22.5, 25.89671637200493, 0.7134706419915865, 1.0, 1.0, 25.0, 18.24253517405888], 
processed observation next is [1.0, 0.6521739130434783, 0.37026777469990774, 0.5133333333333333, 0.2688888888888889, 0.648987108655617, 0.375, 0.6580596976670776, 0.7378235473305289, 1.0, 1.0, 0.2, 0.1824253517405888], 
reward next is 0.8176, 
noisyNet noise sample is [array([-0.594284], dtype=float32), -0.73841536]. 
=============================================
[2019-04-09 14:51:12,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01928247 0.10263074 0.10698768 0.06939697 0.04444362 0.02858884
 0.15877134 0.0925674  0.08947761 0.08609925 0.20175406], sum to 1.0000
[2019-04-09 14:51:12,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9822
[2019-04-09 14:51:12,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 69.0, 0.0, 0.0, 19.0, 26.26557011044959, 0.6674592529889217, 0.0, 1.0, 60.0, 49.14622283700528], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2671200.0000, 
sim time next is 2671800.0000, 
raw observation next is [-3.416666666666667, 69.0, 0.0, 0.0, 19.0, 26.30150706334737, 0.6316391121763174, 0.0, 1.0, 65.0, 58.94505840756902], 
processed observation next is [1.0, 0.9565217391304348, 0.36795937211449675, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6917922552789474, 0.7105463707254391, 0.0, 1.0, 1.0, 0.5894505840756902], 
reward next is 0.4105, 
noisyNet noise sample is [array([-0.4431446], dtype=float32), -0.20966317]. 
=============================================
[2019-04-09 14:51:12,566] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72506: loss 24.0108
[2019-04-09 14:51:12,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72506: learning rate 0.0000
[2019-04-09 14:51:12,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01547385 0.14206435 0.12149725 0.08920249 0.05250242 0.031201
 0.08751427 0.07628435 0.15817674 0.07988514 0.14619811], sum to 1.0000
[2019-04-09 14:51:12,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1904
[2019-04-09 14:51:12,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-10.5, 70.0, 117.0, 674.0, 22.5, 25.79853145031568, 0.4778807982919875, 1.0, 1.0, 25.0, 42.16529095247461], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2716200.0000, 
sim time next is 2716800.0000, 
raw observation next is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 22.5, 25.96120272244056, 0.1802917006950472, 1.0, 1.0, 25.0, 46.30114640453571], 
processed observation next is [1.0, 0.43478260869565216, 0.18559556786703602, 0.68, 0.38722222222222236, 0.7644567219152855, 0.375, 0.6634335602033801, 0.5600972335650157, 1.0, 1.0, 0.2, 0.4630114640453571], 
reward next is 0.5370, 
noisyNet noise sample is [array([-0.38572696], dtype=float32), 1.3412228]. 
=============================================
[2019-04-09 14:51:12,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02502019 0.08939255 0.10334456 0.06847535 0.05133203 0.03731476
 0.14808166 0.06957538 0.10976975 0.10253854 0.19515525], sum to 1.0000
[2019-04-09 14:51:12,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4867
[2019-04-09 14:51:12,739] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.33333333333333, 83.0, 0.0, 0.0, 19.0, 23.71004175543619, 0.05008324276181799, 0.0, 1.0, 35.0, 38.16092103145495], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2702400.0000, 
sim time next is 2703000.0000, 
raw observation next is [-15.16666666666667, 83.0, 0.0, 0.0, 19.0, 23.71707694949921, 0.03298477803497458, 0.0, 1.0, 45.0, 38.65095219806419], 
processed observation next is [1.0, 0.2608695652173913, 0.04247460757156039, 0.83, 0.0, 0.0, 0.08333333333333333, 0.47642307912493403, 0.5109949260116582, 0.0, 1.0, 0.6, 0.38650952198064187], 
reward next is 0.6135, 
noisyNet noise sample is [array([0.36330202], dtype=float32), 0.11071174]. 
=============================================
[2019-04-09 14:51:12,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.1391965 ]
 [1.0482457 ]
 [0.91151404]
 [0.99107003]
 [1.0644684 ]], R is [[1.64609599]
 [2.24802589]
 [2.69380331]
 [2.99345994]
 [3.30298519]].
[2019-04-09 14:51:12,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01921603 0.1270999  0.11569773 0.08726164 0.05067297 0.02817409
 0.08238342 0.07312039 0.1379591  0.0903655  0.18804915], sum to 1.0000
[2019-04-09 14:51:12,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4082
[2019-04-09 14:51:12,807] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 22.5, 25.82958841912144, 0.4729155168882084, 1.0, 1.0, 50.0, 32.96136459005907], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2719200.0000, 
sim time next is 2719800.0000, 
raw observation next is [-8.5, 64.0, 112.0, 781.0, 22.5, 25.38316847189584, 0.4426019219969994, 1.0, 1.0, 40.0, 29.86971746161605], 
processed observation next is [1.0, 0.4782608695652174, 0.22714681440443216, 0.64, 0.37333333333333335, 0.8629834254143647, 0.375, 0.6152640393246532, 0.6475339739989998, 1.0, 1.0, 0.5, 0.2986971746161605], 
reward next is 0.7013, 
noisyNet noise sample is [array([0.69875044], dtype=float32), 0.8093838]. 
=============================================
[2019-04-09 14:51:13,002] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02000129 0.10499667 0.12328348 0.08618888 0.06181003 0.03220204
 0.10912392 0.07500209 0.12935635 0.10210001 0.1559353 ], sum to 1.0000
[2019-04-09 14:51:13,006] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0331
[2019-04-09 14:51:13,025] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 22.5, 25.13244001223974, 0.336855026817031, 1.0, 1.0, 25.0, 36.13891859919856], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2713200.0000, 
sim time next is 2713800.0000, 
raw observation next is [-12.33333333333333, 78.5, 103.6666666666667, 632.6666666666667, 22.5, 25.32866259359988, 0.3655220570902516, 1.0, 1.0, 20.0, 34.4906199583422], 
processed observation next is [1.0, 0.391304347826087, 0.12096029547553101, 0.785, 0.34555555555555567, 0.6990791896869246, 0.375, 0.61072188279999, 0.6218406856967506, 1.0, 1.0, 0.1, 0.344906199583422], 
reward next is 0.6551, 
noisyNet noise sample is [array([-0.11273663], dtype=float32), 1.2543534]. 
=============================================
[2019-04-09 14:51:13,030] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01439613 0.1226525  0.10071831 0.08327648 0.04983274 0.04199751
 0.12505916 0.06947002 0.1095829  0.08361252 0.19940172], sum to 1.0000
[2019-04-09 14:51:13,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4340
[2019-04-09 14:51:13,059] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 51.33333333333333, 80.66666666666667, 587.3333333333334, 22.5, 27.34917120246596, 0.7920394062015809, 1.0, 1.0, 65.0, 23.40526758982929], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2734800.0000, 
sim time next is 2735400.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 75.33333333333334, 560.6666666666666, 22.5, 27.35753187266029, 0.7861272884387845, 1.0, 1.0, 45.0, 44.2360525312093], 
processed observation next is [1.0, 0.6521739130434783, 0.3748845798707295, 0.5066666666666667, 0.2511111111111111, 0.6195211786372007, 0.375, 0.7797943227216907, 0.7620424294795948, 1.0, 1.0, 0.6, 0.442360525312093], 
reward next is 0.5576, 
noisyNet noise sample is [array([0.6089032], dtype=float32), 0.17557883]. 
=============================================
[2019-04-09 14:51:13,648] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 73020: loss 19.2499
[2019-04-09 14:51:13,649] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 73022: learning rate 0.0000
[2019-04-09 14:51:13,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01390928 0.13217916 0.09514982 0.074739   0.05900373 0.03265267
 0.13010344 0.07316079 0.10703142 0.07206241 0.2100082 ], sum to 1.0000
[2019-04-09 14:51:13,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8542
[2019-04-09 14:51:13,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03031885 0.09551951 0.09134586 0.07089861 0.05037167 0.04145894
 0.14171107 0.07254602 0.11853354 0.09669681 0.1905991 ], sum to 1.0000
[2019-04-09 14:51:13,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2709
[2019-04-09 14:51:13,683] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 58.16666666666667, 0.0, 0.0, 22.5, 26.54835967322059, 0.692797641378732, 1.0, 1.0, 65.0, 64.4808802418233], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2746200.0000, 
sim time next is 2746800.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.59201973075144, 0.6850342796568799, 1.0, 1.0, 65.0, 63.43991366912611], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.7160016442292866, 0.7283447598856266, 1.0, 1.0, 1.0, 0.6343991366912611], 
reward next is 0.3656, 
noisyNet noise sample is [array([0.29720372], dtype=float32), 0.7085489]. 
=============================================
[2019-04-09 14:51:13,685] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 19.0, 24.02971686395671, 0.1061030612432429, 0.0, 1.0, 40.0, 29.87409979574752], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 19.0, 23.93455125196427, 0.07496396756408312, 0.0, 1.0, 30.0, 28.43130422598444], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4945459376636891, 0.5249879891880277, 0.0, 1.0, 0.3, 0.2843130422598444], 
reward next is 0.7157, 
noisyNet noise sample is [array([-0.4829672], dtype=float32), 0.9105584]. 
=============================================
[2019-04-09 14:51:13,749] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 73077: loss 33.2234
[2019-04-09 14:51:13,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 73077: learning rate 0.0000
[2019-04-09 14:51:13,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02113141 0.09364464 0.08714841 0.06009643 0.05360407 0.03418575
 0.16681452 0.08035891 0.08688356 0.09097998 0.22515231], sum to 1.0000
[2019-04-09 14:51:13,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4902
[2019-04-09 14:51:13,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01693459 0.11369973 0.1183129  0.06946203 0.04502242 0.03768291
 0.09651612 0.08177213 0.13166608 0.07971149 0.20921965], sum to 1.0000
[2019-04-09 14:51:13,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0081
[2019-04-09 14:51:13,885] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 22.5, 26.49310711482666, 0.7655499664728507, 1.0, 1.0, 25.0, 29.31633952440292], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2733600.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 22.5, 26.06587800422323, 0.6727273124649239, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.375, 0.6721565003519357, 0.724242437488308, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33360767], dtype=float32), 0.12501368]. 
=============================================
[2019-04-09 14:51:13,888] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.7079031963565, 0.4897191859691274, 0.0, 1.0, 60.0, 52.09153984272179], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.64456859909359, 0.4800992910951499, 0.0, 1.0, 45.0, 47.18743411519672], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6370473832577991, 0.6600330970317166, 0.0, 1.0, 0.6, 0.47187434115196725], 
reward next is 0.5281, 
noisyNet noise sample is [array([-0.9489171], dtype=float32), -1.0152906]. 
=============================================
[2019-04-09 14:51:14,007] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01673857 0.11119107 0.10568239 0.06821717 0.04773994 0.02991756
 0.13527498 0.0848482  0.10611919 0.06561125 0.22865975], sum to 1.0000
[2019-04-09 14:51:14,011] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6816
[2019-04-09 14:51:14,045] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 19.0, 26.14024509605526, 0.6132081407253288, 0.0, 1.0, 45.0, 46.16095283827499], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [-6.0, 64.0, 0.0, 0.0, 19.0, 26.08053348810067, 0.6037429049003699, 0.0, 1.0, 55.0, 52.30527887967588], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6733777906750559, 0.7012476349667899, 0.0, 1.0, 0.8, 0.5230527887967589], 
reward next is 0.4769, 
noisyNet noise sample is [array([0.5211197], dtype=float32), -0.65150756]. 
=============================================
[2019-04-09 14:51:14,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[1.834722 ]
 [1.8956192]
 [1.9399037]
 [1.8065867]
 [2.0338385]], R is [[2.25124884]
 [2.7671268 ]
 [3.20043993]
 [3.71720338]
 [4.09376669]].
[2019-04-09 14:51:14,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01308578 0.1288254  0.10692704 0.08354197 0.04209712 0.0392886
 0.10769603 0.0778309  0.13919808 0.06884369 0.1926655 ], sum to 1.0000
[2019-04-09 14:51:14,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8355
[2019-04-09 14:51:14,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02363473 0.08438757 0.10657796 0.05485864 0.05197556 0.03114445
 0.13911235 0.09490035 0.10012322 0.10252977 0.21075545], sum to 1.0000
[2019-04-09 14:51:14,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5060
[2019-04-09 14:51:14,111] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.18670427126192, 0.3725840093231448, 0.0, 1.0, 35.0, 32.834515630858], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2766600.0000, 
sim time next is 2767200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.15276841629761, 0.3688721300837472, 0.0, 1.0, 50.0, 37.07711620868912], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5960640346914676, 0.6229573766945824, 0.0, 1.0, 0.7, 0.3707711620868912], 
reward next is 0.6292, 
noisyNet noise sample is [array([-0.40254024], dtype=float32), 1.6819037]. 
=============================================
[2019-04-09 14:51:14,115] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 59.33333333333333, 480.6666666666667, 22.5, 27.20999557811479, 0.4805248522329734, 1.0, 1.0, 45.0, 62.8355501826458], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2737200.0000, 
sim time next is 2737800.0000, 
raw observation next is [-3.0, 50.0, 54.0, 454.0, 22.5, 27.10879146863887, 0.764497533911757, 1.0, 1.0, 65.0, 75.38922221067749], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.18, 0.5016574585635359, 0.375, 0.7590659557199059, 0.754832511303919, 1.0, 1.0, 1.0, 0.7538922221067749], 
reward next is 0.2461, 
noisyNet noise sample is [array([-0.30222458], dtype=float32), 0.15358435]. 
=============================================
[2019-04-09 14:51:14,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01233301 0.10364114 0.0983706  0.06720855 0.04943703 0.02119223
 0.16530778 0.08088206 0.11162853 0.06238431 0.22761476], sum to 1.0000
[2019-04-09 14:51:14,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2038
[2019-04-09 14:51:14,199] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.71294939787371, 0.5190474166171232, 0.0, 1.0, 55.0, 61.00627734005764], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2749200.0000, 
sim time next is 2749800.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.63533852504158, 0.5202178465191418, 1.0, 1.0, 25.0, 53.71863982514322], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.6362782104201316, 0.6734059488397138, 1.0, 1.0, 0.2, 0.5371863982514322], 
reward next is 0.4628, 
noisyNet noise sample is [array([-0.00680012], dtype=float32), 0.19869193]. 
=============================================
[2019-04-09 14:51:14,365] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03154203 0.09717954 0.09511232 0.07506587 0.05201225 0.04697515
 0.14354128 0.07855091 0.10230552 0.07884291 0.19887227], sum to 1.0000
[2019-04-09 14:51:14,367] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4811
[2019-04-09 14:51:14,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.70892376177902, 0.426129643640324, 0.0, 1.0, 20.0, 38.96218067783279], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.63030170164389, 0.4157444701759203, 0.0, 1.0, 45.0, 36.50138336593686], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6358584751369909, 0.6385814900586401, 0.0, 1.0, 0.6, 0.3650138336593686], 
reward next is 0.6350, 
noisyNet noise sample is [array([0.46240252], dtype=float32), 1.5485504]. 
=============================================
[2019-04-09 14:51:14,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02347408 0.09489521 0.09962237 0.07197366 0.049576   0.0311712
 0.17004295 0.08512817 0.09174823 0.09610843 0.18625979], sum to 1.0000
[2019-04-09 14:51:14,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6911
[2019-04-09 14:51:14,449] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.88568804763474, 0.5037894075970112, 0.0, 1.0, 55.0, 46.51025807966031], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2758800.0000, 
sim time next is 2759400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.77615602488883, 0.4825941293731289, 0.0, 1.0, 55.0, 48.24949689452432], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6480130020740692, 0.660864709791043, 0.0, 1.0, 0.8, 0.4824949689452432], 
reward next is 0.5175, 
noisyNet noise sample is [array([0.49067712], dtype=float32), -0.9349795]. 
=============================================
[2019-04-09 14:51:14,754] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01484575 0.11532423 0.1124062  0.0708506  0.0386667  0.028721
 0.11587701 0.06162091 0.13290544 0.06605662 0.2427256 ], sum to 1.0000
[2019-04-09 14:51:14,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-09 14:51:14,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.34589639560743, 0.6223755126739526, 0.0, 1.0, 20.0, 53.31642763104615], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2748000.0000, 
sim time next is 2748600.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.27089342142859, 0.6160174633463613, 1.0, 1.0, 65.0, 75.40623986061439], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.6892411184523825, 0.7053391544487871, 1.0, 1.0, 1.0, 0.7540623986061439], 
reward next is 0.2459, 
noisyNet noise sample is [array([-0.00765982], dtype=float32), 1.262096]. 
=============================================
[2019-04-09 14:51:14,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02949611 0.0891799  0.09969688 0.059597   0.04856047 0.04964925
 0.14683695 0.08982354 0.07454702 0.0879543  0.22465867], sum to 1.0000
[2019-04-09 14:51:14,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-09 14:51:14,905] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.72874532765071, 0.3702815319806587, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2770800.0000, 
sim time next is 2771400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.50199529460204, 0.3770948546969488, 0.0, 1.0, 30.0, 54.92667345730729], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6251662745501699, 0.6256982848989829, 0.0, 1.0, 0.3, 0.5492667345730728], 
reward next is 0.4507, 
noisyNet noise sample is [array([0.28577545], dtype=float32), 0.895629]. 
=============================================
[2019-04-09 14:51:15,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02836391 0.12219077 0.09628492 0.07492076 0.05370495 0.0339227
 0.1351427  0.07894757 0.10534681 0.09581264 0.1753622 ], sum to 1.0000
[2019-04-09 14:51:15,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2841
[2019-04-09 14:51:15,363] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.32201399701453, 0.3434708699566031, 0.0, 1.0, 60.0, 47.93381954333321], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2776200.0000, 
sim time next is 2776800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.25318121293103, 0.3364255381487177, 0.0, 1.0, 30.0, 43.65522794467125], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6044317677442524, 0.6121418460495726, 0.0, 1.0, 0.3, 0.43655227944671254], 
reward next is 0.5634, 
noisyNet noise sample is [array([-2.9053285], dtype=float32), 0.24107291]. 
=============================================
[2019-04-09 14:51:15,733] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02067462 0.09856087 0.08960762 0.0538045  0.04281443 0.03018344
 0.15050976 0.0873089  0.09373499 0.10711553 0.2256853 ], sum to 1.0000
[2019-04-09 14:51:15,737] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-09 14:51:15,756] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.96446151305197, 0.3631711989859972, 0.0, 1.0, 25.0, 34.112393485056], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2762400.0000, 
sim time next is 2763000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.9627165528646, 0.3471374473603207, 0.0, 1.0, 40.0, 34.80734837284644], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5802263794053832, 0.6157124824534402, 0.0, 1.0, 0.5, 0.3480734837284644], 
reward next is 0.6519, 
noisyNet noise sample is [array([-0.09906172], dtype=float32), 1.8567617]. 
=============================================
[2019-04-09 14:51:15,766] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.5158873]
 [1.3350116]
 [1.5506233]
 [1.3921324]
 [1.4957751]], R is [[1.99084496]
 [2.62981272]
 [3.14584684]
 [3.33854985]
 [4.30516434]].
[2019-04-09 14:51:15,992] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02118711 0.0850743  0.10330116 0.0656301  0.0523177  0.03735364
 0.15631981 0.07160101 0.09089498 0.09206052 0.22425964], sum to 1.0000
[2019-04-09 14:51:15,994] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6549
[2019-04-09 14:51:16,004] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.78673175160661, 0.3661187786047433, 0.0, 1.0, 65.0, 62.29265657053091], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2767200.0000, 
sim time next is 2767800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.00876793093206, 0.3866436584615738, 0.0, 1.0, 55.0, 49.02983845593294], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5840639942443383, 0.6288812194871912, 0.0, 1.0, 0.8, 0.4902983845593294], 
reward next is 0.5097, 
noisyNet noise sample is [array([-0.11927527], dtype=float32), -0.3069416]. 
=============================================
[2019-04-09 14:51:16,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02826199 0.09958472 0.08897249 0.06977379 0.04583003 0.03490404
 0.13305715 0.08236603 0.10865429 0.10716357 0.20143196], sum to 1.0000
[2019-04-09 14:51:16,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-09 14:51:16,250] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 24.51664389394166, 0.1505233214242921, 0.0, 1.0, 20.0, 50.65847057383253], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2788800.0000, 
sim time next is 2789400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 24.40624236440356, 0.1066711455216179, 0.0, 1.0, 50.0, 57.27483393310457], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.5338535303669634, 0.535557048507206, 0.0, 1.0, 0.7, 0.5727483393310456], 
reward next is 0.4273, 
noisyNet noise sample is [array([0.54031163], dtype=float32), 0.16257109]. 
=============================================
[2019-04-09 14:51:16,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01599131 0.1320964  0.13017903 0.08100038 0.0555843  0.02886913
 0.10943476 0.09232001 0.1266968  0.07078455 0.15704335], sum to 1.0000
[2019-04-09 14:51:16,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4907
[2019-04-09 14:51:16,428] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 22.5, 26.85148970591015, 0.686232786824481, 1.0, 1.0, 50.0, 47.8022547329495], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [0.5, 47.0, 125.0, 763.0, 22.5, 26.38812536933397, 0.6446941446429513, 1.0, 1.0, 45.0, 44.12056071845881], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.47, 0.4166666666666667, 0.8430939226519337, 0.375, 0.6990104474444975, 0.7148980482143171, 1.0, 1.0, 0.6, 0.4412056071845881], 
reward next is 0.5588, 
noisyNet noise sample is [array([-1.1985072], dtype=float32), 0.21411833]. 
=============================================
[2019-04-09 14:51:16,431] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01318769 0.13881773 0.11905003 0.08799876 0.05799034 0.02593986
 0.11347879 0.08198806 0.13553122 0.0716079  0.15440965], sum to 1.0000
[2019-04-09 14:51:16,433] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-09 14:51:16,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01608523 0.10606159 0.11488903 0.06357547 0.04158093 0.03220499
 0.15456921 0.07085892 0.10900282 0.0751576  0.21601419], sum to 1.0000
[2019-04-09 14:51:16,460] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5252
[2019-04-09 14:51:16,461] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 47.0, 125.0, 763.0, 22.5, 26.38812536933397, 0.6446941446429513, 1.0, 1.0, 45.0, 44.12056071845881], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2806200.0000, 
sim time next is 2806800.0000, 
raw observation next is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 22.5, 26.60384305064923, 0.6574325754420032, 1.0, 1.0, 50.0, 44.54047899980651], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.46, 0.44611111111111096, 0.8287292817679558, 0.375, 0.7169869208874357, 0.7191441918140011, 1.0, 1.0, 0.7, 0.4454047899980651], 
reward next is 0.5546, 
noisyNet noise sample is [array([-1.1985072], dtype=float32), 0.21411833]. 
=============================================
[2019-04-09 14:51:16,476] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 61.5, 0.0, 0.0, 19.0, 25.74155280583189, 0.5609311154028213, 0.0, 1.0, 25.0, 45.06428565626216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2752200.0000, 
sim time next is 2752800.0000, 
raw observation next is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 19.0, 25.76265217258208, 0.5551706362811274, 0.0, 1.0, 45.0, 45.29409905868214], 
processed observation next is [1.0, 0.8695652173913043, 0.30563250230840255, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.6468876810485066, 0.6850568787603758, 0.0, 1.0, 0.6, 0.4529409905868214], 
reward next is 0.5471, 
noisyNet noise sample is [array([-0.8353395], dtype=float32), -0.67259485]. 
=============================================
[2019-04-09 14:51:16,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01448551 0.12704408 0.11012247 0.07404944 0.04160333 0.03075032
 0.11611469 0.10311452 0.13250153 0.07540824 0.17480586], sum to 1.0000
[2019-04-09 14:51:16,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3421
[2019-04-09 14:51:16,712] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 22.5, 27.05798194495942, 0.7247756365437824, 1.0, 1.0, 35.0, 50.95568797895331], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [6.5, 27.0, 118.0, 0.0, 22.5, 27.149479311417, 0.7177895399020185, 1.0, 1.0, 35.0, 39.04554456609428], 
processed observation next is [1.0, 0.6086956521739131, 0.6426592797783934, 0.27, 0.3933333333333333, 0.0, 0.375, 0.76245660928475, 0.7392631799673395, 1.0, 1.0, 0.4, 0.3904554456609428], 
reward next is 0.6095, 
noisyNet noise sample is [array([0.21311025], dtype=float32), -2.0592895]. 
=============================================
[2019-04-09 14:51:16,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[1.8130677]
 [1.884918 ]
 [1.7718666]
 [1.8996706]
 [1.8997166]], R is [[2.54276252]
 [3.00777817]
 [3.31401277]
 [3.87767911]
 [4.37315559]].
[2019-04-09 14:51:16,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02100051 0.11050866 0.11176662 0.07668896 0.05226476 0.02547414
 0.13094015 0.07532943 0.11912535 0.06850934 0.20839201], sum to 1.0000
[2019-04-09 14:51:16,748] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5410
[2019-04-09 14:51:16,774] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 19.0, 25.81128003084609, 0.5733178531597275, 0.0, 1.0, 45.0, 54.57803328302786], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [-6.0, 64.0, 0.0, 0.0, 19.0, 25.80855971754094, 0.5619184615821472, 0.0, 1.0, 50.0, 38.72353161688912], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6507133097950785, 0.6873061538607157, 0.0, 1.0, 0.7, 0.3872353161688912], 
reward next is 0.6128, 
noisyNet noise sample is [array([1.9298725], dtype=float32), 0.029228635]. 
=============================================
[2019-04-09 14:51:16,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[1.6790075]
 [1.7643987]
 [1.7432145]
 [1.7500125]
 [1.8397349]], R is [[2.32794857]
 [2.75888872]
 [3.05767345]
 [3.52871537]
 [3.93232656]].
[2019-04-09 14:51:16,800] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01313738 0.15034702 0.11027312 0.0788056  0.05069852 0.02632353
 0.10908593 0.09125432 0.10882667 0.07750586 0.18374205], sum to 1.0000
[2019-04-09 14:51:16,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4444
[2019-04-09 14:51:16,841] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 151.5, 724.0, 22.5, 27.02884438752533, 0.608422712555794, 1.0, 1.0, 20.0, 61.98794935357898], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2808000.0000, 
sim time next is 2808600.0000, 
raw observation next is [2.333333333333333, 42.50000000000001, 160.3333333333333, 711.0, 22.5, 26.33758222449267, 0.6605850188435722, 1.0, 1.0, 45.0, 51.50052250356234], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.42500000000000004, 0.5344444444444443, 0.7856353591160221, 0.375, 0.6947985187077226, 0.7201950062811907, 1.0, 1.0, 0.6, 0.5150052250356234], 
reward next is 0.4850, 
noisyNet noise sample is [array([-0.90352213], dtype=float32), 0.51066685]. 
=============================================
[2019-04-09 14:51:16,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02928749 0.09261757 0.09839287 0.05710674 0.04695133 0.03146965
 0.15202065 0.08296832 0.10848704 0.10197462 0.19872369], sum to 1.0000
[2019-04-09 14:51:16,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6398
[2019-04-09 14:51:16,979] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 24.91265361985774, 0.2837889654837156, 0.0, 1.0, 25.0, 38.50918433284109], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2784000.0000, 
sim time next is 2784600.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 24.97271959845459, 0.270475343682737, 0.0, 1.0, 55.0, 38.3588992331502], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.5810599665378824, 0.5901584478942457, 0.0, 1.0, 0.8, 0.38358899233150195], 
reward next is 0.6164, 
noisyNet noise sample is [array([0.52654845], dtype=float32), 1.0579315]. 
=============================================
[2019-04-09 14:51:17,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01633234 0.16146913 0.10813569 0.10402376 0.05428043 0.03297404
 0.0965528  0.07202271 0.13549566 0.08212622 0.1365872 ], sum to 1.0000
[2019-04-09 14:51:17,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9173
[2019-04-09 14:51:17,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 53.33333333333334, 170.3333333333333, 462.1666666666667, 22.5, 26.61960176686322, 0.602963400724311, 1.0, 1.0, 40.0, 38.4449686065484], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2802000.0000, 
sim time next is 2802600.0000, 
raw observation next is [-2.0, 52.5, 174.0, 508.0, 22.5, 26.68547307784353, 0.6261344662243481, 1.0, 1.0, 65.0, 49.50696626349012], 
processed observation next is [1.0, 0.43478260869565216, 0.40720221606648205, 0.525, 0.58, 0.5613259668508287, 0.375, 0.7237894231536274, 0.7087114887414493, 1.0, 1.0, 1.0, 0.4950696626349012], 
reward next is 0.5049, 
noisyNet noise sample is [array([0.39782104], dtype=float32), 0.96543056]. 
=============================================
[2019-04-09 14:51:17,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01211995 0.13250087 0.12023234 0.07996871 0.04478231 0.02391984
 0.13887145 0.08853977 0.13752386 0.07257362 0.14896716], sum to 1.0000
[2019-04-09 14:51:17,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6882
[2019-04-09 14:51:17,419] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02437367 0.10758564 0.11998299 0.06904037 0.04633245 0.03563799
 0.12986492 0.08138728 0.11154175 0.08456015 0.18969278], sum to 1.0000
[2019-04-09 14:51:17,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4535
[2019-04-09 14:51:17,437] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 22.5, 27.28033962378865, 0.7648436440292823, 1.0, 1.0, 55.0, 56.77902926964126], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [4.0, 33.5, 0.0, 0.0, 22.5, 27.09298923666697, 0.7410730443080031, 1.0, 1.0, 55.0, 54.1624792484234], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.335, 0.0, 0.0, 0.375, 0.7577491030555809, 0.7470243481026677, 1.0, 1.0, 0.8, 0.541624792484234], 
reward next is 0.4584, 
noisyNet noise sample is [array([-1.0583014], dtype=float32), -1.9705654]. 
=============================================
[2019-04-09 14:51:17,438] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 64.0, 108.0, 207.0, 22.5, 25.45289543712347, 0.3945276151971331, 1.0, 1.0, 40.0, 45.87320463029486], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2795400.0000, 
sim time next is 2796000.0000, 
raw observation next is [-6.0, 64.0, 115.3333333333333, 211.3333333333333, 22.5, 25.63714441438679, 0.4127377527873324, 1.0, 1.0, 45.0, 42.38774912958591], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.3844444444444443, 0.23351749539594838, 0.375, 0.636428701198899, 0.6375792509291108, 1.0, 1.0, 0.6, 0.4238774912958591], 
reward next is 0.5761, 
noisyNet noise sample is [array([0.4176229], dtype=float32), -0.27936324]. 
=============================================
[2019-04-09 14:51:17,443] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.4158694]
 [1.3789095]
 [1.3017888]
 [1.3335611]
 [1.2952503]], R is [[1.92813706]
 [2.45012355]
 [2.89438581]
 [3.24894762]
 [3.37073541]].
[2019-04-09 14:51:17,562] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01500564 0.11919882 0.11334959 0.07379591 0.05898915 0.02776782
 0.11654703 0.08074795 0.10466131 0.09773627 0.19220053], sum to 1.0000
[2019-04-09 14:51:17,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6121
[2019-04-09 14:51:17,583] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.333333333333333, 35.83333333333334, 0.0, 0.0, 22.5, 26.81603845424277, 0.7177129418314402, 1.0, 1.0, 55.0, 58.78513463208183], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2832600.0000, 
sim time next is 2833200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 22.5, 26.73961273562204, 0.7073716976997234, 1.0, 1.0, 55.0, 56.39714892397655], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 0.375, 0.7283010613018366, 0.7357905658999079, 1.0, 1.0, 0.8, 0.5639714892397655], 
reward next is 0.4360, 
noisyNet noise sample is [array([-1.9914204], dtype=float32), 1.0895529]. 
=============================================
[2019-04-09 14:51:17,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01794844 0.07714049 0.11037868 0.07096242 0.0383465  0.02540684
 0.17239541 0.08281468 0.11228909 0.09257971 0.1997377 ], sum to 1.0000
[2019-04-09 14:51:17,942] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2833
[2019-04-09 14:51:17,956] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.07006909518039, 0.6146517207144699, 0.0, 1.0, 30.0, 51.27871427757869], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2840400.0000, 
sim time next is 2841000.0000, 
raw observation next is [2.0, 44.00000000000001, 0.0, 0.0, 19.0, 26.13631733907251, 0.6174015367640563, 0.0, 1.0, 35.0, 38.12185817963068], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44000000000000006, 0.0, 0.0, 0.08333333333333333, 0.678026444922709, 0.7058005122546854, 0.0, 1.0, 0.4, 0.3812185817963068], 
reward next is 0.6188, 
noisyNet noise sample is [array([-0.2412585], dtype=float32), -0.22170018]. 
=============================================
[2019-04-09 14:51:17,962] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[1.8140961]
 [1.7826915]
 [1.94118  ]
 [1.8332858]
 [1.8241268]], R is [[2.47526884]
 [2.93772912]
 [3.26312017]
 [3.79298425]
 [4.211689  ]].
[2019-04-09 14:51:17,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01407134 0.14129712 0.111894   0.10437734 0.05645121 0.02513806
 0.09290483 0.06738926 0.14026764 0.0706536  0.17555565], sum to 1.0000
[2019-04-09 14:51:17,986] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6207
[2019-04-09 14:51:18,045] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.5, 47.0, 125.0, 763.0, 22.5, 26.90078149520779, 0.5663097940061322, 1.0, 1.0, 45.0, 49.98733399208965], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2806200.0000, 
sim time next is 2806800.0000, 
raw observation next is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 22.5, 26.04185535667073, 0.6132124661512574, 1.0, 1.0, 55.0, 64.3103301514735], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.46, 0.44611111111111096, 0.8287292817679558, 0.375, 0.6701546130558942, 0.7044041553837525, 1.0, 1.0, 0.8, 0.6431033015147349], 
reward next is 0.3569, 
noisyNet noise sample is [array([0.74706584], dtype=float32), -0.45704395]. 
=============================================
[2019-04-09 14:51:18,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01811358 0.10899078 0.10752543 0.06606889 0.03979659 0.02939236
 0.14009479 0.07664263 0.10702183 0.0693338  0.23701939], sum to 1.0000
[2019-04-09 14:51:18,060] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1593
[2019-04-09 14:51:18,094] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.25024623915765, 0.6404017482243012, 0.0, 1.0, 25.0, 46.59066799216848], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2839200.0000, 
sim time next is 2839800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 26.23073195852522, 0.6317481926168514, 0.0, 1.0, 25.0, 35.85996526282015], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.6858943298771015, 0.7105827308722837, 0.0, 1.0, 0.2, 0.3585996526282015], 
reward next is 0.6414, 
noisyNet noise sample is [array([0.6408432], dtype=float32), -0.09659257]. 
=============================================
[2019-04-09 14:51:18,171] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02484787 0.09224065 0.10340781 0.06638037 0.04974353 0.02961022
 0.167316   0.09273476 0.08574217 0.09904348 0.1889332 ], sum to 1.0000
[2019-04-09 14:51:18,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5677
[2019-04-09 14:51:18,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 56.0, 0.0, 0.0, 19.0, 26.47594137962544, 0.6419013093644631, 0.0, 1.0, 30.0, 38.04010238020671], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2846400.0000, 
sim time next is 2847000.0000, 
raw observation next is [2.0, 59.0, 0.0, 0.0, 19.0, 26.40976326630537, 0.6250099414912483, 0.0, 1.0, 50.0, 35.47370694606226], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.59, 0.0, 0.0, 0.08333333333333333, 0.7008136055254474, 0.7083366471637494, 0.0, 1.0, 0.7, 0.35473706946062256], 
reward next is 0.6453, 
noisyNet noise sample is [array([0.6825913], dtype=float32), -0.7057418]. 
=============================================
[2019-04-09 14:51:18,190] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.5990065]
 [1.6027302]
 [1.6717417]
 [1.5870757]
 [1.7408514]], R is [[2.10033512]
 [2.69893074]
 [3.2623086 ]
 [3.77843809]
 [4.23416424]].
[2019-04-09 14:51:18,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01125283 0.13641421 0.10969773 0.07937834 0.0466202  0.02619925
 0.12358564 0.05705863 0.14178957 0.06609607 0.20190752], sum to 1.0000
[2019-04-09 14:51:18,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2944
[2019-04-09 14:51:18,410] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.5, 27.0, 118.0, 0.0, 22.5, 26.72232318144587, 0.6730389430552566, 1.0, 1.0, 65.0, 55.9917184581335], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2817000.0000, 
sim time next is 2817600.0000, 
raw observation next is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 22.5, 27.02341304165063, 0.7192962843526912, 1.0, 1.0, 30.0, 45.63691074021204], 
processed observation next is [1.0, 0.6086956521739131, 0.6472760849492153, 0.26, 0.38055555555555565, 0.0, 0.375, 0.7519510868042193, 0.7397654281175637, 1.0, 1.0, 0.3, 0.4563691074021204], 
reward next is 0.5436, 
noisyNet noise sample is [array([1.0783461], dtype=float32), -0.07880652]. 
=============================================
[2019-04-09 14:51:18,629] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01351984 0.09587641 0.13797587 0.08059969 0.04699887 0.02744393
 0.12090453 0.10066707 0.11663452 0.07791761 0.18146169], sum to 1.0000
[2019-04-09 14:51:18,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5705
[2019-04-09 14:51:18,644] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 22.5, 27.41411524688186, 0.7714690665369098, 1.0, 1.0, 30.0, 42.52178055226559], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 22.5, 27.49813463780583, 0.7844085098711394, 1.0, 1.0, 50.0, 37.32349353272252], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 0.375, 0.7915112198171524, 0.7614695032903799, 1.0, 1.0, 0.7, 0.37323493532722524], 
reward next is 0.6268, 
noisyNet noise sample is [array([-0.50921637], dtype=float32), -0.0026969067]. 
=============================================
[2019-04-09 14:51:18,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0113558  0.1424794  0.1003965  0.09779513 0.05293602 0.02958297
 0.12469936 0.05833749 0.12016592 0.06961127 0.19264013], sum to 1.0000
[2019-04-09 14:51:18,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7295
[2019-04-09 14:51:18,721] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 22.5, 27.2268661425151, 0.7572341152121762, 1.0, 1.0, 25.0, 40.93569960798774], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [6.5, 27.0, 118.0, 0.0, 22.5, 27.30219755762193, 0.749170083107101, 1.0, 1.0, 55.0, 49.24429757153358], 
processed observation next is [1.0, 0.6086956521739131, 0.6426592797783934, 0.27, 0.3933333333333333, 0.0, 0.375, 0.7751831298018276, 0.7497233610357004, 1.0, 1.0, 0.8, 0.4924429757153358], 
reward next is 0.5076, 
noisyNet noise sample is [array([-0.59555256], dtype=float32), 0.5683308]. 
=============================================
[2019-04-09 14:51:18,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.9641924]
 [2.0818985]
 [2.1520598]
 [2.190935 ]
 [2.036807 ]], R is [[2.67955017]
 [3.24339771]
 [3.69770455]
 [4.02646589]
 [4.48315954]].
[2019-04-09 14:51:18,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01302717 0.11453703 0.12939054 0.08329569 0.05223988 0.02594828
 0.12367041 0.07341664 0.10957018 0.07103679 0.20386742], sum to 1.0000
[2019-04-09 14:51:18,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7186
[2019-04-09 14:51:18,785] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 22.5, 27.18894424544585, 0.7326908887933018, 1.0, 1.0, 50.0, 46.58547865172544], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 22.5, 27.07793309894726, 0.7271176874031694, 1.0, 1.0, 55.0, 61.04599076083687], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 0.3233333333333334, 0.0, 0.0, 0.375, 0.7564944249122716, 0.7423725624677231, 1.0, 1.0, 0.8, 0.6104599076083688], 
reward next is 0.3895, 
noisyNet noise sample is [array([0.5123617], dtype=float32), 2.6285772]. 
=============================================
[2019-04-09 14:51:19,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0229625  0.09709963 0.09958308 0.07520591 0.0523152  0.02849231
 0.15888265 0.06461433 0.11926493 0.09111419 0.19046518], sum to 1.0000
[2019-04-09 14:51:19,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2343
[2019-04-09 14:51:19,116] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 19.0, 26.13516609952979, 0.535150989217083, 0.0, 1.0, 55.0, 54.28018344036742], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 19.0, 26.08408929625712, 0.5366089935902195, 0.0, 1.0, 30.0, 52.09708191377113], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.08333333333333333, 0.6736741080214266, 0.6788696645300732, 0.0, 1.0, 0.3, 0.5209708191377113], 
reward next is 0.4790, 
noisyNet noise sample is [array([-0.10800324], dtype=float32), 0.35028505]. 
=============================================
[2019-04-09 14:51:19,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02333951 0.09954251 0.09343017 0.07028674 0.04978972 0.03889107
 0.15286542 0.09069958 0.1000082  0.08613688 0.1950102 ], sum to 1.0000
[2019-04-09 14:51:19,300] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5465
[2019-04-09 14:51:19,316] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 19.0, 26.15547823706637, 0.5530834266243324, 0.0, 1.0, 50.0, 49.42047590301457], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2853600.0000, 
sim time next is 2854200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 26.11032712784065, 0.5401875765357141, 0.0, 1.0, 50.0, 47.53438014270923], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6758605939867209, 0.6800625255119047, 0.0, 1.0, 0.7, 0.4753438014270923], 
reward next is 0.5247, 
noisyNet noise sample is [array([-0.26350927], dtype=float32), 0.11437809]. 
=============================================
[2019-04-09 14:51:19,358] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01529066 0.12322931 0.1159549  0.06720971 0.04610829 0.02936168
 0.09976503 0.09065462 0.13437924 0.08128233 0.19676428], sum to 1.0000
[2019-04-09 14:51:19,361] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6589
[2019-04-09 14:51:19,401] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 22.5, 26.5124375403254, 0.6412073756369533, 1.0, 1.0, 20.0, 41.4349178359697], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2806800.0000, 
sim time next is 2807400.0000, 
raw observation next is [1.5, 45.0, 142.6666666666667, 737.0, 22.5, 26.64076768287415, 0.4999971523683255, 1.0, 1.0, 55.0, 65.99188915587703], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.45, 0.47555555555555573, 0.8143646408839779, 0.375, 0.7200639735728457, 0.6666657174561085, 1.0, 1.0, 0.8, 0.6599188915587704], 
reward next is 0.3401, 
noisyNet noise sample is [array([-1.0507014], dtype=float32), 0.6061687]. 
=============================================
[2019-04-09 14:51:20,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02806799 0.10826195 0.0940783  0.05901999 0.04481111 0.03061614
 0.15750295 0.08457545 0.09919996 0.09612115 0.19774498], sum to 1.0000
[2019-04-09 14:51:20,089] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-09 14:51:20,105] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.0, 96.5, 0.0, 0.0, 19.0, 26.1372557237159, 0.5253174095224508, 0.0, 1.0, 20.0, 47.25304276260074], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2871000.0000, 
sim time next is 2871600.0000, 
raw observation next is [1.0, 97.66666666666666, 0.0, 0.0, 19.0, 26.12730460931628, 0.5119087991331158, 0.0, 1.0, 60.0, 67.83238651860025], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9766666666666666, 0.0, 0.0, 0.08333333333333333, 0.67727538410969, 0.6706362663777052, 0.0, 1.0, 0.9, 0.6783238651860024], 
reward next is 0.3217, 
noisyNet noise sample is [array([0.84562486], dtype=float32), 0.40396857]. 
=============================================
[2019-04-09 14:51:20,117] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02882652 0.08570271 0.10026678 0.08088007 0.04932211 0.03414132
 0.1546146  0.08188061 0.10578752 0.09961843 0.17895932], sum to 1.0000
[2019-04-09 14:51:20,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0087
[2019-04-09 14:51:20,127] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 25.89153872947089, 0.4934124740097929, 0.0, 1.0, 35.0, 55.38901680226792], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 25.96654133401317, 0.3875727149759808, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.6638784445010973, 0.6291909049919936, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2726408], dtype=float32), 0.18892108]. 
=============================================
[2019-04-09 14:51:20,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01451989 0.11088031 0.13083246 0.06628599 0.04825043 0.02730878
 0.13775262 0.07234728 0.12303725 0.06980193 0.19898297], sum to 1.0000
[2019-04-09 14:51:20,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1810
[2019-04-09 14:51:20,378] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.733333333333333, 24.66666666666666, 91.0, 12.66666666666666, 22.5, 27.38496828031683, 0.7942859794236745, 1.0, 1.0, 25.0, 38.15409085047739], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2821200.0000, 
sim time next is 2821800.0000, 
raw observation next is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 22.5, 27.54539398893014, 0.8136992802299483, 1.0, 1.0, 25.0, 40.16994955076871], 
processed observation next is [1.0, 0.6521739130434783, 0.6472760849492153, 0.2483333333333334, 0.29, 0.02799263351749539, 0.375, 0.7954494990775117, 0.7712330934099828, 1.0, 1.0, 0.2, 0.40169949550768713], 
reward next is 0.5983, 
noisyNet noise sample is [array([0.12984854], dtype=float32), 0.17891672]. 
=============================================
[2019-04-09 14:51:20,386] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02244651 0.08658136 0.10024489 0.05849032 0.04205932 0.03335663
 0.14394744 0.08466045 0.10331491 0.09705351 0.22784461], sum to 1.0000
[2019-04-09 14:51:20,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9572
[2019-04-09 14:51:20,408] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 25.32817452252175, 0.3915948305117716, 0.0, 1.0, 55.0, 63.20600527213576], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2861400.0000, 
sim time next is 2862000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 25.3151674685987, 0.4158787264677872, 0.0, 1.0, 65.0, 78.86766525430147], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6095972890498915, 0.6386262421559291, 0.0, 1.0, 1.0, 0.7886766525430147], 
reward next is 0.2113, 
noisyNet noise sample is [array([-0.87944484], dtype=float32), -0.45956942]. 
=============================================
[2019-04-09 14:51:20,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[1.4023085]
 [1.2324085]
 [1.1956338]
 [1.2858101]
 [1.3971596]], R is [[1.36839116]
 [1.72264719]
 [2.3049078 ]
 [2.88499618]
 [3.45604539]].
[2019-04-09 14:51:20,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01232778 0.12865715 0.11602432 0.06292126 0.04926429 0.02671521
 0.11060749 0.07849041 0.16264938 0.09299923 0.15934344], sum to 1.0000
[2019-04-09 14:51:20,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7627
[2019-04-09 14:51:20,602] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [6.1, 27.5, 49.0, 66.0, 22.5, 27.02947750432538, 0.77627433193444, 1.0, 1.0, 25.0, 34.73079846634278], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2825400.0000, 
sim time next is 2826000.0000, 
raw observation next is [6.0, 28.0, 38.0, 61.0, 22.5, 27.43003888041168, 0.8014153855179161, 1.0, 1.0, 60.0, 32.2519545008107], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 0.28, 0.12666666666666668, 0.06740331491712707, 0.375, 0.7858365733676399, 0.7671384618393055, 1.0, 1.0, 0.9, 0.322519545008107], 
reward next is 0.6775, 
noisyNet noise sample is [array([-0.22093658], dtype=float32), -0.12616593]. 
=============================================
[2019-04-09 14:51:20,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[1.9747703]
 [2.1050742]
 [2.0220058]
 [2.0080683]
 [2.057544 ]], R is [[2.75587392]
 [3.38100719]
 [3.94344187]
 [4.32690763]
 [4.92948151]].
[2019-04-09 14:51:20,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02391645 0.10914263 0.08623055 0.06488833 0.04753457 0.03600696
 0.14454237 0.07546467 0.10963497 0.07754047 0.22509801], sum to 1.0000
[2019-04-09 14:51:20,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-09 14:51:20,800] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 25.70257621873044, 0.474889452572552, 0.0, 1.0, 55.0, 55.94165736901402], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 25.73458836209711, 0.4848167476134074, 0.0, 1.0, 65.0, 79.31483352522258], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6445490301747592, 0.6616055825378024, 0.0, 1.0, 1.0, 0.7931483352522258], 
reward next is 0.2069, 
noisyNet noise sample is [array([0.19547729], dtype=float32), -0.6007395]. 
=============================================
[2019-04-09 14:51:20,967] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01073421 0.11844507 0.11016537 0.07027765 0.04619014 0.0221946
 0.12906818 0.07831119 0.12099274 0.07351859 0.22010224], sum to 1.0000
[2019-04-09 14:51:20,968] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9889
[2019-04-09 14:51:20,986] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 22.5, 26.15295909886815, 0.6316069254752784, 1.0, 1.0, 35.0, 50.02923505760302], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 22.5, 26.29350329424613, 0.6624023105431623, 1.0, 1.0, 65.0, 63.6034670927173], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.375, 0.6911252745205108, 0.7208007701810542, 1.0, 1.0, 1.0, 0.636034670927173], 
reward next is 0.3640, 
noisyNet noise sample is [array([0.10263218], dtype=float32), 0.5197161]. 
=============================================
[2019-04-09 14:51:21,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01126209 0.16204834 0.1007305  0.088012   0.04274599 0.02144782
 0.12152376 0.06610509 0.13239579 0.08487835 0.16885024], sum to 1.0000
[2019-04-09 14:51:21,074] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8234
[2019-04-09 14:51:21,086] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.0, 100.0, 114.6666666666667, 0.0, 22.5, 27.38313122427805, 0.8215334018739963, 1.0, 1.0, 45.0, 25.36820157925924], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2902200.0000, 
sim time next is 2902800.0000, 
raw observation next is [2.0, 100.0, 102.3333333333333, 0.0, 22.5, 27.36202098631208, 0.8311398125294893, 1.0, 1.0, 60.0, 34.47189697139211], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.341111111111111, 0.0, 0.375, 0.7801684155260066, 0.7770466041764964, 1.0, 1.0, 0.9, 0.34471896971392113], 
reward next is 0.6553, 
noisyNet noise sample is [array([0.22940704], dtype=float32), 0.56776047]. 
=============================================
[2019-04-09 14:51:21,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02176842 0.09780049 0.09116339 0.07240664 0.04819307 0.03292686
 0.17418255 0.06316848 0.1095863  0.09075998 0.19804376], sum to 1.0000
[2019-04-09 14:51:21,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5256
[2019-04-09 14:51:21,279] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 95.33333333333334, 0.0, 0.0, 19.0, 25.95378591625835, 0.5501492080413478, 0.0, 1.0, 65.0, 67.31548661917557], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 19.0, 26.13882574642416, 0.5509070299010729, 0.0, 1.0, 45.0, 53.7177780950169], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 0.08333333333333333, 0.6782354788686801, 0.683635676633691, 0.0, 1.0, 0.6, 0.537177780950169], 
reward next is 0.4628, 
noisyNet noise sample is [array([0.80025303], dtype=float32), 0.9158505]. 
=============================================
[2019-04-09 14:51:21,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.4293364]
 [1.3795466]
 [1.2542806]
 [1.3706112]
 [1.2470062]], R is [[1.74508417]
 [2.05447841]
 [2.49016285]
 [2.69145966]
 [2.948107  ]].
[2019-04-09 14:51:21,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01198283 0.11137477 0.12360172 0.07254428 0.04911111 0.0204747
 0.11355633 0.08626328 0.13536257 0.10100652 0.17472193], sum to 1.0000
[2019-04-09 14:51:21,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-09 14:51:21,343] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 22.5, 27.39008144488751, 0.871604838979337, 1.0, 1.0, 30.0, 32.4449176107874], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2909400.0000, 
sim time next is 2910000.0000, 
raw observation next is [2.0, 97.66666666666667, 73.33333333333334, 45.0, 22.5, 27.5497088439493, 0.8743209893203711, 1.0, 1.0, 30.0, 24.23720643301403], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9766666666666667, 0.24444444444444446, 0.049723756906077346, 0.375, 0.7958090703291084, 0.791440329773457, 1.0, 1.0, 0.3, 0.2423720643301403], 
reward next is 0.7576, 
noisyNet noise sample is [array([-0.2850673], dtype=float32), -0.24064013]. 
=============================================
[2019-04-09 14:51:21,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[2.3613787]
 [2.3047197]
 [2.3883462]
 [2.265735 ]
 [2.358615 ]], R is [[3.03281116]
 [3.67803383]
 [4.47407293]
 [5.05600595]
 [5.38554764]].
[2019-04-09 14:51:21,404] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01222157 0.11832842 0.09448436 0.07181893 0.04531291 0.02356398
 0.11929351 0.07214087 0.14924127 0.08762404 0.20597015], sum to 1.0000
[2019-04-09 14:51:21,409] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-09 14:51:21,426] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 98.83333333333334, 116.3333333333333, 0.0, 22.5, 27.03534381727711, 0.7251114718287862, 1.0, 1.0, 65.0, 61.54790578288731], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2893800.0000, 
sim time next is 2894400.0000, 
raw observation next is [1.0, 100.0, 131.0, 0.0, 22.5, 27.10322586885307, 0.7376847345834904, 1.0, 1.0, 35.0, 36.35771483402577], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 1.0, 0.43666666666666665, 0.0, 0.375, 0.7586021557377558, 0.74589491152783, 1.0, 1.0, 0.4, 0.3635771483402577], 
reward next is 0.6364, 
noisyNet noise sample is [array([-0.7596753], dtype=float32), 0.40600386]. 
=============================================
[2019-04-09 14:51:21,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01242865 0.12239154 0.10877572 0.08014499 0.04685322 0.02141633
 0.1251182  0.07204591 0.12761223 0.09783512 0.18537806], sum to 1.0000
[2019-04-09 14:51:21,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0371
[2019-04-09 14:51:21,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01677666 0.1130419  0.10309872 0.0837504  0.05714198 0.03544535
 0.10654887 0.08579725 0.13556166 0.10343392 0.15940328], sum to 1.0000
[2019-04-09 14:51:21,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1773
[2019-04-09 14:51:21,683] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 95.33333333333334, 85.5, 0.0, 22.5, 27.09986777592628, 0.7397194247611562, 1.0, 1.0, 60.0, 43.13833286291106], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2892000.0000, 
sim time next is 2892600.0000, 
raw observation next is [1.0, 96.5, 87.0, 0.0, 22.5, 27.12156953126072, 0.753652518855093, 1.0, 1.0, 20.0, 34.82917060453647], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.965, 0.29, 0.0, 0.375, 0.7601307942717268, 0.7512175062850309, 1.0, 1.0, 0.1, 0.34829170604536475], 
reward next is 0.6517, 
noisyNet noise sample is [array([-0.4738959], dtype=float32), -0.6762269]. 
=============================================
[2019-04-09 14:51:21,686] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 96.5, 49.0, 0.0, 22.5, 26.06234095222106, 0.50064786393461, 1.0, 1.0, 55.0, 57.45149103638804], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2885400.0000, 
sim time next is 2886000.0000, 
raw observation next is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 22.5, 26.09738504341966, 0.5187050304710431, 1.0, 1.0, 25.0, 43.7740573683931], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.9766666666666666, 0.17944444444444443, 0.0, 0.375, 0.6747820869516383, 0.6729016768236811, 1.0, 1.0, 0.2, 0.43774057368393104], 
reward next is 0.5623, 
noisyNet noise sample is [array([0.36653832], dtype=float32), 0.46544394]. 
=============================================
[2019-04-09 14:51:21,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[1.7697332]
 [1.6685572]
 [1.7547319]
 [1.544642 ]
 [1.6791435]], R is [[2.33015776]
 [2.73234129]
 [3.39972591]
 [4.03796148]
 [4.65336609]].
[2019-04-09 14:51:21,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01730659 0.0818058  0.12362145 0.05235324 0.04421409 0.02783231
 0.17307782 0.08159136 0.08029535 0.10438205 0.21351993], sum to 1.0000
[2019-04-09 14:51:21,749] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2631
[2019-04-09 14:51:21,772] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 19.0, 26.16693938538175, 0.533338187021801, 0.0, 1.0, 50.0, 45.62712013146743], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2850600.0000, 
sim time next is 2851200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 26.0516638529398, 0.4982498415745129, 0.0, 1.0, 30.0, 47.04910293475582], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6709719877449833, 0.6660832805248377, 0.0, 1.0, 0.3, 0.4704910293475582], 
reward next is 0.5295, 
noisyNet noise sample is [array([0.5509786], dtype=float32), 1.0250297]. 
=============================================
[2019-04-09 14:51:21,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01236007 0.14013956 0.10700563 0.0841506  0.04460921 0.01578489
 0.10587817 0.06639204 0.16056947 0.09836031 0.16475008], sum to 1.0000
[2019-04-09 14:51:21,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3130
[2019-04-09 14:51:21,990] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 100.0, 165.8333333333333, 0.0, 22.5, 27.39920158744201, 0.8357192683337288, 1.0, 1.0, 20.0, 25.41997169356118], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2899200.0000, 
sim time next is 2899800.0000, 
raw observation next is [2.0, 100.0, 164.0, 0.0, 22.5, 27.44727769035394, 0.8446748880377335, 1.0, 1.0, 30.0, 27.55044340943801], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5466666666666666, 0.0, 0.375, 0.7872731408628283, 0.7815582960125779, 1.0, 1.0, 0.3, 0.2755044340943801], 
reward next is 0.7245, 
noisyNet noise sample is [array([-1.2300899], dtype=float32), -0.58761644]. 
=============================================
[2019-04-09 14:51:22,005] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01435214 0.10250799 0.11558363 0.05435841 0.04560792 0.02528361
 0.14453183 0.08654214 0.0914469  0.09487206 0.22491336], sum to 1.0000
[2019-04-09 14:51:22,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-09 14:51:22,019] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 19.0, 26.6421345076484, 0.7831833351562426, 0.0, 1.0, 25.0, 38.73581838427862], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2926200.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 19.0, 26.62579708573775, 0.7803088347182622, 0.0, 1.0, 65.0, 46.43594130490762], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7188164238114793, 0.7601029449060874, 0.0, 1.0, 1.0, 0.46435941304907624], 
reward next is 0.5356, 
noisyNet noise sample is [array([-1.6203501], dtype=float32), 0.14201674]. 
=============================================
[2019-04-09 14:51:22,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01393179 0.12044637 0.09792819 0.07674632 0.04683583 0.0303858
 0.11729286 0.06872154 0.12635611 0.08690438 0.2144508 ], sum to 1.0000
[2019-04-09 14:51:22,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5910
[2019-04-09 14:51:22,249] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 94.16666666666666, 84.0, 0.0, 22.5, 27.08347876834145, 0.7189707806361757, 1.0, 1.0, 20.0, 33.34158104372779], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2891400.0000, 
sim time next is 2892000.0000, 
raw observation next is [1.0, 95.33333333333334, 85.5, 0.0, 22.5, 27.06101995166173, 0.7235225842161283, 1.0, 1.0, 55.0, 42.97264296845343], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9533333333333335, 0.285, 0.0, 0.375, 0.7550849959718109, 0.7411741947387095, 1.0, 1.0, 0.8, 0.4297264296845343], 
reward next is 0.5703, 
noisyNet noise sample is [array([-0.16495682], dtype=float32), 1.6777823]. 
=============================================
[2019-04-09 14:51:22,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.1207771]
 [2.1814704]
 [2.0019119]
 [2.156891 ]
 [2.0657492]], R is [[2.67415571]
 [3.31399846]
 [3.94039345]
 [4.51332617]
 [5.01655149]].
[2019-04-09 14:51:22,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01146436 0.12764393 0.11347336 0.07811026 0.04601791 0.0182011
 0.12088359 0.06551749 0.12869017 0.0924107  0.19758722], sum to 1.0000
[2019-04-09 14:51:22,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2235
[2019-04-09 14:51:22,770] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 97.66666666666667, 73.33333333333334, 45.0, 22.5, 27.56006975028569, 0.8845850106940006, 1.0, 1.0, 65.0, 46.64310595838263], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2910000.0000, 
sim time next is 2910600.0000, 
raw observation next is [2.0, 96.5, 71.0, 54.0, 22.5, 27.53588169110413, 0.8918102184949848, 1.0, 1.0, 25.0, 34.23814840328025], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.965, 0.23666666666666666, 0.05966850828729282, 0.375, 0.7946568075920108, 0.7972700728316616, 1.0, 1.0, 0.2, 0.3423814840328025], 
reward next is 0.6576, 
noisyNet noise sample is [array([-1.1107016], dtype=float32), -1.067284]. 
=============================================
[2019-04-09 14:51:22,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02845326 0.1089665  0.08705183 0.0744538  0.0598664  0.04162942
 0.16563095 0.08218247 0.07460444 0.09997953 0.17718135], sum to 1.0000
[2019-04-09 14:51:22,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1461
[2019-04-09 14:51:22,796] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.02200270151123, 0.6436200115393623, 0.0, 1.0, 65.0, 53.22872200029477], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2942400.0000, 
sim time next is 2943000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.07606117272086, 0.6500872957570127, 0.0, 1.0, 25.0, 49.80559234257952], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6730050977267382, 0.7166957652523376, 0.0, 1.0, 0.2, 0.49805592342579524], 
reward next is 0.5019, 
noisyNet noise sample is [array([0.7145271], dtype=float32), -1.6642591]. 
=============================================
[2019-04-09 14:51:22,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.0926797]
 [1.0302005]
 [1.1048882]
 [1.2715521]
 [1.2160444]], R is [[1.4728992 ]
 [1.92588294]
 [2.44851375]
 [2.79699039]
 [2.92327404]].
[2019-04-09 14:51:23,102] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03387121 0.11873    0.08628161 0.08536505 0.06824311 0.0462239
 0.13527998 0.08614387 0.08087353 0.08544522 0.17354243], sum to 1.0000
[2019-04-09 14:51:23,104] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2634
[2019-04-09 14:51:23,114] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 25.38723692577178, 0.4704518937384153, 0.0, 1.0, 45.0, 32.57569148968994], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2944800.0000, 
sim time next is 2945400.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 19.0, 25.32352385328749, 0.4547015772388991, 0.0, 1.0, 30.0, 29.30261691198973], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.08333333333333333, 0.6102936544406242, 0.6515671924129663, 0.0, 1.0, 0.3, 0.2930261691198973], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.31010994], dtype=float32), -0.55778205]. 
=============================================
[2019-04-09 14:51:23,115] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0383241  0.10888484 0.08770236 0.07697    0.06538099 0.04379845
 0.1386989  0.0965082  0.08124654 0.09657853 0.1659071 ], sum to 1.0000
[2019-04-09 14:51:23,115] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7863
[2019-04-09 14:51:23,129] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 19.0, 25.32352385328749, 0.4547015772388991, 0.0, 1.0, 30.0, 29.30261691198973], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2945400.0000, 
sim time next is 2946000.0000, 
raw observation next is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 19.0, 25.27616845077618, 0.4524894850191616, 0.0, 1.0, 60.0, 57.53128668861891], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6063473708980149, 0.6508298283397206, 0.0, 1.0, 0.9, 0.5753128668861891], 
reward next is 0.4247, 
noisyNet noise sample is [array([0.31010994], dtype=float32), -0.55778205]. 
=============================================
[2019-04-09 14:51:23,138] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.1026306]
 [1.1182592]
 [1.0803682]
 [1.0822151]
 [1.165286 ]], R is [[1.35865414]
 [2.05204153]
 [2.70576429]
 [3.32638478]
 [3.85449409]].
[2019-04-09 14:51:23,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04028323 0.1290034  0.09954526 0.09379574 0.07782804 0.05310296
 0.11616084 0.0758237  0.0878306  0.09147815 0.1351481 ], sum to 1.0000
[2019-04-09 14:51:23,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7945
[2019-04-09 14:51:23,380] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 19.0, 25.29493290966685, 0.4611740132930495, 0.0, 1.0, 35.0, 45.08123620302982], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 19.0, 25.30283994186388, 0.4616647447849693, 0.0, 1.0, 60.0, 49.86445319554213], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.08333333333333333, 0.6085699951553233, 0.6538882482616565, 0.0, 1.0, 0.9, 0.49864453195542136], 
reward next is 0.5014, 
noisyNet noise sample is [array([-0.42147365], dtype=float32), -1.2727824]. 
=============================================
[2019-04-09 14:51:24,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03620512 0.12278254 0.09938253 0.08871634 0.08561951 0.04614062
 0.12161628 0.07354347 0.09077194 0.10133471 0.13388692], sum to 1.0000
[2019-04-09 14:51:24,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1728
[2019-04-09 14:51:24,421] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 19.0, 24.92631473130346, 0.4039994102703324, 0.0, 1.0, 20.0, 48.60497370586623], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 19.0, 24.94590987215267, 0.410457573594926, 0.0, 1.0, 25.0, 38.49921653577142], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.08333333333333333, 0.5788258226793891, 0.6368191911983087, 0.0, 1.0, 0.2, 0.3849921653577142], 
reward next is 0.6150, 
noisyNet noise sample is [array([0.65721416], dtype=float32), 0.4971273]. 
=============================================
[2019-04-09 14:51:24,553] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03766216 0.14372902 0.08435438 0.1051264  0.07549898 0.04358479
 0.1078781  0.08914466 0.09260588 0.10294278 0.11747291], sum to 1.0000
[2019-04-09 14:51:24,556] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-09 14:51:24,574] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 243.0, 240.6666666666667, 19.0, 24.39882639472519, 0.2660163834267239, 0.0, 1.0, 25.0, 32.11816134459133], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2978400.0000, 
sim time next is 2979000.0000, 
raw observation next is [-3.0, 65.0, 256.0, 284.0, 19.0, 24.37874742017503, 0.2620434272402544, 0.0, 1.0, 20.0, 30.04147795258935], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8533333333333334, 0.3138121546961326, 0.08333333333333333, 0.5315622850145859, 0.5873478090800849, 0.0, 1.0, 0.1, 0.3004147795258935], 
reward next is 0.6996, 
noisyNet noise sample is [array([0.7233667], dtype=float32), -0.31239203]. 
=============================================
[2019-04-09 14:51:24,579] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.86520433]
 [0.92044115]
 [0.94143605]
 [0.8856609 ]
 [0.92023575]], R is [[1.61035609]
 [2.27307105]
 [2.91394567]
 [3.47242641]
 [3.85032988]].
[2019-04-09 14:51:24,776] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79028: loss 23.1260
[2019-04-09 14:51:24,779] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03756997 0.10143382 0.09985023 0.08524632 0.06464273 0.04995308
 0.1163552  0.08285365 0.1088973  0.1125875  0.1406102 ], sum to 1.0000
[2019-04-09 14:51:24,779] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-09 14:51:24,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79031: learning rate 0.0000
[2019-04-09 14:51:24,866] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 19.0, 26.23360441302906, 0.6385608115246513, 0.0, 1.0, 50.0, 46.0550573376519], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.23095815087331, 0.6305992478412663, 0.0, 1.0, 55.0, 41.53490243534637], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6859131792394425, 0.7101997492804221, 0.0, 1.0, 0.8, 0.41534902435346366], 
reward next is 0.5847, 
noisyNet noise sample is [array([0.10195841], dtype=float32), 1.8812878]. 
=============================================
[2019-04-09 14:51:24,977] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03502449 0.11878076 0.0968435  0.08927072 0.07519012 0.04966542
 0.11385863 0.08511452 0.09626816 0.09540011 0.14458361], sum to 1.0000
[2019-04-09 14:51:24,977] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-09 14:51:24,993] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 71.0, 119.0, 90.5, 19.0, 24.81887152695751, 0.3496342663592364, 0.0, 1.0, 30.0, 38.88625340643183], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 19.0, 24.77347299664436, 0.3350708469763381, 0.0, 1.0, 20.0, 36.49912795386808], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4355555555555557, 0.11528545119705337, 0.08333333333333333, 0.5644560830536968, 0.6116902823254461, 0.0, 1.0, 0.1, 0.3649912795386808], 
reward next is 0.6350, 
noisyNet noise sample is [array([0.50629514], dtype=float32), -1.548008]. 
=============================================
[2019-04-09 14:51:25,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03684774 0.13017604 0.08479793 0.10607303 0.0761316  0.04733437
 0.12102964 0.07560822 0.09107527 0.09954245 0.13138372], sum to 1.0000
[2019-04-09 14:51:25,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3139
[2019-04-09 14:51:25,084] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 158.0, 114.0, 19.0, 24.46980267925381, 0.3093671511947338, 0.0, 1.0, 55.0, 49.79767421005941], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [-4.0, 71.0, 162.0, 96.00000000000001, 19.0, 24.47183093031167, 0.3115737647251856, 0.0, 1.0, 30.0, 45.75024864516382], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.54, 0.10607734806629836, 0.08333333333333333, 0.539319244192639, 0.6038579215750618, 0.0, 1.0, 0.3, 0.4575024864516382], 
reward next is 0.5425, 
noisyNet noise sample is [array([0.25189745], dtype=float32), -2.5488806]. 
=============================================
[2019-04-09 14:51:25,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[0.9715984 ]
 [0.8879453 ]
 [0.85665274]
 [1.0094764 ]
 [0.9383452 ]], R is [[1.53272259]
 [2.01941872]
 [2.64189816]
 [3.02403164]
 [3.99379134]].
[2019-04-09 14:51:25,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79317: loss 28.6046
[2019-04-09 14:51:25,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79318: learning rate 0.0000
[2019-04-09 14:51:25,501] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79353: loss 29.3145
[2019-04-09 14:51:25,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79353: learning rate 0.0000
[2019-04-09 14:51:25,726] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79441: loss 22.8641
[2019-04-09 14:51:25,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79441: learning rate 0.0000
[2019-04-09 14:51:25,814] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 79454: loss 24.7933
[2019-04-09 14:51:25,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79454: loss 25.0507
[2019-04-09 14:51:25,822] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 79454: learning rate 0.0000
[2019-04-09 14:51:25,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79454: learning rate 0.0000
[2019-04-09 14:51:26,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03145245 0.13448897 0.10420811 0.09677919 0.06704234 0.05280106
 0.09567787 0.07566082 0.09029054 0.0984409  0.15315777], sum to 1.0000
[2019-04-09 14:51:26,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4203
[2019-04-09 14:51:26,325] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 59.16666666666666, 89.0, 695.0, 19.0, 25.86075205566894, 0.6107581718744223, 0.0, 1.0, 45.0, 26.42835095927628], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2992200.0000, 
sim time next is 2992800.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 86.0, 681.0, 19.0, 25.84198235888638, 0.6046206612476243, 0.0, 1.0, 35.0, 25.30267043190872], 
processed observation next is [0.0, 0.6521739130434783, 0.4164358264081256, 0.5833333333333335, 0.2866666666666667, 0.7524861878453039, 0.08333333333333333, 0.6534985299071984, 0.7015402204158748, 0.0, 1.0, 0.4, 0.2530267043190872], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.2444553], dtype=float32), -1.4133346]. 
=============================================
[2019-04-09 14:51:26,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03591533 0.11886557 0.08613671 0.08227988 0.07790891 0.04188628
 0.12531057 0.08548242 0.08420469 0.11579459 0.14621498], sum to 1.0000
[2019-04-09 14:51:26,495] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4253
[2019-04-09 14:51:26,508] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 19.0, 25.77237958845417, 0.5543415391214029, 0.0, 1.0, 20.0, 36.0655906570768], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 19.0, 25.83651608129295, 0.5543052791045667, 0.0, 1.0, 40.0, 35.52864958654903], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6530430067744124, 0.6847684263681889, 0.0, 1.0, 0.5, 0.35528649586549027], 
reward next is 0.6447, 
noisyNet noise sample is [array([0.34726083], dtype=float32), 0.21160477]. 
=============================================
[2019-04-09 14:51:26,616] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79827: loss 23.8635
[2019-04-09 14:51:26,618] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79830: learning rate 0.0000
[2019-04-09 14:51:26,703] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 79841: loss 23.0050
[2019-04-09 14:51:26,705] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 79841: learning rate 0.0000
[2019-04-09 14:51:26,790] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79854: loss 29.9391
[2019-04-09 14:51:26,797] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79854: learning rate 0.0000
[2019-04-09 14:51:27,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03497006 0.12176616 0.0973229  0.09526874 0.07365654 0.04609584
 0.11460588 0.0772666  0.08527118 0.10495062 0.14882554], sum to 1.0000
[2019-04-09 14:51:27,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6469
[2019-04-09 14:51:27,082] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 19.0, 24.01370921536554, 0.1983309659125391, 0.0, 1.0, 20.0, 63.20807321949891], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2970600.0000, 
sim time next is 2971200.0000, 
raw observation next is [-4.0, 71.0, 142.3333333333333, 118.1666666666667, 19.0, 23.94940756803383, 0.220338746585586, 0.0, 1.0, 50.0, 47.91615186794991], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4744444444444443, 0.13057090239410685, 0.08333333333333333, 0.4957839640028192, 0.573446248861862, 0.0, 1.0, 0.7, 0.4791615186794991], 
reward next is 0.5208, 
noisyNet noise sample is [array([0.6473222], dtype=float32), -0.29501963]. 
=============================================
[2019-04-09 14:51:27,125] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 14:51:27,128] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:51:27,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:51:27,131] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:51:27,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02958412 0.10992113 0.10544287 0.08383617 0.07841737 0.04497835
 0.12636551 0.07132581 0.08572849 0.1003382  0.16406198], sum to 1.0000
[2019-04-09 14:51:27,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-09 14:51:27,132] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:51:27,133] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:51:27,133] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:51:27,137] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run9
[2019-04-09 14:51:27,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run9
[2019-04-09 14:51:27,138] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run9
[2019-04-09 14:51:27,152] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.09513162614569, 0.5964172480606447, 0.0, 1.0, 20.0, 42.65823208002229], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.15322156912958, 0.5952653726661941, 0.0, 1.0, 30.0, 36.52217836469492], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.08333333333333333, 0.6794351307607984, 0.6984217908887315, 0.0, 1.0, 0.3, 0.36522178364694924], 
reward next is 0.6348, 
noisyNet noise sample is [array([0.73640144], dtype=float32), 0.19907182]. 
=============================================
[2019-04-09 14:51:54,095] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01029797], dtype=float32), 0.015010224]
[2019-04-09 14:51:54,096] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [7.631069103, 100.0, 0.0, 0.0, 19.0, 27.59477397983348, 1.157631232376823, 0.0, 1.0, 25.0, 26.84167686181054]
[2019-04-09 14:51:54,096] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:51:54,096] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.02903779 0.12394152 0.09911688 0.07907746 0.07295671 0.0400039
 0.13871323 0.07903119 0.08041088 0.10684583 0.1508646 ], sampled 0.11257678525518255
[2019-04-09 14:52:45,567] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01029797], dtype=float32), 0.015010224]
[2019-04-09 14:52:45,567] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-3.833333333333333, 76.0, 0.0, 0.0, 19.0, 26.71608569363247, 0.7031580846303891, 0.0, 1.0, 45.0, 39.33659326944087]
[2019-04-09 14:52:45,567] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:52:45,568] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.03599357 0.12147126 0.09365638 0.0861934  0.07135517 0.04939791
 0.1312584  0.07222785 0.09513824 0.08525962 0.15804818], sampled 0.36652855247927296
[2019-04-09 14:52:56,462] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5683.1549 280357.9490 2741.6867
[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,482] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:52:56,613] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,832] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5376.0179 312005.6521 1817.1877
[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,853] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:01,965] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,253] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5461.5448 303450.3690 2293.0121
[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:04,383] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:53:05,275] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 80000, evaluation results [80000.0, 5461.544836303219, 303450.36899639404, 2293.012124094194, 5683.154901406753, 280357.9489803338, 2741.6866978173266, 5376.017900047548, 312005.6521404075, 1817.1876583181922]
[2019-04-09 14:53:05,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03107095 0.12196156 0.0967397  0.08529472 0.07382236 0.0407325
 0.14740978 0.07757451 0.09064762 0.07809215 0.15665421], sum to 1.0000
[2019-04-09 14:53:05,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9636
[2019-04-09 14:53:05,418] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 25.26436705435628, 0.3663798593472752, 0.0, 1.0, 55.0, 38.94302030484249], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 19.0, 25.2004996364863, 0.37908507818077, 0.0, 1.0, 65.0, 64.03938176287843], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.08333333333333333, 0.6000416363738582, 0.6263616927269233, 0.0, 1.0, 1.0, 0.6403938176287843], 
reward next is 0.3596, 
noisyNet noise sample is [array([0.62240255], dtype=float32), -0.9436179]. 
=============================================
[2019-04-09 14:53:05,440] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80101: loss 24.6873
[2019-04-09 14:53:05,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80101: learning rate 0.0000
[2019-04-09 14:53:05,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03271999 0.12182517 0.09621893 0.10016273 0.07413817 0.05198341
 0.11204466 0.06962302 0.09277498 0.10151682 0.14699215], sum to 1.0000
[2019-04-09 14:53:05,559] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0470
[2019-04-09 14:53:05,569] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 25.23604780346119, 0.4218204117390145, 0.0, 1.0, 65.0, 57.64568547942857], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2951400.0000, 
sim time next is 2952000.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 25.1349936295717, 0.416659360341674, 0.0, 1.0, 20.0, 44.95973790500241], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.5945828024643083, 0.6388864534472246, 0.0, 1.0, 0.1, 0.4495973790500241], 
reward next is 0.5504, 
noisyNet noise sample is [array([-0.39971986], dtype=float32), 0.055091474]. 
=============================================
[2019-04-09 14:53:05,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.9196743 ]
 [0.87759316]
 [0.9004111 ]
 [0.960335  ]
 [0.9248549 ]], R is [[1.45400953]
 [1.86301255]
 [2.54769468]
 [3.21109223]
 [3.85139942]].
[2019-04-09 14:53:05,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02758677 0.11888763 0.11079268 0.11341292 0.06758852 0.04492835
 0.10464894 0.07632128 0.09073856 0.09124023 0.15385416], sum to 1.0000
[2019-04-09 14:53:05,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3083
[2019-04-09 14:53:05,671] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80180: loss 27.5805
[2019-04-09 14:53:05,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80181: learning rate 0.0000
[2019-04-09 14:53:05,752] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 19.0, 26.29200259049569, 0.6981016908327417, 0.0, 1.0, 25.0, 30.76632111761625], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2989200.0000, 
sim time next is 2989800.0000, 
raw observation next is [-2.0, 60.0, 101.0, 751.0, 19.0, 26.33894491089188, 0.6944055392850612, 0.0, 1.0, 30.0, 29.53206253330048], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.33666666666666667, 0.8298342541436464, 0.08333333333333333, 0.6949120759076566, 0.7314685130950204, 0.0, 1.0, 0.3, 0.2953206253330048], 
reward next is 0.7047, 
noisyNet noise sample is [array([-0.7166964], dtype=float32), 1.6846958]. 
=============================================
[2019-04-09 14:53:05,796] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80206: loss 25.5643
[2019-04-09 14:53:05,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80206: learning rate 0.0000
[2019-04-09 14:53:05,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0308684  0.11784841 0.0975669  0.08119045 0.07424507 0.05267517
 0.13236114 0.07301924 0.0810956  0.10014324 0.15898637], sum to 1.0000
[2019-04-09 14:53:05,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8657
[2019-04-09 14:53:05,944] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 25.06958123391692, 0.4694095403844727, 0.0, 1.0, 65.0, 64.3060702826035], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3001200.0000, 
sim time next is 3001800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 19.0, 25.32238532312302, 0.5094399779013031, 0.0, 1.0, 65.0, 59.84326446678576], 
processed observation next is [0.0, 0.7391304347826086, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.08333333333333333, 0.6101987769269183, 0.669813325967101, 0.0, 1.0, 1.0, 0.5984326446678576], 
reward next is 0.4016, 
noisyNet noise sample is [array([0.13747343], dtype=float32), -0.12036017]. 
=============================================
[2019-04-09 14:53:06,393] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80472: loss 26.3106
[2019-04-09 14:53:06,404] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80474: learning rate 0.0000
[2019-04-09 14:53:06,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02682771 0.12879993 0.09156846 0.07859978 0.08090775 0.04604565
 0.12610662 0.07405602 0.09315265 0.09761965 0.15631577], sum to 1.0000
[2019-04-09 14:53:06,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1436
[2019-04-09 14:53:06,524] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 25.99793720414369, 0.5525871490930828, 0.0, 1.0, 65.0, 55.78705753756737], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3009600.0000, 
sim time next is 3010200.0000, 
raw observation next is [-3.083333333333333, 65.0, 0.0, 0.0, 19.0, 25.91520071493945, 0.5497587045814699, 0.0, 1.0, 20.0, 43.15386715872975], 
processed observation next is [0.0, 0.8695652173913043, 0.3771929824561404, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6596000595782874, 0.6832529015271566, 0.0, 1.0, 0.1, 0.43153867158729753], 
reward next is 0.5685, 
noisyNet noise sample is [array([-0.43536165], dtype=float32), -0.01831639]. 
=============================================
[2019-04-09 14:53:06,569] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03587252 0.11726718 0.09342237 0.10247636 0.08079286 0.05213148
 0.11224043 0.06861327 0.10432749 0.09234234 0.14051367], sum to 1.0000
[2019-04-09 14:53:06,572] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-09 14:53:06,586] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 61.5, 83.0, 359.0, 19.0, 25.08002087783165, 0.3484063125946075, 0.0, 1.0, 65.0, 53.02686178089501], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.0, 25.18953530154845, 0.3635563253093297, 0.0, 1.0, 25.0, 47.39865162878307], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6066666666666666, 0.28555555555555556, 0.44751381215469616, 0.08333333333333333, 0.5991279417957042, 0.6211854417697765, 0.0, 1.0, 0.2, 0.47398651628783073], 
reward next is 0.5260, 
noisyNet noise sample is [array([0.6624561], dtype=float32), -1.1380894]. 
=============================================
[2019-04-09 14:53:06,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03978452 0.12050802 0.08560119 0.09845404 0.08079421 0.05640113
 0.1197464  0.06928596 0.10368637 0.08377279 0.14196543], sum to 1.0000
[2019-04-09 14:53:06,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-09 14:53:06,853] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.0, 73.0, 95.66666666666666, 62.83333333333333, 19.0, 24.82734392130957, 0.3583727027783185, 0.0, 1.0, 60.0, 66.7843865077475], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2968800.0000, 
sim time next is 2969400.0000, 
raw observation next is [-4.0, 72.0, 107.3333333333333, 76.66666666666666, 19.0, 24.87222447990944, 0.3797766413252242, 0.0, 1.0, 60.0, 64.75148977881025], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.72, 0.3577777777777777, 0.08471454880294658, 0.08333333333333333, 0.5726853733257867, 0.6265922137750747, 0.0, 1.0, 0.9, 0.6475148977881026], 
reward next is 0.3525, 
noisyNet noise sample is [array([0.42764166], dtype=float32), -0.8351817]. 
=============================================
[2019-04-09 14:53:06,977] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80728: loss 30.8219
[2019-04-09 14:53:06,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80729: learning rate 0.0000
[2019-04-09 14:53:07,179] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02787242 0.11324673 0.09509368 0.07443514 0.07235613 0.04391682
 0.1353351  0.08151373 0.08117714 0.09686205 0.17819105], sum to 1.0000
[2019-04-09 14:53:07,181] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6927
[2019-04-09 14:53:07,203] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 25.63421661818222, 0.4550809897839674, 0.0, 1.0, 35.0, 47.49489849462628], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 19.0, 25.65258776973372, 0.4499326723271751, 0.0, 1.0, 55.0, 34.09321470726036], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.08333333333333333, 0.6377156474778101, 0.6499775574423917, 0.0, 1.0, 0.8, 0.3409321470726036], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.8378525], dtype=float32), -1.0888915]. 
=============================================
[2019-04-09 14:53:07,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0326725  0.11592896 0.09519341 0.08871521 0.07158794 0.05020636
 0.1122293  0.07651738 0.09872929 0.09263108 0.16558865], sum to 1.0000
[2019-04-09 14:53:07,259] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-09 14:53:07,279] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.5, 54.5, 111.0, 805.0, 19.0, 25.79154620975144, 0.5331638043787857, 0.0, 1.0, 65.0, 52.18713738023724], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3065400.0000, 
sim time next is 3066000.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 111.5, 807.0, 19.0, 25.73872507315924, 0.5404541092350127, 0.0, 1.0, 25.0, 37.43459170983836], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.37166666666666665, 0.8917127071823204, 0.08333333333333333, 0.6448937560966034, 0.6801513697450042, 0.0, 1.0, 0.2, 0.3743459170983836], 
reward next is 0.6257, 
noisyNet noise sample is [array([-1.5381864], dtype=float32), -1.1695199]. 
=============================================
[2019-04-09 14:53:07,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[1.213802 ]
 [1.1015928]
 [1.0306753]
 [1.0527439]
 [1.0961688]], R is [[1.63488317]
 [2.096663  ]
 [2.78528142]
 [3.44435692]
 [4.11766338]].
[2019-04-09 14:53:07,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03816601 0.12729035 0.09081245 0.09844868 0.07579455 0.04526305
 0.10991444 0.08652904 0.09927689 0.09747842 0.13102618], sum to 1.0000
[2019-04-09 14:53:07,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1652
[2019-04-09 14:53:07,496] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.333333333333333, 54.83333333333333, 101.3333333333333, 676.3333333333333, 19.0, 23.84932133764431, 0.1054139191471494, 0.0, 1.0, 20.0, 28.79333674485397], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3059400.0000, 
sim time next is 3060000.0000, 
raw observation next is [-4.0, 54.0, 102.5, 697.0, 19.0, 23.93897891976817, 0.1120900506459236, 0.0, 1.0, 20.0, 27.34989864226807], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.3416666666666667, 0.7701657458563536, 0.08333333333333333, 0.49491490998068094, 0.5373633502153079, 0.0, 1.0, 0.1, 0.27349898642268067], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.7239465], dtype=float32), -0.90951943]. 
=============================================
[2019-04-09 14:53:07,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.0165133]
 [0.8746017]
 [0.8734472]
 [0.8955765]
 [0.8733221]], R is [[1.62412596]
 [2.31995153]
 [2.94347525]
 [3.40343142]
 [4.10432625]].
[2019-04-09 14:53:07,514] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03741436 0.132089   0.095038   0.10264912 0.06858777 0.07055265
 0.10440428 0.0675065  0.08021137 0.07800957 0.16353738], sum to 1.0000
[2019-04-09 14:53:07,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0166
[2019-04-09 14:53:07,527] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.833333333333333, 48.66666666666667, 110.6666666666667, 809.6666666666666, 19.0, 26.06166768471203, 0.5912532172149693, 0.0, 1.0, 50.0, 25.24214847642406], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3071400.0000, 
sim time next is 3072000.0000, 
raw observation next is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 19.0, 26.04790764808719, 0.5381584079840569, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4164358264081256, 0.47333333333333344, 0.366111111111111, 0.892633517495396, 0.08333333333333333, 0.6706589706739324, 0.6793861359946857, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3873507], dtype=float32), -2.3658924]. 
=============================================
[2019-04-09 14:53:07,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[1.1116956]
 [1.2160472]
 [0.9963919]
 [1.1062587]
 [1.1738223]], R is [[2.09683466]
 [2.82344484]
 [3.53042078]
 [4.21535683]
 [4.87864494]].
[2019-04-09 14:53:07,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04200428 0.11135565 0.09212747 0.0951584  0.08346225 0.06527647
 0.12046451 0.0688812  0.08721875 0.08874768 0.14530335], sum to 1.0000
[2019-04-09 14:53:07,655] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-09 14:53:07,680] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 24.278094205369, 0.1910693164590916, 0.0, 1.0, 65.0, 70.5783310361227], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 24.32223939804024, 0.2385432162472781, 0.0, 1.0, 65.0, 66.73230256532197], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5268532831700199, 0.5795144054157594, 0.0, 1.0, 1.0, 0.6673230256532198], 
reward next is 0.3327, 
noisyNet noise sample is [array([-0.71974665], dtype=float32), 1.4070678]. 
=============================================
[2019-04-09 14:53:07,791] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02883629 0.1274712  0.09362797 0.08591948 0.08730382 0.04371801
 0.14044093 0.08929976 0.08289806 0.08249985 0.13798474], sum to 1.0000
[2019-04-09 14:53:07,791] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2099
[2019-04-09 14:53:07,804] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.583333333333333, 65.0, 0.0, 0.0, 19.0, 25.55362236358118, 0.4699550427784165, 0.0, 1.0, 45.0, 48.86781889858637], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [-3.666666666666667, 65.0, 0.0, 0.0, 19.0, 25.63175901689382, 0.4684500440456307, 0.0, 1.0, 25.0, 36.11272877432826], 
processed observation next is [0.0, 0.9130434782608695, 0.3610341643582641, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6359799180744851, 0.656150014681877, 0.0, 1.0, 0.2, 0.3611272877432826], 
reward next is 0.6389, 
noisyNet noise sample is [array([0.9461736], dtype=float32), -3.3973014]. 
=============================================
[2019-04-09 14:53:07,968] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03946678 0.12227693 0.09633968 0.08885526 0.082516   0.05587526
 0.10673656 0.07697428 0.08444993 0.09486994 0.15163934], sum to 1.0000
[2019-04-09 14:53:07,973] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9108
[2019-04-09 14:53:07,974] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 81219: loss 30.5805
[2019-04-09 14:53:07,977] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 81222: learning rate 0.0000
[2019-04-09 14:53:08,059] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 19.0, 25.56763466117697, 0.4916502963224792, 0.0, 1.0, 25.0, 40.9515539706155], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 19.0, 25.72403307927952, 0.5181679443572911, 0.0, 1.0, 65.0, 43.77485740014848], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.358888888888889, 0.8556169429097605, 0.08333333333333333, 0.6436694232732932, 0.672722648119097, 0.0, 1.0, 1.0, 0.43774857400148476], 
reward next is 0.5623, 
noisyNet noise sample is [array([-0.7112881], dtype=float32), -1.4016721]. 
=============================================
[2019-04-09 14:53:08,069] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.77279276]
 [0.83168703]
 [0.78594834]
 [0.89401823]
 [0.7387238 ]], R is [[1.51590931]
 [2.09123468]
 [2.56853628]
 [3.00088906]
 [3.42398882]].
[2019-04-09 14:53:08,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04082841 0.12208023 0.09595323 0.09813187 0.07389264 0.05219948
 0.10812297 0.07320867 0.10313303 0.09086212 0.14158739], sum to 1.0000
[2019-04-09 14:53:08,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-09 14:53:08,181] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 24.92717968103577, 0.2981902956419718, 0.0, 1.0, 35.0, 41.45410768911316], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3039000.0000, 
sim time next is 3039600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 24.9364926183387, 0.289115361111667, 0.0, 1.0, 20.0, 34.57432625918402], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.578041051528225, 0.5963717870372224, 0.0, 1.0, 0.1, 0.3457432625918402], 
reward next is 0.6543, 
noisyNet noise sample is [array([-0.51406103], dtype=float32), -0.8756099]. 
=============================================
[2019-04-09 14:53:08,554] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 81478: loss 27.6425
[2019-04-09 14:53:08,557] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 81478: learning rate 0.0000
[2019-04-09 14:53:08,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.03571778 0.11922373 0.09928877 0.11327776 0.07443212 0.05938278
 0.1109439  0.07106381 0.09533883 0.09082089 0.13050964], sum to 1.0000
[2019-04-09 14:53:08,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6874
[2019-04-09 14:53:08,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 19.0, 24.86265935670384, 0.2799207540048716, 0.0, 1.0, 50.0, 32.51190592428096], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3055800.0000, 
sim time next is 3056400.0000, 
raw observation next is [-6.0, 59.0, 91.0, 497.0, 19.0, 24.84387141613673, 0.2809105290219823, 0.0, 1.0, 55.0, 39.03257460217703], 
processed observation next is [0.0, 0.391304347826087, 0.296398891966759, 0.59, 0.30333333333333334, 0.549171270718232, 0.08333333333333333, 0.5703226180113941, 0.5936368430073274, 0.0, 1.0, 0.8, 0.3903257460217703], 
reward next is 0.6097, 
noisyNet noise sample is [array([-1.4060198], dtype=float32), -0.6760083]. 
=============================================
[2019-04-09 14:53:08,884] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03349819 0.11576574 0.09017851 0.08005288 0.06826547 0.05370643
 0.12939861 0.08719507 0.10020588 0.10339841 0.1383348 ], sum to 1.0000
[2019-04-09 14:53:08,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5766
[2019-04-09 14:53:08,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03336276 0.14073005 0.09666311 0.09987542 0.07032859 0.05030714
 0.10390718 0.07233917 0.09404889 0.08777382 0.15066381], sum to 1.0000
[2019-04-09 14:53:08,908] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3050
[2019-04-09 14:53:08,919] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 55.0, 112.5, 811.0, 19.0, 25.34138507065173, 0.4603627624192595, 0.0, 1.0, 25.0, 22.6297715792271], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3067200.0000, 
sim time next is 3067800.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 113.0, 813.0, 19.0, 25.35931547178721, 0.4596346444323484, 0.0, 1.0, 30.0, 23.15998600701579], 
processed observation next is [0.0, 0.5217391304347826, 0.3841181902123731, 0.5416666666666667, 0.37666666666666665, 0.8983425414364641, 0.08333333333333333, 0.6132762893156007, 0.6532115481441162, 0.0, 1.0, 0.3, 0.2315998600701579], 
reward next is 0.7684, 
noisyNet noise sample is [array([-1.5235995], dtype=float32), -0.26957554]. 
=============================================
[2019-04-09 14:53:08,923] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 73.5, 0.0, 0.0, 19.0, 25.14852934661151, 0.3004948164192169, 0.0, 1.0, 40.0, 29.61731559134727], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3043800.0000, 
sim time next is 3044400.0000, 
raw observation next is [-6.0, 72.33333333333333, 0.0, 0.0, 19.0, 25.05263830250956, 0.2778456310571023, 0.0, 1.0, 50.0, 33.71107466728601], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7233333333333333, 0.0, 0.0, 0.08333333333333333, 0.5877198585424633, 0.5926152103523674, 0.0, 1.0, 0.7, 0.3371107466728601], 
reward next is 0.6629, 
noisyNet noise sample is [array([0.08794221], dtype=float32), 1.2820832]. 
=============================================
[2019-04-09 14:53:08,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.036472   0.11908028 0.10045388 0.10156705 0.07849832 0.0544731
 0.10887189 0.07504866 0.09209    0.08389045 0.1495544 ], sum to 1.0000
[2019-04-09 14:53:08,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2790
[2019-04-09 14:53:09,000] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.166666666666667, 50.83333333333334, 112.3333333333333, 813.3333333333334, 19.0, 25.37806221386316, 0.434658445251307, 0.0, 1.0, 50.0, 28.86351792537961], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3070200.0000, 
sim time next is 3070800.0000, 
raw observation next is [-2.0, 50.0, 111.5, 811.5, 19.0, 25.34326703499782, 0.432650255001824, 0.0, 1.0, 30.0, 25.64792890822483], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.37166666666666665, 0.8966850828729281, 0.08333333333333333, 0.6119389195831516, 0.6442167516672747, 0.0, 1.0, 0.3, 0.2564792890822483], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.15948474], dtype=float32), -1.4736916]. 
=============================================
[2019-04-09 14:53:09,280] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03364103 0.1281169  0.11879279 0.07616258 0.06791697 0.04174297
 0.12773499 0.07170866 0.07182553 0.10814539 0.1542122 ], sum to 1.0000
[2019-04-09 14:53:09,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1974
[2019-04-09 14:53:09,310] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 19.0, 26.33648208113681, 0.6083977398763243, 0.0, 1.0, 30.0, 37.62542521322255], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 19.0, 26.30844444000017, 0.6010003410658349, 0.0, 1.0, 25.0, 35.94046775019513], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6923703700000141, 0.700333447021945, 0.0, 1.0, 0.2, 0.3594046775019513], 
reward next is 0.6406, 
noisyNet noise sample is [array([-0.5618294], dtype=float32), 0.04378262]. 
=============================================
[2019-04-09 14:53:09,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03113226 0.1122441  0.11289526 0.08777955 0.07898174 0.05308302
 0.12986727 0.06871304 0.09271322 0.10179973 0.13079078], sum to 1.0000
[2019-04-09 14:53:09,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2522
[2019-04-09 14:53:09,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.14983747340503, 0.5381050382686194, 0.0, 1.0, 45.0, 38.30833438579663], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3093600.0000, 
sim time next is 3094200.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.15890683744853, 0.5273506400797646, 0.0, 1.0, 25.0, 35.91469792695857], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6799089031207108, 0.6757835466932548, 0.0, 1.0, 0.2, 0.35914697926958566], 
reward next is 0.6409, 
noisyNet noise sample is [array([0.22550282], dtype=float32), -0.8313749]. 
=============================================
[2019-04-09 14:53:09,576] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03025878 0.13433617 0.09456624 0.09133823 0.08112826 0.03900114
 0.14258942 0.06602161 0.08573337 0.08285832 0.15216853], sum to 1.0000
[2019-04-09 14:53:09,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1648
[2019-04-09 14:53:09,600] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.11445188566947, 0.5604764569642482, 0.0, 1.0, 65.0, 55.65172107941628], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3018000.0000, 
sim time next is 3018600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.10956517886071, 0.5600326956709819, 0.0, 1.0, 25.0, 46.12463737467148], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6757970982383924, 0.6866775652236606, 0.0, 1.0, 0.2, 0.4612463737467148], 
reward next is 0.5388, 
noisyNet noise sample is [array([-0.11515576], dtype=float32), -0.5922847]. 
=============================================
[2019-04-09 14:53:09,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02723001 0.11759413 0.10364937 0.11065146 0.07157676 0.05256596
 0.10500275 0.07647486 0.09178336 0.08022889 0.16324241], sum to 1.0000
[2019-04-09 14:53:09,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03895424 0.13678966 0.10632571 0.09499761 0.08300684 0.05889302
 0.09964182 0.08165409 0.08185425 0.09876271 0.11912005], sum to 1.0000
[2019-04-09 14:53:09,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1265
[2019-04-09 14:53:09,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-09 14:53:09,740] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 19.0, 26.09100683250144, 0.6327772756064208, 0.0, 1.0, 35.0, 26.02756076091593], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 19.0, 26.19119386485557, 0.6456390508964357, 0.0, 1.0, 20.0, 27.15532797192298], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.08333333333333333, 0.6825994887379642, 0.7152130169654786, 0.0, 1.0, 0.1, 0.27155327971922977], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.9570279], dtype=float32), 1.1065398]. 
=============================================
[2019-04-09 14:53:09,769] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 19.0, 24.45957334996515, 0.2781112607245941, 0.0, 1.0, 25.0, 35.95573385160611], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 19.0, 24.60361953375977, 0.3097399714594236, 0.0, 1.0, 55.0, 39.43081272441405], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.08333333333333333, 0.550301627813314, 0.6032466571531412, 0.0, 1.0, 0.8, 0.39430812724414044], 
reward next is 0.6057, 
noisyNet noise sample is [array([-0.24248898], dtype=float32), 0.8745867]. 
=============================================
[2019-04-09 14:53:09,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02977414 0.11332918 0.10837509 0.07607566 0.06934265 0.04180765
 0.12412054 0.07813931 0.07677942 0.1163859  0.16587047], sum to 1.0000
[2019-04-09 14:53:09,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7114
[2019-04-09 14:53:09,867] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 19.0, 26.21735109105991, 0.5780596390337837, 0.0, 1.0, 65.0, 59.20956540048792], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 19.0, 26.18708078418454, 0.5902082199204014, 0.0, 1.0, 55.0, 49.73638374433495], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.6822567320153784, 0.6967360733068005, 0.0, 1.0, 0.8, 0.4973638374433495], 
reward next is 0.5026, 
noisyNet noise sample is [array([-0.558706], dtype=float32), -0.79126495]. 
=============================================
[2019-04-09 14:53:09,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03127374 0.1210184  0.11883906 0.09666454 0.07239452 0.05734228
 0.10322724 0.07734678 0.08955495 0.08786415 0.1444743 ], sum to 1.0000
[2019-04-09 14:53:09,968] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2903
[2019-04-09 14:53:09,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 40.5, 99.0, 775.0, 19.0, 25.66514208617924, 0.5136615414277229, 0.0, 1.0, 20.0, 26.99871495551116], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3076200.0000, 
sim time next is 3076800.0000, 
raw observation next is [-0.3333333333333334, 40.0, 96.5, 758.0, 19.0, 25.68996287873794, 0.5132593752719642, 0.0, 1.0, 30.0, 25.57810448836022], 
processed observation next is [0.0, 0.6086956521739131, 0.4533702677747, 0.4, 0.32166666666666666, 0.8375690607734807, 0.08333333333333333, 0.6408302398948283, 0.6710864584239881, 0.0, 1.0, 0.3, 0.2557810448836022], 
reward next is 0.7442, 
noisyNet noise sample is [array([-0.22578849], dtype=float32), -0.12747894]. 
=============================================
[2019-04-09 14:53:10,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02870938 0.12860905 0.10349549 0.10053743 0.08325339 0.04753017
 0.13653612 0.06376471 0.07949714 0.0986872  0.12937987], sum to 1.0000
[2019-04-09 14:53:10,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5395
[2019-04-09 14:53:10,055] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 75.33333333333334, 15.33333333333333, 154.3333333333333, 19.0, 26.56500074633206, 0.6546779942958039, 0.0, 1.0, 35.0, 35.00177701479553], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3086400.0000, 
sim time next is 3087000.0000, 
raw observation next is [-0.3, 77.0, 7.0, 88.0, 19.0, 26.57254783589931, 0.6456489651906117, 0.0, 1.0, 45.0, 34.02497500205848], 
processed observation next is [0.0, 0.7391304347826086, 0.4542936288088643, 0.77, 0.023333333333333334, 0.09723756906077348, 0.08333333333333333, 0.7143789863249426, 0.7152163217302038, 0.0, 1.0, 0.6, 0.34024975002058483], 
reward next is 0.6598, 
noisyNet noise sample is [array([0.37814936], dtype=float32), 0.74639297]. 
=============================================
[2019-04-09 14:53:10,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[1.2133179]
 [1.1924586]
 [1.2546284]
 [1.2761722]
 [1.1991246]], R is [[1.95318234]
 [2.58363271]
 [3.23213887]
 [3.83256197]
 [4.2828598 ]].
[2019-04-09 14:53:10,065] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03257808 0.13507324 0.1093568  0.10679874 0.074005   0.05636799
 0.09709548 0.0664776  0.0893002  0.07898206 0.15396489], sum to 1.0000
[2019-04-09 14:53:10,068] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4993
[2019-04-09 14:53:10,080] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 54.83333333333334, 112.0, 809.0, 19.0, 25.82215056415285, 0.5231633603850792, 0.0, 1.0, 45.0, 31.65401539119885], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3066600.0000, 
sim time next is 3067200.0000, 
raw observation next is [-3.0, 55.0, 112.5, 811.0, 19.0, 25.80572005826926, 0.5187287339410882, 0.0, 1.0, 40.0, 29.35714066805353], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.375, 0.8961325966850828, 0.08333333333333333, 0.6504766715224383, 0.6729095779803628, 0.0, 1.0, 0.5, 0.2935714066805353], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.06293931], dtype=float32), 0.31152397]. 
=============================================
[2019-04-09 14:53:10,261] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02872929 0.13691385 0.10959413 0.07942972 0.08217171 0.03821589
 0.12705818 0.07244806 0.07532037 0.09920219 0.15091668], sum to 1.0000
[2019-04-09 14:53:10,262] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1732
[2019-04-09 14:53:10,277] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 26.57868505881001, 0.6269711254685022, 0.0, 1.0, 65.0, 45.11680630789773], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [-0.1666666666666666, 100.0, 0.0, 0.0, 19.0, 26.61538598501663, 0.6229410825649359, 0.0, 1.0, 25.0, 42.56956037030721], 
processed observation next is [0.0, 0.9565217391304348, 0.4579870729455217, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7179488320847192, 0.7076470275216452, 0.0, 1.0, 0.2, 0.4256956037030721], 
reward next is 0.5743, 
noisyNet noise sample is [array([1.1899637], dtype=float32), -0.3702956]. 
=============================================
[2019-04-09 14:53:10,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0259174  0.12302851 0.10459962 0.09284945 0.08069614 0.04282666
 0.13253266 0.06675626 0.09177253 0.09855428 0.14046653], sum to 1.0000
[2019-04-09 14:53:10,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4429
[2019-04-09 14:53:10,404] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.53723508196248, 0.6297011876926072, 0.0, 1.0, 50.0, 45.31309210599822], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.5400783952593, 0.631530155780446, 0.0, 1.0, 40.0, 36.50998508614128], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7116731996049417, 0.7105100519268154, 0.0, 1.0, 0.5, 0.3650998508614128], 
reward next is 0.6349, 
noisyNet noise sample is [array([-0.6164357], dtype=float32), 1.0403051]. 
=============================================
[2019-04-09 14:53:10,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02905411 0.11415063 0.10879391 0.07070158 0.06424783 0.03963363
 0.15270387 0.08112083 0.06460703 0.10883297 0.1661536 ], sum to 1.0000
[2019-04-09 14:53:10,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[1.2387949]
 [1.2625465]
 [1.1712812]
 [1.2090625]
 [1.2124655]], R is [[1.9641695 ]
 [2.4913969 ]
 [2.99333882]
 [3.58243656]
 [4.17124128]].
[2019-04-09 14:53:10,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-09 14:53:10,441] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 26.37866727286642, 0.5937964088923243, 0.0, 1.0, 20.0, 37.50454818715495], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3108600.0000, 
sim time next is 3109200.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 26.40666374451406, 0.5969146904327983, 0.0, 1.0, 65.0, 48.39538428497159], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7005553120428383, 0.6989715634775995, 0.0, 1.0, 1.0, 0.48395384284971593], 
reward next is 0.5160, 
noisyNet noise sample is [array([0.71280956], dtype=float32), 1.0324745]. 
=============================================
[2019-04-09 14:53:10,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02856609 0.10856398 0.11727323 0.0885907  0.06479394 0.03744044
 0.13640773 0.07156762 0.0905593  0.10401317 0.15222372], sum to 1.0000
[2019-04-09 14:53:10,511] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2998
[2019-04-09 14:53:10,526] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 19.0, 26.55037218068724, 0.6364986680612691, 0.0, 1.0, 60.0, 46.04888295012443], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 19.0, 26.5410913697021, 0.6385517128411397, 0.0, 1.0, 30.0, 41.95351193153989], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.7117576141418418, 0.7128505709470465, 0.0, 1.0, 0.3, 0.41953511931539894], 
reward next is 0.5805, 
noisyNet noise sample is [array([-0.9439083], dtype=float32), 0.374848]. 
=============================================
[2019-04-09 14:53:10,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02002646 0.08342721 0.09397621 0.0511808  0.04142357 0.03023694
 0.16382222 0.06730022 0.10897321 0.10116024 0.23847285], sum to 1.0000
[2019-04-09 14:53:10,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-09 14:53:10,631] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.4, 100.0, 0.0, 0.0, 19.0, 25.80569158952307, 0.4292098333762691, 0.0, 1.0, 55.0, 62.23523647959657], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3123600.0000, 
sim time next is 3124200.0000, 
raw observation next is [2.5, 100.0, 0.0, 0.0, 19.0, 25.63186890995786, 0.421907886461297, 0.0, 1.0, 25.0, 48.72750952782865], 
processed observation next is [1.0, 0.13043478260869565, 0.5318559556786704, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6359890758298216, 0.6406359621537657, 0.0, 1.0, 0.2, 0.4872750952782865], 
reward next is 0.5127, 
noisyNet noise sample is [array([0.35366264], dtype=float32), -1.6306441]. 
=============================================
[2019-04-09 14:53:10,673] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03287581 0.07791314 0.12380119 0.04927597 0.05256157 0.04382852
 0.16616693 0.07168869 0.06293117 0.10156815 0.21738881], sum to 1.0000
[2019-04-09 14:53:10,675] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7748
[2019-04-09 14:53:10,690] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 26.09724606390398, 0.4910617145569396, 0.0, 1.0, 30.0, 24.19672352835945], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3117000.0000, 
sim time next is 3117600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 25.98690163937223, 0.4958790829492654, 0.0, 1.0, 60.0, 52.35580386063269], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6655751366143526, 0.6652930276497552, 0.0, 1.0, 0.9, 0.5235580386063269], 
reward next is 0.4764, 
noisyNet noise sample is [array([-1.1895486], dtype=float32), 0.75817364]. 
=============================================
[2019-04-09 14:53:11,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03009321 0.11898728 0.10665996 0.09635292 0.08147368 0.05185485
 0.11709233 0.07526554 0.08039648 0.08990702 0.15191677], sum to 1.0000
[2019-04-09 14:53:11,164] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3874
[2019-04-09 14:53:11,195] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 19.0, 26.80778975753854, 0.6893954030028496, 0.0, 1.0, 40.0, 33.96892549429178], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [-0.5, 80.33333333333334, 0.0, 0.0, 19.0, 26.77549770536409, 0.680834573519847, 0.0, 1.0, 35.0, 33.15592403683537], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7312914754470073, 0.726944857839949, 0.0, 1.0, 0.4, 0.3315592403683537], 
reward next is 0.6684, 
noisyNet noise sample is [array([0.5363934], dtype=float32), -1.3366365]. 
=============================================
[2019-04-09 14:53:11,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0282804  0.14064988 0.08840633 0.07750301 0.07384527 0.03128072
 0.11984443 0.08991797 0.10004806 0.11461703 0.13560696], sum to 1.0000
[2019-04-09 14:53:11,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2481
[2019-04-09 14:53:11,431] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 26.2634007593532, 0.4846779629202595, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [-0.1666666666666666, 100.0, 0.0, 0.0, 19.0, 26.09519318009207, 0.5044359919014516, 0.0, 1.0, 25.0, 56.0675077266625], 
processed observation next is [0.0, 0.9565217391304348, 0.4579870729455217, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6745994316743392, 0.6681453306338172, 0.0, 1.0, 0.2, 0.560675077266625], 
reward next is 0.4393, 
noisyNet noise sample is [array([0.76171446], dtype=float32), -1.2984537]. 
=============================================
[2019-04-09 14:53:11,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02661685 0.11548692 0.10407866 0.07573313 0.06709369 0.0347533
 0.13428894 0.08263635 0.09878128 0.11952865 0.14100236], sum to 1.0000
[2019-04-09 14:53:11,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8566
[2019-04-09 14:53:11,567] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 26.52856059978507, 0.608999392940162, 0.0, 1.0, 25.0, 35.59360175697063], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [-0.1666666666666666, 100.0, 0.0, 0.0, 19.0, 26.55964369982858, 0.6042594447537067, 0.0, 1.0, 60.0, 44.02404905110569], 
processed observation next is [0.0, 0.9565217391304348, 0.4579870729455217, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7133036416523817, 0.7014198149179022, 0.0, 1.0, 0.9, 0.4402404905110569], 
reward next is 0.5598, 
noisyNet noise sample is [array([-0.06556702], dtype=float32), 0.58552384]. 
=============================================
[2019-04-09 14:53:11,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02777557 0.11515515 0.11134221 0.09431905 0.07894447 0.04546335
 0.11854202 0.07291639 0.0810452  0.09192151 0.16257516], sum to 1.0000
[2019-04-09 14:53:11,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0846
[2019-04-09 14:53:11,615] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 56.0, 57.0, 486.0, 19.0, 26.24873266423063, 0.5552608875903708, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3083400.0000, 
sim time next is 3084000.0000, 
raw observation next is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 26.14035751559469, 0.5677724206201084, 0.0, 1.0, 20.0, 42.95880249143298], 
processed observation next is [0.0, 0.6956521739130435, 0.4718374884579871, 0.6133333333333334, 0.16222222222222218, 0.4637200736648251, 0.08333333333333333, 0.6783631262995575, 0.6892574735400361, 0.0, 1.0, 0.1, 0.4295880249143298], 
reward next is 0.5704, 
noisyNet noise sample is [array([-2.3501244], dtype=float32), -0.742893]. 
=============================================
[2019-04-09 14:53:11,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.3402212]
 [1.2744693]
 [1.1686804]
 [1.2773902]
 [1.2021129]], R is [[1.7614572 ]
 [2.7438426 ]
 [3.39657664]
 [3.97915769]
 [4.7001338 ]].
[2019-04-09 14:53:11,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0350882  0.129284   0.09458875 0.08348854 0.0815594  0.06865332
 0.10504408 0.06820689 0.08872212 0.09765113 0.14771362], sum to 1.0000
[2019-04-09 14:53:11,678] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-09 14:53:11,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 26.0626803871531, 0.5561533078672906, 0.0, 1.0, 20.0, 28.08271907788425], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3084600.0000, 
sim time next is 3085200.0000, 
raw observation next is [0.0, 72.0, 32.0, 287.0, 19.0, 26.0351141670923, 0.5359278363114088, 0.0, 1.0, 25.0, 25.99001092933771], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.72, 0.10666666666666667, 0.31712707182320443, 0.08333333333333333, 0.6695928472576916, 0.6786426121038028, 0.0, 1.0, 0.2, 0.2599001092933771], 
reward next is 0.7401, 
noisyNet noise sample is [array([-0.925713], dtype=float32), 0.99365234]. 
=============================================
[2019-04-09 14:53:11,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03009711 0.11205945 0.11481015 0.06497556 0.0534666  0.0397316
 0.15660094 0.06633352 0.08714621 0.09951042 0.17526849], sum to 1.0000
[2019-04-09 14:53:11,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-09 14:53:11,855] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03021225 0.08668517 0.10846512 0.05889355 0.06059325 0.04453703
 0.16930197 0.07561734 0.0682549  0.1015325  0.19590692], sum to 1.0000
[2019-04-09 14:53:11,856] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666667, 100.0, 0.0, 0.0, 19.0, 26.02505881284887, 0.5297037338427116, 0.0, 1.0, 45.0, 33.53292538134054], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3111000.0000, 
sim time next is 3111600.0000, 
raw observation next is [0.3333333333333333, 100.0, 0.0, 0.0, 19.0, 26.03370127148029, 0.5262092895878804, 0.0, 1.0, 35.0, 32.34915438854779], 
processed observation next is [1.0, 0.0, 0.4718374884579871, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6694751059566908, 0.6754030965292935, 0.0, 1.0, 0.4, 0.3234915438854779], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.6247247], dtype=float32), -0.22743185]. 
=============================================
[2019-04-09 14:53:11,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5936
[2019-04-09 14:53:11,869] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 26.16623226736977, 0.516297596359672, 0.0, 1.0, 35.0, 33.03667243025294], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3115200.0000, 
sim time next is 3115800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 26.14336386972196, 0.499013553637351, 0.0, 1.0, 50.0, 31.10888138254468], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6786136558101633, 0.6663378512124504, 0.0, 1.0, 0.7, 0.3110888138254468], 
reward next is 0.6889, 
noisyNet noise sample is [array([0.5884022], dtype=float32), 2.4056118]. 
=============================================
[2019-04-09 14:53:11,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02191781 0.1038909  0.12206586 0.06647716 0.06321526 0.03557296
 0.17531742 0.06664302 0.07917271 0.08822327 0.17750359], sum to 1.0000
[2019-04-09 14:53:11,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2998
[2019-04-09 14:53:11,962] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 25.93028452307631, 0.526888918196775, 0.0, 1.0, 45.0, 36.77511034689081], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3113400.0000, 
sim time next is 3114000.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 26.07232732190942, 0.5287065755143551, 0.0, 1.0, 30.0, 31.80398758314391], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6726939434924516, 0.6762355251714517, 0.0, 1.0, 0.3, 0.3180398758314391], 
reward next is 0.6820, 
noisyNet noise sample is [array([-0.40320766], dtype=float32), -0.33632904]. 
=============================================
[2019-04-09 14:53:11,973] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[1.7926023]
 [1.794085 ]
 [1.5426509]
 [1.513206 ]
 [1.6218739]], R is [[2.25372672]
 [2.86343837]
 [3.2667737 ]
 [4.23410606]
 [4.59845257]].
[2019-04-09 14:53:11,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03454205 0.11181162 0.11581641 0.07974554 0.07268052 0.0585697
 0.11304549 0.08095517 0.07728282 0.10591251 0.14963809], sum to 1.0000
[2019-04-09 14:53:12,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1840
[2019-04-09 14:53:12,016] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 19.0, 25.92219776085426, 0.5750568859345666, 0.0, 1.0, 20.0, 29.76377060681155], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 19.0, 25.97305261971534, 0.5830323911662928, 0.0, 1.0, 25.0, 31.65568465182567], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.08333333333333333, 0.664421051642945, 0.6943441303887643, 0.0, 1.0, 0.2, 0.3165568465182567], 
reward next is 0.6834, 
noisyNet noise sample is [array([1.0393198], dtype=float32), -0.23028253]. 
=============================================
[2019-04-09 14:53:12,036] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[1.1486722]
 [1.0448266]
 [1.0226444]
 [1.1125529]
 [1.0681605]], R is [[1.72902989]
 [2.41410208]
 [2.96677065]
 [3.38601899]
 [4.05075598]].
[2019-04-09 14:53:12,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0195062  0.08624192 0.08891007 0.06925175 0.03876268 0.02364396
 0.17538427 0.07609326 0.10996938 0.09309531 0.21914126], sum to 1.0000
[2019-04-09 14:53:12,211] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1954
[2019-04-09 14:53:12,231] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 19.0, 26.46481856047523, 0.5551759900128044, 0.0, 1.0, 30.0, 50.44774484095579], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [2.1, 100.0, 0.0, 0.0, 19.0, 26.40984012740913, 0.5561417480769132, 0.0, 1.0, 30.0, 55.00499137726725], 
processed observation next is [1.0, 0.13043478260869565, 0.5207756232686982, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7008200106174275, 0.6853805826923044, 0.0, 1.0, 0.3, 0.5500499137726725], 
reward next is 0.4500, 
noisyNet noise sample is [array([-0.33046913], dtype=float32), -0.49202257]. 
=============================================
[2019-04-09 14:53:12,602] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01165449 0.12417857 0.13273777 0.06883921 0.04061128 0.02648149
 0.10532296 0.06400871 0.12551881 0.08450861 0.21613811], sum to 1.0000
[2019-04-09 14:53:12,606] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5665
[2019-04-09 14:53:12,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01949056 0.10135277 0.10730551 0.06064461 0.03437485 0.02359987
 0.14114983 0.09235198 0.12261179 0.12378085 0.17333725], sum to 1.0000
[2019-04-09 14:53:12,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7232
[2019-04-09 14:53:12,620] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.166666666666667, 98.83333333333334, 112.0, 785.3333333333334, 22.5, 28.34724152627737, 1.075203924839666, 1.0, 1.0, 30.0, 14.01486375654032], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3150600.0000, 
sim time next is 3151200.0000, 
raw observation next is [7.333333333333334, 97.66666666666667, 113.0, 795.1666666666666, 22.5, 28.36975927737097, 1.078173144397955, 1.0, 1.0, 50.0, 15.15364087294116], 
processed observation next is [1.0, 0.4782608695652174, 0.6657433056325024, 0.9766666666666667, 0.37666666666666665, 0.8786372007366482, 0.375, 0.8641466064475809, 0.8593910481326517, 1.0, 1.0, 0.7, 0.1515364087294116], 
reward next is 0.8485, 
noisyNet noise sample is [array([-2.519473], dtype=float32), -1.0066366]. 
=============================================
[2019-04-09 14:53:12,638] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [6.0, 100.0, 42.0, 237.0, 22.5, 26.36565573553714, 0.6140759193408373, 1.0, 1.0, 25.0, 41.43668349285272], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3139200.0000, 
sim time next is 3139800.0000, 
raw observation next is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 22.5, 26.51364922885429, 0.6300187522083466, 1.0, 1.0, 60.0, 43.56561080248511], 
processed observation next is [1.0, 0.34782608695652173, 0.6334256694367498, 1.0, 0.18555555555555558, 0.31896869244935544, 0.375, 0.7094707690711909, 0.7100062507361155, 1.0, 1.0, 0.9, 0.4356561080248511], 
reward next is 0.5643, 
noisyNet noise sample is [array([-0.03418538], dtype=float32), -0.51768345]. 
=============================================
[2019-04-09 14:53:12,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02123806 0.11620937 0.10548396 0.07101475 0.04516026 0.03306848
 0.12087881 0.06613646 0.12398038 0.08993306 0.20689648], sum to 1.0000
[2019-04-09 14:53:12,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1960
[2019-04-09 14:53:12,750] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.666666666666666, 100.0, 85.66666666666667, 434.5, 22.5, 27.04773188703954, 0.7257541396562358, 1.0, 1.0, 30.0, 31.32268052395589], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3141600.0000, 
sim time next is 3142200.0000, 
raw observation next is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 22.5, 27.13291221854987, 0.7616288955216607, 1.0, 1.0, 25.0, 31.12418113792209], 
processed observation next is [1.0, 0.34782608695652173, 0.651892890120037, 1.0, 0.29444444444444445, 0.5270718232044199, 0.375, 0.761076018212489, 0.7538762985072203, 1.0, 1.0, 0.2, 0.3112418113792209], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.0168933], dtype=float32), 1.0066856]. 
=============================================
[2019-04-09 14:53:12,941] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02271203 0.09174978 0.08627777 0.05984296 0.03745876 0.02268942
 0.17695478 0.0665379  0.11155239 0.10272917 0.22149506], sum to 1.0000
[2019-04-09 14:53:12,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-09 14:53:12,967] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.34680563943007, 0.5789177316116606, 0.0, 1.0, 65.0, 83.21995677814837], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3125400.0000, 
sim time next is 3126000.0000, 
raw observation next is [2.733333333333333, 100.0, 0.0, 0.0, 19.0, 26.36266414606916, 0.5940840389952458, 0.0, 1.0, 45.0, 57.58646751089108], 
processed observation next is [1.0, 0.17391304347826086, 0.538319482917821, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6968886788390968, 0.6980280129984152, 0.0, 1.0, 0.6, 0.5758646751089108], 
reward next is 0.4241, 
noisyNet noise sample is [array([-0.35730803], dtype=float32), 0.27967066]. 
=============================================
[2019-04-09 14:53:12,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.688474 ]
 [1.6200027]
 [1.5533398]
 [1.6175922]
 [1.5607884]], R is [[2.10526133]
 [2.25200915]
 [2.5114522 ]
 [2.97753215]
 [3.48796368]].
[2019-04-09 14:53:13,164] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02283594 0.09317457 0.103907   0.0648669  0.03977567 0.03073654
 0.15613078 0.07701593 0.10985945 0.08756758 0.21412975], sum to 1.0000
[2019-04-09 14:53:13,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2746
[2019-04-09 14:53:13,201] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.166666666666667, 100.0, 0.0, 0.0, 19.0, 26.63150251212172, 0.6017223684743646, 0.0, 1.0, 20.0, 52.07310694027192], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3129000.0000, 
sim time next is 3129600.0000, 
raw observation next is [3.333333333333333, 100.0, 0.0, 0.0, 19.0, 26.64044450451621, 0.6171178949649769, 0.0, 1.0, 30.0, 46.58796288644864], 
processed observation next is [1.0, 0.21739130434782608, 0.5549399815327793, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7200370420430175, 0.7057059649883257, 0.0, 1.0, 0.3, 0.4658796288644864], 
reward next is 0.5341, 
noisyNet noise sample is [array([0.9990365], dtype=float32), 1.0366343]. 
=============================================
[2019-04-09 14:53:13,700] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02869966 0.12269221 0.10546786 0.07607235 0.06659698 0.03947329
 0.13921289 0.07780453 0.08958954 0.10473282 0.14965786], sum to 1.0000
[2019-04-09 14:53:13,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-09 14:53:13,730] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.54947387346544, 0.6182883770232048, 0.0, 1.0, 20.0, 32.96966892208044], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.4957913506716, 0.6052550480342105, 0.0, 1.0, 45.0, 31.18296127381735], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7079826125559666, 0.7017516826780702, 0.0, 1.0, 0.6, 0.3118296127381735], 
reward next is 0.6882, 
noisyNet noise sample is [array([1.4745764], dtype=float32), 0.37537268]. 
=============================================
[2019-04-09 14:53:14,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00763538 0.12235866 0.11171042 0.05257592 0.03979178 0.01718567
 0.11195555 0.06897393 0.12708637 0.10662328 0.23410296], sum to 1.0000
[2019-04-09 14:53:14,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3962
[2019-04-09 14:53:14,027] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 22.5, 28.33792008474472, 1.234297161906737, 0.0, 1.0, 65.0, 19.47478142789049], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3177000.0000, 
sim time next is 3177600.0000, 
raw observation next is [4.666666666666666, 100.0, 0.0, 0.0, 22.5, 28.1881089093348, 1.239681117056258, 0.0, 1.0, 45.0, 19.13616528898963], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 1.0, 0.0, 0.0, 0.375, 0.8490090757779001, 0.9132270390187527, 0.0, 1.0, 0.6, 0.19136165288989632], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.19590062], dtype=float32), -0.4779945]. 
=============================================
[2019-04-09 14:53:14,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02364349 0.09384803 0.10550504 0.0594965  0.03869215 0.02533294
 0.15475063 0.07153749 0.09851477 0.09688427 0.23179466], sum to 1.0000
[2019-04-09 14:53:14,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6998
[2019-04-09 14:53:14,129] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.333333333333334, 100.0, 0.0, 0.0, 19.0, 26.43589418216265, 0.5498183421854683, 0.0, 1.0, 25.0, 44.61360643162303], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3132600.0000, 
sim time next is 3133200.0000, 
raw observation next is [4.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.36928209448129, 0.552176769276172, 0.0, 1.0, 60.0, 60.13725436685045], 
processed observation next is [1.0, 0.2608695652173913, 0.5918744228993538, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6974401745401074, 0.6840589230920573, 0.0, 1.0, 0.9, 0.6013725436685045], 
reward next is 0.3986, 
noisyNet noise sample is [array([-0.06188441], dtype=float32), -1.1363307]. 
=============================================
[2019-04-09 14:53:14,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01760439 0.07349897 0.10473251 0.04362332 0.04080239 0.03126726
 0.15967226 0.08561534 0.07821511 0.09855756 0.26641095], sum to 1.0000
[2019-04-09 14:53:14,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2384
[2019-04-09 14:53:14,261] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666667, 100.0, 0.0, 0.0, 19.0, 27.40583989030135, 1.012954721041713, 0.0, 1.0, 40.0, 32.26048652586055], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3201600.0000, 
sim time next is 3202200.0000, 
raw observation next is [0.5, 100.0, 0.0, 0.0, 19.0, 27.40326418079543, 1.001656146085153, 0.0, 1.0, 30.0, 27.86237494434555], 
processed observation next is [1.0, 0.043478260869565216, 0.4764542936288089, 1.0, 0.0, 0.0, 0.08333333333333333, 0.783605348399619, 0.8338853820283844, 0.0, 1.0, 0.3, 0.2786237494434555], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.31852508], dtype=float32), 0.9236081]. 
=============================================
[2019-04-09 14:53:14,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0174747  0.07986908 0.10405488 0.04202126 0.035001   0.02352597
 0.17399374 0.06998289 0.07463862 0.10656901 0.2728689 ], sum to 1.0000
[2019-04-09 14:53:14,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8344
[2019-04-09 14:53:14,469] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 27.25005397216574, 0.969444876097446, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 27.10286033610573, 1.007907671906343, 0.0, 1.0, 65.0, 61.50849630925045], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7585716946754776, 0.8359692239687809, 0.0, 1.0, 1.0, 0.6150849630925045], 
reward next is 0.3849, 
noisyNet noise sample is [array([-1.0018337], dtype=float32), 0.9471088]. 
=============================================
[2019-04-09 14:53:14,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02582305 0.09195714 0.11976065 0.07410266 0.05195337 0.0412856
 0.17059903 0.06950426 0.07711173 0.08883479 0.18906774], sum to 1.0000
[2019-04-09 14:53:14,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-09 14:53:14,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 26.59253644656286, 0.6226475551347576, 0.0, 1.0, 25.0, 35.32292838430962], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3117600.0000, 
sim time next is 3118200.0000, 
raw observation next is [1.166666666666667, 100.0, 0.0, 0.0, 19.0, 26.61600438885586, 0.6205704040591403, 0.0, 1.0, 20.0, 32.77247460557679], 
processed observation next is [1.0, 0.08695652173913043, 0.49492151431209613, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7180003657379883, 0.7068568013530467, 0.0, 1.0, 0.1, 0.3277247460557679], 
reward next is 0.6723, 
noisyNet noise sample is [array([-0.60407436], dtype=float32), 1.090269]. 
=============================================
[2019-04-09 14:53:14,860] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00916537 0.1122992  0.13843381 0.06832657 0.04154542 0.02039728
 0.11764862 0.0575078  0.11622024 0.08702797 0.23142773], sum to 1.0000
[2019-04-09 14:53:14,860] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8431
[2019-04-09 14:53:14,879] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 100.0, 33.0, 307.5, 22.5, 28.69557850716096, 1.339300110846723, 1.0, 1.0, 45.0, 3.831343966273753], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3171600.0000, 
sim time next is 3172200.0000, 
raw observation next is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 22.5, 29.01541361142057, 1.145655476815107, 1.0, 1.0, 65.0, 43.83869476022988], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0822222222222222, 0.269244935543278, 0.375, 0.9179511342850475, 0.881885158938369, 1.0, 1.0, 1.0, 0.4383869476022988], 
reward next is 0.5616, 
noisyNet noise sample is [array([0.09720587], dtype=float32), -0.28312746]. 
=============================================
[2019-04-09 14:53:15,469] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0187789  0.11643866 0.13214242 0.05902734 0.04090411 0.02884544
 0.13532941 0.06350214 0.1144556  0.11313413 0.17744188], sum to 1.0000
[2019-04-09 14:53:15,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0139
[2019-04-09 14:53:15,483] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 22.5, 26.93275636798446, 0.8954135904232352, 1.0, 1.0, 50.0, 35.4680155565125], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [-3.0, 92.0, 85.0, 370.0, 22.5, 27.0598554456879, 0.8778469870293196, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2833333333333333, 0.4088397790055249, 0.375, 0.7549879538073251, 0.7926156623431065, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3140403], dtype=float32), 1.3619658]. 
=============================================
[2019-04-09 14:53:15,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01776777 0.08446781 0.11153399 0.06035441 0.04346872 0.0282047
 0.15620323 0.07961948 0.10726746 0.09998088 0.21113151], sum to 1.0000
[2019-04-09 14:53:15,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0656
[2019-04-09 14:53:15,746] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.333333333333333, 100.0, 0.0, 0.0, 19.0, 26.98341994299257, 0.8885215661866545, 0.0, 1.0, 50.0, 41.08850075176379], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3212400.0000, 
sim time next is 3213000.0000, 
raw observation next is [-1.5, 100.0, 0.0, 0.0, 19.0, 26.93031145477475, 0.8844848316662697, 0.0, 1.0, 20.0, 35.00644907355083], 
processed observation next is [1.0, 0.17391304347826086, 0.4210526315789474, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7441926212312291, 0.7948282772220899, 0.0, 1.0, 0.1, 0.3500644907355083], 
reward next is 0.6499, 
noisyNet noise sample is [array([-0.3650969], dtype=float32), 0.15521444]. 
=============================================
[2019-04-09 14:53:15,765] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.8465629]
 [1.6596072]
 [1.716368 ]
 [1.6405611]
 [1.7307012]], R is [[2.36518335]
 [2.93064642]
 [3.42341566]
 [3.93795419]
 [4.54275799]].
[2019-04-09 14:53:15,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01477589 0.08538985 0.12639959 0.05246361 0.03345229 0.02847358
 0.15727717 0.07201336 0.10808061 0.10889489 0.21277909], sum to 1.0000
[2019-04-09 14:53:15,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2174
[2019-04-09 14:53:15,828] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.6666666666666666, 100.0, 0.0, 0.0, 19.0, 27.12899164565832, 0.9535069691149595, 0.0, 1.0, 60.0, 33.01729256900423], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3206400.0000, 
sim time next is 3207000.0000, 
raw observation next is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 27.20569473719443, 0.9415417331964986, 0.0, 1.0, 25.0, 35.23989575173946], 
processed observation next is [1.0, 0.08695652173913043, 0.43951985226223456, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7671412280995359, 0.8138472443988328, 0.0, 1.0, 0.2, 0.3523989575173946], 
reward next is 0.6476, 
noisyNet noise sample is [array([-0.43049788], dtype=float32), 0.30741754]. 
=============================================
[2019-04-09 14:53:15,830] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02050314 0.08468087 0.09994339 0.05500567 0.03802929 0.02636058
 0.1882102  0.06558356 0.10055148 0.09743676 0.22369502], sum to 1.0000
[2019-04-09 14:53:15,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[1.8084303]
 [1.7112082]
 [1.8672456]
 [1.9178926]
 [1.8756944]], R is [[2.39849782]
 [3.04434013]
 [3.65496206]
 [4.01578236]
 [4.97562456]].
[2019-04-09 14:53:15,837] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-09 14:53:15,851] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.54093198696649, 0.8020486139161703, 0.0, 1.0, 65.0, 55.65000668860282], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [-3.0, 100.0, 0.0, 0.0, 19.0, 26.48634896035556, 0.8180382712941482, 0.0, 1.0, 55.0, 49.56798246964423], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7071957466962967, 0.772679423764716, 0.0, 1.0, 0.8, 0.4956798246964423], 
reward next is 0.5043, 
noisyNet noise sample is [array([1.2785546], dtype=float32), -1.5887371]. 
=============================================
[2019-04-09 14:53:15,969] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02050436 0.09946233 0.10025414 0.05453458 0.04551465 0.03381361
 0.16484569 0.0710104  0.09958662 0.10124645 0.20922716], sum to 1.0000
[2019-04-09 14:53:15,969] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7671
[2019-04-09 14:53:15,993] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 26.42695272746867, 0.7596886885969272, 0.0, 1.0, 50.0, 35.47749874276372], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [-3.0, 93.33333333333334, 0.0, 0.0, 19.0, 26.34170365486032, 0.7602711083887637, 0.0, 1.0, 60.0, 59.67189318423587], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.6951419712383601, 0.7534237027962546, 0.0, 1.0, 0.9, 0.5967189318423587], 
reward next is 0.4033, 
noisyNet noise sample is [array([2.020456], dtype=float32), 0.07773119]. 
=============================================
[2019-04-09 14:53:16,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01535388 0.08621585 0.1196291  0.05274323 0.03616747 0.02161727
 0.14730273 0.06439643 0.10531264 0.10600612 0.24525532], sum to 1.0000
[2019-04-09 14:53:16,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3770
[2019-04-09 14:53:16,118] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 19.0, 27.20614757421895, 1.010629072777871, 0.0, 1.0, 65.0, 43.7837342893512], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3197400.0000, 
sim time next is 3198000.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 19.0, 27.18447784010308, 1.004707354353597, 0.0, 1.0, 45.0, 37.64603935037403], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.7653731533419235, 0.834902451451199, 0.0, 1.0, 0.6, 0.37646039350374033], 
reward next is 0.6235, 
noisyNet noise sample is [array([0.546173], dtype=float32), 2.2988942]. 
=============================================
[2019-04-09 14:53:16,136] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.3014162]
 [2.2789962]
 [2.2884648]
 [2.1795816]
 [2.2448733]], R is [[2.6601212 ]
 [3.19568276]
 [3.78561831]
 [4.22861671]
 [4.90803051]].
[2019-04-09 14:53:16,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00818397 0.12440062 0.0968643  0.09481953 0.04415873 0.02694341
 0.13093746 0.05639494 0.1443039  0.07371478 0.19927837], sum to 1.0000
[2019-04-09 14:53:16,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5920
[2019-04-09 14:53:16,440] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 22.5, 27.41199017329312, 1.076500330306561, 1.0, 1.0, 35.0, 11.51381431969018], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 22.5, 27.87919759098848, 0.8089291216053347, 1.0, 1.0, 55.0, 40.14907630877468], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.785, 0.37222222222222234, 0.874401473296501, 0.375, 0.8232664659157066, 0.7696430405351116, 1.0, 1.0, 0.8, 0.40149076308774684], 
reward next is 0.5985, 
noisyNet noise sample is [array([1.4670376], dtype=float32), 0.20679988]. 
=============================================
[2019-04-09 14:53:16,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[2.5702446]
 [2.5261536]
 [2.3565233]
 [2.3941863]
 [2.3285484]], R is [[3.03335643]
 [3.88788486]
 [4.57027531]
 [5.00266552]
 [5.71033716]].
[2019-04-09 14:53:16,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01925762 0.13255258 0.11107864 0.07727252 0.04318878 0.02637138
 0.11436188 0.07350581 0.12336506 0.09512275 0.18392298], sum to 1.0000
[2019-04-09 14:53:16,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8484
[2019-04-09 14:53:16,750] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 22.5, 27.38750483018565, 0.9510871884236415, 1.0, 1.0, 30.0, 28.82757747276133], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3228600.0000, 
sim time next is 3229200.0000, 
raw observation next is [-3.0, 92.0, 93.0, 511.5, 22.5, 27.46315514393958, 0.9642904703063381, 1.0, 1.0, 65.0, 29.96552114024835], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.31, 0.5651933701657459, 0.375, 0.788596261994965, 0.8214301567687793, 1.0, 1.0, 1.0, 0.29965521140248347], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.39719585], dtype=float32), -0.28524587]. 
=============================================
[2019-04-09 14:53:16,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02037846 0.11166324 0.11793678 0.06649845 0.04468155 0.0288757
 0.1523619  0.06331446 0.11293179 0.09416025 0.18719737], sum to 1.0000
[2019-04-09 14:53:16,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5247
[2019-04-09 14:53:16,859] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 22.5, 26.7039648812953, 0.8629243259429855, 1.0, 1.0, 45.0, 62.55947853066215], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3226800.0000, 
sim time next is 3227400.0000, 
raw observation next is [-3.0, 92.0, 85.0, 370.0, 22.5, 26.80736365840145, 0.9079708540492318, 1.0, 1.0, 45.0, 28.34589030467309], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2833333333333333, 0.4088397790055249, 0.375, 0.7339469715334541, 0.802656951349744, 1.0, 1.0, 0.6, 0.28345890304673094], 
reward next is 0.7165, 
noisyNet noise sample is [array([1.1224773], dtype=float32), -0.9013151]. 
=============================================
[2019-04-09 14:53:16,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01271611 0.11001837 0.13752063 0.05170242 0.03219901 0.02660642
 0.10706779 0.06782786 0.1172151  0.0810277  0.25609857], sum to 1.0000
[2019-04-09 14:53:16,864] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0084
[2019-04-09 14:53:16,887] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.0, 71.0, 93.0, 727.5, 22.5, 28.4299657662933, 0.9698985792557894, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3250800.0000, 
sim time next is 3251400.0000, 
raw observation next is [-2.166666666666667, 71.0, 90.33333333333333, 713.6666666666666, 22.5, 27.34490459584444, 0.9898954144436534, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.71, 0.3011111111111111, 0.7885819521178636, 0.375, 0.7787420496537033, 0.8299651381478844, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49896872], dtype=float32), 0.7906645]. 
=============================================
[2019-04-09 14:53:17,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00983431 0.12865482 0.09539947 0.06371779 0.03839405 0.02331646
 0.11275822 0.05830931 0.15604995 0.08614571 0.22741991], sum to 1.0000
[2019-04-09 14:53:17,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2601
[2019-04-09 14:53:17,034] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 22.5, 27.53420911406511, 0.9953566275326989, 1.0, 1.0, 65.0, 46.98739007995928], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3230400.0000, 
sim time next is 3231000.0000, 
raw observation next is [-3.0, 92.0, 101.0, 653.0, 22.5, 27.624622973285, 1.015098107358156, 1.0, 1.0, 30.0, 22.2715017863364], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.33666666666666667, 0.7215469613259669, 0.375, 0.8020519144404167, 0.8383660357860521, 1.0, 1.0, 0.3, 0.222715017863364], 
reward next is 0.7773, 
noisyNet noise sample is [array([-0.37023127], dtype=float32), 0.35951173]. 
=============================================
[2019-04-09 14:53:17,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[2.251881 ]
 [2.1469028]
 [2.2069638]
 [2.1010735]
 [2.0844088]], R is [[2.98455954]
 [3.48484015]
 [4.15547705]
 [4.84043217]
 [5.46537685]].
[2019-04-09 14:53:17,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01789751 0.11043179 0.08310725 0.06097905 0.05047585 0.02515862
 0.14616692 0.06446778 0.11366751 0.110826   0.21682169], sum to 1.0000
[2019-04-09 14:53:17,313] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2206
[2019-04-09 14:53:17,338] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.56702437344501, 0.818493011396606, 0.0, 1.0, 65.0, 56.67623910641793], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [-3.0, 100.0, 0.0, 0.0, 19.0, 26.63087099413394, 0.825973767414416, 0.0, 1.0, 30.0, 44.6233882752761], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7192392495111616, 0.7753245891381386, 0.0, 1.0, 0.3, 0.446233882752761], 
reward next is 0.5538, 
noisyNet noise sample is [array([0.8322005], dtype=float32), 1.2864872]. 
=============================================
[2019-04-09 14:53:17,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00951339 0.10927574 0.12366822 0.07361478 0.02875745 0.02083301
 0.11756469 0.06133097 0.11981804 0.07074752 0.26487628], sum to 1.0000
[2019-04-09 14:53:17,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3549
[2019-04-09 14:53:17,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01034274 0.09105776 0.12346645 0.05114577 0.0361607  0.0174201
 0.17757241 0.06700906 0.09362186 0.09399146 0.23821165], sum to 1.0000
[2019-04-09 14:53:17,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0407
[2019-04-09 14:53:17,412] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.63523097347027, 1.146566318656032, 0.0, 1.0, 45.0, 24.04519846758786], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3186000.0000, 
sim time next is 3186600.0000, 
raw observation next is [2.833333333333333, 100.0, 0.0, 0.0, 19.0, 27.62834685722957, 1.141405159821402, 0.0, 1.0, 20.0, 23.7575501075727], 
processed observation next is [1.0, 0.9130434782608695, 0.541089566020314, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8023622381024641, 0.880468386607134, 0.0, 1.0, 0.1, 0.237575501075727], 
reward next is 0.7624, 
noisyNet noise sample is [array([-1.1786047], dtype=float32), 0.19370686]. 
=============================================
[2019-04-09 14:53:17,413] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 22.5, 27.36064616166555, 1.016107438390814, 1.0, 1.0, 20.0, 18.85489499171302], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3260400.0000, 
sim time next is 3261000.0000, 
raw observation next is [-4.0, 67.0, 0.0, 0.0, 22.5, 27.64324450517146, 1.046658743919364, 1.0, 1.0, 40.0, 26.2689381227006], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.67, 0.0, 0.0, 0.375, 0.8036037087642883, 0.8488862479731214, 1.0, 1.0, 0.5, 0.262689381227006], 
reward next is 0.7373, 
noisyNet noise sample is [array([-0.6428097], dtype=float32), -0.65343136]. 
=============================================
[2019-04-09 14:53:17,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02055842 0.09432702 0.09276512 0.05503275 0.03626943 0.03035415
 0.16100717 0.07029948 0.10781373 0.11232822 0.21924448], sum to 1.0000
[2019-04-09 14:53:17,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-09 14:53:17,424] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[2.6754558]
 [2.6508675]
 [2.6973395]
 [2.5835488]
 [2.640875 ]], R is [[3.28497458]
 [4.06357574]
 [4.68760681]
 [5.0613699 ]
 [5.80374765]].
[2019-04-09 14:53:17,444] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 96.0, 0.0, 0.0, 19.0, 26.66853185390839, 0.7963562291311977, 0.0, 1.0, 30.0, 34.74801268056859], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 26.59862153713397, 0.7932577620162388, 0.0, 1.0, 60.0, 47.3791923052797], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9466666666666665, 0.0, 0.0, 0.08333333333333333, 0.716551794761164, 0.7644192540054129, 0.0, 1.0, 0.9, 0.473791923052797], 
reward next is 0.5262, 
noisyNet noise sample is [array([-0.85409695], dtype=float32), 0.26457074]. 
=============================================
[2019-04-09 14:53:17,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00973041 0.11269873 0.1298597  0.08365962 0.04049921 0.02726856
 0.09738407 0.05214321 0.11649828 0.07672873 0.2535295 ], sum to 1.0000
[2019-04-09 14:53:17,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3932
[2019-04-09 14:53:17,469] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 22.5, 28.35990075503556, 1.172635993758643, 1.0, 1.0, 20.0, 17.25121832704557], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3248400.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 22.5, 28.39778063405955, 1.181046720170617, 1.0, 1.0, 40.0, 17.1016331220887], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.375, 0.8664817195049626, 0.8936822400568722, 1.0, 1.0, 0.5, 0.171016331220887], 
reward next is 0.8290, 
noisyNet noise sample is [array([0.6978765], dtype=float32), -1.5124071]. 
=============================================
[2019-04-09 14:53:17,482] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[2.6915188]
 [2.741005 ]
 [2.8065913]
 [2.7937891]
 [2.780694 ]], R is [[3.53807116]
 [4.33017826]
 [5.10041189]
 [5.86809826]
 [6.62821388]].
[2019-04-09 14:53:17,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02221447 0.08288341 0.12430823 0.05477465 0.04691953 0.03686423
 0.1486289  0.07924458 0.07983576 0.09449412 0.22983211], sum to 1.0000
[2019-04-09 14:53:17,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6856
[2019-04-09 14:53:17,773] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.1666666666666666, 100.0, 0.0, 0.0, 19.0, 27.23606076970117, 0.9739152190201832, 0.0, 1.0, 45.0, 34.4925720396627], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3203400.0000, 
sim time next is 3204000.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 27.21380785699423, 0.9626454661532579, 0.0, 1.0, 60.0, 36.96586836588668], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7678173214161857, 0.820881822051086, 0.0, 1.0, 0.9, 0.3696586836588668], 
reward next is 0.6303, 
noisyNet noise sample is [array([1.2593889], dtype=float32), 0.43598887]. 
=============================================
[2019-04-09 14:53:17,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[1.9748976]
 [2.1268835]
 [1.9789135]
 [2.0387516]
 [1.8755002]], R is [[2.61961794]
 [3.24849606]
 [3.84194589]
 [4.43429279]
 [4.8941927 ]].
[2019-04-09 14:53:17,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01299635 0.10089648 0.12761182 0.07983296 0.04119316 0.02570196
 0.11889548 0.06568737 0.14195965 0.07464201 0.21058272], sum to 1.0000
[2019-04-09 14:53:17,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8262
[2019-04-09 14:53:17,918] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 22.5, 27.55738684291686, 0.999822717926881, 1.0, 1.0, 25.0, 26.38610045130293], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3230400.0000, 
sim time next is 3231000.0000, 
raw observation next is [-3.0, 92.0, 101.0, 653.0, 22.5, 27.64285988725254, 1.018944748478056, 1.0, 1.0, 65.0, 27.39679554709398], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.33666666666666667, 0.7215469613259669, 0.375, 0.8035716572710451, 0.8396482494926852, 1.0, 1.0, 1.0, 0.2739679554709398], 
reward next is 0.7260, 
noisyNet noise sample is [array([0.91014814], dtype=float32), 1.041466]. 
=============================================
[2019-04-09 14:53:17,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[2.392817 ]
 [2.3616166]
 [2.2817214]
 [2.2571726]
 [2.1684656]], R is [[2.97333813]
 [3.67974377]
 [4.3487649 ]
 [5.04820013]
 [5.72407913]].
[2019-04-09 14:53:18,120] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.011589   0.09024702 0.12684456 0.05320668 0.03698678 0.02152336
 0.17007251 0.06851672 0.08772649 0.10067825 0.23260853], sum to 1.0000
[2019-04-09 14:53:18,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3313
[2019-04-09 14:53:18,147] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.68200807220217, 1.158215449950782, 0.0, 1.0, 25.0, 28.9970444457457], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3185400.0000, 
sim time next is 3186000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 27.66241716329427, 1.154946259837868, 0.0, 1.0, 40.0, 20.49626296644115], 
processed observation next is [1.0, 0.9130434782608695, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8052014302745224, 0.8849820866126227, 0.0, 1.0, 0.5, 0.2049626296644115], 
reward next is 0.7950, 
noisyNet noise sample is [array([0.8151878], dtype=float32), -1.0287151]. 
=============================================
[2019-04-09 14:53:18,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[2.4904444]
 [2.5742848]
 [2.553222 ]
 [2.5544524]
 [2.6103394]], R is [[3.25652933]
 [3.93399358]
 [4.46396923]
 [5.18292713]
 [5.89988852]].
[2019-04-09 14:53:18,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01102048 0.11801649 0.12954225 0.05570365 0.03313577 0.02449078
 0.11691351 0.05903328 0.11638347 0.08338195 0.2523784 ], sum to 1.0000
[2019-04-09 14:53:18,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5152
[2019-04-09 14:53:18,407] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 75.0, 25.66666666666666, 239.6666666666666, 22.5, 27.78372640547221, 1.082428030495013, 1.0, 1.0, 65.0, 11.72529094461884], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 22.5, 28.18204574481125, 0.8950559445860118, 1.0, 1.0, 50.0, 56.56239909623781], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.73, 0.05777777777777776, 0.18987108655616938, 0.375, 0.8485038120676043, 0.7983519815286706, 1.0, 1.0, 0.7, 0.5656239909623781], 
reward next is 0.4344, 
noisyNet noise sample is [array([-1.7284837], dtype=float32), 1.0061433]. 
=============================================
[2019-04-09 14:53:18,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01052692 0.14962965 0.13610855 0.06730768 0.04412251 0.02480361
 0.13662165 0.05256536 0.11739577 0.06935897 0.19155936], sum to 1.0000
[2019-04-09 14:53:18,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-09 14:53:18,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 85.0, 113.0, 817.5, 22.5, 26.49172495004755, 1.004018933055436, 1.0, 1.0, 65.0, 5.187814714069366], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3243600.0000, 
sim time next is 3244200.0000, 
raw observation next is [-2.333333333333333, 87.5, 112.3333333333333, 815.6666666666666, 22.5, 27.77106098060611, 1.075089207683067, 1.0, 1.0, 25.0, 18.98870316207836], 
processed observation next is [1.0, 0.5652173913043478, 0.3979686057248385, 0.875, 0.37444444444444436, 0.9012891344383057, 0.375, 0.8142550817171758, 0.858363069227689, 1.0, 1.0, 0.2, 0.18988703162078358], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.16661435], dtype=float32), 0.49853158]. 
=============================================
[2019-04-09 14:53:18,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02500735 0.08869116 0.09013571 0.0559177  0.03993513 0.02850436
 0.16292438 0.07649861 0.10279828 0.08726574 0.24232158], sum to 1.0000
[2019-04-09 14:53:18,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8608
[2019-04-09 14:53:18,853] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 100.0, 0.0, 0.0, 19.0, 26.87748762694418, 0.8856557255029962, 0.0, 1.0, 45.0, 38.24409653063753], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [-2.166666666666667, 100.0, 0.0, 0.0, 19.0, 26.91809273379412, 0.8763909530441291, 0.0, 1.0, 65.0, 40.15842578980072], 
processed observation next is [1.0, 0.21739130434782608, 0.4025854108956602, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7431743944828432, 0.7921303176813764, 0.0, 1.0, 1.0, 0.40158425789800717], 
reward next is 0.5984, 
noisyNet noise sample is [array([-1.1825823], dtype=float32), -0.33008566]. 
=============================================
[2019-04-09 14:53:19,497] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87102: loss 25.3449
[2019-04-09 14:53:19,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87102: learning rate 0.0000
[2019-04-09 14:53:19,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01310987 0.11415873 0.10279317 0.05367284 0.04145142 0.02135657
 0.17207465 0.07002089 0.11344896 0.09272783 0.20518513], sum to 1.0000
[2019-04-09 14:53:19,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2974
[2019-04-09 14:53:19,559] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.166666666666667, 82.83333333333333, 0.0, 0.0, 19.0, 27.11127202099323, 0.902557800390522, 0.0, 1.0, 20.0, 34.37732711976019], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3273000.0000, 
sim time next is 3273600.0000, 
raw observation next is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 27.08011654330125, 0.8973277573314871, 0.0, 1.0, 35.0, 33.82991783090525], 
processed observation next is [1.0, 0.9130434782608695, 0.31486611265004616, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7566763786084376, 0.799109252443829, 0.0, 1.0, 0.4, 0.33829917830905254], 
reward next is 0.6617, 
noisyNet noise sample is [array([-2.2339602], dtype=float32), -0.059417542]. 
=============================================
[2019-04-09 14:53:19,681] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87194: loss 22.2249
[2019-04-09 14:53:19,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01124838 0.11741758 0.11571936 0.05811229 0.04672155 0.02537696
 0.15147479 0.06024228 0.09869328 0.06156038 0.25343308], sum to 1.0000
[2019-04-09 14:53:19,683] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87195: learning rate 0.0000
[2019-04-09 14:53:19,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2664
[2019-04-09 14:53:19,708] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 19.0, 27.10929879006689, 0.9270956394935865, 0.0, 1.0, 65.0, 38.84090940931642], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 27.14032273759993, 0.9190306725724184, 0.0, 1.0, 30.0, 34.85789893721496], 
processed observation next is [1.0, 0.9130434782608695, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.7616935614666609, 0.8063435575241394, 0.0, 1.0, 0.3, 0.3485789893721496], 
reward next is 0.6514, 
noisyNet noise sample is [array([1.7471808], dtype=float32), 0.35996246]. 
=============================================
[2019-04-09 14:53:19,904] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87303: loss 25.0725
[2019-04-09 14:53:19,907] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87306: learning rate 0.0000
[2019-04-09 14:53:19,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02337642 0.08932316 0.10029102 0.06050925 0.04410174 0.03445041
 0.1641397  0.08054673 0.09848157 0.09615421 0.20862584], sum to 1.0000
[2019-04-09 14:53:20,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-09 14:53:20,033] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.9, 77.0, 0.0, 0.0, 19.0, 25.81979052593205, 0.5103242091312423, 0.0, 1.0, 20.0, 32.70680826377271], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3297600.0000, 
sim time next is 3298200.0000, 
raw observation next is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 19.0, 25.62036038525464, 0.4999510220091721, 0.0, 1.0, 50.0, 37.66437004105282], 
processed observation next is [1.0, 0.17391304347826086, 0.21098799630655585, 0.7683333333333334, 0.0, 0.0, 0.08333333333333333, 0.6350300321045532, 0.6666503406697241, 0.0, 1.0, 0.7, 0.3766437004105282], 
reward next is 0.6234, 
noisyNet noise sample is [array([-1.5817266], dtype=float32), -0.4447298]. 
=============================================
[2019-04-09 14:53:20,107] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87401: loss 31.4791
[2019-04-09 14:53:20,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87401: learning rate 0.0000
[2019-04-09 14:53:20,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00791159 0.12511194 0.1229582  0.07139286 0.04025348 0.02384414
 0.11173216 0.05647908 0.13417318 0.0734774  0.23266597], sum to 1.0000
[2019-04-09 14:53:20,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-09 14:53:20,309] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 87502: loss 28.1312
[2019-04-09 14:53:20,310] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 87502: learning rate 0.0000
[2019-04-09 14:53:20,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 85.5, 101.0, 769.0, 22.5, 28.38032198601553, 1.172106867852435, 1.0, 1.0, 65.0, 21.27434928517598], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3249000.0000, 
sim time next is 3249600.0000, 
raw observation next is [-2.666666666666667, 80.66666666666666, 98.33333333333334, 755.1666666666667, 22.5, 28.4194513451546, 1.170590104948191, 1.0, 1.0, 60.0, 22.26038566762906], 
processed observation next is [1.0, 0.6086956521739131, 0.38873499538319484, 0.8066666666666665, 0.32777777777777783, 0.834438305709024, 0.375, 0.8682876120962165, 0.8901967016493971, 1.0, 1.0, 0.9, 0.2226038566762906], 
reward next is 0.7774, 
noisyNet noise sample is [array([1.6852092], dtype=float32), 0.18578105]. 
=============================================
[2019-04-09 14:53:20,384] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0093629  0.1484625  0.10899405 0.06546425 0.04011326 0.02238566
 0.129609   0.07136224 0.1223446  0.08695346 0.1949481 ], sum to 1.0000
[2019-04-09 14:53:20,386] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4010
[2019-04-09 14:53:20,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01271078 0.125863   0.12756003 0.08642444 0.04915939 0.02287777
 0.12367646 0.07339138 0.14190924 0.07670731 0.15972023], sum to 1.0000
[2019-04-09 14:53:20,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3666
[2019-04-09 14:53:20,411] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 22.5, 27.57628007780259, 1.006835781359188, 1.0, 1.0, 20.0, 25.39618301469911], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3230400.0000, 
sim time next is 3231000.0000, 
raw observation next is [-3.0, 92.0, 101.0, 653.0, 22.5, 27.6644052995118, 1.025728258481837, 1.0, 1.0, 35.0, 23.49202571659637], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.33666666666666667, 0.7215469613259669, 0.375, 0.8053671082926499, 0.8419094194939456, 1.0, 1.0, 0.4, 0.2349202571659637], 
reward next is 0.7651, 
noisyNet noise sample is [array([1.448637], dtype=float32), 1.4401656]. 
=============================================
[2019-04-09 14:53:20,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[2.446349 ]
 [2.370441 ]
 [2.4417415]
 [2.226036 ]
 [2.3107333]], R is [[3.13595724]
 [3.85063601]
 [4.55736876]
 [5.27171516]
 [5.91187763]].
[2019-04-09 14:53:20,421] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 22.5, 27.71367117966988, 1.067710233852396, 1.0, 1.0, 45.0, 25.9251494942315], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3243000.0000, 
sim time next is 3243600.0000, 
raw observation next is [-2.0, 85.0, 113.0, 817.5, 22.5, 27.91281032250591, 1.084793444748291, 1.0, 1.0, 45.0, 18.74996086161847], 
processed observation next is [1.0, 0.5652173913043478, 0.40720221606648205, 0.85, 0.37666666666666665, 0.9033149171270718, 0.375, 0.8260675268754923, 0.861597814916097, 1.0, 1.0, 0.6, 0.1874996086161847], 
reward next is 0.8125, 
noisyNet noise sample is [array([-0.3373852], dtype=float32), 0.96242774]. 
=============================================
[2019-04-09 14:53:20,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02139349 0.09526581 0.09469223 0.06614058 0.04091573 0.02437751
 0.15995161 0.07775349 0.09922655 0.09358559 0.2266974 ], sum to 1.0000
[2019-04-09 14:53:20,477] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8769
[2019-04-09 14:53:20,508] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.45, 77.0, 0.0, 0.0, 19.0, 26.05946168686124, 0.6431845343020287, 0.0, 1.0, 50.0, 48.34613246761766], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3295800.0000, 
sim time next is 3296400.0000, 
raw observation next is [-8.600000000000001, 77.0, 0.0, 0.0, 19.0, 26.2361442271968, 0.6506988048854223, 0.0, 1.0, 65.0, 51.91575625348021], 
processed observation next is [1.0, 0.13043478260869565, 0.22437673130193903, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6863453522663999, 0.7168996016284742, 0.0, 1.0, 1.0, 0.5191575625348022], 
reward next is 0.4808, 
noisyNet noise sample is [array([1.345527], dtype=float32), 1.0699755]. 
=============================================
[2019-04-09 14:53:20,558] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87627: loss 22.6721
[2019-04-09 14:53:20,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87630: learning rate 0.0000
[2019-04-09 14:53:20,739] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 87712: loss 16.8335
[2019-04-09 14:53:20,739] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 87712: learning rate 0.0000
[2019-04-09 14:53:20,809] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87750: loss 19.1887
[2019-04-09 14:53:20,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87751: learning rate 0.0000
[2019-04-09 14:53:21,195] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01675831 0.07228648 0.12396131 0.03746659 0.03102528 0.02638327
 0.1841804  0.08384664 0.08386585 0.10675384 0.23347194], sum to 1.0000
[2019-04-09 14:53:21,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-09 14:53:21,228] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 77.0, 0.0, 0.0, 19.0, 26.06554982954198, 0.6643587077888625, 0.0, 1.0, 25.0, 46.68860703014261], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3285000.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 19.0, 26.07150922617387, 0.680552902601184, 0.0, 1.0, 65.0, 61.02090279892077], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6726257688478224, 0.726850967533728, 0.0, 1.0, 1.0, 0.6102090279892077], 
reward next is 0.3898, 
noisyNet noise sample is [array([0.88817966], dtype=float32), -1.000778]. 
=============================================
[2019-04-09 14:53:21,257] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87943: loss 27.2680
[2019-04-09 14:53:21,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87943: learning rate 0.0000
[2019-04-09 14:53:21,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02001304 0.09216712 0.10061944 0.06245489 0.04374176 0.02931297
 0.16290705 0.07401045 0.10124732 0.08728218 0.22624385], sum to 1.0000
[2019-04-09 14:53:21,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2067
[2019-04-09 14:53:21,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01227528 0.15306713 0.11662564 0.07294366 0.03052057 0.02471763
 0.09204521 0.06687069 0.129687   0.09463999 0.20660722], sum to 1.0000
[2019-04-09 14:53:21,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5307
[2019-04-09 14:53:21,500] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-10.83333333333333, 76.0, 0.0, 0.0, 19.0, 25.5090748923658, 0.508401266320814, 0.0, 1.0, 60.0, 58.8814699994957], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3304200.0000, 
sim time next is 3304800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 25.53124538074016, 0.5051058367677544, 0.0, 1.0, 25.0, 43.33872337619504], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6276037817283466, 0.6683686122559181, 0.0, 1.0, 0.2, 0.43338723376195043], 
reward next is 0.5666, 
noisyNet noise sample is [array([1.339554], dtype=float32), 2.4076657]. 
=============================================
[2019-04-09 14:53:21,523] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 22.5, 27.17097473269124, 0.8504428292796495, 1.0, 1.0, 20.0, 36.36452165473509], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 22.5, 26.72407777739753, 0.7903747484307609, 1.0, 1.0, 50.0, 27.03564378706043], 
processed observation next is [1.0, 0.5217391304347826, 0.3056325023084026, 0.54, 0.3922222222222223, 0.8937384898710866, 0.375, 0.7270064814497941, 0.7634582494769203, 1.0, 1.0, 0.7, 0.2703564378706043], 
reward next is 0.7296, 
noisyNet noise sample is [array([0.70234346], dtype=float32), -0.71389395]. 
=============================================
[2019-04-09 14:53:21,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88090: loss 24.9095
[2019-04-09 14:53:21,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88090: learning rate 0.0000
[2019-04-09 14:53:21,749] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0101095  0.12994243 0.13937202 0.08235072 0.037934   0.02328131
 0.0965409  0.07132787 0.13045356 0.06384146 0.21484631], sum to 1.0000
[2019-04-09 14:53:21,750] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7857
[2019-04-09 14:53:21,784] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 59.0, 116.0, 798.0, 22.5, 27.19608178955329, 0.8818353770869461, 1.0, 1.0, 65.0, 34.12466971062751], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3324600.0000, 
sim time next is 3325200.0000, 
raw observation next is [-6.333333333333334, 57.33333333333334, 116.3333333333333, 800.1666666666666, 22.5, 26.77105302986817, 0.825713931590491, 1.0, 1.0, 65.0, 17.47456827469088], 
processed observation next is [1.0, 0.4782608695652174, 0.28716528162511545, 0.5733333333333335, 0.38777777777777767, 0.8841620626151012, 0.375, 0.7309210858223475, 0.7752379771968303, 1.0, 1.0, 1.0, 0.17474568274690883], 
reward next is 0.8253, 
noisyNet noise sample is [array([1.7684541], dtype=float32), -0.9459834]. 
=============================================
[2019-04-09 14:53:21,854] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88235: loss 26.4921
[2019-04-09 14:53:21,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88235: learning rate 0.0000
[2019-04-09 14:53:21,885] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01126251 0.15000266 0.11011368 0.07034084 0.04323773 0.02178546
 0.09056391 0.05151197 0.12940541 0.0931555  0.22862041], sum to 1.0000
[2019-04-09 14:53:21,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1174
[2019-04-09 14:53:21,940] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 22.5, 27.17704601005821, 0.8710241233259218, 1.0, 1.0, 35.0, 42.7792793791684], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [-5.166666666666667, 54.0, 116.6666666666667, 807.3333333333334, 22.5, 26.63794733524925, 0.814129287427436, 1.0, 1.0, 20.0, 21.82463108450531], 
processed observation next is [1.0, 0.5217391304347826, 0.31948291782086796, 0.54, 0.388888888888889, 0.8920810313075507, 0.375, 0.7198289446041043, 0.7713764291424786, 1.0, 1.0, 0.1, 0.21824631084505308], 
reward next is 0.7818, 
noisyNet noise sample is [array([-2.2411637], dtype=float32), -0.81465065]. 
=============================================
[2019-04-09 14:53:21,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02541294 0.08476735 0.10263788 0.05610008 0.04225197 0.03438269
 0.14792311 0.08174623 0.10321771 0.09790013 0.2236599 ], sum to 1.0000
[2019-04-09 14:53:21,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4191
[2019-04-09 14:53:21,970] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 24.20146350581261, 0.2322340966916071, 0.0, 1.0, 35.0, 32.33110324131003], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3308400.0000, 
sim time next is 3309000.0000, 
raw observation next is [-11.0, 77.33333333333334, 0.0, 0.0, 22.5, 24.26542021311753, 0.2146789952069102, 0.0, 1.0, 45.0, 33.21987174072947], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7733333333333334, 0.0, 0.0, 0.375, 0.5221183510931274, 0.57155966506897, 0.0, 1.0, 0.6, 0.3321987174072947], 
reward next is 0.6678, 
noisyNet noise sample is [array([2.2821918], dtype=float32), -0.08291334]. 
=============================================
[2019-04-09 14:53:21,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00992076 0.09302938 0.11966969 0.04735632 0.03632975 0.0166477
 0.16599192 0.06610758 0.12212326 0.08173124 0.24109235], sum to 1.0000
[2019-04-09 14:53:21,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[1.7513936]
 [1.7372307]
 [1.7888663]
 [1.8600335]
 [1.7859318]], R is [[2.37484741]
 [3.02778792]
 [3.52286553]
 [3.89317131]
 [4.51868153]].
[2019-04-09 14:53:21,999] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88302: loss 13.9979
[2019-04-09 14:53:22,000] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88302: learning rate 0.0000
[2019-04-09 14:53:22,001] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-09 14:53:22,025] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 27.11931292676956, 0.9237909754960105, 0.0, 1.0, 55.0, 35.67064705599552], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 19.0, 27.10037424670864, 0.9254765577611431, 0.0, 1.0, 35.0, 32.36906894974824], 
processed observation next is [1.0, 0.8695652173913043, 0.32871652816251157, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.7583645205590533, 0.8084921859203811, 0.0, 1.0, 0.4, 0.3236906894974824], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.62431246], dtype=float32), 0.588665]. 
=============================================
[2019-04-09 14:53:22,037] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88314: loss 17.5421
[2019-04-09 14:53:22,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88314: learning rate 0.0000
[2019-04-09 14:53:22,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00989153 0.11953025 0.13248785 0.06942815 0.03529286 0.03038996
 0.12127818 0.06826349 0.14870515 0.05814418 0.20658845], sum to 1.0000
[2019-04-09 14:53:22,197] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-09 14:53:22,233] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 22.5, 27.38286430900895, 0.9416240535877528, 1.0, 1.0, 65.0, 11.00975500093054], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 27.33333333333333, 255.6666666666666, 22.5, 27.89412865472702, 0.9665133550108987, 1.0, 1.0, 50.0, 20.43550810492152], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.5083333333333334, 0.0911111111111111, 0.2825046040515653, 0.375, 0.8245107212272517, 0.8221711183369663, 1.0, 1.0, 0.7, 0.2043550810492152], 
reward next is 0.7956, 
noisyNet noise sample is [array([-2.0833836], dtype=float32), -1.7787178]. 
=============================================
[2019-04-09 14:53:22,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[2.6579604]
 [2.8205705]
 [2.6994314]
 [2.7139955]
 [2.6526682]], R is [[3.32634735]
 [4.18298626]
 [4.82632208]
 [5.33527279]
 [6.10605288]].
[2019-04-09 14:53:22,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01631656 0.08526444 0.0895299  0.04601965 0.04290505 0.0301258
 0.21201421 0.07701778 0.08581698 0.08889184 0.22609772], sum to 1.0000
[2019-04-09 14:53:22,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-09 14:53:22,512] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.5, 73.5, 0.0, 0.0, 19.0, 26.53835153821646, 0.7090914848951022, 0.0, 1.0, 40.0, 39.56451997932936], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3288600.0000, 
sim time next is 3289200.0000, 
raw observation next is [-7.666666666666666, 74.66666666666666, 0.0, 0.0, 19.0, 26.45353715773805, 0.6886285670358899, 0.0, 1.0, 45.0, 36.48296794529482], 
processed observation next is [1.0, 0.043478260869565216, 0.25023084025854114, 0.7466666666666666, 0.0, 0.0, 0.08333333333333333, 0.7044614298115043, 0.72954285567863, 0.0, 1.0, 0.6, 0.3648296794529482], 
reward next is 0.6352, 
noisyNet noise sample is [array([0.1755829], dtype=float32), 1.367044]. 
=============================================
[2019-04-09 14:53:22,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01059165 0.17118359 0.12910219 0.06367686 0.03959243 0.02566866
 0.08820968 0.06424026 0.13243565 0.08753774 0.18776129], sum to 1.0000
[2019-04-09 14:53:22,564] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2152
[2019-04-09 14:53:22,593] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 22.5, 27.94046028313327, 0.9753857576536564, 1.0, 1.0, 55.0, 16.61979342730183], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3339600.0000, 
sim time next is 3340200.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 22.5, 28.04627581071338, 0.998223445853809, 1.0, 1.0, 25.0, 21.63097674069485], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.46666666666666673, 0.26, 0.6810313075506446, 0.375, 0.8371896508927815, 0.8327411486179362, 1.0, 1.0, 0.2, 0.21630976740694852], 
reward next is 0.7837, 
noisyNet noise sample is [array([0.9771074], dtype=float32), -1.1705211]. 
=============================================
[2019-04-09 14:53:23,175] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88843: loss 20.2959
[2019-04-09 14:53:23,182] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88843: learning rate 0.0000
[2019-04-09 14:53:23,311] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00942703 0.11581692 0.11322026 0.0734354  0.04200086 0.02489191
 0.13865903 0.07351954 0.09179787 0.07974895 0.23748223], sum to 1.0000
[2019-04-09 14:53:23,314] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8585
[2019-04-09 14:53:23,331] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.42324349278547, 0.8939460619029118, 1.0, 1.0, 20.0, 32.29266158649624], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3350400.0000, 
sim time next is 3351000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.3525881057188, 0.8790665431622312, 1.0, 1.0, 65.0, 32.35067310342694], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7793823421432334, 0.7930221810540771, 1.0, 1.0, 1.0, 0.3235067310342694], 
reward next is 0.6765, 
noisyNet noise sample is [array([1.9624777], dtype=float32), 1.4085318]. 
=============================================
[2019-04-09 14:53:23,340] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[2.7019405]
 [2.9341524]
 [2.6402514]
 [2.8551471]
 [2.8162682]], R is [[3.51312971]
 [4.15507174]
 [4.76207781]
 [5.44880772]
 [6.12496662]].
[2019-04-09 14:53:23,740] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 89102: loss 18.8271
[2019-04-09 14:53:23,741] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 89103: learning rate 0.0000
[2019-04-09 14:53:23,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01028522 0.13657813 0.13338415 0.08657346 0.03642609 0.02627428
 0.0895219  0.0680223  0.10850336 0.06134824 0.2430828 ], sum to 1.0000
[2019-04-09 14:53:23,828] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00936854 0.12103638 0.13425994 0.05642745 0.0282603  0.01851187
 0.17387144 0.05182815 0.11690316 0.06856265 0.2209701 ], sum to 1.0000
[2019-04-09 14:53:23,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6094
[2019-04-09 14:53:23,831] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5795
[2019-04-09 14:53:23,842] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.28274002031259, 0.8647772930683325, 0.0, 1.0, 65.0, 38.90391686340314], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.18976391305788, 0.8576439574197275, 1.0, 1.0, 50.0, 37.48975888947413], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.76581365942149, 0.7858813191399091, 1.0, 1.0, 0.7, 0.37489758889474134], 
reward next is 0.6251, 
noisyNet noise sample is [array([-0.3455334], dtype=float32), 0.18185379]. 
=============================================
[2019-04-09 14:53:23,847] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[2.9343436]
 [2.8638198]
 [2.9654129]
 [2.8307621]
 [2.8854516]], R is [[3.47608519]
 [4.05228519]
 [4.71342754]
 [5.42208385]
 [6.05326939]].
[2019-04-09 14:53:23,863] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 22.5, 27.96243968567376, 0.9819584687350877, 1.0, 1.0, 65.0, 23.38666461561772], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3340200.0000, 
sim time next is 3340800.0000, 
raw observation next is [-2.0, 46.0, 73.5, 587.5, 22.5, 27.99543066788412, 0.983377333535485, 1.0, 1.0, 55.0, 19.50701027263603], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46, 0.245, 0.649171270718232, 0.375, 0.8329525556570099, 0.8277924445118283, 1.0, 1.0, 0.8, 0.1950701027263603], 
reward next is 0.8049, 
noisyNet noise sample is [array([1.1456711], dtype=float32), 0.676853]. 
=============================================
[2019-04-09 14:53:24,151] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 89315: loss 22.8794
[2019-04-09 14:53:24,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 89317: learning rate 0.0000
[2019-04-09 14:53:24,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0205159  0.06538306 0.11343541 0.03956861 0.04337522 0.03399085
 0.17760418 0.0784599  0.08054205 0.09498666 0.25213817], sum to 1.0000
[2019-04-09 14:53:24,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-09 14:53:24,797] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.35681694585704, 0.6332678254805942, 0.0, 1.0, 55.0, 46.89514100513427], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3374400.0000, 
sim time next is 3375000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.36011994092798, 0.6283308699915663, 0.0, 1.0, 50.0, 39.62336169135027], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6966766617439983, 0.7094436233305221, 0.0, 1.0, 0.7, 0.3962336169135027], 
reward next is 0.6038, 
noisyNet noise sample is [array([0.21111195], dtype=float32), 1.7073659]. 
=============================================
[2019-04-09 14:53:24,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.8631691]
 [1.9212941]
 [1.9305567]
 [2.1519608]
 [1.9599153]], R is [[2.42445517]
 [2.93125916]
 [3.41125202]
 [3.919209  ]
 [4.3716464 ]].
[2019-04-09 14:53:24,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0137501  0.08227275 0.10970315 0.05815415 0.03586965 0.01892412
 0.17944197 0.07233369 0.08595376 0.08718778 0.25640893], sum to 1.0000
[2019-04-09 14:53:24,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8723
[2019-04-09 14:53:24,863] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.333333333333334, 67.0, 0.0, 0.0, 19.0, 26.78343962522503, 0.7374746820083627, 0.0, 1.0, 35.0, 39.88356280116131], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3363600.0000, 
sim time next is 3364200.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 19.0, 26.71785622513147, 0.7279051931721061, 0.0, 1.0, 65.0, 58.95339118120685], 
processed observation next is [1.0, 0.9565217391304348, 0.3379501385041552, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7264880187609558, 0.742635064390702, 0.0, 1.0, 1.0, 0.5895339118120685], 
reward next is 0.4105, 
noisyNet noise sample is [array([-1.0447141], dtype=float32), -1.2835001]. 
=============================================
[2019-04-09 14:53:24,958] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01595828 0.08093493 0.12787712 0.04255085 0.03455016 0.01749234
 0.20113727 0.08400514 0.08328738 0.09762915 0.21457747], sum to 1.0000
[2019-04-09 14:53:24,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-09 14:53:24,989] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.56031896637041, 0.7015474816718603, 0.0, 1.0, 55.0, 45.46560038660625], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3366600.0000, 
sim time next is 3367200.0000, 
raw observation next is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 26.53667953883891, 0.6945541699856501, 0.0, 1.0, 25.0, 38.18349095256978], 
processed observation next is [1.0, 1.0, 0.31486611265004616, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7113899615699092, 0.7315180566618834, 0.0, 1.0, 0.2, 0.38183490952569776], 
reward next is 0.6182, 
noisyNet noise sample is [array([0.17384075], dtype=float32), -0.1501933]. 
=============================================
[2019-04-09 14:53:25,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01147477 0.09648051 0.11366749 0.05767924 0.0478973  0.02017224
 0.16916002 0.06316332 0.09212958 0.07595989 0.2522156 ], sum to 1.0000
[2019-04-09 14:53:25,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7184
[2019-04-09 14:53:25,175] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 19.0, 26.95448282655719, 0.8113545072572972, 0.0, 1.0, 45.0, 37.23240452862609], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3358200.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.92974271548262, 0.8047865565435077, 0.0, 1.0, 60.0, 39.57197953201413], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7441452262902185, 0.7682621855145025, 0.0, 1.0, 0.9, 0.39571979532014134], 
reward next is 0.6043, 
noisyNet noise sample is [array([-0.02310003], dtype=float32), -1.368187]. 
=============================================
[2019-04-09 14:53:25,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01874013 0.10726821 0.09183193 0.07893638 0.0373811  0.03068421
 0.15243842 0.07443786 0.08457983 0.09038395 0.233318  ], sum to 1.0000
[2019-04-09 14:53:25,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-09 14:53:25,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.82751764205797, 0.5467394212880866, 0.0, 1.0, 60.0, 54.70369728996238], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3383400.0000, 
sim time next is 3384000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.96131585679966, 0.5483241566316684, 0.0, 1.0, 55.0, 38.85403278200064], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6634429880666385, 0.6827747188772229, 0.0, 1.0, 0.8, 0.38854032782000636], 
reward next is 0.6115, 
noisyNet noise sample is [array([-0.30694976], dtype=float32), 0.114948995]. 
=============================================
[2019-04-09 14:53:25,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.9912581]
 [1.8858032]
 [1.8750451]
 [1.7235495]
 [1.8350115]], R is [[2.46331573]
 [2.89164567]
 [3.11738849]
 [3.53748703]
 [4.1840682 ]].
[2019-04-09 14:53:25,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01990495 0.10603074 0.10544755 0.0530439  0.03411959 0.02635628
 0.13399546 0.07244042 0.10950266 0.10812988 0.23102859], sum to 1.0000
[2019-04-09 14:53:25,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0264
[2019-04-09 14:53:25,342] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 25.02682720716998, 0.3820325708138924, 1.0, 1.0, 60.0, 62.06636787728252], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 22.5, 25.22564661230585, 0.4293531486188941, 1.0, 1.0, 20.0, 44.3051704684131], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.375, 0.6021372176921543, 0.6431177162062981, 1.0, 1.0, 0.1, 0.443051704684131], 
reward next is 0.5569, 
noisyNet noise sample is [array([0.1794183], dtype=float32), -0.23040573]. 
=============================================
[2019-04-09 14:53:25,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0123308  0.12199656 0.12205612 0.06660066 0.04439374 0.0302043
 0.13803965 0.078106   0.12185    0.07139006 0.19303212], sum to 1.0000
[2019-04-09 14:53:25,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7058
[2019-04-09 14:53:25,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 22.5, 27.96569510425995, 0.9854530382978653, 1.0, 1.0, 65.0, 19.28338375571241], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3339600.0000, 
sim time next is 3340200.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 22.5, 28.06311283461407, 1.007975313783697, 1.0, 1.0, 55.0, 21.01536855887204], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.46666666666666673, 0.26, 0.6810313075506446, 0.375, 0.8385927362178393, 0.8359917712612323, 1.0, 1.0, 0.8, 0.2101536855887204], 
reward next is 0.7898, 
noisyNet noise sample is [array([-0.6234891], dtype=float32), -0.491037]. 
=============================================
[2019-04-09 14:53:25,520] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-09 14:53:25,520] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:53:25,521] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:53:25,521] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:53:25,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:53:25,521] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:53:25,523] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:53:25,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run10
[2019-04-09 14:53:25,546] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run10
[2019-04-09 14:53:25,565] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run10
[2019-04-09 14:53:31,155] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01207982], dtype=float32), 0.016941348]
[2019-04-09 14:53:31,156] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [7.7, 93.0, 0.0, 0.0, 19.0, 23.66882148065363, -0.001675462577235003, 0.0, 1.0, 60.0, 47.0955929526026]
[2019-04-09 14:53:31,156] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:53:31,156] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.0341994  0.12587953 0.0870253  0.09525444 0.07562017 0.04525105
 0.1244054  0.08123007 0.09917933 0.09627207 0.13568331], sampled 0.6424594068674991
[2019-04-09 14:53:45,534] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01207982], dtype=float32), 0.016941348]
[2019-04-09 14:53:45,534] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [0.9733298770000001, 87.71200582333333, 0.0, 0.0, 19.0, 25.58966233750976, 0.4110796831337613, 0.0, 1.0, 30.0, 38.03010261865798]
[2019-04-09 14:53:45,534] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:53:45,535] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.03558191 0.12892993 0.10095561 0.0947714  0.07910693 0.04633107
 0.12297264 0.07410157 0.08490357 0.1015688  0.13077654], sampled 0.9227284303565305
[2019-04-09 14:54:12,508] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.01207982], dtype=float32), 0.016941348]
[2019-04-09 14:54:12,508] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-2.033333333333334, 78.16666666666667, 40.00000000000001, 76.66666666666667, 19.0, 25.31930592352369, 0.3776948265111776, 0.0, 1.0, 50.0, 33.87613742501989]
[2019-04-09 14:54:12,509] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:54:12,509] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.03488868 0.11673351 0.09873828 0.09224048 0.07041639 0.04835273
 0.12858972 0.07290446 0.09505956 0.08759531 0.15448084], sampled 0.5176877574928033
[2019-04-09 14:55:03,764] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01207982], dtype=float32), 0.016941348]
[2019-04-09 14:55:03,764] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-5.666666666666666, 40.0, 0.0, 0.0, 19.0, 26.53726878737269, 0.6236060158002509, 0.0, 1.0, 25.0, 42.14275425090587]
[2019-04-09 14:55:03,764] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:55:03,765] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.02839566 0.12670416 0.10004907 0.09284227 0.06876926 0.04268648
 0.14385374 0.06532154 0.08597288 0.07824766 0.16715726], sampled 0.45997367032036474
[2019-04-09 14:55:10,828] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5683.0455 281229.9518 2755.9514
[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:10,863] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:11,024] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,474] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5453.9796 304107.6467 2337.3199
[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,509] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,548] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5369.0492 312684.3145 1885.6546
[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,577] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:19,702] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:55:20,579] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 90000, evaluation results [90000.0, 5453.979572670329, 304107.64668694604, 2337.319947347076, 5683.045530826787, 281229.9518206926, 2755.951375462436, 5369.049210727206, 312684.3145119987, 1885.6546309737678]
[2019-04-09 14:55:20,710] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02084043 0.10606013 0.10149048 0.06236825 0.04174112 0.02585625
 0.17652863 0.07432638 0.1019391  0.08288793 0.20596123], sum to 1.0000
[2019-04-09 14:55:20,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8663
[2019-04-09 14:55:20,723] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 19.0, 26.37071358347231, 0.5737824903987986, 0.0, 1.0, 60.0, 50.02957124518366], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 19.0, 26.27852979134351, 0.575869712538523, 0.0, 1.0, 65.0, 55.51503311042946], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.689877482611959, 0.6919565708461745, 0.0, 1.0, 1.0, 0.5551503311042946], 
reward next is 0.4448, 
noisyNet noise sample is [array([0.44106182], dtype=float32), -0.86918765]. 
=============================================
[2019-04-09 14:55:20,760] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0087327  0.12945624 0.12500905 0.07271083 0.02673272 0.02430254
 0.10495235 0.05077283 0.12288224 0.07186209 0.26258639], sum to 1.0000
[2019-04-09 14:55:20,761] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6364
[2019-04-09 14:55:20,772] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 22.5, 27.69344822622204, 0.9283522557411064, 1.0, 1.0, 45.0, 18.12296561573365], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3334800.0000, 
sim time next is 3335400.0000, 
raw observation next is [-3.5, 50.0, 106.0, 752.0, 22.5, 27.79794405418846, 0.9521779763890322, 1.0, 1.0, 55.0, 23.13405493068616], 
processed observation next is [1.0, 0.6086956521739131, 0.36565096952908593, 0.5, 0.35333333333333333, 0.830939226519337, 0.375, 0.8164953378490383, 0.817392658796344, 1.0, 1.0, 0.8, 0.23134054930686157], 
reward next is 0.7687, 
noisyNet noise sample is [array([-1.457453], dtype=float32), -0.5438408]. 
=============================================
[2019-04-09 14:55:21,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01718047 0.07560419 0.10977302 0.05243792 0.03827282 0.03236334
 0.18479154 0.07259268 0.08041657 0.1035718  0.23299569], sum to 1.0000
[2019-04-09 14:55:21,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2449
[2019-04-09 14:55:21,415] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.23747538544945, 0.6304455920487593, 0.0, 1.0, 50.0, 45.12276377633874], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3373800.0000, 
sim time next is 3374400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.26083265164912, 0.6280341733125546, 0.0, 1.0, 30.0, 37.32274112433761], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.68840272097076, 0.7093447244375182, 0.0, 1.0, 0.3, 0.3732274112433761], 
reward next is 0.6268, 
noisyNet noise sample is [array([0.74463785], dtype=float32), -1.1350858]. 
=============================================
[2019-04-09 14:55:21,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01540334 0.12471683 0.15211134 0.08434992 0.04199575 0.0302121
 0.11620229 0.069517   0.11382739 0.06757375 0.18409035], sum to 1.0000
[2019-04-09 14:55:21,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6713
[2019-04-09 14:55:21,638] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 22.5, 27.63932730624911, 0.8955009139793136, 1.0, 1.0, 25.0, 24.58367759258489], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 22.5, 27.68417170488162, 0.9094868425578938, 1.0, 1.0, 55.0, 21.32560266496404], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 0.375, 0.807014308740135, 0.8031622808526313, 1.0, 1.0, 0.8, 0.2132560266496404], 
reward next is 0.7867, 
noisyNet noise sample is [array([0.89027643], dtype=float32), 0.58109087]. 
=============================================
[2019-04-09 14:55:21,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[2.5730503]
 [2.5856342]
 [2.5369217]
 [2.5032213]
 [2.3548622]], R is [[3.55881405]
 [4.27738905]
 [4.96189928]
 [5.67588091]
 [6.35664845]].
[2019-04-09 14:55:22,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00773761 0.1136314  0.12449607 0.06115505 0.03838406 0.02546968
 0.12687986 0.06390134 0.11965054 0.05919746 0.2594969 ], sum to 1.0000
[2019-04-09 14:55:22,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2213
[2019-04-09 14:55:22,268] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 67.0, 12.0, 121.0, 22.5, 27.45709804202334, 1.024467290142087, 1.0, 1.0, 45.0, 46.89279299310672], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3432600.0000, 
sim time next is 3433200.0000, 
raw observation next is [2.0, 67.0, 10.0, 100.8333333333333, 22.5, 26.45926702410687, 0.9723828796802466, 1.0, 1.0, 25.0, 8.881441317886539], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.03333333333333333, 0.11141804788213625, 0.375, 0.7049389186755727, 0.8241276265600822, 1.0, 1.0, 0.2, 0.08881441317886539], 
reward next is 0.9112, 
noisyNet noise sample is [array([-0.808862], dtype=float32), -0.5658829]. 
=============================================
[2019-04-09 14:55:22,332] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01939632 0.09896524 0.10452357 0.05508075 0.03660245 0.02520191
 0.16910122 0.0703097  0.11022718 0.10653707 0.20405461], sum to 1.0000
[2019-04-09 14:55:22,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4705
[2019-04-09 14:55:22,358] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 22.5, 25.73293494965072, 0.4886421900941327, 0.0, 1.0, 65.0, 63.66176038661276], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3395400.0000, 
sim time next is 3396000.0000, 
raw observation next is [-2.666666666666667, 63.33333333333334, 0.0, 0.0, 22.5, 25.79819979730485, 0.5052116709909553, 1.0, 1.0, 45.0, 46.89559545925867], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.6333333333333334, 0.0, 0.0, 0.375, 0.6498499831087375, 0.6684038903303184, 1.0, 1.0, 0.6, 0.4689559545925867], 
reward next is 0.5310, 
noisyNet noise sample is [array([0.17924353], dtype=float32), 0.39564118]. 
=============================================
[2019-04-09 14:55:22,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[2.0656667]
 [2.0304852]
 [1.9775264]
 [2.1958447]
 [2.0597816]], R is [[2.60485291]
 [2.94218683]
 [3.27976203]
 [3.8886919 ]
 [4.47208738]].
[2019-04-09 14:55:22,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.014231   0.13456228 0.1146606  0.06024123 0.04103709 0.02631476
 0.13457546 0.07016496 0.11351123 0.08158956 0.20911185], sum to 1.0000
[2019-04-09 14:55:22,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0004
[2019-04-09 14:55:22,734] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.5, 58.0, 95.0, 579.3333333333334, 22.5, 26.99887261023548, 0.7330155245349875, 1.0, 1.0, 45.0, 33.42012630157927], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3402600.0000, 
sim time next is 3403200.0000, 
raw observation next is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 22.5, 27.12223809084442, 0.7116837148440723, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.56, 0.3233333333333333, 0.6836095764272561, 0.375, 0.7601865075703683, 0.737227904948024, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2596598], dtype=float32), 2.0912633]. 
=============================================
[2019-04-09 14:55:23,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01833672 0.09368871 0.11385736 0.05910503 0.03360388 0.02907482
 0.16193391 0.06369866 0.08700108 0.08904412 0.25065577], sum to 1.0000
[2019-04-09 14:55:23,239] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1528
[2019-04-09 14:55:23,260] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.43459153530898, 0.6373500818900979, 0.0, 1.0, 45.0, 45.18327000968439], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3376200.0000, 
sim time next is 3376800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.40693774736438, 0.6280675603000193, 0.0, 1.0, 25.0, 38.32301383852517], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7005781456136985, 0.7093558534333398, 0.0, 1.0, 0.2, 0.3832301383852517], 
reward next is 0.6168, 
noisyNet noise sample is [array([-0.10285033], dtype=float32), -0.32483277]. 
=============================================
[2019-04-09 14:55:23,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00853962 0.12044055 0.12745848 0.07107847 0.02744566 0.02480024
 0.11156309 0.05970423 0.10936745 0.07262863 0.26697358], sum to 1.0000
[2019-04-09 14:55:23,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-09 14:55:23,493] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 22.5, 27.61951043218204, 1.059435636922008, 1.0, 1.0, 55.0, 29.12164147472624], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3422400.0000, 
sim time next is 3423000.0000, 
raw observation next is [3.0, 56.5, 96.66666666666666, 751.3333333333334, 22.5, 27.07992594517698, 0.9937098418251895, 1.0, 1.0, 55.0, 9.087101924103838], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.565, 0.3222222222222222, 0.8302025782688767, 0.375, 0.7566604954314151, 0.8312366139417299, 1.0, 1.0, 0.8, 0.09087101924103838], 
reward next is 0.9091, 
noisyNet noise sample is [array([0.8491221], dtype=float32), -1.6492947]. 
=============================================
[2019-04-09 14:55:23,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00954703 0.1109448  0.11291789 0.06613778 0.03781665 0.03289624
 0.13980183 0.05362477 0.10929976 0.07481222 0.25220102], sum to 1.0000
[2019-04-09 14:55:23,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4458
[2019-04-09 14:55:23,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.8381386]
 [2.8165257]
 [2.9611676]
 [2.8764584]
 [2.9466493]], R is [[3.69308305]
 [4.36493587]
 [4.57008839]
 [5.08630896]
 [5.78433657]].
[2019-04-09 14:55:23,530] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 22.5, 27.31176103880095, 1.038272566330557, 1.0, 1.0, 55.0, 8.900928573948871], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 22.5, 28.03457989516096, 1.097903805883156, 1.0, 1.0, 45.0, 18.63534657369323], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.2161111111111111, 0.602025782688766, 0.375, 0.8362149912634133, 0.8659679352943854, 1.0, 1.0, 0.6, 0.1863534657369323], 
reward next is 0.8136, 
noisyNet noise sample is [array([0.44337958], dtype=float32), 0.26139742]. 
=============================================
[2019-04-09 14:55:23,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0148614  0.07956086 0.10027637 0.04213263 0.03479363 0.01973473
 0.15258573 0.05835669 0.07914424 0.08693042 0.33162338], sum to 1.0000
[2019-04-09 14:55:23,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7736
[2019-04-09 14:55:23,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.07145790790143, 0.8383285633851104, 0.0, 1.0, 30.0, 36.84169996903496], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3454800.0000, 
sim time next is 3455400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.03875316361076, 0.8348756523469287, 0.0, 1.0, 65.0, 43.65967768635004], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7532294303008967, 0.7782918841156429, 0.0, 1.0, 1.0, 0.43659677686350035], 
reward next is 0.5634, 
noisyNet noise sample is [array([0.683507], dtype=float32), 1.9219493]. 
=============================================
[2019-04-09 14:55:24,093] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01404807 0.07903694 0.12003583 0.0517842  0.03289557 0.02224274
 0.16883013 0.08106411 0.08183026 0.10778121 0.24045093], sum to 1.0000
[2019-04-09 14:55:24,095] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9743
[2019-04-09 14:55:24,107] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 26.95938485105688, 0.841958696423827, 0.0, 1.0, 65.0, 41.12247955393808], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3453600.0000, 
sim time next is 3454200.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.00456663996207, 0.8346147993670492, 0.0, 1.0, 65.0, 40.47688113078488], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7503805533301726, 0.7782049331223497, 0.0, 1.0, 1.0, 0.4047688113078488], 
reward next is 0.5952, 
noisyNet noise sample is [array([0.01600285], dtype=float32), -0.21448134]. 
=============================================
[2019-04-09 14:55:24,139] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01331657 0.0952125  0.12823531 0.04962907 0.03304319 0.02526006
 0.16618016 0.06967632 0.09962132 0.0811464  0.23867905], sum to 1.0000
[2019-04-09 14:55:24,139] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1920
[2019-04-09 14:55:24,157] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 26.92842889145145, 0.8109700602057456, 0.0, 1.0, 65.0, 41.40180462164305], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 26.95368050563335, 0.8128142713587162, 0.0, 1.0, 25.0, 38.50664454860221], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7461400421361125, 0.7709380904529054, 0.0, 1.0, 0.2, 0.3850664454860221], 
reward next is 0.6149, 
noisyNet noise sample is [array([-1.6565343], dtype=float32), -2.7081127]. 
=============================================
[2019-04-09 14:55:24,235] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01143139 0.11259928 0.1372144  0.05398839 0.0365033  0.01664678
 0.16115455 0.06898849 0.10395046 0.08267029 0.21485277], sum to 1.0000
[2019-04-09 14:55:24,235] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-09 14:55:24,250] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 82.5, 0.0, 0.0, 19.0, 27.28240090942218, 0.9329719024321297, 0.0, 1.0, 45.0, 31.28768333304863], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3447000.0000, 
sim time next is 3447600.0000, 
raw observation next is [1.0, 83.66666666666666, 0.0, 0.0, 19.0, 27.27305076955513, 0.9300588170696207, 0.0, 1.0, 55.0, 38.94509697632899], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8366666666666666, 0.0, 0.0, 0.08333333333333333, 0.7727542307962608, 0.8100196056898735, 0.0, 1.0, 0.8, 0.3894509697632899], 
reward next is 0.6105, 
noisyNet noise sample is [array([0.8450945], dtype=float32), -0.27917448]. 
=============================================
[2019-04-09 14:55:24,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01605131 0.09380938 0.11549991 0.04772729 0.03306533 0.02266054
 0.17040779 0.06694672 0.08515571 0.08741745 0.26125857], sum to 1.0000
[2019-04-09 14:55:24,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6311
[2019-04-09 14:55:24,356] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 27.0870062050247, 0.8140843653148874, 0.0, 1.0, 50.0, 31.98552968335092], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3462600.0000, 
sim time next is 3463200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 27.06882068820358, 0.805390533359842, 0.0, 1.0, 55.0, 39.4236946842583], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7557350573502983, 0.7684635111199474, 0.0, 1.0, 0.8, 0.39423694684258304], 
reward next is 0.6058, 
noisyNet noise sample is [array([-0.68389076], dtype=float32), 0.85389066]. 
=============================================
[2019-04-09 14:55:24,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00727849 0.12190931 0.11715011 0.05720387 0.03508152 0.01980642
 0.1496403  0.07273255 0.0971127  0.08983602 0.23224871], sum to 1.0000
[2019-04-09 14:55:24,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8511
[2019-04-09 14:55:24,853] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 75.0, 0.0, 0.0, 22.5, 27.73270751862713, 1.018459709621707, 1.0, 1.0, 50.0, 32.22585693114116], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [1.166666666666667, 77.0, 0.0, 0.0, 22.5, 27.73993005664935, 1.020757230594903, 1.0, 1.0, 65.0, 28.28407869136739], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.77, 0.0, 0.0, 0.375, 0.8116608380541125, 0.840252410198301, 1.0, 1.0, 1.0, 0.2828407869136739], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.6224172], dtype=float32), -0.37919626]. 
=============================================
[2019-04-09 14:55:25,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0103742  0.0861226  0.12842636 0.03654247 0.03722635 0.01817992
 0.17784388 0.07401118 0.08011013 0.09724707 0.25391582], sum to 1.0000
[2019-04-09 14:55:25,041] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6640
[2019-04-09 14:55:25,058] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.0449872708128, 0.8435472229447339, 0.0, 1.0, 65.0, 42.62159346170908], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3452400.0000, 
sim time next is 3453000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.0186715171123, 0.845312205045424, 0.0, 1.0, 60.0, 42.44075338087602], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7515559597593583, 0.7817707350151414, 0.0, 1.0, 0.9, 0.4244075338087602], 
reward next is 0.5756, 
noisyNet noise sample is [array([-0.9710147], dtype=float32), 0.36096475]. 
=============================================
[2019-04-09 14:55:25,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.516728 ]
 [2.3158836]
 [2.5189435]
 [2.4928784]
 [2.5496354]], R is [[2.84075356]
 [3.38613009]
 [3.98583841]
 [4.61863756]
 [5.16662025]].
[2019-04-09 14:55:25,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01409769 0.08984192 0.11968236 0.04528125 0.03515507 0.02582882
 0.15537846 0.07390691 0.08984586 0.1036749  0.24730667], sum to 1.0000
[2019-04-09 14:55:25,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5036
[2019-04-09 14:55:25,272] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 26.89430693176573, 0.8203124425493583, 0.0, 1.0, 55.0, 41.87913035615395], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 26.94385745856115, 0.8238013874275175, 0.0, 1.0, 50.0, 32.8143023685909], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7453214548800959, 0.7746004624758392, 0.0, 1.0, 0.7, 0.32814302368590903], 
reward next is 0.6719, 
noisyNet noise sample is [array([0.28322536], dtype=float32), 0.8108184]. 
=============================================
[2019-04-09 14:55:25,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02475564 0.0831034  0.11720716 0.05884422 0.04136324 0.02534483
 0.16239622 0.06769984 0.09221659 0.07465349 0.2524154 ], sum to 1.0000
[2019-04-09 14:55:25,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7968
[2019-04-09 14:55:25,344] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 19.0, 27.04409894735102, 0.8055757638368847, 0.0, 1.0, 65.0, 42.13334910332229], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 19.0, 27.04518704264723, 0.8057676175172331, 0.0, 1.0, 65.0, 40.25228447661912], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.753765586887269, 0.7685892058390777, 0.0, 1.0, 1.0, 0.40252284476619116], 
reward next is 0.5975, 
noisyNet noise sample is [array([-0.93329495], dtype=float32), 0.8574784]. 
=============================================
[2019-04-09 14:55:25,393] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01631415 0.06548782 0.1207414  0.04347764 0.03442913 0.02643067
 0.20930803 0.0727578  0.06498133 0.10327087 0.24280113], sum to 1.0000
[2019-04-09 14:55:25,393] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9402
[2019-04-09 14:55:25,413] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 26.98496886359851, 0.8146523441791933, 0.0, 1.0, 60.0, 42.11088320220024], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 19.0, 26.9997484561145, 0.8088907779519806, 0.0, 1.0, 65.0, 50.11040525547354], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.08333333333333333, 0.7499790380095416, 0.7696302593173269, 0.0, 1.0, 1.0, 0.5011040525547353], 
reward next is 0.4989, 
noisyNet noise sample is [array([-1.5251434], dtype=float32), -0.5982069]. 
=============================================
[2019-04-09 14:55:25,739] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01404052 0.10816778 0.107647   0.04684925 0.03948107 0.0253652
 0.19358075 0.06014287 0.09138736 0.09413189 0.2192063 ], sum to 1.0000
[2019-04-09 14:55:25,742] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7081
[2019-04-09 14:55:25,761] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.5, 71.5, 3.0, 107.0, 22.5, 26.71319364913111, 0.7273583884541085, 1.0, 1.0, 55.0, 48.01541360073538], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 22.5, 26.75035182701315, 0.7253947828184443, 0.0, 1.0, 60.0, 56.5820541359703], 
processed observation next is [1.0, 0.30434782608695654, 0.44413665743305636, 0.7133333333333333, 0.0572222222222222, 0.17200736648250467, 0.375, 0.7291959855844293, 0.7417982609394814, 0.0, 1.0, 0.9, 0.565820541359703], 
reward next is 0.4342, 
noisyNet noise sample is [array([2.0393076], dtype=float32), -1.4597943]. 
=============================================
[2019-04-09 14:55:25,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02038001 0.08533017 0.11982661 0.05767933 0.04498489 0.02404991
 0.16526785 0.07373846 0.11023523 0.09928324 0.19922431], sum to 1.0000
[2019-04-09 14:55:25,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6990
[2019-04-09 14:55:25,918] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.63922274782489, 0.7079010625760151, 0.0, 1.0, 60.0, 46.99096500230057], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.62342379331699, 0.7102681262050128, 0.0, 1.0, 55.0, 46.76129773371014], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7186186494430826, 0.7367560420683376, 0.0, 1.0, 0.8, 0.4676129773371014], 
reward next is 0.5324, 
noisyNet noise sample is [array([-0.9435647], dtype=float32), -1.1583542]. 
=============================================
[2019-04-09 14:55:26,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02456449 0.09718386 0.10894252 0.05707886 0.04330742 0.0290946
 0.1523159  0.08189706 0.09908218 0.10178307 0.20475009], sum to 1.0000
[2019-04-09 14:55:26,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-09 14:55:26,112] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01658075 0.09268275 0.11416361 0.0585751  0.03524944 0.02706583
 0.18679188 0.08316019 0.09103381 0.0772465  0.21745016], sum to 1.0000
[2019-04-09 14:55:26,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7159
[2019-04-09 14:55:26,131] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 19.0, 27.00787598666408, 0.6974212987572882, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3470400.0000, 
sim time next is 3471000.0000, 
raw observation next is [0.8333333333333334, 67.83333333333334, 0.0, 0.0, 19.0, 26.72749925768738, 0.7369649455855067, 0.0, 1.0, 25.0, 66.64990419411873], 
processed observation next is [1.0, 0.17391304347826086, 0.4856879039704525, 0.6783333333333335, 0.0, 0.0, 0.08333333333333333, 0.7272916048072817, 0.7456549818618355, 0.0, 1.0, 0.2, 0.6664990419411874], 
reward next is 0.3335, 
noisyNet noise sample is [array([1.1963302], dtype=float32), 0.040863406]. 
=============================================
[2019-04-09 14:55:26,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.0124843]
 [2.1684062]
 [2.05167  ]
 [2.0892463]
 [2.0663538]], R is [[2.53707838]
 [3.51170754]
 [4.05799103]
 [4.59039307]
 [5.15485811]].
[2019-04-09 14:55:26,167] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.77566019604493, 0.7418683542911956, 0.0, 1.0, 65.0, 46.84682429515033], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3477000.0000, 
sim time next is 3477600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.81761798950124, 0.7350620458529974, 0.0, 1.0, 60.0, 37.74036176253021], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7348014991251034, 0.7450206819509991, 0.0, 1.0, 0.9, 0.3774036176253021], 
reward next is 0.6226, 
noisyNet noise sample is [array([0.70898014], dtype=float32), -0.8871285]. 
=============================================
[2019-04-09 14:55:26,178] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01055797 0.16574988 0.12395459 0.07543094 0.03418474 0.01942622
 0.09650505 0.06055811 0.15620522 0.07553197 0.18189523], sum to 1.0000
[2019-04-09 14:55:26,178] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6092
[2019-04-09 14:55:26,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01761513 0.08744399 0.11316478 0.05931316 0.03711659 0.02581437
 0.17934597 0.06994624 0.10780241 0.09114611 0.21129131], sum to 1.0000
[2019-04-09 14:55:26,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9026
[2019-04-09 14:55:26,201] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 22.5, 27.84936875488786, 0.9670277390676675, 1.0, 1.0, 45.0, 25.43206937483549], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3491400.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 22.5, 27.93093242930592, 0.9838505575967597, 1.0, 1.0, 55.0, 25.17110357016187], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.375, 0.8275777024421599, 0.8279501858655866, 1.0, 1.0, 0.8, 0.2517110357016187], 
reward next is 0.7483, 
noisyNet noise sample is [array([1.7097397], dtype=float32), 1.1725597]. 
=============================================
[2019-04-09 14:55:26,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.7500255657245, 0.7387709095504827, 0.0, 1.0, 50.0, 37.72341605987923], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3474600.0000, 
sim time next is 3475200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.73545104694778, 0.7412076573786953, 0.0, 1.0, 65.0, 59.48703806779397], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.727954253912315, 0.7470692191262317, 0.0, 1.0, 1.0, 0.5948703806779396], 
reward next is 0.4051, 
noisyNet noise sample is [array([-0.82594824], dtype=float32), 1.6689426]. 
=============================================
[2019-04-09 14:55:26,212] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[2.7818778]
 [2.4477003]
 [2.6738827]
 [2.5807726]
 [2.4563005]], R is [[3.36647415]
 [4.07848883]
 [4.78036165]
 [5.43763685]
 [6.08771801]].
[2019-04-09 14:55:26,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00791727 0.11223441 0.13290098 0.06709394 0.04120687 0.02011394
 0.11702327 0.03729062 0.10081369 0.0529203  0.31048477], sum to 1.0000
[2019-04-09 14:55:26,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7571
[2019-04-09 14:55:26,703] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 51.0, 115.1666666666667, 808.8333333333334, 22.5, 27.70722167212617, 1.128884088022426, 1.0, 1.0, 60.0, 19.40024282148154], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3504000.0000, 
sim time next is 3504600.0000, 
raw observation next is [2.5, 50.5, 115.0, 806.0, 22.5, 27.26318588185018, 1.062026977404203, 1.0, 1.0, 65.0, 1.767228979859688], 
processed observation next is [1.0, 0.5652173913043478, 0.5318559556786704, 0.505, 0.38333333333333336, 0.8906077348066298, 0.375, 0.7719321568208484, 0.8540089924680677, 1.0, 1.0, 1.0, 0.01767228979859688], 
reward next is 0.9823, 
noisyNet noise sample is [array([0.8901753], dtype=float32), -0.4850566]. 
=============================================
[2019-04-09 14:55:26,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00886478 0.14151502 0.12778875 0.07805694 0.03614651 0.02633339
 0.11500934 0.04857465 0.1335447  0.08508984 0.19907613], sum to 1.0000
[2019-04-09 14:55:26,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-09 14:55:26,915] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 49.0, 102.0, 781.0, 22.5, 28.58854002881565, 1.184178811147917, 1.0, 1.0, 20.0, 12.97534802929589], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3508200.0000, 
sim time next is 3508800.0000, 
raw observation next is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 22.5, 28.64357826365442, 1.062299679259845, 1.0, 1.0, 55.0, 62.56750345857576], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3322222222222222, 0.8456721915285451, 0.375, 0.886964855304535, 0.854099893086615, 1.0, 1.0, 0.8, 0.6256750345857576], 
reward next is 0.3743, 
noisyNet noise sample is [array([-1.9307384], dtype=float32), -1.3255895]. 
=============================================
[2019-04-09 14:55:26,986] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00903137 0.11674039 0.12100688 0.03997093 0.04548717 0.01710599
 0.16320547 0.06808932 0.09356043 0.08143532 0.24436666], sum to 1.0000
[2019-04-09 14:55:26,987] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-09 14:55:26,999] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 19.0, 27.32272326172128, 0.9428447196210522, 1.0, 1.0, 65.0, 36.20091801677289], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3442200.0000, 
sim time next is 3442800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 27.34109584611648, 0.9400952913477241, 0.0, 1.0, 30.0, 30.38252980579566], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.77842465384304, 0.813365097115908, 0.0, 1.0, 0.3, 0.3038252980579566], 
reward next is 0.6962, 
noisyNet noise sample is [array([-0.8767386], dtype=float32), 0.19268954]. 
=============================================
[2019-04-09 14:55:27,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00826911 0.10765762 0.13499732 0.04912413 0.03732269 0.01589132
 0.18182726 0.06544302 0.08656897 0.0879554  0.22494318], sum to 1.0000
[2019-04-09 14:55:27,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6463
[2019-04-09 14:55:27,122] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.41902561954789, 1.005853498712399, 0.0, 1.0, 65.0, 34.83161836519392], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.44316901328866, 0.9991144744586578, 0.0, 1.0, 55.0, 28.79622513027281], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7869307511073883, 0.833038158152886, 0.0, 1.0, 0.8, 0.28796225130272807], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.40792674], dtype=float32), -1.7025741]. 
=============================================
[2019-04-09 14:55:27,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01093175 0.07924399 0.08968291 0.03791652 0.03149588 0.02099297
 0.20562536 0.06144269 0.07995669 0.07903574 0.30367544], sum to 1.0000
[2019-04-09 14:55:27,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3371
[2019-04-09 14:55:27,223] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 26.9847944721167, 0.8312136323346068, 0.0, 1.0, 25.0, 39.31499985367701], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 26.99669065866183, 0.8316392881724308, 0.0, 1.0, 45.0, 33.31844634030767], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7497242215551525, 0.7772130960574769, 0.0, 1.0, 0.6, 0.3331844634030767], 
reward next is 0.6668, 
noisyNet noise sample is [array([-1.025821], dtype=float32), 0.9839011]. 
=============================================
[2019-04-09 14:55:27,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02074051 0.09528073 0.10316413 0.05803824 0.03727251 0.02995499
 0.16810581 0.06569222 0.09305929 0.09200204 0.23668957], sum to 1.0000
[2019-04-09 14:55:27,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2365
[2019-04-09 14:55:27,369] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 71.5, 3.0, 107.0, 22.5, 26.68851431516314, 0.7194610357677472, 1.0, 1.0, 45.0, 55.99923895468215], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 22.5, 26.73614019915863, 0.7160369076796952, 0.0, 1.0, 30.0, 43.17082194755086], 
processed observation next is [1.0, 0.30434782608695654, 0.44413665743305636, 0.7133333333333333, 0.0572222222222222, 0.17200736648250467, 0.375, 0.7280116832632192, 0.7386789692265651, 0.0, 1.0, 0.3, 0.43170821947550864], 
reward next is 0.5683, 
noisyNet noise sample is [array([-0.7932672], dtype=float32), -0.83626497]. 
=============================================
[2019-04-09 14:55:27,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0095078  0.13029875 0.12705335 0.05056531 0.03771688 0.02342997
 0.1733886  0.06691536 0.08515009 0.10297452 0.19299936], sum to 1.0000
[2019-04-09 14:55:27,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1502
[2019-04-09 14:55:27,786] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 28.29740036871414, 1.112323158191096, 1.0, 1.0, 65.0, 23.99575573995662], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3522600.0000, 
sim time next is 3523200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 28.15545507121176, 1.099655642472509, 1.0, 1.0, 25.0, 25.6811023710963], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8462879226009802, 0.8665518808241698, 1.0, 1.0, 0.2, 0.25681102371096304], 
reward next is 0.7432, 
noisyNet noise sample is [array([-1.8821522], dtype=float32), -0.5300762]. 
=============================================
[2019-04-09 14:55:28,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01580009 0.08760954 0.10861123 0.04557761 0.03484022 0.02445231
 0.19335416 0.07149278 0.07368485 0.08915274 0.25542444], sum to 1.0000
[2019-04-09 14:55:28,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9139
[2019-04-09 14:55:28,260] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 27.02512517498503, 0.832699351857665, 0.0, 1.0, 65.0, 40.27110772510641], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 27.03678922309979, 0.8333383072169385, 0.0, 1.0, 65.0, 40.60940654401822], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7530657685916493, 0.7777794357389795, 0.0, 1.0, 1.0, 0.40609406544018223], 
reward next is 0.5939, 
noisyNet noise sample is [array([0.4006468], dtype=float32), -0.7333168]. 
=============================================
[2019-04-09 14:55:28,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00867437 0.11861252 0.13216527 0.04406857 0.0293068  0.02038817
 0.13252378 0.07005699 0.15138152 0.06702891 0.22579311], sum to 1.0000
[2019-04-09 14:55:28,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7531
[2019-04-09 14:55:28,276] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01193038 0.09669138 0.12795135 0.05142052 0.03432515 0.02113546
 0.1754655  0.07117607 0.08418168 0.088165   0.23755762], sum to 1.0000
[2019-04-09 14:55:28,276] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3437
[2019-04-09 14:55:28,290] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 73.0, 0.0, 0.0, 19.0, 27.41899456187862, 0.9951847372464703, 0.0, 1.0, 65.0, 34.90792910159762], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 19.0, 27.45686165445817, 0.9904152115401419, 0.0, 1.0, 45.0, 33.12789512650943], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7880718045381808, 0.830138403846714, 0.0, 1.0, 0.6, 0.33127895126509427], 
reward next is 0.6687, 
noisyNet noise sample is [array([-1.2621939], dtype=float32), 0.09759108]. 
=============================================
[2019-04-09 14:55:28,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 22.5, 27.9118785033041, 1.185305618844227, 1.0, 1.0, 65.0, 18.309657403516], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3507600.0000, 
sim time next is 3508200.0000, 
raw observation next is [3.0, 49.0, 102.0, 781.0, 22.5, 27.45449958264149, 1.116345034663598, 1.0, 1.0, 60.0, 0.860486672543135], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.34, 0.8629834254143647, 0.375, 0.7878749652201241, 0.8721150115545327, 1.0, 1.0, 0.9, 0.00860486672543135], 
reward next is 0.9914, 
noisyNet noise sample is [array([-1.2083157], dtype=float32), -2.1600661]. 
=============================================
[2019-04-09 14:55:28,648] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01492406 0.08388508 0.12972644 0.05904014 0.03117676 0.02577576
 0.1596308  0.06485432 0.098569   0.09511998 0.23729774], sum to 1.0000
[2019-04-09 14:55:28,648] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3270
[2019-04-09 14:55:28,674] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 19.0, 26.79752631775872, 0.7440603863327361, 0.0, 1.0, 65.0, 45.30669039212968], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.76368179249059, 0.7397383234517739, 0.0, 1.0, 40.0, 44.44446649564086], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7303068160408825, 0.7465794411505913, 0.0, 1.0, 0.5, 0.4444446649564086], 
reward next is 0.5556, 
noisyNet noise sample is [array([0.17557868], dtype=float32), -1.0414568]. 
=============================================
[2019-04-09 14:55:28,683] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01527575 0.10055406 0.11307342 0.04654912 0.03701388 0.02411352
 0.1780498  0.06867152 0.09020877 0.11067751 0.21581255], sum to 1.0000
[2019-04-09 14:55:28,684] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[2.0818202]
 [2.1101084]
 [2.1207979]
 [2.1337538]
 [2.134935 ]], R is [[2.59886479]
 [3.11980939]
 [3.68973565]
 [4.25564289]
 [4.80912209]].
[2019-04-09 14:55:28,688] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9287
[2019-04-09 14:55:28,712] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 63.0, 0.0, 0.0, 19.0, 27.09974436816, 0.8775148690378533, 0.0, 1.0, 20.0, 40.22311407923972], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3540600.0000, 
sim time next is 3541200.0000, 
raw observation next is [-1.666666666666667, 62.0, 0.0, 0.0, 19.0, 27.06482725555946, 0.8705601792429544, 0.0, 1.0, 65.0, 43.3539415468981], 
processed observation next is [1.0, 1.0, 0.4164358264081256, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7554022712966217, 0.7901867264143182, 0.0, 1.0, 1.0, 0.433539415468981], 
reward next is 0.5665, 
noisyNet noise sample is [array([1.5686021], dtype=float32), -0.96422213]. 
=============================================
[2019-04-09 14:55:28,879] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00786699 0.10046997 0.1050315  0.04951205 0.03785416 0.01747154
 0.14253306 0.06993861 0.11686924 0.08966722 0.2627857 ], sum to 1.0000
[2019-04-09 14:55:28,879] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9341
[2019-04-09 14:55:28,902] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.59109444201719, 1.007562611047155, 0.0, 1.0, 45.0, 27.68522059902534], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3528000.0000, 
sim time next is 3528600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.54089897574635, 1.000679391503047, 0.0, 1.0, 65.0, 33.39775859863872], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7950749146455293, 0.8335597971676822, 0.0, 1.0, 1.0, 0.33397758598638716], 
reward next is 0.6660, 
noisyNet noise sample is [array([0.08732137], dtype=float32), 0.12836862]. 
=============================================
[2019-04-09 14:55:29,234] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02152241 0.11445975 0.09247175 0.08573804 0.05746346 0.03421387
 0.14812873 0.06740641 0.0986464  0.08860286 0.19134633], sum to 1.0000
[2019-04-09 14:55:29,239] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6603
[2019-04-09 14:55:29,266] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 19.0, 26.84562674502729, 0.7903633640297283, 0.0, 1.0, 45.0, 36.88527953405215], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3551400.0000, 
sim time next is 3552000.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 19.0, 26.80376973156257, 0.7821385515169589, 0.0, 1.0, 55.0, 41.97819420679079], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7336474776302143, 0.760712850505653, 0.0, 1.0, 0.8, 0.41978194206790787], 
reward next is 0.5802, 
noisyNet noise sample is [array([0.48852536], dtype=float32), 0.86611944]. 
=============================================
[2019-04-09 14:55:29,301] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.4701039]
 [1.4563144]
 [1.5562407]
 [1.5865036]
 [1.5942461]], R is [[2.04752445]
 [2.65819645]
 [3.24889565]
 [3.86575031]
 [4.44082403]].
[2019-04-09 14:55:29,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00763965 0.12434771 0.11951377 0.06163678 0.04384483 0.02003916
 0.14654548 0.07521424 0.10696643 0.06622643 0.2280254 ], sum to 1.0000
[2019-04-09 14:55:29,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2831
[2019-04-09 14:55:29,437] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 28.29411072356606, 1.118514650363454, 1.0, 1.0, 65.0, 24.04937334620064], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3522600.0000, 
sim time next is 3523200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 28.1584562736904, 1.106399430221656, 1.0, 1.0, 30.0, 25.38131115017108], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8465380228075334, 0.8687998100738854, 1.0, 1.0, 0.3, 0.2538131115017108], 
reward next is 0.7462, 
noisyNet noise sample is [array([0.48070252], dtype=float32), 1.098501]. 
=============================================
[2019-04-09 14:55:29,583] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01830971 0.12412754 0.11618593 0.08637641 0.04124673 0.0276509
 0.12947495 0.07376917 0.10710196 0.06942219 0.20633452], sum to 1.0000
[2019-04-09 14:55:29,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2137
[2019-04-09 14:55:29,617] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.8333333333333334, 69.16666666666667, 95.33333333333334, 579.6666666666667, 22.5, 27.34135175076914, 0.8787107435691993, 1.0, 1.0, 25.0, 33.14370690519937], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3489000.0000, 
sim time next is 3489600.0000, 
raw observation next is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 22.5, 27.48441374009284, 0.8838104136343867, 1.0, 1.0, 25.0, 30.36272598448537], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6733333333333335, 0.32388888888888884, 0.6904235727440148, 0.375, 0.7903678116744034, 0.7946034712114622, 1.0, 1.0, 0.2, 0.30362725984485367], 
reward next is 0.6964, 
noisyNet noise sample is [array([2.0203378], dtype=float32), -1.8217431]. 
=============================================
[2019-04-09 14:55:29,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01034497 0.12106523 0.13011466 0.05315912 0.04464933 0.01712729
 0.16547188 0.04754754 0.13789184 0.06203131 0.21059689], sum to 1.0000
[2019-04-09 14:55:29,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-09 14:55:29,969] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.34941060847281, 0.9897648167565554, 0.0, 1.0, 60.0, 54.84024620631085], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3529800.0000, 
sim time next is 3530400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.3447942693663, 0.995767118652231, 0.0, 1.0, 25.0, 32.75119561568397], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.778732855780525, 0.831922372884077, 0.0, 1.0, 0.2, 0.3275119561568397], 
reward next is 0.6725, 
noisyNet noise sample is [array([0.02817339], dtype=float32), 1.1968633]. 
=============================================
[2019-04-09 14:55:30,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0146295  0.08383956 0.12478415 0.0526799  0.04200858 0.02094531
 0.17085448 0.0558222  0.10922241 0.07339147 0.25182232], sum to 1.0000
[2019-04-09 14:55:30,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3416
[2019-04-09 14:55:30,170] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 74.0, 0.0, 0.0, 19.0, 27.45214131280724, 0.9896456540260189, 0.0, 1.0, 60.0, 33.34678529534618], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3532800.0000, 
sim time next is 3533400.0000, 
raw observation next is [-0.5, 75.0, 0.0, 0.0, 19.0, 27.41978171427941, 0.9817019582597064, 0.0, 1.0, 45.0, 28.82373499976063], 
processed observation next is [1.0, 0.9130434782608695, 0.44875346260387816, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7849818095232841, 0.8272339860865688, 0.0, 1.0, 0.6, 0.2882373499976063], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.37298337], dtype=float32), 0.5045279]. 
=============================================
[2019-04-09 14:55:30,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00852399 0.13286605 0.13901153 0.04567157 0.02945816 0.02056237
 0.11388548 0.06689867 0.1340211  0.09662677 0.21247429], sum to 1.0000
[2019-04-09 14:55:30,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9770
[2019-04-09 14:55:30,367] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 75.0, 606.0, 22.5, 28.90626528760664, 1.108298819426841, 1.0, 1.0, 25.0, 60.64690633281275], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3513600.0000, 
sim time next is 3514200.0000, 
raw observation next is [3.0, 49.0, 70.66666666666666, 579.0, 22.5, 28.20974938390888, 1.225803199899044, 1.0, 1.0, 65.0, 18.73003409721852], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.23555555555555552, 0.6397790055248619, 0.375, 0.8508124486590732, 0.9086010666330147, 1.0, 1.0, 1.0, 0.1873003409721852], 
reward next is 0.8127, 
noisyNet noise sample is [array([-0.55594176], dtype=float32), 1.4197025]. 
=============================================
[2019-04-09 14:55:30,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00943485 0.12299705 0.153877   0.04520713 0.03145514 0.02108735
 0.10546045 0.07318868 0.13219099 0.10255188 0.2025495 ], sum to 1.0000
[2019-04-09 14:55:30,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5098
[2019-04-09 14:55:30,423] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.0, 49.0, 62.0, 525.0, 22.5, 28.21454163094223, 1.178446111822704, 1.0, 1.0, 25.0, 19.61185004597158], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3515400.0000, 
sim time next is 3516000.0000, 
raw observation next is [3.0, 49.0, 53.83333333333334, 462.6666666666667, 22.5, 28.60039886478904, 1.192480939432039, 1.0, 1.0, 50.0, 6.478359153113797], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.1794444444444445, 0.5112338858195212, 0.375, 0.8833665720657532, 0.8974936464773463, 1.0, 1.0, 0.7, 0.06478359153113797], 
reward next is 0.9352, 
noisyNet noise sample is [array([-0.55594176], dtype=float32), 1.4197025]. 
=============================================
[2019-04-09 14:55:30,431] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[3.1013691]
 [3.0049858]
 [3.0687764]
 [3.2386963]
 [3.0628798]], R is [[3.95273113]
 [4.71708536]
 [5.66991472]
 [6.42591524]
 [6.75518703]].
[2019-04-09 14:55:30,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01417795 0.0805402  0.09770921 0.03820451 0.02795286 0.02386845
 0.2003639  0.06553292 0.07822067 0.10572787 0.2677014 ], sum to 1.0000
[2019-04-09 14:55:30,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8009
[2019-04-09 14:55:30,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02450691 0.12273193 0.11765437 0.10759928 0.05803329 0.03141388
 0.11378844 0.06466004 0.08932044 0.08645135 0.18384002], sum to 1.0000
[2019-04-09 14:55:30,570] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3735
[2019-04-09 14:55:30,586] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 54.83333333333334, 115.3333333333333, 818.3333333333334, 19.0, 26.73079006196586, 0.79380509807424, 0.0, 1.0, 65.0, 40.97592819846216], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3585000.0000, 
sim time next is 3585600.0000, 
raw observation next is [-3.0, 55.0, 116.0, 819.5, 19.0, 26.76370716138671, 0.8014632331075663, 0.0, 1.0, 35.0, 34.1529860843014], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.38666666666666666, 0.905524861878453, 0.08333333333333333, 0.7303089301155591, 0.7671544110358554, 0.0, 1.0, 0.4, 0.341529860843014], 
reward next is 0.6585, 
noisyNet noise sample is [array([-0.65992224], dtype=float32), 1.0080408]. 
=============================================
[2019-04-09 14:55:30,594] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.833333333333333, 61.0, 0.0, 0.0, 19.0, 26.89317547334828, 0.852429700588139, 0.0, 1.0, 45.0, 41.49327479863471], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3541800.0000, 
sim time next is 3542400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.90885624215419, 0.8529422860455389, 0.0, 1.0, 65.0, 44.72436751761041], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7424046868461825, 0.7843140953485129, 0.0, 1.0, 1.0, 0.4472436751761041], 
reward next is 0.5528, 
noisyNet noise sample is [array([-1.9261318], dtype=float32), -0.6773272]. 
=============================================
[2019-04-09 14:55:30,600] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 94788: loss 38.9801
[2019-04-09 14:55:30,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 94788: learning rate 0.0000
[2019-04-09 14:55:31,317] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95160: loss 25.7044
[2019-04-09 14:55:31,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95162: learning rate 0.0000
[2019-04-09 14:55:31,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02988387 0.10020211 0.10362315 0.09003002 0.06702092 0.04515529
 0.13319805 0.06631901 0.09768625 0.08780587 0.17907543], sum to 1.0000
[2019-04-09 14:55:31,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-09 14:55:31,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.22872977314435, 0.6292235124401725, 0.0, 1.0, 20.0, 36.64133981850696], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.18554957603532, 0.6097412878012848, 0.0, 1.0, 25.0, 30.94090691287461], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6821291313362767, 0.7032470959337616, 0.0, 1.0, 0.2, 0.3094090691287461], 
reward next is 0.6906, 
noisyNet noise sample is [array([2.2020173], dtype=float32), -0.23212105]. 
=============================================
[2019-04-09 14:55:31,706] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 95364: loss 31.0825
[2019-04-09 14:55:31,707] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 95364: learning rate 0.0000
[2019-04-09 14:55:31,811] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95425: loss 32.3220
[2019-04-09 14:55:31,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95425: learning rate 0.0000
[2019-04-09 14:55:31,886] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95462: loss 31.2195
[2019-04-09 14:55:31,888] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95463: learning rate 0.0000
[2019-04-09 14:55:31,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02592143 0.12868945 0.11107841 0.08252621 0.07019679 0.04367599
 0.1430213  0.06172369 0.09609659 0.09258257 0.1444876 ], sum to 1.0000
[2019-04-09 14:55:31,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2871
[2019-04-09 14:55:31,961] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 19.0, 25.72588083867984, 0.6039159841122215, 0.0, 1.0, 60.0, 47.79682975971669], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3573600.0000, 
sim time next is 3574200.0000, 
raw observation next is [-6.166666666666666, 70.0, 92.0, 508.6666666666666, 19.0, 25.81978882731776, 0.6151838396233319, 0.0, 1.0, 30.0, 42.36843114037884], 
processed observation next is [0.0, 0.34782608695652173, 0.29178208679593726, 0.7, 0.30666666666666664, 0.5620626151012891, 0.08333333333333333, 0.6516490689431466, 0.7050612798744439, 0.0, 1.0, 0.3, 0.42368431140378837], 
reward next is 0.5763, 
noisyNet noise sample is [array([-0.716098], dtype=float32), -0.03593289]. 
=============================================
[2019-04-09 14:55:31,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03046882 0.09867702 0.10844071 0.09372532 0.07396625 0.04924499
 0.133846   0.06931007 0.09858344 0.08363044 0.16010685], sum to 1.0000
[2019-04-09 14:55:31,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0366
[2019-04-09 14:55:32,015] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 19.0, 25.35590189088458, 0.5311369739748261, 0.0, 1.0, 55.0, 51.79400945967744], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [-6.5, 70.0, 88.0, 425.0, 19.0, 25.45560920955712, 0.5456533702668542, 0.0, 1.0, 20.0, 35.20619343621787], 
processed observation next is [0.0, 0.34782608695652173, 0.28254847645429365, 0.7, 0.29333333333333333, 0.4696132596685083, 0.08333333333333333, 0.6213007674630934, 0.681884456755618, 0.0, 1.0, 0.1, 0.35206193436217875], 
reward next is 0.6479, 
noisyNet noise sample is [array([0.21355946], dtype=float32), -2.1105278]. 
=============================================
[2019-04-09 14:55:32,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[1.3155159]
 [1.549083 ]
 [1.3944725]
 [1.4226042]
 [1.4386313]], R is [[1.92181015]
 [2.3846519 ]
 [2.74204731]
 [3.37760067]
 [3.92072749]].
[2019-04-09 14:55:32,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03108499 0.12194341 0.11790786 0.07990914 0.05781351 0.04205669
 0.13057268 0.0627119  0.08423775 0.08924491 0.18251722], sum to 1.0000
[2019-04-09 14:55:32,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4618
[2019-04-09 14:55:32,132] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 19.0, 27.38759601737725, 0.9373320344039918, 0.0, 1.0, 50.0, 29.04266757572051], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3599400.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 19.0, 27.37827925440632, 0.9388111621201753, 0.0, 1.0, 45.0, 25.24526189431801], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 0.08333333333333333, 0.7815232712005266, 0.8129370540400584, 0.0, 1.0, 0.6, 0.2524526189431801], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.20224878], dtype=float32), 1.7194451]. 
=============================================
[2019-04-09 14:55:32,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.763236 ]
 [1.7656829]
 [1.7981863]
 [1.7981826]
 [1.7723918]], R is [[2.5555234 ]
 [3.23954153]
 [3.9299829 ]
 [4.65882874]
 [5.33271503]].
[2019-04-09 14:55:32,144] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95601: loss 34.0603
[2019-04-09 14:55:32,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95601: learning rate 0.0000
[2019-04-09 14:55:32,450] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01101795 0.08418567 0.09056514 0.03970841 0.02835536 0.01834622
 0.18931653 0.06448338 0.09384275 0.08246536 0.2977133 ], sum to 1.0000
[2019-04-09 14:55:32,454] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-09 14:55:32,480] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.00125571305666, 0.8603381269019564, 0.0, 1.0, 65.0, 44.00668205264319], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3542400.0000, 
sim time next is 3543000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.00348277037531, 0.8535557065488933, 0.0, 1.0, 20.0, 41.52838263610077], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7502902308646092, 0.7845185688496311, 0.0, 1.0, 0.1, 0.41528382636100775], 
reward next is 0.5847, 
noisyNet noise sample is [array([1.4269391], dtype=float32), -2.1718576]. 
=============================================
[2019-04-09 14:55:32,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[2.5491123]
 [2.4760754]
 [2.6028423]
 [2.583434 ]
 [2.5523314]], R is [[2.88201284]
 [3.41312599]
 [4.0270977 ]
 [4.55969191]
 [5.08250284]].
[2019-04-09 14:55:32,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02560401 0.1337604  0.1282714  0.10202271 0.056017   0.04936985
 0.10726897 0.06745896 0.09423931 0.08814993 0.14783743], sum to 1.0000
[2019-04-09 14:55:32,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1305
[2019-04-09 14:55:32,549] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0239553  0.1217992  0.10594596 0.08278591 0.06194184 0.04815594
 0.12115905 0.06174877 0.08940401 0.09015398 0.19295001], sum to 1.0000
[2019-04-09 14:55:32,552] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.3333333333333334, 42.66666666666666, 82.16666666666667, 668.3333333333333, 19.0, 27.39676372593877, 0.9428390787147002, 0.0, 1.0, 25.0, 23.77102873228956], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3598800.0000, 
sim time next is 3599400.0000, 
raw observation next is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 19.0, 27.40031366903377, 0.9431416292033777, 0.0, 1.0, 20.0, 26.40012100621205], 
processed observation next is [0.0, 0.6521739130434783, 0.4579870729455217, 0.42833333333333345, 0.2611111111111111, 0.7046040515653776, 0.08333333333333333, 0.7833594724194809, 0.8143805430677925, 0.0, 1.0, 0.1, 0.2640012100621205], 
reward next is 0.7360, 
noisyNet noise sample is [array([-1.3422797], dtype=float32), 0.28630826]. 
=============================================
[2019-04-09 14:55:32,553] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 95813: loss 29.3958
[2019-04-09 14:55:32,554] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 95814: learning rate 0.0000
[2019-04-09 14:55:32,555] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3475
[2019-04-09 14:55:32,583] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 19.0, 26.46036228483963, 0.7467070989358539, 0.0, 1.0, 65.0, 50.41026199350178], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 19.0, 26.47969559182725, 0.7561323923592346, 0.0, 1.0, 35.0, 37.06064093511295], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.08333333333333333, 0.7066412993189376, 0.7520441307864115, 0.0, 1.0, 0.4, 0.3706064093511295], 
reward next is 0.6294, 
noisyNet noise sample is [array([0.4275199], dtype=float32), 0.5738308]. 
=============================================
[2019-04-09 14:55:32,635] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95857: loss 28.0675
[2019-04-09 14:55:32,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95858: learning rate 0.0000
[2019-04-09 14:55:32,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02235508 0.13292596 0.11821267 0.07478986 0.05971678 0.04243921
 0.12158202 0.07142098 0.09067296 0.08211905 0.18376544], sum to 1.0000
[2019-04-09 14:55:32,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-09 14:55:32,659] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 104.0, 792.0, 19.0, 27.17364694552506, 0.8920099116563939, 0.0, 1.0, 20.0, 25.2647444740416], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3594000.0000, 
sim time next is 3594600.0000, 
raw observation next is [-1.0, 42.0, 102.0, 788.0, 19.0, 27.1797050978604, 0.9011380923078031, 0.0, 1.0, 65.0, 47.55573952687835], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.34, 0.8707182320441988, 0.08333333333333333, 0.7649754248216999, 0.800379364102601, 0.0, 1.0, 1.0, 0.4755573952687835], 
reward next is 0.5244, 
noisyNet noise sample is [array([0.55506456], dtype=float32), -0.3004212]. 
=============================================
[2019-04-09 14:55:32,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02744858 0.13841088 0.12858029 0.08657072 0.06047861 0.04289175
 0.11467706 0.06034883 0.08589285 0.07497626 0.17972413], sum to 1.0000
[2019-04-09 14:55:32,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0985
[2019-04-09 14:55:32,737] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.166666666666667, 43.33333333333333, 110.0, 804.0, 19.0, 26.96362006856244, 0.8561300368618046, 0.0, 1.0, 55.0, 32.31517430651918], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3592200.0000, 
sim time next is 3592800.0000, 
raw observation next is [-1.0, 42.0, 108.0, 800.0, 19.0, 26.99974039015776, 0.8640470542717029, 0.0, 1.0, 20.0, 29.50890697316419], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.36, 0.8839779005524862, 0.08333333333333333, 0.7499783658464801, 0.7880156847572343, 0.0, 1.0, 0.1, 0.2950890697316419], 
reward next is 0.7049, 
noisyNet noise sample is [array([-0.52457523], dtype=float32), 0.6714625]. 
=============================================
[2019-04-09 14:55:32,928] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96004: loss 34.8169
[2019-04-09 14:55:32,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0260834  0.12504086 0.09281484 0.08180907 0.06827883 0.03998224
 0.1615443  0.06602754 0.08280126 0.08557479 0.1700429 ], sum to 1.0000
[2019-04-09 14:55:32,938] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96006: learning rate 0.0000
[2019-04-09 14:55:32,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-09 14:55:32,951] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.00172209342774, 0.7971584240659461, 0.0, 1.0, 20.0, 37.77085512253949], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3615000.0000, 
sim time next is 3615600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.02777999751667, 0.7985810325392425, 0.0, 1.0, 35.0, 28.50260253522918], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7523149997930559, 0.7661936775130808, 0.0, 1.0, 0.4, 0.2850260253522918], 
reward next is 0.7150, 
noisyNet noise sample is [array([-0.73705673], dtype=float32), -0.18775846]. 
=============================================
[2019-04-09 14:55:32,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02334049 0.10882453 0.12219013 0.06430056 0.06643064 0.04750739
 0.11735271 0.06319641 0.09925234 0.10204757 0.18555728], sum to 1.0000
[2019-04-09 14:55:32,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9697
[2019-04-09 14:55:32,975] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 94.0, 743.5, 19.0, 27.02472767747038, 0.88492561572275, 0.0, 1.0, 60.0, 36.53009395638074], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3596400.0000, 
sim time next is 3597000.0000, 
raw observation next is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 19.0, 27.08503010504565, 0.9005587546078476, 0.0, 1.0, 65.0, 33.44857000548458], 
processed observation next is [0.0, 0.6521739130434783, 0.43951985226223456, 0.4216666666666666, 0.30444444444444446, 0.8051565377532229, 0.08333333333333333, 0.7570858420871375, 0.8001862515359491, 0.0, 1.0, 1.0, 0.3344857000548458], 
reward next is 0.6655, 
noisyNet noise sample is [array([-0.8063773], dtype=float32), 0.49968094]. 
=============================================
[2019-04-09 14:55:32,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[1.8455374]
 [1.8627219]
 [1.8363439]
 [1.8937287]
 [1.7789671]], R is [[2.50519061]
 [3.11483788]
 [3.59860611]
 [4.33129072]
 [5.04183531]].
[2019-04-09 14:55:33,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02173872 0.1358348  0.10566825 0.07736117 0.05731193 0.05654358
 0.11838803 0.06415419 0.07211359 0.08305602 0.20782974], sum to 1.0000
[2019-04-09 14:55:33,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6210
[2019-04-09 14:55:33,041] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 19.0, 27.35550251081542, 0.9344445552406101, 0.0, 1.0, 20.0, 29.45284829008936], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 19.0, 27.37587826914287, 0.9353173330523186, 0.0, 1.0, 50.0, 28.89604476449216], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41666666666666674, 0.22277777777777777, 0.6029465930018416, 0.08333333333333333, 0.7813231890952391, 0.8117724443507729, 0.0, 1.0, 0.7, 0.2889604476449216], 
reward next is 0.7110, 
noisyNet noise sample is [array([0.9925116], dtype=float32), 0.58181137]. 
=============================================
[2019-04-09 14:55:33,155] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96122: loss 25.7794
[2019-04-09 14:55:33,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96126: learning rate 0.0000
[2019-04-09 14:55:33,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02332509 0.12930763 0.10603858 0.08413363 0.0614962  0.04135353
 0.15063165 0.06876623 0.08886844 0.08588868 0.16019034], sum to 1.0000
[2019-04-09 14:55:33,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9079
[2019-04-09 14:55:33,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03088548 0.1277564  0.11652176 0.07738351 0.0637693  0.03764049
 0.13356978 0.06949843 0.07676828 0.11038815 0.15581837], sum to 1.0000
[2019-04-09 14:55:33,346] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.5, 31.0, 60.66666666666668, 309.0, 19.0, 27.11830056403953, 0.7748253833756983, 0.0, 1.0, 20.0, 32.50097862469782], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3658200.0000, 
sim time next is 3658800.0000, 
raw observation next is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 19.0, 27.15675533438912, 0.7873543642318034, 0.0, 1.0, 65.0, 31.92136946398547], 
processed observation next is [0.0, 0.34782608695652173, 0.7119113573407203, 0.3, 0.24944444444444447, 0.3933701657458565, 0.08333333333333333, 0.7630629445324267, 0.7624514547439345, 0.0, 1.0, 1.0, 0.3192136946398547], 
reward next is 0.6808, 
noisyNet noise sample is [array([-1.3272542], dtype=float32), 1.3689194]. 
=============================================
[2019-04-09 14:55:33,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2295
[2019-04-09 14:55:33,363] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.333333333333334, 69.0, 0.0, 0.0, 19.0, 26.56762353787108, 0.6612449156997187, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3558000.0000, 
sim time next is 3558600.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 19.0, 26.42097708586275, 0.681087552283481, 0.0, 1.0, 25.0, 54.12502218358721], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7017480904885623, 0.7270291840944937, 0.0, 1.0, 0.2, 0.5412502218358721], 
reward next is 0.4587, 
noisyNet noise sample is [array([-0.47241688], dtype=float32), 0.57560277]. 
=============================================
[2019-04-09 14:55:33,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96242: loss 35.5114
[2019-04-09 14:55:33,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96242: learning rate 0.0000
[2019-04-09 14:55:33,382] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96242: loss 30.8428
[2019-04-09 14:55:33,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96242: learning rate 0.0000
[2019-04-09 14:55:33,520] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02634019 0.11338055 0.11231156 0.0758772  0.07867434 0.03987156
 0.16697767 0.0661088  0.08268003 0.08458953 0.15318865], sum to 1.0000
[2019-04-09 14:55:33,521] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0477
[2019-04-09 14:55:33,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02579663 0.12118168 0.12154812 0.07334343 0.05730366 0.04842803
 0.12671556 0.0621978  0.09706109 0.07332452 0.19309945], sum to 1.0000
[2019-04-09 14:55:33,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0656
[2019-04-09 14:55:33,548] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 27.08276670129208, 0.7356913360111202, 0.0, 1.0, 60.0, 35.88693620744476], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3636000.0000, 
sim time next is 3636600.0000, 
raw observation next is [8.866666666666667, 25.33333333333334, 0.0, 0.0, 19.0, 27.09517934627806, 0.7358661584211063, 0.0, 1.0, 20.0, 33.09309519555619], 
processed observation next is [0.0, 0.08695652173913043, 0.7082179132040629, 0.2533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7579316121898382, 0.745288719473702, 0.0, 1.0, 0.1, 0.3309309519555619], 
reward next is 0.6691, 
noisyNet noise sample is [array([-2.197362], dtype=float32), 0.16781902]. 
=============================================
[2019-04-09 14:55:33,553] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 39.0, 38.5, 328.5, 19.0, 27.39025802056464, 0.9210461317604643, 0.0, 1.0, 35.0, 29.99020330176663], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3603600.0000, 
sim time next is 3604200.0000, 
raw observation next is [-0.1666666666666667, 39.5, 30.33333333333333, 266.3333333333333, 19.0, 27.37658265479038, 0.9126598344278741, 0.0, 1.0, 20.0, 30.45448523467142], 
processed observation next is [0.0, 0.7391304347826086, 0.4579870729455217, 0.395, 0.1011111111111111, 0.29429097605893184, 0.08333333333333333, 0.7813818878991983, 0.8042199448092914, 0.0, 1.0, 0.1, 0.3045448523467142], 
reward next is 0.6955, 
noisyNet noise sample is [array([-0.15437353], dtype=float32), -0.45477682]. 
=============================================
[2019-04-09 14:55:33,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02568212 0.11267413 0.1151292  0.08065496 0.07832435 0.04009091
 0.1420125  0.06123046 0.09434813 0.07845397 0.17139928], sum to 1.0000
[2019-04-09 14:55:33,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0043
[2019-04-09 14:55:33,623] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 19.0, 26.95762522059065, 0.7525906271590356, 0.0, 1.0, 65.0, 38.34093484538614], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 19.0, 26.94936152764682, 0.679521897369717, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7457801273039015, 0.726507299123239, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22151466], dtype=float32), 0.4213208]. 
=============================================
[2019-04-09 14:55:33,645] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 96387: loss 32.3080
[2019-04-09 14:55:33,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 96388: learning rate 0.0000
[2019-04-09 14:55:34,071] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02736424 0.12472872 0.09908006 0.08950721 0.08009669 0.04794075
 0.15774204 0.0648309  0.09044565 0.07204368 0.1462201 ], sum to 1.0000
[2019-04-09 14:55:34,071] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8186
[2019-04-09 14:55:34,086] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.666666666666668, 28.66666666666667, 0.0, 0.0, 19.0, 27.15267280340247, 0.7417537220068112, 0.0, 1.0, 30.0, 30.44128797553383], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3655200.0000, 
sim time next is 3655800.0000, 
raw observation next is [8.5, 29.5, 4.0, 121.0, 19.0, 27.12277548401507, 0.7486371041369954, 0.0, 1.0, 60.0, 34.97358142929748], 
processed observation next is [0.0, 0.30434782608695654, 0.698060941828255, 0.295, 0.013333333333333334, 0.13370165745856355, 0.08333333333333333, 0.760231290334589, 0.7495457013789985, 0.0, 1.0, 0.9, 0.34973581429297484], 
reward next is 0.6503, 
noisyNet noise sample is [array([0.15096532], dtype=float32), -0.11815284]. 
=============================================
[2019-04-09 14:55:34,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02435748 0.13151975 0.09202185 0.07936908 0.09075969 0.04544182
 0.14344649 0.07675771 0.09265877 0.08211152 0.14155582], sum to 1.0000
[2019-04-09 14:55:34,161] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3306
[2019-04-09 14:55:34,176] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 19.0, 27.15259563797501, 0.7460479948957143, 0.0, 1.0, 55.0, 34.32423814516878], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3647400.0000, 
sim time next is 3648000.0000, 
raw observation next is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 19.0, 27.13477075840241, 0.7449154950091286, 0.0, 1.0, 35.0, 31.85133437833846], 
processed observation next is [0.0, 0.21739130434782608, 0.7211449676823639, 0.2633333333333334, 0.0, 0.0, 0.08333333333333333, 0.7612308965335343, 0.7483051650030429, 0.0, 1.0, 0.4, 0.3185133437833846], 
reward next is 0.6815, 
noisyNet noise sample is [array([0.76078653], dtype=float32), 0.74808365]. 
=============================================
[2019-04-09 14:55:34,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.3717507]
 [1.4561111]
 [1.2175122]
 [1.5108563]
 [1.4233042]], R is [[2.17090631]
 [2.80595493]
 [3.4993279 ]
 [4.15581274]
 [4.77966547]].
[2019-04-09 14:55:34,549] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96876: loss 29.9843
[2019-04-09 14:55:34,552] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96877: learning rate 0.0000
[2019-04-09 14:55:34,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0238583  0.13475966 0.10046978 0.08311991 0.06954543 0.03686251
 0.12981501 0.07091958 0.0870259  0.08522376 0.17840016], sum to 1.0000
[2019-04-09 14:55:34,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3044
[2019-04-09 14:55:34,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.333333333333332, 26.66666666666666, 0.0, 0.0, 19.0, 27.08906839167504, 0.7352272933238341, 0.0, 1.0, 45.0, 26.43121365389056], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3639000.0000, 
sim time next is 3639600.0000, 
raw observation next is [8.2, 27.0, 0.0, 0.0, 19.0, 27.07720238552994, 0.7305306082227578, 0.0, 1.0, 30.0, 28.5676452813704], 
processed observation next is [0.0, 0.13043478260869565, 0.6897506925207757, 0.27, 0.0, 0.0, 0.08333333333333333, 0.756433532127495, 0.7435102027409193, 0.0, 1.0, 0.3, 0.285676452813704], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.8636364], dtype=float32), -0.34926462]. 
=============================================
[2019-04-09 14:55:35,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02027497 0.11149957 0.10620432 0.080096   0.06024431 0.02861718
 0.17415942 0.06359456 0.0807687  0.0985157  0.17602521], sum to 1.0000
[2019-04-09 14:55:35,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0968
[2019-04-09 14:55:35,028] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 48.33333333333334, 0.0, 0.0, 19.0, 26.84543438678266, 0.7309649500198402, 0.0, 1.0, 55.0, 34.05815148262596], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3626400.0000, 
sim time next is 3627000.0000, 
raw observation next is [3.0, 42.5, 0.0, 0.0, 19.0, 26.87010629373033, 0.7356610206946285, 0.0, 1.0, 20.0, 34.05351265991892], 
processed observation next is [0.0, 1.0, 0.5457063711911359, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7391755244775275, 0.7452203402315428, 0.0, 1.0, 0.1, 0.3405351265991892], 
reward next is 0.6595, 
noisyNet noise sample is [array([0.41099432], dtype=float32), -0.54701644]. 
=============================================
[2019-04-09 14:55:35,034] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.9488492]
 [1.8795717]
 [2.0083141]
 [1.8463156]
 [1.9990656]], R is [[2.54923964]
 [3.18316555]
 [3.78974032]
 [4.34782314]
 [4.89899063]].
[2019-04-09 14:55:35,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0273264  0.12011313 0.10402621 0.09811851 0.0789827  0.04158175
 0.14103147 0.06801486 0.08638386 0.08323142 0.15118961], sum to 1.0000
[2019-04-09 14:55:35,284] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8058
[2019-04-09 14:55:35,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0237426  0.13534202 0.10204586 0.08268714 0.07530719 0.03752554
 0.1274212  0.07269651 0.09716148 0.09173352 0.15433687], sum to 1.0000
[2019-04-09 14:55:35,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3499
[2019-04-09 14:55:35,298] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.033333333333333, 28.66666666666667, 0.0, 0.0, 19.0, 27.10188584302919, 0.7269228036956852, 0.0, 1.0, 30.0, 28.71045135271036], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3642600.0000, 
sim time next is 3643200.0000, 
raw observation next is [8.0, 29.0, 0.0, 0.0, 19.0, 27.05759624905976, 0.7258632402897125, 0.0, 1.0, 60.0, 37.59061397570199], 
processed observation next is [0.0, 0.17391304347826086, 0.6842105263157896, 0.29, 0.0, 0.0, 0.08333333333333333, 0.7547996874216466, 0.7419544134299042, 0.0, 1.0, 0.9, 0.3759061397570199], 
reward next is 0.6241, 
noisyNet noise sample is [array([-0.68609065], dtype=float32), -0.3145864]. 
=============================================
[2019-04-09 14:55:35,336] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.166666666666666, 31.16666666666667, 32.33333333333333, 215.0, 19.0, 27.10907067433362, 0.754289948053852, 0.0, 1.0, 60.0, 33.92913720277824], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3657000.0000, 
sim time next is 3657600.0000, 
raw observation next is [8.0, 32.0, 46.5, 262.0, 19.0, 27.12144143995472, 0.7661656760748649, 0.0, 1.0, 35.0, 32.02601215447135], 
processed observation next is [0.0, 0.34782608695652173, 0.6842105263157896, 0.32, 0.155, 0.28950276243093925, 0.08333333333333333, 0.7601201199962265, 0.7553885586916217, 0.0, 1.0, 0.4, 0.3202601215447135], 
reward next is 0.6797, 
noisyNet noise sample is [array([-1.1017232], dtype=float32), 0.1173303]. 
=============================================
[2019-04-09 14:55:35,347] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01814272 0.11538317 0.12745954 0.07558937 0.05232497 0.0479922
 0.12477765 0.06514619 0.09682874 0.08748844 0.18886702], sum to 1.0000
[2019-04-09 14:55:35,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7039
[2019-04-09 14:55:35,358] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 42.0, 104.0, 792.0, 19.0, 27.20768581392289, 0.9130099287587635, 0.0, 1.0, 55.0, 24.76608834525399], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3594000.0000, 
sim time next is 3594600.0000, 
raw observation next is [-1.0, 42.0, 102.0, 788.0, 19.0, 27.22242681264655, 0.9206876589232317, 0.0, 1.0, 55.0, 27.05086079238371], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.34, 0.8707182320441988, 0.08333333333333333, 0.7685355677205458, 0.8068958863077439, 0.0, 1.0, 0.8, 0.2705086079238371], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.38960204], dtype=float32), 1.4190395]. 
=============================================
[2019-04-09 14:55:35,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02169045 0.14207482 0.11692415 0.08432894 0.05927339 0.03560474
 0.14121908 0.05976478 0.08955432 0.06761326 0.18195218], sum to 1.0000
[2019-04-09 14:55:35,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6158
[2019-04-09 14:55:35,391] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.0, 34.5, 116.0, 816.0, 19.0, 27.82015424602111, 0.9849426611922768, 0.0, 1.0, 25.0, 17.03004290503754], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3670200.0000, 
sim time next is 3670800.0000, 
raw observation next is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 19.0, 27.80164855182939, 0.9841935708753521, 0.0, 1.0, 50.0, 18.87794385457734], 
processed observation next is [0.0, 0.4782608695652174, 0.6472760849492153, 0.38, 0.38722222222222236, 0.9040515653775322, 0.08333333333333333, 0.8168040459857826, 0.8280645236251174, 0.0, 1.0, 0.7, 0.1887794385457734], 
reward next is 0.8112, 
noisyNet noise sample is [array([0.31606665], dtype=float32), -0.29081678]. 
=============================================
[2019-04-09 14:55:35,569] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02516488 0.14886497 0.09819682 0.08859062 0.0663851  0.03760401
 0.14792061 0.06481876 0.08037994 0.09714476 0.14492948], sum to 1.0000
[2019-04-09 14:55:35,570] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-09 14:55:35,576] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 97422: loss 26.2610
[2019-04-09 14:55:35,578] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 97423: learning rate 0.0000
[2019-04-09 14:55:35,582] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.733333333333334, 25.66666666666667, 0.0, 0.0, 19.0, 27.15253935416165, 0.7492680833963307, 0.0, 1.0, 30.0, 29.16245888156515], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3637200.0000, 
sim time next is 3637800.0000, 
raw observation next is [8.6, 26.0, 0.0, 0.0, 19.0, 27.13121919040875, 0.7436677096953427, 0.0, 1.0, 45.0, 28.12174705931655], 
processed observation next is [0.0, 0.08695652173913043, 0.700831024930748, 0.26, 0.0, 0.0, 0.08333333333333333, 0.7609349325340625, 0.7478892365651143, 0.0, 1.0, 0.6, 0.2812174705931655], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.21731208], dtype=float32), 0.9662068]. 
=============================================
[2019-04-09 14:55:35,684] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 97480: loss 29.2860
[2019-04-09 14:55:35,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 97480: learning rate 0.0000
[2019-04-09 14:55:35,872] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02038216 0.12387378 0.11894017 0.08580949 0.06352594 0.03640535
 0.13620815 0.06018909 0.09090569 0.08043803 0.18332215], sum to 1.0000
[2019-04-09 14:55:35,874] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-09 14:55:35,882] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 19.0, 28.01909416495479, 1.070379802170729, 0.0, 1.0, 60.0, 22.24638718559808], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3682200.0000, 
sim time next is 3682800.0000, 
raw observation next is [6.0, 47.0, 95.5, 740.5, 19.0, 28.00593623785841, 1.074210865573284, 0.0, 1.0, 55.0, 21.3653834526367], 
processed observation next is [0.0, 0.6521739130434783, 0.6288088642659281, 0.47, 0.31833333333333336, 0.8182320441988951, 0.08333333333333333, 0.8338280198215342, 0.858070288524428, 0.0, 1.0, 0.8, 0.213653834526367], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.87494105], dtype=float32), -0.19441159]. 
=============================================
[2019-04-09 14:55:36,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02469494 0.13883227 0.09335448 0.09603936 0.07074281 0.04597893
 0.13480967 0.06109096 0.09042714 0.08610517 0.15792423], sum to 1.0000
[2019-04-09 14:55:36,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9065
[2019-04-09 14:55:36,036] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 19.0, 27.5488957308665, 0.9109202657303865, 0.0, 1.0, 20.0, 22.22180269393015], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3665400.0000, 
sim time next is 3666000.0000, 
raw observation next is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 27.59309474632559, 0.9230850745266838, 0.0, 1.0, 65.0, 22.47689447323784], 
processed observation next is [0.0, 0.43478260869565216, 0.7765466297322253, 0.2666666666666667, 0.36444444444444435, 0.8246777163904236, 0.08333333333333333, 0.7994245621937992, 0.807695024842228, 0.0, 1.0, 1.0, 0.2247689447323784], 
reward next is 0.7752, 
noisyNet noise sample is [array([-0.24886678], dtype=float32), -1.6869539]. 
=============================================
[2019-04-09 14:55:36,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[1.5755028]
 [1.5877583]
 [1.6538283]
 [1.5593038]
 [1.6177486]], R is [[2.44784975]
 [3.20115328]
 [3.92937016]
 [4.65301752]
 [5.37078142]].
[2019-04-09 14:55:36,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0201536  0.13218714 0.12569621 0.09635906 0.05743695 0.03086562
 0.11471849 0.06537537 0.06944291 0.09579595 0.19196872], sum to 1.0000
[2019-04-09 14:55:36,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-09 14:55:36,075] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 50.0, 75.5, 613.5, 19.0, 28.07656977079555, 1.094275223240508, 0.0, 1.0, 20.0, 18.78208348911936], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3686400.0000, 
sim time next is 3687000.0000, 
raw observation next is [4.833333333333334, 51.5, 71.66666666666666, 583.0, 19.0, 28.08530899986841, 1.097056175632477, 0.0, 1.0, 20.0, 18.71819751758155], 
processed observation next is [0.0, 0.6956521739130435, 0.5964912280701755, 0.515, 0.23888888888888885, 0.6441988950276243, 0.08333333333333333, 0.8404424166557009, 0.8656853918774923, 0.0, 1.0, 0.1, 0.1871819751758155], 
reward next is 0.8128, 
noisyNet noise sample is [array([0.4589807], dtype=float32), 1.5729871]. 
=============================================
[2019-04-09 14:55:36,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[2.0349934]
 [1.8707838]
 [2.0172162]
 [1.8131738]
 [2.085126 ]], R is [[2.73580074]
 [3.52062178]
 [4.29706717]
 [5.06167316]
 [5.82324266]].
[2019-04-09 14:55:36,119] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02783928 0.11393826 0.09703898 0.09935984 0.07470311 0.04366015
 0.13732202 0.06602383 0.10639247 0.07734201 0.15638007], sum to 1.0000
[2019-04-09 14:55:36,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-09 14:55:36,138] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.5, 26.0, 0.0, 0.0, 19.0, 27.09769564385141, 0.7393048852405184, 0.0, 1.0, 65.0, 50.04776166691057], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3648600.0000, 
sim time next is 3649200.0000, 
raw observation next is [9.666666666666666, 25.66666666666666, 0.0, 0.0, 19.0, 27.09485799730312, 0.7398377989597732, 0.0, 1.0, 65.0, 35.09438967260888], 
processed observation next is [0.0, 0.21739130434782608, 0.7303785780240075, 0.2566666666666666, 0.0, 0.0, 0.08333333333333333, 0.7579048331085932, 0.7466125996532577, 0.0, 1.0, 1.0, 0.3509438967260888], 
reward next is 0.6491, 
noisyNet noise sample is [array([-0.1594393], dtype=float32), 0.65346956]. 
=============================================
[2019-04-09 14:55:36,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02977555 0.12599465 0.10167921 0.09216203 0.07754447 0.04116945
 0.14267923 0.06881106 0.08901514 0.08884506 0.14232409], sum to 1.0000
[2019-04-09 14:55:36,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7609
[2019-04-09 14:55:36,354] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 19.0, 27.1663508271604, 0.7447729450394075, 0.0, 1.0, 45.0, 29.47186567554541], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3652800.0000, 
sim time next is 3653400.0000, 
raw observation next is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 19.0, 27.16441649871408, 0.7492461379365701, 0.0, 1.0, 55.0, 34.2037942453768], 
processed observation next is [0.0, 0.2608695652173913, 0.7165281625115422, 0.2666666666666666, 0.0, 0.0, 0.08333333333333333, 0.76370137489284, 0.7497487126455233, 0.0, 1.0, 0.8, 0.342037942453768], 
reward next is 0.6580, 
noisyNet noise sample is [array([-0.09029683], dtype=float32), -0.24365234]. 
=============================================
[2019-04-09 14:55:36,556] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01936576 0.11856034 0.08578827 0.07373165 0.06016745 0.03302338
 0.17028075 0.06733566 0.09410919 0.09043465 0.18720293], sum to 1.0000
[2019-04-09 14:55:36,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9895
[2019-04-09 14:55:36,569] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 19.0, 27.50797855929541, 0.9177404675698312, 0.0, 1.0, 65.0, 49.28700269437909], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3703200.0000, 
sim time next is 3703800.0000, 
raw observation next is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 19.0, 27.50700397905489, 0.9146431320718545, 0.0, 1.0, 65.0, 52.12625383201185], 
processed observation next is [0.0, 0.8695652173913043, 0.5226223453370269, 0.6216666666666667, 0.0, 0.0, 0.08333333333333333, 0.7922503315879075, 0.8048810440239516, 0.0, 1.0, 1.0, 0.5212625383201185], 
reward next is 0.4787, 
noisyNet noise sample is [array([-1.0553311], dtype=float32), 0.76776075]. 
=============================================
[2019-04-09 14:55:36,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02405744 0.12963988 0.12622666 0.08936937 0.06119651 0.03997783
 0.12157079 0.06437073 0.07937606 0.09448003 0.16973467], sum to 1.0000
[2019-04-09 14:55:36,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3055
[2019-04-09 14:55:36,628] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 48.5, 87.0, 705.0, 19.0, 28.06425043372364, 1.084532948711705, 0.0, 1.0, 45.0, 15.55988087202169], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3684600.0000, 
sim time next is 3685200.0000, 
raw observation next is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 19.0, 28.06372533348888, 1.086562202357548, 0.0, 1.0, 30.0, 19.63411559893577], 
processed observation next is [0.0, 0.6521739130434783, 0.6103416435826409, 0.49, 0.2772222222222222, 0.7453038674033149, 0.08333333333333333, 0.83864377779074, 0.8621874007858494, 0.0, 1.0, 0.3, 0.19634115598935772], 
reward next is 0.8037, 
noisyNet noise sample is [array([-0.61215585], dtype=float32), -0.32017612]. 
=============================================
[2019-04-09 14:55:36,994] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02934925 0.14091091 0.1207587  0.07590472 0.0679372  0.04519209
 0.13622473 0.06611272 0.08486046 0.09184088 0.14090836], sum to 1.0000
[2019-04-09 14:55:36,997] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6589
[2019-04-09 14:55:37,008] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 19.0, 27.56267869652554, 0.9169901167386478, 0.0, 1.0, 20.0, 21.25774355835677], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3665400.0000, 
sim time next is 3666000.0000, 
raw observation next is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 27.58660519659946, 0.9278697939116566, 0.0, 1.0, 50.0, 22.06920996691055], 
processed observation next is [0.0, 0.43478260869565216, 0.7765466297322253, 0.2666666666666667, 0.36444444444444435, 0.8246777163904236, 0.08333333333333333, 0.7988837663832884, 0.8092899313038856, 0.0, 1.0, 0.7, 0.2206920996691055], 
reward next is 0.7793, 
noisyNet noise sample is [array([-0.4609464], dtype=float32), 0.27219748]. 
=============================================
[2019-04-09 14:55:37,015] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.019255   0.13111319 0.12300689 0.0848352  0.05891174 0.04286662
 0.11467402 0.06030051 0.08987576 0.08665111 0.18850993], sum to 1.0000
[2019-04-09 14:55:37,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-09 14:55:37,032] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 19.0, 27.68221964479365, 0.9942611713246058, 0.0, 1.0, 65.0, 40.2562461590757], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3675000.0000, 
sim time next is 3675600.0000, 
raw observation next is [5.0, 42.0, 115.0, 823.5, 19.0, 27.69367675062915, 0.9993812609933856, 0.0, 1.0, 35.0, 26.05878257228079], 
processed observation next is [0.0, 0.5652173913043478, 0.6011080332409973, 0.42, 0.38333333333333336, 0.9099447513812154, 0.08333333333333333, 0.8078063958857626, 0.8331270869977953, 0.0, 1.0, 0.4, 0.2605878257228079], 
reward next is 0.7394, 
noisyNet noise sample is [array([1.7016256], dtype=float32), -0.12771568]. 
=============================================
[2019-04-09 14:55:37,035] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.6385067]
 [1.7947226]
 [1.8143815]
 [1.9005592]
 [1.7555177]], R is [[2.65925765]
 [3.42008781]
 [4.11277723]
 [4.8355751 ]
 [5.55072498]].
[2019-04-09 14:55:37,047] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02332217 0.13108046 0.12716994 0.12386108 0.0534783  0.04182221
 0.11559728 0.06055893 0.08978409 0.06907909 0.16424635], sum to 1.0000
[2019-04-09 14:55:37,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4348
[2019-04-09 14:55:37,059] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666666, 43.0, 116.3333333333333, 827.1666666666667, 19.0, 27.67484535420036, 0.9924978736755458, 0.0, 1.0, 65.0, 24.02981578566764], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 19.0, 27.6809779621814, 0.9975804157516674, 0.0, 1.0, 35.0, 23.1347604677356], 
processed observation next is [0.0, 0.5217391304347826, 0.5964912280701755, 0.425, 0.38555555555555565, 0.9119705340699816, 0.08333333333333333, 0.8067481635151168, 0.8325268052505558, 0.0, 1.0, 0.4, 0.231347604677356], 
reward next is 0.7687, 
noisyNet noise sample is [array([1.1907371], dtype=float32), 1.449439]. 
=============================================
[2019-04-09 14:55:37,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[2.1333354]
 [2.0749693]
 [2.1381943]
 [1.8435525]
 [2.0379663]], R is [[2.79729462]
 [3.52902365]
 [4.25487709]
 [4.81469631]
 [5.55787945]].
[2019-04-09 14:55:37,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01905549 0.12289679 0.11930121 0.05271834 0.03575236 0.02155044
 0.17671381 0.06265958 0.09554923 0.06588905 0.22791375], sum to 1.0000
[2019-04-09 14:55:37,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2082
[2019-04-09 14:55:37,867] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.01851216631529, 0.7343816693947618, 0.0, 1.0, 55.0, 35.92919933338151], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3730800.0000, 
sim time next is 3731400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.04864597951433, 0.7241139607285577, 0.0, 1.0, 25.0, 35.18429041345944], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7540538316261941, 0.7413713202428526, 0.0, 1.0, 0.2, 0.35184290413459446], 
reward next is 0.6482, 
noisyNet noise sample is [array([0.4492617], dtype=float32), -0.0010216251]. 
=============================================
[2019-04-09 14:55:37,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01927312 0.12106644 0.13703382 0.08532881 0.05413725 0.03765414
 0.12793946 0.04761276 0.08384604 0.08012434 0.20598379], sum to 1.0000
[2019-04-09 14:55:37,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6890
[2019-04-09 14:55:37,907] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.0, 59.0, 12.5, 137.5, 19.0, 27.98535614763466, 1.03506459561166, 0.0, 1.0, 60.0, 25.73528506901074], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3692400.0000, 
sim time next is 3693000.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 19.0, 27.95830718937338, 1.01793338231321, 0.0, 1.0, 55.0, 23.55180592081978], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.0, 0.0, 0.08333333333333333, 0.8298589324477815, 0.8393111274377366, 0.0, 1.0, 0.8, 0.23551805920819782], 
reward next is 0.7645, 
noisyNet noise sample is [array([0.1620588], dtype=float32), -0.7163765]. 
=============================================
[2019-04-09 14:55:37,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[2.0343227]
 [1.9064819]
 [1.9864835]
 [2.068901 ]
 [1.9925905]], R is [[2.75832915]
 [3.47339296]
 [4.22135592]
 [4.96779919]
 [5.68176413]].
[2019-04-09 14:55:38,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01768799 0.11152387 0.11083838 0.05657196 0.0336643  0.02456548
 0.17071295 0.07349797 0.09242618 0.07573574 0.2327752 ], sum to 1.0000
[2019-04-09 14:55:38,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6857
[2019-04-09 14:55:38,133] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.75260493886849, 0.6618127249158476, 0.0, 1.0, 45.0, 31.4982889652566], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3735600.0000, 
sim time next is 3736200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.67174352423209, 0.6596378340763357, 0.0, 1.0, 55.0, 41.97132222578439], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7226452936860076, 0.7198792780254452, 0.0, 1.0, 0.8, 0.4197132222578439], 
reward next is 0.5803, 
noisyNet noise sample is [array([0.5705089], dtype=float32), -2.320138]. 
=============================================
[2019-04-09 14:55:38,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01926082 0.11738227 0.10534933 0.09240872 0.05243705 0.03378879
 0.14493836 0.06224753 0.10314674 0.07918588 0.18985458], sum to 1.0000
[2019-04-09 14:55:38,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-09 14:55:38,224] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.166666666666667, 57.5, 47.66666666666667, 403.0000000000001, 19.0, 28.02491862852149, 1.075556631219646, 0.0, 1.0, 65.0, 22.79073326427312], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3689400.0000, 
sim time next is 3690000.0000, 
raw observation next is [4.0, 59.0, 39.5, 343.5, 19.0, 28.06389872485792, 1.069682857328229, 0.0, 1.0, 60.0, 22.12110995008868], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.13166666666666665, 0.37955801104972375, 0.08333333333333333, 0.8386582270714934, 0.856560952442743, 0.0, 1.0, 0.9, 0.2212110995008868], 
reward next is 0.7788, 
noisyNet noise sample is [array([0.8710711], dtype=float32), -1.336885]. 
=============================================
[2019-04-09 14:55:38,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.137841 ]
 [2.006799 ]
 [2.176066 ]
 [1.9052303]
 [2.0219247]], R is [[2.71754575]
 [3.4624629 ]
 [4.18989706]
 [4.80367661]
 [5.75564003]].
[2019-04-09 14:55:38,549] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02354959 0.16383849 0.11862582 0.0808738  0.06698076 0.03474607
 0.12868157 0.06264258 0.08975812 0.07599214 0.15431096], sum to 1.0000
[2019-04-09 14:55:38,553] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2466
[2019-04-09 14:55:38,571] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [11.0, 26.33333333333334, 97.0, 576.3333333333334, 19.0, 27.435198475073, 0.8638778191732622, 0.0, 1.0, 25.0, 22.91580531999763], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 19.0, 27.48619158200459, 0.8750547164856481, 0.0, 1.0, 25.0, 24.07683449031309], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2666666666666667, 0.33, 0.6847145488029466, 0.08333333333333333, 0.7905159651670491, 0.7916849054952161, 0.0, 1.0, 0.2, 0.2407683449031309], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.75543916], dtype=float32), -0.8570105]. 
=============================================
[2019-04-09 14:55:39,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02417743 0.12058332 0.1181237  0.0846578  0.06186198 0.04323711
 0.12672663 0.06921851 0.09743043 0.07902919 0.17495391], sum to 1.0000
[2019-04-09 14:55:39,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3118
[2019-04-09 14:55:39,388] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.833333333333333, 42.83333333333334, 110.0, 804.0, 19.0, 27.80723201333026, 1.04037774247769, 0.0, 1.0, 20.0, 19.9930030606182], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3678600.0000, 
sim time next is 3679200.0000, 
raw observation next is [6.0, 43.0, 108.5, 797.0, 19.0, 27.83380717301842, 1.052906633862766, 0.0, 1.0, 30.0, 19.73656389654125], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.43, 0.3616666666666667, 0.8806629834254144, 0.08333333333333333, 0.8194839310848684, 0.8509688779542554, 0.0, 1.0, 0.3, 0.19736563896541248], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.24438398], dtype=float32), 0.7524762]. 
=============================================
[2019-04-09 14:55:39,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0100097  0.14103314 0.13036355 0.05722384 0.03513467 0.02374097
 0.14566141 0.06387699 0.10541277 0.08108652 0.2064565 ], sum to 1.0000
[2019-04-09 14:55:39,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8325
[2019-04-09 14:55:39,455] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 22.5, 27.36812184532435, 0.8345254495139528, 1.0, 1.0, 35.0, 26.79624364979975], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3748200.0000, 
sim time next is 3748800.0000, 
raw observation next is [-3.666666666666667, 77.0, 98.16666666666667, 634.0, 22.5, 27.60475610643803, 0.8576513605068566, 1.0, 1.0, 65.0, 29.64307811955338], 
processed observation next is [1.0, 0.391304347826087, 0.3610341643582641, 0.77, 0.32722222222222225, 0.7005524861878453, 0.375, 0.800396342203169, 0.7858837868356189, 1.0, 1.0, 1.0, 0.2964307811955338], 
reward next is 0.7036, 
noisyNet noise sample is [array([0.49244902], dtype=float32), 0.17229295]. 
=============================================
[2019-04-09 14:55:39,866] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01501524 0.10028871 0.09736437 0.0550964  0.03981783 0.02154316
 0.18064895 0.08431471 0.09891269 0.08928394 0.21771409], sum to 1.0000
[2019-04-09 14:55:39,873] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-09 14:55:39,890] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.49536687414069, 0.6443489881080608, 0.0, 1.0, 65.0, 60.67835592526941], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.43463041349841, 0.6484131384562462, 0.0, 1.0, 20.0, 46.30180606031152], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7028858677915343, 0.7161377128187487, 0.0, 1.0, 0.1, 0.4630180606031152], 
reward next is 0.5370, 
noisyNet noise sample is [array([-0.84576595], dtype=float32), 1.2976286]. 
=============================================
[2019-04-09 14:55:40,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01447454 0.09794877 0.10707832 0.05432875 0.03114744 0.0236553
 0.17031926 0.06469002 0.11083367 0.07317348 0.25235048], sum to 1.0000
[2019-04-09 14:55:40,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6255
[2019-04-09 14:55:40,078] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.73201680294201, 0.6664899671009895, 0.0, 1.0, 65.0, 47.96651959279221], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3740400.0000, 
sim time next is 3741000.0000, 
raw observation next is [-4.0, 76.0, 0.0, 0.0, 22.5, 26.75363098820021, 0.6721798431026405, 0.0, 1.0, 55.0, 42.2158936924122], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.76, 0.0, 0.0, 0.375, 0.7294692490166842, 0.7240599477008801, 0.0, 1.0, 0.8, 0.42215893692412204], 
reward next is 0.5778, 
noisyNet noise sample is [array([0.21876], dtype=float32), -0.12585996]. 
=============================================
[2019-04-09 14:55:40,086] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.3995311]
 [2.5653985]
 [2.387008 ]
 [2.4827127]
 [2.306863 ]], R is [[2.97035909]
 [3.46099019]
 [3.96684742]
 [4.49766636]
 [5.00219822]].
[2019-04-09 14:55:40,305] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-09 14:55:40,306] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:55:40,308] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:55:40,308] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:55:40,309] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:55:40,310] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:55:40,310] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:55:40,315] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run11
[2019-04-09 14:55:40,316] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run11
[2019-04-09 14:55:40,358] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run11
[2019-04-09 14:56:22,641] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.0139338], dtype=float32), 0.01890407]
[2019-04-09 14:56:22,642] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [2.8, 75.83333333333333, 20.0, 83.0, 22.5, 26.31685044203374, 0.7064099118613196, 0.0, 1.0, 25.0, 22.42709517601272]
[2019-04-09 14:56:22,642] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:56:22,642] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.01972933 0.1313739  0.10173946 0.08159257 0.05505306 0.02776359
 0.15005602 0.0652021  0.09533338 0.09531764 0.17683898], sampled 0.8200350465107474
[2019-04-09 14:57:18,879] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5672.1853 282343.4397 2808.9798
[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:18,899] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:19,007] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,018] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5439.3674 305695.5775 2361.5971
[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,542] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5351.4149 314463.5973 1967.6488
[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,562] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:25,665] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:57:26,564] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 100000, evaluation results [100000.0, 5439.367384796008, 305695.57750714634, 2361.5970561620916, 5672.185270843681, 282343.43972258136, 2808.9797726822553, 5351.414892709819, 314463.59728198906, 1967.6487972770778]
[2019-04-09 14:57:26,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00725425 0.12051902 0.14255269 0.07512973 0.03766674 0.01184284
 0.148991   0.04959736 0.13704212 0.0560688  0.2133355 ], sum to 1.0000
[2019-04-09 14:57:26,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-09 14:57:26,781] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.666666666666667, 69.0, 114.3333333333333, 813.1666666666666, 22.5, 28.13472927032653, 1.00848423080334, 1.0, 1.0, 35.0, 20.33150411513445], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3756000.0000, 
sim time next is 3756600.0000, 
raw observation next is [-2.5, 68.0, 115.0, 822.0, 22.5, 28.14698317336201, 1.02108442493398, 1.0, 1.0, 55.0, 20.81668137248437], 
processed observation next is [1.0, 0.4782608695652174, 0.39335180055401664, 0.68, 0.38333333333333336, 0.9082872928176795, 0.375, 0.8455819311135008, 0.8403614749779934, 1.0, 1.0, 0.8, 0.20816681372484372], 
reward next is 0.7918, 
noisyNet noise sample is [array([-1.0230175], dtype=float32), -1.8480052]. 
=============================================
[2019-04-09 14:57:26,882] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01556839 0.08135799 0.12849593 0.04693586 0.03350623 0.01953371
 0.18253466 0.06109901 0.06593757 0.06866498 0.2963657 ], sum to 1.0000
[2019-04-09 14:57:26,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2007
[2019-04-09 14:57:26,919] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.90352491689604, 0.7912741698775335, 0.0, 1.0, 55.0, 37.03419508007548], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.91791760283947, 0.7949240695463752, 0.0, 1.0, 25.0, 39.39826262083138], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7431598002366225, 0.7649746898487918, 0.0, 1.0, 0.2, 0.3939826262083138], 
reward next is 0.6060, 
noisyNet noise sample is [array([0.37612593], dtype=float32), -1.0086325]. 
=============================================
[2019-04-09 14:57:27,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00657386 0.13699004 0.12639228 0.06326478 0.03089328 0.0194771
 0.13051105 0.06425638 0.10363217 0.08770331 0.23030576], sum to 1.0000
[2019-04-09 14:57:27,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3658
[2019-04-09 14:57:27,260] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.00000000000001, 90.16666666666666, 721.8333333333333, 22.5, 28.66783348500144, 1.170576492204865, 1.0, 1.0, 45.0, 11.9144794714988], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [0.0, 60.0, 87.0, 711.0, 22.5, 28.72138084989255, 1.183564341301159, 1.0, 1.0, 40.0, 15.0603275639153], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.29, 0.7856353591160221, 0.375, 0.8934484041577125, 0.8945214471003863, 1.0, 1.0, 0.5, 0.150603275639153], 
reward next is 0.8494, 
noisyNet noise sample is [array([2.7598543], dtype=float32), 0.12677963]. 
=============================================
[2019-04-09 14:57:27,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.4997294]
 [3.4541647]
 [3.5402124]
 [3.5188944]
 [3.7171383]], R is [[4.25753117]
 [5.09581089]
 [5.92521238]
 [6.85367393]
 [7.5791831 ]].
[2019-04-09 14:57:27,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00687509 0.10313985 0.14358835 0.04939769 0.03102606 0.0127554
 0.15815578 0.04361366 0.11650932 0.06374923 0.27118957], sum to 1.0000
[2019-04-09 14:57:27,441] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6390
[2019-04-09 14:57:27,455] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 68.0, 0.0, 0.0, 22.5, 27.58226372413008, 0.9885678444103944, 0.0, 1.0, 65.0, 34.94153491593745], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3785400.0000, 
sim time next is 3786000.0000, 
raw observation next is [-2.0, 67.0, 0.0, 0.0, 22.5, 27.49894776316136, 0.9801939921488753, 1.0, 1.0, 55.0, 35.60112718563617], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.67, 0.0, 0.0, 0.375, 0.7915789802634468, 0.8267313307162918, 1.0, 1.0, 0.8, 0.3560112718563617], 
reward next is 0.6440, 
noisyNet noise sample is [array([0.98958296], dtype=float32), -0.78808266]. 
=============================================
[2019-04-09 14:57:27,479] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[3.4934156]
 [3.5084596]
 [3.5707252]
 [3.504055 ]
 [3.5042057]], R is [[4.10781479]
 [4.7173214 ]
 [5.42482185]
 [6.02633429]
 [6.53539848]].
[2019-04-09 14:57:28,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00721881 0.1059813  0.13585807 0.05107912 0.03363798 0.01691608
 0.14842366 0.06993762 0.12992653 0.07884411 0.22217672], sum to 1.0000
[2019-04-09 14:57:28,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-09 14:57:28,038] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.833333333333333, 70.0, 0.0, 0.0, 19.0, 27.30068183617553, 0.954928498305868, 0.0, 1.0, 45.0, 29.48540081750188], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3790200.0000, 
sim time next is 3790800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 27.28260818177878, 0.9504755206360506, 0.0, 1.0, 60.0, 37.88897860798093], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7735506818148984, 0.8168251735453502, 0.0, 1.0, 0.9, 0.37888978607980933], 
reward next is 0.6211, 
noisyNet noise sample is [array([1.1029338], dtype=float32), -0.38465917]. 
=============================================
[2019-04-09 14:57:28,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01446333 0.0874907  0.09628873 0.06322777 0.03846214 0.01944809
 0.19845636 0.06759564 0.09188053 0.08771612 0.23497054], sum to 1.0000
[2019-04-09 14:57:28,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7082
[2019-04-09 14:57:28,562] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.5, 71.0, 0.0, 0.0, 19.0, 26.89737850166676, 0.6746456525238655, 0.0, 1.0, 65.0, 46.27584965272847], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [-3.666666666666667, 73.0, 0.0, 0.0, 19.0, 26.81367629991173, 0.6754393652333431, 0.0, 1.0, 60.0, 46.54062914978537], 
processed observation next is [1.0, 0.2608695652173913, 0.3610341643582641, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7344730249926442, 0.725146455077781, 0.0, 1.0, 0.9, 0.4654062914978537], 
reward next is 0.5346, 
noisyNet noise sample is [array([0.506967], dtype=float32), -0.053441904]. 
=============================================
[2019-04-09 14:57:29,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01261763 0.0885741  0.11463889 0.03797989 0.02935465 0.02007815
 0.18262795 0.08351225 0.06512143 0.09569733 0.2697977 ], sum to 1.0000
[2019-04-09 14:57:29,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4464
[2019-04-09 14:57:29,374] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.87250608489293, 0.7886084651167286, 0.0, 1.0, 25.0, 37.33855197540827], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3807600.0000, 
sim time next is 3808200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.9277309262228, 0.7913690264250125, 0.0, 1.0, 45.0, 37.89424429944268], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7439775771852334, 0.7637896754750041, 0.0, 1.0, 0.6, 0.3789424429944268], 
reward next is 0.6211, 
noisyNet noise sample is [array([0.21821178], dtype=float32), -2.4273977]. 
=============================================
[2019-04-09 14:57:29,384] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01286728 0.09421504 0.10876719 0.04364843 0.03115775 0.02313585
 0.180135   0.07337161 0.07489201 0.08978886 0.2680211 ], sum to 1.0000
[2019-04-09 14:57:29,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-09 14:57:29,420] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.9277309262228, 0.7913690264250125, 0.0, 1.0, 45.0, 37.89424429944268], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3808200.0000, 
sim time next is 3808800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.97924086830865, 0.7826276386443038, 0.0, 1.0, 25.0, 37.03307772587443], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7482700723590542, 0.7608758795481013, 0.0, 1.0, 0.2, 0.3703307772587443], 
reward next is 0.6297, 
noisyNet noise sample is [array([0.21821178], dtype=float32), -2.4273977]. 
=============================================
[2019-04-09 14:57:29,565] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00881742 0.10906759 0.15531935 0.06950573 0.03310117 0.01321215
 0.14236484 0.06776138 0.11216081 0.08735824 0.20133124], sum to 1.0000
[2019-04-09 14:57:29,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2281
[2019-04-09 14:57:29,578] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.166666666666667, 66.0, 0.0, 0.0, 19.0, 27.36728091748619, 0.9705991997336408, 0.0, 1.0, 50.0, 32.24838971633981], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3787800.0000, 
sim time next is 3788400.0000, 
raw observation next is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 27.34900162703248, 0.9653711029772761, 0.0, 1.0, 25.0, 31.93036288345409], 
processed observation next is [1.0, 0.8695652173913043, 0.3979686057248385, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7790834689193732, 0.8217903676590921, 0.0, 1.0, 0.2, 0.3193036288345409], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.9710516], dtype=float32), -0.11263743]. 
=============================================
[2019-04-09 14:57:29,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00667401 0.11340638 0.1689107  0.05445261 0.03454051 0.01737203
 0.13318735 0.06508833 0.11958058 0.06115597 0.22563152], sum to 1.0000
[2019-04-09 14:57:29,730] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5157
[2019-04-09 14:57:29,743] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 60.83333333333333, 117.6666666666667, 825.3333333333334, 22.5, 28.0956149666935, 1.061277509898739, 1.0, 1.0, 65.0, 25.74090751272101], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3761400.0000, 
sim time next is 3762000.0000, 
raw observation next is [-1.0, 60.0, 117.0, 823.5, 22.5, 28.12121744245218, 1.057154894583348, 1.0, 1.0, 45.0, 21.13001946568147], 
processed observation next is [1.0, 0.5652173913043478, 0.4349030470914128, 0.6, 0.39, 0.9099447513812154, 0.375, 0.8434347868710151, 0.852384964861116, 1.0, 1.0, 0.6, 0.2113001946568147], 
reward next is 0.7887, 
noisyNet noise sample is [array([0.26894426], dtype=float32), 0.13689947]. 
=============================================
[2019-04-09 14:57:29,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[3.4627457]
 [3.410078 ]
 [3.3751795]
 [3.5452337]
 [3.3610258]], R is [[4.24887562]
 [4.94897795]
 [5.64418125]
 [6.3690033 ]
 [7.08529234]].
[2019-04-09 14:57:29,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01649304 0.09854349 0.13438953 0.04955857 0.03070139 0.02381169
 0.17185026 0.06241124 0.08182106 0.098649   0.23177074], sum to 1.0000
[2019-04-09 14:57:29,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-09 14:57:29,957] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 73.0, 0.0, 0.0, 19.0, 26.85678909494739, 0.7558780303724926, 0.0, 1.0, 55.0, 43.52608150895549], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3811200.0000, 
sim time next is 3811800.0000, 
raw observation next is [-4.0, 72.0, 0.0, 0.0, 19.0, 26.75059790101189, 0.748087678942167, 0.0, 1.0, 50.0, 42.61378777331803], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7292164917509908, 0.749362559647389, 0.0, 1.0, 0.7, 0.4261378777331803], 
reward next is 0.5739, 
noisyNet noise sample is [array([-1.0567586], dtype=float32), -1.3733871]. 
=============================================
[2019-04-09 14:57:30,354] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01722995 0.09440058 0.10967969 0.06800881 0.03734193 0.02243245
 0.1886178  0.05878536 0.08875438 0.08528219 0.22946678], sum to 1.0000
[2019-04-09 14:57:30,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4645
[2019-04-09 14:57:30,394] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 19.0, 26.48575453689085, 0.6598113043057694, 0.0, 1.0, 45.0, 48.84722509385315], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3825000.0000, 
sim time next is 3825600.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 19.0, 26.47399860434675, 0.6508609268360858, 0.0, 1.0, 45.0, 40.24961858769319], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7061665503622292, 0.7169536422786953, 0.0, 1.0, 0.6, 0.4024961858769319], 
reward next is 0.5975, 
noisyNet noise sample is [array([0.21919958], dtype=float32), -0.7875391]. 
=============================================
[2019-04-09 14:57:30,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00606109 0.15321368 0.15265237 0.04578238 0.02472453 0.01397214
 0.09531339 0.04367411 0.11807074 0.09321522 0.25332037], sum to 1.0000
[2019-04-09 14:57:30,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6491
[2019-04-09 14:57:30,673] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.0, 48.0, 96.5, 749.5, 22.5, 27.93030528479735, 1.165731633337121, 1.0, 1.0, 65.0, 19.00547380756396], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3855600.0000, 
sim time next is 3856200.0000, 
raw observation next is [2.166666666666667, 47.5, 93.33333333333334, 738.6666666666667, 22.5, 27.42435189177941, 1.079917547843505, 1.0, 1.0, 55.0, 6.512002826907151], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.475, 0.3111111111111111, 0.816206261510129, 0.375, 0.7853626576482841, 0.859972515947835, 1.0, 1.0, 0.8, 0.06512002826907151], 
reward next is 0.9349, 
noisyNet noise sample is [array([0.2789536], dtype=float32), 0.6325382]. 
=============================================
[2019-04-09 14:57:30,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00879144 0.12236349 0.13255395 0.07478864 0.03029284 0.02200348
 0.1261337  0.06566089 0.10381289 0.08459114 0.22900753], sum to 1.0000
[2019-04-09 14:57:30,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5909
[2019-04-09 14:57:30,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.6666666666666667, 58.5, 117.0, 830.6666666666667, 22.5, 27.92233398367776, 1.058215117111655, 1.0, 1.0, 20.0, 21.88448827607087], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3845400.0000, 
sim time next is 3846000.0000, 
raw observation next is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 22.5, 27.58295680285688, 0.9948160717679532, 1.0, 1.0, 55.0, 12.76333891785264], 
processed observation next is [1.0, 0.5217391304347826, 0.4533702677747, 0.5700000000000001, 0.39, 0.9202578268876612, 0.375, 0.7985797335714068, 0.8316053572559844, 1.0, 1.0, 0.8, 0.1276333891785264], 
reward next is 0.8724, 
noisyNet noise sample is [array([1.8273253], dtype=float32), 0.32813114]. 
=============================================
[2019-04-09 14:57:30,748] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[3.3729534]
 [3.4732823]
 [3.4803088]
 [3.2811165]
 [3.4891455]], R is [[4.17593002]
 [4.91532564]
 [5.37229061]
 [6.12456369]
 [6.93947744]].
[2019-04-09 14:57:30,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00725454 0.13408951 0.13077863 0.05825683 0.0228271  0.01232843
 0.13771185 0.05145567 0.11609042 0.08705939 0.24214762], sum to 1.0000
[2019-04-09 14:57:30,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0629
[2019-04-09 14:57:30,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 27.93398475069135, 1.061992310269235, 1.0, 1.0, 30.0, 26.03687385823369], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3782400.0000, 
sim time next is 3783000.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 27.86149694030438, 1.054514332842651, 1.0, 1.0, 20.0, 25.70134428705315], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.8217914116920317, 0.851504777614217, 1.0, 1.0, 0.1, 0.2570134428705315], 
reward next is 0.7430, 
noisyNet noise sample is [array([-0.9454378], dtype=float32), -0.20101106]. 
=============================================
[2019-04-09 14:57:30,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[3.4448326]
 [3.6165369]
 [3.6242125]
 [3.5496619]
 [3.4957058]], R is [[4.42558956]
 [5.120965  ]
 [5.8068676 ]
 [6.50181675]
 [7.20337391]].
[2019-04-09 14:57:30,955] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00589149 0.11227889 0.12692806 0.05632274 0.02166641 0.0119137
 0.16938257 0.04712072 0.11940765 0.06679331 0.26229447], sum to 1.0000
[2019-04-09 14:57:30,956] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9235
[2019-04-09 14:57:30,982] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 27.34387062632349, 0.9506154805524597, 0.0, 1.0, 55.0, 35.13237712041563], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3788400.0000, 
sim time next is 3789000.0000, 
raw observation next is [-2.5, 68.0, 0.0, 0.0, 19.0, 27.32676579668302, 0.9490724352584031, 0.0, 1.0, 65.0, 37.2192069806045], 
processed observation next is [1.0, 0.8695652173913043, 0.39335180055401664, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7772304830569183, 0.8163574784194677, 0.0, 1.0, 1.0, 0.37219206980604497], 
reward next is 0.6278, 
noisyNet noise sample is [array([-0.5809347], dtype=float32), 1.223598]. 
=============================================
[2019-04-09 14:57:30,992] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[3.9103675]
 [3.8499184]
 [3.8167834]
 [3.7904797]
 [3.9233713]], R is [[4.44015837]
 [5.04443312]
 [5.63278103]
 [6.21872187]
 [6.84602261]].
[2019-04-09 14:57:31,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00589482 0.13523775 0.13746381 0.05428055 0.03532952 0.00985685
 0.15502308 0.06869717 0.10395468 0.06347498 0.23078682], sum to 1.0000
[2019-04-09 14:57:31,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0171
[2019-04-09 14:57:31,318] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 55.5, 117.0, 835.0, 22.5, 27.56766677267683, 1.014089560429776, 1.0, 1.0, 35.0, 12.15840776546737], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [0.3333333333333333, 54.0, 116.3333333333333, 833.1666666666667, 22.5, 27.92244224113283, 1.0406633847547, 1.0, 1.0, 20.0, 18.39796674830945], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.54, 0.38777777777777767, 0.9206261510128915, 0.375, 0.8268701867610693, 0.8468877949182333, 1.0, 1.0, 0.1, 0.1839796674830945], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.18130414], dtype=float32), 0.18910867]. 
=============================================
[2019-04-09 14:57:31,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00524682 0.12410351 0.13807885 0.06032382 0.02903558 0.01378613
 0.13299978 0.06281435 0.14226012 0.07982156 0.2115294 ], sum to 1.0000
[2019-04-09 14:57:31,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4888
[2019-04-09 14:57:31,662] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 114.6666666666667, 806.3333333333334, 22.5, 27.43380554658937, 0.9532159702399382, 1.0, 1.0, 65.0, 19.88742146497524], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3841800.0000, 
sim time next is 3842400.0000, 
raw observation next is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 22.5, 27.6573413343144, 0.978665014142606, 1.0, 1.0, 45.0, 22.94667883836099], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6000000000000001, 0.386111111111111, 0.8996316758747698, 0.375, 0.8047784445262, 0.8262216713808687, 1.0, 1.0, 0.6, 0.22946678838360993], 
reward next is 0.7705, 
noisyNet noise sample is [array([-0.73516333], dtype=float32), 1.5380415]. 
=============================================
[2019-04-09 14:57:31,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00519303 0.11451454 0.13898443 0.06614789 0.02639614 0.0197909
 0.14085905 0.06137542 0.14111519 0.06651984 0.21910347], sum to 1.0000
[2019-04-09 14:57:31,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-09 14:57:31,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00842326 0.10784404 0.1418685  0.0782361  0.03775303 0.01985406
 0.14417265 0.04741439 0.10992752 0.0569875  0.24751897], sum to 1.0000
[2019-04-09 14:57:31,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-09 14:57:31,697] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 106.0, 782.0, 22.5, 28.51021932274291, 1.13798623061018, 1.0, 1.0, 50.0, 13.91152109736562], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 22.5, 28.55653500491422, 1.14851604126406, 1.0, 1.0, 45.0, 13.54668959009052], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3427777777777777, 0.8521178637200737, 0.375, 0.8797112504095184, 0.8828386804213534, 1.0, 1.0, 0.6, 0.1354668959009052], 
reward next is 0.8645, 
noisyNet noise sample is [array([-0.68327075], dtype=float32), -0.600286]. 
=============================================
[2019-04-09 14:57:31,723] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 22.5, 28.59456286478476, 1.158888327261856, 1.0, 1.0, 65.0, 15.13051066451557], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3854400.0000, 
sim time next is 3855000.0000, 
raw observation next is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 22.5, 28.62851293994318, 1.039808751295495, 1.0, 1.0, 65.0, 68.53172259780419], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3322222222222222, 0.840147329650092, 0.375, 0.8857094116619318, 0.8466029170984983, 1.0, 1.0, 1.0, 0.6853172259780419], 
reward next is 0.3147, 
noisyNet noise sample is [array([-0.7934755], dtype=float32), -2.3286288]. 
=============================================
[2019-04-09 14:57:31,739] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.70996  ]
 [3.7486298]
 [3.847005 ]
 [3.667961 ]
 [3.844347 ]], R is [[4.01716089]
 [4.82568407]
 [5.64179993]
 [6.47617388]
 [7.2352066 ]].
[2019-04-09 14:57:31,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01792708 0.10613865 0.10858135 0.05008036 0.03871681 0.0236344
 0.18377072 0.06331265 0.1010666  0.07394471 0.23282675], sum to 1.0000
[2019-04-09 14:57:31,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9934
[2019-04-09 14:57:31,787] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.57787953812017, 0.7011484572462351, 0.0, 1.0, 20.0, 41.35975040117616], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3819000.0000, 
sim time next is 3819600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.58546950280131, 0.7091929290077209, 0.0, 1.0, 65.0, 49.17208356451486], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7154557919001091, 0.7363976430025736, 0.0, 1.0, 1.0, 0.49172083564514857], 
reward next is 0.5083, 
noisyNet noise sample is [array([-1.5128019], dtype=float32), -2.1222067]. 
=============================================
[2019-04-09 14:57:31,809] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 102651: loss 25.6178
[2019-04-09 14:57:31,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 102651: learning rate 0.0000
[2019-04-09 14:57:32,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01766641 0.09771797 0.14189471 0.04152266 0.02704495 0.02008492
 0.16711509 0.06012462 0.08048897 0.08447047 0.2618692 ], sum to 1.0000
[2019-04-09 14:57:32,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5864
[2019-04-09 14:57:32,261] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 22.5, 26.50896903507061, 0.6564170925709558, 0.0, 1.0, 30.0, 50.13213615941803], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3827400.0000, 
sim time next is 3828000.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 22.5, 26.50748685337642, 0.6484116125001357, 1.0, 1.0, 55.0, 37.41140456421821], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.0, 0.0, 0.375, 0.7089572377813683, 0.7161372041667119, 1.0, 1.0, 0.8, 0.3741140456421821], 
reward next is 0.6259, 
noisyNet noise sample is [array([0.01701982], dtype=float32), 0.589535]. 
=============================================
[2019-04-09 14:57:32,269] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01785087 0.10395843 0.09705796 0.05809736 0.03382831 0.02032971
 0.18442668 0.06504016 0.09624444 0.09954971 0.2236164 ], sum to 1.0000
[2019-04-09 14:57:32,271] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2732
[2019-04-09 14:57:32,287] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 26.62560556414132, 0.6816898119878628, 0.0, 1.0, 45.0, 39.95725093028986], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3908400.0000, 
sim time next is 3909000.0000, 
raw observation next is [-5.666666666666666, 62.0, 0.0, 0.0, 19.0, 26.57088220400905, 0.6642193776044939, 0.0, 1.0, 30.0, 36.18421308772568], 
processed observation next is [1.0, 0.21739130434782608, 0.3056325023084026, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7142401836674207, 0.721406459201498, 0.0, 1.0, 0.3, 0.3618421308772568], 
reward next is 0.6382, 
noisyNet noise sample is [array([-1.0885072], dtype=float32), 1.113887]. 
=============================================
[2019-04-09 14:57:32,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[2.5143616]
 [2.5368314]
 [2.5250537]
 [2.6510303]
 [2.704643 ]], R is [[3.23035598]
 [3.69673109]
 [4.07583714]
 [4.61879349]
 [5.18127108]].
[2019-04-09 14:57:32,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00712403 0.08193818 0.1468096  0.04733112 0.02758781 0.01540563
 0.12519483 0.06425834 0.12121469 0.06214547 0.30099034], sum to 1.0000
[2019-04-09 14:57:32,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8703
[2019-04-09 14:57:32,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[2.3949888]
 [2.606666 ]
 [2.460343 ]
 [2.5153008]
 [2.5602517]], R is [[3.15450335]
 [3.72338581]
 [4.27109385]
 [4.86596537]
 [5.39516163]].
[2019-04-09 14:57:32,341] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 43.33333333333334, 25.66666666666666, 241.0, 22.5, 28.80907024899219, 1.147277157914978, 1.0, 1.0, 65.0, 18.0247246866031], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3864000.0000, 
sim time next is 3864600.0000, 
raw observation next is [2.5, 44.5, 18.0, 179.0, 22.5, 28.63134426417781, 0.903081986149073, 1.0, 1.0, 65.0, 56.14578988054119], 
processed observation next is [1.0, 0.7391304347826086, 0.5318559556786704, 0.445, 0.06, 0.19779005524861878, 0.375, 0.8859453553481508, 0.8010273287163576, 1.0, 1.0, 1.0, 0.5614578988054119], 
reward next is 0.4385, 
noisyNet noise sample is [array([0.71595013], dtype=float32), 1.079442]. 
=============================================
[2019-04-09 14:57:32,629] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00782126 0.08418431 0.110213   0.03421157 0.03025983 0.01529946
 0.2153764  0.05575813 0.08027792 0.07595645 0.29064164], sum to 1.0000
[2019-04-09 14:57:32,629] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2181
[2019-04-09 14:57:32,646] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.00075636448918, 0.8409066461981842, 0.0, 1.0, 25.0, 37.89633262433834], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.98631512973775, 0.8359929999645009, 0.0, 1.0, 30.0, 33.751472598374], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7488595941448125, 0.7786643333215003, 0.0, 1.0, 0.3, 0.33751472598374], 
reward next is 0.6625, 
noisyNet noise sample is [array([1.7379885], dtype=float32), -0.7932044]. 
=============================================
[2019-04-09 14:57:32,714] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0083402  0.10149587 0.15393718 0.05973554 0.02513799 0.02123561
 0.11156482 0.06192571 0.13606335 0.06603856 0.2545251 ], sum to 1.0000
[2019-04-09 14:57:32,717] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-09 14:57:32,743] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 49.5, 113.0, 824.0, 22.5, 28.16963179852481, 1.079010625717572, 1.0, 1.0, 45.0, 13.32342826849655], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3850200.0000, 
sim time next is 3850800.0000, 
raw observation next is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 22.5, 28.41291490744587, 1.087697864765911, 1.0, 1.0, 65.0, 10.81421544360856], 
processed observation next is [1.0, 0.5652173913043478, 0.5087719298245615, 0.49, 0.37277777777777765, 0.9027624309392265, 0.375, 0.8677429089538226, 0.8625659549219703, 1.0, 1.0, 1.0, 0.1081421544360856], 
reward next is 0.8919, 
noisyNet noise sample is [array([-0.4383337], dtype=float32), -0.6873154]. 
=============================================
[2019-04-09 14:57:32,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00930956 0.09907878 0.11536767 0.04669835 0.02615353 0.01823923
 0.1649307  0.06276557 0.09618487 0.08095826 0.28031346], sum to 1.0000
[2019-04-09 14:57:32,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7036
[2019-04-09 14:57:32,801] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.94748793207978, 0.8192148883512459, 0.0, 1.0, 25.0, 34.62893523990886], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3891000.0000, 
sim time next is 3891600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.93899381665491, 0.8200839250690738, 0.0, 1.0, 60.0, 41.21257707941263], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7449161513879092, 0.7733613083563579, 0.0, 1.0, 0.9, 0.4121257707941263], 
reward next is 0.5879, 
noisyNet noise sample is [array([-0.17211723], dtype=float32), -1.6429356]. 
=============================================
[2019-04-09 14:57:32,999] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103235: loss 27.8641
[2019-04-09 14:57:33,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103235: learning rate 0.0000
[2019-04-09 14:57:33,169] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103319: loss 32.5792
[2019-04-09 14:57:33,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103320: learning rate 0.0000
[2019-04-09 14:57:33,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01267677 0.10926347 0.1251894  0.06071227 0.03554307 0.01993177
 0.16807692 0.06078804 0.10811509 0.09351747 0.2061857 ], sum to 1.0000
[2019-04-09 14:57:33,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7184
[2019-04-09 14:57:33,477] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 22.5, 26.15458296250949, 0.6552780134037639, 1.0, 1.0, 20.0, 42.8644486270741], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 22.5, 26.24108010398441, 0.698871595302777, 1.0, 1.0, 60.0, 41.79261336052504], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.375, 0.6867566753320341, 0.732957198434259, 1.0, 1.0, 0.9, 0.4179261336052504], 
reward next is 0.5821, 
noisyNet noise sample is [array([1.1937417], dtype=float32), -0.3200266]. 
=============================================
[2019-04-09 14:57:33,497] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 103493: loss 25.4869
[2019-04-09 14:57:33,499] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 103493: learning rate 0.0000
[2019-04-09 14:57:33,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01274311 0.11355684 0.13320729 0.05949658 0.03831148 0.02171048
 0.15407647 0.07267607 0.10631955 0.08556437 0.20233785], sum to 1.0000
[2019-04-09 14:57:33,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7173
[2019-04-09 14:57:33,514] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 74.0, 91.0, 447.0, 22.5, 26.53721987355199, 0.7353606878085728, 1.0, 1.0, 65.0, 33.19010538259558], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3832200.0000, 
sim time next is 3832800.0000, 
raw observation next is [-4.333333333333334, 73.0, 92.66666666666667, 485.8333333333333, 22.5, 26.97846651639095, 0.7670043590019074, 1.0, 1.0, 60.0, 28.74266456060772], 
processed observation next is [1.0, 0.34782608695652173, 0.3425669436749769, 0.73, 0.3088888888888889, 0.5368324125230203, 0.375, 0.748205543032579, 0.7556681196673024, 1.0, 1.0, 0.9, 0.2874266456060772], 
reward next is 0.7126, 
noisyNet noise sample is [array([1.1937417], dtype=float32), -0.3200266]. 
=============================================
[2019-04-09 14:57:33,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00477142 0.12334397 0.13155112 0.06548362 0.02772363 0.0164183
 0.17254329 0.05380166 0.08209455 0.08118682 0.24108161], sum to 1.0000
[2019-04-09 14:57:33,540] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9777
[2019-04-09 14:57:33,564] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 42.16666666666667, 33.33333333333333, 303.0, 22.5, 28.83288870670751, 1.188705827005814, 1.0, 1.0, 65.0, 12.0256014279876], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3863400.0000, 
sim time next is 3864000.0000, 
raw observation next is [2.666666666666667, 43.33333333333334, 25.66666666666666, 241.0, 22.5, 28.87314565083842, 0.950845216705451, 1.0, 1.0, 65.0, 54.50622533813085], 
processed observation next is [1.0, 0.7391304347826086, 0.5364727608494922, 0.4333333333333334, 0.08555555555555554, 0.2662983425414365, 0.375, 0.9060954709032017, 0.8169484055684837, 1.0, 1.0, 1.0, 0.5450622533813084], 
reward next is 0.4549, 
noisyNet noise sample is [array([-0.65939105], dtype=float32), 0.3246279]. 
=============================================
[2019-04-09 14:57:33,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[3.8234377]
 [3.7176595]
 [3.7316883]
 [3.8443046]
 [3.790789 ]], R is [[4.26243782]
 [5.0995574 ]
 [5.93048382]
 [6.75594139]
 [7.64295053]].
[2019-04-09 14:57:33,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01294735 0.09915821 0.11687084 0.06627429 0.03545725 0.0197306
 0.17517905 0.06343198 0.08904353 0.08320784 0.23869912], sum to 1.0000
[2019-04-09 14:57:33,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4094
[2019-04-09 14:57:33,699] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.94529241788568, 0.7611602191944732, 0.0, 1.0, 65.0, 56.41569915622571], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3898800.0000, 
sim time next is 3899400.0000, 
raw observation next is [-2.166666666666667, 66.0, 0.0, 0.0, 19.0, 26.88456981286041, 0.7722332466056318, 0.0, 1.0, 45.0, 37.23366681173326], 
processed observation next is [1.0, 0.13043478260869565, 0.4025854108956602, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7403808177383674, 0.7574110822018773, 0.0, 1.0, 0.6, 0.37233666811733257], 
reward next is 0.6277, 
noisyNet noise sample is [array([-0.5035725], dtype=float32), 1.0672729]. 
=============================================
[2019-04-09 14:57:33,727] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103608: loss 23.9953
[2019-04-09 14:57:33,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103608: learning rate 0.0000
[2019-04-09 14:57:33,754] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103620: loss 23.3624
[2019-04-09 14:57:33,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103621: learning rate 0.0000
[2019-04-09 14:57:33,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01122477 0.08029877 0.12201238 0.05139884 0.03683549 0.01838332
 0.16940692 0.07399012 0.0924868  0.08262244 0.26134014], sum to 1.0000
[2019-04-09 14:57:34,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6279
[2019-04-09 14:57:34,023] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103759: loss 22.0612
[2019-04-09 14:57:34,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103759: learning rate 0.0000
[2019-04-09 14:57:34,032] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103763: loss 19.4283
[2019-04-09 14:57:34,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103763: learning rate 0.0000
[2019-04-09 14:57:34,039] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.5, 62.5, 0.0, 0.0, 19.0, 27.0367116391127, 0.8419953852373537, 0.0, 1.0, 45.0, 30.64695013976749], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3886200.0000, 
sim time next is 3886800.0000, 
raw observation next is [-1.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 26.96654277495158, 0.8340144505581826, 0.0, 1.0, 55.0, 39.46752811281273], 
processed observation next is [1.0, 1.0, 0.4164358264081256, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.7472118979126318, 0.7780048168527275, 0.0, 1.0, 0.8, 0.3946752811281273], 
reward next is 0.6053, 
noisyNet noise sample is [array([-2.2624502], dtype=float32), 0.7099464]. 
=============================================
[2019-04-09 14:57:34,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00626839 0.0729262  0.09377019 0.03756149 0.02498422 0.01814735
 0.20287447 0.06347762 0.09222431 0.09862223 0.28914353], sum to 1.0000
[2019-04-09 14:57:34,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4170
[2019-04-09 14:57:34,112] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.96725776540313, 0.8155846771494984, 0.0, 1.0, 60.0, 40.90911175142524], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3889200.0000, 
sim time next is 3889800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.95280181992354, 0.8110864968203941, 0.0, 1.0, 45.0, 37.28488003304899], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7460668183269616, 0.770362165606798, 0.0, 1.0, 0.6, 0.37284880033048995], 
reward next is 0.6272, 
noisyNet noise sample is [array([-0.75815254], dtype=float32), -0.2971771]. 
=============================================
[2019-04-09 14:57:34,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0123952  0.07620168 0.12888655 0.04781082 0.03595016 0.01527485
 0.19928575 0.06062317 0.08539037 0.08014205 0.25803944], sum to 1.0000
[2019-04-09 14:57:34,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8402
[2019-04-09 14:57:34,518] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.02098424407823, 0.8184489644202743, 0.0, 1.0, 65.0, 33.11678792379327], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3896400.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.18390686120915, 0.7979727567625122, 0.0, 1.0, 65.0, 39.13834753721352], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7653255717674291, 0.7659909189208375, 0.0, 1.0, 1.0, 0.3913834753721352], 
reward next is 0.6086, 
noisyNet noise sample is [array([1.8489431], dtype=float32), 0.39793333]. 
=============================================
[2019-04-09 14:57:34,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.9348285]
 [2.7836819]
 [2.8254702]
 [2.8522792]
 [2.9272647]], R is [[3.3657589 ]
 [4.00093365]
 [4.58692408]
 [5.17357111]
 [5.70307589]].
[2019-04-09 14:57:34,569] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104028: loss 18.0783
[2019-04-09 14:57:34,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104028: learning rate 0.0000
[2019-04-09 14:57:34,696] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104091: loss 22.2507
[2019-04-09 14:57:34,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104091: learning rate 0.0000
[2019-04-09 14:57:34,702] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104093: loss 31.3319
[2019-04-09 14:57:34,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104093: learning rate 0.0000
[2019-04-09 14:57:34,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00489844 0.12808253 0.12941699 0.04643733 0.02584783 0.01496971
 0.11271997 0.04596941 0.10219382 0.06286648 0.32659757], sum to 1.0000
[2019-04-09 14:57:34,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9817
[2019-04-09 14:57:34,963] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.833333333333333, 40.5, 0.0, 0.0, 22.5, 27.56436749025049, 0.9494447682275554, 1.0, 1.0, 65.0, 29.36423095635291], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.6644464096364, 0.9411857910788802, 1.0, 1.0, 65.0, 28.14512052732616], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.375, 0.8053705341363667, 0.8137285970262934, 1.0, 1.0, 1.0, 0.2814512052732616], 
reward next is 0.7185, 
noisyNet noise sample is [array([1.0407705], dtype=float32), -1.3291234]. 
=============================================
[2019-04-09 14:57:35,045] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104268: loss 25.7018
[2019-04-09 14:57:35,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104268: learning rate 0.0000
[2019-04-09 14:57:35,093] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01075047 0.07782056 0.14431962 0.03455766 0.03017088 0.01968936
 0.2084844  0.06815954 0.07729914 0.08940121 0.23934712], sum to 1.0000
[2019-04-09 14:57:35,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8664
[2019-04-09 14:57:35,118] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 57.5, 0.0, 0.0, 19.0, 27.21372287347765, 0.8770513197372877, 0.0, 1.0, 65.0, 39.4340418020638], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3882600.0000, 
sim time next is 3883200.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 19.0, 27.13730150745072, 0.8672950516986005, 0.0, 1.0, 65.0, 39.21357479346662], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.7614417922875599, 0.7890983505662001, 0.0, 1.0, 1.0, 0.3921357479346662], 
reward next is 0.6079, 
noisyNet noise sample is [array([-0.00519052], dtype=float32), -0.22134148]. 
=============================================
[2019-04-09 14:57:35,124] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01462987 0.10321563 0.13158223 0.05621192 0.03299969 0.0197782
 0.1640635  0.06839155 0.08202514 0.08203346 0.24506877], sum to 1.0000
[2019-04-09 14:57:35,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6123
[2019-04-09 14:57:35,142] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 26.66767736422882, 0.6966714503385322, 0.0, 1.0, 30.0, 43.36590432416075], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3908400.0000, 
sim time next is 3909000.0000, 
raw observation next is [-5.666666666666666, 62.0, 0.0, 0.0, 19.0, 26.63290821774572, 0.6867432021128727, 0.0, 1.0, 55.0, 39.62691517902725], 
processed observation next is [1.0, 0.21739130434782608, 0.3056325023084026, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7194090181454765, 0.7289144007042908, 0.0, 1.0, 0.8, 0.3962691517902725], 
reward next is 0.6037, 
noisyNet noise sample is [array([3.7505977], dtype=float32), -0.08725109]. 
=============================================
[2019-04-09 14:57:35,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[2.478433 ]
 [2.5443053]
 [2.5973196]
 [2.493335 ]
 [2.5658453]], R is [[3.16028309]
 [3.69502115]
 [4.18206024]
 [4.70377541]
 [5.27463722]].
[2019-04-09 14:57:35,361] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01546972 0.09225097 0.10444389 0.05093683 0.03018119 0.02171914
 0.1576483  0.07621841 0.0902753  0.09875441 0.2621018 ], sum to 1.0000
[2019-04-09 14:57:35,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5302
[2019-04-09 14:57:35,384] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.166666666666667, 63.0, 0.0, 0.0, 22.5, 26.13157480187066, 0.5232323714982925, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3913800.0000, 
sim time next is 3914400.0000, 
raw observation next is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 22.5, 25.93604399963382, 0.5650636291968826, 1.0, 1.0, 50.0, 53.42150315617644], 
processed observation next is [1.0, 0.30434782608695654, 0.2594644506001847, 0.62, 0.016666666666666666, 0.1500920810313075, 0.375, 0.661336999969485, 0.6883545430656275, 1.0, 1.0, 0.7, 0.5342150315617644], 
reward next is 0.4658, 
noisyNet noise sample is [array([1.9373184], dtype=float32), -0.30859068]. 
=============================================
[2019-04-09 14:57:35,454] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 104469: loss 21.4144
[2019-04-09 14:57:35,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 104469: learning rate 0.0000
[2019-04-09 14:57:35,481] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00523567 0.11796911 0.16648163 0.04894105 0.03055182 0.01693819
 0.13473123 0.05219845 0.0877099  0.06351555 0.27572742], sum to 1.0000
[2019-04-09 14:57:35,481] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8132
[2019-04-09 14:57:35,516] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 22.5, 28.17416083728897, 1.096530212681037, 1.0, 1.0, 30.0, 24.04755449728856], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3948000.0000, 
sim time next is 3948600.0000, 
raw observation next is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 22.5, 27.72839480719487, 1.029003308975848, 1.0, 1.0, 65.0, 9.982059604936097], 
processed observation next is [1.0, 0.6956521739130435, 0.32871652816251157, 0.3733333333333334, 0.1677777777777778, 0.45709023941068144, 0.375, 0.8106995672662393, 0.8430011029919493, 1.0, 1.0, 1.0, 0.09982059604936097], 
reward next is 0.9002, 
noisyNet noise sample is [array([0.2663779], dtype=float32), 0.45857537]. 
=============================================
[2019-04-09 14:57:35,534] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01421728 0.08419351 0.09965739 0.05152494 0.03683823 0.02175114
 0.16863225 0.07629839 0.09476442 0.08083577 0.27128655], sum to 1.0000
[2019-04-09 14:57:35,534] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5169
[2019-04-09 14:57:35,549] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.34867076844767, 0.68023487928513, 0.0, 1.0, 65.0, 53.6225066190406], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3909600.0000, 
sim time next is 3910200.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 19.0, 26.43656864845276, 0.6875003652553797, 0.0, 1.0, 50.0, 41.397825264329], 
processed observation next is [1.0, 0.2608695652173913, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.08333333333333333, 0.7030473873710633, 0.7291667884184599, 0.0, 1.0, 0.7, 0.41397825264329], 
reward next is 0.5860, 
noisyNet noise sample is [array([-1.0763967], dtype=float32), -0.7374491]. 
=============================================
[2019-04-09 14:57:35,954] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104710: loss 18.7525
[2019-04-09 14:57:35,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104712: learning rate 0.0000
[2019-04-09 14:57:36,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0053048  0.12005725 0.14220339 0.05266486 0.03451972 0.0160478
 0.09606947 0.04416828 0.1557925  0.06174009 0.27143177], sum to 1.0000
[2019-04-09 14:57:36,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5145
[2019-04-09 14:57:36,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00610845 0.11156949 0.12679815 0.05241168 0.02640869 0.01419588
 0.1344188  0.05729757 0.11237034 0.07977327 0.27864766], sum to 1.0000
[2019-04-09 14:57:36,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2170
[2019-04-09 14:57:36,254] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 36.0, 88.0, 724.0, 22.5, 26.82124590871719, 0.9553912167644971, 1.0, 1.0, 55.0, 5.853216341123815], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3943800.0000, 
sim time next is 3944400.0000, 
raw observation next is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 22.5, 27.96676944823686, 1.047833258350666, 1.0, 1.0, 55.0, 22.36305375767758], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3533333333333333, 0.28111111111111114, 0.765377532228361, 0.375, 0.8305641206864051, 0.8492777527835553, 1.0, 1.0, 0.8, 0.2236305375767758], 
reward next is 0.7764, 
noisyNet noise sample is [array([0.4353993], dtype=float32), 0.081531875]. 
=============================================
[2019-04-09 14:57:36,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00736416 0.17582503 0.15616357 0.07321362 0.02872725 0.01430336
 0.13791658 0.0517616  0.1185859  0.07104613 0.16509275], sum to 1.0000
[2019-04-09 14:57:36,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4236
[2019-04-09 14:57:36,292] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.5, 39.5, 19.0, 169.0, 22.5, 28.02572694454841, 0.9993657147046219, 1.0, 1.0, 30.0, 44.67476590049403], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3951000.0000, 
sim time next is 3951600.0000, 
raw observation next is [-5.666666666666667, 40.0, 15.83333333333333, 140.8333333333333, 22.5, 27.25524591400811, 0.9618771058468901, 1.0, 1.0, 25.0, 17.73155755888758], 
processed observation next is [1.0, 0.7391304347826086, 0.30563250230840255, 0.4, 0.05277777777777777, 0.15561694290976053, 0.375, 0.7712704928340092, 0.8206257019489634, 1.0, 1.0, 0.2, 0.1773155755888758], 
reward next is 0.8227, 
noisyNet noise sample is [array([0.6734036], dtype=float32), 2.4092932]. 
=============================================
[2019-04-09 14:57:36,292] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 22.5, 27.6203367313405, 0.6495484400326975, 1.0, 1.0, 20.0, 45.99403053327525], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 22.5, 27.64831400557095, 0.9932064758381184, 1.0, 1.0, 35.0, 27.66031071425739], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 0.375, 0.8040261671309125, 0.8310688252793729, 1.0, 1.0, 0.4, 0.27660310714257386], 
reward next is 0.7234, 
noisyNet noise sample is [array([1.4996272], dtype=float32), 0.57383925]. 
=============================================
[2019-04-09 14:57:36,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01238568 0.08594003 0.12197717 0.0405321  0.03024948 0.0182914
 0.16472165 0.07086823 0.08731721 0.08248878 0.28522825], sum to 1.0000
[2019-04-09 14:57:36,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2371
[2019-04-09 14:57:36,433] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 26.85124504151079, 0.8340872945341298, 0.0, 1.0, 55.0, 31.49779398551626], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3886800.0000, 
sim time next is 3887400.0000, 
raw observation next is [-1.833333333333333, 64.16666666666667, 0.0, 0.0, 19.0, 26.89113948204039, 0.8304501807449148, 0.0, 1.0, 30.0, 35.52760082335677], 
processed observation next is [1.0, 1.0, 0.41181902123730385, 0.6416666666666667, 0.0, 0.0, 0.08333333333333333, 0.7409282901700326, 0.7768167269149716, 0.0, 1.0, 0.3, 0.3552760082335677], 
reward next is 0.6447, 
noisyNet noise sample is [array([-0.0267066], dtype=float32), -0.11516659]. 
=============================================
[2019-04-09 14:57:36,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01473322 0.1168304  0.12057484 0.06165218 0.03299396 0.02101691
 0.16433795 0.05786778 0.09967157 0.09427454 0.2160467 ], sum to 1.0000
[2019-04-09 14:57:36,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4431
[2019-04-09 14:57:36,512] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 19.0, 26.60509435063296, 0.6906396591075684, 0.0, 1.0, 45.0, 38.43800002668097], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3910200.0000, 
sim time next is 3910800.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 26.64149655666085, 0.6818258657485488, 0.0, 1.0, 45.0, 36.72364885420819], 
processed observation next is [1.0, 0.2608695652173913, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.08333333333333333, 0.7201247130550709, 0.7272752885828496, 0.0, 1.0, 0.6, 0.3672364885420819], 
reward next is 0.6328, 
noisyNet noise sample is [array([1.9056976], dtype=float32), 0.38348418]. 
=============================================
[2019-04-09 14:57:36,584] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01095598 0.07090043 0.12747031 0.04531662 0.03268364 0.01879756
 0.19401963 0.07444271 0.07766548 0.09787921 0.24986841], sum to 1.0000
[2019-04-09 14:57:36,584] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1017
[2019-04-09 14:57:36,599] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.93642736037478, 0.8060336034922221, 0.0, 1.0, 45.0, 35.85705715756489], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.96251652961668, 0.8116116079242582, 0.0, 1.0, 25.0, 33.2142982455385], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7468763774680566, 0.7705372026414193, 0.0, 1.0, 0.2, 0.332142982455385], 
reward next is 0.6679, 
noisyNet noise sample is [array([0.5518598], dtype=float32), -0.022560712]. 
=============================================
[2019-04-09 14:57:36,672] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00721072 0.10108802 0.14114419 0.05414781 0.0336242  0.01632362
 0.165921   0.0501344  0.09781122 0.08045652 0.25213835], sum to 1.0000
[2019-04-09 14:57:36,673] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9244
[2019-04-09 14:57:36,699] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 43.0, 0.0, 0.0, 22.5, 27.0885570493149, 0.8679535918231113, 1.0, 1.0, 25.0, 35.34653614689293], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3958200.0000, 
sim time next is 3958800.0000, 
raw observation next is [-6.666666666666666, 43.66666666666667, 0.0, 0.0, 22.5, 27.05622616036208, 0.8624406672933981, 1.0, 1.0, 65.0, 39.37732706212595], 
processed observation next is [1.0, 0.8260869565217391, 0.2779316712834719, 0.4366666666666667, 0.0, 0.0, 0.375, 0.7546855133635066, 0.7874802224311327, 1.0, 1.0, 1.0, 0.3937732706212595], 
reward next is 0.6062, 
noisyNet noise sample is [array([1.3133276], dtype=float32), 1.4524221]. 
=============================================
[2019-04-09 14:57:36,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00805405 0.10939014 0.10149084 0.04566978 0.03397529 0.01241504
 0.18661585 0.05382478 0.11471564 0.07950155 0.25434703], sum to 1.0000
[2019-04-09 14:57:36,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2451
[2019-04-09 14:57:36,828] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 26.96705899970052, 0.8375432396126339, 0.0, 1.0, 20.0, 36.79538707409367], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3961200.0000, 
sim time next is 3961800.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 26.94697107770147, 0.8321653074563535, 0.0, 1.0, 30.0, 34.70131574067522], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7455809231417891, 0.7773884358187845, 0.0, 1.0, 0.3, 0.3470131574067522], 
reward next is 0.6530, 
noisyNet noise sample is [array([-1.148636], dtype=float32), -0.5968009]. 
=============================================
[2019-04-09 14:57:36,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00506842 0.11529033 0.15036239 0.04687117 0.03500777 0.0204311
 0.14228795 0.0516188  0.11747961 0.07959338 0.23598912], sum to 1.0000
[2019-04-09 14:57:36,867] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9084
[2019-04-09 14:57:36,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.66287135537812, 0.9464626138962999, 1.0, 1.0, 20.0, 30.55354297739101], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3952800.0000, 
sim time next is 3953400.0000, 
raw observation next is [-6.0, 41.00000000000001, 0.0, 0.0, 22.5, 27.58213868505368, 0.9504840096456225, 1.0, 1.0, 65.0, 33.47509958501742], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.4100000000000001, 0.0, 0.0, 0.375, 0.7985115570878065, 0.8168280032152074, 1.0, 1.0, 1.0, 0.3347509958501742], 
reward next is 0.6652, 
noisyNet noise sample is [array([-0.12565745], dtype=float32), 1.8064883]. 
=============================================
[2019-04-09 14:57:37,132] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 105252: loss 17.1951
[2019-04-09 14:57:37,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 105253: learning rate 0.0000
[2019-04-09 14:57:37,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 105255: loss 23.4035
[2019-04-09 14:57:37,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 105256: learning rate 0.0000
[2019-04-09 14:57:37,157] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00724565 0.09262576 0.12654066 0.04125796 0.03575866 0.0123234
 0.19066863 0.06692827 0.09059929 0.08111905 0.25493258], sum to 1.0000
[2019-04-09 14:57:37,157] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8850
[2019-04-09 14:57:37,194] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 19.0, 26.80818243025054, 0.7903267719658258, 0.0, 1.0, 25.0, 35.14688356141595], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 19.0, 26.78931302121621, 0.7419320875229324, 0.0, 1.0, 65.0, 49.64535983953327], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 0.08333333333333333, 0.7324427517680174, 0.7473106958409774, 0.0, 1.0, 1.0, 0.4964535983953327], 
reward next is 0.5035, 
noisyNet noise sample is [array([1.1012814], dtype=float32), -0.2343863]. 
=============================================
[2019-04-09 14:57:37,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0063188  0.11272095 0.12383082 0.04808988 0.02106301 0.0127706
 0.12026282 0.05788726 0.13093376 0.0573624  0.30875972], sum to 1.0000
[2019-04-09 14:57:37,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4089
[2019-04-09 14:57:37,539] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 22.5, 28.20686934127703, 1.090918310657005, 1.0, 1.0, 20.0, 19.9047105748348], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3948600.0000, 
sim time next is 3949200.0000, 
raw observation next is [-5.0, 38.0, 42.5, 352.5, 22.5, 27.74168770536737, 1.020281421089068, 1.0, 1.0, 45.0, 10.90434512822423], 
processed observation next is [1.0, 0.7391304347826086, 0.32409972299168976, 0.38, 0.14166666666666666, 0.38950276243093923, 0.375, 0.811807308780614, 0.8400938070296894, 1.0, 1.0, 0.6, 0.1090434512822423], 
reward next is 0.8910, 
noisyNet noise sample is [array([-0.65090966], dtype=float32), -1.4925234]. 
=============================================
[2019-04-09 14:57:37,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00981992 0.09135672 0.13545322 0.03645694 0.03129272 0.01714785
 0.21185505 0.05498239 0.08983293 0.10292438 0.21887782], sum to 1.0000
[2019-04-09 14:57:37,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3590
[2019-04-09 14:57:37,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 19.0, 26.31412173922203, 0.6574796332505392, 0.0, 1.0, 60.0, 50.28199001113496], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3973200.0000, 
sim time next is 3973800.0000, 
raw observation next is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 19.0, 26.27519049339979, 0.6470933663030873, 0.0, 1.0, 45.0, 45.10922701816864], 
processed observation next is [1.0, 1.0, 0.1902123730378578, 0.5716666666666668, 0.0, 0.0, 0.08333333333333333, 0.6895992077833158, 0.7156977887676957, 0.0, 1.0, 0.6, 0.4510922701816864], 
reward next is 0.5489, 
noisyNet noise sample is [array([-0.527336], dtype=float32), 0.14650041]. 
=============================================
[2019-04-09 14:57:38,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01348736 0.09787612 0.10343051 0.05264458 0.03063708 0.02186464
 0.17852376 0.05615745 0.08120865 0.09062845 0.27354142], sum to 1.0000
[2019-04-09 14:57:38,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6536
[2019-04-09 14:57:38,291] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 25.92201436953004, 0.5483032597765441, 0.0, 1.0, 20.0, 38.13457737092074], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.93960032547849, 0.5373405153554593, 0.0, 1.0, 60.0, 49.88716275607293], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6616333604565409, 0.6791135051184863, 0.0, 1.0, 0.9, 0.49887162756072934], 
reward next is 0.5011, 
noisyNet noise sample is [array([0.45298785], dtype=float32), 0.48641083]. 
=============================================
[2019-04-09 14:57:38,937] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01385723 0.09548093 0.12909438 0.05139106 0.03085339 0.01775575
 0.2050302  0.05715796 0.09658461 0.08671382 0.21608065], sum to 1.0000
[2019-04-09 14:57:38,939] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-09 14:57:38,952] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.83333333333333, 68.0, 0.0, 0.0, 19.0, 25.52442530969264, 0.4485791649780074, 0.0, 1.0, 55.0, 40.84237202807035], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [-13.0, 69.0, 0.0, 0.0, 19.0, 25.53054346422385, 0.4340715310945575, 0.0, 1.0, 45.0, 40.60468715856156], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6275452886853209, 0.6446905103648525, 0.0, 1.0, 0.6, 0.4060468715856156], 
reward next is 0.5940, 
noisyNet noise sample is [array([-0.8227003], dtype=float32), -1.3472255]. 
=============================================
[2019-04-09 14:57:39,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00473207 0.08893189 0.13191128 0.05817483 0.02842424 0.01298439
 0.12105092 0.06030705 0.11781931 0.07659587 0.2990681 ], sum to 1.0000
[2019-04-09 14:57:39,243] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6441
[2019-04-09 14:57:39,271] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 22.5, 27.30347186671902, 0.8942893413129972, 1.0, 1.0, 20.0, 30.94460884535707], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3957600.0000, 
sim time next is 3958200.0000, 
raw observation next is [-6.5, 43.0, 0.0, 0.0, 22.5, 27.21668356534773, 0.882108951648585, 1.0, 1.0, 60.0, 36.99555078291435], 
processed observation next is [1.0, 0.8260869565217391, 0.28254847645429365, 0.43, 0.0, 0.0, 0.375, 0.7680569637789775, 0.7940363172161949, 1.0, 1.0, 0.9, 0.3699555078291435], 
reward next is 0.6300, 
noisyNet noise sample is [array([1.2757175], dtype=float32), 0.4939127]. 
=============================================
[2019-04-09 14:57:39,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01762787 0.09067823 0.128562   0.05383281 0.03307463 0.02268957
 0.16444941 0.0623712  0.07724708 0.10623994 0.24322727], sum to 1.0000
[2019-04-09 14:57:39,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-09 14:57:39,566] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.83330292988457, 0.513827774069429, 0.0, 1.0, 20.0, 51.30588075246393], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3987600.0000, 
sim time next is 3988200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.84777456576818, 0.4973371199028878, 0.0, 1.0, 50.0, 36.01480555435781], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.653981213814015, 0.6657790399676292, 0.0, 1.0, 0.7, 0.3601480555435781], 
reward next is 0.6399, 
noisyNet noise sample is [array([-1.0681912], dtype=float32), 0.5924146]. 
=============================================
[2019-04-09 14:57:39,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01349605 0.09176349 0.11730426 0.05891198 0.03545988 0.01989572
 0.15912114 0.06997556 0.09790382 0.09745812 0.23871002], sum to 1.0000
[2019-04-09 14:57:39,716] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2018
[2019-04-09 14:57:39,739] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 25.19309715125513, 0.3757551416877645, 1.0, 1.0, 40.0, 40.53617608650561], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 22.5, 25.25033838054882, 0.3863700646337215, 1.0, 1.0, 25.0, 39.87023005229751], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.375, 0.6041948650457348, 0.6287900215445738, 1.0, 1.0, 0.2, 0.39870230052297506], 
reward next is 0.6013, 
noisyNet noise sample is [array([0.4161644], dtype=float32), -0.40887818]. 
=============================================
[2019-04-09 14:57:40,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01567876 0.08913115 0.11660595 0.05954265 0.03272358 0.02569509
 0.16710368 0.06671314 0.08362284 0.09251897 0.25066414], sum to 1.0000
[2019-04-09 14:57:40,057] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6021
[2019-04-09 14:57:40,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00968713 0.08431122 0.11235325 0.03507565 0.03204751 0.01902904
 0.17797062 0.06664234 0.07458428 0.11656912 0.27172977], sum to 1.0000
[2019-04-09 14:57:40,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5812
[2019-04-09 14:57:40,082] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-13.0, 63.0, 0.0, 0.0, 19.0, 24.31410589351081, 0.2553482566305056, 0.0, 1.0, 45.0, 48.31853132748148], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3996000.0000, 
sim time next is 3996600.0000, 
raw observation next is [-13.16666666666667, 64.0, 0.0, 0.0, 19.0, 24.53254213934258, 0.2660264123383094, 0.0, 1.0, 55.0, 46.32871293668168], 
processed observation next is [1.0, 0.2608695652173913, 0.09787626962142189, 0.64, 0.0, 0.0, 0.08333333333333333, 0.5443785116118818, 0.5886754707794365, 0.0, 1.0, 0.8, 0.4632871293668168], 
reward next is 0.5367, 
noisyNet noise sample is [array([-0.89951533], dtype=float32), -0.3593125]. 
=============================================
[2019-04-09 14:57:40,098] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 19.0, 26.20006326137368, 0.6426125306306286, 0.0, 1.0, 25.0, 50.64491609748912], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3973800.0000, 
sim time next is 3974400.0000, 
raw observation next is [-10.0, 58.0, 0.0, 0.0, 19.0, 26.21016259675464, 0.6375831062142616, 0.0, 1.0, 55.0, 40.69153594132412], 
processed observation next is [1.0, 0.0, 0.18559556786703602, 0.58, 0.0, 0.0, 0.08333333333333333, 0.68418021639622, 0.7125277020714206, 0.0, 1.0, 0.8, 0.4069153594132412], 
reward next is 0.5931, 
noisyNet noise sample is [array([0.09956791], dtype=float32), -0.87399787]. 
=============================================
[2019-04-09 14:57:40,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01540098 0.08929672 0.11646846 0.06053722 0.03291785 0.02348174
 0.17020871 0.06417849 0.08314608 0.10026032 0.24410336], sum to 1.0000
[2019-04-09 14:57:40,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2483
[2019-04-09 14:57:40,152] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 19.0, 24.70817257905258, 0.2464853800653174, 0.0, 1.0, 50.0, 35.38557767807952], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3997800.0000, 
sim time next is 3998400.0000, 
raw observation next is [-13.66666666666667, 67.0, 0.0, 0.0, 19.0, 24.65623756837435, 0.2293393609436357, 0.0, 1.0, 30.0, 33.83304384173191], 
processed observation next is [1.0, 0.2608695652173913, 0.08402585410895651, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5546864640311959, 0.5764464536478785, 0.0, 1.0, 0.3, 0.3383304384173191], 
reward next is 0.6617, 
noisyNet noise sample is [array([-0.89951533], dtype=float32), -0.3593125]. 
=============================================
[2019-04-09 14:57:40,187] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01384307 0.10351386 0.11958515 0.06112994 0.03514193 0.01977257
 0.17475593 0.08167455 0.09679265 0.10456533 0.18922505], sum to 1.0000
[2019-04-09 14:57:40,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9550
[2019-04-09 14:57:40,225] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 24.10868785673818, 0.1880606979496563, 1.0, 1.0, 65.0, 64.60194348102401], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 22.5, 24.23398545611506, 0.254011989587831, 1.0, 1.0, 65.0, 67.59582050271243], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.375, 0.5194987880095884, 0.5846706631959436, 1.0, 1.0, 1.0, 0.6759582050271242], 
reward next is 0.3240, 
noisyNet noise sample is [array([-0.9710962], dtype=float32), 0.49759632]. 
=============================================
[2019-04-09 14:57:40,418] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01369204 0.10532888 0.10612691 0.05523135 0.03245303 0.01904968
 0.20920777 0.06767569 0.09832413 0.08129143 0.21161914], sum to 1.0000
[2019-04-09 14:57:40,419] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6838
[2019-04-09 14:57:40,438] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.35481657053193, 0.4328294922065441, 0.0, 1.0, 65.0, 75.87667787307126], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3987600.0000, 
sim time next is 3988200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.3947467499277, 0.4382239340511011, 0.0, 1.0, 55.0, 49.4941012233889], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6162288958273084, 0.6460746446837003, 0.0, 1.0, 0.8, 0.49494101223388903], 
reward next is 0.5051, 
noisyNet noise sample is [array([0.06441474], dtype=float32), -2.7520347]. 
=============================================
[2019-04-09 14:57:40,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00691828 0.09914    0.19041699 0.05094482 0.02470052 0.01225813
 0.15044355 0.0462336  0.11957291 0.07557452 0.22379667], sum to 1.0000
[2019-04-09 14:57:40,665] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0738
[2019-04-09 14:57:40,682] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 22.5, 27.54944016132013, 0.8740711026105138, 1.0, 1.0, 65.0, 30.44657511736151], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 22.5, 27.42092346906421, 0.8663267719800372, 1.0, 1.0, 20.0, 29.42360229332949], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.375, 0.7850769557553509, 0.7887755906600123, 1.0, 1.0, 0.1, 0.2942360229332949], 
reward next is 0.7058, 
noisyNet noise sample is [array([2.016359], dtype=float32), -2.0375977]. 
=============================================
[2019-04-09 14:57:40,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[4.0875716]
 [4.1554637]
 [4.1153383]
 [3.9472902]
 [4.2928534]], R is [[4.72944307]
 [5.37768269]
 [6.02214384]
 [6.74871397]
 [7.49881458]].
[2019-04-09 14:57:40,717] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00521565 0.11732207 0.15068984 0.06453098 0.02760247 0.01817534
 0.12844706 0.04630475 0.09539973 0.06957024 0.27674192], sum to 1.0000
[2019-04-09 14:57:40,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0516
[2019-04-09 14:57:40,743] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 22.5, 26.88354317655147, 0.822102018403183, 1.0, 1.0, 45.0, 11.81514102810091], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4022400.0000, 
sim time next is 4023000.0000, 
raw observation next is [-3.5, 27.5, 114.0, 830.0, 22.5, 27.40862387036424, 0.8627094178280474, 1.0, 1.0, 20.0, 22.28689426937367], 
processed observation next is [1.0, 0.5652173913043478, 0.36565096952908593, 0.275, 0.38, 0.9171270718232044, 0.375, 0.7840519891970201, 0.7875698059426824, 1.0, 1.0, 0.1, 0.2228689426937367], 
reward next is 0.7771, 
noisyNet noise sample is [array([1.3904438], dtype=float32), -0.40976828]. 
=============================================
[2019-04-09 14:57:40,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[3.6765091]
 [3.7281392]
 [3.6522527]
 [3.6583097]
 [3.5733008]], R is [[4.42815018]
 [5.26571751]
 [5.87151194]
 [6.34960651]
 [7.07519388]].
[2019-04-09 14:57:40,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01185842 0.09261696 0.13823915 0.04744071 0.02492071 0.01921489
 0.15911676 0.05567855 0.10065839 0.08828718 0.26196826], sum to 1.0000
[2019-04-09 14:57:40,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8553
[2019-04-09 14:57:40,801] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.01218161201226, 0.5513748030026014, 0.0, 1.0, 50.0, 39.46089725916665], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3985200.0000, 
sim time next is 3985800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.04335292056083, 0.5519282271411156, 0.0, 1.0, 65.0, 51.64105456118592], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.670279410046736, 0.6839760757137051, 0.0, 1.0, 1.0, 0.5164105456118592], 
reward next is 0.4836, 
noisyNet noise sample is [array([-0.02636362], dtype=float32), 0.50501126]. 
=============================================
[2019-04-09 14:57:40,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00733865 0.14779471 0.12442057 0.07000568 0.03024311 0.01559979
 0.11574147 0.05782114 0.12963098 0.07791663 0.22348732], sum to 1.0000
[2019-04-09 14:57:40,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3975
[2019-04-09 14:57:40,959] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 22.5, 26.46393275956013, 0.6048569095601162, 1.0, 1.0, 65.0, 45.81807638480296], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4007400.0000, 
sim time next is 4008000.0000, 
raw observation next is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 22.5, 26.61051641771051, 0.6327046843862995, 1.0, 1.0, 45.0, 35.85951885575925], 
processed observation next is [1.0, 0.391304347826087, 0.17636195752539252, 0.5, 0.33222222222222225, 0.7244935543278086, 0.375, 0.7175430348092092, 0.7109015614620998, 1.0, 1.0, 0.6, 0.3585951885575925], 
reward next is 0.6414, 
noisyNet noise sample is [array([1.1222533], dtype=float32), 0.66508293]. 
=============================================
[2019-04-09 14:57:40,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[3.1771555]
 [3.245345 ]
 [3.2552774]
 [3.1868086]
 [3.2170577]], R is [[3.8884654 ]
 [4.39139986]
 [4.99741697]
 [5.5977478 ]
 [6.16831636]].
[2019-04-09 14:57:40,988] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00935862 0.14226724 0.13862178 0.06178825 0.04186483 0.01347097
 0.1291455  0.0537136  0.13860446 0.07092582 0.20023897], sum to 1.0000
[2019-04-09 14:57:40,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9484
[2019-04-09 14:57:41,014] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 22.5, 26.61051641771051, 0.6327046843862995, 1.0, 1.0, 45.0, 35.85951885575925], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [-10.0, 48.5, 101.0, 698.0, 22.5, 26.74053224432714, 0.6515113239265033, 1.0, 1.0, 65.0, 33.59175335601556], 
processed observation next is [1.0, 0.391304347826087, 0.18559556786703602, 0.485, 0.33666666666666667, 0.7712707182320442, 0.375, 0.7283776870272618, 0.7171704413088346, 1.0, 1.0, 1.0, 0.3359175335601556], 
reward next is 0.6641, 
noisyNet noise sample is [array([0.03785792], dtype=float32), -0.27276173]. 
=============================================
[2019-04-09 14:57:41,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00774543 0.10275708 0.11823222 0.04096302 0.02945204 0.01494202
 0.18218976 0.05949167 0.068578   0.07995977 0.29568905], sum to 1.0000
[2019-04-09 14:57:41,020] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3518
[2019-04-09 14:57:41,037] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.166666666666666, 53.83333333333334, 0.0, 0.0, 19.0, 26.4769478192502, 0.6900905931776627, 0.0, 1.0, 65.0, 52.085355806123], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 19.0, 26.44997372230914, 0.6829573807149653, 0.0, 1.0, 45.0, 47.95343699928215], 
processed observation next is [1.0, 1.0, 0.20406278855032317, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7041644768590949, 0.7276524602383218, 0.0, 1.0, 0.6, 0.47953436999282145], 
reward next is 0.5205, 
noisyNet noise sample is [array([0.616948], dtype=float32), -2.2871945]. 
=============================================
[2019-04-09 14:57:41,053] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[3.333173 ]
 [3.165424 ]
 [3.3559995]
 [3.2846339]
 [3.6644378]], R is [[3.66488576]
 [4.10738325]
 [4.63043451]
 [5.1406908 ]
 [5.58933926]].
[2019-04-09 14:57:41,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00767222 0.15143697 0.10881101 0.06747241 0.03554444 0.0123524
 0.14297451 0.05809049 0.09405861 0.060306   0.26128086], sum to 1.0000
[2019-04-09 14:57:41,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4668
[2019-04-09 14:57:41,259] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 22.5, 27.00946202282046, 0.7845868383364433, 1.0, 1.0, 20.0, 26.67259125816751], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4020600.0000, 
sim time next is 4021200.0000, 
raw observation next is [-4.0, 29.0, 116.0, 835.5, 22.5, 27.2571359445712, 0.8062649396716401, 1.0, 1.0, 45.0, 23.92077866750564], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.29, 0.38666666666666666, 0.9232044198895027, 0.375, 0.7714279953809333, 0.7687549798905468, 1.0, 1.0, 0.6, 0.2392077866750564], 
reward next is 0.7608, 
noisyNet noise sample is [array([-0.01809062], dtype=float32), 0.9225926]. 
=============================================
[2019-04-09 14:57:41,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01110763 0.10792983 0.1158844  0.04387232 0.03500014 0.02137443
 0.20146509 0.07146724 0.09434471 0.07490268 0.22265157], sum to 1.0000
[2019-04-09 14:57:41,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8632
[2019-04-09 14:57:41,884] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 39.0, 0.0, 0.0, 19.0, 26.40340007244189, 0.6365576265460877, 0.0, 1.0, 45.0, 38.53598445815121], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4062600.0000, 
sim time next is 4063200.0000, 
raw observation next is [-6.0, 39.66666666666666, 0.0, 0.0, 19.0, 26.41343425605167, 0.6312065468307095, 0.0, 1.0, 65.0, 47.4348104396614], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39666666666666656, 0.0, 0.0, 0.08333333333333333, 0.7011195213376391, 0.7104021822769031, 0.0, 1.0, 1.0, 0.47434810439661396], 
reward next is 0.5257, 
noisyNet noise sample is [array([1.6888593], dtype=float32), 1.1795914]. 
=============================================
[2019-04-09 14:57:42,821] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01298334 0.10451241 0.10268822 0.05502493 0.03755577 0.01914078
 0.18257247 0.06241696 0.10543904 0.09462982 0.22303636], sum to 1.0000
[2019-04-09 14:57:42,824] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5610
[2019-04-09 14:57:42,842] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 19.0, 26.39879887273153, 0.5838656148385984, 0.0, 1.0, 25.0, 38.32945904825036], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 19.0, 26.35268836456433, 0.5740460140907191, 0.0, 1.0, 50.0, 35.69544549627133], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.08333333333333333, 0.6960573637136941, 0.6913486713635729, 0.0, 1.0, 0.7, 0.35695445496271333], 
reward next is 0.6430, 
noisyNet noise sample is [array([-0.29158756], dtype=float32), 0.13696088]. 
=============================================
[2019-04-09 14:57:43,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00748409 0.1100693  0.13859585 0.04681067 0.02424802 0.01252301
 0.1522758  0.05607352 0.08516069 0.08833762 0.27842137], sum to 1.0000
[2019-04-09 14:57:43,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2718
[2019-04-09 14:57:43,226] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 29.66666666666666, 0.0, 0.0, 19.0, 26.83886331312998, 0.789630570982316, 0.0, 1.0, 20.0, 57.32553607779816], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [-4.0, 29.33333333333333, 0.0, 0.0, 19.0, 26.78390509650862, 0.7927762244174642, 0.0, 1.0, 30.0, 32.4327701749277], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.2933333333333333, 0.0, 0.0, 0.08333333333333333, 0.7319920913757182, 0.7642587414724881, 0.0, 1.0, 0.3, 0.32432770174927694], 
reward next is 0.6757, 
noisyNet noise sample is [array([-0.10936364], dtype=float32), -0.11133639]. 
=============================================
[2019-04-09 14:57:43,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00819717 0.08603963 0.10446674 0.04597401 0.03818274 0.01561411
 0.2149885  0.04766797 0.09365061 0.06892669 0.27629188], sum to 1.0000
[2019-04-09 14:57:43,244] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1963
[2019-04-09 14:57:43,270] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 19.0, 26.89291374171154, 0.7588784413563626, 0.0, 1.0, 65.0, 38.07383317276742], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [-5.0, 31.0, 0.0, 0.0, 19.0, 26.88072805051554, 0.7550449992234834, 0.0, 1.0, 25.0, 33.59119620505287], 
processed observation next is [1.0, 0.9565217391304348, 0.32409972299168976, 0.31, 0.0, 0.0, 0.08333333333333333, 0.740060670876295, 0.7516816664078277, 0.0, 1.0, 0.2, 0.33591196205052865], 
reward next is 0.6641, 
noisyNet noise sample is [array([1.3436863], dtype=float32), -1.5257298]. 
=============================================
[2019-04-09 14:57:43,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0054892  0.17131491 0.18926667 0.05286429 0.03410216 0.01522768
 0.11764574 0.0487147  0.11510258 0.05911358 0.19115849], sum to 1.0000
[2019-04-09 14:57:43,379] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-09 14:57:43,398] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 22.5, 27.16735306890712, 0.9827181603725323, 1.0, 1.0, 65.0, 0.5847829566147695], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4110000.0000, 
sim time next is 4110600.0000, 
raw observation next is [3.0, 30.66666666666667, 112.6666666666667, 818.0, 22.5, 28.29981761845173, 1.056870291691966, 1.0, 1.0, 25.0, 4.132812125490919], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3066666666666667, 0.37555555555555564, 0.9038674033149171, 0.375, 0.8583181348709775, 0.8522900972306552, 1.0, 1.0, 0.2, 0.04132812125490919], 
reward next is 0.9587, 
noisyNet noise sample is [array([-0.70077926], dtype=float32), -1.6062988]. 
=============================================
[2019-04-09 14:57:43,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00435157 0.12869987 0.12854095 0.03966517 0.02622615 0.010981
 0.13930716 0.04339031 0.09069906 0.05808819 0.3300506 ], sum to 1.0000
[2019-04-09 14:57:43,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7286
[2019-04-09 14:57:43,567] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 30.16666666666667, 0.0, 0.0, 22.5, 27.27525507641984, 0.8627313588434147, 1.0, 1.0, 60.0, 33.91011860883781], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4042200.0000, 
sim time next is 4042800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.27389961048527, 0.8619136803899012, 1.0, 1.0, 65.0, 31.42560393628038], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.375, 0.7728249675404392, 0.7873045601299671, 1.0, 1.0, 1.0, 0.3142560393628038], 
reward next is 0.6857, 
noisyNet noise sample is [array([1.6539081], dtype=float32), 1.8403877]. 
=============================================
[2019-04-09 14:57:43,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00585126 0.11899287 0.17293046 0.04395023 0.02475324 0.01145415
 0.14397836 0.06623928 0.101468   0.07011065 0.24027148], sum to 1.0000
[2019-04-09 14:57:43,806] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4793
[2019-04-09 14:57:43,822] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 93.66666666666666, 609.0, 22.5, 28.74713214966296, 1.155740284220313, 1.0, 1.0, 60.0, 4.635056825434949], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4117800.0000, 
sim time next is 4118400.0000, 
raw observation next is [4.0, 35.0, 93.5, 566.0, 22.5, 28.77837346459931, 1.1623031422999, 1.0, 1.0, 45.0, 5.457231019369669], 
processed observation next is [1.0, 0.6956521739130435, 0.5734072022160666, 0.35, 0.31166666666666665, 0.625414364640884, 0.375, 0.898197788716609, 0.8874343807666333, 1.0, 1.0, 0.6, 0.05457231019369669], 
reward next is 0.9454, 
noisyNet noise sample is [array([0.19153576], dtype=float32), -0.24055043]. 
=============================================
[2019-04-09 14:57:43,828] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01267725 0.09510089 0.12657145 0.04128342 0.02360747 0.01639871
 0.18157287 0.05856045 0.081692   0.0885504  0.27398515], sum to 1.0000
[2019-04-09 14:57:43,846] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-09 14:57:43,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 39.66666666666666, 0.0, 0.0, 19.0, 26.3793252947527, 0.5985837473682588, 0.0, 1.0, 20.0, 32.55371556982546], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4063200.0000, 
sim time next is 4063800.0000, 
raw observation next is [-6.0, 40.33333333333334, 0.0, 0.0, 19.0, 26.30717269691839, 0.601509353262805, 0.0, 1.0, 65.0, 60.41039290507959], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.40333333333333343, 0.0, 0.0, 0.08333333333333333, 0.6922643914098657, 0.7005031177542683, 0.0, 1.0, 1.0, 0.6041039290507959], 
reward next is 0.3959, 
noisyNet noise sample is [array([-0.00617289], dtype=float32), -0.8689093]. 
=============================================
[2019-04-09 14:57:44,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01104868 0.13740635 0.12296172 0.06319492 0.03601326 0.01929068
 0.14055485 0.06068377 0.10523571 0.08775433 0.21585572], sum to 1.0000
[2019-04-09 14:57:44,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7407
[2019-04-09 14:57:44,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 38.0, 98.0, 574.0, 22.5, 27.11778181328478, 0.7209857455024564, 1.0, 1.0, 20.0, 29.42120521977612], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4093200.0000, 
sim time next is 4093800.0000, 
raw observation next is [-2.833333333333333, 37.5, 100.0, 609.0, 22.5, 27.13864462837631, 0.7303213973858321, 1.0, 1.0, 60.0, 29.65996054391218], 
processed observation next is [1.0, 0.391304347826087, 0.3841181902123731, 0.375, 0.3333333333333333, 0.6729281767955801, 0.375, 0.761553719031359, 0.7434404657952774, 1.0, 1.0, 0.9, 0.2965996054391218], 
reward next is 0.7034, 
noisyNet noise sample is [array([-1.5130966], dtype=float32), -0.33200553]. 
=============================================
[2019-04-09 14:57:44,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01768475 0.09901289 0.12552756 0.0691197  0.03080798 0.02413087
 0.18027113 0.07252244 0.08998442 0.07278503 0.21815324], sum to 1.0000
[2019-04-09 14:57:44,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-09 14:57:44,296] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 19.0, 25.81492720417489, 0.4923143229954479, 0.0, 1.0, 45.0, 47.84616177006752], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4077000.0000, 
sim time next is 4077600.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 19.0, 25.8318609274222, 0.4836846963673999, 0.0, 1.0, 25.0, 32.89789509285328], 
processed observation next is [1.0, 0.17391304347826086, 0.3425669436749769, 0.3633333333333333, 0.0, 0.0, 0.08333333333333333, 0.6526550772851832, 0.6612282321224666, 0.0, 1.0, 0.2, 0.3289789509285328], 
reward next is 0.6710, 
noisyNet noise sample is [array([3.033256], dtype=float32), 0.48697072]. 
=============================================
[2019-04-09 14:57:44,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00971615 0.07648575 0.12163024 0.03455848 0.0326948  0.02127236
 0.1914964  0.07217483 0.06179743 0.09318585 0.28498772], sum to 1.0000
[2019-04-09 14:57:44,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0727
[2019-04-09 14:57:44,567] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 39.0, 0.0, 0.0, 19.0, 26.10022668330941, 0.5753062463313391, 0.0, 1.0, 45.0, 36.2894300502092], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4062600.0000, 
sim time next is 4063200.0000, 
raw observation next is [-6.0, 39.66666666666666, 0.0, 0.0, 19.0, 26.08043907116546, 0.5576367416773332, 0.0, 1.0, 20.0, 28.99224079827329], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39666666666666656, 0.0, 0.0, 0.08333333333333333, 0.6733699225971218, 0.6858789138924445, 0.0, 1.0, 0.1, 0.2899224079827329], 
reward next is 0.7101, 
noisyNet noise sample is [array([1.0065596], dtype=float32), 0.25440404]. 
=============================================
[2019-04-09 14:57:44,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01374576 0.09073852 0.11176763 0.06049649 0.03563977 0.02429345
 0.16831501 0.0747345  0.10463316 0.08557885 0.23005682], sum to 1.0000
[2019-04-09 14:57:44,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9034
[2019-04-09 14:57:44,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 22.5, 25.69336892257357, 0.4143327789388003, 1.0, 1.0, 60.0, 54.66464556921564], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4087800.0000, 
sim time next is 4088400.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 22.5, 25.61116423892099, 0.4444025054685146, 1.0, 1.0, 65.0, 72.92527653493684], 
processed observation next is [1.0, 0.30434782608695654, 0.3425669436749769, 0.3633333333333333, 0.0511111111111111, 0.08637200736648248, 0.375, 0.634263686576749, 0.6481341684895049, 1.0, 1.0, 1.0, 0.7292527653493684], 
reward next is 0.2707, 
noisyNet noise sample is [array([0.03258858], dtype=float32), -0.5669721]. 
=============================================
[2019-04-09 14:57:45,055] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0060069  0.15181822 0.11051499 0.05440015 0.02706829 0.01429544
 0.11031316 0.06746362 0.13603106 0.06938768 0.25270045], sum to 1.0000
[2019-04-09 14:57:45,067] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0112
[2019-04-09 14:57:45,083] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.833333333333333, 35.33333333333334, 93.33333333333333, 523.0, 22.5, 28.82477715365002, 1.168390206377966, 1.0, 1.0, 20.0, 5.466997812012985], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 22.5, 28.87659289959818, 0.966888115626939, 1.0, 1.0, 20.0, 50.2867693629753], 
processed observation next is [1.0, 0.6956521739130435, 0.564173591874423, 0.3566666666666667, 0.31055555555555553, 0.5303867403314917, 0.375, 0.9063827416331817, 0.822296038542313, 1.0, 1.0, 0.1, 0.502867693629753], 
reward next is 0.4971, 
noisyNet noise sample is [array([-1.2308851], dtype=float32), -1.4047672]. 
=============================================
[2019-04-09 14:57:45,093] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0054546  0.16549997 0.10361227 0.06584179 0.0267575  0.01452397
 0.11062666 0.05292159 0.1425923  0.05104677 0.2611226 ], sum to 1.0000
[2019-04-09 14:57:45,098] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2071
[2019-04-09 14:57:45,158] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 22.5, 28.87659289959818, 0.966888115626939, 1.0, 1.0, 20.0, 50.2867693629753], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4119600.0000, 
sim time next is 4120200.0000, 
raw observation next is [3.5, 36.0, 93.0, 437.0, 22.5, 28.72994256109605, 1.204658887035521, 1.0, 1.0, 25.0, 11.97223037961908], 
processed observation next is [1.0, 0.6956521739130435, 0.5595567867036012, 0.36, 0.31, 0.48287292817679556, 0.375, 0.8941618800913375, 0.9015529623451736, 1.0, 1.0, 0.2, 0.11972230379619081], 
reward next is 0.8803, 
noisyNet noise sample is [array([-1.2308851], dtype=float32), -1.4047672]. 
=============================================
[2019-04-09 14:57:45,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00498365 0.1613855  0.17832585 0.05974123 0.01720656 0.01319019
 0.08936752 0.04895227 0.11482747 0.04941056 0.2626092 ], sum to 1.0000
[2019-04-09 14:57:45,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8378
[2019-04-09 14:57:45,229] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 30.0, 116.0, 830.0, 22.5, 27.49222766348769, 1.031447947366517, 1.0, 1.0, 60.0, 15.89737132337612], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 22.5, 27.18837201591569, 0.9908900798837773, 1.0, 1.0, 65.0, 0.517009361710255], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3033333333333333, 0.381111111111111, 0.9104972375690608, 0.375, 0.7656976679929741, 0.8302966932945924, 1.0, 1.0, 1.0, 0.00517009361710255], 
reward next is 0.9948, 
noisyNet noise sample is [array([1.2717091], dtype=float32), 0.014182454]. 
=============================================
[2019-04-09 14:57:45,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[4.342181 ]
 [4.097247 ]
 [4.1847982]
 [4.1684337]
 [4.166896 ]], R is [[5.07296276]
 [5.86325932]
 [6.23943281]
 [6.96841431]
 [7.77225351]].
[2019-04-09 14:57:45,435] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00537248 0.11717723 0.12966964 0.05924971 0.02742463 0.01241122
 0.16786154 0.05857975 0.12863973 0.04650393 0.24711007], sum to 1.0000
[2019-04-09 14:57:45,441] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0295
[2019-04-09 14:57:45,471] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 22.5, 27.79810544034905, 0.9679737637378398, 1.0, 1.0, 65.0, 17.0566021718841], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [1.0, 28.0, 120.5, 828.5, 22.5, 27.49202195721379, 0.9320826333007625, 1.0, 1.0, 20.0, 9.545585219369471], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.28, 0.40166666666666667, 0.9154696132596685, 0.375, 0.7910018297678159, 0.8106942111002541, 1.0, 1.0, 0.1, 0.0954558521936947], 
reward next is 0.9045, 
noisyNet noise sample is [array([-0.14665836], dtype=float32), -0.082396045]. 
=============================================
[2019-04-09 14:57:45,499] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[4.1481543]
 [4.291967 ]
 [4.058824 ]
 [4.121735 ]
 [4.0295672]], R is [[4.98333311]
 [5.76293373]
 [6.27602482]
 [7.12238026]
 [7.84907246]].
[2019-04-09 14:57:45,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01211427 0.09354863 0.12407354 0.06821633 0.03397143 0.02170659
 0.1823402  0.05767625 0.07926403 0.07099587 0.2560929 ], sum to 1.0000
[2019-04-09 14:57:45,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-09 14:57:45,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00516612 0.15205681 0.13865875 0.06115421 0.02355697 0.01678071
 0.16087918 0.05573884 0.09828945 0.0627897  0.22492926], sum to 1.0000
[2019-04-09 14:57:45,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4323
[2019-04-09 14:57:45,563] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.333333333333333, 39.0, 0.0, 0.0, 19.0, 26.38806855280736, 0.5840213118903664, 0.0, 1.0, 65.0, 49.90518801336223], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4070400.0000, 
sim time next is 4071000.0000, 
raw observation next is [-5.166666666666667, 38.5, 0.0, 0.0, 19.0, 26.41937161988061, 0.5841202375624216, 0.0, 1.0, 30.0, 42.83248767545341], 
processed observation next is [1.0, 0.08695652173913043, 0.31948291782086796, 0.385, 0.0, 0.0, 0.08333333333333333, 0.7016143016567176, 0.6947067458541406, 0.0, 1.0, 0.3, 0.42832487675453407], 
reward next is 0.5717, 
noisyNet noise sample is [array([1.4298301], dtype=float32), -0.9972881]. 
=============================================
[2019-04-09 14:57:45,566] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 33.5, 114.0, 769.0, 22.5, 27.64302213275544, 0.8535789945643041, 1.0, 1.0, 65.0, 22.17253186865764], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4098600.0000, 
sim time next is 4099200.0000, 
raw observation next is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 22.5, 27.68035265913092, 0.8669339102807937, 1.0, 1.0, 45.0, 18.09322050854005], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.33, 0.383888888888889, 0.8583793738489871, 0.375, 0.8066960549275768, 0.7889779700935979, 1.0, 1.0, 0.6, 0.1809322050854005], 
reward next is 0.8191, 
noisyNet noise sample is [array([-1.0850226], dtype=float32), 0.81298256]. 
=============================================
[2019-04-09 14:57:45,577] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[2.8517797]
 [3.0835702]
 [2.844405 ]
 [2.846057 ]
 [2.9773366]], R is [[3.41194654]
 [3.87877536]
 [4.30880451]
 [4.80478716]
 [5.15946865]].
[2019-04-09 14:57:45,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00539841 0.14980318 0.16792807 0.04885135 0.01987088 0.01461725
 0.10501759 0.05605518 0.08079993 0.06967098 0.2819872 ], sum to 1.0000
[2019-04-09 14:57:45,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2208
[2019-04-09 14:57:45,636] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.0, 35.0, 93.5, 566.0, 22.5, 28.79744871386369, 1.166941892352354, 1.0, 1.0, 20.0, 3.094415217489679], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4118400.0000, 
sim time next is 4119000.0000, 
raw observation next is [3.833333333333333, 35.33333333333334, 93.33333333333333, 523.0, 22.5, 28.83640749109975, 1.170527511285765, 1.0, 1.0, 25.0, 2.711692196715538], 
processed observation next is [1.0, 0.6956521739130435, 0.5687903970452447, 0.35333333333333344, 0.3111111111111111, 0.5779005524861879, 0.375, 0.9030339575916457, 0.8901758370952549, 1.0, 1.0, 0.2, 0.02711692196715538], 
reward next is 0.9729, 
noisyNet noise sample is [array([0.50163805], dtype=float32), -1.3589131]. 
=============================================
[2019-04-09 14:57:45,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.292948 ]
 [4.2301264]
 [4.2484097]
 [4.3827806]
 [4.3551683]], R is [[5.21408367]
 [6.13099861]
 [7.0455389 ]
 [7.95525789]
 [8.86070728]].
[2019-04-09 14:57:45,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0037088  0.13196562 0.18728773 0.05240412 0.03145659 0.01167464
 0.12051957 0.06328989 0.1347101  0.05283246 0.21015047], sum to 1.0000
[2019-04-09 14:57:45,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9675
[2019-04-09 14:57:45,971] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 22.5, 27.99137883206296, 0.7210650503111887, 1.0, 1.0, 55.0, 41.60531826399183], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4104600.0000, 
sim time next is 4105200.0000, 
raw observation next is [1.666666666666667, 28.33333333333334, 120.1666666666667, 836.8333333333334, 22.5, 27.86438123705997, 0.9826751322095714, 1.0, 1.0, 65.0, 21.54770634046524], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 0.2833333333333334, 0.40055555555555566, 0.9246777163904236, 0.375, 0.8220317697549975, 0.8275583774031904, 1.0, 1.0, 1.0, 0.2154770634046524], 
reward next is 0.7845, 
noisyNet noise sample is [array([1.3293854], dtype=float32), -1.7932197]. 
=============================================
[2019-04-09 14:57:46,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00476462 0.11153268 0.17870803 0.04371872 0.0277703  0.01641745
 0.15306242 0.06722127 0.09655575 0.03607599 0.2641728 ], sum to 1.0000
[2019-04-09 14:57:46,140] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2051
[2019-04-09 14:57:46,153] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.666666666666667, 28.83333333333334, 118.6666666666667, 837.3333333333334, 22.5, 27.9937633914607, 0.9781399073533489, 1.0, 1.0, 65.0, 16.17768065464725], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4107000.0000, 
sim time next is 4107600.0000, 
raw observation next is [3.0, 29.0, 118.0, 835.5, 22.5, 28.04247185363704, 0.9787753640625771, 1.0, 1.0, 25.0, 19.0368643067203], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.29, 0.3933333333333333, 0.9232044198895027, 0.375, 0.8368726544697532, 0.8262584546875257, 1.0, 1.0, 0.2, 0.190368643067203], 
reward next is 0.8096, 
noisyNet noise sample is [array([-0.11474078], dtype=float32), -1.8468859]. 
=============================================
[2019-04-09 14:57:46,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00506444 0.12802751 0.14759429 0.05162778 0.0255028  0.01311209
 0.1522457  0.056954   0.12436488 0.04053799 0.25496846], sum to 1.0000
[2019-04-09 14:57:46,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9379
[2019-04-09 14:57:46,463] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.833333333333333, 35.33333333333334, 93.33333333333333, 523.0, 22.5, 28.42557533271349, 1.148873325225226, 1.0, 1.0, 55.0, 1.283651691646931], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 22.5, 28.77623234099453, 1.161229507430395, 1.0, 1.0, 65.0, 4.671783372057293], 
processed observation next is [1.0, 0.6956521739130435, 0.564173591874423, 0.3566666666666667, 0.31055555555555553, 0.5303867403314917, 0.375, 0.8980193617495441, 0.8870765024767984, 1.0, 1.0, 1.0, 0.04671783372057293], 
reward next is 0.9533, 
noisyNet noise sample is [array([-1.6744298], dtype=float32), -0.2881111]. 
=============================================
[2019-04-09 14:57:46,777] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-09 14:57:46,778] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:57:46,778] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:46,780] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run12
[2019-04-09 14:57:46,795] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:57:46,796] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:46,798] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:57:46,798] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:46,801] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run12
[2019-04-09 14:57:46,815] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run12
[2019-04-09 14:58:16,548] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01584527], dtype=float32), 0.02085463]
[2019-04-09 14:58:16,548] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [15.341589955, 100.0, 0.0, 0.0, 19.0, 28.00956965683738, 1.23653595527427, 0.0, 1.0, 55.0, 21.38643966097077]
[2019-04-09 14:58:16,548] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:58:16,549] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.01629762 0.12481099 0.11091525 0.07407953 0.05377754 0.02864533
 0.16551828 0.06010504 0.09654414 0.08810201 0.18120429], sampled 0.30713514358699534
[2019-04-09 14:58:19,194] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.01584527], dtype=float32), 0.02085463]
[2019-04-09 14:58:19,194] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [13.4, 84.0, 202.5, 265.5, 22.5, 28.07407989424988, 1.046518347367091, 1.0, 1.0, 20.0, 18.08364532384179]
[2019-04-09 14:58:19,194] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:58:19,195] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00716633 0.17601603 0.11600315 0.07818527 0.0392025  0.01246702
 0.13779843 0.05224699 0.12469596 0.07534403 0.18087426], sampled 0.16781389173761474
[2019-04-09 14:58:21,156] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01584527], dtype=float32), 0.02085463]
[2019-04-09 14:58:21,156] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [8.003725744, 62.07442578333333, 0.0, 0.0, 22.5, 28.24929472431001, 1.136682567397156, 1.0, 1.0, 30.0, 14.88051498225011]
[2019-04-09 14:58:21,156] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 14:58:21,157] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00703664 0.16060114 0.11933879 0.08286686 0.04114072 0.01479046
 0.14693053 0.04947245 0.12167702 0.06449745 0.191648  ], sampled 0.7784708380900413
[2019-04-09 14:59:25,140] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.01584527], dtype=float32), 0.02085463]
[2019-04-09 14:59:25,141] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [18.15, 54.66666666666667, 144.3333333333333, 540.3333333333334, 22.5, 29.57527821767984, 1.642520037546324, 1.0, 0.0, 45.0, 0.3149329932403365]
[2019-04-09 14:59:25,142] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:59:25,143] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.0114873  0.16672422 0.12553261 0.08646844 0.0462016  0.02039305
 0.13218777 0.05955619 0.12089955 0.06919887 0.16135043], sampled 0.01051612281307035
[2019-04-09 14:59:30,332] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5663.7532 283075.8334 2832.1178
[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,353] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:30,466] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,335] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5423.4615 307276.2894 2433.0137
[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,354] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:36,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,089] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5329.5772 316642.2805 2078.5614
[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,112] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:37,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:59:38,114] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 110000, evaluation results [110000.0, 5423.461528667647, 307276.2894111063, 2433.013736859391, 5663.753227389465, 283075.8334451683, 2832.117799781444, 5329.577194993351, 316642.2805006637, 2078.561364644855]
[2019-04-09 14:59:38,249] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01397811 0.11071607 0.1223924  0.04595104 0.03167131 0.01910995
 0.18957359 0.07274099 0.10703284 0.07305941 0.2137744 ], sum to 1.0000
[2019-04-09 14:59:38,258] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5875
[2019-04-09 14:59:38,276] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 39.0, 0.0, 0.0, 19.0, 26.27909758353043, 0.568400100598825, 0.0, 1.0, 35.0, 45.59295902112291], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4072800.0000, 
sim time next is 4073400.0000, 
raw observation next is [-5.0, 39.5, 0.0, 0.0, 19.0, 26.29061651467806, 0.567656759369393, 0.0, 1.0, 50.0, 40.41160744772693], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.395, 0.0, 0.0, 0.08333333333333333, 0.690884709556505, 0.6892189197897977, 0.0, 1.0, 0.7, 0.40411607447726927], 
reward next is 0.5959, 
noisyNet noise sample is [array([-0.32797852], dtype=float32), -0.4080706]. 
=============================================
[2019-04-09 14:59:38,315] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00892514 0.10994724 0.12043761 0.0454491  0.0404645  0.01095321
 0.15692638 0.08328465 0.11691952 0.07230617 0.23438643], sum to 1.0000
[2019-04-09 14:59:38,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0588
[2019-04-09 14:59:38,327] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 19.0, 27.37397294688081, 0.9459525203948612, 0.0, 1.0, 45.0, 22.8521751840615], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4138800.0000, 
sim time next is 4139400.0000, 
raw observation next is [1.0, 39.33333333333334, 0.0, 0.0, 19.0, 27.40041387871088, 0.9421519467951477, 0.0, 1.0, 20.0, 25.2402245425663], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7833678232259066, 0.8140506489317159, 0.0, 1.0, 0.1, 0.252402245425663], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.5415571], dtype=float32), -0.2806808]. 
=============================================
[2019-04-09 14:59:38,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00320558 0.12948644 0.1218835  0.04011097 0.02417329 0.01220398
 0.15749435 0.05259073 0.09983177 0.07598539 0.28303397], sum to 1.0000
[2019-04-09 14:59:38,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5626
[2019-04-09 14:59:38,551] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 34.5, 15.33333333333334, 38.00000000000001, 22.5, 28.26267547618411, 1.106556014137253, 1.0, 1.0, 65.0, 8.82337895290156], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4125000.0000, 
sim time next is 4125600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 22.5, 28.45724995394765, 1.100267384868938, 1.0, 1.0, 55.0, 8.02339806815323], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.34, 0.0, 0.0, 0.375, 0.871437496162304, 0.8667557949563127, 1.0, 1.0, 0.8, 0.08023398068153231], 
reward next is 0.9198, 
noisyNet noise sample is [array([0.0530786], dtype=float32), -0.5740928]. 
=============================================
[2019-04-09 14:59:38,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00700025 0.11087008 0.11467259 0.04048469 0.02917684 0.01585259
 0.18284374 0.05570148 0.07331318 0.06959698 0.3004876 ], sum to 1.0000
[2019-04-09 14:59:38,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-09 14:59:38,787] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 19.0, 27.36497398203602, 0.8930436023305593, 0.0, 1.0, 65.0, 36.21085961739635], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4140600.0000, 
sim time next is 4141200.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 19.0, 27.34723404441282, 0.8930009316120907, 0.0, 1.0, 60.0, 33.20312427345169], 
processed observation next is [1.0, 0.9565217391304348, 0.4810710987996307, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7789361703677349, 0.7976669772040302, 0.0, 1.0, 0.9, 0.3320312427345169], 
reward next is 0.6680, 
noisyNet noise sample is [array([-0.0054769], dtype=float32), -1.0419074]. 
=============================================
[2019-04-09 14:59:38,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00790182 0.10545409 0.14604989 0.04038338 0.02692117 0.0159143
 0.19128174 0.06268556 0.08839985 0.08351138 0.23149689], sum to 1.0000
[2019-04-09 14:59:38,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9055
[2019-04-09 14:59:38,845] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 19.0, 27.22068737506871, 0.8721648685803727, 0.0, 1.0, 50.0, 31.90403629143723], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 19.0, 27.19141251226979, 0.8648298744914849, 0.0, 1.0, 65.0, 36.88035394631197], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.425, 0.0, 0.0, 0.08333333333333333, 0.765951042689149, 0.788276624830495, 0.0, 1.0, 1.0, 0.3688035394631197], 
reward next is 0.6312, 
noisyNet noise sample is [array([-0.8829021], dtype=float32), -0.6564877]. 
=============================================
[2019-04-09 14:59:38,849] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[3.4903321]
 [3.4057295]
 [3.4016898]
 [3.4148757]
 [3.4732132]], R is [[3.78948641]
 [4.43255138]
 [5.0925045 ]
 [5.70632553]
 [6.28913307]].
[2019-04-09 14:59:38,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00973754 0.08311629 0.12773447 0.04552814 0.03887082 0.01299197
 0.2109095  0.06386574 0.0812175  0.10205168 0.22397633], sum to 1.0000
[2019-04-09 14:59:38,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7597
[2019-04-09 14:59:38,888] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.1666666666666666, 42.5, 0.0, 0.0, 19.0, 27.19141251226979, 0.8648298744914849, 0.0, 1.0, 65.0, 36.88035394631197], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4143000.0000, 
sim time next is 4143600.0000, 
raw observation next is [0.0, 43.0, 0.0, 0.0, 19.0, 27.14463041441024, 0.8564303973625288, 0.0, 1.0, 60.0, 38.89222501595864], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.43, 0.0, 0.0, 0.08333333333333333, 0.7620525345341868, 0.7854767991208429, 0.0, 1.0, 0.9, 0.38892225015958637], 
reward next is 0.6111, 
noisyNet noise sample is [array([0.690319], dtype=float32), 0.36517242]. 
=============================================
[2019-04-09 14:59:39,136] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01037135 0.0751038  0.08946391 0.04467641 0.02979537 0.01656551
 0.204901   0.0675737  0.08889805 0.08983396 0.28281695], sum to 1.0000
[2019-04-09 14:59:39,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3882
[2019-04-09 14:59:39,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.02045747 0.11612843 0.11115507 0.08895695 0.058094   0.043324
 0.14378469 0.06749885 0.09526899 0.0914146  0.16391695], sum to 1.0000
[2019-04-09 14:59:39,164] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 40.5, 0.0, 0.0, 19.0, 26.9659898221075, 0.8095154789792883, 0.0, 1.0, 60.0, 36.85168663751966], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4149000.0000, 
sim time next is 4149600.0000, 
raw observation next is [-1.0, 40.0, 0.0, 0.0, 19.0, 26.97838120518956, 0.8059106745436143, 0.0, 1.0, 45.0, 34.30327172804721], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7481984337657966, 0.7686368915145381, 0.0, 1.0, 0.6, 0.3430327172804721], 
reward next is 0.6570, 
noisyNet noise sample is [array([0.4733793], dtype=float32), -1.2828522]. 
=============================================
[2019-04-09 14:59:39,165] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-09 14:59:39,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 39.0, 205.0, 475.6666666666667, 19.0, 27.28936129715115, 0.8960452977072578, 0.0, 1.0, 25.0, 22.47793495187521], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4197000.0000, 
sim time next is 4197600.0000, 
raw observation next is [2.0, 40.0, 200.5, 379.0, 19.0, 27.30141630873656, 0.8997582965872634, 0.0, 1.0, 40.0, 24.72530774158452], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4, 0.6683333333333333, 0.41878453038674035, 0.08333333333333333, 0.7751180257280467, 0.7999194321957545, 0.0, 1.0, 0.5, 0.24725307741584518], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.21410695], dtype=float32), -0.30843946]. 
=============================================
[2019-04-09 14:59:39,309] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 110622: loss 28.2855
[2019-04-09 14:59:39,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 110622: learning rate 0.0000
[2019-04-09 14:59:39,469] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00554995 0.08708383 0.10506822 0.03764866 0.03194309 0.00937756
 0.2517237  0.06375525 0.08105633 0.06852397 0.25826943], sum to 1.0000
[2019-04-09 14:59:39,471] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6932
[2019-04-09 14:59:39,490] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 19.0, 27.41183139809354, 0.8936514194094375, 0.0, 1.0, 20.0, 34.17415939653716], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4140600.0000, 
sim time next is 4141200.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 19.0, 27.3775941041497, 0.8922498817776248, 0.0, 1.0, 60.0, 40.86904757740828], 
processed observation next is [1.0, 0.9565217391304348, 0.4810710987996307, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7814661753458084, 0.7974166272592083, 0.0, 1.0, 0.9, 0.4086904757740828], 
reward next is 0.5913, 
noisyNet noise sample is [array([-0.6553285], dtype=float32), 0.73921597]. 
=============================================
[2019-04-09 14:59:39,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02551399 0.12002207 0.09534796 0.07837684 0.05947885 0.03539474
 0.1604326  0.05979851 0.08964252 0.07678663 0.19920531], sum to 1.0000
[2019-04-09 14:59:39,592] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7828
[2019-04-09 14:59:39,604] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.38504942950089, 0.6643128537354638, 0.0, 1.0, 20.0, 29.93829575398721], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4158600.0000, 
sim time next is 4159200.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.32468136452426, 0.6724219355077808, 0.0, 1.0, 60.0, 49.66992703385878], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.6937234470436883, 0.7241406451692604, 0.0, 1.0, 0.9, 0.49669927033858774], 
reward next is 0.5033, 
noisyNet noise sample is [array([-0.00554963], dtype=float32), -1.3684243]. 
=============================================
[2019-04-09 14:59:40,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01603465 0.13438211 0.13258626 0.07964665 0.04670431 0.03346824
 0.10063908 0.05950527 0.09624444 0.08323987 0.21754919], sum to 1.0000
[2019-04-09 14:59:40,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0206
[2019-04-09 14:59:40,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00467115 0.13891108 0.12766278 0.04840796 0.01937659 0.01364558
 0.12920721 0.0401831  0.12305629 0.05586419 0.299014  ], sum to 1.0000
[2019-04-09 14:59:40,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-09 14:59:40,107] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 40.0, 200.5, 379.0, 19.0, 27.3659036195029, 0.9116245231048375, 0.0, 1.0, 30.0, 26.43806730714044], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4197600.0000, 
sim time next is 4198200.0000, 
raw observation next is [2.0, 40.66666666666667, 196.0, 282.3333333333333, 19.0, 27.42339195263854, 0.9034220917391864, 0.0, 1.0, 20.0, 26.37247135768575], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.40666666666666673, 0.6533333333333333, 0.31197053406998154, 0.08333333333333333, 0.7852826627198782, 0.8011406972463955, 0.0, 1.0, 0.1, 0.2637247135768575], 
reward next is 0.7363, 
noisyNet noise sample is [array([-1.2223101], dtype=float32), 0.6159243]. 
=============================================
[2019-04-09 14:59:40,110] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.833333333333333, 34.33333333333334, 102.0, 761.0, 22.5, 28.54134993610103, 1.11179539031336, 1.0, 1.0, 65.0, 11.30154276033128], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4114200.0000, 
sim time next is 4114800.0000, 
raw observation next is [4.0, 35.0, 100.0, 744.5, 22.5, 28.56221465848029, 1.119625034561319, 1.0, 1.0, 20.0, 9.126483911172578], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3333333333333333, 0.8226519337016575, 0.375, 0.8801845548733574, 0.8732083448537731, 1.0, 1.0, 0.1, 0.09126483911172578], 
reward next is 0.9087, 
noisyNet noise sample is [array([-1.1953475], dtype=float32), -1.3005663]. 
=============================================
[2019-04-09 14:59:40,195] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111071: loss 28.0300
[2019-04-09 14:59:40,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111071: learning rate 0.0000
[2019-04-09 14:59:40,376] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 111158: loss 35.1204
[2019-04-09 14:59:40,379] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 111158: learning rate 0.0000
[2019-04-09 14:59:40,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02704765 0.12295216 0.09417343 0.08407079 0.07486815 0.0446251
 0.14156526 0.06062256 0.10050593 0.07865225 0.17091675], sum to 1.0000
[2019-04-09 14:59:40,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3601
[2019-04-09 14:59:40,545] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.7643107308367, 0.7270597129221871, 0.0, 1.0, 50.0, 34.20379435994094], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4159800.0000, 
sim time next is 4160400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.7552518714051, 0.7159095454450033, 0.0, 1.0, 35.0, 34.40957620114261], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7296043226170917, 0.7386365151483344, 0.0, 1.0, 0.4, 0.3440957620114261], 
reward next is 0.6559, 
noisyNet noise sample is [array([1.7494469], dtype=float32), 1.8761953]. 
=============================================
[2019-04-09 14:59:40,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02287996 0.11739014 0.11653096 0.07548885 0.05612585 0.03729777
 0.1431667  0.05658067 0.0976052  0.09208703 0.18484683], sum to 1.0000
[2019-04-09 14:59:40,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5529
[2019-04-09 14:59:40,693] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 49.5, 92.0, 488.0, 19.0, 26.50575001587281, 0.6816821124388871, 0.0, 1.0, 35.0, 39.62083562651078], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4177800.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 19.0, 26.51778047016876, 0.6901714470665983, 0.0, 1.0, 45.0, 36.33960350393566], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.08333333333333333, 0.70981503918073, 0.7300571490221994, 0.0, 1.0, 0.6, 0.36339603503935664], 
reward next is 0.6366, 
noisyNet noise sample is [array([1.7683553], dtype=float32), -1.231139]. 
=============================================
[2019-04-09 14:59:40,733] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01649616 0.13903715 0.1301511  0.07956544 0.05633004 0.0377242
 0.11950331 0.06389395 0.09028511 0.08209439 0.18491924], sum to 1.0000
[2019-04-09 14:59:40,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2423
[2019-04-09 14:59:40,750] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 37.0, 214.0, 669.0, 19.0, 27.22199686895355, 0.8863814703265436, 0.0, 1.0, 65.0, 28.9467725335555], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4195800.0000, 
sim time next is 4196400.0000, 
raw observation next is [2.0, 38.0, 209.5, 572.3333333333334, 19.0, 27.2617186392181, 0.8938027049352671, 0.0, 1.0, 25.0, 27.18327683299684], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.38, 0.6983333333333334, 0.6324125230202579, 0.08333333333333333, 0.7718098866015083, 0.7979342349784223, 0.0, 1.0, 0.2, 0.2718327683299684], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.6230132], dtype=float32), 0.64118236]. 
=============================================
[2019-04-09 14:59:40,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02002434 0.13466409 0.11055855 0.07050204 0.06979286 0.03110428
 0.12636214 0.07671173 0.08631226 0.08451217 0.18945555], sum to 1.0000
[2019-04-09 14:59:40,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3207
[2019-04-09 14:59:40,942] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 50.66666666666667, 0.0, 0.0, 19.0, 26.408777037171, 0.648784150132377, 0.0, 1.0, 60.0, 49.20080596660922], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4168200.0000, 
sim time next is 4168800.0000, 
raw observation next is [-4.0, 50.0, 0.0, 0.0, 19.0, 26.40733141513387, 0.6452350196480124, 0.0, 1.0, 30.0, 39.50726142986117], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7006109512611559, 0.7150783398826709, 0.0, 1.0, 0.3, 0.3950726142986117], 
reward next is 0.6049, 
noisyNet noise sample is [array([1.3857056], dtype=float32), -0.24053739]. 
=============================================
[2019-04-09 14:59:40,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01938893 0.1584021  0.13364135 0.08531812 0.05913938 0.02630918
 0.13907808 0.05556856 0.09290149 0.07103051 0.15922232], sum to 1.0000
[2019-04-09 14:59:40,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8461
[2019-04-09 14:59:40,958] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01213636 0.12130245 0.14520511 0.05522893 0.04896737 0.02338016
 0.16070822 0.04946322 0.08218995 0.07505226 0.22636606], sum to 1.0000
[2019-04-09 14:59:40,967] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 42.0, 187.0, 89.0, 19.0, 27.33461588524394, 0.873355678263574, 0.0, 1.0, 45.0, 31.11456806657818], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4199400.0000, 
sim time next is 4200000.0000, 
raw observation next is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 19.0, 27.30456458267816, 0.8799973235629793, 0.0, 1.0, 65.0, 31.87564705671381], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4266666666666667, 0.6083333333333333, 0.1810313075506445, 0.08333333333333333, 0.7753803818898467, 0.7933324411876598, 0.0, 1.0, 1.0, 0.3187564705671381], 
reward next is 0.6812, 
noisyNet noise sample is [array([1.5649658], dtype=float32), 0.35360909]. 
=============================================
[2019-04-09 14:59:40,969] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[2.5033612]
 [2.549965 ]
 [2.5116687]
 [2.6928918]
 [2.6199586]], R is [[3.44690323]
 [4.1012888 ]
 [4.76247025]
 [5.44550514]
 [6.12020683]].
[2019-04-09 14:59:40,969] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111450: loss 33.4272
[2019-04-09 14:59:40,970] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111450: learning rate 0.0000
[2019-04-09 14:59:40,977] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4025
[2019-04-09 14:59:41,008] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 19.0, 27.53461014364357, 0.900354065910963, 0.0, 1.0, 60.0, 26.07671757694448], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4209600.0000, 
sim time next is 4210200.0000, 
raw observation next is [1.85, 40.5, 24.0, 163.0, 19.0, 27.4800105211009, 0.8884826434886531, 0.0, 1.0, 40.0, 26.81125215673052], 
processed observation next is [0.0, 0.7391304347826086, 0.5138504155124655, 0.405, 0.08, 0.18011049723756906, 0.08333333333333333, 0.7900008767584085, 0.7961608811628844, 0.0, 1.0, 0.5, 0.2681125215673052], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.40453553], dtype=float32), 0.06278415]. 
=============================================
[2019-04-09 14:59:41,073] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111500: loss 31.0235
[2019-04-09 14:59:41,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111500: learning rate 0.0000
[2019-04-09 14:59:41,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02344522 0.12344856 0.11861657 0.08539563 0.05454274 0.03503386
 0.132663   0.06045356 0.11011859 0.08442231 0.17186001], sum to 1.0000
[2019-04-09 14:59:41,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9961
[2019-04-09 14:59:41,109] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 52.5, 61.33333333333334, 325.3333333333334, 19.0, 25.26366687714322, 0.4445215043406869, 0.0, 1.0, 50.0, 33.7751492696452], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4176600.0000, 
sim time next is 4177200.0000, 
raw observation next is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 19.0, 25.29490579876617, 0.4712501100840711, 0.0, 1.0, 65.0, 60.43066175391395], 
processed observation next is [0.0, 0.34782608695652173, 0.3333333333333333, 0.5100000000000001, 0.2555555555555556, 0.44935543278084716, 0.08333333333333333, 0.6079088165638474, 0.6570833700280237, 0.0, 1.0, 1.0, 0.6043066175391395], 
reward next is 0.3957, 
noisyNet noise sample is [array([-0.07412161], dtype=float32), 0.0660043]. 
=============================================
[2019-04-09 14:59:41,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02639554 0.12789138 0.12096595 0.08108967 0.07098404 0.04499909
 0.1564298  0.06049282 0.07100703 0.08355912 0.15618546], sum to 1.0000
[2019-04-09 14:59:41,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7215
[2019-04-09 14:59:41,226] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.30133758290471, 0.6560386086953868, 0.0, 1.0, 50.0, 46.70697868011079], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4161600.0000, 
sim time next is 4162200.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 0.0, 0.0, 19.0, 26.38008145540455, 0.6535882274580737, 0.0, 1.0, 55.0, 41.28392024954965], 
processed observation next is [0.0, 0.17391304347826086, 0.3748845798707295, 0.5066666666666667, 0.0, 0.0, 0.08333333333333333, 0.6983401212837125, 0.7178627424860246, 0.0, 1.0, 0.8, 0.41283920249549655], 
reward next is 0.5872, 
noisyNet noise sample is [array([-1.0222024], dtype=float32), -0.8441]. 
=============================================
[2019-04-09 14:59:41,348] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111638: loss 29.8774
[2019-04-09 14:59:41,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111639: learning rate 0.0000
[2019-04-09 14:59:41,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02018557 0.15396264 0.11921495 0.0768231  0.06283788 0.0328283
 0.12436996 0.06479187 0.09251143 0.09772474 0.15474956], sum to 1.0000
[2019-04-09 14:59:41,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1136
[2019-04-09 14:59:41,432] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.666666666666667, 38.33333333333334, 109.0, 679.0, 19.0, 26.1747547631912, 0.6310233761600322, 0.0, 1.0, 50.0, 31.84825643632399], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4182000.0000, 
sim time next is 4182600.0000, 
raw observation next is [-2.333333333333333, 36.66666666666666, 110.0, 698.0, 19.0, 26.19212651383926, 0.6314162787977708, 0.0, 1.0, 20.0, 29.40728470418509], 
processed observation next is [0.0, 0.391304347826087, 0.3979686057248385, 0.3666666666666666, 0.36666666666666664, 0.7712707182320442, 0.08333333333333333, 0.6826772094866049, 0.7104720929325903, 0.0, 1.0, 0.1, 0.29407284704185094], 
reward next is 0.7059, 
noisyNet noise sample is [array([0.7830245], dtype=float32), -0.8962603]. 
=============================================
[2019-04-09 14:59:41,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02055663 0.12174675 0.13479012 0.0890374  0.06229908 0.03646893
 0.1411702  0.06598416 0.09204447 0.06920934 0.16669296], sum to 1.0000
[2019-04-09 14:59:41,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-09 14:59:41,500] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 45.0, 100.0, 574.0, 19.0, 26.35080347920703, 0.6832984145720963, 0.0, 1.0, 60.0, 44.36105065955321], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4179600.0000, 
sim time next is 4180200.0000, 
raw observation next is [-3.666666666666667, 43.33333333333334, 102.6666666666667, 602.6666666666667, 19.0, 26.43424287371632, 0.694710412756954, 0.0, 1.0, 20.0, 34.1751356786559], 
processed observation next is [0.0, 0.391304347826087, 0.3610341643582641, 0.4333333333333334, 0.3422222222222223, 0.6659300184162064, 0.08333333333333333, 0.7028535728096935, 0.7315701375856514, 0.0, 1.0, 0.1, 0.341751356786559], 
reward next is 0.6582, 
noisyNet noise sample is [array([-0.20755094], dtype=float32), 0.48385295]. 
=============================================
[2019-04-09 14:59:41,598] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111780: loss 30.4344
[2019-04-09 14:59:41,609] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111782: learning rate 0.0000
[2019-04-09 14:59:41,765] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111861: loss 30.9385
[2019-04-09 14:59:41,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111862: learning rate 0.0000
[2019-04-09 14:59:41,946] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 111955: loss 30.9926
[2019-04-09 14:59:41,949] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 111955: learning rate 0.0000
[2019-04-09 14:59:42,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01368146 0.13024056 0.12473102 0.07014707 0.04896588 0.02625181
 0.17742871 0.04733093 0.09397601 0.08640521 0.18084128], sum to 1.0000
[2019-04-09 14:59:42,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0849
[2019-04-09 14:59:42,242] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.85, 40.5, 24.0, 163.0, 19.0, 27.55025299923096, 0.9136967054582247, 0.0, 1.0, 40.0, 26.11800611426875], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4210200.0000, 
sim time next is 4210800.0000, 
raw observation next is [1.8, 40.66666666666667, 20.0, 135.8333333333333, 19.0, 27.50798303431969, 0.9051011946271591, 0.0, 1.0, 20.0, 26.91321310549601], 
processed observation next is [0.0, 0.7391304347826086, 0.5124653739612189, 0.40666666666666673, 0.06666666666666667, 0.1500920810313075, 0.08333333333333333, 0.7923319195266408, 0.801700398209053, 0.0, 1.0, 0.1, 0.2691321310549601], 
reward next is 0.7309, 
noisyNet noise sample is [array([0.15703161], dtype=float32), -0.71801466]. 
=============================================
[2019-04-09 14:59:42,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01406027 0.11552306 0.13604136 0.05904716 0.04348503 0.02718177
 0.17876469 0.05072388 0.08506159 0.09803436 0.19207689], sum to 1.0000
[2019-04-09 14:59:42,270] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-09 14:59:42,282] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.75, 40.83333333333334, 16.0, 108.6666666666667, 19.0, 27.4720661529307, 0.9018461735900282, 0.0, 1.0, 50.0, 27.31029476509725], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [1.7, 41.0, 0.0, 0.0, 19.0, 27.47625593172767, 0.887664107589309, 0.0, 1.0, 45.0, 26.19531070182884], 
processed observation next is [0.0, 0.782608695652174, 0.5096952908587258, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7896879943106393, 0.795888035863103, 0.0, 1.0, 0.6, 0.2619531070182884], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.15703161], dtype=float32), -0.71801466]. 
=============================================
[2019-04-09 14:59:42,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[2.6900537]
 [2.6200974]
 [2.5911798]
 [2.748508 ]
 [2.7348838]], R is [[3.31743383]
 [4.01115656]
 [4.70191288]
 [5.39371395]
 [6.0868969 ]].
[2019-04-09 14:59:42,345] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112163: loss 28.2433
[2019-04-09 14:59:42,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112163: learning rate 0.0000
[2019-04-09 14:59:42,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01509664 0.10842064 0.10763039 0.06715638 0.04563548 0.02207776
 0.17387225 0.05991591 0.07486057 0.09398872 0.23134534], sum to 1.0000
[2019-04-09 14:59:42,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5676
[2019-04-09 14:59:42,422] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 27.05668868666232, 0.7565009290442809, 0.0, 1.0, 25.0, 31.15164853821467], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4229400.0000, 
sim time next is 4230000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 19.0, 27.05507144021534, 0.7547711370543136, 0.0, 1.0, 50.0, 34.79126128978505], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7545892866846117, 0.7515903790181045, 0.0, 1.0, 0.7, 0.34791261289785047], 
reward next is 0.6521, 
noisyNet noise sample is [array([1.0459577], dtype=float32), -0.80593324]. 
=============================================
[2019-04-09 14:59:42,428] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[2.627711 ]
 [2.4666672]
 [2.5885565]
 [2.5117474]
 [2.592641 ]], R is [[3.18649626]
 [3.84311485]
 [4.42760468]
 [4.88019609]
 [5.49220037]].
[2019-04-09 14:59:42,485] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01194682 0.12137575 0.12270483 0.06239891 0.06069537 0.02013414
 0.2147055  0.05725507 0.07628697 0.07603557 0.1764611 ], sum to 1.0000
[2019-04-09 14:59:42,485] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1074
[2019-04-09 14:59:42,499] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 27.06707910389375, 0.7640075476621456, 0.0, 1.0, 65.0, 46.39465777565754], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4227600.0000, 
sim time next is 4228200.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 19.0, 27.06768419295268, 0.7655241351035807, 0.0, 1.0, 20.0, 39.13249958850066], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7556403494127233, 0.7551747117011937, 0.0, 1.0, 0.1, 0.3913249958850066], 
reward next is 0.6087, 
noisyNet noise sample is [array([0.60397184], dtype=float32), 0.83565086]. 
=============================================
[2019-04-09 14:59:42,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01613697 0.13287316 0.14216207 0.07830129 0.04276059 0.02374341
 0.13226156 0.07088584 0.08967056 0.07347686 0.19772765], sum to 1.0000
[2019-04-09 14:59:42,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0694
[2019-04-09 14:59:42,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 34.0, 166.0, 758.0, 19.0, 27.20946392250097, 0.8844847360411024, 0.0, 1.0, 55.0, 24.04060628253264], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4194000.0000, 
sim time next is 4194600.0000, 
raw observation next is [2.0, 35.0, 182.0, 728.3333333333333, 19.0, 27.24419266756522, 0.8918008912359775, 0.0, 1.0, 20.0, 24.33693978166136], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.35, 0.6066666666666667, 0.8047882136279926, 0.08333333333333333, 0.7703493889637683, 0.7972669637453258, 0.0, 1.0, 0.1, 0.2433693978166136], 
reward next is 0.7566, 
noisyNet noise sample is [array([0.36368984], dtype=float32), -0.037659775]. 
=============================================
[2019-04-09 14:59:42,525] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112268: loss 31.0274
[2019-04-09 14:59:42,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112268: learning rate 0.0000
[2019-04-09 14:59:42,817] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112410: loss 29.0347
[2019-04-09 14:59:42,818] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01962965 0.12500258 0.09820086 0.07437704 0.05765277 0.03004746
 0.11917338 0.06569595 0.1067002  0.09279595 0.21072415], sum to 1.0000
[2019-04-09 14:59:42,819] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112410: learning rate 0.0000
[2019-04-09 14:59:42,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-09 14:59:42,835] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 19.0, 26.66722257812877, 0.7500840170392342, 0.0, 1.0, 30.0, 32.77840482076106], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [-1.5, 35.0, 114.0, 774.0, 19.0, 26.75385063480313, 0.7593155135708116, 0.0, 1.0, 60.0, 36.00408177503838], 
processed observation next is [0.0, 0.43478260869565216, 0.4210526315789474, 0.35, 0.38, 0.8552486187845304, 0.08333333333333333, 0.7294875529002608, 0.7531051711902705, 0.0, 1.0, 0.9, 0.36004081775038377], 
reward next is 0.6400, 
noisyNet noise sample is [array([0.62346053], dtype=float32), -2.128638]. 
=============================================
[2019-04-09 14:59:42,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01494991 0.12658864 0.1138982  0.06788027 0.05066103 0.02790402
 0.19042927 0.06073425 0.11193006 0.07209606 0.16292837], sum to 1.0000
[2019-04-09 14:59:42,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-09 14:59:42,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[2.3886085]
 [2.1463249]
 [2.3479612]
 [2.2206125]
 [2.1460419]], R is [[2.76133561]
 [3.40593815]
 [4.01779652]
 [4.60182858]
 [5.10856438]].
[2019-04-09 14:59:42,854] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 53.83333333333333, 189.3333333333333, 288.6666666666666, 19.0, 27.00994546550961, 0.7710402632404098, 0.0, 1.0, 55.0, 38.41579205884835], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4269000.0000, 
sim time next is 4269600.0000, 
raw observation next is [4.0, 54.0, 193.0, 367.5, 19.0, 27.10195984880268, 0.7790808404170386, 0.0, 1.0, 45.0, 32.12978529492338], 
processed observation next is [0.0, 0.43478260869565216, 0.5734072022160666, 0.54, 0.6433333333333333, 0.40607734806629836, 0.08333333333333333, 0.7584966540668899, 0.7596936134723462, 0.0, 1.0, 0.6, 0.3212978529492338], 
reward next is 0.6787, 
noisyNet noise sample is [array([-0.60417765], dtype=float32), 0.7760817]. 
=============================================
[2019-04-09 14:59:42,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01309034 0.10937338 0.10434964 0.07834033 0.04954217 0.0246249
 0.18924023 0.04790127 0.0835176  0.08351351 0.21650663], sum to 1.0000
[2019-04-09 14:59:42,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7818
[2019-04-09 14:59:42,877] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 26.99982918080991, 0.7342933831019093, 0.0, 1.0, 65.0, 54.76202664987978], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 26.9928743235621, 0.7339139742435954, 0.0, 1.0, 60.0, 41.14411661091587], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7494061936301749, 0.7446379914145318, 0.0, 1.0, 0.9, 0.4114411661091587], 
reward next is 0.5886, 
noisyNet noise sample is [array([1.6096058], dtype=float32), -0.3979548]. 
=============================================
[2019-04-09 14:59:43,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02082495 0.08526721 0.11351435 0.06616014 0.04787503 0.02770016
 0.19379023 0.05929482 0.08210097 0.08011586 0.22335626], sum to 1.0000
[2019-04-09 14:59:43,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2461
[2019-04-09 14:59:43,054] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 47.0, 0.0, 0.0, 19.0, 26.9692098265493, 0.7224043415200191, 0.0, 1.0, 35.0, 34.48159223774577], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [2.5, 46.5, 0.0, 0.0, 19.0, 26.9657241222923, 0.7210806497660153, 0.0, 1.0, 30.0, 36.12853879212846], 
processed observation next is [0.0, 0.043478260869565216, 0.5318559556786704, 0.465, 0.0, 0.0, 0.08333333333333333, 0.7471436768576917, 0.7403602165886718, 0.0, 1.0, 0.3, 0.3612853879212846], 
reward next is 0.6387, 
noisyNet noise sample is [array([1.3479636], dtype=float32), 0.03613684]. 
=============================================
[2019-04-09 14:59:43,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[2.3945684]
 [2.4821053]
 [2.5706995]
 [2.572506 ]
 [2.7206144]], R is [[2.96102118]
 [3.58659506]
 [4.16346169]
 [4.70682764]
 [5.27527714]].
[2019-04-09 14:59:43,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02313213 0.1087205  0.11306826 0.0761751  0.06251416 0.02976899
 0.1727016  0.051206   0.08055011 0.07230271 0.2098603 ], sum to 1.0000
[2019-04-09 14:59:43,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9845
[2019-04-09 14:59:43,179] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112597: loss 26.3817
[2019-04-09 14:59:43,181] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112599: learning rate 0.0000
[2019-04-09 14:59:43,191] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.96401810668588, 0.7177922461030318, 0.0, 1.0, 20.0, 40.62712734890236], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4240800.0000, 
sim time next is 4241400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.96469838820443, 0.7164617429899764, 0.0, 1.0, 65.0, 41.40155946123725], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7470581990170357, 0.7388205809966588, 0.0, 1.0, 1.0, 0.4140155946123725], 
reward next is 0.5860, 
noisyNet noise sample is [array([-1.0751852], dtype=float32), -1.5925362]. 
=============================================
[2019-04-09 14:59:43,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02169421 0.13414538 0.11213506 0.08756527 0.06390949 0.03044756
 0.14497556 0.07301723 0.09991935 0.07484347 0.15734746], sum to 1.0000
[2019-04-09 14:59:43,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0802
[2019-04-09 14:59:43,264] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.833333333333334, 49.16666666666667, 0.0, 0.0, 19.0, 26.38622876101185, 0.6257764961093539, 0.0, 1.0, 50.0, 37.02464083146127], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4171800.0000, 
sim time next is 4172400.0000, 
raw observation next is [-5.0, 49.0, 0.0, 0.0, 19.0, 26.34398668109711, 0.6102660686475998, 0.0, 1.0, 20.0, 35.26693379180657], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.49, 0.0, 0.0, 0.08333333333333333, 0.6953322234247592, 0.7034220228825333, 0.0, 1.0, 0.1, 0.3526693379180657], 
reward next is 0.6473, 
noisyNet noise sample is [array([-0.48386654], dtype=float32), -0.89432496]. 
=============================================
[2019-04-09 14:59:43,383] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01964226 0.11681748 0.09286319 0.08041804 0.06025658 0.02986524
 0.13809805 0.05447239 0.11948473 0.07521141 0.2128706 ], sum to 1.0000
[2019-04-09 14:59:43,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-09 14:59:43,398] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.86280218915903, 0.6948889970159907, 0.0, 1.0, 65.0, 42.60443982665002], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4246800.0000, 
sim time next is 4247400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.88177468851221, 0.7048796880388792, 0.0, 1.0, 20.0, 40.00840441389689], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7401478907093507, 0.7349598960129597, 0.0, 1.0, 0.1, 0.4000840441389689], 
reward next is 0.5999, 
noisyNet noise sample is [array([-0.5519742], dtype=float32), 1.67521]. 
=============================================
[2019-04-09 14:59:43,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02174847 0.11150006 0.10732814 0.07193451 0.04738164 0.02750648
 0.18936111 0.05871907 0.07418307 0.08422349 0.20611402], sum to 1.0000
[2019-04-09 14:59:43,456] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2222
[2019-04-09 14:59:43,471] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01073382 0.11466733 0.1631007  0.06728489 0.03560515 0.02776116
 0.17693609 0.0602405  0.0997104  0.07713494 0.16682504], sum to 1.0000
[2019-04-09 14:59:43,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3647
[2019-04-09 14:59:43,489] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 38.0, 83.33333333333333, 548.0, 19.0, 27.64238314933176, 0.9595071278480835, 0.0, 1.0, 20.0, 19.32578123163074], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4206000.0000, 
sim time next is 4206600.0000, 
raw observation next is [2.5, 38.5, 68.0, 550.0, 19.0, 27.66006119612244, 0.9662449177226299, 0.0, 1.0, 35.0, 24.44605475270622], 
processed observation next is [0.0, 0.6956521739130435, 0.5318559556786704, 0.385, 0.22666666666666666, 0.6077348066298343, 0.08333333333333333, 0.8050050996768702, 0.8220816392408766, 0.0, 1.0, 0.4, 0.2444605475270622], 
reward next is 0.7555, 
noisyNet noise sample is [array([0.87504363], dtype=float32), 0.09400782]. 
=============================================
[2019-04-09 14:59:43,490] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.97110924354225, 0.7203725348938025, 0.0, 1.0, 25.0, 36.22054403964396], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4240800.0000, 
sim time next is 4241400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.97169885873819, 0.7190098126803021, 0.0, 1.0, 25.0, 34.95348457012876], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7476415715615158, 0.7396699375601007, 0.0, 1.0, 0.2, 0.34953484570128757], 
reward next is 0.6505, 
noisyNet noise sample is [array([-0.73830837], dtype=float32), -0.5168422]. 
=============================================
[2019-04-09 14:59:43,621] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112825: loss 26.6596
[2019-04-09 14:59:43,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112825: learning rate 0.0000
[2019-04-09 14:59:43,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02299835 0.09288249 0.11492485 0.06172194 0.05534953 0.04013081
 0.19754513 0.06048905 0.07031007 0.06207102 0.22157678], sum to 1.0000
[2019-04-09 14:59:43,864] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4011
[2019-04-09 14:59:43,875] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.80891117707944, 0.6729953656134553, 0.0, 1.0, 45.0, 26.86628266891245], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4241400.0000, 
sim time next is 4242000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.73344063342371, 0.6542639694912925, 0.0, 1.0, 45.0, 25.58957541003905], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7277867194519759, 0.7180879898304308, 0.0, 1.0, 0.6, 0.2558957541003905], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.3381299], dtype=float32), 1.4764464]. 
=============================================
[2019-04-09 14:59:43,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[2.3556929]
 [2.3538103]
 [2.456249 ]
 [2.630782 ]
 [2.403733 ]], R is [[3.07259035]
 [3.77320147]
 [4.45325375]
 [5.11155558]
 [5.74258423]].
[2019-04-09 14:59:44,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01777962 0.12215982 0.15672873 0.08046087 0.05201017 0.02783463
 0.13500857 0.05106422 0.09064306 0.07477553 0.19153483], sum to 1.0000
[2019-04-09 14:59:44,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-09 14:59:44,359] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 19.0, 27.13982114661992, 0.848031584996482, 0.0, 1.0, 60.0, 26.42857689310191], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 27.14531634308244, 0.8567685534553758, 0.0, 1.0, 40.0, 24.46500481954882], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.3133333333333334, 0.393888888888889, 0.9313075506445673, 0.08333333333333333, 0.7621096952568699, 0.7855895178184586, 0.0, 1.0, 0.5, 0.2446500481954882], 
reward next is 0.7553, 
noisyNet noise sample is [array([-0.7757043], dtype=float32), -0.8906765]. 
=============================================
[2019-04-09 14:59:44,409] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01521472 0.12243697 0.1340023  0.10518824 0.04161809 0.02468053
 0.12955128 0.04928298 0.09248182 0.07272309 0.21282001], sum to 1.0000
[2019-04-09 14:59:44,410] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1976
[2019-04-09 14:59:44,425] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.3333333333333333, 31.66666666666667, 118.8333333333333, 826.1666666666666, 19.0, 27.06347603946045, 0.8341380729681146, 0.0, 1.0, 65.0, 31.11358485554078], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4189200.0000, 
sim time next is 4189800.0000, 
raw observation next is [0.6666666666666667, 30.83333333333333, 118.6666666666667, 830.3333333333334, 19.0, 27.07531569365395, 0.846369740108813, 0.0, 1.0, 25.0, 27.30478913591262], 
processed observation next is [0.0, 0.4782608695652174, 0.4810710987996307, 0.3083333333333333, 0.39555555555555566, 0.9174953959484347, 0.08333333333333333, 0.7562763078044957, 0.7821232467029376, 0.0, 1.0, 0.2, 0.2730478913591262], 
reward next is 0.7270, 
noisyNet noise sample is [array([-1.3452966], dtype=float32), -0.3193414]. 
=============================================
[2019-04-09 14:59:44,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0195213  0.09442361 0.11432029 0.06283063 0.0517513  0.03058966
 0.17196158 0.07012458 0.08149028 0.0845102  0.21847656], sum to 1.0000
[2019-04-09 14:59:44,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9049
[2019-04-09 14:59:44,598] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 46.0, 0.0, 0.0, 19.0, 26.94781867649868, 0.7111282327073631, 0.0, 1.0, 45.0, 30.28176664198902], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4239600.0000, 
sim time next is 4240200.0000, 
raw observation next is [2.833333333333333, 45.5, 0.0, 0.0, 19.0, 26.9098502847871, 0.7127337299351625, 0.0, 1.0, 65.0, 56.65976772501377], 
processed observation next is [0.0, 0.043478260869565216, 0.541089566020314, 0.455, 0.0, 0.0, 0.08333333333333333, 0.7424875237322585, 0.7375779099783876, 0.0, 1.0, 1.0, 0.5665976772501378], 
reward next is 0.4334, 
noisyNet noise sample is [array([0.24273647], dtype=float32), 2.23263]. 
=============================================
[2019-04-09 14:59:44,688] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0209636  0.11499905 0.10789421 0.08371007 0.04969892 0.03724416
 0.1690948  0.05667796 0.09273037 0.08124709 0.1857398 ], sum to 1.0000
[2019-04-09 14:59:44,702] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 113416: loss 24.8157
[2019-04-09 14:59:44,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 113417: learning rate 0.0000
[2019-04-09 14:59:44,708] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5007
[2019-04-09 14:59:44,742] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 36.66666666666666, 17.66666666666666, 19.0, 26.52603287973296, 0.6532009431632045, 0.0, 1.0, 40.0, 41.16159000664418], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4261800.0000, 
sim time next is 4262400.0000, 
raw observation next is [3.0, 49.0, 55.0, 26.5, 19.0, 26.60472571792581, 0.660826152153923, 0.0, 1.0, 45.0, 31.50134943413999], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.49, 0.18333333333333332, 0.029281767955801105, 0.08333333333333333, 0.7170604764938174, 0.7202753840513076, 0.0, 1.0, 0.6, 0.3150134943413999], 
reward next is 0.6850, 
noisyNet noise sample is [array([0.2877584], dtype=float32), 0.572692]. 
=============================================
[2019-04-09 14:59:44,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01563859 0.09459992 0.11429688 0.0713232  0.04838439 0.02733371
 0.20059842 0.0570965  0.05433848 0.08158062 0.23480932], sum to 1.0000
[2019-04-09 14:59:44,832] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9247
[2019-04-09 14:59:44,843] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 27.00333646522013, 0.7354797026744363, 0.0, 1.0, 65.0, 40.82348616002182], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 26.99912271404872, 0.7347968860612569, 0.0, 1.0, 65.0, 40.88521750935417], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7499268928373933, 0.7449322953537524, 0.0, 1.0, 1.0, 0.4088521750935417], 
reward next is 0.5911, 
noisyNet noise sample is [array([-0.12003512], dtype=float32), 0.40246266]. 
=============================================
[2019-04-09 14:59:44,996] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 113565: loss 32.0378
[2019-04-09 14:59:44,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 113565: learning rate 0.0000
[2019-04-09 14:59:45,530] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02017422 0.11198977 0.1158686  0.08617143 0.05058453 0.03079708
 0.16481656 0.06428559 0.08931689 0.08573028 0.18026508], sum to 1.0000
[2019-04-09 14:59:45,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-09 14:59:45,554] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.87251691767221, 0.6893547076415993, 0.0, 1.0, 65.0, 56.26224946201938], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.86548555537257, 0.688500813246049, 0.0, 1.0, 65.0, 43.1230666870178], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7387904629477141, 0.7295002710820163, 0.0, 1.0, 1.0, 0.43123066687017797], 
reward next is 0.5688, 
noisyNet noise sample is [array([-1.057567], dtype=float32), 1.7252458]. 
=============================================
[2019-04-09 14:59:45,587] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.3751836]
 [2.229548 ]
 [2.13269  ]
 [2.2220402]
 [2.299583 ]], R is [[2.75731373]
 [3.16711807]
 [3.78359628]
 [4.32512522]
 [4.87433767]].
[2019-04-09 14:59:45,617] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0126689  0.12700263 0.12625599 0.07682421 0.04493224 0.02748924
 0.16248158 0.05203483 0.0989158  0.07826502 0.19312955], sum to 1.0000
[2019-04-09 14:59:45,623] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4845
[2019-04-09 14:59:45,637] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.0, 847.0, 19.0, 27.55049937262877, 0.934657961377924, 0.0, 1.0, 65.0, 25.41539574541761], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 19.0, 27.56821201354035, 0.9492268187861841, 0.0, 1.0, 45.0, 26.9710127053227], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4377777777777776, 0.9119705340699816, 0.08333333333333333, 0.7973510011283625, 0.8164089395953947, 0.0, 1.0, 0.6, 0.269710127053227], 
reward next is 0.7303, 
noisyNet noise sample is [array([1.6648445], dtype=float32), -0.91224366]. 
=============================================
[2019-04-09 14:59:45,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01101636 0.10999523 0.09502528 0.05508747 0.04193715 0.0203712
 0.17494886 0.06444716 0.09729819 0.08950111 0.240372  ], sum to 1.0000
[2019-04-09 14:59:45,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7558
[2019-04-09 14:59:45,674] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.9, 75.0, 0.0, 0.0, 19.0, 27.39618644104567, 0.8949217255322018, 0.0, 1.0, 60.0, 33.69478881386411], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4311600.0000, 
sim time next is 4312200.0000, 
raw observation next is [4.85, 75.5, 0.0, 0.0, 19.0, 27.41722538962184, 0.8935834651997823, 0.0, 1.0, 60.0, 34.39433908556153], 
processed observation next is [0.0, 0.9130434782608695, 0.5969529085872576, 0.755, 0.0, 0.0, 0.08333333333333333, 0.7847687824684867, 0.797861155066594, 0.0, 1.0, 0.9, 0.3439433908556153], 
reward next is 0.6561, 
noisyNet noise sample is [array([0.319266], dtype=float32), -0.256808]. 
=============================================
[2019-04-09 14:59:45,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01601192 0.1238206  0.11761782 0.08437014 0.05225811 0.03457357
 0.14690222 0.0571356  0.08397099 0.0883035  0.19503556], sum to 1.0000
[2019-04-09 14:59:45,825] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6754
[2019-04-09 14:59:45,849] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.0, 26.8799870412593, 0.684459788122445, 0.0, 1.0, 65.0, 42.21077458678565], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4255200.0000, 
sim time next is 4255800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.87201162632292, 0.6834574194509334, 0.0, 1.0, 55.0, 38.36134995405936], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7393343021935767, 0.7278191398169778, 0.0, 1.0, 0.8, 0.38361349954059354], 
reward next is 0.6164, 
noisyNet noise sample is [array([2.7633996], dtype=float32), -1.4738982]. 
=============================================
[2019-04-09 14:59:46,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01864097 0.1416082  0.10580023 0.06919198 0.05741908 0.02814245
 0.15240417 0.06598627 0.09583192 0.09261056 0.17236425], sum to 1.0000
[2019-04-09 14:59:46,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4094
[2019-04-09 14:59:46,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.77311124163819, 0.6746915070527756, 0.0, 1.0, 45.0, 34.52805953592716], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4248600.0000, 
sim time next is 4249200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.81034471030051, 0.6754551258931945, 0.0, 1.0, 40.0, 36.20449901789266], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7341953925250424, 0.7251517086310648, 0.0, 1.0, 0.5, 0.3620449901789266], 
reward next is 0.6380, 
noisyNet noise sample is [array([1.046171], dtype=float32), 0.85437053]. 
=============================================
[2019-04-09 14:59:46,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01110451 0.1204521  0.11706132 0.06393739 0.04125683 0.02149183
 0.18481664 0.04980049 0.07595387 0.09604008 0.21808498], sum to 1.0000
[2019-04-09 14:59:46,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2797
[2019-04-09 14:59:46,579] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.949999999999999, 74.5, 0.0, 0.0, 19.0, 27.52499484517912, 0.9047025630605653, 0.0, 1.0, 20.0, 26.63902068208094], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4311000.0000, 
sim time next is 4311600.0000, 
raw observation next is [4.9, 75.0, 0.0, 0.0, 19.0, 27.51030281084134, 0.9007186888597087, 0.0, 1.0, 30.0, 28.89111330689898], 
processed observation next is [0.0, 0.9130434782608695, 0.5983379501385043, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7925252342367782, 0.8002395629532363, 0.0, 1.0, 0.3, 0.2889111330689898], 
reward next is 0.7111, 
noisyNet noise sample is [array([0.22667955], dtype=float32), 0.28629786]. 
=============================================
[2019-04-09 14:59:46,595] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00854151 0.1036244  0.14490332 0.03742412 0.03347974 0.01754496
 0.1711485  0.04515229 0.06669829 0.06887737 0.30260557], sum to 1.0000
[2019-04-09 14:59:46,597] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9581
[2019-04-09 14:59:46,610] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.45, 72.5, 0.0, 0.0, 19.0, 27.57225538721489, 0.8681777331341757, 0.0, 1.0, 60.0, 35.67805759258169], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [4.5, 72.0, 0.0, 0.0, 19.0, 27.56633799439551, 0.8605809623059141, 0.0, 1.0, 65.0, 31.89183352299794], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7971948328662926, 0.7868603207686381, 0.0, 1.0, 1.0, 0.31891833522997937], 
reward next is 0.6811, 
noisyNet noise sample is [array([-0.1726602], dtype=float32), 0.83273774]. 
=============================================
[2019-04-09 14:59:46,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01146317 0.13997768 0.1130963  0.07821994 0.05747908 0.02796936
 0.17194118 0.05188482 0.08548959 0.07754778 0.1849311 ], sum to 1.0000
[2019-04-09 14:59:46,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1789
[2019-04-09 14:59:46,692] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 19.0, 27.8788843273924, 0.9883518768727796, 0.0, 1.0, 65.0, 28.40075466511253], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 19.0, 27.84249597428972, 0.9829295611699163, 0.0, 1.0, 25.0, 28.66094355004955], 
processed observation next is [0.0, 0.782608695652174, 0.6325023084025855, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8202079978574766, 0.8276431870566388, 0.0, 1.0, 0.2, 0.2866094355004955], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.28372648], dtype=float32), 0.7348745]. 
=============================================
[2019-04-09 14:59:46,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01948826 0.11923938 0.13378102 0.07881758 0.05460791 0.03101577
 0.16186179 0.06100173 0.07418026 0.0691739  0.19683248], sum to 1.0000
[2019-04-09 14:59:46,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-09 14:59:46,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.97361176043625, 0.7189002038245175, 0.0, 1.0, 55.0, 40.87265570199], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4242000.0000, 
sim time next is 4242600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.97116556768594, 0.7173387410475112, 0.0, 1.0, 30.0, 32.68228250727348], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.747597130640495, 0.7391129136825038, 0.0, 1.0, 0.3, 0.32682282507273475], 
reward next is 0.6732, 
noisyNet noise sample is [array([-0.5268451], dtype=float32), -1.6502368]. 
=============================================
[2019-04-09 14:59:46,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01286732 0.0989661  0.17367871 0.06333697 0.05211443 0.02068679
 0.16013184 0.04883304 0.08438827 0.09396276 0.1910337 ], sum to 1.0000
[2019-04-09 14:59:46,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9318
[2019-04-09 14:59:46,997] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.300000000000001, 73.0, 0.0, 0.0, 19.0, 27.62857823185891, 0.9299916018024391, 0.0, 1.0, 65.0, 46.46773620870417], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4306800.0000, 
sim time next is 4307400.0000, 
raw observation next is [5.25, 73.0, 0.0, 0.0, 19.0, 27.61420532008569, 0.925063593622672, 0.0, 1.0, 65.0, 32.31306127344553], 
processed observation next is [0.0, 0.8695652173913043, 0.60803324099723, 0.73, 0.0, 0.0, 0.08333333333333333, 0.8011837766738076, 0.8083545312075574, 0.0, 1.0, 1.0, 0.32313061273445526], 
reward next is 0.6769, 
noisyNet noise sample is [array([0.18707576], dtype=float32), -0.11010594]. 
=============================================
[2019-04-09 14:59:47,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00877514 0.10927311 0.13759878 0.04352812 0.05027337 0.01791039
 0.1914627  0.0465894  0.07927097 0.10420655 0.21111147], sum to 1.0000
[2019-04-09 14:59:47,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3986
[2019-04-09 14:59:47,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01290565 0.12881744 0.10388029 0.05915316 0.03960193 0.02026338
 0.1557783  0.04851301 0.09184427 0.08311088 0.2561317 ], sum to 1.0000
[2019-04-09 14:59:47,064] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.35, 75.5, 0.0, 0.0, 19.0, 27.38411652422563, 0.8533097416772208, 0.0, 1.0, 40.0, 37.14821551002225], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [4.3, 75.33333333333334, 0.0, 0.0, 19.0, 27.35233994029064, 0.8549223192189324, 0.0, 1.0, 45.0, 28.80036442616186], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7793616616908867, 0.7849741064063108, 0.0, 1.0, 0.6, 0.2880036442616186], 
reward next is 0.7120, 
noisyNet noise sample is [array([1.5210966], dtype=float32), 1.2663413]. 
=============================================
[2019-04-09 14:59:47,065] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2157
[2019-04-09 14:59:47,075] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 19.0, 27.58158376341709, 0.9185754494259961, 0.0, 1.0, 45.0, 26.06812051672486], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 19.0, 27.5629430251257, 0.9142390265928525, 0.0, 1.0, 25.0, 28.67012711336002], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7969119187604751, 0.8047463421976175, 0.0, 1.0, 0.2, 0.2867012711336002], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.12742586], dtype=float32), 2.2213924]. 
=============================================
[2019-04-09 14:59:47,151] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01079145 0.12951352 0.09770178 0.05514357 0.04342862 0.01969862
 0.17333652 0.06005141 0.09296256 0.09689558 0.22047636], sum to 1.0000
[2019-04-09 14:59:47,152] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7599
[2019-04-09 14:59:47,175] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 19.0, 27.48828787232641, 0.895709359082498, 0.0, 1.0, 65.0, 33.74722391780985], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 19.0, 27.4741813130272, 0.8920156593414523, 0.0, 1.0, 60.0, 34.06383649731651], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.08333333333333333, 0.7895151094189332, 0.7973385531138174, 0.0, 1.0, 0.9, 0.3406383649731651], 
reward next is 0.6594, 
noisyNet noise sample is [array([0.77458894], dtype=float32), -0.15587194]. 
=============================================
[2019-04-09 14:59:47,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01187689 0.12444744 0.13390693 0.06769558 0.03757397 0.02360403
 0.17581639 0.05103926 0.0932521  0.08839106 0.1923963 ], sum to 1.0000
[2019-04-09 14:59:47,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6488
[2019-04-09 14:59:47,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.866666666666667, 57.33333333333334, 77.66666666666666, 572.5, 19.0, 28.01347138377209, 1.062886158495581, 0.0, 1.0, 30.0, 19.83425475995185], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4292400.0000, 
sim time next is 4293000.0000, 
raw observation next is [6.8, 58.0, 70.0, 556.0, 19.0, 28.02059892758284, 1.062925228262096, 0.0, 1.0, 55.0, 22.71417543918271], 
processed observation next is [0.0, 0.6956521739130435, 0.6509695290858727, 0.58, 0.23333333333333334, 0.6143646408839779, 0.08333333333333333, 0.8350499106319033, 0.8543084094206987, 0.0, 1.0, 0.8, 0.2271417543918271], 
reward next is 0.7729, 
noisyNet noise sample is [array([-0.25590903], dtype=float32), -0.1906188]. 
=============================================
[2019-04-09 14:59:47,213] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[2.8742075]
 [2.8877127]
 [2.9304304]
 [2.8872576]
 [2.6616247]], R is [[3.74970198]
 [4.51386261]
 [5.27222204]
 [6.03806114]
 [6.76937485]].
[2019-04-09 14:59:47,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01326157 0.11441775 0.10991767 0.05273189 0.05668325 0.02221506
 0.18649374 0.05835097 0.06432662 0.10693388 0.21466762], sum to 1.0000
[2019-04-09 14:59:47,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5693
[2019-04-09 14:59:47,399] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.6, 75.5, 0.0, 0.0, 19.0, 27.43988915349758, 0.8835363233712865, 0.0, 1.0, 20.0, 29.92749331247521], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4314600.0000, 
sim time next is 4315200.0000, 
raw observation next is [4.533333333333333, 75.33333333333334, 0.0, 0.0, 19.0, 27.42572637661811, 0.886082539873419, 0.0, 1.0, 50.0, 30.11990706027654], 
processed observation next is [0.0, 0.9565217391304348, 0.5881809787626964, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7854771980515091, 0.7953608466244729, 0.0, 1.0, 0.7, 0.3011990706027654], 
reward next is 0.6988, 
noisyNet noise sample is [array([0.33149636], dtype=float32), -1.7920226]. 
=============================================
[2019-04-09 14:59:47,434] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01851407 0.10913456 0.10905702 0.0797275  0.06067459 0.02485277
 0.15802482 0.0585963  0.09395767 0.07859497 0.20886572], sum to 1.0000
[2019-04-09 14:59:47,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3886
[2019-04-09 14:59:47,464] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 19.0, 26.90055985508771, 0.6905230203447531, 0.0, 1.0, 65.0, 41.23519487185365], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.89481393947149, 0.6888809566550292, 0.0, 1.0, 40.0, 38.85699655918226], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7412344949559575, 0.7296269855516764, 0.0, 1.0, 0.5, 0.38856996559182255], 
reward next is 0.6114, 
noisyNet noise sample is [array([0.6286169], dtype=float32), -2.2577043]. 
=============================================
[2019-04-09 14:59:47,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0127296  0.09293175 0.12638046 0.05246157 0.04533423 0.01875284
 0.16404757 0.05946825 0.10656071 0.08245209 0.23888093], sum to 1.0000
[2019-04-09 14:59:47,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01155706 0.11500168 0.13721281 0.06059459 0.04693938 0.02155875
 0.14091009 0.05380731 0.0962787  0.08883748 0.22730216], sum to 1.0000
[2019-04-09 14:59:47,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4718
[2019-04-09 14:59:47,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7760
[2019-04-09 14:59:47,548] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01082204 0.11626676 0.1491267  0.04821469 0.04211574 0.02053905
 0.22008428 0.06291936 0.08397848 0.08446463 0.16146834], sum to 1.0000
[2019-04-09 14:59:47,549] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8024
[2019-04-09 14:59:47,557] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.300000000000001, 73.0, 0.0, 0.0, 19.0, 27.64542134431567, 0.9329617400609219, 0.0, 1.0, 50.0, 25.02983670957556], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4306800.0000, 
sim time next is 4307400.0000, 
raw observation next is [5.25, 73.0, 0.0, 0.0, 19.0, 27.63056056955718, 0.927921805505609, 0.0, 1.0, 45.0, 28.05322277811619], 
processed observation next is [0.0, 0.8695652173913043, 0.60803324099723, 0.73, 0.0, 0.0, 0.08333333333333333, 0.802546714129765, 0.8093072685018696, 0.0, 1.0, 0.6, 0.2805322277811619], 
reward next is 0.7195, 
noisyNet noise sample is [array([-1.1977521], dtype=float32), 0.88730603]. 
=============================================
[2019-04-09 14:59:47,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 56.0, 93.0, 605.5, 19.0, 27.96118237782792, 1.055114103362107, 0.0, 1.0, 65.0, 24.37541321814144], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4291200.0000, 
sim time next is 4291800.0000, 
raw observation next is [6.933333333333334, 56.66666666666667, 85.33333333333333, 589.0, 19.0, 27.98149267067567, 1.056055893395949, 0.0, 1.0, 65.0, 22.82611544406653], 
processed observation next is [0.0, 0.6956521739130435, 0.6546629732225301, 0.5666666666666668, 0.28444444444444444, 0.6508287292817679, 0.08333333333333333, 0.8317910558896392, 0.852018631131983, 0.0, 1.0, 1.0, 0.2282611544406653], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.88477904], dtype=float32), 1.013558]. 
=============================================
[2019-04-09 14:59:47,584] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.5, 76.0, 0.0, 0.0, 19.0, 27.36827908939556, 0.8668651359131573, 0.0, 1.0, 45.0, 28.53643909145967], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [4.45, 75.83333333333334, 0.0, 0.0, 19.0, 27.39763229333946, 0.8652822595100718, 0.0, 1.0, 60.0, 33.87100292207413], 
processed observation next is [1.0, 0.0, 0.5858725761772854, 0.7583333333333334, 0.0, 0.0, 0.08333333333333333, 0.7831360244449549, 0.7884274198366906, 0.0, 1.0, 0.9, 0.33871002922074134], 
reward next is 0.6613, 
noisyNet noise sample is [array([-1.06155], dtype=float32), -0.4761499]. 
=============================================
[2019-04-09 14:59:47,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01451318 0.1164838  0.14119849 0.06871204 0.04412606 0.02607085
 0.15587355 0.07251672 0.08473442 0.08422592 0.19154498], sum to 1.0000
[2019-04-09 14:59:47,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6607
[2019-04-09 14:59:47,915] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.966666666666667, 57.5, 215.3333333333333, 495.6666666666666, 19.0, 27.76782771198678, 0.9988556362593458, 0.0, 1.0, 30.0, 21.62888345486283], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4284600.0000, 
sim time next is 4285200.0000, 
raw observation next is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 19.0, 27.79183209799977, 1.002380873387875, 0.0, 1.0, 55.0, 21.02624677764663], 
processed observation next is [0.0, 0.6086956521739131, 0.6546629732225301, 0.5800000000000001, 0.7405555555555557, 0.4865561694290977, 0.08333333333333333, 0.8159860081666475, 0.8341269577959584, 0.0, 1.0, 0.8, 0.2102624677764663], 
reward next is 0.7897, 
noisyNet noise sample is [array([0.41454986], dtype=float32), 0.79514796]. 
=============================================
[2019-04-09 14:59:48,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01125601 0.11332352 0.13063218 0.06505109 0.03726882 0.02444554
 0.1700552  0.03893512 0.09264704 0.08816091 0.22822458], sum to 1.0000
[2019-04-09 14:59:48,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7280
[2019-04-09 14:59:48,101] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [4.9, 75.0, 0.0, 0.0, 19.0, 27.52394023546384, 0.904726369964617, 0.0, 1.0, 60.0, 33.45144915450783], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4311600.0000, 
sim time next is 4312200.0000, 
raw observation next is [4.85, 75.5, 0.0, 0.0, 19.0, 27.5067895923292, 0.9019643881784262, 0.0, 1.0, 60.0, 33.81745285657203], 
processed observation next is [0.0, 0.9130434782608695, 0.5969529085872576, 0.755, 0.0, 0.0, 0.08333333333333333, 0.7922324660274332, 0.8006547960594754, 0.0, 1.0, 0.9, 0.33817452856572033], 
reward next is 0.6618, 
noisyNet noise sample is [array([0.9314961], dtype=float32), -1.1300346]. 
=============================================
[2019-04-09 14:59:48,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00651499 0.1333384  0.1436607  0.0700807  0.03084958 0.00987327
 0.1814507  0.05210285 0.09974065 0.064353   0.20803522], sum to 1.0000
[2019-04-09 14:59:48,590] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8708
[2019-04-09 14:59:48,608] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.75, 59.83333333333333, 97.0, 553.0, 22.5, 27.9138083449696, 0.9491280968294955, 1.0, 1.0, 65.0, 23.01710966926948], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4351800.0000, 
sim time next is 4352400.0000, 
raw observation next is [6.3, 57.0, 99.5, 584.0, 22.5, 27.96995596842807, 0.9632180262587028, 1.0, 1.0, 65.0, 21.20014985923807], 
processed observation next is [1.0, 0.391304347826087, 0.6371191135734073, 0.57, 0.33166666666666667, 0.6453038674033149, 0.375, 0.8308296640356726, 0.8210726754195675, 1.0, 1.0, 1.0, 0.2120014985923807], 
reward next is 0.7880, 
noisyNet noise sample is [array([0.42533332], dtype=float32), -0.59314144]. 
=============================================
[2019-04-09 14:59:48,789] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01431448 0.13186352 0.10933348 0.07477924 0.05235404 0.02478527
 0.1542293  0.06902178 0.10516076 0.08864816 0.17550993], sum to 1.0000
[2019-04-09 14:59:48,793] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5835
[2019-04-09 14:59:48,807] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 19.0, 27.24044786626833, 0.8455341879101693, 0.0, 1.0, 45.0, 30.35190012173573], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4272000.0000, 
sim time next is 4272600.0000, 
raw observation next is [4.833333333333334, 54.83333333333334, 176.3333333333333, 676.6666666666666, 19.0, 27.27589012000585, 0.8558633169346176, 0.0, 1.0, 50.0, 29.09984849194088], 
processed observation next is [0.0, 0.43478260869565216, 0.5964912280701755, 0.5483333333333335, 0.5877777777777776, 0.7476979742173112, 0.08333333333333333, 0.772990843333821, 0.7852877723115391, 0.0, 1.0, 0.7, 0.2909984849194088], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.02001027], dtype=float32), 0.55390966]. 
=============================================
[2019-04-09 14:59:48,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01311245 0.12264266 0.11514932 0.05193979 0.03885981 0.01649355
 0.15769607 0.05481003 0.09227318 0.09839887 0.23862429], sum to 1.0000
[2019-04-09 14:59:48,871] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00881191 0.10255344 0.10888765 0.05538968 0.03133252 0.01397673
 0.22317542 0.05445571 0.09878356 0.09075599 0.21187739], sum to 1.0000
[2019-04-09 14:59:48,872] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-09 14:59:48,874] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0150
[2019-04-09 14:59:48,888] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.933333333333334, 74.66666666666667, 0.0, 0.0, 22.5, 27.2795422901125, 0.7887052870421217, 0.0, 1.0, 45.0, 30.43529434043605], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4346400.0000, 
sim time next is 4347000.0000, 
raw observation next is [2.95, 74.5, 0.0, 0.0, 22.5, 27.26759447760158, 0.7829936485332237, 1.0, 1.0, 20.0, 32.88246459046193], 
processed observation next is [1.0, 0.30434782608695654, 0.5443213296398892, 0.745, 0.0, 0.0, 0.375, 0.7722995398001317, 0.760997882844408, 1.0, 1.0, 0.1, 0.3288246459046193], 
reward next is 0.6712, 
noisyNet noise sample is [array([-0.9932795], dtype=float32), -0.8188293]. 
=============================================
[2019-04-09 14:59:48,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.2712867]
 [3.2257438]
 [3.2630928]
 [3.2793734]
 [3.3376896]], R is [[3.71981382]
 [4.378263  ]
 [5.02780342]
 [5.60179424]
 [6.22980356]].
[2019-04-09 14:59:48,902] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 19.0, 27.42220087274684, 0.8675391140843409, 0.0, 1.0, 20.0, 30.96452438019056], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4318800.0000, 
sim time next is 4319400.0000, 
raw observation next is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 19.0, 27.38998442530022, 0.8621287585972676, 0.0, 1.0, 50.0, 32.01041080812226], 
processed observation next is [0.0, 1.0, 0.5867959372114497, 0.7583333333333333, 0.0, 0.0, 0.08333333333333333, 0.7824987021083517, 0.7873762528657559, 0.0, 1.0, 0.7, 0.3201041080812226], 
reward next is 0.6799, 
noisyNet noise sample is [array([0.4275365], dtype=float32), 1.6042066]. 
=============================================
[2019-04-09 14:59:48,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01163691 0.1064684  0.11046232 0.05798065 0.04581242 0.01535319
 0.19944063 0.05869058 0.09181283 0.06706803 0.2352741 ], sum to 1.0000
[2019-04-09 14:59:48,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0868
[2019-04-09 14:59:48,930] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 19.0, 27.58483796752777, 0.9193790176816458, 0.0, 1.0, 25.0, 27.9860580060475], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 19.0, 27.56616242204837, 0.915058350970639, 0.0, 1.0, 20.0, 28.44067824789917], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7971802018373643, 0.8050194503235463, 0.0, 1.0, 0.1, 0.2844067824789917], 
reward next is 0.7156, 
noisyNet noise sample is [array([-1.6315286], dtype=float32), 1.1353934]. 
=============================================
[2019-04-09 14:59:48,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01024424 0.08852416 0.13350931 0.04758095 0.0309513  0.01138834
 0.18603943 0.05720542 0.07918371 0.10643989 0.24893318], sum to 1.0000
[2019-04-09 14:59:49,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-09 14:59:49,032] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.983333333333333, 74.16666666666667, 30.66666666666666, 163.6666666666666, 22.5, 27.18911297748171, 0.8022368713465698, 1.0, 1.0, 25.0, 31.93064443299142], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4348200.0000, 
sim time next is 4348800.0000, 
raw observation next is [3.0, 74.0, 46.0, 245.5, 22.5, 27.1863709932255, 0.8123556565404262, 1.0, 1.0, 60.0, 36.38506211957054], 
processed observation next is [1.0, 0.34782608695652173, 0.5457063711911359, 0.74, 0.15333333333333332, 0.2712707182320442, 0.375, 0.7655309161021249, 0.7707852188468087, 1.0, 1.0, 0.9, 0.36385062119570544], 
reward next is 0.6361, 
noisyNet noise sample is [array([-0.6310662], dtype=float32), 2.6297371]. 
=============================================
[2019-04-09 14:59:49,161] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00465561 0.14472152 0.14856477 0.08035149 0.01991414 0.01061943
 0.12217306 0.04714547 0.14144665 0.07576447 0.20464335], sum to 1.0000
[2019-04-09 14:59:49,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2229
[2019-04-09 14:59:49,184] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [8.15, 49.5, 107.0, 677.0, 22.5, 28.20205793217408, 1.024564479611597, 1.0, 1.0, 55.0, 11.46654286068236], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 22.5, 28.37357768989967, 1.05022000201223, 1.0, 1.0, 25.0, 11.44330845211831], 
processed observation next is [1.0, 0.391304347826087, 0.7054478301015698, 0.47, 0.361111111111111, 0.767034990791897, 0.375, 0.8644648074916391, 0.8500733340040766, 1.0, 1.0, 0.2, 0.11443308452118311], 
reward next is 0.8856, 
noisyNet noise sample is [array([1.002672], dtype=float32), 0.8763015]. 
=============================================
[2019-04-09 14:59:49,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00933565 0.09947316 0.1364371  0.05169541 0.02777858 0.01429056
 0.22457795 0.05132802 0.08058807 0.08084608 0.22364943], sum to 1.0000
[2019-04-09 14:59:49,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6462
[2019-04-09 14:59:49,251] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.35, 70.66666666666667, 0.0, 0.0, 19.0, 27.24073959149898, 0.8109308619434413, 0.0, 1.0, 35.0, 34.63542783433448], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4341000.0000, 
sim time next is 4341600.0000, 
raw observation next is [3.3, 71.0, 0.0, 0.0, 19.0, 27.33605993917001, 0.8135276795431098, 0.0, 1.0, 55.0, 27.20873464123846], 
processed observation next is [1.0, 0.2608695652173913, 0.554016620498615, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7780049949308342, 0.7711758931810366, 0.0, 1.0, 0.8, 0.27208734641238463], 
reward next is 0.7279, 
noisyNet noise sample is [array([-0.4606574], dtype=float32), -1.0315301]. 
=============================================
[2019-04-09 14:59:49,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00470024 0.11126387 0.13400568 0.03900668 0.03360124 0.0119698
 0.14772034 0.0448871  0.12495245 0.06979371 0.2780989 ], sum to 1.0000
[2019-04-09 14:59:49,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-09 14:59:49,397] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00527863 0.06529147 0.13960196 0.03583485 0.02301008 0.01104302
 0.25009286 0.04526168 0.06237355 0.06625932 0.2959526 ], sum to 1.0000
[2019-04-09 14:59:49,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00412893 0.14234926 0.12190934 0.03462612 0.02205074 0.00991838
 0.15452097 0.05292738 0.13492398 0.03892073 0.28372413], sum to 1.0000
[2019-04-09 14:59:49,404] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.13333333333333, 48.0, 0.0, 0.0, 22.5, 28.54955028770322, 1.391980853180872, 1.0, 1.0, 60.0, 13.9307777117064], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4387200.0000, 
sim time next is 4387800.0000, 
raw observation next is [12.06666666666667, 49.0, 0.0, 0.0, 22.5, 28.15827919262598, 1.303501675151916, 1.0, 1.0, 45.0, 5.618456799909106], 
processed observation next is [1.0, 0.782608695652174, 0.7968605724838413, 0.49, 0.0, 0.0, 0.375, 0.846523266052165, 0.934500558383972, 1.0, 1.0, 0.6, 0.056184567999091065], 
reward next is 0.9438, 
noisyNet noise sample is [array([0.4833752], dtype=float32), 2.1545668]. 
=============================================
[2019-04-09 14:59:49,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0430
[2019-04-09 14:59:49,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8721
[2019-04-09 14:59:49,445] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.749999999999999, 62.83333333333333, 0.0, 0.0, 19.0, 27.90321967298315, 1.123556144014791, 0.0, 1.0, 25.0, 28.09517335532336], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4405800.0000, 
sim time next is 4406400.0000, 
raw observation next is [7.6, 63.0, 0.0, 0.0, 19.0, 27.90078796200952, 1.117641403728874, 0.0, 1.0, 20.0, 21.7127302058965], 
processed observation next is [1.0, 0.0, 0.6731301939058172, 0.63, 0.0, 0.0, 0.08333333333333333, 0.8250656635007934, 0.8725471345762914, 0.0, 1.0, 0.1, 0.217127302058965], 
reward next is 0.7829, 
noisyNet noise sample is [array([0.35616353], dtype=float32), -2.0488122]. 
=============================================
[2019-04-09 14:59:49,448] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 22.5, 29.28414872169836, 1.284408683763665, 1.0, 1.0, 55.0, 57.09482028933021], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4367400.0000, 
sim time next is 4368000.0000, 
raw observation next is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 22.5, 28.74887579640423, 1.398734163936411, 1.0, 1.0, 65.0, 5.087924272878275], 
processed observation next is [1.0, 0.5652173913043478, 0.8661126500461682, 0.2966666666666667, 0.385, 0.9324125230202579, 0.375, 0.8957396497003526, 0.9662447213121371, 1.0, 1.0, 1.0, 0.05087924272878275], 
reward next is 0.9491, 
noisyNet noise sample is [array([-1.3576488], dtype=float32), -0.5791328]. 
=============================================
[2019-04-09 14:59:49,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[4.8024664]
 [4.680031 ]
 [4.670513 ]
 [4.669036 ]
 [4.723661 ]], R is [[5.5601902 ]
 [5.93364   ]
 [6.84627628]
 [7.75319386]
 [8.6281004 ]].
[2019-04-09 14:59:49,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00360988 0.16149361 0.17868082 0.04644636 0.01468762 0.00890227
 0.14318499 0.04521168 0.1082774  0.04788792 0.24161747], sum to 1.0000
[2019-04-09 14:59:49,494] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-09 14:59:49,510] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.3, 38.0, 115.0, 780.0, 22.5, 28.86578425529738, 1.160865871992657, 1.0, 1.0, 30.0, 7.965020380295007], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4357800.0000, 
sim time next is 4358400.0000, 
raw observation next is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 22.5, 28.86849205667301, 1.184012662214522, 1.0, 1.0, 65.0, 7.035262912031495], 
processed observation next is [1.0, 0.43478260869565216, 0.7876269621421976, 0.3666666666666666, 0.386111111111111, 0.8707182320441988, 0.375, 0.9057076713894174, 0.8946708874048407, 1.0, 1.0, 1.0, 0.07035262912031495], 
reward next is 0.9296, 
noisyNet noise sample is [array([0.51521796], dtype=float32), 1.3749391]. 
=============================================
[2019-04-09 14:59:49,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00367121 0.11256822 0.16006805 0.03256569 0.01897566 0.00863394
 0.16889815 0.04291767 0.10098854 0.05801191 0.29270095], sum to 1.0000
[2019-04-09 14:59:49,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1080
[2019-04-09 14:59:49,752] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.8, 28.5, 118.0, 853.0, 22.5, 29.36279295284814, 1.320997965410371, 1.0, 1.0, 60.0, 5.769197228720265], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4365000.0000, 
sim time next is 4365600.0000, 
raw observation next is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 22.5, 29.26382220402662, 1.329218629549903, 1.0, 1.0, 20.0, 4.93761675323178], 
processed observation next is [1.0, 0.5217391304347826, 0.8707294552169899, 0.2866666666666666, 0.39166666666666666, 0.9405156537753223, 0.375, 0.9386518503355518, 0.9430728765166343, 1.0, 1.0, 0.1, 0.0493761675323178], 
reward next is 0.9506, 
noisyNet noise sample is [array([1.1137381], dtype=float32), -1.4523215]. 
=============================================
[2019-04-09 14:59:49,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0121134  0.13116616 0.12345842 0.07789291 0.03689152 0.019174
 0.15305002 0.05870057 0.08997329 0.07761805 0.2199617 ], sum to 1.0000
[2019-04-09 14:59:49,811] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0500
[2019-04-09 14:59:49,823] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.6, 60.0, 47.0, 392.0, 19.0, 28.08292092706358, 1.053548539923418, 0.0, 1.0, 60.0, 22.51599282510556], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4294800.0000, 
sim time next is 4295400.0000, 
raw observation next is [6.533333333333333, 60.66666666666666, 39.33333333333333, 337.3333333333333, 19.0, 28.08041505502838, 1.048399825123293, 0.0, 1.0, 20.0, 23.72906745935979], 
processed observation next is [0.0, 0.7391304347826086, 0.6435826408125578, 0.6066666666666666, 0.1311111111111111, 0.372744014732965, 0.08333333333333333, 0.8400345879190315, 0.849466608374431, 0.0, 1.0, 0.1, 0.2372906745935979], 
reward next is 0.7627, 
noisyNet noise sample is [array([-0.8327836], dtype=float32), -0.693461]. 
=============================================
[2019-04-09 14:59:49,932] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00943635 0.0777092  0.1289116  0.04853711 0.02313674 0.01320071
 0.18812715 0.06152025 0.08006788 0.07279252 0.29656053], sum to 1.0000
[2019-04-09 14:59:49,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1726
[2019-04-09 14:59:49,951] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 19.0, 27.26266670497347, 0.7950064948467833, 0.0, 1.0, 25.0, 35.79848009199991], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 19.0, 27.23011466073403, 0.8097363435182005, 0.0, 1.0, 25.0, 34.83991805206489], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 0.08333333333333333, 0.769176221727836, 0.7699121145060669, 0.0, 1.0, 0.2, 0.3483991805206489], 
reward next is 0.6516, 
noisyNet noise sample is [array([0.17911491], dtype=float32), 0.4365557]. 
=============================================
[2019-04-09 14:59:49,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.375766 ]
 [3.2553966]
 [3.3503635]
 [3.242472 ]
 [3.3810408]], R is [[3.84315729]
 [4.4467411 ]
 [5.00477362]
 [5.5835371 ]
 [6.19588089]].
[2019-04-09 14:59:49,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0066656  0.1318552  0.13680775 0.05500422 0.02991057 0.00963544
 0.18726353 0.05174968 0.1267361  0.08376087 0.18061098], sum to 1.0000
[2019-04-09 14:59:49,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2381
[2019-04-09 14:59:49,986] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.3, 57.0, 99.5, 584.0, 22.5, 27.96784096557207, 0.9628948498928719, 1.0, 1.0, 65.0, 21.25403434895388], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4352400.0000, 
sim time next is 4353000.0000, 
raw observation next is [6.916666666666666, 54.5, 102.0, 615.0, 22.5, 28.00400029786172, 0.979196278183613, 1.0, 1.0, 25.0, 18.87527591459493], 
processed observation next is [1.0, 0.391304347826087, 0.6542012927054479, 0.545, 0.34, 0.6795580110497238, 0.375, 0.8336666914884766, 0.8263987593945377, 1.0, 1.0, 0.2, 0.1887527591459493], 
reward next is 0.8112, 
noisyNet noise sample is [array([1.2746494], dtype=float32), -0.63720876]. 
=============================================
[2019-04-09 14:59:49,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[3.9900055]
 [3.7317133]
 [3.8057914]
 [3.7606583]
 [3.7556138]], R is [[4.59036255]
 [5.33191872]
 [6.05577517]
 [6.79291248]
 [7.51935911]].
[2019-04-09 14:59:50,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01062687 0.12106866 0.11825048 0.05071867 0.02541966 0.01758152
 0.18527786 0.05790821 0.08188878 0.08396348 0.2472958 ], sum to 1.0000
[2019-04-09 14:59:50,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8459
[2019-04-09 14:59:50,084] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.833333333333334, 67.0, 0.0, 0.0, 19.0, 27.69521146888646, 1.044698913800825, 0.0, 1.0, 45.0, 28.64457684141808], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4418400.0000, 
sim time next is 4419000.0000, 
raw observation next is [4.75, 67.0, 0.0, 0.0, 19.0, 27.69315628113609, 1.044907342389193, 0.0, 1.0, 65.0, 45.99317211092905], 
processed observation next is [1.0, 0.13043478260869565, 0.5941828254847646, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8077630234280075, 0.8483024474630643, 0.0, 1.0, 1.0, 0.45993172110929054], 
reward next is 0.5401, 
noisyNet noise sample is [array([1.3694999], dtype=float32), -0.76702017]. 
=============================================
[2019-04-09 14:59:50,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[3.4496675]
 [3.5686116]
 [3.7094173]
 [3.5035906]
 [3.72149  ]], R is [[4.14204311]
 [4.81417704]
 [5.47867966]
 [6.15689898]
 [6.84722233]].
[2019-04-09 14:59:50,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01096023 0.09661385 0.16982758 0.03942066 0.02179925 0.0134848
 0.17818509 0.06989557 0.07576827 0.07832023 0.24572447], sum to 1.0000
[2019-04-09 14:59:50,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3267
[2019-04-09 14:59:50,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 19.0, 27.23386478609052, 0.7930172435285288, 0.0, 1.0, 45.0, 37.61597432634669], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [2.966666666666666, 74.33333333333333, 0.0, 0.0, 19.0, 27.2356971922158, 0.7900204222507847, 0.0, 1.0, 35.0, 29.52335710293175], 
processed observation next is [1.0, 0.2608695652173913, 0.5447830101569714, 0.7433333333333333, 0.0, 0.0, 0.08333333333333333, 0.7696414326846499, 0.7633401407502616, 0.0, 1.0, 0.4, 0.2952335710293175], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.67990094], dtype=float32), -0.25313687]. 
=============================================
[2019-04-09 14:59:50,438] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00837701 0.09410798 0.13195062 0.05111187 0.02973882 0.01871406
 0.18608785 0.04804198 0.07292562 0.08450791 0.2744362 ], sum to 1.0000
[2019-04-09 14:59:50,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5377
[2019-04-09 14:59:50,457] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.6, 69.0, 0.0, 0.0, 19.0, 27.33633397658574, 0.809709580850627, 0.0, 1.0, 55.0, 30.87557682121218], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [3.55, 69.33333333333333, 0.0, 0.0, 19.0, 27.28065930035551, 0.8064551846547828, 0.0, 1.0, 50.0, 33.19229795592653], 
processed observation next is [1.0, 0.21739130434782608, 0.5609418282548477, 0.6933333333333332, 0.0, 0.0, 0.08333333333333333, 0.7733882750296258, 0.7688183948849275, 0.0, 1.0, 0.7, 0.33192297955926525], 
reward next is 0.6681, 
noisyNet noise sample is [array([1.4431119], dtype=float32), -0.5844027]. 
=============================================
[2019-04-09 14:59:50,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00989306 0.09141657 0.11995799 0.03624158 0.02567725 0.01448003
 0.16193183 0.06698529 0.0925624  0.09132791 0.289526  ], sum to 1.0000
[2019-04-09 14:59:50,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5758
[2019-04-09 14:59:50,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00388409 0.13747056 0.17583556 0.04219013 0.02686599 0.01412828
 0.125413   0.03590908 0.09658963 0.04905501 0.2926587 ], sum to 1.0000
[2019-04-09 14:59:50,525] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3484
[2019-04-09 14:59:50,533] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 22.5, 29.30862142771716, 1.339916357581613, 1.0, 1.0, 25.0, 4.718699342174393], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 22.5, 29.30341984477937, 1.344082522080791, 1.0, 1.0, 30.0, 2.247912892221086], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 0.375, 0.9419516537316142, 0.9480275073602638, 1.0, 1.0, 0.3, 0.022479128922210857], 
reward next is 0.9775, 
noisyNet noise sample is [array([0.88312656], dtype=float32), 0.6062725]. 
=============================================
[2019-04-09 14:59:50,539] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 68.0, 0.0, 0.0, 19.0, 27.47742481104795, 0.9799945258644579, 0.0, 1.0, 45.0, 31.14625438429883], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4428000.0000, 
sim time next is 4428600.0000, 
raw observation next is [2.833333333333333, 70.0, 0.0, 0.0, 19.0, 27.51053722581383, 0.976342892264634, 0.0, 1.0, 55.0, 33.4118828571241], 
processed observation next is [1.0, 0.2608695652173913, 0.541089566020314, 0.7, 0.0, 0.0, 0.08333333333333333, 0.7925447688178192, 0.825447630754878, 0.0, 1.0, 0.8, 0.334118828571241], 
reward next is 0.6659, 
noisyNet noise sample is [array([-2.3705597], dtype=float32), -0.28055382]. 
=============================================
[2019-04-09 14:59:50,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00357215 0.07663229 0.18702604 0.03292967 0.02404705 0.0065353
 0.19289042 0.05472559 0.08189173 0.07619371 0.26355606], sum to 1.0000
[2019-04-09 14:59:50,584] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9286
[2019-04-09 14:59:50,594] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.533333333333333, 60.66666666666666, 0.0, 0.0, 19.0, 28.34188096540443, 1.232227334102675, 0.0, 1.0, 55.0, 15.56466510681327], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4398600.0000, 
sim time next is 4399200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 19.0, 28.31811380850578, 1.227620334621975, 0.0, 1.0, 65.0, 19.15898971909785], 
processed observation next is [1.0, 0.9565217391304348, 0.7229916897506927, 0.61, 0.0, 0.0, 0.08333333333333333, 0.8598428173754818, 0.909206778207325, 0.0, 1.0, 1.0, 0.1915898971909785], 
reward next is 0.8084, 
noisyNet noise sample is [array([-0.73393893], dtype=float32), -0.28650016]. 
=============================================
[2019-04-09 14:59:50,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00424726 0.15175842 0.16490991 0.06569029 0.0204782  0.01007225
 0.11197801 0.07182278 0.10771909 0.07349577 0.21782804], sum to 1.0000
[2019-04-09 14:59:50,808] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6359
[2019-04-09 14:59:50,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00460128 0.12467132 0.13726668 0.05282727 0.02229925 0.01128387
 0.154811   0.05358389 0.12257076 0.05055298 0.26553178], sum to 1.0000
[2019-04-09 14:59:50,815] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6949
[2019-04-09 14:59:50,819] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 22.5, 28.39299864666508, 1.059685562974698, 1.0, 1.0, 20.0, 10.70729294412403], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4354800.0000, 
sim time next is 4355400.0000, 
raw observation next is [9.383333333333333, 44.5, 109.6666666666667, 711.3333333333333, 22.5, 28.52698341010171, 1.08310740905763, 1.0, 1.0, 55.0, 9.006399405589745], 
processed observation next is [1.0, 0.391304347826087, 0.7225300092336104, 0.445, 0.3655555555555557, 0.7860036832412522, 0.375, 0.8772486175084758, 0.86103580301921, 1.0, 1.0, 0.8, 0.09006399405589745], 
reward next is 0.9099, 
noisyNet noise sample is [array([1.619739], dtype=float32), -0.6679707]. 
=============================================
[2019-04-09 14:59:50,827] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 22.5, 28.71011856246644, 1.126838396906511, 1.0, 1.0, 65.0, 8.225541514889674], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4356600.0000, 
sim time next is 4357200.0000, 
raw observation next is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 22.5, 28.78941700715345, 1.1486472224874, 1.0, 1.0, 60.0, 7.177404063773739], 
processed observation next is [1.0, 0.43478260869565216, 0.7636195752539245, 0.3933333333333334, 0.378888888888889, 0.8429097605893185, 0.375, 0.8991180839294541, 0.8828824074958, 1.0, 1.0, 0.9, 0.07177404063773739], 
reward next is 0.9282, 
noisyNet noise sample is [array([0.28878704], dtype=float32), -0.084002435]. 
=============================================
[2019-04-09 14:59:51,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00361345 0.12863581 0.18387248 0.04780119 0.02391275 0.00693055
 0.10310487 0.04637226 0.12671722 0.07505253 0.25398695], sum to 1.0000
[2019-04-09 14:59:51,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3795
[2019-04-09 14:59:51,172] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.86666666666667, 28.33333333333334, 118.3333333333333, 848.8333333333334, 22.5, 29.45654622053993, 1.335793105117801, 1.0, 1.0, 55.0, 1.757379441581588], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4364400.0000, 
sim time next is 4365000.0000, 
raw observation next is [14.8, 28.5, 118.0, 853.0, 22.5, 29.40797171371019, 1.332236023926673, 1.0, 1.0, 35.0, 5.386235402278412], 
processed observation next is [1.0, 0.5217391304347826, 0.8725761772853187, 0.285, 0.3933333333333333, 0.9425414364640884, 0.375, 0.9506643094758491, 0.9440786746422244, 1.0, 1.0, 0.4, 0.053862354022784116], 
reward next is 0.9461, 
noisyNet noise sample is [array([-2.2171597], dtype=float32), -0.5442259]. 
=============================================
[2019-04-09 14:59:51,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01080372 0.10103222 0.1267064  0.05448106 0.02682124 0.01485787
 0.19462635 0.05956746 0.07884368 0.07195928 0.26030064], sum to 1.0000
[2019-04-09 14:59:51,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8123
[2019-04-09 14:59:51,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[4.9371014]
 [5.11241  ]
 [4.947825 ]
 [5.146198 ]
 [4.9448624]], R is [[5.89061546]
 [6.81413555]
 [7.72718334]
 [8.63137245]
 [9.52427101]].
[2019-04-09 14:59:51,221] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 19.0, 27.42285093752128, 0.8297094391121491, 0.0, 1.0, 65.0, 37.24747347379873], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 27.36134995088074, 0.8232121198168749, 0.0, 1.0, 65.0, 37.51138746171721], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.7801124959067284, 0.7744040399389583, 0.0, 1.0, 1.0, 0.3751138746171721], 
reward next is 0.6249, 
noisyNet noise sample is [array([1.281747], dtype=float32), 1.2803857]. 
=============================================
[2019-04-09 14:59:51,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00343849 0.09879898 0.1785275  0.0494805  0.02332635 0.00873394
 0.19426464 0.04801131 0.09140209 0.06143317 0.24258302], sum to 1.0000
[2019-04-09 14:59:51,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6789
[2019-04-09 14:59:51,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00250731 0.09651764 0.11792839 0.03850354 0.01981505 0.00608711
 0.11680821 0.04817432 0.13442294 0.04920098 0.3700346 ], sum to 1.0000
[2019-04-09 14:59:51,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-09 14:59:51,279] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 19.0, 28.52478408355596, 1.273969072195509, 1.0, 1.0, 65.0, 15.69405587188089], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4392600.0000, 
sim time next is 4393200.0000, 
raw observation next is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 19.0, 28.49941249154968, 1.269090857847928, 0.0, 1.0, 55.0, 14.85241284074502], 
processed observation next is [1.0, 0.8695652173913043, 0.7599261311172669, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.8749510409624733, 0.9230302859493094, 0.0, 1.0, 0.8, 0.1485241284074502], 
reward next is 0.8515, 
noisyNet noise sample is [array([-0.8575434], dtype=float32), -1.0038295]. 
=============================================
[2019-04-09 14:59:51,288] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.0, 37.0, 34.0, 0.0, 22.5, 29.32108036397602, 1.452009729222896, 1.0, 1.0, 45.0, 7.831290730410769], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [13.0, 37.5, 29.0, 0.0, 22.5, 28.90771195304207, 1.353095422431519, 1.0, 1.0, 65.0, 2.912202232141771], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.375, 0.09666666666666666, 0.0, 0.375, 0.9089759960868392, 0.951031807477173, 1.0, 1.0, 1.0, 0.02912202232141771], 
reward next is 0.9709, 
noisyNet noise sample is [array([-0.20322147], dtype=float32), -0.22342844]. 
=============================================
[2019-04-09 14:59:51,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00297244 0.07629541 0.2129758  0.02812778 0.01829254 0.00555626
 0.15408866 0.05989688 0.10259403 0.09423995 0.24496025], sum to 1.0000
[2019-04-09 14:59:51,308] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00362226 0.10407016 0.13426012 0.04575636 0.02638987 0.00758362
 0.15219504 0.05383881 0.12038495 0.08327875 0.26862007], sum to 1.0000
[2019-04-09 14:59:51,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0418
[2019-04-09 14:59:51,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2263
[2019-04-09 14:59:51,321] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 19.0, 28.41436233361482, 1.245338880405717, 0.0, 1.0, 25.0, 14.92411526428177], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 19.0, 28.39029196889513, 1.24009364821837, 0.0, 1.0, 20.0, 15.67211042593124], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.08333333333333333, 0.8658576640745942, 0.9133645494061233, 0.0, 1.0, 0.1, 0.1567211042593124], 
reward next is 0.8433, 
noisyNet noise sample is [array([-0.69813967], dtype=float32), 0.6785652]. 
=============================================
[2019-04-09 14:59:51,322] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 22.5, 28.48046567936977, 1.158092117585177, 1.0, 1.0, 65.0, 20.66100615398955], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 22.5, 28.49822207130864, 1.158598780349063, 1.0, 1.0, 25.0, 18.50342726727039], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.375, 0.87485183927572, 0.8861995934496877, 1.0, 1.0, 0.2, 0.1850342726727039], 
reward next is 0.8150, 
noisyNet noise sample is [array([-0.3831075], dtype=float32), 0.7913451]. 
=============================================
[2019-04-09 14:59:51,329] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[4.7334557]
 [4.7734623]
 [4.6672363]
 [4.6924524]
 [4.469795 ]], R is [[5.34782219]
 [6.08773375]
 [6.85591555]
 [7.63226938]
 [8.38795853]].
[2019-04-09 14:59:51,472] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00245501 0.06967578 0.16753666 0.02969043 0.01880004 0.01047116
 0.17843723 0.04691956 0.08533352 0.05899701 0.3316836 ], sum to 1.0000
[2019-04-09 14:59:51,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0167
[2019-04-09 14:59:51,483] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 22.5, 29.34172887257763, 1.392185789011924, 1.0, 1.0, 25.0, 4.201046048234617], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 22.5, 29.31287973129812, 1.398035383349574, 1.0, 1.0, 20.0, 4.95291598208145], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 0.375, 0.9427399776081767, 0.966011794449858, 1.0, 1.0, 0.1, 0.049529159820814496], 
reward next is 0.9505, 
noisyNet noise sample is [array([-0.3808606], dtype=float32), 0.8684324]. 
=============================================
[2019-04-09 14:59:51,734] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00861849 0.0916358  0.13608204 0.04634111 0.02380143 0.01356311
 0.1883778  0.04982175 0.10414712 0.0938663  0.24374506], sum to 1.0000
[2019-04-09 14:59:51,735] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2815
[2019-04-09 14:59:51,747] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 19.0, 27.42522551450288, 0.8304317914117911, 0.0, 1.0, 45.0, 32.76089708346505], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 27.36515049964031, 0.8240451925902876, 0.0, 1.0, 30.0, 32.37555026443852], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.7804292083033593, 0.7746817308634292, 0.0, 1.0, 0.3, 0.32375550264438524], 
reward next is 0.6762, 
noisyNet noise sample is [array([0.54641575], dtype=float32), -2.9008548]. 
=============================================
[2019-04-09 14:59:51,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00454355 0.10847158 0.17218457 0.04010995 0.0199173  0.00943563
 0.1606659  0.0567579  0.11950196 0.06838711 0.2400246 ], sum to 1.0000
[2019-04-09 14:59:51,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6877
[2019-04-09 14:59:51,788] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [13.3, 34.0, 92.33333333333334, 0.0, 22.5, 29.6687589597591, 1.443819326078768, 1.0, 1.0, 45.0, 0.1953444594006519], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [13.15, 34.5, 81.66666666666667, 0.0, 22.5, 29.81511875064276, 1.290426040978176, 1.0, 1.0, 55.0, 28.71740561313706], 
processed observation next is [1.0, 0.6521739130434783, 0.826869806094183, 0.345, 0.27222222222222225, 0.0, 0.375, 0.9845932292202301, 0.930142013659392, 1.0, 1.0, 0.8, 0.2871740561313706], 
reward next is 0.7128, 
noisyNet noise sample is [array([-1.4864519], dtype=float32), 1.2345109]. 
=============================================
[2019-04-09 14:59:51,801] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[5.0617857]
 [5.0843325]
 [5.0533185]
 [4.968591 ]
 [4.9487486]], R is [[5.74026203]
 [6.68090582]
 [7.60014582]
 [8.25531197]
 [9.17275906]].
[2019-04-09 14:59:52,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00521726 0.09005595 0.15148808 0.03227665 0.02004554 0.01176432
 0.27340198 0.05837611 0.07024528 0.07851429 0.20861454], sum to 1.0000
[2019-04-09 14:59:52,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5638
[2019-04-09 14:59:52,070] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.100000000000001, 61.33333333333334, 0.0, 0.0, 19.0, 28.26196935190073, 1.178169489573611, 0.0, 1.0, 60.0, 26.16236186361267], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4400400.0000, 
sim time next is 4401000.0000, 
raw observation next is [8.95, 61.5, 0.0, 0.0, 19.0, 28.17406178795201, 1.171634991629639, 0.0, 1.0, 45.0, 23.0212743037415], 
processed observation next is [1.0, 0.9565217391304348, 0.7105263157894738, 0.615, 0.0, 0.0, 0.08333333333333333, 0.8478384823293341, 0.8905449972098797, 0.0, 1.0, 0.6, 0.23021274303741499], 
reward next is 0.7698, 
noisyNet noise sample is [array([0.56958693], dtype=float32), 0.058732495]. 
=============================================
[2019-04-09 14:59:52,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[4.4796896]
 [4.6394215]
 [4.629735 ]
 [4.880021 ]
 [4.758408 ]], R is [[5.27132702]
 [5.95699024]
 [6.65023613]
 [7.41453457]
 [8.18643856]].
[2019-04-09 14:59:52,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00506523 0.10252866 0.11924768 0.04803259 0.02220339 0.00830306
 0.23587473 0.04981245 0.09139634 0.08727978 0.23025617], sum to 1.0000
[2019-04-09 14:59:52,304] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9306
[2019-04-09 14:59:52,319] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.350000000000001, 62.16666666666667, 0.0, 0.0, 19.0, 28.00100602482055, 1.143309658692778, 0.0, 1.0, 55.0, 28.5606927599233], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4403400.0000, 
sim time next is 4404000.0000, 
raw observation next is [8.2, 62.33333333333334, 0.0, 0.0, 19.0, 27.97438731789654, 1.137701524714426, 0.0, 1.0, 65.0, 27.19322247457557], 
processed observation next is [1.0, 1.0, 0.6897506925207757, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.831198943158045, 0.8792338415714753, 0.0, 1.0, 1.0, 0.2719322247457557], 
reward next is 0.7281, 
noisyNet noise sample is [array([2.9320886], dtype=float32), -0.117193215]. 
=============================================
[2019-04-09 14:59:52,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[4.203756 ]
 [4.41779  ]
 [4.486216 ]
 [4.594488 ]
 [4.7596197]], R is [[5.10163021]
 [5.76500702]
 [6.31663609]
 [7.02735806]
 [7.7344799 ]].
[2019-04-09 14:59:52,595] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0075238  0.11595144 0.12115482 0.0535649  0.02626677 0.00959944
 0.22381645 0.04161982 0.109084   0.06295538 0.22846311], sum to 1.0000
[2019-04-09 14:59:52,598] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6388
[2019-04-09 14:59:52,613] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [5.75, 59.83333333333333, 97.0, 553.0, 22.5, 27.92750826042694, 0.9522512274516101, 1.0, 1.0, 65.0, 22.85461747310803], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4351800.0000, 
sim time next is 4352400.0000, 
raw observation next is [6.3, 57.0, 99.5, 584.0, 22.5, 27.98393307751713, 0.966460435452956, 1.0, 1.0, 55.0, 22.31132074168992], 
processed observation next is [1.0, 0.391304347826087, 0.6371191135734073, 0.57, 0.33166666666666667, 0.6453038674033149, 0.375, 0.8319944231264275, 0.8221534784843186, 1.0, 1.0, 0.8, 0.2231132074168992], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.5178993], dtype=float32), 0.6872207]. 
=============================================
[2019-04-09 14:59:52,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00208879 0.13612017 0.11275741 0.04049097 0.02570021 0.00506506
 0.12294899 0.05291215 0.11136775 0.09633522 0.29421324], sum to 1.0000
[2019-04-09 14:59:52,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2387
[2019-04-09 14:59:52,920] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 22.5, 28.48011329255558, 1.158048043975644, 1.0, 1.0, 20.0, 19.00015412680585], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 22.5, 28.49787072355763, 1.158555488627396, 1.0, 1.0, 25.0, 16.11177090930126], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.375, 0.8748225602964691, 0.8861851628757987, 1.0, 1.0, 0.2, 0.1611177090930126], 
reward next is 0.8389, 
noisyNet noise sample is [array([1.5137084], dtype=float32), 0.17007814]. 
=============================================
[2019-04-09 14:59:52,940] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.863096 ]
 [4.7852306]
 [4.721695 ]
 [4.5519533]
 [4.605726 ]], R is [[5.72147179]
 [6.47425556]
 [7.21377039]
 [7.97598791]
 [8.70509624]].
[2019-04-09 14:59:52,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00355809 0.11471257 0.1374488  0.03457859 0.0189476  0.00678229
 0.20198067 0.05245945 0.12082703 0.09148808 0.21721677], sum to 1.0000
[2019-04-09 14:59:52,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-09 14:59:53,006] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 19.0, 28.51840371044499, 1.272709067113988, 1.0, 1.0, 20.0, 13.56743058335324], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4392600.0000, 
sim time next is 4393200.0000, 
raw observation next is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 19.0, 28.49370807460502, 1.267329680817512, 0.0, 1.0, 45.0, 14.55379464414091], 
processed observation next is [1.0, 0.8695652173913043, 0.7599261311172669, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.8744756728837517, 0.9224432269391706, 0.0, 1.0, 0.6, 0.1455379464414091], 
reward next is 0.8545, 
noisyNet noise sample is [array([-0.40576088], dtype=float32), -0.6635104]. 
=============================================
[2019-04-09 14:59:53,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00578487 0.12388569 0.12458321 0.04539992 0.01897051 0.00818906
 0.19333057 0.05314225 0.0871626  0.07953024 0.2600211 ], sum to 1.0000
[2019-04-09 14:59:53,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-09 14:59:53,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 22.5, 27.77784018371631, 1.043183207638149, 1.0, 1.0, 20.0, 21.99457954698448], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 22.5, 28.00101328628576, 1.055456297664267, 1.0, 1.0, 20.0, 19.64873171317353], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 0.375, 0.8334177738571468, 0.851818765888089, 1.0, 1.0, 0.1, 0.19648731713173528], 
reward next is 0.8035, 
noisyNet noise sample is [array([1.1218226], dtype=float32), 1.2423671]. 
=============================================
[2019-04-09 14:59:53,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00449166 0.12184954 0.13834488 0.0584955  0.0294286  0.00981824
 0.18819268 0.04723869 0.09080938 0.07528269 0.23604815], sum to 1.0000
[2019-04-09 14:59:53,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-09 14:59:53,355] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 22.5, 28.0045989249623, 1.057422786659321, 1.0, 1.0, 20.0, 19.67559319056754], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4437600.0000, 
sim time next is 4438200.0000, 
raw observation next is [1.416666666666667, 83.33333333333333, 135.0, 165.0, 22.5, 28.10080738460773, 1.078676458805488, 1.0, 1.0, 45.0, 21.42153276028397], 
processed observation next is [1.0, 0.34782608695652173, 0.5018467220683288, 0.8333333333333333, 0.45, 0.18232044198895028, 0.375, 0.8417339487173109, 0.8595588196018293, 1.0, 1.0, 0.6, 0.2142153276028397], 
reward next is 0.7858, 
noisyNet noise sample is [array([-0.30454352], dtype=float32), -0.7742105]. 
=============================================
[2019-04-09 14:59:53,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00286388 0.12107433 0.18271284 0.04854513 0.01617151 0.0064962
 0.14329894 0.03872448 0.11701013 0.07222522 0.25087732], sum to 1.0000
[2019-04-09 14:59:53,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6412
[2019-04-09 14:59:53,960] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [12.2, 47.0, 0.0, 0.0, 22.5, 28.37692204349419, 1.31019943747192, 1.0, 1.0, 60.0, 9.10101371874042], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4386600.0000, 
sim time next is 4387200.0000, 
raw observation next is [12.13333333333333, 48.0, 0.0, 0.0, 22.5, 28.62354652544928, 1.334974333567965, 1.0, 1.0, 55.0, 7.000229677717338], 
processed observation next is [1.0, 0.782608695652174, 0.7987072945521699, 0.48, 0.0, 0.0, 0.375, 0.8852955437874401, 0.944991444522655, 1.0, 1.0, 0.8, 0.07000229677717339], 
reward next is 0.9300, 
noisyNet noise sample is [array([0.18979058], dtype=float32), 0.21116953]. 
=============================================
[2019-04-09 14:59:54,026] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00306358 0.12482739 0.17916185 0.05013402 0.0178027  0.00701787
 0.14849982 0.03794776 0.11213262 0.07391216 0.24550031], sum to 1.0000
[2019-04-09 14:59:54,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1228
[2019-04-09 14:59:54,035] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.0, 50.0, 0.0, 0.0, 22.5, 28.99422163507832, 1.339043925998111, 1.0, 1.0, 45.0, 6.490181172212894], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4388400.0000, 
sim time next is 4389000.0000, 
raw observation next is [11.83333333333333, 51.33333333333334, 0.0, 0.0, 22.5, 28.97260264987916, 1.330420505480067, 1.0, 1.0, 20.0, 9.164345078909248], 
processed observation next is [1.0, 0.8260869565217391, 0.7903970452446908, 0.5133333333333334, 0.0, 0.0, 0.375, 0.9143835541565967, 0.943473501826689, 1.0, 1.0, 0.1, 0.09164345078909247], 
reward next is 0.9084, 
noisyNet noise sample is [array([0.18979058], dtype=float32), 0.21116953]. 
=============================================
[2019-04-09 14:59:54,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[5.1610637]
 [5.015451 ]
 [5.1467614]
 [5.313148 ]
 [5.233789 ]], R is [[5.93368149]
 [6.809443  ]
 [7.69160509]
 [8.54468727]
 [9.36823082]].
[2019-04-09 14:59:54,085] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 118514: loss 20.8814
[2019-04-09 14:59:54,088] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 118514: learning rate 0.0000
[2019-04-09 14:59:54,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00301557 0.14762254 0.10300552 0.0312593  0.01584289 0.00614963
 0.1421935  0.04755182 0.11187976 0.08800745 0.30347207], sum to 1.0000
[2019-04-09 14:59:54,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1014
[2019-04-09 14:59:54,250] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 196.5, 73.0, 22.5, 28.4956615316148, 1.15885321451993, 1.0, 1.0, 20.0, 15.62049789267457], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4446000.0000, 
sim time next is 4446600.0000, 
raw observation next is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 22.5, 28.50999501505244, 1.159227708382113, 1.0, 1.0, 20.0, 17.44987441319581], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5944444444444443, 0.05377532228360957, 0.375, 0.8758329179210366, 0.886409236127371, 1.0, 1.0, 0.1, 0.17449874413195812], 
reward next is 0.8255, 
noisyNet noise sample is [array([-0.5544305], dtype=float32), -0.5387351]. 
=============================================
[2019-04-09 14:59:54,288] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00297016 0.10140873 0.12163583 0.04240451 0.01862729 0.00776552
 0.16469198 0.05043868 0.11612558 0.06454381 0.30938792], sum to 1.0000
[2019-04-09 14:59:54,291] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4164
[2019-04-09 14:59:54,304] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.21317860373399, 0.9293631930456486, 0.0, 1.0, 65.0, 38.67347401393553], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.20118762365241, 0.9297576539473243, 0.0, 1.0, 45.0, 35.90315917677778], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7667656353043674, 0.8099192179824414, 0.0, 1.0, 0.6, 0.35903159176777777], 
reward next is 0.6410, 
noisyNet noise sample is [array([0.9252669], dtype=float32), -0.86148554]. 
=============================================
[2019-04-09 14:59:54,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00213592 0.130838   0.10072157 0.04593957 0.01775729 0.00671825
 0.1795853  0.04006054 0.12602006 0.09123982 0.25898364], sum to 1.0000
[2019-04-09 14:59:54,316] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2898
[2019-04-09 14:59:54,328] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 22.5, 28.33827892707598, 1.136429567307143, 1.0, 1.0, 60.0, 23.45817101306697], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 22.5, 28.36854095281183, 1.134612069115483, 1.0, 1.0, 35.0, 22.95203873983209], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 0.375, 0.8640450794009858, 0.8782040230384943, 1.0, 1.0, 0.4, 0.2295203873983209], 
reward next is 0.7705, 
noisyNet noise sample is [array([-1.0551591], dtype=float32), -0.25529906]. 
=============================================
[2019-04-09 14:59:54,334] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00344634 0.1054037  0.20107768 0.03188366 0.01580846 0.00699457
 0.21127146 0.04137691 0.0946375  0.05140108 0.23669869], sum to 1.0000
[2019-04-09 14:59:54,335] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8627
[2019-04-09 14:59:54,344] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.2, 59.0, 0.0, 0.0, 19.0, 28.47009475401106, 1.26038355333653, 0.0, 1.0, 20.0, 15.3508267861908], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4395600.0000, 
sim time next is 4396200.0000, 
raw observation next is [10.06666666666667, 59.33333333333334, 0.0, 0.0, 19.0, 28.45875716160025, 1.254700495477617, 0.0, 1.0, 65.0, 17.20230711727293], 
processed observation next is [1.0, 0.9130434782608695, 0.7414589104339798, 0.5933333333333334, 0.0, 0.0, 0.08333333333333333, 0.8715630968000209, 0.918233498492539, 0.0, 1.0, 1.0, 0.1720230711727293], 
reward next is 0.8280, 
noisyNet noise sample is [array([0.28773803], dtype=float32), -0.54308414]. 
=============================================
[2019-04-09 14:59:54,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00251234 0.12241907 0.122584   0.03865262 0.02421217 0.0049216
 0.21683091 0.04911048 0.11846366 0.06903816 0.23125508], sum to 1.0000
[2019-04-09 14:59:54,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-09 14:59:54,455] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.94568169694406, 0.8949472736758676, 1.0, 1.0, 30.0, 70.86672498665777], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.00094452170035, 0.9963637489932546, 1.0, 1.0, 45.0, 41.44083936313103], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.7500787101416959, 0.8321212496644182, 1.0, 1.0, 0.6, 0.4144083936313103], 
reward next is 0.5856, 
noisyNet noise sample is [array([-0.07746704], dtype=float32), -0.3520557]. 
=============================================
[2019-04-09 14:59:54,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0032748  0.13061891 0.1273597  0.04338205 0.01898229 0.00703849
 0.15425201 0.04211626 0.09269132 0.0788435  0.30144066], sum to 1.0000
[2019-04-09 14:59:54,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3658
[2019-04-09 14:59:54,503] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 22.5, 28.32987699711698, 1.127303850809369, 1.0, 1.0, 45.0, 22.94432585628099], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4452000.0000, 
sim time next is 4452600.0000, 
raw observation next is [0.1666666666666666, 91.0, 133.3333333333333, 2.0, 22.5, 28.25032802048488, 1.137094174530143, 1.0, 1.0, 45.0, 24.3584012265389], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.91, 0.4444444444444443, 0.0022099447513812156, 0.375, 0.8541940017070733, 0.8790313915100477, 1.0, 1.0, 0.6, 0.24358401226538898], 
reward next is 0.7564, 
noisyNet noise sample is [array([-0.02671576], dtype=float32), 1.5617652]. 
=============================================
[2019-04-09 14:59:54,507] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00784982 0.09334254 0.10768224 0.04099591 0.02527893 0.01387252
 0.20683698 0.07225487 0.08521342 0.06507845 0.28159437], sum to 1.0000
[2019-04-09 14:59:54,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2488
[2019-04-09 14:59:54,526] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8833333333333333, 72.66666666666667, 0.0, 0.0, 19.0, 26.8540934735882, 0.7775303009838889, 0.0, 1.0, 25.0, 40.93181857455757], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4507800.0000, 
sim time next is 4508400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 26.79893982734416, 0.7728894065535842, 0.0, 1.0, 30.0, 39.18401694736848], 
processed observation next is [1.0, 0.17391304347826086, 0.4385964912280702, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7332449856120133, 0.7576298021845281, 0.0, 1.0, 0.3, 0.3918401694736848], 
reward next is 0.6082, 
noisyNet noise sample is [array([-0.46790165], dtype=float32), -1.3825259]. 
=============================================
[2019-04-09 14:59:54,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00281952 0.13660009 0.1238161  0.04203005 0.02101114 0.00740412
 0.1786671  0.04427806 0.10652508 0.09770359 0.23914517], sum to 1.0000
[2019-04-09 14:59:54,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4020
[2019-04-09 14:59:54,695] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.2569845584521, 0.9425051020532087, 1.0, 1.0, 25.0, 35.19263541149479], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.24391214235428, 0.9387015028321247, 1.0, 1.0, 45.0, 31.98981641188595], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.7703260118628567, 0.8129005009440416, 1.0, 1.0, 0.6, 0.3198981641188595], 
reward next is 0.6801, 
noisyNet noise sample is [array([0.7569542], dtype=float32), -0.45187774]. 
=============================================
[2019-04-09 14:59:54,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00285813 0.13224475 0.1407431  0.04091714 0.01423773 0.00503127
 0.17846324 0.03682736 0.11057786 0.0795572  0.25854224], sum to 1.0000
[2019-04-09 14:59:54,808] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2583
[2019-04-09 14:59:54,810] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00475613 0.0899462  0.11666819 0.03337535 0.01752639 0.00843908
 0.20768873 0.04306239 0.10023232 0.07092674 0.30737847], sum to 1.0000
[2019-04-09 14:59:54,812] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6709
[2019-04-09 14:59:54,826] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.4, 72.5, 0.0, 0.0, 19.0, 26.68323608194023, 0.8007166212271205, 0.0, 1.0, 55.0, 44.03758849518614], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4491000.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 19.0, 26.67740727954437, 0.802111587212571, 0.0, 1.0, 60.0, 48.64338298824308], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.723117273295364, 0.767370529070857, 0.0, 1.0, 0.9, 0.48643382988243083], 
reward next is 0.5136, 
noisyNet noise sample is [array([0.33094823], dtype=float32), 0.14273137]. 
=============================================
[2019-04-09 14:59:54,855] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118928: loss 15.5546
[2019-04-09 14:59:54,857] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 78.0, 41.0, 18.33333333333333, 22.5, 28.2462133306924, 1.092277710075982, 1.0, 1.0, 25.0, 26.26634588123331], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4467000.0000, 
sim time next is 4467600.0000, 
raw observation next is [0.0, 78.0, 37.0, 27.5, 22.5, 28.27034813355349, 0.935610860309034, 1.0, 1.0, 25.0, 68.92686530867442], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.78, 0.12333333333333334, 0.03038674033149171, 0.375, 0.8558623444627909, 0.811870286769678, 1.0, 1.0, 0.2, 0.6892686530867442], 
reward next is 0.3107, 
noisyNet noise sample is [array([-0.51343083], dtype=float32), -1.0559841]. 
=============================================
[2019-04-09 14:59:54,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118928: learning rate 0.0000
[2019-04-09 14:59:55,083] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00647334 0.1098392  0.10788355 0.06126025 0.0232124  0.0103918
 0.16357708 0.05752845 0.08519793 0.08858802 0.286048  ], sum to 1.0000
[2019-04-09 14:59:55,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-09 14:59:55,102] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 80.0, 20.0, 38.66666666666666, 22.5, 27.52727720212597, 0.970314143792281, 1.0, 1.0, 50.0, 29.30856244739957], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4434000.0000, 
sim time next is 4434600.0000, 
raw observation next is [2.0, 80.0, 39.99999999999999, 77.33333333333331, 22.5, 27.5775971504894, 0.9736876122458394, 1.0, 1.0, 20.0, 28.500075053078], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.1333333333333333, 0.08545119705340698, 0.375, 0.7981330958741166, 0.8245625374152797, 1.0, 1.0, 0.1, 0.28500075053078], 
reward next is 0.7150, 
noisyNet noise sample is [array([-0.55285764], dtype=float32), 0.37494883]. 
=============================================
[2019-04-09 14:59:55,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00380051 0.11196806 0.1356904  0.04318642 0.03039771 0.00794403
 0.16610801 0.0531697  0.11080668 0.07737584 0.2595526 ], sum to 1.0000
[2019-04-09 14:59:55,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8438
[2019-04-09 14:59:55,122] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 251.0, 146.0, 22.5, 28.45450146599662, 1.157574736614209, 1.0, 1.0, 30.0, 16.21000742605431], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4444200.0000, 
sim time next is 4444800.0000, 
raw observation next is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 22.5, 28.4763937652642, 1.156655809129278, 1.0, 1.0, 65.0, 19.62904482591543], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.776111111111111, 0.134438305709024, 0.375, 0.8730328137720166, 0.885551936376426, 1.0, 1.0, 1.0, 0.1962904482591543], 
reward next is 0.8037, 
noisyNet noise sample is [array([1.1236346], dtype=float32), -0.89639753]. 
=============================================
[2019-04-09 14:59:55,248] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00932251 0.10315894 0.10873843 0.03893286 0.02315633 0.01352622
 0.1899742  0.04454772 0.08068003 0.06616498 0.3217978 ], sum to 1.0000
[2019-04-09 14:59:55,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0111
[2019-04-09 14:59:55,262] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 119148: loss 19.4321
[2019-04-09 14:59:55,263] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 119149: learning rate 0.0000
[2019-04-09 14:59:55,283] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.366666666666667, 66.66666666666667, 0.0, 0.0, 19.0, 27.88975251595546, 1.075751670520425, 0.0, 1.0, 25.0, 25.73812640801219], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4416000.0000, 
sim time next is 4416600.0000, 
raw observation next is [5.183333333333334, 66.83333333333333, 0.0, 0.0, 19.0, 27.8868928220317, 1.067219075159567, 0.0, 1.0, 20.0, 23.64089645023655], 
processed observation next is [1.0, 0.08695652173913043, 0.6061865189289013, 0.6683333333333333, 0.0, 0.0, 0.08333333333333333, 0.8239077351693084, 0.8557396917198558, 0.0, 1.0, 0.1, 0.2364089645023655], 
reward next is 0.7636, 
noisyNet noise sample is [array([-2.096175], dtype=float32), 1.089711]. 
=============================================
[2019-04-09 14:59:55,655] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00364791 0.14601603 0.14185223 0.04996849 0.03254486 0.00702989
 0.16916859 0.04318169 0.12590063 0.08878881 0.19190091], sum to 1.0000
[2019-04-09 14:59:55,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3061
[2019-04-09 14:59:55,671] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 59.0, 221.0, 18.0, 22.5, 27.82074853984059, 0.9533204332593561, 1.0, 1.0, 60.0, 21.40649287518659], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4530600.0000, 
sim time next is 4531200.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 22.5, 27.86245835149512, 0.9557452321512173, 1.0, 1.0, 30.0, 21.16919677426169], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5833333333333335, 0.6794444444444443, 0.016574585635359115, 0.375, 0.8218715292912601, 0.8185817440504057, 1.0, 1.0, 0.3, 0.2116919677426169], 
reward next is 0.7883, 
noisyNet noise sample is [array([0.22846568], dtype=float32), 0.009611587]. 
=============================================
[2019-04-09 14:59:55,729] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00197138 0.10011927 0.09775636 0.03924111 0.01472726 0.00528648
 0.1823586  0.0342017  0.11232597 0.04811987 0.36389202], sum to 1.0000
[2019-04-09 14:59:55,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8531
[2019-04-09 14:59:55,759] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.58541908476522, 1.000423465639925, 0.0, 1.0, 50.0, 30.6389724488925], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4474200.0000, 
sim time next is 4474800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.51794132663147, 0.9927515444413202, 1.0, 1.0, 65.0, 36.5191841892785], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.793161777219289, 0.83091718148044, 1.0, 1.0, 1.0, 0.365191841892785], 
reward next is 0.6348, 
noisyNet noise sample is [array([-1.0948929], dtype=float32), -0.40889555]. 
=============================================
[2019-04-09 14:59:55,826] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119434: loss 19.4198
[2019-04-09 14:59:55,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119435: learning rate 0.0000
[2019-04-09 14:59:55,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00256438 0.10606752 0.12976095 0.03711686 0.02122686 0.00723868
 0.18194342 0.04063531 0.11900327 0.07657855 0.27786416], sum to 1.0000
[2019-04-09 14:59:55,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4333
[2019-04-09 14:59:55,892] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 122.5, 0.0, 22.5, 27.93902684887938, 0.9808727847439415, 1.0, 1.0, 30.0, 19.65312725116796], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4536000.0000, 
sim time next is 4536600.0000, 
raw observation next is [2.0, 48.66666666666666, 124.0, 0.0, 22.5, 27.97165858527624, 0.9827154114733102, 1.0, 1.0, 45.0, 20.9049841349891], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.4866666666666666, 0.41333333333333333, 0.0, 0.375, 0.8309715487730202, 0.8275718038244367, 1.0, 1.0, 0.6, 0.209049841349891], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.5135743], dtype=float32), -0.42301947]. 
=============================================
[2019-04-09 14:59:55,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00331736 0.10307629 0.12426085 0.03384586 0.02424786 0.0058099
 0.25194484 0.0391869  0.08037236 0.07349367 0.26044422], sum to 1.0000
[2019-04-09 14:59:55,933] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0367
[2019-04-09 14:59:55,956] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.51860606477421, 0.992799519310615, 1.0, 1.0, 35.0, 31.51495732993473], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4474800.0000, 
sim time next is 4475400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.45819636286383, 0.9840236060457759, 1.0, 1.0, 20.0, 32.16276928702351], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.7881830302386525, 0.8280078686819253, 1.0, 1.0, 0.1, 0.32162769287023507], 
reward next is 0.6784, 
noisyNet noise sample is [array([1.980653], dtype=float32), -0.074579306]. 
=============================================
[2019-04-09 14:59:56,068] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119551: loss 16.4178
[2019-04-09 14:59:56,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119551: learning rate 0.0000
[2019-04-09 14:59:56,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00896408 0.0683923  0.14738914 0.03576998 0.02322838 0.01311718
 0.26124856 0.0447595  0.06498465 0.06885591 0.26329035], sum to 1.0000
[2019-04-09 14:59:56,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9575
[2019-04-09 14:59:56,346] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.40702773232957, 0.7543245060091369, 0.0, 1.0, 65.0, 72.2713548703106], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4499400.0000, 
sim time next is 4500000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.47460357607726, 0.7675585926681187, 0.0, 1.0, 65.0, 52.75782496863455], 
processed observation next is [1.0, 0.08695652173913043, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.706216964673105, 0.7558528642227063, 0.0, 1.0, 1.0, 0.5275782496863455], 
reward next is 0.4724, 
noisyNet noise sample is [array([-0.65354174], dtype=float32), 0.5442056]. 
=============================================
[2019-04-09 14:59:56,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[3.695704 ]
 [3.638574 ]
 [3.524228 ]
 [3.4743285]
 [3.4590669]], R is [[3.89052415]
 [4.1289053 ]
 [4.5188756 ]
 [5.05257654]
 [5.4416914 ]].
[2019-04-09 14:59:56,357] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119697: loss 8.8905
[2019-04-09 14:59:56,360] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119698: learning rate 0.0000
[2019-04-09 14:59:56,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00439929 0.0675234  0.09801739 0.02707156 0.01831643 0.00939298
 0.21394373 0.0742825  0.08242108 0.08978576 0.3148458 ], sum to 1.0000
[2019-04-09 14:59:56,431] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3996
[2019-04-09 14:59:56,451] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.05, 72.0, 0.0, 0.0, 19.0, 27.13328853737552, 0.8802732492967729, 0.0, 1.0, 20.0, 42.66095683501458], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4486200.0000, 
sim time next is 4486800.0000, 
raw observation next is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 27.11648663270873, 0.8815919453300677, 0.0, 1.0, 45.0, 40.36850831050842], 
processed observation next is [1.0, 0.9565217391304348, 0.4598337950138504, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7597072193923943, 0.7938639817766893, 0.0, 1.0, 0.6, 0.40368508310508416], 
reward next is 0.5963, 
noisyNet noise sample is [array([-1.2632649], dtype=float32), -1.5273387]. 
=============================================
[2019-04-09 14:59:56,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00299379 0.09338885 0.14359917 0.03550746 0.02314924 0.00628634
 0.17433654 0.03958613 0.08036117 0.06346468 0.33732665], sum to 1.0000
[2019-04-09 14:59:56,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1677
[2019-04-09 14:59:56,522] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 119780: loss 14.1150
[2019-04-09 14:59:56,524] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 80.33333333333334, 67.33333333333334, 0.0, 22.5, 27.49930320909198, 1.098051215179307, 1.0, 1.0, 30.0, 33.47844145218374], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [0.0, 79.16666666666666, 63.66666666666666, 0.0, 22.5, 26.82937389389383, 0.97900195153192, 1.0, 1.0, 25.0, 13.77713134892817], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.7916666666666665, 0.2122222222222222, 0.0, 0.375, 0.7357811578244858, 0.8263339838439734, 1.0, 1.0, 0.2, 0.1377713134892817], 
reward next is 0.8622, 
noisyNet noise sample is [array([1.7669895], dtype=float32), -0.19895302]. 
=============================================
[2019-04-09 14:59:56,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 119781: learning rate 0.0000
[2019-04-09 14:59:56,623] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119840: loss 25.7066
[2019-04-09 14:59:56,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119840: learning rate 0.0000
[2019-04-09 14:59:56,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00263031 0.0861391  0.12312055 0.03534213 0.01214778 0.00606228
 0.2013775  0.04299416 0.12644736 0.07914469 0.28459415], sum to 1.0000
[2019-04-09 14:59:56,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5805
[2019-04-09 14:59:56,776] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00476636 0.07297717 0.11276757 0.02963882 0.01718657 0.01233677
 0.1811952  0.05239551 0.08650677 0.08464262 0.34558663], sum to 1.0000
[2019-04-09 14:59:56,777] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.27661565027527, 0.964579128227422, 0.0, 1.0, 25.0, 34.51002969016084], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.28282619535338, 0.9591297574636363, 1.0, 1.0, 55.0, 31.7544878060331], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.7735688496127816, 0.8197099191545454, 1.0, 1.0, 0.8, 0.317544878060331], 
reward next is 0.6825, 
noisyNet noise sample is [array([1.1591574], dtype=float32), 0.51697475]. 
=============================================
[2019-04-09 14:59:56,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0672
[2019-04-09 14:59:56,821] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 19.0, 26.88950119545327, 0.8518674454961387, 0.0, 1.0, 50.0, 39.05225914712617], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4494600.0000, 
sim time next is 4495200.0000, 
raw observation next is [-0.5666666666666667, 73.0, 0.0, 0.0, 19.0, 27.01624766103265, 0.8398028784373627, 0.0, 1.0, 20.0, 39.00098538856979], 
processed observation next is [1.0, 0.0, 0.44690674053554946, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7513539717527209, 0.7799342928124542, 0.0, 1.0, 0.1, 0.39000985388569787], 
reward next is 0.6100, 
noisyNet noise sample is [array([-0.23389466], dtype=float32), -0.8804576]. 
=============================================
[2019-04-09 14:59:56,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00361573 0.07679109 0.16272005 0.03949331 0.01655485 0.00451391
 0.24309446 0.04069072 0.09127237 0.04263619 0.27861723], sum to 1.0000
[2019-04-09 14:59:56,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9341
[2019-04-09 14:59:56,855] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.24187869735438, 0.9512617559827374, 0.0, 1.0, 25.0, 38.60225007638889], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4479600.0000, 
sim time next is 4480200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.22220175096074, 0.9483434606636264, 0.0, 1.0, 65.0, 38.87311759271752], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7685168125800615, 0.8161144868878755, 0.0, 1.0, 1.0, 0.3887311759271752], 
reward next is 0.6113, 
noisyNet noise sample is [array([0.07089608], dtype=float32), -1.4889419]. 
=============================================
[2019-04-09 14:59:56,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0114444  0.08754519 0.12264892 0.05210228 0.02632165 0.01615428
 0.17888458 0.0648753  0.06923602 0.10258486 0.2682025 ], sum to 1.0000
[2019-04-09 14:59:56,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0162
[2019-04-09 14:59:56,930] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 22.5, 26.57243790477989, 0.6956971690822398, 1.0, 1.0, 30.0, 29.74959175176447], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4519200.0000, 
sim time next is 4519800.0000, 
raw observation next is [-0.9, 72.0, 0.0, 0.0, 22.5, 26.56603058523321, 0.6807505599581748, 1.0, 1.0, 20.0, 28.24724492903448], 
processed observation next is [1.0, 0.30434782608695654, 0.43767313019390586, 0.72, 0.0, 0.0, 0.375, 0.7138358821027676, 0.7269168533193916, 1.0, 1.0, 0.1, 0.2824724492903448], 
reward next is 0.7175, 
noisyNet noise sample is [array([-0.00847619], dtype=float32), 1.1404524]. 
=============================================
[2019-04-09 14:59:56,933] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-09 14:59:56,941] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:59:56,942] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:56,943] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:59:56,944] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:59:56,946] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:56,947] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:56,953] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run13
[2019-04-09 14:59:56,970] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run13
[2019-04-09 14:59:56,970] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run13
[2019-04-09 15:00:28,950] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01783294], dtype=float32), 0.022808935]
[2019-04-09 15:00:28,950] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-1.317463699, 81.09028227, 0.0, 0.0, 19.0, 27.16921722323389, 1.003452134517998, 0.0, 1.0, 30.0, 33.16912879354776]
[2019-04-09 15:00:28,950] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:00:28,951] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.01356219 0.11252385 0.11735192 0.06821975 0.04020316 0.02158535
 0.18082418 0.05418108 0.08930507 0.07717065 0.22507277], sampled 0.8831644727341574
[2019-04-09 15:00:30,212] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.01783294], dtype=float32), 0.022808935]
[2019-04-09 15:00:30,212] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [3.9, 75.5, 0.0, 0.0, 22.5, 27.95104961752021, 1.150193564249455, 1.0, 1.0, 35.0, 21.58416674560987]
[2019-04-09 15:00:30,212] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:00:30,212] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00511452 0.14503896 0.13564223 0.05936798 0.03139915 0.01059387
 0.16001576 0.04883992 0.10848676 0.07595022 0.21955065], sampled 0.09196793490761557
[2019-04-09 15:01:41,414] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5664.5214 283070.9826 2848.6035
[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,450] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:41,551] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,753] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5420.4145 307572.0290 2451.3191
[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:47,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,145] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5332.1856 316382.5567 2070.6838
[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,165] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:49,268] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:50,168] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 120000, evaluation results [120000.0, 5420.414453050401, 307572.02896986715, 2451.319109421513, 5664.5214395060275, 283070.9825917054, 2848.6034526739677, 5332.185588205156, 316382.5567032769, 2070.683849732724]
[2019-04-09 15:01:50,244] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00632626 0.08193171 0.14306667 0.0327678  0.0180294  0.0133519
 0.20232168 0.05338995 0.06256651 0.06895709 0.31729096], sum to 1.0000
[2019-04-09 15:01:50,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7718
[2019-04-09 15:01:50,266] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.98360416022932, 0.8334024494370795, 0.0, 1.0, 65.0, 44.62225007472858], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4498800.0000, 
sim time next is 4499400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.99931723026513, 0.8206997424309616, 0.0, 1.0, 65.0, 41.61129643474496], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7499431025220943, 0.7735665808103205, 0.0, 1.0, 1.0, 0.4161129643474496], 
reward next is 0.5839, 
noisyNet noise sample is [array([0.58282244], dtype=float32), -0.64430374]. 
=============================================
[2019-04-09 15:01:50,287] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120073: loss 16.8509
[2019-04-09 15:01:50,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120073: learning rate 0.0000
[2019-04-09 15:01:50,307] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120082: loss 29.4807
[2019-04-09 15:01:50,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120082: learning rate 0.0000
[2019-04-09 15:01:50,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00305089 0.14077896 0.12874879 0.04206274 0.02012919 0.00570703
 0.17137507 0.03702686 0.14685225 0.05502058 0.24924761], sum to 1.0000
[2019-04-09 15:01:50,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5490
[2019-04-09 15:01:50,355] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 22.5, 28.05274170677977, 1.043236079386071, 1.0, 1.0, 55.0, 17.45147751776071], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [3.0, 47.0, 264.0, 113.0, 22.5, 28.09618627688457, 1.058351371695519, 1.0, 1.0, 55.0, 17.94338937431271], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47, 0.88, 0.12486187845303867, 0.375, 0.8413488564070475, 0.852783790565173, 1.0, 1.0, 0.8, 0.1794338937431271], 
reward next is 0.8206, 
noisyNet noise sample is [array([0.79114294], dtype=float32), -0.051812362]. 
=============================================
[2019-04-09 15:01:50,367] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[5.3189874]
 [5.264878 ]
 [5.108772 ]
 [5.278621 ]
 [5.164457 ]], R is [[6.10903358]
 [6.87342834]
 [7.53963947]
 [8.08064938]
 [8.8195734 ]].
[2019-04-09 15:01:50,401] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120136: loss 18.5516
[2019-04-09 15:01:50,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120136: learning rate 0.0000
[2019-04-09 15:01:50,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00738939 0.09393992 0.11995022 0.04438635 0.02179689 0.01025142
 0.16290691 0.04998174 0.11354794 0.08361083 0.2922384 ], sum to 1.0000
[2019-04-09 15:01:50,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3575
[2019-04-09 15:01:50,677] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9833333333333334, 73.0, 0.0, 0.0, 19.0, 26.9801866053525, 0.7984665100798495, 0.0, 1.0, 45.0, 33.17375300418332], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4504200.0000, 
sim time next is 4504800.0000, 
raw observation next is [-0.9666666666666668, 73.0, 0.0, 0.0, 19.0, 26.91309813646092, 0.78350739520718, 0.0, 1.0, 45.0, 33.13294138726143], 
processed observation next is [1.0, 0.13043478260869565, 0.43582640812557716, 0.73, 0.0, 0.0, 0.08333333333333333, 0.74275817803841, 0.7611691317357266, 0.0, 1.0, 0.6, 0.33132941387261433], 
reward next is 0.6687, 
noisyNet noise sample is [array([0.0953386], dtype=float32), -0.39651996]. 
=============================================
[2019-04-09 15:01:50,681] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00239107 0.10918814 0.1744797  0.03550171 0.0182693  0.00868761
 0.16200686 0.03805733 0.11251124 0.07149181 0.26741526], sum to 1.0000
[2019-04-09 15:01:50,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-09 15:01:50,701] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.39335604784662, 0.9579969082399725, 0.0, 1.0, 25.0, 27.54300851885001], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4564200.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.38039494321904, 0.9537842173140776, 1.0, 1.0, 45.0, 26.81176329772184], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.7816995786015868, 0.8179280724380259, 1.0, 1.0, 0.6, 0.2681176329772184], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.32630175], dtype=float32), -0.25681236]. 
=============================================
[2019-04-09 15:01:50,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00334933 0.12797318 0.09973536 0.07024126 0.01969678 0.00904505
 0.19667575 0.05554564 0.11364818 0.08089983 0.22318958], sum to 1.0000
[2019-04-09 15:01:50,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-09 15:01:50,771] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 59.0, 221.0, 18.0, 22.5, 27.80902162189381, 0.951053593088314, 1.0, 1.0, 45.0, 21.46716887041511], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4530600.0000, 
sim time next is 4531200.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 22.5, 27.85049302643708, 0.9534221213874615, 1.0, 1.0, 45.0, 21.31353913597063], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5833333333333335, 0.6794444444444443, 0.016574585635359115, 0.375, 0.8208744188697565, 0.8178073737958206, 1.0, 1.0, 0.6, 0.2131353913597063], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.3379772], dtype=float32), -0.7575433]. 
=============================================
[2019-04-09 15:01:50,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00391558 0.13587116 0.10077319 0.07782213 0.02073064 0.01078307
 0.1967178  0.0516225  0.10678457 0.0698145  0.22516482], sum to 1.0000
[2019-04-09 15:01:50,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9256
[2019-04-09 15:01:50,791] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 22.5, 27.85049302643708, 0.9534221213874615, 1.0, 1.0, 45.0, 21.31353913597063], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 22.5, 27.87285020022363, 0.9598247595093419, 1.0, 1.0, 65.0, 41.87309834412748], 
processed observation next is [1.0, 0.43478260869565216, 0.5133887349953832, 0.5766666666666665, 0.6222222222222223, 0.013259668508287293, 0.375, 0.8227375166853026, 0.819941586503114, 1.0, 1.0, 1.0, 0.4187309834412748], 
reward next is 0.5813, 
noisyNet noise sample is [array([1.3379772], dtype=float32), -0.7575433]. 
=============================================
[2019-04-09 15:01:50,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00469841 0.124755   0.13037592 0.05036946 0.02239449 0.00964749
 0.1636098  0.05457451 0.11306643 0.07822131 0.24828722], sum to 1.0000
[2019-04-09 15:01:50,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1915
[2019-04-09 15:01:50,976] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 61.0, 172.0, 9.0, 22.5, 27.62266665796757, 0.9068956789239199, 1.0, 1.0, 55.0, 22.68765951369189], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4528800.0000, 
sim time next is 4529400.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 188.3333333333333, 12.0, 22.5, 27.68330308588467, 0.9141912916216279, 1.0, 1.0, 25.0, 23.6412532071407], 
processed observation next is [1.0, 0.43478260869565216, 0.49492151431209613, 0.6033333333333334, 0.6277777777777777, 0.013259668508287293, 0.375, 0.8069419238237225, 0.8047304305405426, 1.0, 1.0, 0.2, 0.236412532071407], 
reward next is 0.7636, 
noisyNet noise sample is [array([-0.30431902], dtype=float32), -0.34932244]. 
=============================================
[2019-04-09 15:01:51,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00756512 0.09717871 0.14821081 0.04730611 0.02035709 0.01075754
 0.18100092 0.05116616 0.06999843 0.09152514 0.27493396], sum to 1.0000
[2019-04-09 15:01:51,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5787
[2019-04-09 15:01:51,148] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.9666666666666667, 71.0, 0.0, 0.0, 19.0, 26.78678117844832, 0.769766176355588, 0.0, 1.0, 30.0, 41.51639149800367], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4513800.0000, 
sim time next is 4514400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.84178803210683, 0.7604043262491182, 0.0, 1.0, 55.0, 33.63438055512999], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7368156693422359, 0.753468108749706, 0.0, 1.0, 0.8, 0.3363438055512999], 
reward next is 0.6637, 
noisyNet noise sample is [array([-0.23266068], dtype=float32), -1.1092968]. 
=============================================
[2019-04-09 15:01:51,153] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120522: loss 25.6984
[2019-04-09 15:01:51,155] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120523: learning rate 0.0000
[2019-04-09 15:01:51,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00753966 0.0962894  0.15366034 0.04135375 0.02282605 0.00920137
 0.20430519 0.0497227  0.05241463 0.07628431 0.28640267], sum to 1.0000
[2019-04-09 15:01:51,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-09 15:01:51,312] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.81552011855387, 0.7570420543453303, 0.0, 1.0, 65.0, 41.2511373677156], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.82830100088233, 0.7468233485626898, 0.0, 1.0, 65.0, 43.2879625008893], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7356917500735275, 0.7489411161875633, 0.0, 1.0, 1.0, 0.432879625008893], 
reward next is 0.5671, 
noisyNet noise sample is [array([-0.06740909], dtype=float32), 0.88230884]. 
=============================================
[2019-04-09 15:01:51,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[3.7425826]
 [3.4251795]
 [3.7798157]
 [3.496376 ]
 [3.5566022]], R is [[4.05807734]
 [4.60498524]
 [5.15193462]
 [5.64646816]
 [6.16599226]].
[2019-04-09 15:01:51,395] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120645: loss 23.9977
[2019-04-09 15:01:51,399] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120648: learning rate 0.0000
[2019-04-09 15:01:51,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00208503 0.10990191 0.14813845 0.03528677 0.01861935 0.00738084
 0.16385365 0.04057155 0.11356981 0.04702969 0.31356296], sum to 1.0000
[2019-04-09 15:01:51,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3321
[2019-04-09 15:01:51,594] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 48.33333333333334, 258.3333333333334, 91.33333333333334, 22.5, 28.07725298792875, 1.030497276869529, 1.0, 1.0, 50.0, 22.17818595198642], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4543800.0000, 
sim time next is 4544400.0000, 
raw observation next is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 22.5, 28.05706806950404, 1.044700270808595, 1.0, 1.0, 45.0, 20.00311879529329], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47666666666666674, 0.8705555555555552, 0.11289134438305713, 0.375, 0.8380890057920034, 0.8482334236028649, 1.0, 1.0, 0.6, 0.2000311879529329], 
reward next is 0.8000, 
noisyNet noise sample is [array([-0.78278923], dtype=float32), 0.89603984]. 
=============================================
[2019-04-09 15:01:51,604] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120752: loss 16.4254
[2019-04-09 15:01:51,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120752: learning rate 0.0000
[2019-04-09 15:01:51,634] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00264018 0.15505154 0.11666808 0.05556588 0.0234936  0.00592507
 0.26028132 0.04651052 0.10532293 0.05053528 0.1780055 ], sum to 1.0000
[2019-04-09 15:01:51,634] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6940
[2019-04-09 15:01:51,644] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 47.0, 264.0, 113.0, 22.5, 28.10038560961971, 1.059502864689504, 1.0, 1.0, 30.0, 17.64158245264546], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 22.5, 28.17650722917637, 1.066514435455914, 1.0, 1.0, 55.0, 16.27704681209761], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.46333333333333343, 0.8183333333333334, 0.10626151012891345, 0.375, 0.8480422690980308, 0.8555048118186379, 1.0, 1.0, 0.8, 0.1627704681209761], 
reward next is 0.8372, 
noisyNet noise sample is [array([0.02642141], dtype=float32), -0.96585065]. 
=============================================
[2019-04-09 15:01:51,789] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00273317 0.15479027 0.13281938 0.032793   0.01630487 0.00748106
 0.16409148 0.03879577 0.10844461 0.07997797 0.26176837], sum to 1.0000
[2019-04-09 15:01:51,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5745
[2019-04-09 15:01:51,813] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.0, 52.0, 187.0, 24.0, 22.5, 26.19701768495214, 0.8448326285381164, 1.0, 1.0, 20.0, 11.65693699587921], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4539600.0000, 
sim time next is 4540200.0000, 
raw observation next is [2.166666666666667, 51.5, 207.0, 32.0, 22.5, 27.4354107790744, 0.9479792738347069, 1.0, 1.0, 55.0, 19.16869810925607], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.69, 0.03535911602209945, 0.375, 0.7862842315895332, 0.8159930912782357, 1.0, 1.0, 0.8, 0.1916869810925607], 
reward next is 0.8083, 
noisyNet noise sample is [array([-1.6096995], dtype=float32), 1.7842491]. 
=============================================
[2019-04-09 15:01:51,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00317167 0.07256714 0.13837923 0.03182926 0.02221861 0.00802168
 0.23370719 0.03813119 0.06888891 0.05102599 0.33205912], sum to 1.0000
[2019-04-09 15:01:51,886] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9761
[2019-04-09 15:01:51,899] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.37971920493235, 0.9466152691412789, 1.0, 1.0, 25.0, 23.56334344324017], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4564800.0000, 
sim time next is 4565400.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 19.0, 27.36044746610841, 0.9473816326016286, 1.0, 1.0, 65.0, 31.4799713801121], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.08333333333333333, 0.7800372888423676, 0.8157938775338761, 1.0, 1.0, 1.0, 0.314799713801121], 
reward next is 0.6852, 
noisyNet noise sample is [array([-0.6083139], dtype=float32), -1.2410989]. 
=============================================
[2019-04-09 15:01:52,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00223578 0.09337471 0.15555455 0.04215657 0.01995746 0.0055075
 0.14332426 0.03634978 0.08559234 0.1286928  0.28725427], sum to 1.0000
[2019-04-09 15:01:52,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2991
[2019-04-09 15:01:52,450] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 48.33333333333334, 258.3333333333334, 91.33333333333334, 22.5, 28.08244778942154, 1.032537383689031, 1.0, 1.0, 25.0, 22.36696157653045], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4543800.0000, 
sim time next is 4544400.0000, 
raw observation next is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 22.5, 28.06278492595613, 1.046604999740385, 1.0, 1.0, 35.0, 19.82037471689015], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47666666666666674, 0.8705555555555552, 0.11289134438305713, 0.375, 0.8385654104963441, 0.848868333246795, 1.0, 1.0, 0.4, 0.1982037471689015], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.9093716], dtype=float32), -1.2732269]. 
=============================================
[2019-04-09 15:01:52,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00229423 0.10587975 0.1263902  0.03860476 0.01209429 0.00509222
 0.19533211 0.03738985 0.09266951 0.06399828 0.32025474], sum to 1.0000
[2019-04-09 15:01:52,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0436
[2019-04-09 15:01:52,600] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 22.5, 28.2672940018956, 0.9238817271136212, 1.0, 1.0, 45.0, 63.98298425800963], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4552800.0000, 
sim time next is 4553400.0000, 
raw observation next is [2.0, 51.33333333333334, 82.0, 54.66666666666667, 22.5, 27.60691707156543, 1.038758195137837, 1.0, 1.0, 20.0, 25.89158887670735], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5133333333333334, 0.2733333333333333, 0.060405156537753225, 0.375, 0.8005764226304525, 0.8462527317126124, 1.0, 1.0, 0.1, 0.2589158887670735], 
reward next is 0.7411, 
noisyNet noise sample is [array([1.5757315], dtype=float32), 0.93764454]. 
=============================================
[2019-04-09 15:01:52,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00383339 0.11315167 0.19390322 0.04117168 0.01561197 0.00656327
 0.15906873 0.04650634 0.11135429 0.07148047 0.23735489], sum to 1.0000
[2019-04-09 15:01:52,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2932
[2019-04-09 15:01:52,654] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.0, 45.0, 208.5, 62.5, 22.5, 27.63333753101083, 1.055254244076604, 1.0, 1.0, 20.0, 23.19818415898262], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [2.833333333333333, 45.5, 190.0, 45.66666666666666, 22.5, 27.07007176030917, 0.9680514118828589, 1.0, 1.0, 25.0, 4.00396637122956], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.6333333333333333, 0.05046040515653774, 0.375, 0.7558393133590974, 0.822683803960953, 1.0, 1.0, 0.2, 0.0400396637122956], 
reward next is 0.9600, 
noisyNet noise sample is [array([0.45604163], dtype=float32), 0.3819962]. 
=============================================
[2019-04-09 15:01:52,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00533148 0.07795039 0.12893909 0.0263096  0.01654912 0.00910103
 0.20145296 0.06413747 0.0718593  0.08845817 0.3099114 ], sum to 1.0000
[2019-04-09 15:01:52,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9187
[2019-04-09 15:01:52,692] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.09640662932908, 0.8465616342580146, 0.0, 1.0, 65.0, 40.85593789730533], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4497000.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.1065683361851, 0.8340846942125267, 0.0, 1.0, 65.0, 41.67851998046334], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7588806946820915, 0.7780282314041757, 0.0, 1.0, 1.0, 0.4167851998046334], 
reward next is 0.5832, 
noisyNet noise sample is [array([1.99444], dtype=float32), 1.0804111]. 
=============================================
[2019-04-09 15:01:52,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00827412 0.11398187 0.12833402 0.03850304 0.0239899  0.012357
 0.19799718 0.05807624 0.08837681 0.0913819  0.23872793], sum to 1.0000
[2019-04-09 15:01:52,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1712
[2019-04-09 15:01:52,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 27.03861432522421, 0.7855484752473995, 0.0, 1.0, 45.0, 29.24580942323948], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4592400.0000, 
sim time next is 4593000.0000, 
raw observation next is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 19.0, 27.02059929454794, 0.7771825847779029, 0.0, 1.0, 25.0, 28.33326832041103], 
processed observation next is [1.0, 0.13043478260869565, 0.4228993536472761, 0.6866666666666665, 0.0, 0.0, 0.08333333333333333, 0.7517166078789949, 0.7590608615926343, 0.0, 1.0, 0.2, 0.2833326832041103], 
reward next is 0.7167, 
noisyNet noise sample is [array([2.397892], dtype=float32), -0.91800505]. 
=============================================
[2019-04-09 15:01:52,761] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[3.6514587]
 [3.6821918]
 [3.7369227]
 [3.8036494]
 [3.712367 ]], R is [[4.36928225]
 [5.0331316 ]
 [5.68711948]
 [6.27563524]
 [6.86817265]].
[2019-04-09 15:01:52,815] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 121367: loss 20.0079
[2019-04-09 15:01:52,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 121370: learning rate 0.0000
[2019-04-09 15:01:52,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00876467 0.08824033 0.13404474 0.04029552 0.02531621 0.01512828
 0.24217036 0.05862831 0.07935384 0.07859796 0.22945975], sum to 1.0000
[2019-04-09 15:01:52,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4882
[2019-04-09 15:01:52,955] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 19.0, 26.48092603878045, 0.6878052645729179, 0.0, 1.0, 20.0, 46.12303929996389], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4597200.0000, 
sim time next is 4597800.0000, 
raw observation next is [-2.1, 71.5, 0.0, 0.0, 19.0, 26.45836172815338, 0.6786997348667257, 0.0, 1.0, 45.0, 27.09906992288569], 
processed observation next is [1.0, 0.21739130434782608, 0.404432132963989, 0.715, 0.0, 0.0, 0.08333333333333333, 0.7048634773461151, 0.7262332449555752, 0.0, 1.0, 0.6, 0.2709906992288569], 
reward next is 0.7290, 
noisyNet noise sample is [array([1.2475361], dtype=float32), 0.047701087]. 
=============================================
[2019-04-09 15:01:52,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00460156 0.07739121 0.12969995 0.04096463 0.024818   0.00758928
 0.20746164 0.05322825 0.0749883  0.07252251 0.30673462], sum to 1.0000
[2019-04-09 15:01:52,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-09 15:01:52,991] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 19.0, 27.09500392057983, 0.8535738224841029, 0.0, 1.0, 25.0, 30.85246209941669], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4575000.0000, 
sim time next is 4575600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 19.0, 27.06212708256127, 0.8462656029698188, 0.0, 1.0, 30.0, 29.381844458435], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.08333333333333333, 0.7551772568801057, 0.782088534323273, 0.0, 1.0, 0.3, 0.29381844458435], 
reward next is 0.7062, 
noisyNet noise sample is [array([-1.1293744], dtype=float32), -2.3096752]. 
=============================================
[2019-04-09 15:01:53,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00977604 0.08221332 0.12187453 0.03676927 0.02480065 0.01223476
 0.2512492  0.05126289 0.08418814 0.07747895 0.24815226], sum to 1.0000
[2019-04-09 15:01:53,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2331
[2019-04-09 15:01:53,035] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 72.5, 0.0, 0.0, 19.0, 26.34776147096481, 0.683103662930464, 0.0, 1.0, 20.0, 41.32080783125853], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4599000.0000, 
sim time next is 4599600.0000, 
raw observation next is [-2.4, 73.0, 0.0, 0.0, 19.0, 26.38704613232607, 0.6897178943541787, 0.0, 1.0, 30.0, 28.248977044521], 
processed observation next is [1.0, 0.21739130434782608, 0.39612188365650974, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6989205110271725, 0.7299059647847262, 0.0, 1.0, 0.3, 0.28248977044521], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.7061878], dtype=float32), -1.1201974]. 
=============================================
[2019-04-09 15:01:53,056] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 121501: loss 23.7506
[2019-04-09 15:01:53,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 121501: learning rate 0.0000
[2019-04-09 15:01:53,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00307824 0.14598747 0.15353753 0.02797324 0.01895697 0.00432323
 0.17605515 0.04780479 0.10073907 0.07572744 0.24581687], sum to 1.0000
[2019-04-09 15:01:53,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8332
[2019-04-09 15:01:53,301] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 41.5, 34.66666666666666, 22.5, 28.23089049385311, 1.023282359465071, 1.0, 1.0, 25.0, 20.01758109232901], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4555200.0000, 
sim time next is 4555800.0000, 
raw observation next is [2.0, 52.0, 28.0, 28.0, 22.5, 28.16183064471969, 1.003179026679963, 1.0, 1.0, 65.0, 28.9645178877062], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.09333333333333334, 0.030939226519337018, 0.375, 0.8468192203933075, 0.834393008893321, 1.0, 1.0, 1.0, 0.289645178877062], 
reward next is 0.7104, 
noisyNet noise sample is [array([-0.68266064], dtype=float32), -0.56660324]. 
=============================================
[2019-04-09 15:01:53,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00314855 0.11497862 0.13323921 0.05999067 0.02043082 0.00500611
 0.1513992  0.04677765 0.12106793 0.07237954 0.27158177], sum to 1.0000
[2019-04-09 15:01:53,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9923
[2019-04-09 15:01:53,406] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 57.0, 169.5, 9.0, 22.5, 27.8997467086638, 0.9624816483089362, 1.0, 1.0, 30.0, 20.07971415902929], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4532400.0000, 
sim time next is 4533000.0000, 
raw observation next is [2.0, 55.5, 152.3333333333333, 5.999999999999998, 22.5, 27.92111495648228, 0.9657215205761701, 1.0, 1.0, 65.0, 24.78372630961937], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.555, 0.5077777777777777, 0.006629834254143645, 0.375, 0.8267595797068568, 0.82190717352539, 1.0, 1.0, 1.0, 0.24783726309619372], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.16462775], dtype=float32), 0.6203829]. 
=============================================
[2019-04-09 15:01:53,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[4.84513  ]
 [4.799701 ]
 [5.1678395]
 [4.810808 ]
 [4.755701 ]], R is [[5.65709543]
 [6.39972734]
 [7.08672237]
 [7.76867914]
 [8.4371233 ]].
[2019-04-09 15:01:53,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00237291 0.10058917 0.13075924 0.03212043 0.02074093 0.0080773
 0.16704029 0.03373371 0.1117472  0.0505839  0.34223488], sum to 1.0000
[2019-04-09 15:01:53,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1701
[2019-04-09 15:01:53,480] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.70528602187745, 0.9855485671456382, 1.0, 1.0, 20.0, 20.37628039546664], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4558800.0000, 
sim time next is 4559400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.81744787959818, 0.9853193715729739, 1.0, 1.0, 25.0, 23.65863105079458], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8181206566331817, 0.8284397905243246, 1.0, 1.0, 0.2, 0.2365863105079458], 
reward next is 0.7634, 
noisyNet noise sample is [array([-1.4437871], dtype=float32), 0.34734604]. 
=============================================
[2019-04-09 15:01:53,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00987352 0.088207   0.13113685 0.05659641 0.03137976 0.01194165
 0.20064133 0.06633734 0.08575011 0.07968242 0.23845363], sum to 1.0000
[2019-04-09 15:01:53,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7030
[2019-04-09 15:01:53,583] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.9666666666666667, 71.0, 0.0, 0.0, 19.0, 26.70728401869978, 0.7557285513420364, 0.0, 1.0, 60.0, 45.36113328216864], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4513800.0000, 
sim time next is 4514400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.74592494735603, 0.7513565340623477, 0.0, 1.0, 60.0, 45.00507874061057], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7288270789463359, 0.7504521780207826, 0.0, 1.0, 0.9, 0.4500507874061057], 
reward next is 0.5499, 
noisyNet noise sample is [array([1.516313], dtype=float32), 1.4612101]. 
=============================================
[2019-04-09 15:01:53,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00288265 0.12527543 0.14579093 0.04528712 0.02461643 0.0053326
 0.18024504 0.0438932  0.12219779 0.06697092 0.2375079 ], sum to 1.0000
[2019-04-09 15:01:53,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7340
[2019-04-09 15:01:53,695] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.833333333333333, 49.5, 252.6666666666667, 69.66666666666666, 22.5, 28.05224992641576, 1.020894892118936, 1.0, 1.0, 65.0, 20.93423161419836], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4542600.0000, 
sim time next is 4543200.0000, 
raw observation next is [3.0, 49.0, 255.5, 80.5, 22.5, 28.08081986197515, 1.022061767945426, 1.0, 1.0, 60.0, 20.66806932573524], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.8516666666666667, 0.08895027624309393, 0.375, 0.8400683218312626, 0.8406872559818087, 1.0, 1.0, 0.9, 0.2066806932573524], 
reward next is 0.7933, 
noisyNet noise sample is [array([-0.8004992], dtype=float32), 0.53906506]. 
=============================================
[2019-04-09 15:01:53,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00236729 0.08593412 0.20994923 0.04359692 0.0169801  0.00801414
 0.15809327 0.03945904 0.1310156  0.04540889 0.25918138], sum to 1.0000
[2019-04-09 15:01:53,975] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-09 15:01:53,982] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 22.5, 28.31194916749942, 1.174020109812065, 1.0, 1.0, 20.0, 2.700470590458936], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4628400.0000, 
sim time next is 4629000.0000, 
raw observation next is [4.583333333333334, 49.0, 160.3333333333333, 741.6666666666667, 22.5, 28.60397966691103, 1.201063531679032, 1.0, 1.0, 25.0, 3.29650849483994], 
processed observation next is [1.0, 0.5652173913043478, 0.5895660203139428, 0.49, 0.5344444444444443, 0.8195211786372009, 0.375, 0.8836649722425859, 0.9003545105596773, 1.0, 1.0, 0.2, 0.0329650849483994], 
reward next is 0.9670, 
noisyNet noise sample is [array([-0.59325784], dtype=float32), -0.3188559]. 
=============================================
[2019-04-09 15:01:53,990] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[5.5154967]
 [5.5506797]
 [5.6456246]
 [5.730732 ]
 [5.665585 ]], R is [[6.25987625]
 [7.17027283]
 [8.08736229]
 [8.86686325]
 [9.20950794]].
[2019-04-09 15:01:54,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00823156 0.11268248 0.10289583 0.03958619 0.02133106 0.01136532
 0.21984652 0.04957761 0.09229606 0.05755645 0.2846309 ], sum to 1.0000
[2019-04-09 15:01:54,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4472
[2019-04-09 15:01:54,589] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 19.0, 27.03799755612467, 0.7923608136875253, 0.0, 1.0, 60.0, 31.20737480513851], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4593000.0000, 
sim time next is 4593600.0000, 
raw observation next is [-1.5, 69.0, 0.0, 0.0, 19.0, 26.98703431512331, 0.783102282267334, 0.0, 1.0, 45.0, 33.09592535187495], 
processed observation next is [1.0, 0.17391304347826086, 0.4210526315789474, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7489195262602758, 0.7610340940891113, 0.0, 1.0, 0.6, 0.33095925351874955], 
reward next is 0.6690, 
noisyNet noise sample is [array([0.44549456], dtype=float32), -0.46332604]. 
=============================================
[2019-04-09 15:01:54,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00628004 0.0803248  0.1328763  0.03357358 0.02238659 0.00821229
 0.19319536 0.04802086 0.09795357 0.08924294 0.28793365], sum to 1.0000
[2019-04-09 15:01:54,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7752
[2019-04-09 15:01:54,622] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8, 61.66666666666667, 0.0, 0.0, 19.0, 27.04649813873476, 0.8395358911896373, 0.0, 1.0, 25.0, 32.57559286386997], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4580400.0000, 
sim time next is 4581000.0000, 
raw observation next is [0.7, 62.0, 0.0, 0.0, 19.0, 27.05042466740213, 0.8383454102474953, 0.0, 1.0, 65.0, 34.20475355579141], 
processed observation next is [1.0, 0.0, 0.4819944598337951, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7542020556168442, 0.7794484700824985, 0.0, 1.0, 1.0, 0.3420475355579141], 
reward next is 0.6580, 
noisyNet noise sample is [array([-2.2670338], dtype=float32), -0.23912972]. 
=============================================
[2019-04-09 15:01:54,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[3.9291105]
 [4.0511665]
 [4.14012  ]
 [4.067739 ]
 [4.1362343]], R is [[4.64630795]
 [5.27408886]
 [5.91539907]
 [6.54551363]
 [7.12304306]].
[2019-04-09 15:01:55,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00429013 0.14689551 0.13057452 0.06415049 0.02108997 0.00764052
 0.12182175 0.05091225 0.13242331 0.05930519 0.26089627], sum to 1.0000
[2019-04-09 15:01:55,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6965
[2019-04-09 15:01:55,473] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.3333333333333333, 58.66666666666666, 140.6666666666667, 681.0, 22.5, 27.94164365952079, 0.9970319376783273, 1.0, 1.0, 20.0, 17.2203376184815], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4615800.0000, 
sim time next is 4616400.0000, 
raw observation next is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 22.5, 28.02030767884697, 1.016915419694295, 1.0, 1.0, 60.0, 16.25468494957043], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.5733333333333335, 0.4494444444444443, 0.8, 0.375, 0.8350256399039141, 0.8389718065647651, 1.0, 1.0, 0.9, 0.16254684949570428], 
reward next is 0.8375, 
noisyNet noise sample is [array([-0.5661911], dtype=float32), 0.91739887]. 
=============================================
[2019-04-09 15:01:55,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00406155 0.21420546 0.1373777  0.03932487 0.01685165 0.00662288
 0.1564394  0.03617622 0.10160448 0.0523858  0.23494995], sum to 1.0000
[2019-04-09 15:01:55,526] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5148
[2019-04-09 15:01:55,550] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 56.0, 129.0, 767.0, 22.5, 28.08879661618066, 1.025972651644615, 1.0, 1.0, 35.0, 16.77886255925381], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4617000.0000, 
sim time next is 4617600.0000, 
raw observation next is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 22.5, 28.08642020406844, 0.796916933921367, 1.0, 1.0, 45.0, 42.37622477771772], 
processed observation next is [1.0, 0.43478260869565216, 0.4995383194829178, 0.5466666666666667, 0.426111111111111, 0.8596685082872928, 0.375, 0.8405350170057032, 0.765638977973789, 1.0, 1.0, 0.6, 0.4237622477771772], 
reward next is 0.5762, 
noisyNet noise sample is [array([1.0755795], dtype=float32), 1.3294758]. 
=============================================
[2019-04-09 15:01:55,650] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00391186 0.15079145 0.14946726 0.03841254 0.02873887 0.0063333
 0.15133174 0.03910631 0.10358789 0.06699021 0.26132858], sum to 1.0000
[2019-04-09 15:01:55,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5088
[2019-04-09 15:01:55,666] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.88040728082262, 1.111210434600333, 0.0, 1.0, 60.0, 23.74543115665309], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4651200.0000, 
sim time next is 4651800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 27.83558392345571, 1.105165726090204, 0.0, 1.0, 45.0, 22.64572291114714], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.8196319936213093, 0.8683885753634013, 0.0, 1.0, 0.6, 0.2264572291114714], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.12112749], dtype=float32), -0.013663696]. 
=============================================
[2019-04-09 15:01:55,840] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00184707 0.1491826  0.14270994 0.04073821 0.01542914 0.00583742
 0.15171999 0.03829626 0.12099576 0.03896256 0.294281  ], sum to 1.0000
[2019-04-09 15:01:55,841] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2899
[2019-04-09 15:01:55,869] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 52.0, 125.5, 800.0, 22.5, 27.64162895359063, 1.024300822377753, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4618800.0000, 
sim time next is 4619400.0000, 
raw observation next is [2.166666666666667, 51.5, 124.3333333333333, 811.0, 22.5, 27.90346390404286, 0.8124703814829949, 1.0, 1.0, 25.0, 47.92308646207341], 
processed observation next is [1.0, 0.4782608695652174, 0.5226223453370269, 0.515, 0.41444444444444434, 0.8961325966850828, 0.375, 0.8252886586702383, 0.7708234604943316, 1.0, 1.0, 0.2, 0.47923086462073405], 
reward next is 0.5208, 
noisyNet noise sample is [array([-1.0995026], dtype=float32), 0.887782]. 
=============================================
[2019-04-09 15:01:55,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00794887 0.12311467 0.15772071 0.04549416 0.0240886  0.01138208
 0.17807446 0.07002078 0.08112072 0.08051264 0.22052222], sum to 1.0000
[2019-04-09 15:01:55,943] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8903
[2019-04-09 15:01:55,964] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.9500000000000001, 66.66666666666667, 0.0, 0.0, 19.0, 27.00006373377573, 0.8046595140083673, 0.0, 1.0, 45.0, 35.20211911271512], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4589400.0000, 
sim time next is 4590000.0000, 
raw observation next is [-1.1, 67.0, 0.0, 0.0, 19.0, 26.97922067105137, 0.8050028189333305, 0.0, 1.0, 65.0, 35.54554048380217], 
processed observation next is [1.0, 0.13043478260869565, 0.4321329639889197, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7482683892542807, 0.7683342729777768, 0.0, 1.0, 1.0, 0.3554554048380217], 
reward next is 0.6445, 
noisyNet noise sample is [array([1.3533978], dtype=float32), -0.91280353]. 
=============================================
[2019-04-09 15:01:55,975] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[3.6377137]
 [3.7844799]
 [3.745982 ]
 [3.950856 ]
 [3.7210813]], R is [[4.23801231]
 [4.84361124]
 [5.42531109]
 [6.10373449]
 [6.72179937]].
[2019-04-09 15:01:55,977] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00556654 0.14111353 0.12457906 0.05664918 0.02694852 0.01311196
 0.15510456 0.04356454 0.12865719 0.07076721 0.23393777], sum to 1.0000
[2019-04-09 15:01:55,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4710
[2019-04-09 15:01:56,000] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 69.16666666666667, 150.3333333333333, 396.3333333333334, 22.5, 27.48287274464167, 0.8742506548511936, 1.0, 1.0, 20.0, 21.07506733661893], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4612200.0000, 
sim time next is 4612800.0000, 
raw observation next is [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 22.5, 27.47471301900046, 0.8976600327372752, 1.0, 1.0, 45.0, 24.38645267784547], 
processed observation next is [1.0, 0.391304347826087, 0.42566943674976926, 0.6733333333333335, 0.5238888888888891, 0.5001841620626151, 0.375, 0.7895594182500384, 0.7992200109124251, 1.0, 1.0, 0.6, 0.2438645267784547], 
reward next is 0.7561, 
noisyNet noise sample is [array([-1.762345], dtype=float32), -1.5321083]. 
=============================================
[2019-04-09 15:01:56,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00730216 0.07003713 0.09344102 0.03610132 0.02039097 0.00951805
 0.1741045  0.0512203  0.07506623 0.05667993 0.4061384 ], sum to 1.0000
[2019-04-09 15:01:56,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-09 15:01:56,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00195669 0.12685458 0.19133191 0.03230826 0.01963244 0.00738641
 0.11209738 0.03150294 0.08165902 0.05613238 0.339138  ], sum to 1.0000
[2019-04-09 15:01:56,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8098
[2019-04-09 15:01:56,160] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1, 64.0, 0.0, 0.0, 19.0, 27.00241693098962, 0.8084319163604494, 0.0, 1.0, 55.0, 31.05445587230505], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4584600.0000, 
sim time next is 4585200.0000, 
raw observation next is [2.775557561562891e-17, 64.33333333333333, 0.0, 0.0, 19.0, 26.98518748515763, 0.8173985219501069, 0.0, 1.0, 45.0, 32.00611434695124], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.6433333333333333, 0.0, 0.0, 0.08333333333333333, 0.7487656237631359, 0.772466173983369, 0.0, 1.0, 0.6, 0.3200611434695124], 
reward next is 0.6799, 
noisyNet noise sample is [array([-1.0609157], dtype=float32), -0.97732383]. 
=============================================
[2019-04-09 15:01:56,171] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.333333333333333, 49.0, 120.3333333333333, 854.6666666666667, 22.5, 28.54066871917516, 0.9232807265799708, 1.0, 1.0, 50.0, 42.12342521017786], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4623600.0000, 
sim time next is 4624200.0000, 
raw observation next is [3.5, 49.0, 120.0, 859.0, 22.5, 28.38411881303984, 1.167074698707548, 1.0, 1.0, 65.0, 14.89269616161419], 
processed observation next is [1.0, 0.5217391304347826, 0.5595567867036012, 0.49, 0.4, 0.949171270718232, 0.375, 0.8653432344199867, 0.8890248995691827, 1.0, 1.0, 1.0, 0.14892696161614188], 
reward next is 0.8511, 
noisyNet noise sample is [array([-0.01553123], dtype=float32), -1.0432105]. 
=============================================
[2019-04-09 15:01:56,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00331143 0.08584169 0.14984566 0.03411636 0.01644397 0.00667315
 0.2139205  0.03980089 0.06289564 0.05335558 0.3337952 ], sum to 1.0000
[2019-04-09 15:01:56,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8542
[2019-04-09 15:01:56,213] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 27.32256193562361, 0.933475873204957, 0.0, 1.0, 65.0, 31.68335409535752], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4567200.0000, 
sim time next is 4567800.0000, 
raw observation next is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 27.30383082525055, 0.9307911762954083, 0.0, 1.0, 65.0, 31.45768147914966], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5616666666666665, 0.0, 0.0, 0.08333333333333333, 0.7753192354375459, 0.8102637254318027, 0.0, 1.0, 1.0, 0.3145768147914966], 
reward next is 0.6854, 
noisyNet noise sample is [array([0.81716794], dtype=float32), 0.30671766]. 
=============================================
[2019-04-09 15:01:56,409] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00227821 0.12736177 0.13264148 0.03657183 0.01543352 0.00544187
 0.13781312 0.03121869 0.16837667 0.07414114 0.26872173], sum to 1.0000
[2019-04-09 15:01:56,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7962
[2019-04-09 15:01:56,425] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.800000000000001, 47.0, 52.83333333333333, 145.3333333333333, 22.5, 29.23387857630127, 1.250069561268208, 1.0, 1.0, 65.0, 11.45970064449579], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4641600.0000, 
sim time next is 4642200.0000, 
raw observation next is [4.6, 47.5, 40.0, 145.0, 22.5, 29.01381595135018, 1.228775746420672, 1.0, 1.0, 65.0, 21.88331245479114], 
processed observation next is [1.0, 0.7391304347826086, 0.590027700831025, 0.475, 0.13333333333333333, 0.16022099447513813, 0.375, 0.9178179959458482, 0.9095919154735573, 1.0, 1.0, 1.0, 0.21883312454791143], 
reward next is 0.7812, 
noisyNet noise sample is [array([0.42669392], dtype=float32), -0.6470042]. 
=============================================
[2019-04-09 15:01:56,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00238683 0.12249563 0.11574512 0.05096295 0.01666857 0.00848833
 0.25383663 0.03128191 0.10573042 0.06351091 0.22889273], sum to 1.0000
[2019-04-09 15:01:56,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8452
[2019-04-09 15:01:56,602] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 22.5, 29.01187951630057, 1.26202269377295, 1.0, 1.0, 55.0, 2.568517847894721], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 22.5, 29.03166417007908, 1.269090730350077, 1.0, 1.0, 65.0, 3.315078530753476], 
processed observation next is [1.0, 0.6086956521739131, 0.5997229916897507, 0.4983333333333334, 0.6677777777777776, 0.48876611418047894, 0.375, 0.9193053475065899, 0.9230302434500256, 1.0, 1.0, 1.0, 0.033150785307534764], 
reward next is 0.9668, 
noisyNet noise sample is [array([-0.15348372], dtype=float32), -1.94457]. 
=============================================
[2019-04-09 15:01:57,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0042906  0.13675316 0.12376844 0.04186857 0.02418946 0.00448564
 0.1663279  0.04940489 0.09327242 0.09707118 0.2585678 ], sum to 1.0000
[2019-04-09 15:01:57,098] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9031
[2019-04-09 15:01:57,119] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 27.83682942887068, 1.102102688774045, 0.0, 1.0, 45.0, 23.79877463689609], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 27.80287232860226, 1.096866466349707, 0.0, 1.0, 65.0, 25.64554490985514], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.8169060273835216, 0.8656221554499023, 0.0, 1.0, 1.0, 0.2564554490985514], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.06755705], dtype=float32), -0.01996815]. 
=============================================
[2019-04-09 15:01:57,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0034178  0.12272959 0.12426798 0.03615315 0.02160139 0.00566719
 0.15523371 0.0371458  0.09289074 0.07189836 0.3289943 ], sum to 1.0000
[2019-04-09 15:01:57,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4450
[2019-04-09 15:01:57,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 47.66666666666667, 196.3333333333333, 207.3333333333333, 22.5, 28.7668553504962, 1.277164604762447, 1.0, 1.0, 55.0, 1.395567908449678], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4634400.0000, 
sim time next is 4635000.0000, 
raw observation next is [5.5, 46.5, 195.0, 129.0, 22.5, 29.08626326731661, 1.285503135430034, 1.0, 1.0, 45.0, 1.900938761897254], 
processed observation next is [1.0, 0.6521739130434783, 0.6149584487534627, 0.465, 0.65, 0.1425414364640884, 0.375, 0.9238552722763842, 0.9285010451433445, 1.0, 1.0, 0.6, 0.01900938761897254], 
reward next is 0.9810, 
noisyNet noise sample is [array([1.269309], dtype=float32), 2.6543822]. 
=============================================
[2019-04-09 15:01:57,157] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[5.355895 ]
 [5.3409643]
 [5.2753954]
 [5.254029 ]
 [5.1998463]], R is [[6.28256607]
 [7.2057848 ]
 [8.04723835]
 [8.50523949]
 [9.38423061]].
[2019-04-09 15:01:57,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00271841 0.09582542 0.1440942  0.05597207 0.0159389  0.0080393
 0.13583814 0.04672266 0.09136786 0.08557666 0.31790647], sum to 1.0000
[2019-04-09 15:01:57,292] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7989
[2019-04-09 15:01:57,303] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00195019 0.09726591 0.1506296  0.0345535  0.01359767 0.00526274
 0.17987627 0.02586173 0.10228363 0.06293879 0.32578   ], sum to 1.0000
[2019-04-09 15:01:57,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5100
[2019-04-09 15:01:57,306] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 22.5, 28.33000849480032, 1.160685126835653, 1.0, 1.0, 60.0, 18.75566384537094], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 22.5, 28.19430589474042, 1.154309767410683, 0.0, 1.0, 65.0, 22.6577844364454], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.375, 0.8495254912283684, 0.8847699224702277, 0.0, 1.0, 1.0, 0.226577844364454], 
reward next is 0.7734, 
noisyNet noise sample is [array([1.1898398], dtype=float32), 0.70598596]. 
=============================================
[2019-04-09 15:01:57,329] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.0, 43.0, 156.0, 138.0, 22.5, 28.48212372687344, 1.30195005335931, 1.0, 1.0, 20.0, 13.54904559687283], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4636800.0000, 
sim time next is 4637400.0000, 
raw observation next is [5.866666666666667, 43.5, 143.0, 141.0, 22.5, 28.00266006095898, 1.242138647328207, 1.0, 1.0, 55.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6251154201292707, 0.435, 0.4766666666666667, 0.1558011049723757, 0.375, 0.8335550050799151, 0.9140462157760689, 1.0, 1.0, 0.8, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4016737], dtype=float32), -1.017825]. 
=============================================
[2019-04-09 15:01:57,584] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00298436 0.12651059 0.1616615  0.04770479 0.01780783 0.00461914
 0.1724967  0.03491699 0.08314166 0.06562003 0.2825364 ], sum to 1.0000
[2019-04-09 15:01:57,588] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5441
[2019-04-09 15:01:57,598] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.83661986774542, 1.084090494958531, 0.0, 1.0, 25.0, 21.65772975248728], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4651200.0000, 
sim time next is 4651800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 27.80987987466581, 1.079465806356103, 0.0, 1.0, 50.0, 20.70320725639187], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.8174899895554842, 0.8598219354520342, 0.0, 1.0, 0.7, 0.2070320725639187], 
reward next is 0.7930, 
noisyNet noise sample is [array([0.8643498], dtype=float32), -0.6102142]. 
=============================================
[2019-04-09 15:01:57,615] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01013163 0.1096371  0.13420057 0.04806285 0.04008135 0.01292417
 0.18531723 0.05265961 0.07307819 0.08473824 0.24916911], sum to 1.0000
[2019-04-09 15:01:57,621] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6011
[2019-04-09 15:01:57,644] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.666666666666667, 67.0, 0.0, 0.0, 19.0, 27.44962248958818, 0.9101807002486525, 0.0, 1.0, 45.0, 25.41077043623808], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4677000.0000, 
sim time next is 4677600.0000, 
raw observation next is [1.333333333333333, 72.0, 0.0, 0.0, 19.0, 27.44843794866682, 0.9009151449956295, 0.0, 1.0, 55.0, 26.25513572871116], 
processed observation next is [1.0, 0.13043478260869565, 0.4995383194829178, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7873698290555685, 0.8003050483318765, 0.0, 1.0, 0.8, 0.2625513572871116], 
reward next is 0.7374, 
noisyNet noise sample is [array([-1.0990751], dtype=float32), -0.14428069]. 
=============================================
[2019-04-09 15:01:57,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00440842 0.0954     0.16510454 0.05298405 0.02456551 0.00831601
 0.17546491 0.03843682 0.11108248 0.06831665 0.25592065], sum to 1.0000
[2019-04-09 15:01:57,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4687
[2019-04-09 15:01:57,788] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 22.5, 27.89204865711648, 0.9325393000152414, 1.0, 1.0, 60.0, 21.79270819594278], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 22.5, 27.91833032874225, 0.9444583609068834, 1.0, 1.0, 45.0, 18.87116225826453], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.375, 0.8265275273951875, 0.8148194536356278, 1.0, 1.0, 0.6, 0.1887116225826453], 
reward next is 0.8113, 
noisyNet noise sample is [array([0.19717063], dtype=float32), 1.6952848]. 
=============================================
[2019-04-09 15:01:57,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00550706 0.08383416 0.12092143 0.02894665 0.01704217 0.00686188
 0.220666   0.04931537 0.08201744 0.06415679 0.32073104], sum to 1.0000
[2019-04-09 15:01:57,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3463
[2019-04-09 15:01:57,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00756351 0.09322546 0.15085387 0.03815252 0.02894506 0.00756807
 0.19464017 0.06135077 0.06272238 0.08230793 0.2726702 ], sum to 1.0000
[2019-04-09 15:01:57,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8062
[2019-04-09 15:01:57,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[5.2195773]
 [5.186811 ]
 [4.9685197]
 [4.855337 ]
 [4.8797445]], R is [[5.99918175]
 [6.72126293]
 [7.43491602]
 [8.18151474]
 [8.83419609]].
[2019-04-09 15:01:57,816] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 54.5, 0.0, 0.0, 19.0, 27.46710570808192, 0.9716490603225827, 0.0, 1.0, 65.0, 31.47266730959119], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4663800.0000, 
sim time next is 4664400.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 27.43507391714699, 0.9647230265661455, 0.0, 1.0, 45.0, 33.5227407981576], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7862561597622492, 0.8215743421887152, 0.0, 1.0, 0.6, 0.335227407981576], 
reward next is 0.6648, 
noisyNet noise sample is [array([-1.0243255], dtype=float32), -0.5784077]. 
=============================================
[2019-04-09 15:01:57,820] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 27.45547540486402, 0.9513007508716905, 0.0, 1.0, 45.0, 26.78656168669144], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4669800.0000, 
sim time next is 4670400.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 27.48229904965352, 0.946840399679429, 0.0, 1.0, 65.0, 30.86213152723622], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.08333333333333333, 0.7901915874711266, 0.8156134665598097, 0.0, 1.0, 1.0, 0.3086213152723622], 
reward next is 0.6914, 
noisyNet noise sample is [array([1.0059466], dtype=float32), 0.28127322]. 
=============================================
[2019-04-09 15:01:57,974] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00723174 0.08846453 0.13757603 0.04174468 0.02469899 0.00994195
 0.21672551 0.05837678 0.07375322 0.0887762  0.25271028], sum to 1.0000
[2019-04-09 15:01:57,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5572
[2019-04-09 15:01:57,992] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.6, 74.0, 0.0, 0.0, 19.0, 26.84867832159934, 0.74742002307082, 0.0, 1.0, 25.0, 33.00660075234093], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4600800.0000, 
sim time next is 4601400.0000, 
raw observation next is [-2.666666666666667, 74.5, 0.0, 0.0, 19.0, 26.88246921273102, 0.7358168087639756, 0.0, 1.0, 50.0, 31.48980587540449], 
processed observation next is [1.0, 0.2608695652173913, 0.38873499538319484, 0.745, 0.0, 0.0, 0.08333333333333333, 0.7402057677275851, 0.7452722695879919, 0.0, 1.0, 0.7, 0.31489805875404486], 
reward next is 0.6851, 
noisyNet noise sample is [array([1.0630003], dtype=float32), -0.5560173]. 
=============================================
[2019-04-09 15:01:58,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00342137 0.12840798 0.13136576 0.03884312 0.01667345 0.00622818
 0.2172624  0.05222113 0.09489467 0.06921793 0.24146391], sum to 1.0000
[2019-04-09 15:01:58,168] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3222
[2019-04-09 15:01:58,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00385605 0.10490965 0.12468047 0.03418538 0.01577403 0.0068816
 0.30945987 0.04502472 0.06856125 0.0597393  0.22692761], sum to 1.0000
[2019-04-09 15:01:58,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9642
[2019-04-09 15:01:58,181] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 22.5, 27.8910894173195, 1.112998112841107, 0.0, 1.0, 25.0, 21.0615281157528], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4650600.0000, 
sim time next is 4651200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.85305772121318, 1.106473852627033, 0.0, 1.0, 40.0, 21.89694527588879], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8210881434344316, 0.8688246175423444, 0.0, 1.0, 0.5, 0.2189694527588879], 
reward next is 0.7810, 
noisyNet noise sample is [array([-0.06522422], dtype=float32), 0.2218139]. 
=============================================
[2019-04-09 15:01:58,187] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 27.74906305985802, 1.088380406330133, 0.0, 1.0, 45.0, 22.26889206867377], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4654200.0000, 
sim time next is 4654800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 27.76062851716431, 1.083568028101714, 0.0, 1.0, 65.0, 24.39145827974253], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.8133857097636925, 0.8611893427005713, 0.0, 1.0, 1.0, 0.24391458279742528], 
reward next is 0.7561, 
noisyNet noise sample is [array([0.10046946], dtype=float32), -0.28153643]. 
=============================================
[2019-04-09 15:01:58,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00205911 0.22633757 0.17394626 0.05419051 0.01558207 0.00566038
 0.12153733 0.04593971 0.07899548 0.05405666 0.22169489], sum to 1.0000
[2019-04-09 15:01:58,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2723
[2019-04-09 15:01:58,225] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.166666666666667, 49.0, 120.6666666666667, 850.3333333333334, 22.5, 28.05642028575922, 1.117607596317284, 1.0, 1.0, 25.0, 5.560079232416746], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4623000.0000, 
sim time next is 4623600.0000, 
raw observation next is [3.333333333333333, 49.0, 120.3333333333333, 854.6666666666667, 22.5, 28.4060820129475, 1.12591095650223, 1.0, 1.0, 25.0, 10.20580731722687], 
processed observation next is [1.0, 0.5217391304347826, 0.5549399815327793, 0.49, 0.401111111111111, 0.9443830570902395, 0.375, 0.8671735010789584, 0.87530365216741, 1.0, 1.0, 0.2, 0.10205807317226871], 
reward next is 0.8979, 
noisyNet noise sample is [array([2.5713434], dtype=float32), -0.27709112]. 
=============================================
[2019-04-09 15:01:58,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00371889 0.12302022 0.13358472 0.04563203 0.02089993 0.00465757
 0.19690698 0.03341344 0.11955461 0.08610579 0.23250584], sum to 1.0000
[2019-04-09 15:01:58,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9156
[2019-04-09 15:01:58,329] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 22.5, 28.07673633028194, 1.005921153978902, 1.0, 1.0, 25.0, 18.35597687207507], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 22.5, 28.08888958315098, 1.005762861241153, 1.0, 1.0, 65.0, 21.32957371777512], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.375, 0.840740798595915, 0.8352542870803843, 1.0, 1.0, 1.0, 0.2132957371777512], 
reward next is 0.7867, 
noisyNet noise sample is [array([1.400386], dtype=float32), -0.04215702]. 
=============================================
[2019-04-09 15:01:58,428] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00809778 0.10630934 0.13048315 0.04844943 0.03022083 0.01665841
 0.17877613 0.05811659 0.09192895 0.06109212 0.26986727], sum to 1.0000
[2019-04-09 15:01:58,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9083
[2019-04-09 15:01:58,445] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 96.0, 0.0, 0.0, 19.0, 27.33843664565983, 0.8852058048629466, 0.0, 1.0, 65.0, 50.24911985437445], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4681800.0000, 
sim time next is 4682400.0000, 
raw observation next is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 19.0, 27.32443687515668, 0.8853414682351267, 0.0, 1.0, 65.0, 32.17341726261433], 
processed observation next is [1.0, 0.17391304347826086, 0.44413665743305636, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.7770364062630568, 0.7951138227450422, 0.0, 1.0, 1.0, 0.3217341726261433], 
reward next is 0.6783, 
noisyNet noise sample is [array([1.1674153], dtype=float32), 0.562953]. 
=============================================
[2019-04-09 15:01:58,633] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0020937  0.14486052 0.11199874 0.0405487  0.01120584 0.00422821
 0.1568171  0.02930404 0.11367889 0.05756214 0.32770208], sum to 1.0000
[2019-04-09 15:01:58,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-09 15:01:58,645] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.2, 48.5, 26.66666666666667, 96.66666666666667, 22.5, 28.78685584069277, 1.27309335376611, 1.0, 1.0, 65.0, 5.652848466271089], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4643400.0000, 
sim time next is 4644000.0000, 
raw observation next is [4.0, 49.0, 0.0, 0.0, 22.5, 28.86323942600109, 1.242570330418206, 1.0, 1.0, 65.0, 7.882777953155253], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.49, 0.0, 0.0, 0.375, 0.9052699521667575, 0.9141901101394021, 1.0, 1.0, 1.0, 0.07882777953155254], 
reward next is 0.9212, 
noisyNet noise sample is [array([1.6142198], dtype=float32), -0.25139442]. 
=============================================
[2019-04-09 15:01:58,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[5.790962 ]
 [5.6018124]
 [5.637535 ]
 [5.735242 ]
 [5.7699366]], R is [[6.57494116]
 [7.45266342]
 [8.30530643]
 [9.00289726]
 [9.82866478]].
[2019-04-09 15:01:58,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00309775 0.18712176 0.14856897 0.05042411 0.01691073 0.00673225
 0.15666693 0.05455773 0.09724959 0.04948375 0.22918648], sum to 1.0000
[2019-04-09 15:01:58,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1851
[2019-04-09 15:01:58,694] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 56.0, 129.0, 767.0, 22.5, 28.09281319100321, 1.023764543490165, 1.0, 1.0, 20.0, 16.89446035646113], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4617000.0000, 
sim time next is 4617600.0000, 
raw observation next is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 22.5, 28.09039321719247, 1.043749428164663, 1.0, 1.0, 20.0, 16.08065431196277], 
processed observation next is [1.0, 0.43478260869565216, 0.4995383194829178, 0.5466666666666667, 0.426111111111111, 0.8596685082872928, 0.375, 0.8408661014327059, 0.8479164760548876, 1.0, 1.0, 0.1, 0.16080654311962772], 
reward next is 0.8392, 
noisyNet noise sample is [array([-1.6930299], dtype=float32), 0.7623656]. 
=============================================
[2019-04-09 15:01:58,785] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00318381 0.16030964 0.1541186  0.0487709  0.01681077 0.007412
 0.17461933 0.05974749 0.0882186  0.04801351 0.23879537], sum to 1.0000
[2019-04-09 15:01:58,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7017
[2019-04-09 15:01:58,810] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.166666666666667, 51.5, 124.3333333333333, 811.0, 22.5, 27.73961799909222, 1.039786817234571, 1.0, 1.0, 20.0, 9.629716606230476], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4619400.0000, 
sim time next is 4620000.0000, 
raw observation next is [2.333333333333333, 51.0, 123.1666666666667, 822.0, 22.5, 28.0393305248836, 0.827050779439433, 1.0, 1.0, 55.0, 34.74889711051487], 
processed observation next is [1.0, 0.4782608695652174, 0.5272391505078486, 0.51, 0.4105555555555557, 0.9082872928176795, 0.375, 0.8366108770736332, 0.7756835931464776, 1.0, 1.0, 0.8, 0.3474889711051487], 
reward next is 0.6525, 
noisyNet noise sample is [array([-1.6930299], dtype=float32), 0.7623656]. 
=============================================
[2019-04-09 15:01:58,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[4.9966164]
 [5.023364 ]
 [4.941853 ]
 [5.0378866]
 [4.9231305]], R is [[5.70046949]
 [6.54716778]
 [7.29849052]
 [7.800879  ]
 [8.56206417]].
[2019-04-09 15:01:58,956] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00887897 0.08078072 0.138912   0.04818488 0.02477614 0.01324312
 0.21622406 0.0471955  0.08709231 0.0768506  0.25786176], sum to 1.0000
[2019-04-09 15:01:58,959] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4860
[2019-04-09 15:01:58,972] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 19.0, 27.3227028287729, 0.8845208275457033, 0.0, 1.0, 45.0, 27.81873542308232], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4682400.0000, 
sim time next is 4683000.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 19.0, 27.33764031343235, 0.8827774301120979, 0.0, 1.0, 45.0, 27.01803095348861], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 0.08333333333333333, 0.778136692786029, 0.7942591433706992, 0.0, 1.0, 0.6, 0.2701803095348861], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.57350236], dtype=float32), -0.56861824]. 
=============================================
[2019-04-09 15:01:58,993] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.7324555]
 [3.7895617]
 [3.736827 ]
 [3.7274842]
 [3.7858114]], R is [[4.55167007]
 [5.22796631]
 [5.88588715]
 [6.50833559]
 [7.17135096]].
[2019-04-09 15:01:59,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00253832 0.1333689  0.1816493  0.05577863 0.01598362 0.00611201
 0.11851275 0.04492958 0.12646681 0.05626008 0.2584    ], sum to 1.0000
[2019-04-09 15:01:59,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-09 15:01:59,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0027923  0.09639207 0.1408001  0.04029791 0.01393957 0.00812106
 0.17985266 0.0422605  0.07887947 0.05515619 0.34150827], sum to 1.0000
[2019-04-09 15:01:59,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6961
[2019-04-09 15:01:59,349] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.583333333333334, 49.0, 160.3333333333333, 741.6666666666667, 22.5, 28.27069017585488, 1.18854585351159, 1.0, 1.0, 35.0, 2.380189061671494], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4629000.0000, 
sim time next is 4629600.0000, 
raw observation next is [4.7, 49.0, 171.0, 706.0, 22.5, 28.6456019423613, 1.214843905943273, 1.0, 1.0, 20.0, 2.622150924639311], 
processed observation next is [1.0, 0.6086956521739131, 0.592797783933518, 0.49, 0.57, 0.7801104972375691, 0.375, 0.887133495196775, 0.9049479686477576, 1.0, 1.0, 0.1, 0.026221509246393108], 
reward next is 0.9738, 
noisyNet noise sample is [array([0.9410563], dtype=float32), 0.1858143]. 
=============================================
[2019-04-09 15:01:59,350] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.0, 106.3333333333333, 0.0, 22.5, 27.69454508637267, 0.930506931494365, 1.0, 1.0, 45.0, 20.29478998408891], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4699200.0000, 
sim time next is 4699800.0000, 
raw observation next is [0.0, 92.0, 115.0, 0.0, 22.5, 27.80341861950217, 0.9421391832482585, 1.0, 1.0, 65.0, 21.9097649478087], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.38333333333333336, 0.0, 0.375, 0.816951551625181, 0.8140463944160862, 1.0, 1.0, 1.0, 0.219097649478087], 
reward next is 0.7809, 
noisyNet noise sample is [array([0.4945554], dtype=float32), -0.0057748565]. 
=============================================
[2019-04-09 15:01:59,483] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00682141 0.09203675 0.13418007 0.0357538  0.01810831 0.01224041
 0.20160198 0.04634713 0.09065017 0.06339072 0.29886925], sum to 1.0000
[2019-04-09 15:01:59,483] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-09 15:01:59,509] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.16957072997671, 0.8259882215743289, 0.0, 1.0, 25.0, 30.49910847987579], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4690800.0000, 
sim time next is 4691400.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 22.5, 27.0970050230929, 0.8284988428645171, 0.0, 1.0, 55.0, 35.55729600116151], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 0.375, 0.7580837519244085, 0.776166280954839, 0.0, 1.0, 0.8, 0.3555729600116151], 
reward next is 0.6444, 
noisyNet noise sample is [array([1.2067734], dtype=float32), 0.38899752]. 
=============================================
[2019-04-09 15:01:59,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00218353 0.11234363 0.15475991 0.03862666 0.01558999 0.00642521
 0.22361945 0.04366539 0.10835252 0.06326005 0.23117363], sum to 1.0000
[2019-04-09 15:01:59,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8805
[2019-04-09 15:01:59,559] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 22.5, 28.33095162264473, 1.160433405273783, 1.0, 1.0, 25.0, 17.57013764762322], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 22.5, 28.19556949527853, 1.154427132939046, 0.0, 1.0, 65.0, 22.65474101236898], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.375, 0.8496307912732108, 0.8848090443130153, 0.0, 1.0, 1.0, 0.2265474101236898], 
reward next is 0.7735, 
noisyNet noise sample is [array([1.1622014], dtype=float32), -1.0037726]. 
=============================================
[2019-04-09 15:01:59,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00131241 0.10538983 0.10549564 0.03416133 0.01993951 0.00411073
 0.17746563 0.03499304 0.15122181 0.09167552 0.27423456], sum to 1.0000
[2019-04-09 15:01:59,723] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3716
[2019-04-09 15:01:59,740] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 22.5, 28.03942153537306, 1.004953144776099, 1.0, 1.0, 65.0, 41.2339547748549], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 22.5, 28.03899109427696, 1.00549761986528, 1.0, 1.0, 45.0, 39.54865838648453], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.375, 0.8365825911897465, 0.8351658732884267, 1.0, 1.0, 0.6, 0.3954865838648453], 
reward next is 0.6045, 
noisyNet noise sample is [array([0.38494903], dtype=float32), 1.7459046]. 
=============================================
[2019-04-09 15:02:00,600] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00633307 0.07598098 0.15013792 0.02950163 0.01654995 0.00862938
 0.21098766 0.05335493 0.08189697 0.09053706 0.27609038], sum to 1.0000
[2019-04-09 15:02:00,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5197
[2019-04-09 15:02:00,638] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 77.5, 0.0, 0.0, 19.0, 26.48477067861125, 0.6983047080161121, 0.0, 1.0, 45.0, 30.09730729406591], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4753800.0000, 
sim time next is 4754400.0000, 
raw observation next is [-4.0, 75.33333333333333, 0.0, 0.0, 19.0, 26.35834128966498, 0.6735972290597121, 0.0, 1.0, 50.0, 34.83093465788527], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.7533333333333333, 0.0, 0.0, 0.08333333333333333, 0.6965284408054151, 0.7245324096865707, 0.0, 1.0, 0.7, 0.3483093465788527], 
reward next is 0.6517, 
noisyNet noise sample is [array([-1.0502429], dtype=float32), -0.6633972]. 
=============================================
[2019-04-09 15:02:00,808] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00344264 0.0993791  0.12054745 0.03910151 0.01657606 0.00812668
 0.18051584 0.06652652 0.09115081 0.0504582  0.32417515], sum to 1.0000
[2019-04-09 15:02:00,808] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1957
[2019-04-09 15:02:00,822] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 27.76608403292361, 1.065558726781991, 0.0, 1.0, 45.0, 21.18774010559804], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4654800.0000, 
sim time next is 4655400.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 19.0, 27.75771802237955, 1.060720798891887, 0.0, 1.0, 25.0, 20.42233726413365], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 0.08333333333333333, 0.8131431685316292, 0.853573599630629, 0.0, 1.0, 0.2, 0.2042233726413365], 
reward next is 0.7958, 
noisyNet noise sample is [array([-1.2213396], dtype=float32), -1.43402]. 
=============================================
[2019-04-09 15:02:00,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00224799 0.10497402 0.1343991  0.03952608 0.0155582  0.00385951
 0.14159617 0.03878365 0.11270474 0.07847759 0.32787293], sum to 1.0000
[2019-04-09 15:02:00,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9961
[2019-04-09 15:02:00,922] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 76.33333333333334, 16.66666666666666, 22.5, 27.91823108443599, 1.007378979235413, 1.0, 1.0, 20.0, 29.21303925656683], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [1.0, 72.0, 64.5, 19.5, 22.5, 28.08652809969354, 1.017632504623444, 1.0, 1.0, 65.0, 24.38628623167383], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.72, 0.215, 0.02154696132596685, 0.375, 0.8405440083077949, 0.8392108348744814, 1.0, 1.0, 1.0, 0.2438628623167383], 
reward next is 0.7561, 
noisyNet noise sample is [array([0.09596732], dtype=float32), 0.5882781]. 
=============================================
[2019-04-09 15:02:01,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01173648 0.08599643 0.11198153 0.06762642 0.03282286 0.02173567
 0.1754336  0.06574669 0.06085289 0.07930745 0.28676003], sum to 1.0000
[2019-04-09 15:02:01,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0048
[2019-04-09 15:02:01,038] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.333333333333334, 74.5, 0.0, 0.0, 19.0, 25.92595084519473, 0.6112909192897148, 0.0, 1.0, 25.0, 43.61097462146334], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 4759800.0000, 
sim time next is 4760400.0000, 
raw observation next is [-4.666666666666667, 78.0, 0.0, 0.0, 19.0, 25.92959147119427, 0.534884244179512, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.3333333333333333, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6607992892661892, 0.6782947480598374, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.460347], dtype=float32), -0.13506842]. 
=============================================
[2019-04-09 15:02:01,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00151417 0.12440711 0.12110789 0.04005587 0.0166166  0.00426979
 0.15037133 0.03314699 0.08986603 0.05407795 0.36456627], sum to 1.0000
[2019-04-09 15:02:01,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0100
[2019-04-09 15:02:01,215] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 121.5, 0.0, 22.5, 27.7880766163233, 0.9729368877416599, 1.0, 1.0, 20.0, 38.57560207695035], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4712400.0000, 
sim time next is 4713000.0000, 
raw observation next is [1.166666666666667, 83.83333333333334, 126.0, 0.0, 22.5, 27.76288005341518, 0.9655034207517224, 1.0, 1.0, 20.0, 39.74325326535613], 
processed observation next is [1.0, 0.5652173913043478, 0.49492151431209613, 0.8383333333333334, 0.42, 0.0, 0.375, 0.8135733377845984, 0.8218344735839075, 1.0, 1.0, 0.1, 0.3974325326535613], 
reward next is 0.6026, 
noisyNet noise sample is [array([0.4503792], dtype=float32), 1.1100478]. 
=============================================
[2019-04-09 15:02:01,233] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[5.6802297]
 [5.6516957]
 [5.7139363]
 [5.537476 ]
 [5.689591 ]], R is [[6.34934521]
 [6.90009594]
 [7.39130878]
 [7.86281633]
 [8.4052887 ]].
[2019-04-09 15:02:01,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00261767 0.14660975 0.10629744 0.03454563 0.01465472 0.00446064
 0.17900243 0.04602762 0.08651278 0.11133638 0.267935  ], sum to 1.0000
[2019-04-09 15:02:01,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6671
[2019-04-09 15:02:01,633] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 22.5, 28.06738371267501, 1.004328982962626, 1.0, 1.0, 25.0, 20.09962757970103], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4705800.0000, 
sim time next is 4706400.0000, 
raw observation next is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 22.5, 28.02017374923382, 1.011952079740627, 1.0, 1.0, 60.0, 21.12792814700076], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.9, 0.7072222222222224, 0.0066298342541436465, 0.375, 0.8350144791028183, 0.8373173599135423, 1.0, 1.0, 0.9, 0.2112792814700076], 
reward next is 0.7887, 
noisyNet noise sample is [array([1.0468525], dtype=float32), 1.8747138]. 
=============================================
[2019-04-09 15:02:01,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01518443 0.12876159 0.11480018 0.07331125 0.04957537 0.02416375
 0.14730579 0.0554881  0.10551772 0.0772314  0.20866038], sum to 1.0000
[2019-04-09 15:02:01,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3766
[2019-04-09 15:02:01,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00136324 0.13235806 0.1312037  0.03476643 0.01345692 0.00397768
 0.21281488 0.03116698 0.0717511  0.07453304 0.29260793], sum to 1.0000
[2019-04-09 15:02:01,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5600
[2019-04-09 15:02:01,839] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 19.0, 25.07459959346383, 0.426096416731735, 0.0, 1.0, 60.0, 49.86456541198996], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 19.0, 25.1351422162776, 0.4283613486686496, 0.0, 1.0, 35.0, 44.47166078181823], 
processed observation next is [0.0, 0.2608695652173913, 0.29362880886426596, 0.925, 0.0, 0.0, 0.08333333333333333, 0.5945951846898, 0.6427871162228832, 0.0, 1.0, 0.4, 0.4447166078181823], 
reward next is 0.5553, 
noisyNet noise sample is [array([0.4655616], dtype=float32), -1.8643252]. 
=============================================
[2019-04-09 15:02:01,856] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 75.0, 29.0, 28.0, 22.5, 27.93522303965658, 0.984906979404849, 1.0, 1.0, 55.0, 31.79032966384303], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4728600.0000, 
sim time next is 4729200.0000, 
raw observation next is [0.3333333333333334, 76.0, 24.16666666666667, 23.33333333333334, 22.5, 27.75614470293765, 1.027314306736671, 1.0, 1.0, 50.0, 23.68838717158977], 
processed observation next is [1.0, 0.7391304347826086, 0.4718374884579871, 0.76, 0.08055555555555557, 0.025782688766114188, 0.375, 0.8130120585781375, 0.842438102245557, 1.0, 1.0, 0.7, 0.2368838717158977], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.86030924], dtype=float32), 1.9573214]. 
=============================================
[2019-04-09 15:02:02,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0066301  0.10297899 0.11532269 0.04195523 0.02584394 0.00966402
 0.21570864 0.05146624 0.08401899 0.08913139 0.25727966], sum to 1.0000
[2019-04-09 15:02:02,096] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2223
[2019-04-09 15:02:02,117] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 92.0, 0.0, 0.0, 19.0, 27.29166288010654, 0.8823001074695968, 0.0, 1.0, 45.0, 30.61236555454932], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4680000.0000, 
sim time next is 4680600.0000, 
raw observation next is [-0.1666666666666667, 93.33333333333334, 0.0, 0.0, 19.0, 27.3891708779728, 0.8740611125179217, 0.0, 1.0, 25.0, 23.47502962341397], 
processed observation next is [1.0, 0.17391304347826086, 0.4579870729455217, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.7824309064977332, 0.7913537041726406, 0.0, 1.0, 0.2, 0.2347502962341397], 
reward next is 0.7652, 
noisyNet noise sample is [array([1.016983], dtype=float32), -0.51815474]. 
=============================================
[2019-04-09 15:02:02,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00606104 0.09297373 0.10499161 0.05078064 0.02705413 0.00906131
 0.23134626 0.05322388 0.08428991 0.07750356 0.26271403], sum to 1.0000
[2019-04-09 15:02:02,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8850
[2019-04-09 15:02:02,460] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 73.16666666666667, 0.0, 0.0, 19.0, 26.52654918997487, 0.721242408778589, 0.0, 1.0, 55.0, 41.53999009320405], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4755000.0000, 
sim time next is 4755600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.47698011761982, 0.7197889687763189, 0.0, 1.0, 65.0, 62.88292111539225], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7064150098016517, 0.739929656258773, 0.0, 1.0, 1.0, 0.6288292111539225], 
reward next is 0.3712, 
noisyNet noise sample is [array([-0.67558515], dtype=float32), -0.24845731]. 
=============================================
[2019-04-09 15:02:02,468] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00133943 0.14579554 0.14110784 0.02067429 0.01594278 0.00343055
 0.21510442 0.02857604 0.09609137 0.04177132 0.29016647], sum to 1.0000
[2019-04-09 15:02:02,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8295
[2019-04-09 15:02:02,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00182355 0.09271617 0.11710131 0.0215115  0.0113888  0.00497193
 0.20254913 0.03606362 0.09870251 0.06266681 0.35050467], sum to 1.0000
[2019-04-09 15:02:02,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4598
[2019-04-09 15:02:02,485] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 171.5, 3.0, 22.5, 28.05767225921062, 1.019255840495211, 1.0, 1.0, 65.0, 30.18669290751808], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4719600.0000, 
sim time next is 4720200.0000, 
raw observation next is [1.0, 72.0, 163.3333333333333, 2.0, 22.5, 28.11704937911535, 1.038468101415607, 1.0, 1.0, 65.0, 32.76512591725548], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.5444444444444443, 0.0022099447513812156, 0.375, 0.8430874482596126, 0.8461560338052023, 1.0, 1.0, 1.0, 0.32765125917255483], 
reward next is 0.6723, 
noisyNet noise sample is [array([0.29358873], dtype=float32), 0.3449605]. 
=============================================
[2019-04-09 15:02:02,499] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 22.5, 27.20962651108933, 0.9071994157244253, 1.0, 1.0, 45.0, 30.95087113398058], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4736400.0000, 
sim time next is 4737000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 22.5, 27.17620064177875, 0.901696098237331, 0.0, 1.0, 50.0, 32.22932937296166], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.375, 0.7646833868148958, 0.8005653660791103, 0.0, 1.0, 0.7, 0.32229329372961657], 
reward next is 0.6777, 
noisyNet noise sample is [array([0.47200978], dtype=float32), -0.8843788]. 
=============================================
[2019-04-09 15:02:02,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[5.842852 ]
 [5.6420546]
 [5.7234054]
 [5.711312 ]
 [5.5646577]], R is [[6.28489304]
 [6.91253519]
 [7.50527477]
 [8.0749321 ]
 [8.6975708 ]].
[2019-04-09 15:02:02,631] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00746965 0.10605051 0.11303878 0.03401232 0.021738   0.00995332
 0.23231064 0.04718618 0.05977936 0.07133196 0.29712927], sum to 1.0000
[2019-04-09 15:02:02,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0269
[2019-04-09 15:02:02,648] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.17388044749385, 0.8102429926153522, 0.0, 1.0, 65.0, 35.12590569510657], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4690800.0000, 
sim time next is 4691400.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 22.5, 27.09932323830596, 0.8126386062937114, 0.0, 1.0, 20.0, 34.10344955124911], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 0.375, 0.7582769365254967, 0.7708795354312371, 0.0, 1.0, 0.1, 0.34103449551249115], 
reward next is 0.6590, 
noisyNet noise sample is [array([-0.0443633], dtype=float32), 1.0139191]. 
=============================================
[2019-04-09 15:02:02,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01604996 0.1133218  0.11479469 0.05862793 0.04341758 0.01940708
 0.2171375  0.05283294 0.06935855 0.07631592 0.2187361 ], sum to 1.0000
[2019-04-09 15:02:02,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5085
[2019-04-09 15:02:02,996] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.28735450937249, 0.6443796729582816, 0.0, 1.0, 20.0, 35.92851054980513], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4768800.0000, 
sim time next is 4769400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.32292187008733, 0.6355237726492923, 0.0, 1.0, 45.0, 39.39792623717229], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6935768225072776, 0.711841257549764, 0.0, 1.0, 0.6, 0.3939792623717229], 
reward next is 0.6060, 
noisyNet noise sample is [array([0.35103366], dtype=float32), 1.0028819]. 
=============================================
[2019-04-09 15:02:03,053] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0073993  0.07736289 0.11547879 0.04538923 0.02685823 0.01364719
 0.21996251 0.05088609 0.07242796 0.08003255 0.2905554 ], sum to 1.0000
[2019-04-09 15:02:03,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-09 15:02:03,072] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.54414647475933, 0.7441154452197228, 0.0, 1.0, 65.0, 48.12758847337129], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.57136612029119, 0.7438744726986527, 0.0, 1.0, 45.0, 43.88755007374753], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.714280510024266, 0.7479581575662175, 0.0, 1.0, 0.6, 0.43887550073747533], 
reward next is 0.5611, 
noisyNet noise sample is [array([-1.6139857], dtype=float32), 1.3120081]. 
=============================================
[2019-04-09 15:02:03,132] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 126717: loss 23.5050
[2019-04-09 15:02:03,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 126718: learning rate 0.0000
[2019-04-09 15:02:03,350] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126836: loss 26.4645
[2019-04-09 15:02:03,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126836: learning rate 0.0000
[2019-04-09 15:02:03,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00186072 0.09431861 0.16361332 0.03107363 0.01655084 0.00563261
 0.20990469 0.05276696 0.09862364 0.0723424  0.2533125 ], sum to 1.0000
[2019-04-09 15:02:03,488] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2190
[2019-04-09 15:02:03,523] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 19.0, 27.09865794798166, 0.9038130355682771, 0.0, 1.0, 55.0, 35.4865658611745], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 19.0, 27.07731703468775, 0.8997009170862893, 0.0, 1.0, 25.0, 32.01204574672028], 
processed observation next is [1.0, 0.8695652173913043, 0.4210526315789474, 0.815, 0.0, 0.0, 0.08333333333333333, 0.756443086223979, 0.7999003056954298, 0.0, 1.0, 0.2, 0.3201204574672028], 
reward next is 0.6799, 
noisyNet noise sample is [array([0.2896184], dtype=float32), 0.40358517]. 
=============================================
[2019-04-09 15:02:03,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.002734   0.09159816 0.11765794 0.03443678 0.01284871 0.00691208
 0.18107419 0.03268295 0.07465249 0.08836063 0.35704204], sum to 1.0000
[2019-04-09 15:02:03,625] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7915
[2019-04-09 15:02:03,641] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 19.0, 27.03308042161464, 0.8711804684755693, 0.0, 1.0, 20.0, 38.45939117945677], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4741800.0000, 
sim time next is 4742400.0000, 
raw observation next is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 19.0, 27.01098679587749, 0.8674337997365824, 0.0, 1.0, 65.0, 39.515551405529], 
processed observation next is [1.0, 0.9130434782608695, 0.3979686057248385, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7509155663231241, 0.7891445999121941, 0.0, 1.0, 1.0, 0.39515551405529004], 
reward next is 0.6048, 
noisyNet noise sample is [array([-1.3904612], dtype=float32), -0.7156082]. 
=============================================
[2019-04-09 15:02:03,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00226681 0.12221225 0.17542246 0.03174704 0.01262641 0.00445183
 0.17440188 0.03225813 0.11581115 0.08446855 0.24433349], sum to 1.0000
[2019-04-09 15:02:03,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2419
[2019-04-09 15:02:03,689] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 73.0, 165.5, 3.0, 22.5, 28.01517142618595, 1.021772352437864, 1.0, 1.0, 45.0, 37.40376453182582], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4716000.0000, 
sim time next is 4716600.0000, 
raw observation next is [1.833333333333333, 72.83333333333334, 175.6666666666667, 4.0, 22.5, 28.09174401551749, 1.031231872367963, 1.0, 1.0, 25.0, 27.4397340151937], 
processed observation next is [1.0, 0.6086956521739131, 0.5133887349953832, 0.7283333333333334, 0.5855555555555557, 0.004419889502762431, 0.375, 0.8409786679597909, 0.8437439574559876, 1.0, 1.0, 0.2, 0.274397340151937], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.8274124], dtype=float32), -0.4325195]. 
=============================================
[2019-04-09 15:02:03,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01043173 0.117944   0.1160582  0.07128883 0.04551078 0.01726525
 0.16072837 0.05778908 0.08616383 0.0787532  0.23806676], sum to 1.0000
[2019-04-09 15:02:03,710] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9054
[2019-04-09 15:02:03,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 19.0, 25.72163068067154, 0.6025965298289893, 0.0, 1.0, 65.0, 51.5728562494898], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4783200.0000, 
sim time next is 4783800.0000, 
raw observation next is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 19.0, 25.92472186034689, 0.6245795395390622, 0.0, 1.0, 65.0, 50.73409738355976], 
processed observation next is [0.0, 0.34782608695652173, 0.31948291782086796, 0.795, 0.468888888888889, 0.4637200736648251, 0.08333333333333333, 0.6603934883622408, 0.708193179846354, 0.0, 1.0, 1.0, 0.5073409738355976], 
reward next is 0.4927, 
noisyNet noise sample is [array([0.03636521], dtype=float32), 0.3211382]. 
=============================================
[2019-04-09 15:02:03,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00319269 0.10777822 0.19492687 0.02792896 0.01720884 0.00404163
 0.16652511 0.05014614 0.08319467 0.07486201 0.2701949 ], sum to 1.0000
[2019-04-09 15:02:03,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1791
[2019-04-09 15:02:03,830] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 75.0, 29.0, 28.0, 22.5, 27.89972693200893, 0.9832892645523628, 1.0, 1.0, 20.0, 32.50191983647051], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4728600.0000, 
sim time next is 4729200.0000, 
raw observation next is [0.3333333333333334, 76.0, 24.16666666666667, 23.33333333333334, 22.5, 27.72898946208114, 1.024059336888937, 1.0, 1.0, 25.0, 23.9711595325407], 
processed observation next is [1.0, 0.7391304347826086, 0.4718374884579871, 0.76, 0.08055555555555557, 0.025782688766114188, 0.375, 0.8107491218400952, 0.8413531122963124, 1.0, 1.0, 0.2, 0.239711595325407], 
reward next is 0.7603, 
noisyNet noise sample is [array([-0.45068705], dtype=float32), 1.270049]. 
=============================================
[2019-04-09 15:02:03,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01162648 0.12836844 0.10022457 0.06789105 0.04276973 0.02076666
 0.16544375 0.06476798 0.09016282 0.1069741  0.20100443], sum to 1.0000
[2019-04-09 15:02:03,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01685302 0.12495258 0.11837682 0.05874624 0.04885682 0.02698626
 0.17879297 0.06441473 0.09569746 0.0859149  0.18040822], sum to 1.0000
[2019-04-09 15:02:03,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0267
[2019-04-09 15:02:03,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7041
[2019-04-09 15:02:03,867] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.666666666666666, 88.5, 0.0, 0.0, 19.0, 26.53236910283101, 0.694192609230273, 0.0, 1.0, 45.0, 40.00278621614154], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4762200.0000, 
sim time next is 4762800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.48904321754743, 0.6836853461938396, 0.0, 1.0, 20.0, 39.62757898452958], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7074202681289524, 0.7278951153979465, 0.0, 1.0, 0.1, 0.39627578984529577], 
reward next is 0.6037, 
noisyNet noise sample is [array([-1.1752807], dtype=float32), 0.016698258]. 
=============================================
[2019-04-09 15:02:03,881] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.09014122445281, 0.6068563097983496, 0.0, 1.0, 55.0, 41.3874424570396], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.07059603153537, 0.5930482262080038, 0.0, 1.0, 55.0, 42.89472901581601], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6725496692946141, 0.6976827420693347, 0.0, 1.0, 0.8, 0.42894729015816013], 
reward next is 0.5711, 
noisyNet noise sample is [array([0.53565764], dtype=float32), -0.48814535]. 
=============================================
[2019-04-09 15:02:03,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00290695 0.06525993 0.11971941 0.0216096  0.02156412 0.00402523
 0.22617851 0.04043562 0.07242103 0.07271181 0.35316777], sum to 1.0000
[2019-04-09 15:02:03,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8327
[2019-04-09 15:02:03,971] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 80.5, 0.0, 0.0, 19.0, 26.82238585905574, 0.8152892012177686, 0.0, 1.0, 55.0, 41.22630517989294], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4746600.0000, 
sim time next is 4747200.0000, 
raw observation next is [-3.0, 79.33333333333333, 0.0, 0.0, 19.0, 26.77016426928395, 0.8102796870728818, 0.0, 1.0, 65.0, 45.8346713654774], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.7308470224403291, 0.7700932290242939, 0.0, 1.0, 1.0, 0.458346713654774], 
reward next is 0.5417, 
noisyNet noise sample is [array([0.5715741], dtype=float32), 0.87645125]. 
=============================================
[2019-04-09 15:02:04,086] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00774264 0.11757541 0.15215056 0.0820106  0.03904204 0.01919518
 0.16109848 0.05195228 0.08380535 0.07394959 0.21147785], sum to 1.0000
[2019-04-09 15:02:04,092] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6699
[2019-04-09 15:02:04,114] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.5, 41.5, 170.0, 787.0, 19.0, 26.98522669623494, 0.8555355399954429, 0.0, 1.0, 25.0, 31.2023226135914], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4797000.0000, 
sim time next is 4797600.0000, 
raw observation next is [1.666666666666667, 41.0, 178.3333333333333, 750.5, 19.0, 27.02651105573372, 0.8740410795230025, 0.0, 1.0, 55.0, 25.36335441204941], 
processed observation next is [0.0, 0.5217391304347826, 0.5087719298245615, 0.41, 0.5944444444444443, 0.8292817679558011, 0.08333333333333333, 0.7522092546444767, 0.7913470265076675, 0.0, 1.0, 0.8, 0.2536335441204941], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.03504284], dtype=float32), -1.1807739]. 
=============================================
[2019-04-09 15:02:04,220] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 127275: loss 31.1495
[2019-04-09 15:02:04,221] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 127276: learning rate 0.0000
[2019-04-09 15:02:04,596] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127459: loss 27.7983
[2019-04-09 15:02:04,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127459: learning rate 0.0000
[2019-04-09 15:02:04,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01472158 0.1172141  0.12735546 0.0567401  0.04598763 0.02030949
 0.17417294 0.05413223 0.09339289 0.07826036 0.2177133 ], sum to 1.0000
[2019-04-09 15:02:04,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8284
[2019-04-09 15:02:04,693] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 19.0, 25.66482318876959, 0.4859329213872576, 0.0, 1.0, 45.0, 34.29995054167568], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4777800.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 19.0, 25.59137939706883, 0.4844873813940208, 0.0, 1.0, 65.0, 61.99360102674527], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.632614949755736, 0.6614957937980069, 0.0, 1.0, 1.0, 0.6199360102674527], 
reward next is 0.3801, 
noisyNet noise sample is [array([-0.6846908], dtype=float32), 0.6018947]. 
=============================================
[2019-04-09 15:02:04,842] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127571: loss 26.8269
[2019-04-09 15:02:04,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127572: learning rate 0.0000
[2019-04-09 15:02:05,009] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0033321  0.09516032 0.11601261 0.0341601  0.02036751 0.00403809
 0.20323218 0.03403262 0.08114573 0.08192641 0.32659233], sum to 1.0000
[2019-04-09 15:02:05,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6246
[2019-04-09 15:02:05,037] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 19.0, 27.03332501691504, 0.8795457108084838, 0.0, 1.0, 55.0, 38.61669033918231], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 27.04432491267097, 0.8744762213771433, 0.0, 1.0, 60.0, 37.61146170919725], 
processed observation next is [1.0, 0.9130434782608695, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7536937427225808, 0.7914920737923811, 0.0, 1.0, 0.9, 0.3761146170919725], 
reward next is 0.6239, 
noisyNet noise sample is [array([0.13317215], dtype=float32), 0.011355609]. 
=============================================
[2019-04-09 15:02:05,120] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127718: loss 34.1118
[2019-04-09 15:02:05,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127720: learning rate 0.0000
[2019-04-09 15:02:05,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01191162 0.11118207 0.09817667 0.06140422 0.0513893  0.017902
 0.16974775 0.05078099 0.070597   0.09657094 0.26033753], sum to 1.0000
[2019-04-09 15:02:05,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0820
[2019-04-09 15:02:05,171] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.7849799181222, 0.7075450605816794, 0.0, 1.0, 55.0, 36.50860499634452], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4842600.0000, 
sim time next is 4843200.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 19.0, 26.76639795697024, 0.7009118810299962, 0.0, 1.0, 20.0, 35.06057550083001], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.08333333333333333, 0.7305331630808535, 0.7336372936766654, 0.0, 1.0, 0.1, 0.3506057550083001], 
reward next is 0.6494, 
noisyNet noise sample is [array([-0.4907803], dtype=float32), 0.74205613]. 
=============================================
[2019-04-09 15:02:05,301] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127803: loss 34.6681
[2019-04-09 15:02:05,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127803: learning rate 0.0000
[2019-04-09 15:02:05,415] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 127856: loss 23.2064
[2019-04-09 15:02:05,418] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 127857: learning rate 0.0000
[2019-04-09 15:02:05,582] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127945: loss 29.8932
[2019-04-09 15:02:05,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127945: learning rate 0.0000
[2019-04-09 15:02:05,693] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01041198 0.08384535 0.13594888 0.05350459 0.0335741  0.0132821
 0.22375423 0.0472932  0.06780459 0.08843309 0.24214788], sum to 1.0000
[2019-04-09 15:02:05,694] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0018
[2019-04-09 15:02:05,703] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.1666666666666667, 51.66666666666666, 0.0, 0.0, 19.0, 27.08565007434228, 0.8141856080445334, 0.0, 1.0, 45.0, 33.82785546780728], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 4828200.0000, 
sim time next is 4828800.0000, 
raw observation next is [-0.3333333333333333, 52.33333333333334, 0.0, 0.0, 19.0, 27.08476519916883, 0.748163617719321, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4533702677747, 0.5233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7570637665974026, 0.749387872573107, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02040339], dtype=float32), -0.4030763]. 
=============================================
[2019-04-09 15:02:05,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01142929 0.08861617 0.16145276 0.0506264  0.02311448 0.01250545
 0.19781245 0.08334054 0.06126272 0.06385087 0.24598892], sum to 1.0000
[2019-04-09 15:02:05,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8085
[2019-04-09 15:02:05,875] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.56080947804569, 0.7137418651198676, 0.0, 1.0, 25.0, 37.84784366654728], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.50267145362604, 0.7121703632296318, 0.0, 1.0, 65.0, 57.58398357385604], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7085559544688366, 0.7373901210765439, 0.0, 1.0, 1.0, 0.5758398357385603], 
reward next is 0.4242, 
noisyNet noise sample is [array([0.56931245], dtype=float32), -0.3003854]. 
=============================================
[2019-04-09 15:02:05,922] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128118: loss 18.1696
[2019-04-09 15:02:05,926] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128121: learning rate 0.0000
[2019-04-09 15:02:06,051] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128189: loss 32.4800
[2019-04-09 15:02:06,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128189: learning rate 0.0000
[2019-04-09 15:02:06,105] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00923954 0.13198021 0.13494433 0.05783024 0.03666297 0.01926245
 0.15840521 0.04271276 0.10337137 0.06679314 0.23879787], sum to 1.0000
[2019-04-09 15:02:06,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7260
[2019-04-09 15:02:06,126] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.833333333333333, 35.0, 49.33333333333333, 306.3333333333333, 19.0, 27.59110129854335, 0.9272773814559964, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4813800.0000, 
sim time next is 4814400.0000, 
raw observation next is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 19.0, 27.46726710324999, 0.9555790576139297, 0.0, 1.0, 60.0, 47.81032693978155], 
processed observation next is [0.0, 0.7391304347826086, 0.5364727608494922, 0.36, 0.1372222222222222, 0.27145488029465936, 0.08333333333333333, 0.7889389252708326, 0.8185263525379766, 0.0, 1.0, 0.9, 0.47810326939781544], 
reward next is 0.5219, 
noisyNet noise sample is [array([1.1184224], dtype=float32), 0.80129224]. 
=============================================
[2019-04-09 15:02:06,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00770611 0.09800038 0.12499693 0.04253566 0.03405913 0.0142871
 0.19737726 0.05592727 0.08172289 0.08876519 0.2546221 ], sum to 1.0000
[2019-04-09 15:02:06,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-09 15:02:06,400] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 37.0, 105.5, 729.5, 19.0, 27.13256976122232, 0.9230092862827286, 0.0, 1.0, 25.0, 33.05396209832472], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [3.0, 37.0, 97.0, 727.0, 19.0, 27.23485661767065, 0.9372075743060299, 0.0, 1.0, 55.0, 30.48690972768263], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3233333333333333, 0.8033149171270718, 0.08333333333333333, 0.7695713848058876, 0.8124025247686766, 0.0, 1.0, 0.8, 0.3048690972768263], 
reward next is 0.6951, 
noisyNet noise sample is [array([-0.2488754], dtype=float32), 0.09566908]. 
=============================================
[2019-04-09 15:02:06,452] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00724124 0.09811437 0.16118412 0.04095827 0.02978254 0.01498899
 0.16938666 0.04694606 0.08387408 0.07825995 0.26926365], sum to 1.0000
[2019-04-09 15:02:06,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9630
[2019-04-09 15:02:06,464] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 43.66666666666667, 0.0, 0.0, 19.0, 27.27462795622664, 0.8786808128590158, 0.0, 1.0, 55.0, 28.90396768742296], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4821000.0000, 
sim time next is 4821600.0000, 
raw observation next is [1.0, 44.33333333333334, 0.0, 0.0, 19.0, 27.26469372454621, 0.8774774128369733, 0.0, 1.0, 65.0, 34.8060452322868], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4433333333333334, 0.0, 0.0, 0.08333333333333333, 0.7720578103788508, 0.7924924709456578, 0.0, 1.0, 1.0, 0.348060452322868], 
reward next is 0.6519, 
noisyNet noise sample is [array([0.12219038], dtype=float32), -0.2811684]. 
=============================================
[2019-04-09 15:02:06,552] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01108302 0.11200869 0.15418516 0.0612471  0.02948892 0.01884972
 0.1446099  0.05217936 0.08215239 0.08046867 0.25372708], sum to 1.0000
[2019-04-09 15:02:06,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7799
[2019-04-09 15:02:06,570] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 19.0, 26.58506993254646, 0.75512433081335, 0.0, 1.0, 20.0, 32.89017196227335], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4792800.0000, 
sim time next is 4793400.0000, 
raw observation next is [-0.5, 44.5, 122.0, 839.0, 19.0, 26.58390794841171, 0.7662005719663254, 0.0, 1.0, 65.0, 43.57135329528509], 
processed observation next is [0.0, 0.4782608695652174, 0.44875346260387816, 0.445, 0.4066666666666667, 0.9270718232044199, 0.08333333333333333, 0.7153256623676425, 0.7554001906554418, 0.0, 1.0, 1.0, 0.4357135329528509], 
reward next is 0.5643, 
noisyNet noise sample is [array([-1.8826094], dtype=float32), 0.8430728]. 
=============================================
[2019-04-09 15:02:06,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01844571 0.10738654 0.12342516 0.06939073 0.05817197 0.02790923
 0.18308686 0.05704267 0.07749828 0.0700836  0.20755926], sum to 1.0000
[2019-04-09 15:02:06,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2569
[2019-04-09 15:02:06,734] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 26.32384759972173, 0.5957935924940561, 0.0, 1.0, 20.0, 46.50161038367095], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4862400.0000, 
sim time next is 4863000.0000, 
raw observation next is [-3.833333333333333, 64.16666666666667, 0.0, 0.0, 19.0, 26.33251118718997, 0.5916615246255789, 0.0, 1.0, 30.0, 34.45277006691133], 
processed observation next is [0.0, 0.2608695652173913, 0.3564173591874424, 0.6416666666666667, 0.0, 0.0, 0.08333333333333333, 0.6943759322658307, 0.6972205082085262, 0.0, 1.0, 0.3, 0.34452770066911326], 
reward next is 0.6555, 
noisyNet noise sample is [array([-0.20733194], dtype=float32), -0.113370314]. 
=============================================
[2019-04-09 15:02:06,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[2.6382782]
 [2.9815938]
 [2.678399 ]
 [2.9775085]
 [2.7820375]], R is [[3.7001493 ]
 [4.19813156]
 [4.55252504]
 [5.16059828]
 [5.74191332]].
[2019-04-09 15:02:06,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00776091 0.10706679 0.12703599 0.05678871 0.03396804 0.01804549
 0.14627557 0.04926354 0.07945424 0.06397608 0.31036466], sum to 1.0000
[2019-04-09 15:02:06,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2808
[2019-04-09 15:02:06,881] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 41.0, 0.0, 0.0, 19.0, 27.34567696493392, 0.8940388212913474, 0.0, 1.0, 30.0, 28.16923755002707], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4818000.0000, 
sim time next is 4818600.0000, 
raw observation next is [1.5, 41.5, 0.0, 0.0, 19.0, 27.35418962160004, 0.8846747082814349, 0.0, 1.0, 30.0, 25.71413506092476], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.415, 0.0, 0.0, 0.08333333333333333, 0.7795158018000032, 0.7948915694271449, 0.0, 1.0, 0.3, 0.2571413506092476], 
reward next is 0.7429, 
noisyNet noise sample is [array([0.30370396], dtype=float32), 1.8625706]. 
=============================================
[2019-04-09 15:02:06,927] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128640: loss 26.6285
[2019-04-09 15:02:06,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128642: learning rate 0.0000
[2019-04-09 15:02:06,933] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128643: loss 27.2567
[2019-04-09 15:02:06,934] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128645: learning rate 0.0000
[2019-04-09 15:02:07,153] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00904317 0.13256398 0.14612818 0.06756452 0.04071254 0.02436492
 0.13380182 0.05095006 0.09176997 0.08662651 0.21647428], sum to 1.0000
[2019-04-09 15:02:07,154] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3010
[2019-04-09 15:02:07,186] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 160.0, 713.0, 19.0, 27.16856530583201, 0.9108758041982767, 0.0, 1.0, 65.0, 29.26827969206849], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4803600.0000, 
sim time next is 4804200.0000, 
raw observation next is [3.0, 37.0, 148.0, 742.0, 19.0, 27.20199921489541, 0.9170362921113601, 0.0, 1.0, 30.0, 25.85832194845567], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.49333333333333335, 0.8198895027624309, 0.08333333333333333, 0.7668332679079507, 0.80567876403712, 0.0, 1.0, 0.3, 0.2585832194845567], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.09062509], dtype=float32), 0.17341675]. 
=============================================
[2019-04-09 15:02:07,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00978657 0.13180016 0.13630542 0.06805759 0.04209543 0.02135388
 0.14036667 0.05563918 0.08944964 0.08999883 0.21514665], sum to 1.0000
[2019-04-09 15:02:07,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2882
[2019-04-09 15:02:07,247] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 37.0, 139.5, 739.5, 19.0, 27.23815352434936, 0.9236318347829354, 0.0, 1.0, 65.0, 29.53907854260068], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4804800.0000, 
sim time next is 4805400.0000, 
raw observation next is [3.0, 37.0, 131.0, 737.0, 19.0, 27.26796712165911, 0.9304429511654287, 0.0, 1.0, 30.0, 27.08556960640766], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.43666666666666665, 0.8143646408839779, 0.08333333333333333, 0.7723305934715926, 0.8101476503884762, 0.0, 1.0, 0.3, 0.2708556960640766], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.09062509], dtype=float32), 0.17341675]. 
=============================================
[2019-04-09 15:02:07,521] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128938: loss 28.1285
[2019-04-09 15:02:07,522] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128939: learning rate 0.0000
[2019-04-09 15:02:07,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00804476 0.10485079 0.14651808 0.04750565 0.0301097  0.01228669
 0.18223126 0.05314489 0.07962483 0.09803105 0.23765232], sum to 1.0000
[2019-04-09 15:02:07,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7904
[2019-04-09 15:02:07,564] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 19.0, 27.15969067962908, 0.8873154179713819, 0.0, 1.0, 20.0, 26.71678317897939], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4800000.0000, 
sim time next is 4800600.0000, 
raw observation next is [2.5, 38.5, 220.0, 568.0, 19.0, 27.18269145484707, 0.8943380997549936, 0.0, 1.0, 65.0, 29.88961697239567], 
processed observation next is [0.0, 0.5652173913043478, 0.5318559556786704, 0.385, 0.7333333333333333, 0.6276243093922652, 0.08333333333333333, 0.7652242879039225, 0.7981126999183311, 0.0, 1.0, 1.0, 0.2988961697239567], 
reward next is 0.7011, 
noisyNet noise sample is [array([-2.3484435], dtype=float32), -0.07194036]. 
=============================================
[2019-04-09 15:02:07,868] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 129131: loss 25.7541
[2019-04-09 15:02:07,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 129133: learning rate 0.0000
[2019-04-09 15:02:07,907] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.01181745 0.11686562 0.13342369 0.06557715 0.03457263 0.02220052
 0.16813411 0.07274809 0.09723005 0.08248823 0.19494252], sum to 1.0000
[2019-04-09 15:02:07,911] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5312
[2019-04-09 15:02:07,927] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 61.66666666666667, 222.5, 176.5, 19.0, 26.53943315642305, 0.6747442370125504, 0.0, 1.0, 30.0, 35.33536832169786], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4873200.0000, 
sim time next is 4873800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 238.0, 174.0, 19.0, 26.58073382154498, 0.6841330846742535, 0.0, 1.0, 45.0, 34.9019405527704], 
processed observation next is [0.0, 0.391304347826087, 0.4025854108956602, 0.6083333333333333, 0.7933333333333333, 0.19226519337016573, 0.08333333333333333, 0.7150611517954152, 0.7280443615580845, 0.0, 1.0, 0.6, 0.349019405527704], 
reward next is 0.6510, 
noisyNet noise sample is [array([-1.6970271], dtype=float32), 1.0425043]. 
=============================================
[2019-04-09 15:02:08,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00849867 0.11898135 0.10860448 0.0422376  0.04377146 0.01541486
 0.2028974  0.05097874 0.07405058 0.05572076 0.27884406], sum to 1.0000
[2019-04-09 15:02:08,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0720
[2019-04-09 15:02:08,104] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.86867725242089, 0.7262943501024735, 0.0, 1.0, 55.0, 37.09342121705005], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4840800.0000, 
sim time next is 4841400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.84286260794601, 0.7215294827322029, 0.0, 1.0, 20.0, 35.85270206280327], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7369052173288342, 0.740509827577401, 0.0, 1.0, 0.1, 0.35852702062803266], 
reward next is 0.6415, 
noisyNet noise sample is [array([0.3409395], dtype=float32), -0.4340625]. 
=============================================
[2019-04-09 15:02:08,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01396758 0.13125111 0.12485122 0.07212632 0.05565056 0.02740201
 0.17454484 0.0460684  0.07910133 0.08433346 0.19070314], sum to 1.0000
[2019-04-09 15:02:08,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1886
[2019-04-09 15:02:08,127] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 19.0, 26.17346469780228, 0.5617760060058937, 0.0, 1.0, 30.0, 34.2022584845019], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4857600.0000, 
sim time next is 4858200.0000, 
raw observation next is [-3.5, 65.5, 0.0, 0.0, 19.0, 26.15156784593792, 0.5459328644979753, 0.0, 1.0, 25.0, 32.32615410665512], 
processed observation next is [0.0, 0.21739130434782608, 0.36565096952908593, 0.655, 0.0, 0.0, 0.08333333333333333, 0.6792973204948266, 0.6819776214993251, 0.0, 1.0, 0.2, 0.3232615410665512], 
reward next is 0.6767, 
noisyNet noise sample is [array([-0.54836375], dtype=float32), -0.3621336]. 
=============================================
[2019-04-09 15:02:08,182] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 129293: loss 24.9739
[2019-04-09 15:02:08,185] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 129293: learning rate 0.0000
[2019-04-09 15:02:08,306] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0148604  0.11836608 0.1130698  0.05900525 0.04231426 0.0194625
 0.16993476 0.06943347 0.07740398 0.08466495 0.23148456], sum to 1.0000
[2019-04-09 15:02:08,306] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8105
[2019-04-09 15:02:08,326] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.31110966988691, 0.5929635467658233, 0.0, 1.0, 35.0, 30.30506945080585], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4854000.0000, 
sim time next is 4854600.0000, 
raw observation next is [-3.5, 65.5, 0.0, 0.0, 19.0, 26.26064596011455, 0.600884484259114, 0.0, 1.0, 65.0, 58.72048159513365], 
processed observation next is [0.0, 0.17391304347826086, 0.36565096952908593, 0.655, 0.0, 0.0, 0.08333333333333333, 0.6883871633428793, 0.7002948280863713, 0.0, 1.0, 1.0, 0.5872048159513364], 
reward next is 0.4128, 
noisyNet noise sample is [array([-1.4224337], dtype=float32), -0.10419504]. 
=============================================
[2019-04-09 15:02:08,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01451198 0.10761613 0.09807666 0.05605473 0.05227292 0.02165445
 0.19036686 0.05945663 0.08576591 0.08996199 0.22426176], sum to 1.0000
[2019-04-09 15:02:08,411] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7731
[2019-04-09 15:02:08,433] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.30448461967037, 0.5873824506244018, 0.0, 1.0, 65.0, 60.76663352655815], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4860600.0000, 
sim time next is 4861200.0000, 
raw observation next is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.29638311067136, 0.5903088458493758, 0.0, 1.0, 60.0, 50.33099946888795], 
processed observation next is [0.0, 0.2608695652173913, 0.37026777469990774, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.6913652592226134, 0.6967696152831252, 0.0, 1.0, 0.9, 0.5033099946888795], 
reward next is 0.4967, 
noisyNet noise sample is [array([-1.6213886], dtype=float32), 0.6208033]. 
=============================================
[2019-04-09 15:02:08,446] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01552694 0.12239808 0.11338321 0.0722233  0.05358158 0.02136736
 0.17508495 0.0473636  0.08382846 0.08346219 0.21178031], sum to 1.0000
[2019-04-09 15:02:08,446] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7870
[2019-04-09 15:02:08,476] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.29638311067136, 0.5903088458493758, 0.0, 1.0, 60.0, 50.33099946888795], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 19.0, 26.32372650297973, 0.5922517024182851, 0.0, 1.0, 60.0, 46.90560867576956], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.08333333333333333, 0.693643875248311, 0.6974172341394284, 0.0, 1.0, 0.9, 0.46905608675769556], 
reward next is 0.5309, 
noisyNet noise sample is [array([-1.6213886], dtype=float32), 0.6208033]. 
=============================================
[2019-04-09 15:02:08,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01405138 0.10550933 0.11648211 0.06443553 0.04533776 0.0225957
 0.20887761 0.05667746 0.08776392 0.08319776 0.19507161], sum to 1.0000
[2019-04-09 15:02:08,734] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3122
[2019-04-09 15:02:08,756] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 62.5, 0.0, 0.0, 19.0, 26.3897027268438, 0.601418067060273, 0.0, 1.0, 40.0, 40.05405769454596], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 26.38783877090066, 0.5942289730626369, 0.0, 1.0, 35.0, 36.40689705107339], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.6989865642417218, 0.6980763243542123, 0.0, 1.0, 0.4, 0.36406897051073395], 
reward next is 0.6359, 
noisyNet noise sample is [array([-0.04535362], dtype=float32), -1.2977362]. 
=============================================
[2019-04-09 15:02:09,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00735163 0.14037049 0.13361989 0.04301597 0.02976271 0.01460264
 0.15198243 0.03445432 0.09216379 0.05635817 0.29631793], sum to 1.0000
[2019-04-09 15:02:09,006] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9204
[2019-04-09 15:02:09,041] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 40.0, 16.5, 92.5, 19.0, 27.51716133794987, 0.92925919201884, 0.0, 1.0, 65.0, 37.06071926838328], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4816800.0000, 
sim time next is 4817400.0000, 
raw observation next is [1.833333333333333, 40.5, 0.0, 0.0, 19.0, 27.47307919258609, 0.91876497324269, 0.0, 1.0, 65.0, 31.59951025753036], 
processed observation next is [0.0, 0.782608695652174, 0.5133887349953832, 0.405, 0.0, 0.0, 0.08333333333333333, 0.7894232660488409, 0.8062549910808966, 0.0, 1.0, 1.0, 0.3159951025753036], 
reward next is 0.6840, 
noisyNet noise sample is [array([0.7557003], dtype=float32), 1.0195303]. 
=============================================
[2019-04-09 15:02:09,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01347549 0.11342774 0.15830019 0.0735671  0.0476331  0.02115908
 0.16864881 0.05925079 0.09306357 0.07324944 0.17822471], sum to 1.0000
[2019-04-09 15:02:09,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2523
[2019-04-09 15:02:09,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 19.0, 26.54397255050016, 0.6245050320878875, 0.0, 1.0, 55.0, 43.81031838612438], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.51768021820847, 0.621766536849477, 0.0, 1.0, 25.0, 38.17578023517789], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7098066848507057, 0.707255512283159, 0.0, 1.0, 0.2, 0.3817578023517789], 
reward next is 0.6182, 
noisyNet noise sample is [array([-0.5931866], dtype=float32), 1.1985509]. 
=============================================
[2019-04-09 15:02:09,256] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00926269 0.12110677 0.11801871 0.04044884 0.03684172 0.01079198
 0.14006087 0.05947424 0.09353033 0.10776884 0.26269493], sum to 1.0000
[2019-04-09 15:02:09,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-09 15:02:09,282] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.01585709682944, 0.783099979286305, 0.0, 1.0, 65.0, 40.47617682617637], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4832400.0000, 
sim time next is 4833000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.00197997378208, 0.779539253297139, 0.0, 1.0, 30.0, 37.96945073126908], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.7501649978151734, 0.759846417765713, 0.0, 1.0, 0.3, 0.3796945073126908], 
reward next is 0.6203, 
noisyNet noise sample is [array([-0.5241577], dtype=float32), -0.5110523]. 
=============================================
[2019-04-09 15:02:09,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[3.6086135]
 [3.4890792]
 [3.551173 ]
 [3.5332592]
 [3.636061 ]], R is [[4.22219467]
 [4.77521086]
 [5.3715353 ]
 [5.91584539]
 [6.50890779]].
[2019-04-09 15:02:09,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00735657 0.13181567 0.16858476 0.0540921  0.03109748 0.01600264
 0.16427161 0.06345748 0.1062386  0.08954516 0.16753791], sum to 1.0000
[2019-04-09 15:02:09,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3027
[2019-04-09 15:02:09,339] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.0, 45.0, 163.0, 422.0, 19.0, 27.42580708864652, 0.922479263536825, 0.0, 1.0, 65.0, 27.87152246194575], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4894200.0000, 
sim time next is 4894800.0000, 
raw observation next is [3.0, 45.0, 152.8333333333333, 404.5, 19.0, 27.45240070436955, 0.925420871374539, 0.0, 1.0, 25.0, 27.49070444165871], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5094444444444443, 0.44696132596685084, 0.08333333333333333, 0.7877000586974624, 0.808473623791513, 0.0, 1.0, 0.2, 0.2749070444165871], 
reward next is 0.7251, 
noisyNet noise sample is [array([-1.5482308], dtype=float32), 0.014100053]. 
=============================================
[2019-04-09 15:02:09,536] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-09 15:02:09,539] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:02:09,539] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:02:09,541] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:02:09,541] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:02:09,543] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:02:09,543] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:02:09,543] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run14
[2019-04-09 15:02:09,568] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run14
[2019-04-09 15:02:09,584] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run14
[2019-04-09 15:02:23,298] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.01979652], dtype=float32), 0.024793968]
[2019-04-09 15:02:23,298] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-10.68333333333333, 62.83333333333334, 130.0, 504.3333333333334, 22.5, 24.17902835480764, 0.09618547761829162, 1.0, 1.0, 45.0, 26.78815225264817]
[2019-04-09 15:02:23,298] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:02:23,299] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00708111 0.17298928 0.13707142 0.09161253 0.04214602 0.01444583
 0.13142551 0.04804827 0.11026106 0.07213048 0.17278843], sampled 0.2302013104952647
[2019-04-09 15:02:39,058] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01979652], dtype=float32), 0.024793968]
[2019-04-09 15:02:39,059] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [18.68977015333333, 54.88782693666667, 85.49078401, 0.0, 22.5, 29.18926151366434, 1.411371277089219, 1.0, 0.0, 65.0, 7.837084584823833]
[2019-04-09 15:02:39,059] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:02:39,060] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.0037443  0.15975845 0.12067876 0.0556315  0.02694959 0.00767726
 0.1766527  0.04269196 0.11122628 0.05786811 0.23712116], sampled 0.9088350787544452
[2019-04-09 15:03:04,258] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01979652], dtype=float32), 0.024793968]
[2019-04-09 15:03:04,258] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 22.5, 25.77232238971531, 0.6769196414664496, 1.0, 1.0, 25.0, 18.25050572169995]
[2019-04-09 15:03:04,258] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:03:04,259] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.00451435 0.15115583 0.153873   0.05696552 0.02663481 0.01086474
 0.1372719  0.04070887 0.10046793 0.06475642 0.25278664], sampled 0.12066548010317901
[2019-04-09 15:03:39,763] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5653.3812 284015.6534 2845.9223
[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:39,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:40,009] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,155] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5412.4903 308350.9731 2498.8170
[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:55,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,765] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5315.2730 318072.6991 2136.7406
[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,785] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:57,902] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:58,788] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 130000, evaluation results [130000.0, 5412.4902687812855, 308350.97312187095, 2498.8170309349603, 5653.381172930568, 284015.6533730856, 2845.9222555916936, 5315.273008587187, 318072.6991412836, 2136.7405911195356]
[2019-04-09 15:03:58,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01124832 0.14340907 0.11219783 0.05536062 0.04301196 0.0172409
 0.17445105 0.0559007  0.11212277 0.08262222 0.19243461], sum to 1.0000
[2019-04-09 15:03:58,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-09 15:03:58,958] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.666666666666667, 63.33333333333334, 196.0, 201.3333333333333, 19.0, 26.35701737888977, 0.6485160888218614, 0.0, 1.0, 40.0, 38.838794845549], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4872000.0000, 
sim time next is 4872600.0000, 
raw observation next is [-2.5, 62.5, 207.0, 179.0, 19.0, 26.43296812975539, 0.6570601473432974, 0.0, 1.0, 25.0, 34.71650841729286], 
processed observation next is [0.0, 0.391304347826087, 0.39335180055401664, 0.625, 0.69, 0.19779005524861878, 0.08333333333333333, 0.7027473441462826, 0.7190200491144325, 0.0, 1.0, 0.2, 0.3471650841729286], 
reward next is 0.6528, 
noisyNet noise sample is [array([-0.154035], dtype=float32), 1.3925179]. 
=============================================
[2019-04-09 15:03:59,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00594941 0.14349036 0.11228746 0.05926926 0.02713743 0.01273843
 0.19630374 0.0430887  0.11841766 0.07308067 0.20823693], sum to 1.0000
[2019-04-09 15:03:59,238] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4020
[2019-04-09 15:03:59,261] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 44.83333333333334, 275.0, 388.6666666666666, 19.0, 26.68029129811401, 0.7606018727531173, 0.0, 1.0, 55.0, 37.11203159968628], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4885800.0000, 
sim time next is 4886400.0000, 
raw observation next is [1.6, 44.66666666666667, 273.5, 388.3333333333334, 19.0, 26.69504965717476, 0.7749781857186018, 0.0, 1.0, 45.0, 33.73419323385645], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.4466666666666667, 0.9116666666666666, 0.42909760589318613, 0.08333333333333333, 0.72458747143123, 0.7583260619062006, 0.0, 1.0, 0.6, 0.3373419323385645], 
reward next is 0.6627, 
noisyNet noise sample is [array([0.8985677], dtype=float32), 0.030226903]. 
=============================================
[2019-04-09 15:03:59,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01563747 0.13516407 0.11621909 0.06467318 0.04657671 0.02240905
 0.17693307 0.04989733 0.07026692 0.09935826 0.20286489], sum to 1.0000
[2019-04-09 15:03:59,308] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2385
[2019-04-09 15:03:59,323] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 26.44928873594142, 0.6107098397022206, 0.0, 1.0, 65.0, 47.53375828972477], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4860000.0000, 
sim time next is 4860600.0000, 
raw observation next is [-3.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.44457645633046, 0.6087661278983777, 0.0, 1.0, 25.0, 45.00220226190633], 
processed observation next is [0.0, 0.2608695652173913, 0.3748845798707295, 0.6083333333333333, 0.0, 0.0, 0.08333333333333333, 0.7037147046942049, 0.7029220426327926, 0.0, 1.0, 0.2, 0.4500220226190633], 
reward next is 0.5500, 
noisyNet noise sample is [array([1.6616268], dtype=float32), 0.6862697]. 
=============================================
[2019-04-09 15:03:59,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00676422 0.13827589 0.11638854 0.05990551 0.0353907  0.01338897
 0.18025015 0.04214336 0.09258024 0.06501774 0.24989463], sum to 1.0000
[2019-04-09 15:03:59,456] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-09 15:03:59,470] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 19.0, 27.11648019110217, 0.7846976279725671, 0.0, 1.0, 50.0, 29.68259145851119], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 19.0, 27.11617912297044, 0.7819445086817317, 0.0, 1.0, 20.0, 32.65024635935162], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.08333333333333333, 0.75968159358087, 0.7606481695605772, 0.0, 1.0, 0.1, 0.3265024635935162], 
reward next is 0.6735, 
noisyNet noise sample is [array([-0.6999258], dtype=float32), -1.5381455]. 
=============================================
[2019-04-09 15:03:59,845] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00745909 0.09949306 0.13471244 0.06224649 0.03115506 0.01394536
 0.1453569  0.06563824 0.09359396 0.09409928 0.25230014], sum to 1.0000
[2019-04-09 15:03:59,850] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9027
[2019-04-09 15:03:59,863] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 122.3333333333333, 352.0, 19.0, 27.42896679190586, 0.907470513562929, 0.0, 1.0, 45.0, 23.22679325237963], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4896600.0000, 
sim time next is 4897200.0000, 
raw observation next is [3.0, 45.0, 112.1666666666667, 334.5, 19.0, 27.44948121085392, 0.9181031368931225, 0.0, 1.0, 65.0, 32.86459905691963], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.373888888888889, 0.3696132596685083, 0.08333333333333333, 0.7874567675711601, 0.8060343789643741, 0.0, 1.0, 1.0, 0.3286459905691963], 
reward next is 0.6714, 
noisyNet noise sample is [array([-0.6569705], dtype=float32), -0.22683768]. 
=============================================
[2019-04-09 15:03:59,946] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01062611 0.1212219  0.13381097 0.08171217 0.04267297 0.02091542
 0.17501076 0.04786961 0.08705802 0.08767364 0.19142848], sum to 1.0000
[2019-04-09 15:03:59,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0616
[2019-04-09 15:03:59,975] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.166666666666667, 60.83333333333333, 238.0, 174.0, 19.0, 26.40658216617808, 0.6714780817874174, 0.0, 1.0, 65.0, 43.33105228898651], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [-2.0, 60.0, 253.5, 171.5, 19.0, 26.49462164761315, 0.6832567081156983, 0.0, 1.0, 20.0, 39.35720687355472], 
processed observation next is [0.0, 0.43478260869565216, 0.40720221606648205, 0.6, 0.845, 0.18950276243093922, 0.08333333333333333, 0.7078851373010959, 0.7277522360385661, 0.0, 1.0, 0.1, 0.39357206873554723], 
reward next is 0.6064, 
noisyNet noise sample is [array([-0.12245129], dtype=float32), 0.95511234]. 
=============================================
[2019-04-09 15:04:00,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01012913 0.10758227 0.16048    0.06079061 0.03514069 0.01454848
 0.17190252 0.0457018  0.06955454 0.06962996 0.25454003], sum to 1.0000
[2019-04-09 15:04:00,312] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2220
[2019-04-09 15:04:00,324] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 19.0, 27.52751125248417, 0.9068817393222591, 0.0, 1.0, 65.0, 28.54201432353086], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4901400.0000, 
sim time next is 4902000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 27.5, 155.8333333333333, 19.0, 27.50737905418931, 0.9033895213416194, 0.0, 1.0, 25.0, 25.49025301348665], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.4433333333333334, 0.09166666666666666, 0.17219152854511965, 0.08333333333333333, 0.7922815878491093, 0.8011298404472065, 0.0, 1.0, 0.2, 0.2549025301348665], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.6029677], dtype=float32), -0.2159013]. 
=============================================
[2019-04-09 15:04:00,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[3.9809809]
 [4.0819044]
 [3.9946952]
 [4.1680408]
 [4.074859 ]], R is [[4.73019028]
 [5.39746857]
 [6.06250334]
 [6.72588921]
 [7.38564634]].
[2019-04-09 15:04:00,557] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00736784 0.12800173 0.16416453 0.08125564 0.03944808 0.01281021
 0.1451873  0.04484293 0.09772016 0.08416965 0.19503193], sum to 1.0000
[2019-04-09 15:04:00,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9315
[2019-04-09 15:04:00,576] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.266666666666667, 45.66666666666667, 279.5, 389.6666666666666, 19.0, 27.11503203762398, 0.8267886882777596, 0.0, 1.0, 55.0, 32.58869391938878], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4884000.0000, 
sim time next is 4884600.0000, 
raw observation next is [1.333333333333333, 45.33333333333333, 278.0, 389.3333333333334, 19.0, 27.1294867225119, 0.8337839788138363, 0.0, 1.0, 65.0, 32.6675637745688], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.4533333333333333, 0.9266666666666666, 0.4302025782688767, 0.08333333333333333, 0.7607905602093249, 0.7779279929379453, 0.0, 1.0, 1.0, 0.326675637745688], 
reward next is 0.6733, 
noisyNet noise sample is [array([-0.5507094], dtype=float32), -0.8274278]. 
=============================================
[2019-04-09 15:04:00,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01050287 0.13679335 0.11823532 0.06402063 0.04371899 0.01755756
 0.18931845 0.05201911 0.08791039 0.07677841 0.20314488], sum to 1.0000
[2019-04-09 15:04:00,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2584
[2019-04-09 15:04:00,680] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.26722610868615, 0.5880572089340328, 0.0, 1.0, 45.0, 65.05605771645953], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 19.0, 26.19618427399569, 0.589475368750127, 0.0, 1.0, 25.0, 41.12524372387641], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.08333333333333333, 0.6830153561663076, 0.6964917895833757, 0.0, 1.0, 0.2, 0.4112524372387641], 
reward next is 0.5887, 
noisyNet noise sample is [array([-0.49780333], dtype=float32), 1.0067192]. 
=============================================
[2019-04-09 15:04:00,837] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00759883 0.12541293 0.09741341 0.04401686 0.02159886 0.00873537
 0.20106022 0.05915319 0.10726356 0.07465578 0.253091  ], sum to 1.0000
[2019-04-09 15:04:00,838] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5952
[2019-04-09 15:04:00,864] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 48.66666666666667, 0.0, 0.0, 19.0, 26.58405573896631, 0.6047963366285805, 0.0, 1.0, 45.0, 26.77107930622483], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4940400.0000, 
sim time next is 4941000.0000, 
raw observation next is [-2.0, 48.0, 0.0, 0.0, 19.0, 26.54174310999019, 0.6044893923993243, 0.0, 1.0, 55.0, 38.474535479438], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7118119258325158, 0.7014964641331081, 0.0, 1.0, 0.8, 0.38474535479438005], 
reward next is 0.6153, 
noisyNet noise sample is [array([-2.3287754], dtype=float32), 0.34322897]. 
=============================================
[2019-04-09 15:04:00,868] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[4.139078 ]
 [4.0691376]
 [3.9041414]
 [4.170548 ]
 [4.022911 ]], R is [[4.64548302]
 [5.33131742]
 [5.99660587]
 [6.63825321]
 [7.26671696]].
[2019-04-09 15:04:01,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0060351  0.09244999 0.12892821 0.04233502 0.03292398 0.01614464
 0.21595012 0.0393053  0.07694483 0.06656727 0.28241554], sum to 1.0000
[2019-04-09 15:04:01,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4495
[2019-04-09 15:04:01,272] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 19.0, 26.86720046706413, 0.7143688374147944, 0.0, 1.0, 65.0, 42.96606600763985], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 19.0, 27.01486740336287, 0.7212143090297559, 0.0, 1.0, 45.0, 37.26529527902272], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7512389502802392, 0.7404047696765853, 0.0, 1.0, 0.6, 0.37265295279022725], 
reward next is 0.6273, 
noisyNet noise sample is [array([0.44736582], dtype=float32), -0.59756434]. 
=============================================
[2019-04-09 15:04:01,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00697696 0.07802898 0.13854475 0.04761708 0.02407667 0.01091993
 0.22086826 0.05520298 0.05356311 0.05888817 0.3053131 ], sum to 1.0000
[2019-04-09 15:04:01,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5072
[2019-04-09 15:04:01,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00946587 0.10674609 0.13690428 0.04234594 0.03668956 0.01229668
 0.23570092 0.05437655 0.0718789  0.08045666 0.21313863], sum to 1.0000
[2019-04-09 15:04:01,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1970
[2019-04-09 15:04:01,639] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 19.0, 26.93225749285974, 0.6771985633340503, 0.0, 1.0, 30.0, 33.24241978182946], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 19.0, 26.86061861218206, 0.6710714395498781, 0.0, 1.0, 45.0, 30.41752321544412], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7383848843485049, 0.7236904798499594, 0.0, 1.0, 0.6, 0.3041752321544412], 
reward next is 0.6958, 
noisyNet noise sample is [array([1.4300219], dtype=float32), 0.7071893]. 
=============================================
[2019-04-09 15:04:01,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[4.04243  ]
 [3.9466596]
 [4.0002737]
 [3.8901076]
 [3.849594 ]], R is [[4.60840511]
 [5.22989702]
 [5.83062363]
 [6.42666006]
 [7.02715588]].
[2019-04-09 15:04:01,647] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.3333333333333334, 38.0, 0.0, 0.0, 19.0, 27.10538279464166, 0.7592860866180731, 0.0, 1.0, 45.0, 37.01386104543526], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4920000.0000, 
sim time next is 4920600.0000, 
raw observation next is [0.1666666666666666, 38.5, 0.0, 0.0, 19.0, 27.08633739023217, 0.7602251678167474, 0.0, 1.0, 25.0, 31.8910735016062], 
processed observation next is [0.0, 0.9565217391304348, 0.4672206832871654, 0.385, 0.0, 0.0, 0.08333333333333333, 0.7571947825193476, 0.7534083892722491, 0.0, 1.0, 0.2, 0.318910735016062], 
reward next is 0.6811, 
noisyNet noise sample is [array([1.2064626], dtype=float32), 0.13183944]. 
=============================================
[2019-04-09 15:04:01,924] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00880532 0.11399108 0.12563041 0.06519353 0.03628685 0.01156668
 0.17220974 0.0734254  0.12309263 0.07767314 0.19212529], sum to 1.0000
[2019-04-09 15:04:01,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6279
[2019-04-09 15:04:01,946] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.066666666666667, 46.66666666666667, 281.6666666666666, 362.6666666666667, 19.0, 27.00027015109625, 0.8140928889188195, 0.0, 1.0, 55.0, 32.56888624814979], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [1.133333333333333, 46.33333333333334, 281.3333333333334, 376.3333333333333, 19.0, 27.07492044982197, 0.8245538795511088, 0.0, 1.0, 55.0, 24.76278266666709], 
processed observation next is [0.0, 0.5217391304347826, 0.49399815327793173, 0.46333333333333343, 0.937777777777778, 0.4158379373848987, 0.08333333333333333, 0.7562433708184976, 0.7748512931837029, 0.0, 1.0, 0.8, 0.24762782666667088], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.31692067], dtype=float32), 0.5883202]. 
=============================================
[2019-04-09 15:04:01,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00658755 0.1116632  0.12474941 0.04825485 0.02077457 0.01107863
 0.20772229 0.05129867 0.07677975 0.07666174 0.26442924], sum to 1.0000
[2019-04-09 15:04:01,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6343
[2019-04-09 15:04:01,979] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.64088410605721, 0.6273593171088488, 0.0, 1.0, 20.0, 37.8108104017274], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4946400.0000, 
sim time next is 4947000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.68580719253969, 0.6281230723815507, 0.0, 1.0, 55.0, 34.15246853089219], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7238172660449743, 0.709374357460517, 0.0, 1.0, 0.8, 0.34152468530892194], 
reward next is 0.6585, 
noisyNet noise sample is [array([1.1486068], dtype=float32), 1.1501212]. 
=============================================
[2019-04-09 15:04:01,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[3.776474 ]
 [3.845301 ]
 [4.096497 ]
 [3.7924218]
 [3.8548553]], R is [[4.62190342]
 [5.19757652]
 [5.68626213]
 [6.13193464]
 [6.6745677 ]].
[2019-04-09 15:04:02,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00560343 0.07109293 0.11736151 0.03559862 0.02155361 0.00835055
 0.2027535  0.05032351 0.06377374 0.07056159 0.35302708], sum to 1.0000
[2019-04-09 15:04:02,516] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6542
[2019-04-09 15:04:02,530] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 46.66666666666667, 0.0, 0.0, 19.0, 26.80657925334128, 0.6493723325869225, 0.0, 1.0, 45.0, 37.02917789208237], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4942200.0000, 
sim time next is 4942800.0000, 
raw observation next is [-2.0, 46.0, 0.0, 0.0, 19.0, 26.80657660877962, 0.6473554788565326, 0.0, 1.0, 65.0, 41.29900339102624], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 0.46, 0.0, 0.0, 0.08333333333333333, 0.7338813840649682, 0.7157851596188441, 0.0, 1.0, 1.0, 0.4129900339102624], 
reward next is 0.5870, 
noisyNet noise sample is [array([0.5874596], dtype=float32), 0.09363282]. 
=============================================
[2019-04-09 15:04:02,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00893261 0.10498771 0.13915925 0.03667025 0.02646291 0.01166032
 0.17986527 0.048825   0.07185299 0.07136067 0.30022305], sum to 1.0000
[2019-04-09 15:04:02,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8951
[2019-04-09 15:04:02,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00432696 0.13000396 0.16655171 0.04806842 0.02738131 0.00799918
 0.1508289  0.04777775 0.06666262 0.06817488 0.28222433], sum to 1.0000
[2019-04-09 15:04:02,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7985
[2019-04-09 15:04:02,604] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 39.0, 100.5, 638.5, 22.5, 27.29073914537292, 0.7859000363141507, 1.0, 1.0, 65.0, 39.62835776781619], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4957200.0000, 
sim time next is 4957800.0000, 
raw observation next is [-0.6666666666666667, 37.5, 103.0, 664.6666666666667, 22.5, 27.50542450762939, 0.8037944097852584, 1.0, 1.0, 65.0, 24.97473039834107], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.375, 0.3433333333333333, 0.734438305709024, 0.375, 0.7921187089691157, 0.7679314699284195, 1.0, 1.0, 1.0, 0.2497473039834107], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.23476644], dtype=float32), -0.8907585]. 
=============================================
[2019-04-09 15:04:02,617] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 19.0, 26.77925741837521, 0.640951006607902, 0.0, 1.0, 25.0, 35.55757129921612], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4944000.0000, 
sim time next is 4944600.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 19.0, 26.77408827804395, 0.6304578963007147, 0.0, 1.0, 65.0, 41.86896667532101], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7311740231703293, 0.7101526321002383, 0.0, 1.0, 1.0, 0.4186896667532101], 
reward next is 0.5813, 
noisyNet noise sample is [array([-0.29063773], dtype=float32), -0.3409857]. 
=============================================
[2019-04-09 15:04:02,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00570252 0.08536301 0.17050989 0.04696091 0.01614397 0.00882515
 0.18602386 0.04372291 0.09378275 0.05255435 0.2904107 ], sum to 1.0000
[2019-04-09 15:04:02,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-09 15:04:02,737] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 22.5, 26.662062905067, 0.6843049533871378, 1.0, 1.0, 65.0, 36.02740693663409], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4954800.0000, 
sim time next is 4955400.0000, 
raw observation next is [-1.5, 42.5, 93.0, 560.0, 22.5, 26.85012989352068, 0.7116619808020378, 1.0, 1.0, 65.0, 30.10409398537426], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.425, 0.31, 0.6187845303867403, 0.375, 0.7375108244600567, 0.7372206602673459, 1.0, 1.0, 1.0, 0.3010409398537426], 
reward next is 0.6990, 
noisyNet noise sample is [array([-1.1296996], dtype=float32), 1.0239241]. 
=============================================
[2019-04-09 15:04:02,807] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00222415 0.07209826 0.20442036 0.03083339 0.00958273 0.00919477
 0.19267325 0.02845687 0.0667122  0.05553652 0.3282675 ], sum to 1.0000
[2019-04-09 15:04:02,807] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2070
[2019-04-09 15:04:02,822] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [8.0, 26.0, 113.0, 839.5, 22.5, 28.87182004688233, 1.189733366658726, 1.0, 1.0, 25.0, 4.77813369439975], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4975200.0000, 
sim time next is 4975800.0000, 
raw observation next is [8.0, 26.0, 111.6666666666667, 832.6666666666667, 22.5, 28.93654013726588, 1.200256443873038, 1.0, 1.0, 25.0, 2.504742580160904], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.37222222222222234, 0.9200736648250462, 0.375, 0.9113783447721566, 0.9000854812910127, 1.0, 1.0, 0.2, 0.02504742580160904], 
reward next is 0.9750, 
noisyNet noise sample is [array([0.06007256], dtype=float32), 1.3668544]. 
=============================================
[2019-04-09 15:04:02,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00455534 0.0958887  0.10143696 0.03451434 0.02213818 0.00826287
 0.19089991 0.04268675 0.08588571 0.06330297 0.35042822], sum to 1.0000
[2019-04-09 15:04:02,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5726
[2019-04-09 15:04:02,881] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.72605039755559, 0.6186862084110615, 0.0, 1.0, 65.0, 41.80231571769768], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4947600.0000, 
sim time next is 4948200.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.68956091005436, 0.6209079166817314, 0.0, 1.0, 55.0, 41.32469371171835], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7241300758378634, 0.7069693055605771, 0.0, 1.0, 0.8, 0.4132469371171835], 
reward next is 0.5868, 
noisyNet noise sample is [array([-0.94269556], dtype=float32), -1.1450055]. 
=============================================
[2019-04-09 15:04:03,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00947247 0.08125864 0.18196277 0.04367359 0.02635404 0.01773713
 0.18662435 0.04318177 0.04781447 0.0753865  0.2865343 ], sum to 1.0000
[2019-04-09 15:04:03,034] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3068
[2019-04-09 15:04:03,050] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 46.5, 0.0, 0.0, 19.0, 27.0355531753554, 0.711913921218597, 0.0, 1.0, 65.0, 52.64646802583039], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 19.0, 27.01575296863289, 0.7073938135363437, 0.0, 1.0, 30.0, 38.78162618084354], 
processed observation next is [1.0, 0.043478260869565216, 0.44413665743305636, 0.47666666666666657, 0.0, 0.0, 0.08333333333333333, 0.751312747386074, 0.7357979378454479, 0.0, 1.0, 0.3, 0.38781626180843537], 
reward next is 0.6122, 
noisyNet noise sample is [array([-1.7818785], dtype=float32), -0.21839687]. 
=============================================
[2019-04-09 15:04:03,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00260174 0.16739    0.1299947  0.05863479 0.01864687 0.00950802
 0.11717589 0.04251337 0.14073557 0.0435264  0.26927257], sum to 1.0000
[2019-04-09 15:04:03,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4310
[2019-04-09 15:04:03,142] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 22.5, 28.04735983434896, 0.9309782124376255, 1.0, 1.0, 55.0, 17.58200933952904], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4962000.0000, 
sim time next is 4962600.0000, 
raw observation next is [2.0, 29.5, 117.0, 803.0, 22.5, 28.05571593185781, 0.9486007117363688, 1.0, 1.0, 45.0, 17.49576015526426], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.295, 0.39, 0.887292817679558, 0.375, 0.8379763276548177, 0.8162002372454563, 1.0, 1.0, 0.6, 0.17495760155264262], 
reward next is 0.8250, 
noisyNet noise sample is [array([-0.4854963], dtype=float32), -1.4659265]. 
=============================================
[2019-04-09 15:04:03,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01009212 0.12407302 0.13175733 0.06021351 0.03366932 0.0153007
 0.21127373 0.04527956 0.06948321 0.07427301 0.22458446], sum to 1.0000
[2019-04-09 15:04:03,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-09 15:04:03,286] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.0, 39.0, 0.0, 0.0, 19.0, 27.01797799204921, 0.7305852066518219, 0.0, 1.0, 30.0, 32.7611750133068], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 19.0, 26.99221184536375, 0.725374905371746, 0.0, 1.0, 60.0, 39.61182098127487], 
processed observation next is [0.0, 1.0, 0.4672206832871654, 0.39166666666666655, 0.0, 0.0, 0.08333333333333333, 0.749350987113646, 0.7417916351239153, 0.0, 1.0, 0.9, 0.3961182098127487], 
reward next is 0.6039, 
noisyNet noise sample is [array([1.3400025], dtype=float32), 1.0920322]. 
=============================================
[2019-04-09 15:04:03,411] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00245055 0.15522867 0.16138399 0.04157611 0.01966348 0.0054578
 0.19779693 0.02449656 0.09418461 0.05936855 0.23839286], sum to 1.0000
[2019-04-09 15:04:03,411] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0966
[2019-04-09 15:04:03,435] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 29.5, 117.0, 803.0, 22.5, 28.00121876555337, 0.9385699677467167, 1.0, 1.0, 30.0, 18.1280321065275], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 22.5, 28.04734415947815, 0.9547204867625153, 1.0, 1.0, 20.0, 16.00635285555569], 
processed observation next is [1.0, 0.43478260869565216, 0.5272391505078486, 0.2933333333333334, 0.39277777777777767, 0.8950276243093923, 0.375, 0.8372786799565125, 0.8182401622541717, 1.0, 1.0, 0.1, 0.1600635285555569], 
reward next is 0.8399, 
noisyNet noise sample is [array([0.80889684], dtype=float32), -0.13900374]. 
=============================================
[2019-04-09 15:04:03,805] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00221156 0.1372736  0.1474808  0.04472182 0.01559085 0.0043485
 0.1712267  0.0298026  0.11359619 0.06198818 0.27175924], sum to 1.0000
[2019-04-09 15:04:03,818] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3701
[2019-04-09 15:04:03,860] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 103.3333333333333, 804.0, 22.5, 29.12879832050881, 1.242782489304443, 1.0, 1.0, 60.0, 2.356256156944636], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4978200.0000, 
sim time next is 4978800.0000, 
raw observation next is [8.0, 26.0, 100.5, 796.5, 22.5, 29.16407155114787, 1.172924304831707, 1.0, 1.0, 45.0, 64.00932524484206], 
processed observation next is [1.0, 0.6521739130434783, 0.6842105263157896, 0.26, 0.335, 0.8801104972375691, 0.375, 0.9303392959289892, 0.8909747682772357, 1.0, 1.0, 0.6, 0.6400932524484206], 
reward next is 0.3599, 
noisyNet noise sample is [array([0.7382697], dtype=float32), 2.5798264]. 
=============================================
[2019-04-09 15:04:04,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00666852 0.08185589 0.10396961 0.03025028 0.01826048 0.01025043
 0.2396685  0.04747992 0.06660458 0.05960538 0.33538646], sum to 1.0000
[2019-04-09 15:04:04,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7878
[2019-04-09 15:04:04,051] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 49.33333333333334, 0.0, 0.0, 19.0, 26.77309025096571, 0.6533036363471904, 0.0, 1.0, 60.0, 43.01581304701082], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4939800.0000, 
sim time next is 4940400.0000, 
raw observation next is [-2.0, 48.66666666666667, 0.0, 0.0, 19.0, 26.76089256487811, 0.657374877087847, 0.0, 1.0, 65.0, 41.85594914401036], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.4866666666666667, 0.0, 0.0, 0.08333333333333333, 0.7300743804065091, 0.7191249590292824, 0.0, 1.0, 1.0, 0.4185594914401036], 
reward next is 0.5814, 
noisyNet noise sample is [array([-0.13376403], dtype=float32), -0.37333903]. 
=============================================
[2019-04-09 15:04:04,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00840004 0.09021188 0.14675173 0.05834704 0.01719937 0.01211949
 0.149548   0.06502619 0.09145648 0.08918223 0.27175757], sum to 1.0000
[2019-04-09 15:04:04,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0847
[2019-04-09 15:04:04,313] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.96249159067532, 0.7500173940303659, 0.0, 1.0, 20.0, 31.50372472270495], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 5036400.0000, 
sim time next is 5037000.0000, 
raw observation next is [-2.833333333333333, 65.0, 39.33333333333334, 67.33333333333334, 22.5, 26.8931740151118, 0.7449030644475269, 0.0, 1.0, 20.0, 35.89805993605989], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.65, 0.13111111111111115, 0.07440147329650093, 0.375, 0.74109783459265, 0.7483010214825089, 0.0, 1.0, 0.1, 0.3589805993605989], 
reward next is 0.6410, 
noisyNet noise sample is [array([1.6249249], dtype=float32), 0.6708878]. 
=============================================
[2019-04-09 15:04:04,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[4.2145243]
 [4.3964386]
 [4.3659325]
 [4.3260045]
 [4.398492 ]], R is [[4.82442856]
 [5.46114683]
 [6.07812119]
 [6.68806267]
 [7.26737833]].
[2019-04-09 15:04:04,648] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00372202 0.06752635 0.11449266 0.03795759 0.01890322 0.00639406
 0.22518186 0.04040838 0.0681924  0.06444666 0.35277486], sum to 1.0000
[2019-04-09 15:04:04,658] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8597
[2019-04-09 15:04:04,678] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 27.36842617846754, 0.8838336773787209, 0.0, 1.0, 45.0, 33.41515366009846], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5015400.0000, 
sim time next is 5016000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.36620436367431, 0.8789169598272678, 0.0, 1.0, 65.0, 34.07215818795199], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7805170303061925, 0.7929723199424226, 0.0, 1.0, 1.0, 0.3407215818795199], 
reward next is 0.6593, 
noisyNet noise sample is [array([-0.5409372], dtype=float32), 0.8614225]. 
=============================================
[2019-04-09 15:04:04,684] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[4.640264 ]
 [4.890633 ]
 [4.918121 ]
 [4.8677773]
 [4.910855 ]], R is [[5.32966852]
 [5.94222021]
 [6.56262064]
 [7.17791605]
 [7.75242186]].
[2019-04-09 15:04:04,760] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00374128 0.06992167 0.15310657 0.04513311 0.01449997 0.0093142
 0.19685261 0.04756146 0.04744146 0.05240965 0.36001807], sum to 1.0000
[2019-04-09 15:04:04,764] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2959
[2019-04-09 15:04:04,780] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 27.29367215448359, 0.8631305661156615, 0.0, 1.0, 55.0, 30.67591817746145], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 5017200.0000, 
sim time next is 5017800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.26059608916414, 0.8610529319172094, 0.0, 1.0, 45.0, 31.35012460569508], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7717163407636782, 0.7870176439724031, 0.0, 1.0, 0.6, 0.3135012460569508], 
reward next is 0.6865, 
noisyNet noise sample is [array([0.6552063], dtype=float32), -0.84024924]. 
=============================================
[2019-04-09 15:04:04,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00204839 0.07212797 0.18544987 0.03790162 0.0158609  0.00512457
 0.16782068 0.06140887 0.07148868 0.050731   0.3300375 ], sum to 1.0000
[2019-04-09 15:04:04,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-09 15:04:04,857] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 28.09391590117152, 1.100141403265336, 0.0, 1.0, 45.0, 16.76916205371477], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4996800.0000, 
sim time next is 4997400.0000, 
raw observation next is [5.666666666666667, 24.0, 0.0, 0.0, 19.0, 28.07060370253841, 1.091404032785724, 0.0, 1.0, 20.0, 19.01113015762312], 
processed observation next is [1.0, 0.8695652173913043, 0.6195752539242845, 0.24, 0.0, 0.0, 0.08333333333333333, 0.8392169752115342, 0.863801344261908, 0.0, 1.0, 0.1, 0.1901113015762312], 
reward next is 0.8099, 
noisyNet noise sample is [array([1.6266284], dtype=float32), -1.1236281]. 
=============================================
[2019-04-09 15:04:04,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00657439 0.08675401 0.12230231 0.0332541  0.02158054 0.00891558
 0.22451802 0.04597328 0.0757501  0.08013146 0.29424626], sum to 1.0000
[2019-04-09 15:04:04,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9997
[2019-04-09 15:04:04,898] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 61.83333333333334, 0.0, 0.0, 19.0, 26.93521778587358, 0.761895636164438, 0.0, 1.0, 20.0, 32.6251023939818], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5035800.0000, 
sim time next is 5036400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.96929777181022, 0.7490984502244761, 0.0, 1.0, 65.0, 36.45704009592811], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7474414809841848, 0.7496994834081586, 0.0, 1.0, 1.0, 0.36457040095928106], 
reward next is 0.6354, 
noisyNet noise sample is [array([-0.17370851], dtype=float32), 1.7052604]. 
=============================================
[2019-04-09 15:04:04,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00457418 0.10771639 0.11286452 0.03422515 0.02143504 0.0076865
 0.23976688 0.05522345 0.05873068 0.05851231 0.29926485], sum to 1.0000
[2019-04-09 15:04:04,948] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5725
[2019-04-09 15:04:04,962] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.0, 36.0, 0.0, 0.0, 19.0, 27.69752475348224, 0.9749859747875842, 0.0, 1.0, 50.0, 30.32768549827646], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 5005200.0000, 
sim time next is 5005800.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 19.0, 27.61496215379227, 0.9672787032468385, 0.0, 1.0, 50.0, 28.62916112882007], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.355, 0.0, 0.0, 0.08333333333333333, 0.8012468461493558, 0.8224262344156128, 0.0, 1.0, 0.7, 0.2862916112882007], 
reward next is 0.7137, 
noisyNet noise sample is [array([-0.32279655], dtype=float32), 0.6927618]. 
=============================================
[2019-04-09 15:04:05,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00210842 0.12223613 0.17186826 0.03789648 0.01278434 0.00539731
 0.19193108 0.03341917 0.07692352 0.04092656 0.30450872], sum to 1.0000
[2019-04-09 15:04:05,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0110
[2019-04-09 15:04:05,106] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 23.66666666666666, 0.0, 0.0, 22.5, 28.71485024092309, 1.209741695055112, 1.0, 1.0, 25.0, 13.36495952456468], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4992000.0000, 
sim time next is 4992600.0000, 
raw observation next is [6.0, 23.33333333333334, 0.0, 0.0, 22.5, 28.66283859857418, 1.193607892045365, 1.0, 1.0, 20.0, 11.45605380288867], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2333333333333334, 0.0, 0.0, 0.375, 0.8885698832145149, 0.8978692973484549, 1.0, 1.0, 0.1, 0.1145605380288867], 
reward next is 0.8854, 
noisyNet noise sample is [array([-0.00369537], dtype=float32), -1.8777573]. 
=============================================
[2019-04-09 15:04:05,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00128417 0.09244016 0.1352225  0.04316543 0.00932419 0.00473235
 0.18472965 0.03698104 0.08514147 0.04102269 0.36595628], sum to 1.0000
[2019-04-09 15:04:05,608] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0768
[2019-04-09 15:04:05,614] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.0, 26.0, 103.3333333333333, 804.0, 22.5, 29.0991393164867, 1.234777980798447, 1.0, 1.0, 65.0, 4.941282563055419], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4978200.0000, 
sim time next is 4978800.0000, 
raw observation next is [8.0, 26.0, 100.5, 796.5, 22.5, 29.13259515577446, 1.238642787516306, 1.0, 1.0, 20.0, 2.368078980726795], 
processed observation next is [1.0, 0.6521739130434783, 0.6842105263157896, 0.26, 0.335, 0.8801104972375691, 0.375, 0.927716262981205, 0.912880929172102, 1.0, 1.0, 0.1, 0.02368078980726795], 
reward next is 0.9763, 
noisyNet noise sample is [array([0.64864665], dtype=float32), -1.2482889]. 
=============================================
[2019-04-09 15:04:05,712] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00389224 0.1502287  0.13289338 0.05038406 0.01782779 0.00617883
 0.12836309 0.04012549 0.120194   0.04630165 0.30361074], sum to 1.0000
[2019-04-09 15:04:05,714] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4451
[2019-04-09 15:04:05,741] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 29.83333333333333, 114.0, 774.3333333333333, 22.5, 28.01612135398228, 0.9245323023738514, 1.0, 1.0, 55.0, 17.07727589409119], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4961400.0000, 
sim time next is 4962000.0000, 
raw observation next is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 22.5, 28.06310260001436, 0.9361455553344311, 1.0, 1.0, 45.0, 17.72866862655974], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.2966666666666667, 0.385, 0.8714548802946593, 0.375, 0.83859188333453, 0.8120485184448104, 1.0, 1.0, 0.6, 0.1772866862655974], 
reward next is 0.8227, 
noisyNet noise sample is [array([-0.64483625], dtype=float32), 0.9672963]. 
=============================================
[2019-04-09 15:04:05,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[5.960592 ]
 [5.710224 ]
 [5.6146016]
 [5.596542 ]
 [5.2623434]], R is [[6.73516941]
 [7.49704504]
 [8.24326515]
 [8.97604656]
 [9.69522381]].
[2019-04-09 15:04:05,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00556477 0.08802361 0.11439075 0.03777681 0.02075811 0.00809054
 0.25731966 0.06297343 0.07600193 0.06973661 0.25936386], sum to 1.0000
[2019-04-09 15:04:05,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8405
[2019-04-09 15:04:05,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.19484804642087, 0.8446718334830007, 0.0, 1.0, 45.0, 33.21313426924164], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5022000.0000, 
sim time next is 5022600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.25476871378415, 0.828729419799828, 0.0, 1.0, 65.0, 31.01956253755614], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.771230726148679, 0.776243139933276, 0.0, 1.0, 1.0, 0.3101956253755614], 
reward next is 0.6898, 
noisyNet noise sample is [array([0.33616766], dtype=float32), 1.7753987]. 
=============================================
[2019-04-09 15:04:05,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00742446 0.08942626 0.114059   0.03887447 0.02472962 0.00987221
 0.25319955 0.06808957 0.06977342 0.07209565 0.25245583], sum to 1.0000
[2019-04-09 15:04:05,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6264
[2019-04-09 15:04:05,916] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.22219390925206, 0.8308867868147058, 0.0, 1.0, 40.0, 35.48364916113909], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5023200.0000, 
sim time next is 5023800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.25899321989362, 0.8266171927959034, 0.0, 1.0, 55.0, 28.53628675915074], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.7715827683244681, 0.7755390642653012, 0.0, 1.0, 0.8, 0.2853628675915074], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.33616766], dtype=float32), 1.7753987]. 
=============================================
[2019-04-09 15:04:06,629] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00139979 0.10644698 0.18425569 0.03070604 0.01476396 0.00490483
 0.15925784 0.03323528 0.07318832 0.04356016 0.34828115], sum to 1.0000
[2019-04-09 15:04:06,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2551
[2019-04-09 15:04:06,670] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 28.870119646009, 1.218845029109923, 1.0, 1.0, 20.0, 7.022800445327205], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 22.5, 28.79735858712827, 1.114805355200925, 1.0, 1.0, 25.0, 72.62367905425033], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.375, 0.8997798822606891, 0.871601785066975, 1.0, 1.0, 0.2, 0.7262367905425033], 
reward next is 0.2738, 
noisyNet noise sample is [array([-0.9580729], dtype=float32), 0.2238629]. 
=============================================
[2019-04-09 15:04:06,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00711128 0.08784604 0.10387783 0.04399764 0.02319525 0.00894119
 0.21249172 0.05313374 0.08707321 0.07548253 0.2968495 ], sum to 1.0000
[2019-04-09 15:04:06,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-09 15:04:06,733] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 55.5, 0.0, 0.0, 19.0, 26.9860407292776, 0.7460514973111398, 0.0, 1.0, 65.0, 40.00640107328108], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5034600.0000, 
sim time next is 5035200.0000, 
raw observation next is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 19.0, 26.93778429857498, 0.7458894564182644, 0.0, 1.0, 55.0, 35.00752100945479], 
processed observation next is [1.0, 0.2608695652173913, 0.3979686057248385, 0.5866666666666666, 0.0, 0.0, 0.08333333333333333, 0.7448153582145816, 0.7486298188060881, 0.0, 1.0, 0.8, 0.3500752100945479], 
reward next is 0.6499, 
noisyNet noise sample is [array([1.4402822], dtype=float32), 0.14455622]. 
=============================================
[2019-04-09 15:04:06,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00370965 0.08069713 0.11458801 0.02356235 0.02082448 0.00726233
 0.21797375 0.05823168 0.05394645 0.05099789 0.36820635], sum to 1.0000
[2019-04-09 15:04:06,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6481
[2019-04-09 15:04:06,972] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.350000000000001, 22.5, 0.0, 0.0, 19.0, 28.03626148378032, 1.100392449553681, 0.0, 1.0, 20.0, 22.64622986678105], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5094600.0000, 
sim time next is 5095200.0000, 
raw observation next is [8.3, 25.0, 0.0, 0.0, 19.0, 28.00440347090038, 1.093795123462418, 0.0, 1.0, 65.0, 26.45524868716581], 
processed observation next is [1.0, 1.0, 0.6925207756232689, 0.25, 0.0, 0.0, 0.08333333333333333, 0.8337002892416985, 0.8645983744874727, 0.0, 1.0, 1.0, 0.2645524868716581], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.05748459], dtype=float32), 1.0361366]. 
=============================================
[2019-04-09 15:04:07,115] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:07,162] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00132866 0.1292779  0.19852203 0.03092674 0.01214605 0.00348702
 0.19494772 0.02916344 0.1223371  0.04417267 0.23369075], sum to 1.0000
[2019-04-09 15:04:07,169] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9009
[2019-04-09 15:04:07,176] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 24.66666666666667, 0.0, 0.0, 22.5, 28.8875009849586, 1.243885299817169, 1.0, 1.0, 65.0, 7.270206656387622], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 28.90800199292595, 1.230837236578263, 1.0, 1.0, 65.0, 6.874009993914703], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.375, 0.9090001660771625, 0.9102790788594209, 1.0, 1.0, 1.0, 0.06874009993914702], 
reward next is 0.9313, 
noisyNet noise sample is [array([-1.8297924], dtype=float32), -1.3168494]. 
=============================================
[2019-04-09 15:04:07,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00479496 0.12869841 0.14598186 0.03393245 0.01561253 0.00962358
 0.2171005  0.04697612 0.0673843  0.06563938 0.26425594], sum to 1.0000
[2019-04-09 15:04:07,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0610
[2019-04-09 15:04:07,219] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 62.00000000000001, 84.33333333333333, 389.0, 22.5, 26.97029600057903, 0.8058480391223188, 1.0, 1.0, 25.0, 29.48823810945024], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 5040600.0000, 
sim time next is 5041200.0000, 
raw observation next is [-1.0, 59.0, 90.66666666666667, 461.0, 22.5, 27.03128478998034, 0.8455740350907103, 1.0, 1.0, 20.0, 25.31876526260462], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.59, 0.3022222222222222, 0.5093922651933702, 0.375, 0.752607065831695, 0.7818580116969035, 1.0, 1.0, 0.1, 0.2531876526260462], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.8276334], dtype=float32), -1.7189544]. 
=============================================
[2019-04-09 15:04:07,265] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:07,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00537146 0.09183916 0.11271068 0.03816829 0.02478919 0.0099237
 0.24414364 0.04828429 0.08100969 0.05235067 0.29140925], sum to 1.0000
[2019-04-09 15:04:07,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1904
[2019-04-09 15:04:07,421] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 55.5, 0.0, 0.0, 19.0, 26.98858278305121, 0.7466701562097526, 0.0, 1.0, 65.0, 40.01172515929559], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 5034600.0000, 
sim time next is 5035200.0000, 
raw observation next is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 19.0, 26.94052747839088, 0.74653074884184, 0.0, 1.0, 25.0, 35.00225354618723], 
processed observation next is [1.0, 0.2608695652173913, 0.3979686057248385, 0.5866666666666666, 0.0, 0.0, 0.08333333333333333, 0.7450439565325734, 0.74884358294728, 0.0, 1.0, 0.2, 0.3500225354618723], 
reward next is 0.6500, 
noisyNet noise sample is [array([0.9052665], dtype=float32), -0.6610056]. 
=============================================
[2019-04-09 15:04:07,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00186532 0.15895565 0.1854213  0.03135421 0.01130303 0.00614378
 0.16016707 0.02975417 0.08659845 0.05289083 0.27554616], sum to 1.0000
[2019-04-09 15:04:07,750] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:07,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0313
[2019-04-09 15:04:07,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 22.5, 28.98480944575567, 1.061562029491058, 1.0, 1.0, 60.0, 33.28820418656171], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 5055600.0000, 
sim time next is 5056200.0000, 
raw observation next is [8.5, 25.5, 124.0, 865.0, 22.5, 28.89135140019577, 1.27167786777329, 1.0, 1.0, 20.0, 5.504631621362898], 
processed observation next is [1.0, 0.5217391304347826, 0.698060941828255, 0.255, 0.41333333333333333, 0.9558011049723757, 0.375, 0.9076126166829809, 0.9238926225910967, 1.0, 1.0, 0.1, 0.05504631621362898], 
reward next is 0.9450, 
noisyNet noise sample is [array([-1.1268985], dtype=float32), 0.1279806]. 
=============================================
[2019-04-09 15:04:07,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00193316 0.16814634 0.12625195 0.03863565 0.01380875 0.00610594
 0.17049116 0.03261647 0.08054654 0.04803354 0.31343046], sum to 1.0000
[2019-04-09 15:04:07,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2269
[2019-04-09 15:04:07,834] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [11.5, 19.5, 111.0, 819.0, 22.5, 29.70226519774637, 1.412126056324364, 1.0, 1.0, 45.0, 0.3382675939881361], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 5063400.0000, 
sim time next is 5064000.0000, 
raw observation next is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 22.5, 29.7599630744917, 1.423887517850547, 1.0, 1.0, 25.0, 0.1015050935138851], 
processed observation next is [1.0, 0.6086956521739131, 0.785780240073869, 0.19333333333333338, 0.3616666666666667, 0.8913443830570902, 0.375, 0.9799969228743084, 0.974629172616849, 1.0, 1.0, 0.2, 0.001015050935138851], 
reward next is 0.9990, 
noisyNet noise sample is [array([-1.272023], dtype=float32), 0.02971932]. 
=============================================
[2019-04-09 15:04:07,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[6.4395895]
 [6.6374464]
 [6.3765793]
 [6.5432854]
 [6.318177 ]], R is [[ 7.51511574]
 [ 8.43658257]
 [ 9.34886742]
 [10.24279404]
 [11.13824368]].
[2019-04-09 15:04:07,901] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:08,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00124638 0.13635358 0.13378173 0.03639466 0.01284433 0.00551198
 0.18839279 0.02504308 0.11324392 0.04068952 0.30649802], sum to 1.0000
[2019-04-09 15:04:08,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-09 15:04:08,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00757611 0.08146131 0.13234682 0.04779911 0.0164216  0.01114546
 0.24032982 0.04900846 0.06602703 0.05563137 0.29225287], sum to 1.0000
[2019-04-09 15:04:08,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3320
[2019-04-09 15:04:08,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 28.5306225181997, 1.304650628754342, 1.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 22.5, 29.12311222116077, 1.339334447284309, 1.0, 1.0, 55.0, 0.2087441771541906], 
processed observation next is [1.0, 0.5652173913043478, 0.7580794090489382, 0.2083333333333334, 0.38555555555555565, 0.9351749539594842, 0.375, 0.9269260184300642, 0.9464448157614364, 1.0, 1.0, 0.8, 0.002087441771541906], 
reward next is 0.9979, 
noisyNet noise sample is [array([0.5917829], dtype=float32), 0.94504195]. 
=============================================
[2019-04-09 15:04:08,103] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 27.24278401715333, 0.850404269189843, 0.0, 1.0, 20.0, 30.38864391700846], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 5018400.0000, 
sim time next is 5019000.0000, 
raw observation next is [0.6666666666666667, 42.5, 0.0, 0.0, 19.0, 27.20904054281448, 0.8499584485609625, 0.0, 1.0, 45.0, 32.18307763256411], 
processed observation next is [1.0, 0.08695652173913043, 0.4810710987996307, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7674200452345401, 0.7833194828536542, 0.0, 1.0, 0.6, 0.32183077632564105], 
reward next is 0.6782, 
noisyNet noise sample is [array([-1.0509222], dtype=float32), 0.6414351]. 
=============================================
[2019-04-09 15:04:08,109] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:08,110] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:08,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[4.2873282]
 [4.5370784]
 [4.732003 ]
 [4.5888257]
 [4.615245 ]], R is [[5.0454464 ]
 [5.69110537]
 [6.31907749]
 [6.95318413]
 [7.54952526]].
[2019-04-09 15:04:08,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[6.5658364]
 [6.494589 ]
 [6.575868 ]
 [6.6462803]
 [6.4401455]], R is [[ 7.77099562]
 [ 8.69328594]
 [ 9.57244587]
 [10.00192833]
 [10.88084316]].
[2019-04-09 15:04:08,139] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run2
[2019-04-09 15:04:08,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00125956 0.11465039 0.1817178  0.03561632 0.00944422 0.00378133
 0.18318269 0.027837   0.10450339 0.04045363 0.2975537 ], sum to 1.0000
[2019-04-09 15:04:08,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4941
[2019-04-09 15:04:08,463] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 22.5, 118.0, 860.0, 22.5, 29.30382276622022, 1.251378468472416, 1.0, 1.0, 45.0, 47.86108760721025], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 5059800.0000, 
sim time next is 5060400.0000, 
raw observation next is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 28.89045358696904, 1.354476294400439, 1.0, 1.0, 45.0, 3.242169231238271], 
processed observation next is [1.0, 0.5652173913043478, 0.7488457987072946, 0.21666666666666662, 0.3894444444444443, 0.9427255985267036, 0.375, 0.9075377989140868, 0.9514920981334797, 1.0, 1.0, 0.6, 0.032421692312382705], 
reward next is 0.9676, 
noisyNet noise sample is [array([-1.1059213], dtype=float32), 0.18194951]. 
=============================================
[2019-04-09 15:04:08,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00169556 0.0806266  0.12080221 0.02579205 0.0126689  0.00374438
 0.16395724 0.02407415 0.08327121 0.07201249 0.41135517], sum to 1.0000
[2019-04-09 15:04:08,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-09 15:04:08,515] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 22.5, 29.44635478420162, 1.370129450582547, 1.0, 1.0, 20.0, 4.61954102888758], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5078400.0000, 
sim time next is 5079000.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 22.5, 29.30686946705243, 1.350001636777437, 1.0, 1.0, 55.0, 5.209769725572861], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.375, 0.9422391222543691, 0.9500005455924789, 1.0, 1.0, 0.8, 0.05209769725572861], 
reward next is 0.9479, 
noisyNet noise sample is [array([1.8881379], dtype=float32), -1.4551787]. 
=============================================
[2019-04-09 15:04:08,528] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00482444 0.09268238 0.13607228 0.05063699 0.02284093 0.00768011
 0.24428552 0.03300299 0.06224814 0.06698594 0.2787403 ], sum to 1.0000
[2019-04-09 15:04:08,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[6.861094 ]
 [6.853639 ]
 [6.6944714]
 [6.9466276]
 [6.981354 ]], R is [[ 7.76932621]
 [ 8.64543819]
 [ 9.50421715]
 [10.39266586]
 [11.2887392 ]].
[2019-04-09 15:04:08,534] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7965
[2019-04-09 15:04:08,548] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 27.26256680209102, 0.8571670824487304, 0.0, 1.0, 50.0, 30.76981044185581], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5018400.0000, 
sim time next is 5019000.0000, 
raw observation next is [0.6666666666666667, 42.5, 0.0, 0.0, 19.0, 27.22855214261261, 0.856671874467327, 0.0, 1.0, 65.0, 36.87522016189265], 
processed observation next is [1.0, 0.08695652173913043, 0.4810710987996307, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7690460118843842, 0.785557291489109, 0.0, 1.0, 1.0, 0.36875220161892647], 
reward next is 0.6312, 
noisyNet noise sample is [array([0.0027873], dtype=float32), -0.42262515]. 
=============================================
[2019-04-09 15:04:08,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[4.4846687]
 [4.5631537]
 [4.7068906]
 [4.6376977]
 [4.5167823]], R is [[5.16165066]
 [5.80233574]
 [6.38368464]
 [6.95361948]
 [7.53428364]].
[2019-04-09 15:04:08,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:08,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00205896 0.15913182 0.15528578 0.03588901 0.01455434 0.00399293
 0.1903272  0.02286539 0.11158315 0.05502818 0.24928322], sum to 1.0000
[2019-04-09 15:04:08,644] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6995
[2019-04-09 15:04:08,680] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.333333333333333, 40.16666666666667, 114.6666666666667, 772.0, 22.5, 28.26983018214051, 1.053983622766265, 1.0, 1.0, 65.0, 10.55356145323363], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 5047800.0000, 
sim time next is 5048400.0000, 
raw observation next is [3.666666666666667, 39.33333333333334, 115.3333333333333, 790.5, 22.5, 28.37068364298079, 1.075501265295949, 1.0, 1.0, 60.0, 9.544306781742213], 
processed observation next is [1.0, 0.43478260869565216, 0.564173591874423, 0.3933333333333334, 0.3844444444444443, 0.8734806629834254, 0.375, 0.8642236369150659, 0.8585004217653163, 1.0, 1.0, 0.9, 0.09544306781742212], 
reward next is 0.9046, 
noisyNet noise sample is [array([0.14670853], dtype=float32), -0.8417991]. 
=============================================
[2019-04-09 15:04:08,690] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00080344 0.1066106  0.12971917 0.04356219 0.00970922 0.00405229
 0.21143071 0.02849269 0.09278262 0.04993437 0.3229027 ], sum to 1.0000
[2019-04-09 15:04:08,711] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00494149 0.11860476 0.1647775  0.04673067 0.0169718  0.0120493
 0.18798368 0.05449852 0.069672   0.05903179 0.26473847], sum to 1.0000
[2019-04-09 15:04:08,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7234
[2019-04-09 15:04:08,711] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8705
[2019-04-09 15:04:08,723] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 47.33333333333333, 0.0, 0.0, 19.0, 27.04500299712598, 0.7748125745579313, 0.0, 1.0, 65.0, 37.41377301016608], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5031600.0000, 
sim time next is 5032200.0000, 
raw observation next is [-1.0, 46.66666666666667, 0.0, 0.0, 19.0, 27.06129709796206, 0.7625011110038743, 0.0, 1.0, 65.0, 35.46954302902818], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.7551080914968384, 0.7541670370012915, 0.0, 1.0, 1.0, 0.35469543029028183], 
reward next is 0.6453, 
noisyNet noise sample is [array([1.8951489], dtype=float32), 0.05523787]. 
=============================================
[2019-04-09 15:04:08,739] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 17.66666666666666, 69.33333333333333, 536.1666666666666, 22.5, 30.25628272580839, 1.451116302420236, 1.0, 1.0, 25.0, 45.70414261378468], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5071200.0000, 
sim time next is 5071800.0000, 
raw observation next is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 22.5, 29.75090362935289, 1.516340524333367, 1.0, 1.0, 65.0, 1.491300454649842], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1733333333333334, 0.2088888888888889, 0.5384898710865563, 0.375, 0.9792419691127409, 1.0054468414444557, 1.0, 1.0, 1.0, 0.01491300454649842], 
reward next is 0.9851, 
noisyNet noise sample is [array([-1.9199649], dtype=float32), -0.5168359]. 
=============================================
[2019-04-09 15:04:08,763] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:08,766] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:08,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run2
[2019-04-09 15:04:08,776] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:08,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0018169  0.12545781 0.14228733 0.04692068 0.01435419 0.0035352
 0.18814218 0.03289669 0.13060829 0.05818668 0.25579405], sum to 1.0000
[2019-04-09 15:04:08,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6213
[2019-04-09 15:04:08,872] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [5.0, 36.0, 119.5, 827.0, 22.5, 28.46797028795515, 1.167803873625622, 1.0, 1.0, 45.0, 10.11465791797908], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5050800.0000, 
sim time next is 5051400.0000, 
raw observation next is [5.5, 34.33333333333334, 120.6666666666667, 833.0, 22.5, 28.25499429582817, 1.136707306729578, 1.0, 1.0, 55.0, 3.20437140638209], 
processed observation next is [1.0, 0.4782608695652174, 0.6149584487534627, 0.34333333333333343, 0.4022222222222223, 0.9204419889502763, 0.375, 0.8545828579856808, 0.878902435576526, 1.0, 1.0, 0.8, 0.0320437140638209], 
reward next is 0.9680, 
noisyNet noise sample is [array([0.07508554], dtype=float32), -1.676188]. 
=============================================
[2019-04-09 15:04:08,966] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00252622 0.08789646 0.16861296 0.02651057 0.01466156 0.00660428
 0.13446106 0.03614807 0.1159163  0.06012721 0.34653533], sum to 1.0000
[2019-04-09 15:04:08,968] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2990
[2019-04-09 15:04:08,989] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 41.0, 114.0, 753.5, 22.5, 28.19555435893674, 1.040810958425883, 1.0, 1.0, 45.0, 12.61942022165989], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5047200.0000, 
sim time next is 5047800.0000, 
raw observation next is [3.333333333333333, 40.16666666666667, 114.6666666666667, 772.0, 22.5, 28.24348398681796, 1.058599884944914, 1.0, 1.0, 35.0, 10.5696068436058], 
processed observation next is [1.0, 0.43478260869565216, 0.5549399815327793, 0.4016666666666667, 0.38222222222222235, 0.8530386740331491, 0.375, 0.8536236655681634, 0.8528666283149713, 1.0, 1.0, 0.4, 0.105696068436058], 
reward next is 0.8943, 
noisyNet noise sample is [array([0.4368769], dtype=float32), 0.3426612]. 
=============================================
[2019-04-09 15:04:09,026] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,204] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:09,229] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00161738 0.1005917  0.15801808 0.04336745 0.01323158 0.00772596
 0.11450344 0.03826197 0.09459247 0.03357538 0.39451456], sum to 1.0000
[2019-04-09 15:04:09,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6836
[2019-04-09 15:04:09,250] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.0, 17.0, 18.0, 146.0, 22.5, 29.87087889293927, 1.452780116109239, 1.0, 1.0, 55.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 5076000.0000, 
sim time next is 5076600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 22.5, 29.81708908772568, 1.448598505100516, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.375, 0.9847574239771401, 0.9828661683668386, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32715023], dtype=float32), 0.2735957]. 
=============================================
[2019-04-09 15:04:09,267] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,434] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:09,444] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:09,508] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00327601 0.07191245 0.14989835 0.02260198 0.01216895 0.00487649
 0.30163664 0.0455551  0.05696733 0.08175548 0.24935126], sum to 1.0000
[2019-04-09 15:04:09,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-09 15:04:09,529] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.2, 30.0, 0.0, 0.0, 19.0, 27.94825354346212, 1.085043181945957, 0.0, 1.0, 25.0, 23.25434063703236], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 5096400.0000, 
sim time next is 5097000.0000, 
raw observation next is [8.15, 32.5, 0.0, 0.0, 19.0, 27.96580078459604, 1.077294084254708, 0.0, 1.0, 60.0, 25.09909401270144], 
processed observation next is [1.0, 1.0, 0.6883656509695293, 0.325, 0.0, 0.0, 0.08333333333333333, 0.8304833987163368, 0.8590980280849028, 0.0, 1.0, 0.9, 0.2509909401270144], 
reward next is 0.7490, 
noisyNet noise sample is [array([-3.072801], dtype=float32), -0.23075671]. 
=============================================
[2019-04-09 15:04:09,536] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00203628 0.13514872 0.15629762 0.02631065 0.01201618 0.00305285
 0.17565301 0.02472082 0.06854868 0.06984443 0.32637078], sum to 1.0000
[2019-04-09 15:04:09,536] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1468
[2019-04-09 15:04:09,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[5.0245304]
 [4.9741907]
 [5.0974226]
 [5.138087 ]
 [5.012329 ]], R is [[5.66449594]
 [6.37530756]
 [7.08051682]
 [7.77832747]
 [8.4923315 ]].
[2019-04-09 15:04:09,543] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [9.5, 19.0, 0.0, 0.0, 19.0, 28.53175814135714, 1.239567375907299, 0.0, 1.0, 65.0, 16.04566289133361], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 5085000.0000, 
sim time next is 5085600.0000, 
raw observation next is [9.333333333333334, 19.0, 0.0, 0.0, 19.0, 28.49570122877005, 1.238266451629784, 0.0, 1.0, 25.0, 14.52959156109812], 
processed observation next is [1.0, 0.8695652173913043, 0.7211449676823639, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8746417690641707, 0.9127554838765947, 0.0, 1.0, 0.2, 0.1452959156109812], 
reward next is 0.8547, 
noisyNet noise sample is [array([-0.06782745], dtype=float32), 0.7757406]. 
=============================================
[2019-04-09 15:04:09,621] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,635] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:09,636] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:09,638] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run2
[2019-04-09 15:04:09,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00104954 0.08079267 0.14706188 0.02936612 0.01810463 0.00485501
 0.19153742 0.04884939 0.08140185 0.0460898  0.3508917 ], sum to 1.0000
[2019-04-09 15:04:09,665] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2121
[2019-04-09 15:04:09,698] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [8.85, 19.0, 0.0, 0.0, 19.0, 28.37007660019528, 1.202674296610853, 0.0, 1.0, 65.0, 18.03724367444081], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 5088600.0000, 
sim time next is 5089200.0000, 
raw observation next is [8.799999999999999, 19.0, 0.0, 0.0, 19.0, 28.34549245419605, 1.196527699754501, 0.0, 1.0, 25.0, 18.16851990680073], 
processed observation next is [1.0, 0.9130434782608695, 0.7063711911357342, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8621243711830043, 0.8988425665848337, 0.0, 1.0, 0.2, 0.18168519906800729], 
reward next is 0.8183, 
noisyNet noise sample is [array([1.436834], dtype=float32), 0.37418458]. 
=============================================
[2019-04-09 15:04:09,701] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,762] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:09,808] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:09,890] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:09,940] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:10,027] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,028] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,035] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run2
[2019-04-09 15:04:10,102] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:10,134] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:10,230] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,230] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,232] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run2
[2019-04-09 15:04:10,268] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,269] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run2
[2019-04-09 15:04:10,292] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:10,323] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:10,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00129924 0.12234192 0.18699083 0.0177181  0.00882755 0.00301821
 0.23156862 0.03657209 0.09527465 0.05562794 0.24076077], sum to 1.0000
[2019-04-09 15:04:10,612] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-09 15:04:10,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 19.0, 0.0, 0.0, 19.0, 28.56423321263004, 1.244676428703797, 0.0, 1.0, 45.0, 13.55452940513601], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 5084400.0000, 
sim time next is 5085000.0000, 
raw observation next is [9.5, 19.0, 0.0, 0.0, 19.0, 28.52415685148038, 1.237179831291036, 0.0, 1.0, 45.0, 14.04151320851909], 
processed observation next is [1.0, 0.8695652173913043, 0.7257617728531857, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8770130709566984, 0.9123932770970121, 0.0, 1.0, 0.6, 0.1404151320851909], 
reward next is 0.8596, 
noisyNet noise sample is [array([-3.0211365], dtype=float32), 0.64859855]. 
=============================================
[2019-04-09 15:04:10,625] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,625] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,627] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run2
[2019-04-09 15:04:10,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[6.2898955]
 [6.394492 ]
 [6.5083666]
 [6.6138988]
 [6.557484 ]], R is [[ 7.07935667]
 [ 7.87301779]
 [ 8.6628685 ]
 [ 9.44925117]
 [10.24301338]].
[2019-04-09 15:04:10,705] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,705] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,707] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run2
[2019-04-09 15:04:10,769] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:10,769] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:10,788] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run2
[2019-04-09 15:04:10,975] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:11,101] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:11,101] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:11,103] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run2
[2019-04-09 15:04:11,165] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:11,165] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:11,167] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run2
[2019-04-09 15:04:11,279] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:11,352] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:11,555] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:11,607] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:11,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:11,847] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:11,847] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:11,973] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:11,973] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:11,975] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run2
[2019-04-09 15:04:12,012] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:04:12,249] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:04:12,353] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:12,353] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:12,355] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run2
[2019-04-09 15:04:12,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:12,533] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:12,534] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run2
[2019-04-09 15:04:12,608] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:12,609] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:12,610] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run2
[2019-04-09 15:04:13,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:04:13,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:13,015] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run2
[2019-04-09 15:04:19,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04452813 0.11065688 0.12606414 0.0753112  0.06125948 0.05169891
 0.11932186 0.09627379 0.07313628 0.11704311 0.12470628], sum to 1.0000
[2019-04-09 15:04:19,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0371
[2019-04-09 15:04:19,998] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 21.30480219344338, -0.4863448892087626, 0.0, 1.0, 20.0, 29.50626710580774], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.34381289371859, -0.5183166929054209, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.2786510744765491, 0.32722776903152634, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7476968], dtype=float32), -1.105281]. 
=============================================
[2019-04-09 15:04:20,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[1.1241586 ]
 [0.93964094]
 [0.7988431 ]
 [0.60426974]
 [0.44522375]], R is [[2.24883318]
 [2.93128204]
 [3.58425927]
 [4.33244944]
 [5.06009817]].
[2019-04-09 15:04:20,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00977701 0.14007378 0.12616853 0.06604521 0.04252318 0.01145131
 0.1627485  0.05722437 0.07335094 0.08893937 0.22169779], sum to 1.0000
[2019-04-09 15:04:20,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3236
[2019-04-09 15:04:20,532] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 67.5, 0.0, 19.0, 24.49666270375062, 0.2092778397239237, 0.0, 1.0, 55.0, 37.22166076729565], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [7.7, 93.0, 70.0, 0.0, 19.0, 24.52158053591287, 0.2050803721199137, 0.0, 1.0, 30.0, 34.23388863763466], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.23333333333333334, 0.0, 0.08333333333333333, 0.5434650446594059, 0.5683601240399713, 0.0, 1.0, 0.3, 0.3423388863763466], 
reward next is 0.6577, 
noisyNet noise sample is [array([0.03406285], dtype=float32), 1.0003154]. 
=============================================
[2019-04-09 15:04:21,832] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00667877 0.1210965  0.13763778 0.04329092 0.02841131 0.01185617
 0.16848844 0.05655077 0.07407945 0.10395841 0.24795145], sum to 1.0000
[2019-04-09 15:04:21,833] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6501
[2019-04-09 15:04:21,854] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.066666666666667, 88.0, 0.0, 0.0, 19.0, 25.06119395498692, 0.3921477147270846, 0.0, 1.0, 65.0, 60.01865069571438], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 70800.0000, 
sim time next is 71400.0000, 
raw observation next is [2.883333333333334, 88.5, 0.0, 0.0, 19.0, 25.15131782371655, 0.4024110592837176, 0.0, 1.0, 60.0, 47.75610904055843], 
processed observation next is [0.0, 0.8260869565217391, 0.5424746075715605, 0.885, 0.0, 0.0, 0.08333333333333333, 0.5959431519763791, 0.6341370197612392, 0.0, 1.0, 0.9, 0.47756109040558425], 
reward next is 0.5224, 
noisyNet noise sample is [array([-0.93759954], dtype=float32), -0.2801836]. 
=============================================
[2019-04-09 15:04:21,997] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.06377038 0.11182746 0.09371783 0.09066731 0.08835192 0.06082668
 0.09217899 0.11934068 0.06963023 0.10411974 0.10556885], sum to 1.0000
[2019-04-09 15:04:21,998] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6382
[2019-04-09 15:04:22,009] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.8, 95.66666666666667, 0.0, 0.0, 19.0, 21.00797424931885, -0.5219251636631687, 0.0, 1.0, 20.0, 29.6082613629931], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2400.0000, 
sim time next is 3000.0000, 
raw observation next is [6.0, 95.83333333333333, 0.0, 0.0, 19.0, 21.07525040370938, -0.498539068203575, 0.0, 1.0, 50.0, 38.71370214570148], 
processed observation next is [0.0, 0.0, 0.6288088642659281, 0.9583333333333333, 0.0, 0.0, 0.08333333333333333, 0.2562708669757816, 0.3338203105988083, 0.0, 1.0, 0.7, 0.38713702145701484], 
reward next is 0.6129, 
noisyNet noise sample is [array([0.70418787], dtype=float32), 0.22060461]. 
=============================================
[2019-04-09 15:04:22,014] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.41560054]
 [0.40627217]
 [0.29576558]
 [0.21608083]
 [0.13166545]], R is [[1.22340155]
 [1.91508496]
 [2.56775951]
 [3.24112988]
 [3.86816359]].
[2019-04-09 15:04:23,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04251115 0.1181467  0.11644968 0.07821912 0.06720369 0.05340748
 0.12028459 0.09787188 0.08359887 0.11428055 0.10802629], sum to 1.0000
[2019-04-09 15:04:23,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9297
[2019-04-09 15:04:23,801] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 21.14802426034549, -0.4906508846863891, 0.0, 1.0, 20.0, 37.45639747822945], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 4800.0000, 
sim time next is 5400.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.25007895425911, -0.464359896914439, 0.0, 1.0, 65.0, 52.06792455612653], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.27083991285492576, 0.34521336769518696, 0.0, 1.0, 1.0, 0.5206792455612653], 
reward next is 0.4793, 
noisyNet noise sample is [array([-0.93632716], dtype=float32), 0.34856123]. 
=============================================
[2019-04-09 15:04:23,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00602844 0.09124464 0.11692392 0.03037091 0.01895878 0.010113
 0.24626447 0.05489254 0.07284054 0.07097006 0.2813927 ], sum to 1.0000
[2019-04-09 15:04:23,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5759
[2019-04-09 15:04:23,933] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.2, 76.5, 0.0, 0.0, 19.0, 25.14651317221004, 0.3303269020370047, 0.0, 1.0, 45.0, 36.79415808109378], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 102600.0000, 
sim time next is 103200.0000, 
raw observation next is [-4.466666666666667, 75.66666666666666, 0.0, 0.0, 19.0, 25.05572631051403, 0.3066292515893468, 0.0, 1.0, 55.0, 43.8821421108527], 
processed observation next is [1.0, 0.17391304347826086, 0.3388734995383195, 0.7566666666666666, 0.0, 0.0, 0.08333333333333333, 0.5879771925428358, 0.6022097505297822, 0.0, 1.0, 0.8, 0.43882142110852695], 
reward next is 0.5612, 
noisyNet noise sample is [array([-0.7714239], dtype=float32), 2.0793908]. 
=============================================
[2019-04-09 15:04:24,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01194753 0.12369862 0.12066033 0.06431615 0.0419321  0.01707199
 0.19239269 0.06084815 0.07344222 0.06667864 0.22701162], sum to 1.0000
[2019-04-09 15:04:24,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5459
[2019-04-09 15:04:24,732] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 23.55198153692076, 0.003664565498563007, 0.0, 1.0, 60.0, 53.43942404616646], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 26400.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.64016652418876, 0.0119257913360569, 0.0, 1.0, 45.0, 40.16292744232275], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.47001387701573005, 0.5039752637786856, 0.0, 1.0, 0.6, 0.4016292744232275], 
reward next is 0.5984, 
noisyNet noise sample is [array([1.4853218], dtype=float32), -0.55124956]. 
=============================================
[2019-04-09 15:04:24,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[3.2139614]
 [3.3242118]
 [3.1615686]
 [3.0820248]
 [2.969494 ]], R is [[3.87009835]
 [4.29700327]
 [4.71321821]
 [5.16474295]
 [5.51936722]].
[2019-04-09 15:04:24,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00525487 0.08695411 0.13007553 0.04353174 0.02999463 0.00756195
 0.23235306 0.0339743  0.06740784 0.05643957 0.30645248], sum to 1.0000
[2019-04-09 15:04:24,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2382
[2019-04-09 15:04:24,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00558489 0.11227803 0.09924713 0.0442325  0.03039377 0.00844889
 0.21387832 0.04768712 0.07780351 0.10071278 0.25973302], sum to 1.0000
[2019-04-09 15:04:24,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8062
[2019-04-09 15:04:24,814] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.883333333333333, 90.33333333333334, 0.0, 0.0, 19.0, 25.27308837210882, 0.3740332794309487, 0.0, 1.0, 60.0, 50.64301066604863], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 94200.0000, 
sim time next is 94800.0000, 
raw observation next is [-2.066666666666667, 89.66666666666667, 0.0, 0.0, 19.0, 25.24116656089609, 0.3709719844731055, 0.0, 1.0, 30.0, 43.07536515480505], 
processed observation next is [1.0, 0.08695652173913043, 0.40535549399815335, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6034305467413409, 0.6236573281577018, 0.0, 1.0, 0.3, 0.4307536515480505], 
reward next is 0.5692, 
noisyNet noise sample is [array([-0.32273248], dtype=float32), -1.5918707]. 
=============================================
[2019-04-09 15:04:24,843] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 19.0, 24.89335719491022, 0.3332597742311354, 0.0, 1.0, 45.0, 45.59794759241289], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [8.200000000000001, 86.0, 90.0, 0.0, 19.0, 24.97087886516922, 0.3458645238781852, 0.0, 1.0, 65.0, 52.00087029398252], 
processed observation next is [0.0, 0.5652173913043478, 0.6897506925207757, 0.86, 0.3, 0.0, 0.08333333333333333, 0.5809065720974349, 0.6152881746260618, 0.0, 1.0, 1.0, 0.5200087029398253], 
reward next is 0.4800, 
noisyNet noise sample is [array([0.6698878], dtype=float32), 0.32842267]. 
=============================================
[2019-04-09 15:04:25,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01265078 0.11578379 0.13138635 0.06314369 0.04672031 0.0173783
 0.18879205 0.06337978 0.08345255 0.0880698  0.18924263], sum to 1.0000
[2019-04-09 15:04:25,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8567
[2019-04-09 15:04:25,281] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.616666666666667, 93.5, 0.0, 0.0, 19.0, 23.00048247832907, -0.1431546351076804, 0.0, 1.0, 55.0, 46.99364192974859], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 13800.0000, 
sim time next is 14400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.05046614133971, -0.1205457142526792, 0.0, 1.0, 65.0, 55.99809660785795], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.420872178444976, 0.45981809524910694, 0.0, 1.0, 1.0, 0.5599809660785795], 
reward next is 0.4400, 
noisyNet noise sample is [array([0.9258558], dtype=float32), 0.2659115]. 
=============================================
[2019-04-09 15:04:25,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01514308 0.11138999 0.12676151 0.05617771 0.03962407 0.01609844
 0.21097535 0.07310513 0.07571355 0.08817641 0.18683472], sum to 1.0000
[2019-04-09 15:04:25,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1223
[2019-04-09 15:04:25,347] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.533333333333333, 94.0, 0.0, 0.0, 19.0, 22.80281225612476, -0.1711635130203877, 0.0, 1.0, 20.0, 51.17269604641046], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 13200.0000, 
sim time next is 13800.0000, 
raw observation next is [7.616666666666667, 93.5, 0.0, 0.0, 19.0, 22.90210842295039, -0.1638842674226356, 0.0, 1.0, 20.0, 35.77256621176432], 
processed observation next is [0.0, 0.13043478260869565, 0.6735918744228995, 0.935, 0.0, 0.0, 0.08333333333333333, 0.40850903524586596, 0.4453719108591215, 0.0, 1.0, 0.1, 0.35772566211764323], 
reward next is 0.6423, 
noisyNet noise sample is [array([-0.16365232], dtype=float32), -1.181524]. 
=============================================
[2019-04-09 15:04:25,847] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01494056 0.13828288 0.14499381 0.05665852 0.03646813 0.01445808
 0.1706438  0.06428039 0.07583746 0.08973806 0.19369826], sum to 1.0000
[2019-04-09 15:04:25,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6049
[2019-04-09 15:04:25,864] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 23.70640587245684, -0.01016906987728482, 0.0, 1.0, 25.0, 36.24290617215588], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.68071492813286, -0.01782595941952165, 0.0, 1.0, 50.0, 34.89719708047512], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.4733929106777384, 0.49405801352682616, 0.0, 1.0, 0.7, 0.3489719708047512], 
reward next is 0.6510, 
noisyNet noise sample is [array([-2.3516111], dtype=float32), -1.3479162]. 
=============================================
[2019-04-09 15:04:26,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01277141 0.14230067 0.09974651 0.05541702 0.04831308 0.01989706
 0.17165904 0.06539442 0.09452574 0.09156389 0.19841114], sum to 1.0000
[2019-04-09 15:04:26,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00720688 0.10391364 0.1275348  0.05418519 0.02704995 0.00834453
 0.2136706  0.05787959 0.07298947 0.08798636 0.23923896], sum to 1.0000
[2019-04-09 15:04:26,014] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2962
[2019-04-09 15:04:26,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8848
[2019-04-09 15:04:26,032] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.283333333333333, 95.5, 0.0, 0.0, 19.0, 22.41177305275095, -0.2623555178894135, 0.0, 1.0, 50.0, 36.1490376270542], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 11400.0000, 
sim time next is 12000.0000, 
raw observation next is [7.366666666666667, 95.0, 0.0, 0.0, 19.0, 22.48431808858756, -0.2596986915915304, 0.0, 1.0, 30.0, 31.63632292286106], 
processed observation next is [0.0, 0.13043478260869565, 0.6666666666666667, 0.95, 0.0, 0.0, 0.08333333333333333, 0.37369317404896335, 0.41343376946948984, 0.0, 1.0, 0.3, 0.3163632292286106], 
reward next is 0.6836, 
noisyNet noise sample is [array([-0.86901903], dtype=float32), -0.6930539]. 
=============================================
[2019-04-09 15:04:26,034] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.1, 88.33333333333333, 94.5, 0.0, 19.0, 24.82144578600579, 0.2855001635238856, 0.0, 1.0, 25.0, 32.22093171350792], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 45600.0000, 
sim time next is 46200.0000, 
raw observation next is [8.200000000000001, 87.16666666666667, 93.0, 0.0, 19.0, 24.79709637146641, 0.2986757663923475, 0.0, 1.0, 65.0, 60.07545903546443], 
processed observation next is [0.0, 0.5217391304347826, 0.6897506925207757, 0.8716666666666667, 0.31, 0.0, 0.08333333333333333, 0.5664246976222008, 0.5995585887974492, 0.0, 1.0, 1.0, 0.6007545903546443], 
reward next is 0.3992, 
noisyNet noise sample is [array([0.6416222], dtype=float32), -1.367095]. 
=============================================
[2019-04-09 15:04:26,041] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.985282 ]
 [2.744986 ]
 [2.6941938]
 [2.507424 ]
 [2.445888 ]], R is [[3.54803991]
 [4.15106916]
 [4.79445982]
 [5.24169064]
 [6.18927383]].
[2019-04-09 15:04:26,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00731585 0.10535195 0.1243838  0.0516133  0.02633695 0.00892182
 0.20545131 0.06315488 0.07618966 0.09045082 0.24082956], sum to 1.0000
[2019-04-09 15:04:26,083] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4748
[2019-04-09 15:04:26,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 19.0, 24.81418470219523, 0.3131913172255248, 0.0, 1.0, 25.0, 47.1087310842608], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [8.200000000000001, 86.0, 90.0, 0.0, 19.0, 24.85304142542088, 0.3149208663195885, 0.0, 1.0, 45.0, 31.96321853263486], 
processed observation next is [0.0, 0.5652173913043478, 0.6897506925207757, 0.86, 0.3, 0.0, 0.08333333333333333, 0.57108678545174, 0.6049736221065295, 0.0, 1.0, 0.6, 0.3196321853263486], 
reward next is 0.6804, 
noisyNet noise sample is [array([0.6416222], dtype=float32), -1.367095]. 
=============================================
[2019-04-09 15:04:26,211] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00964323 0.10970908 0.12340612 0.05707096 0.03757188 0.01190297
 0.1848292  0.08182774 0.07631513 0.08775945 0.21996412], sum to 1.0000
[2019-04-09 15:04:26,211] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4447
[2019-04-09 15:04:26,226] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 14.0, 0.0, 19.0, 24.01995342520058, 0.1242528589767842, 0.0, 1.0, 60.0, 54.68262320579508], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [7.7, 93.0, 17.5, 0.0, 19.0, 24.09315821427352, 0.1361718233933902, 0.0, 1.0, 45.0, 39.86935570854583], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.058333333333333334, 0.0, 0.08333333333333333, 0.5077631845227932, 0.5453906077977967, 0.0, 1.0, 0.6, 0.39869355708545834], 
reward next is 0.6013, 
noisyNet noise sample is [array([-2.6666372], dtype=float32), 1.1268221]. 
=============================================
[2019-04-09 15:04:26,245] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.5384126]
 [3.3728352]
 [3.3546245]
 [3.4280462]
 [3.3315587]], R is [[4.23889542]
 [4.64968014]
 [5.00560856]
 [5.60969639]
 [6.13522863]].
[2019-04-09 15:04:26,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0125482  0.12724751 0.12742375 0.0744625  0.04062605 0.01304399
 0.1839662  0.05279916 0.08131018 0.1049025  0.18166998], sum to 1.0000
[2019-04-09 15:04:26,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-09 15:04:26,326] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 19.0, 24.19083126335246, 0.1328400893508016, 0.0, 1.0, 45.0, 34.69825052271344], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 31200.0000, 
sim time next is 31800.0000, 
raw observation next is [7.7, 93.0, 26.66666666666666, 0.0, 19.0, 24.18599573466681, 0.1457111254365906, 0.0, 1.0, 65.0, 62.41232628195614], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.08888888888888886, 0.0, 0.08333333333333333, 0.5154996445555676, 0.5485703751455302, 0.0, 1.0, 1.0, 0.6241232628195614], 
reward next is 0.3759, 
noisyNet noise sample is [array([-1.1444572], dtype=float32), 0.5517544]. 
=============================================
[2019-04-09 15:04:26,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01247317 0.12509993 0.1309764  0.0579239  0.03403094 0.01099278
 0.17776972 0.06644597 0.0914432  0.07379992 0.21904404], sum to 1.0000
[2019-04-09 15:04:26,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6891
[2019-04-09 15:04:26,459] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.7, 93.0, 35.16666666666666, 0.0, 19.0, 23.57900164494649, 0.03085770546488644, 0.0, 1.0, 45.0, 35.11818423500566], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 33600.0000, 
sim time next is 34200.0000, 
raw observation next is [7.7, 93.0, 38.0, 0.0, 19.0, 23.62791619217344, 0.03896039291363954, 0.0, 1.0, 55.0, 40.4361495188881], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.12666666666666668, 0.0, 0.08333333333333333, 0.46899301601445337, 0.5129867976378798, 0.0, 1.0, 0.8, 0.404361495188881], 
reward next is 0.5956, 
noisyNet noise sample is [array([-0.8550541], dtype=float32), 0.7130234]. 
=============================================
[2019-04-09 15:04:26,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01034136 0.1398048  0.14870678 0.06548475 0.0303511  0.01198139
 0.1874678  0.05854827 0.07169883 0.07826818 0.19734679], sum to 1.0000
[2019-04-09 15:04:26,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5128
[2019-04-09 15:04:26,597] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 67.5, 0.0, 19.0, 24.13209745412885, 0.1524384835209356, 0.0, 1.0, 45.0, 34.38469574516447], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [7.7, 93.0, 70.0, 0.0, 19.0, 24.22046547433602, 0.1514684812970214, 0.0, 1.0, 45.0, 33.97922179973015], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.23333333333333334, 0.0, 0.08333333333333333, 0.518372122861335, 0.5504894937656738, 0.0, 1.0, 0.6, 0.33979221799730147], 
reward next is 0.6602, 
noisyNet noise sample is [array([0.26277235], dtype=float32), 1.4455348]. 
=============================================
[2019-04-09 15:04:26,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00141472 0.08997816 0.17176014 0.035789   0.01363522 0.00427848
 0.21239081 0.02726021 0.07929774 0.06510502 0.2990906 ], sum to 1.0000
[2019-04-09 15:04:26,975] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2048
[2019-04-09 15:04:26,994] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.1, 68.66666666666666, 22.5, 2.5, 22.5, 26.6551261669958, 0.640465073842709, 1.0, 1.0, 45.0, 35.36519547215226], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 146400.0000, 
sim time next is 147000.0000, 
raw observation next is [-7.199999999999999, 69.83333333333334, 18.0, 2.0, 22.5, 26.6183546966031, 0.6282564746176652, 1.0, 1.0, 25.0, 33.3384279708856], 
processed observation next is [1.0, 0.6956521739130435, 0.26315789473684215, 0.6983333333333335, 0.06, 0.0022099447513812156, 0.375, 0.7181962247169249, 0.709418824872555, 1.0, 1.0, 0.2, 0.333384279708856], 
reward next is 0.6666, 
noisyNet noise sample is [array([-1.8386058], dtype=float32), -0.9832133]. 
=============================================
[2019-04-09 15:04:26,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[6.777238 ]
 [7.019893 ]
 [6.8103743]
 [7.0480084]
 [6.682989 ]], R is [[7.44610023]
 [8.01798725]
 [8.57047653]
 [9.10906029]
 [9.64354134]].
[2019-04-09 15:04:27,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0084549  0.12529302 0.16489163 0.05949004 0.05017138 0.01201629
 0.1707297  0.04566493 0.07215941 0.07923463 0.21189411], sum to 1.0000
[2019-04-09 15:04:27,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5601
[2019-04-09 15:04:27,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00166685 0.16476388 0.13679911 0.04342764 0.0170045  0.0039278
 0.1376578  0.04243595 0.13097222 0.05622291 0.26512134], sum to 1.0000
[2019-04-09 15:04:27,260] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 19.0, 24.42013211405505, 0.1770508762262956, 0.0, 1.0, 65.0, 53.23762742178441], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 30600.0000, 
sim time next is 31200.0000, 
raw observation next is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 19.0, 24.44415922102598, 0.1816750859593859, 0.0, 1.0, 45.0, 47.14948035835712], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07944444444444443, 0.0, 0.08333333333333333, 0.5370132684188317, 0.5605583619864619, 0.0, 1.0, 0.6, 0.47149480358357115], 
reward next is 0.5285, 
noisyNet noise sample is [array([-0.66640097], dtype=float32), 0.88730544]. 
=============================================
[2019-04-09 15:04:27,262] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8064
[2019-04-09 15:04:27,291] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 22.5, 26.06224839903522, 0.5673530034127593, 1.0, 1.0, 55.0, 43.24950735851655], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 133800.0000, 
sim time next is 134400.0000, 
raw observation next is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 22.5, 26.05825023836915, 0.5893114951770752, 1.0, 1.0, 65.0, 64.9161952140028], 
processed observation next is [1.0, 0.5652173913043478, 0.2566943674976916, 0.61, 0.4583333333333333, 0.5567219152854513, 0.375, 0.6715208531974293, 0.6964371650590251, 1.0, 1.0, 1.0, 0.649161952140028], 
reward next is 0.3508, 
noisyNet noise sample is [array([0.21274744], dtype=float32), -1.7222849]. 
=============================================
[2019-04-09 15:04:28,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00672159 0.09701692 0.15516302 0.03241567 0.02837529 0.01185904
 0.21446109 0.04748914 0.06434061 0.09667958 0.24547803], sum to 1.0000
[2019-04-09 15:04:28,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7391
[2019-04-09 15:04:28,440] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.133333333333334, 74.66666666666667, 0.0, 0.0, 19.0, 23.96334333936763, 0.08175189772793456, 0.0, 1.0, 45.0, 30.90787164103427], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 106800.0000, 
sim time next is 107400.0000, 
raw observation next is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 19.0, 23.79608361289994, 0.06968547574452472, 0.0, 1.0, 60.0, 58.03158538724132], 
processed observation next is [1.0, 0.21739130434782608, 0.2848568790397045, 0.7483333333333333, 0.0, 0.0, 0.08333333333333333, 0.4830069677416615, 0.5232284919148416, 0.0, 1.0, 0.9, 0.5803158538724132], 
reward next is 0.4197, 
noisyNet noise sample is [array([-0.34646836], dtype=float32), 1.1864058]. 
=============================================
[2019-04-09 15:04:28,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00520265 0.1028519  0.1607694  0.06363696 0.03484162 0.01182564
 0.17519888 0.05408537 0.07657342 0.07691895 0.2380952 ], sum to 1.0000
[2019-04-09 15:04:28,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8368
[2019-04-09 15:04:28,552] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 19.0, 25.04880897854728, 0.363832173032675, 0.0, 1.0, 65.0, 58.26961854887752], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 53400.0000, 
sim time next is 54000.0000, 
raw observation next is [7.2, 86.0, 64.5, 0.0, 19.0, 25.06784148385112, 0.3920550201513986, 0.0, 1.0, 65.0, 59.97699282148528], 
processed observation next is [0.0, 0.6521739130434783, 0.662049861495845, 0.86, 0.215, 0.0, 0.08333333333333333, 0.5889867903209266, 0.6306850067171329, 0.0, 1.0, 1.0, 0.5997699282148529], 
reward next is 0.4002, 
noisyNet noise sample is [array([-1.2363219], dtype=float32), 0.8480814]. 
=============================================
[2019-04-09 15:04:28,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00201207 0.12562701 0.14560568 0.04196585 0.01901591 0.0036166
 0.21756342 0.05172147 0.09595265 0.05832933 0.23858997], sum to 1.0000
[2019-04-09 15:04:28,562] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[3.9841056]
 [3.9140425]
 [3.952567 ]
 [3.8339305]
 [3.9456344]], R is [[4.34862566]
 [4.72244358]
 [5.32858753]
 [5.93441534]
 [6.38907528]].
[2019-04-09 15:04:28,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8435
[2019-04-09 15:04:28,601] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 22.5, 24.50679057873963, 0.2382577807296419, 1.0, 1.0, 55.0, 46.01023116546158], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 22.5, 24.38948376934753, 0.2401264554049695, 0.0, 1.0, 65.0, 71.38739279038467], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.375, 0.532456980778961, 0.5800421518016565, 0.0, 1.0, 1.0, 0.7138739279038466], 
reward next is 0.2861, 
noisyNet noise sample is [array([-1.1745903], dtype=float32), 0.28306872]. 
=============================================
[2019-04-09 15:04:28,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00617185 0.09602035 0.12473452 0.04846343 0.02879911 0.01000666
 0.2395878  0.03981306 0.06470833 0.07439412 0.26730072], sum to 1.0000
[2019-04-09 15:04:28,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0609
[2019-04-09 15:04:28,798] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.05, 90.5, 0.0, 0.0, 19.0, 25.45251445922863, 0.4577673757633151, 0.0, 1.0, 45.0, 40.40422066693947], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 77400.0000, 
sim time next is 78000.0000, 
raw observation next is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 19.0, 25.4648303507817, 0.4489083556102874, 0.0, 1.0, 20.0, 39.29609817230849], 
processed observation next is [0.0, 0.9130434782608695, 0.4866112650046169, 0.9233333333333335, 0.0, 0.0, 0.08333333333333333, 0.622069195898475, 0.6496361185367624, 0.0, 1.0, 0.1, 0.3929609817230849], 
reward next is 0.6070, 
noisyNet noise sample is [array([1.2917552], dtype=float32), -0.9281171]. 
=============================================
[2019-04-09 15:04:28,809] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[4.1394167]
 [4.2806563]
 [4.310692 ]
 [4.281646 ]
 [4.1304126]], R is [[4.75178242]
 [5.3002224 ]
 [5.78600216]
 [6.14291286]
 [6.51636934]].
[2019-04-09 15:04:28,817] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00682624 0.12762688 0.15647177 0.06657537 0.03207374 0.00932376
 0.1818258  0.05511031 0.07027779 0.10747729 0.18641105], sum to 1.0000
[2019-04-09 15:04:28,821] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0960
[2019-04-09 15:04:28,834] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 19.0, 25.47062134195199, 0.4418870812270546, 0.0, 1.0, 45.0, 34.13432848173941], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 19.0, 25.43481637621592, 0.4341576204480443, 0.0, 1.0, 20.0, 32.46406586074994], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6195680313513267, 0.6447192068160148, 0.0, 1.0, 0.1, 0.3246406586074994], 
reward next is 0.6754, 
noisyNet noise sample is [array([-1.5922452], dtype=float32), 1.200003]. 
=============================================
[2019-04-09 15:04:29,265] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00454666 0.1092587  0.13977458 0.04126925 0.01553023 0.00726107
 0.20141569 0.0419689  0.08787794 0.07074817 0.28034875], sum to 1.0000
[2019-04-09 15:04:29,271] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9624
[2019-04-09 15:04:29,301] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 24.36081923126434, 0.170755794196197, 0.0, 1.0, 65.0, 56.6821980689874], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 24.36081306502162, 0.177629558148188, 0.0, 1.0, 65.0, 62.5653586161963], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5300677554184684, 0.5592098527160626, 0.0, 1.0, 1.0, 0.625653586161963], 
reward next is 0.3743, 
noisyNet noise sample is [array([0.29840878], dtype=float32), 0.016933173]. 
=============================================
[2019-04-09 15:04:29,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0061352  0.10203747 0.1159865  0.05305203 0.02115438 0.01047986
 0.21570982 0.05659607 0.08189265 0.06167971 0.2752762 ], sum to 1.0000
[2019-04-09 15:04:29,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0408
[2019-04-09 15:04:29,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00677447 0.07596219 0.11085523 0.04851602 0.03884761 0.0137065
 0.23686783 0.04415214 0.08581984 0.08996539 0.24853276], sum to 1.0000
[2019-04-09 15:04:29,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5959
[2019-04-09 15:04:29,648] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 22.5, 24.57961136115739, 0.225245758726354, 0.0, 1.0, 45.0, 38.99316251882026], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 114000.0000, 
sim time next is 114600.0000, 
raw observation next is [-7.299999999999999, 68.0, 12.33333333333333, 2.999999999999999, 22.5, 24.54180423383253, 0.2068646774874088, 1.0, 1.0, 20.0, 39.61467740020095], 
processed observation next is [1.0, 0.30434782608695654, 0.2603878116343491, 0.68, 0.0411111111111111, 0.0033149171270718224, 0.375, 0.5451503528193774, 0.5689548924958029, 1.0, 1.0, 0.1, 0.3961467740020095], 
reward next is 0.6039, 
noisyNet noise sample is [array([-2.3839123], dtype=float32), -0.29947764]. 
=============================================
[2019-04-09 15:04:29,653] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 19.0, 25.46667611262292, 0.468154516284071, 0.0, 1.0, 65.0, 56.15553250880604], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 81600.0000, 
sim time next is 82200.0000, 
raw observation next is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 19.0, 25.50125159983975, 0.4716102632368146, 0.0, 1.0, 55.0, 45.96670786407611], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.9516666666666667, 0.0, 0.0, 0.08333333333333333, 0.625104299986646, 0.6572034210789383, 0.0, 1.0, 0.8, 0.4596670786407611], 
reward next is 0.5403, 
noisyNet noise sample is [array([-0.5286937], dtype=float32), -0.5396296]. 
=============================================
[2019-04-09 15:04:29,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0050461  0.11666252 0.11110783 0.0483159  0.02084749 0.00785473
 0.1970428  0.05023423 0.08578783 0.06478788 0.29231277], sum to 1.0000
[2019-04-09 15:04:29,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-09 15:04:29,687] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.299999999999999, 68.0, 12.33333333333333, 2.999999999999999, 22.5, 24.54180423383253, 0.2068646774874088, 1.0, 1.0, 20.0, 39.61467740020095], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 114600.0000, 
sim time next is 115200.0000, 
raw observation next is [-7.3, 68.0, 18.5, 4.5, 22.5, 24.46795325173509, 0.1917257572443723, 1.0, 1.0, 20.0, 37.99307951394481], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.68, 0.06166666666666667, 0.004972375690607734, 0.375, 0.5389961043112574, 0.5639085857481241, 1.0, 1.0, 0.1, 0.37993079513944805], 
reward next is 0.6201, 
noisyNet noise sample is [array([-2.3839123], dtype=float32), -0.29947764]. 
=============================================
[2019-04-09 15:04:30,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00546382 0.12394126 0.13917963 0.03812153 0.03030936 0.00935956
 0.1924412  0.04191875 0.10033669 0.08169828 0.23723005], sum to 1.0000
[2019-04-09 15:04:30,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2488
[2019-04-09 15:04:30,209] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.1, 95.0, 0.0, 0.0, 19.0, 25.44478088465849, 0.4474118978867082, 0.0, 1.0, 25.0, 39.12165418881948], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 85200.0000, 
sim time next is 85800.0000, 
raw observation next is [0.04999999999999999, 95.0, 0.0, 0.0, 19.0, 25.44155367253524, 0.4357373675108172, 0.0, 1.0, 25.0, 38.29894223657364], 
processed observation next is [0.0, 1.0, 0.4639889196675901, 0.95, 0.0, 0.0, 0.08333333333333333, 0.62012947271127, 0.6452457891702724, 0.0, 1.0, 0.2, 0.38298942236573635], 
reward next is 0.6170, 
noisyNet noise sample is [array([0.12085089], dtype=float32), -0.4990527]. 
=============================================
[2019-04-09 15:04:30,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00164773 0.1428693  0.18501863 0.03422757 0.01152951 0.00446129
 0.17549261 0.03253139 0.07883579 0.03470352 0.2986826 ], sum to 1.0000
[2019-04-09 15:04:30,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8604
[2019-04-09 15:04:30,847] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 22.5, 25.62013127165229, 0.3777858888350362, 1.0, 1.0, 65.0, 53.83324599191091], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 211200.0000, 
sim time next is 211800.0000, 
raw observation next is [-6.383333333333333, 72.5, 133.6666666666667, 0.0, 22.5, 25.66816505562554, 0.3966786695481637, 1.0, 1.0, 65.0, 55.84064101243516], 
processed observation next is [1.0, 0.43478260869565216, 0.28578024007386893, 0.725, 0.4455555555555557, 0.0, 0.375, 0.6390137546354616, 0.6322262231827213, 1.0, 1.0, 1.0, 0.5584064101243515], 
reward next is 0.4416, 
noisyNet noise sample is [array([0.05176795], dtype=float32), 0.15255482]. 
=============================================
[2019-04-09 15:04:31,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00198119 0.10909285 0.16990592 0.04025488 0.01479427 0.00403343
 0.12740158 0.04988816 0.0689347  0.05071666 0.3629963 ], sum to 1.0000
[2019-04-09 15:04:31,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7813
[2019-04-09 15:04:31,117] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.1, 73.5, 184.0, 13.0, 22.5, 26.00569336283208, 0.4946359178012005, 1.0, 1.0, 20.0, 37.84500026530919], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 127800.0000, 
sim time next is 128400.0000, 
raw observation next is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 22.5, 25.93888514967136, 0.5155905569518883, 1.0, 1.0, 65.0, 64.18445273149558], 
processed observation next is [1.0, 0.4782608695652174, 0.23545706371191139, 0.6933333333333335, 0.5833333333333334, 0.12302025782688762, 0.375, 0.6615737624726133, 0.6718635189839627, 1.0, 1.0, 1.0, 0.6418445273149558], 
reward next is 0.3582, 
noisyNet noise sample is [array([-1.2782232], dtype=float32), 0.6005799]. 
=============================================
[2019-04-09 15:04:31,166] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00172741 0.09466677 0.14140162 0.02695096 0.01411752 0.0033033
 0.21970715 0.04683765 0.07296679 0.06674976 0.3115711 ], sum to 1.0000
[2019-04-09 15:04:31,168] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0712
[2019-04-09 15:04:31,197] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 19.0, 24.53575499112761, 0.2919306629810627, 0.0, 1.0, 65.0, 67.12380591036413], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 162000.0000, 
sim time next is 162600.0000, 
raw observation next is [-8.4, 68.5, 0.0, 0.0, 19.0, 24.58390706331794, 0.2981056508764173, 0.0, 1.0, 20.0, 51.81707267828846], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.685, 0.0, 0.0, 0.08333333333333333, 0.5486589219431618, 0.5993685502921391, 0.0, 1.0, 0.1, 0.5181707267828846], 
reward next is 0.4818, 
noisyNet noise sample is [array([-0.4866565], dtype=float32), -0.08578752]. 
=============================================
[2019-04-09 15:04:31,349] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00342494 0.14019665 0.16875397 0.04454631 0.01561668 0.00652021
 0.1509314  0.03940009 0.0727955  0.09571276 0.26210144], sum to 1.0000
[2019-04-09 15:04:31,349] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1418
[2019-04-09 15:04:31,375] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 22.5, 24.13410091801117, 0.02283530857033423, 1.0, 1.0, 20.0, 29.0915287825563], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 207600.0000, 
sim time next is 208200.0000, 
raw observation next is [-7.483333333333333, 75.5, 94.0, 0.0, 22.5, 24.14938011828409, 0.01635230219006239, 1.0, 1.0, 20.0, 27.68785524731951], 
processed observation next is [1.0, 0.391304347826087, 0.25530932594644506, 0.755, 0.31333333333333335, 0.0, 0.375, 0.5124483431903407, 0.5054507673966875, 1.0, 1.0, 0.1, 0.2768785524731951], 
reward next is 0.7231, 
noisyNet noise sample is [array([-1.6864425], dtype=float32), 0.77896893]. 
=============================================
[2019-04-09 15:04:31,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00225806 0.16298607 0.11661202 0.02700839 0.01173216 0.00278118
 0.21599627 0.03907276 0.08099217 0.05943984 0.28112108], sum to 1.0000
[2019-04-09 15:04:31,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7322
[2019-04-09 15:04:31,801] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 60.5, 56.0, 0.0, 22.5, 25.96666859735526, 0.50916494522782, 1.0, 1.0, 25.0, 41.41301618253014], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 228600.0000, 
sim time next is 229200.0000, 
raw observation next is [-3.2, 61.0, 49.66666666666667, 0.0, 22.5, 26.23742067357286, 0.5326531466121788, 1.0, 1.0, 65.0, 57.9818856058297], 
processed observation next is [1.0, 0.6521739130434783, 0.37396121883656513, 0.61, 0.16555555555555557, 0.0, 0.375, 0.6864517227977384, 0.6775510488707263, 1.0, 1.0, 1.0, 0.5798188560582971], 
reward next is 0.4202, 
noisyNet noise sample is [array([-0.426179], dtype=float32), 0.15629752]. 
=============================================
[2019-04-09 15:04:31,853] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00252337 0.05794716 0.12411557 0.01522462 0.01366575 0.00648085
 0.22369358 0.05037881 0.04958358 0.08861716 0.3677695 ], sum to 1.0000
[2019-04-09 15:04:31,855] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0516
[2019-04-09 15:04:31,866] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.566666666666666, 72.0, 0.0, 0.0, 19.0, 24.30205446701528, 0.1474451723697607, 0.0, 1.0, 45.0, 34.24497018163925], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 174000.0000, 
sim time next is 174600.0000, 
raw observation next is [-8.65, 72.5, 0.0, 0.0, 19.0, 24.22095040820328, 0.1259593116394005, 0.0, 1.0, 20.0, 32.5981268205088], 
processed observation next is [1.0, 0.0, 0.22299168975069253, 0.725, 0.0, 0.0, 0.08333333333333333, 0.51841253401694, 0.5419864372131334, 0.0, 1.0, 0.1, 0.32598126820508805], 
reward next is 0.6740, 
noisyNet noise sample is [array([-0.84794295], dtype=float32), -1.2930573]. 
=============================================
[2019-04-09 15:04:31,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0012475  0.10262123 0.21794704 0.03078861 0.00724123 0.00280225
 0.11986373 0.03139184 0.08649063 0.04518999 0.35441595], sum to 1.0000
[2019-04-09 15:04:31,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-09 15:04:32,025] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 22.5, 26.516484247882, 0.5774843969709691, 1.0, 1.0, 65.0, 46.66358142795278], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 22.5, 26.55193409750403, 0.4916446252021969, 1.0, 1.0, 65.0, 72.69606413179633], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.375, 0.7126611747920025, 0.6638815417340657, 1.0, 1.0, 1.0, 0.7269606413179633], 
reward next is 0.2730, 
noisyNet noise sample is [array([1.1407622], dtype=float32), -0.84924316]. 
=============================================
[2019-04-09 15:04:32,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0010081  0.1268686  0.13811024 0.0514853  0.01671244 0.00397309
 0.18008311 0.04171046 0.07973361 0.05655517 0.30375987], sum to 1.0000
[2019-04-09 15:04:32,069] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2254
[2019-04-09 15:04:32,084] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00398434 0.10541963 0.12755509 0.05408162 0.01647958 0.00793894
 0.20578565 0.04262014 0.09040422 0.08082069 0.26491013], sum to 1.0000
[2019-04-09 15:04:32,086] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.883333333333334, 61.0, 142.0, 358.0, 22.5, 26.21630074597627, 0.5996970256243772, 1.0, 1.0, 45.0, 29.33576630046359], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 136200.0000, 
sim time next is 136800.0000, 
raw observation next is [-6.7, 61.0, 143.5, 295.0, 22.5, 26.23275908699769, 0.5894663139024078, 1.0, 1.0, 25.0, 30.18843534981918], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.47833333333333333, 0.3259668508287293, 0.375, 0.6860632572498074, 0.6964887713008027, 1.0, 1.0, 0.2, 0.3018843534981918], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.5549771], dtype=float32), 0.18042336]. 
=============================================
[2019-04-09 15:04:32,087] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6126
[2019-04-09 15:04:32,121] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00244564 0.08250517 0.14187995 0.02622308 0.01611691 0.00382491
 0.2647854  0.04090368 0.07448512 0.08262672 0.26420337], sum to 1.0000
[2019-04-09 15:04:32,122] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.716666666666667, 62.16666666666666, 39.66666666666666, 6.000000000000001, 22.5, 25.57866712297496, 0.4213423768720706, 1.0, 1.0, 60.0, 49.2881274935252], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [-7.8, 61.0, 41.0, 4.5, 22.5, 25.69777929570943, 0.4257421510025497, 1.0, 1.0, 55.0, 46.36312806251706], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.61, 0.13666666666666666, 0.004972375690607734, 0.375, 0.6414816079757859, 0.6419140503341833, 1.0, 1.0, 0.8, 0.4636312806251706], 
reward next is 0.5364, 
noisyNet noise sample is [array([2.1997702], dtype=float32), -0.46589097]. 
=============================================
[2019-04-09 15:04:32,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-09 15:04:32,144] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 24.47459049180471, 0.2458803224147932, 0.0, 1.0, 55.0, 42.68303565915228], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 24.47942633061515, 0.2297565792371947, 0.0, 1.0, 45.0, 42.0791293635598], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5399521942179293, 0.5765855264123982, 0.0, 1.0, 0.6, 0.42079129363559803], 
reward next is 0.5792, 
noisyNet noise sample is [array([-0.25088698], dtype=float32), 1.1857936]. 
=============================================
[2019-04-09 15:04:32,188] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-09 15:04:32,188] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:04:32,189] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:32,190] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run15
[2019-04-09 15:04:32,202] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:04:32,207] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:04:32,208] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:32,210] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:04:32,220] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run15
[2019-04-09 15:04:32,240] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run15
[2019-04-09 15:05:02,972] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02176486], dtype=float32), 0.026653428]
[2019-04-09 15:05:02,972] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-0.799137567, 96.894982105, 0.0, 0.0, 19.0, 26.78321690325514, 0.8051264067904563, 0.0, 1.0, 45.0, 35.75798020548365]
[2019-04-09 15:05:02,972] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:05:02,973] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00406531 0.09056319 0.12576406 0.02944008 0.01968748 0.00704584
 0.24427313 0.03997829 0.06495197 0.06840383 0.30582678], sampled 0.19358409379482544
[2019-04-09 15:05:34,494] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02176486], dtype=float32), 0.026653428]
[2019-04-09 15:05:34,494] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-4.15, 80.0, 0.0, 0.0, 19.0, 26.71017105915546, 0.8391890817922406, 0.0, 1.0, 20.0, 46.74864870646478]
[2019-04-09 15:05:34,494] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:05:34,494] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.0080685  0.11066137 0.13132332 0.05130665 0.0261682  0.01193723
 0.21427254 0.04513742 0.07166357 0.06923924 0.26022193], sampled 0.6262396361271547
[2019-04-09 15:05:42,357] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.02176486], dtype=float32), 0.026653428]
[2019-04-09 15:05:42,358] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 19.0, 26.46080601020637, 0.747973095683612, 0.0, 1.0, 55.0, 41.67270877342171]
[2019-04-09 15:05:42,358] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:05:42,358] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.00775448 0.12138066 0.13012089 0.05326552 0.03043631 0.01440162
 0.18711124 0.04569064 0.08707001 0.06379122 0.2589774 ], sampled 0.4466309962015902
[2019-04-09 15:05:57,709] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5658.8118 283732.7688 2911.4380
[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,837] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:03,983] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5308.0108 318810.3479 2138.4337
[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,003] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,115] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,099] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5404.9009 309114.2149 2543.7346
[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:07,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:08,131] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 140000, evaluation results [140000.0, 5404.90090564377, 309114.21491157124, 2543.734590578461, 5658.811809524941, 283732.7687924504, 2911.4379678433984, 5308.010796871009, 318810.3479476713, 2138.4336653087216]
[2019-04-09 15:06:08,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00102596 0.06829448 0.13972001 0.02879222 0.01205188 0.00333079
 0.16229303 0.04381526 0.09383398 0.0847625  0.36207998], sum to 1.0000
[2019-04-09 15:06:08,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2633
[2019-04-09 15:06:08,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 61.0, 134.5, 543.5, 22.5, 25.78166467061138, 0.5417855653719834, 1.0, 1.0, 55.0, 52.23161548218045], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 133200.0000, 
sim time next is 133800.0000, 
raw observation next is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 22.5, 25.84921195258037, 0.5503521785271204, 1.0, 1.0, 45.0, 36.81241770739362], 
processed observation next is [1.0, 0.5652173913043478, 0.2516158818097876, 0.61, 0.4533333333333333, 0.5786372007366483, 0.375, 0.654100996048364, 0.6834507261757068, 1.0, 1.0, 0.6, 0.3681241770739362], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.9904467], dtype=float32), 0.43808684]. 
=============================================
[2019-04-09 15:06:08,477] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00124163 0.10473303 0.18334307 0.03094404 0.01363042 0.00311332
 0.20624001 0.03729384 0.08518951 0.07094663 0.26332453], sum to 1.0000
[2019-04-09 15:06:08,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1971
[2019-04-09 15:06:08,534] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 22.5, 25.74689000443761, 0.5428002848083532, 1.0, 1.0, 60.0, 55.98631371181475], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 133800.0000, 
sim time next is 134400.0000, 
raw observation next is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 22.5, 25.85565011573153, 0.5674519071472184, 1.0, 1.0, 25.0, 41.75953244845695], 
processed observation next is [1.0, 0.5652173913043478, 0.2566943674976916, 0.61, 0.4583333333333333, 0.5567219152854513, 0.375, 0.6546375096442943, 0.6891506357157394, 1.0, 1.0, 0.2, 0.41759532448456954], 
reward next is 0.5824, 
noisyNet noise sample is [array([-1.1751138], dtype=float32), 0.98897976]. 
=============================================
[2019-04-09 15:06:09,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00133647 0.07736912 0.13466254 0.03185545 0.01705313 0.00305083
 0.2365695  0.02396752 0.07350148 0.06015765 0.3404764 ], sum to 1.0000
[2019-04-09 15:06:09,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-09 15:06:09,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 22.5, 25.63935435070438, 0.4536293232166922, 1.0, 1.0, 65.0, 62.78101937744322], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 150600.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 22.5, 25.52654868426757, 0.4910424353297215, 1.0, 1.0, 65.0, 71.12419848461317], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.375, 0.6272123903556309, 0.6636808117765739, 1.0, 1.0, 1.0, 0.7112419848461318], 
reward next is 0.2888, 
noisyNet noise sample is [array([-1.1883364], dtype=float32), 1.0822254]. 
=============================================
[2019-04-09 15:06:09,324] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00592886 0.0977626  0.14925283 0.04042324 0.01835787 0.01163423
 0.17890148 0.07152894 0.07315254 0.0711019  0.2819555 ], sum to 1.0000
[2019-04-09 15:06:09,325] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8559
[2019-04-09 15:06:09,369] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.92616832570726, 0.04753543527846901, 0.0, 1.0, 25.0, 52.33363037297322], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 193800.0000, 
sim time next is 194400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.90491975902031, 0.04977947426774441, 0.0, 1.0, 65.0, 58.07545603824926], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.49207664658502576, 0.5165931580892481, 0.0, 1.0, 1.0, 0.5807545603824926], 
reward next is 0.4192, 
noisyNet noise sample is [array([-0.5906486], dtype=float32), -0.24686453]. 
=============================================
[2019-04-09 15:06:09,452] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00152035 0.08118305 0.17625113 0.04120872 0.00969357 0.0022635
 0.2535435  0.03095156 0.06198512 0.02989852 0.3115009 ], sum to 1.0000
[2019-04-09 15:06:09,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-09 15:06:09,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 61.0, 148.0, 106.0, 22.5, 26.56852577789732, 0.6460203270077686, 1.0, 1.0, 45.0, 34.19851445364024], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [-6.700000000000001, 61.0, 133.5, 95.83333333333334, 22.5, 26.55269379286039, 0.6403958098856694, 1.0, 1.0, 45.0, 32.90727559407876], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.445, 0.10589318600368325, 0.375, 0.7127244827383657, 0.7134652699618899, 1.0, 1.0, 0.6, 0.3290727559407876], 
reward next is 0.6709, 
noisyNet noise sample is [array([-1.2534907], dtype=float32), -0.92576593]. 
=============================================
[2019-04-09 15:06:09,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00154368 0.11564689 0.14239837 0.03841412 0.01804589 0.00484152
 0.16639061 0.0340974  0.08877353 0.0637078  0.32614022], sum to 1.0000
[2019-04-09 15:06:09,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5056
[2019-04-09 15:06:09,615] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.9, 64.66666666666667, 0.0, 0.0, 22.5, 25.30213741042428, 0.4201294934819226, 1.0, 1.0, 45.0, 39.9183816492214], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [-8.0, 65.33333333333334, 0.0, 0.0, 22.5, 25.28289920907429, 0.3838341854058018, 1.0, 1.0, 50.0, 42.07670700509715], 
processed observation next is [1.0, 0.8260869565217391, 0.24099722991689754, 0.6533333333333334, 0.0, 0.0, 0.375, 0.6069082674228575, 0.6279447284686006, 1.0, 1.0, 0.7, 0.42076707005097147], 
reward next is 0.5792, 
noisyNet noise sample is [array([0.9034742], dtype=float32), 0.88245124]. 
=============================================
[2019-04-09 15:06:09,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[6.270299 ]
 [6.458914 ]
 [6.5190444]
 [6.53748  ]
 [6.5968156]], R is [[7.02104759]
 [7.55165339]
 [8.00592613]
 [8.39036179]
 [8.75661087]].
[2019-04-09 15:06:10,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00576941 0.11268035 0.1411212  0.04078406 0.01543762 0.00892918
 0.19809297 0.05896389 0.06371924 0.07248426 0.2820179 ], sum to 1.0000
[2019-04-09 15:06:10,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3282
[2019-04-09 15:06:10,358] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 23.67280459088783, 0.01317241595883661, 0.0, 1.0, 25.0, 53.52073257049805], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 195600.0000, 
sim time next is 196200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.6937120149114, 0.001849640018324772, 0.0, 1.0, 45.0, 38.94626112404973], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.4744760012426168, 0.5006165466727749, 0.0, 1.0, 0.6, 0.38946261124049725], 
reward next is 0.6105, 
noisyNet noise sample is [array([1.0152423], dtype=float32), 1.066788]. 
=============================================
[2019-04-09 15:06:10,364] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0010426  0.17938513 0.10173707 0.02981413 0.01043909 0.00194739
 0.17241849 0.03146659 0.08709488 0.05868686 0.3259678 ], sum to 1.0000
[2019-04-09 15:06:10,367] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-09 15:06:10,389] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00494531 0.106107   0.13754465 0.04495712 0.01484601 0.00887812
 0.20329396 0.05108634 0.07348799 0.06310669 0.2917469 ], sum to 1.0000
[2019-04-09 15:06:10,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5615
[2019-04-09 15:06:10,406] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.0, 67.5, 27.0, 3.0, 22.5, 26.57386533572604, 0.6549388158717031, 1.0, 1.0, 65.0, 55.74819703824274], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 145800.0000, 
sim time next is 146400.0000, 
raw observation next is [-7.1, 68.66666666666666, 22.5, 2.5, 22.5, 26.70976700515213, 0.674367279496875, 1.0, 1.0, 55.0, 48.08058212300288], 
processed observation next is [1.0, 0.6956521739130435, 0.2659279778393352, 0.6866666666666665, 0.075, 0.0027624309392265192, 0.375, 0.7258139170960108, 0.724789093165625, 1.0, 1.0, 0.8, 0.4808058212300288], 
reward next is 0.5192, 
noisyNet noise sample is [array([0.5153688], dtype=float32), 0.3387626]. 
=============================================
[2019-04-09 15:06:10,440] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.6937120149114, 0.001849640018324772, 0.0, 1.0, 45.0, 38.94626112404973], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.65725162373117, -0.01295991259948814, 0.0, 1.0, 50.0, 39.25145376387214], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.47143763531093086, 0.49568002913350395, 0.0, 1.0, 0.7, 0.3925145376387214], 
reward next is 0.6075, 
noisyNet noise sample is [array([1.0152423], dtype=float32), 1.066788]. 
=============================================
[2019-04-09 15:06:10,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00362815 0.06366006 0.15360533 0.02497295 0.01624434 0.00564226
 0.22011445 0.04665962 0.07637068 0.06423985 0.3248624 ], sum to 1.0000
[2019-04-09 15:06:10,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4455
[2019-04-09 15:06:10,722] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 24.67902180883637, 0.2542832068122489, 0.0, 1.0, 45.0, 42.42620283015157], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 171000.0000, 
sim time next is 171600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 24.61553999258227, 0.2329091667591684, 0.0, 1.0, 45.0, 40.2701327792621], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5512949993818559, 0.5776363889197228, 0.0, 1.0, 0.6, 0.40270132779262097], 
reward next is 0.5973, 
noisyNet noise sample is [array([-1.2293036], dtype=float32), -0.08069556]. 
=============================================
[2019-04-09 15:06:10,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00613473 0.08817846 0.15849659 0.03756581 0.01908455 0.01350371
 0.18643114 0.0505878  0.05838288 0.06757113 0.31406328], sum to 1.0000
[2019-04-09 15:06:10,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7022
[2019-04-09 15:06:10,819] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 24.94389830945845, 0.2689110080919329, 0.0, 1.0, 45.0, 36.78441401335751], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 24.87708213502244, 0.2603602968385154, 0.0, 1.0, 65.0, 64.42499570719566], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5730901779185368, 0.5867867656128385, 0.0, 1.0, 1.0, 0.6442499570719566], 
reward next is 0.3558, 
noisyNet noise sample is [array([0.3427229], dtype=float32), 0.3017374]. 
=============================================
[2019-04-09 15:06:10,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00110817 0.10494723 0.14285183 0.02381847 0.00637701 0.00262271
 0.2756518  0.03133279 0.08770926 0.06747724 0.25610343], sum to 1.0000
[2019-04-09 15:06:10,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2746
[2019-04-09 15:06:10,955] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 22.5, 25.87368504763721, 0.5152022761203375, 1.0, 1.0, 25.0, 45.98151930835186], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 150600.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 22.5, 25.86356872539115, 0.51223973729326, 1.0, 1.0, 35.0, 44.06593054763169], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.375, 0.6552973937825959, 0.6707465790977533, 1.0, 1.0, 0.4, 0.4406593054763169], 
reward next is 0.5593, 
noisyNet noise sample is [array([-0.60648066], dtype=float32), 0.59261084]. 
=============================================
[2019-04-09 15:06:11,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00135459 0.1043909  0.14226216 0.03774207 0.012233   0.00217259
 0.18676332 0.04176886 0.09389532 0.04728664 0.3301305 ], sum to 1.0000
[2019-04-09 15:06:11,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8726
[2019-04-09 15:06:11,391] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 65.33333333333334, 0.0, 0.0, 22.5, 25.43011191049804, 0.4594232307608355, 1.0, 1.0, 45.0, 53.43915815899136], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 22.5, 25.40830003512821, 0.4531342616999312, 1.0, 1.0, 65.0, 59.53903434033517], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.375, 0.6173583362606841, 0.6510447538999771, 1.0, 1.0, 1.0, 0.5953903434033517], 
reward next is 0.4046, 
noisyNet noise sample is [array([-0.88795], dtype=float32), -1.2237984]. 
=============================================
[2019-04-09 15:06:11,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00506249 0.10138789 0.14185767 0.05497948 0.01922706 0.0103256
 0.21662007 0.04465125 0.08968074 0.05505037 0.26115733], sum to 1.0000
[2019-04-09 15:06:11,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1851
[2019-04-09 15:06:11,427] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 19.0, 24.03197065941525, 0.1113989921035493, 0.0, 1.0, 65.0, 63.69391397948909], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 181200.0000, 
sim time next is 181800.0000, 
raw observation next is [-8.9, 76.0, 0.0, 0.0, 19.0, 24.05223719797688, 0.1236257449418148, 0.0, 1.0, 25.0, 50.51627088952878], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.76, 0.0, 0.0, 0.08333333333333333, 0.5043530998314066, 0.5412085816472716, 0.0, 1.0, 0.2, 0.5051627088952878], 
reward next is 0.4948, 
noisyNet noise sample is [array([1.0503811], dtype=float32), 0.2929965]. 
=============================================
[2019-04-09 15:06:11,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00199564 0.1128624  0.11591016 0.04395899 0.01692702 0.00476463
 0.16935407 0.04318042 0.08298283 0.07297038 0.33509344], sum to 1.0000
[2019-04-09 15:06:11,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6642
[2019-04-09 15:06:11,831] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00188056 0.11736542 0.13890459 0.0204729  0.01368441 0.00703255
 0.20937863 0.03991752 0.06380626 0.06719837 0.32035884], sum to 1.0000
[2019-04-09 15:06:11,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1142
[2019-04-09 15:06:11,867] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 24.94052115657462, 0.2107112364146519, 1.0, 1.0, 60.0, 51.47744074110792], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 210000.0000, 
sim time next is 210600.0000, 
raw observation next is [-6.75, 73.5, 124.0, 0.0, 22.5, 24.93704258581264, 0.2279442556899489, 1.0, 1.0, 60.0, 56.65586959660813], 
processed observation next is [1.0, 0.43478260869565216, 0.275623268698061, 0.735, 0.41333333333333333, 0.0, 0.375, 0.5780868821510534, 0.5759814185633163, 1.0, 1.0, 0.9, 0.5665586959660813], 
reward next is 0.4334, 
noisyNet noise sample is [array([-0.720912], dtype=float32), -0.22926176]. 
=============================================
[2019-04-09 15:06:11,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00819028 0.11722118 0.12129425 0.04311638 0.02163799 0.01174528
 0.18462712 0.05125598 0.08447336 0.06836407 0.28807414], sum to 1.0000
[2019-04-09 15:06:11,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6803
[2019-04-09 15:06:11,872] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.4, 68.5, 0.0, 0.0, 19.0, 24.10538115991364, 0.2071719111328905, 0.0, 1.0, 65.0, 75.9090494452516], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 19.0, 24.15595646885199, 0.2196693762725078, 0.0, 1.0, 20.0, 53.09980948529943], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5129963724043325, 0.5732231254241692, 0.0, 1.0, 0.1, 0.5309980948529943], 
reward next is 0.4690, 
noisyNet noise sample is [array([-1.6420124], dtype=float32), 0.12850216]. 
=============================================
[2019-04-09 15:06:11,917] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.76220007935562, 0.03927167342076292, 0.0, 1.0, 45.0, 39.83819313757003], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 186000.0000, 
sim time next is 186600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.6672098195667, 0.03290328503544573, 0.0, 1.0, 60.0, 50.87622491130829], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.4722674849638917, 0.5109677616784819, 0.0, 1.0, 0.9, 0.5087622491130829], 
reward next is 0.4912, 
noisyNet noise sample is [array([-2.2795048], dtype=float32), 0.65800595]. 
=============================================
[2019-04-09 15:06:12,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00152017 0.12784685 0.11258581 0.03092773 0.01662692 0.0040378
 0.2057156  0.03601189 0.10667009 0.06254258 0.29551458], sum to 1.0000
[2019-04-09 15:06:12,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1531
[2019-04-09 15:06:12,232] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 22.5, 25.24578696705169, 0.2939430523438802, 1.0, 1.0, 45.0, 37.94782646458785], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 213600.0000, 
sim time next is 214200.0000, 
raw observation next is [-5.6, 68.5, 153.0, 0.0, 22.5, 25.26552385554387, 0.2971264009528438, 1.0, 1.0, 25.0, 38.48216915927824], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.685, 0.51, 0.0, 0.375, 0.6054603212953223, 0.5990421336509479, 1.0, 1.0, 0.2, 0.38482169159278246], 
reward next is 0.6152, 
noisyNet noise sample is [array([1.4614047], dtype=float32), 0.9347019]. 
=============================================
[2019-04-09 15:06:12,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00648473 0.09224107 0.12407634 0.04257445 0.01793391 0.00822784
 0.24255656 0.04922703 0.0732339  0.07023697 0.2732071 ], sum to 1.0000
[2019-04-09 15:06:12,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0275
[2019-04-09 15:06:12,502] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 24.5226249413212, 0.1921428410761591, 0.0, 1.0, 65.0, 56.33026735530909], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 24.5004043100734, 0.1836242656214609, 0.0, 1.0, 20.0, 51.59405937289124], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5417003591727833, 0.561208088540487, 0.0, 1.0, 0.1, 0.5159405937289123], 
reward next is 0.4841, 
noisyNet noise sample is [array([-1.7621925], dtype=float32), 0.7111761]. 
=============================================
[2019-04-09 15:06:12,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00550214 0.11266767 0.18245469 0.03820852 0.02011534 0.0075831
 0.18988855 0.041807   0.06490868 0.08133241 0.25553188], sum to 1.0000
[2019-04-09 15:06:12,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3935
[2019-04-09 15:06:12,823] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.38333333333333, 67.5, 0.0, 0.0, 22.5, 24.11524679427985, 0.05601218624301935, 0.0, 1.0, 25.0, 46.16858230904904], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 285000.0000, 
sim time next is 285600.0000, 
raw observation next is [-12.46666666666667, 68.0, 0.0, 0.0, 22.5, 23.96459959391328, 0.03026710472279612, 1.0, 1.0, 45.0, 40.52100156185437], 
processed observation next is [1.0, 0.30434782608695654, 0.11726685133887339, 0.68, 0.0, 0.0, 0.375, 0.4970499661594401, 0.5100890349075987, 1.0, 1.0, 0.6, 0.4052100156185437], 
reward next is 0.5948, 
noisyNet noise sample is [array([-0.9457817], dtype=float32), -0.40149713]. 
=============================================
[2019-04-09 15:06:12,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00351421 0.08923277 0.14803082 0.02234054 0.01465117 0.00444141
 0.21527421 0.03420754 0.06317874 0.08595186 0.3191767 ], sum to 1.0000
[2019-04-09 15:06:12,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2976
[2019-04-09 15:06:13,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 73.0, 0.0, 0.0, 19.0, 24.57412068226788, 0.2188928958127206, 0.0, 1.0, 30.0, 41.67409390279412], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 175200.0000, 
sim time next is 175800.0000, 
raw observation next is [-8.816666666666666, 73.5, 0.0, 0.0, 19.0, 24.54192971963492, 0.2034089872080131, 0.0, 1.0, 45.0, 40.91725363585821], 
processed observation next is [1.0, 0.0, 0.21837488457987075, 0.735, 0.0, 0.0, 0.08333333333333333, 0.5451608099695765, 0.5678029957360043, 0.0, 1.0, 0.6, 0.4091725363585821], 
reward next is 0.5908, 
noisyNet noise sample is [array([0.29372624], dtype=float32), 0.5919761]. 
=============================================
[2019-04-09 15:06:13,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00528148 0.077418   0.1171219  0.03099989 0.0168159  0.0084316
 0.25293517 0.05194489 0.07948161 0.06408411 0.29548544], sum to 1.0000
[2019-04-09 15:06:13,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7728
[2019-04-09 15:06:13,115] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 22.5, 23.6657267860876, 0.001212545432412451, 0.0, 1.0, 65.0, 57.71585390958771], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 22.5, 23.67050612198888, 0.01579461393323449, 1.0, 1.0, 65.0, 64.60717187881016], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.375, 0.47254217683240657, 0.5052648713110782, 1.0, 1.0, 1.0, 0.6460717187881017], 
reward next is 0.3539, 
noisyNet noise sample is [array([0.26423785], dtype=float32), -1.5056757]. 
=============================================
[2019-04-09 15:06:13,143] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0045217  0.10139235 0.13090746 0.03490325 0.0153486  0.00686605
 0.21427023 0.047743   0.09866045 0.05667835 0.28870854], sum to 1.0000
[2019-04-09 15:06:13,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4441
[2019-04-09 15:06:13,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00430915 0.08677138 0.14056595 0.03956996 0.01848789 0.00752336
 0.20593137 0.04754152 0.08464332 0.07386253 0.29079366], sum to 1.0000
[2019-04-09 15:06:13,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4660
[2019-04-09 15:06:13,225] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 22.5, 24.29380568629217, 0.2089265631523879, 1.0, 1.0, 65.0, 63.02442157590384], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 289200.0000, 
sim time next is 289800.0000, 
raw observation next is [-12.55, 68.5, 30.0, 386.0, 22.5, 24.6304975133158, 0.2367519131771572, 1.0, 1.0, 45.0, 53.09763637602345], 
processed observation next is [1.0, 0.34782608695652173, 0.11495844875346259, 0.685, 0.1, 0.4265193370165746, 0.375, 0.5525414594429833, 0.5789173043923858, 1.0, 1.0, 0.6, 0.5309763637602345], 
reward next is 0.4690, 
noisyNet noise sample is [array([-0.50153166], dtype=float32), -0.8055429]. 
=============================================
[2019-04-09 15:06:13,232] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 24.14354928087306, 0.1107283299146003, 0.0, 1.0, 35.0, 41.88733652322303], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 24.11623693394284, 0.09570650078379744, 0.0, 1.0, 45.0, 41.44882118977027], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5096864111619034, 0.5319021669279325, 0.0, 1.0, 0.6, 0.4144882118977027], 
reward next is 0.5855, 
noisyNet noise sample is [array([-0.10208727], dtype=float32), -0.25257513]. 
=============================================
[2019-04-09 15:06:13,762] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00377281 0.13768478 0.19322316 0.0466102  0.01360455 0.00679801
 0.17655256 0.04739291 0.07440183 0.08125626 0.21870297], sum to 1.0000
[2019-04-09 15:06:13,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7550
[2019-04-09 15:06:13,803] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.483333333333333, 75.5, 94.0, 0.0, 22.5, 25.07348456353237, 0.2572511466267529, 1.0, 1.0, 45.0, 49.22004073886361], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 208200.0000, 
sim time next is 208800.0000, 
raw observation next is [-7.3, 75.0, 101.5, 0.0, 22.5, 25.14871853112274, 0.2726274178996008, 1.0, 1.0, 60.0, 46.74393628749719], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.75, 0.3383333333333333, 0.0, 0.375, 0.5957265442602283, 0.5908758059665337, 1.0, 1.0, 0.9, 0.4674393628749719], 
reward next is 0.5326, 
noisyNet noise sample is [array([-2.0580816], dtype=float32), 0.350372]. 
=============================================
[2019-04-09 15:06:13,877] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00124679 0.15292865 0.13683172 0.02726294 0.01618469 0.00247417
 0.20354    0.04223926 0.09643162 0.0579993  0.26286086], sum to 1.0000
[2019-04-09 15:06:13,879] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5221
[2019-04-09 15:06:13,934] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.87832110313995, 0.3246221483580045, 1.0, 1.0, 65.0, 65.04860986083686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 239400.0000, 
sim time next is 240000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.08111248493588, 0.2405597220701461, 1.0, 1.0, 45.0, 51.41396649292362], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.5067593737446566, 0.580186574023382, 1.0, 1.0, 0.6, 0.5141396649292362], 
reward next is 0.4859, 
noisyNet noise sample is [array([0.43354642], dtype=float32), -1.0402876]. 
=============================================
[2019-04-09 15:06:13,948] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[6.7289248]
 [6.61367  ]
 [6.70249  ]
 [6.520235 ]
 [6.6329784]], R is [[7.02002335]
 [7.29933739]
 [7.71043396]
 [8.00673962]
 [8.52865887]].
[2019-04-09 15:06:14,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00118966 0.10063311 0.1465116  0.02705491 0.01523088 0.00369266
 0.19287682 0.03889475 0.09029066 0.04280748 0.34081742], sum to 1.0000
[2019-04-09 15:06:14,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6779
[2019-04-09 15:06:14,197] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.15, 61.5, 87.0, 512.0, 22.5, 25.65366616884287, 0.4255465068436281, 1.0, 1.0, 20.0, 44.98386722196704], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 297000.0000, 
sim time next is 297600.0000, 
raw observation next is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 22.5, 25.76209848523349, 0.4427136926145664, 1.0, 1.0, 65.0, 51.2677231278885], 
processed observation next is [1.0, 0.43478260869565216, 0.15881809787626952, 0.61, 0.3005555555555555, 0.592633517495396, 0.375, 0.6468415404361242, 0.6475712308715221, 1.0, 1.0, 1.0, 0.512677231278885], 
reward next is 0.4873, 
noisyNet noise sample is [array([-2.0473459], dtype=float32), 0.39249206]. 
=============================================
[2019-04-09 15:06:14,718] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00549519 0.13517699 0.13618518 0.0369116  0.01766931 0.00641947
 0.19841634 0.04929093 0.10183334 0.06717572 0.24542592], sum to 1.0000
[2019-04-09 15:06:14,718] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9963
[2019-04-09 15:06:14,779] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 22.5, 24.05480263327809, 0.09413994949299603, 1.0, 1.0, 65.0, 57.11518695843581], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 201600.0000, 
sim time next is 202200.0000, 
raw observation next is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 22.5, 24.11156888384959, 0.11227156071298, 1.0, 1.0, 65.0, 63.31614453052423], 
processed observation next is [1.0, 0.34782608695652173, 0.21837488457987075, 0.78, 0.07555555555555557, 0.22467771639042353, 0.375, 0.5092974069874657, 0.5374238535709933, 1.0, 1.0, 1.0, 0.6331614453052423], 
reward next is 0.3668, 
noisyNet noise sample is [array([-1.3128612], dtype=float32), 0.995167]. 
=============================================
[2019-04-09 15:06:15,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00215818 0.08366824 0.14573017 0.03855379 0.01497387 0.00390786
 0.22739927 0.05652441 0.06282546 0.05805175 0.306207  ], sum to 1.0000
[2019-04-09 15:06:15,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8574
[2019-04-09 15:06:15,930] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.899999999999999, 79.66666666666667, 0.0, 0.0, 19.0, 24.33795036187531, 0.1523373753269164, 0.0, 1.0, 60.0, 56.7219956516474], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 254400.0000, 
sim time next is 255000.0000, 
raw observation next is [-3.9, 80.83333333333334, 0.0, 0.0, 19.0, 24.26577932475666, 0.1914556136805936, 0.0, 1.0, 65.0, 69.50169405020675], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.8083333333333335, 0.0, 0.0, 0.08333333333333333, 0.5221482770630551, 0.5638185378935312, 0.0, 1.0, 1.0, 0.6950169405020675], 
reward next is 0.3050, 
noisyNet noise sample is [array([-0.46186197], dtype=float32), -1.4386044]. 
=============================================
[2019-04-09 15:06:15,996] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[5.728121 ]
 [5.7692976]
 [5.7750134]
 [5.994631 ]
 [6.029568 ]], R is [[5.9149909 ]
 [6.28862143]
 [6.89275599]
 [7.47452641]
 [8.03275204]].
[2019-04-09 15:06:16,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00190818 0.07632502 0.17053622 0.03538841 0.01245846 0.00232449
 0.15947257 0.03714579 0.08055445 0.06179629 0.3620902 ], sum to 1.0000
[2019-04-09 15:06:16,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4615
[2019-04-09 15:06:16,193] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.72818875654667, 0.2362096019625866, 0.0, 1.0, 25.0, 35.12199390536033], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 244800.0000, 
sim time next is 245400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 24.62183078347823, 0.2107309574906178, 0.0, 1.0, 50.0, 41.16002770673387], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5518192319565193, 0.5702436524968726, 0.0, 1.0, 0.7, 0.4116002770673387], 
reward next is 0.5884, 
noisyNet noise sample is [array([1.0151559], dtype=float32), -0.18416749]. 
=============================================
[2019-04-09 15:06:17,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000949   0.09832814 0.22179209 0.03174698 0.01650932 0.00613708
 0.15891816 0.03058905 0.07730146 0.06010828 0.2976205 ], sum to 1.0000
[2019-04-09 15:06:17,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4020
[2019-04-09 15:06:17,383] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 49.0, 12.0, 123.0, 22.5, 25.9181948519806, 0.5636686289973624, 1.0, 1.0, 25.0, 48.03886831579005], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 320400.0000, 
sim time next is 321000.0000, 
raw observation next is [-10.78333333333333, 50.33333333333334, 0.0, 0.0, 22.5, 25.52067039051851, 0.4991919847765452, 1.0, 1.0, 45.0, 39.76661867115948], 
processed observation next is [1.0, 0.7391304347826086, 0.1638965835641737, 0.5033333333333334, 0.0, 0.0, 0.375, 0.6267225325432092, 0.6663973282588483, 1.0, 1.0, 0.6, 0.39766618671159476], 
reward next is 0.6023, 
noisyNet noise sample is [array([1.1442673], dtype=float32), 1.2647218]. 
=============================================
[2019-04-09 15:06:17,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[6.416155 ]
 [6.600344 ]
 [6.447304 ]
 [6.433786 ]
 [6.4410367]], R is [[6.92684364]
 [7.37718678]
 [7.55371284]
 [8.09506607]
 [8.54424191]].
[2019-04-09 15:06:17,891] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0015118  0.14489926 0.1851474  0.03794295 0.01120858 0.00304553
 0.181426   0.02999941 0.0788138  0.07794005 0.24806528], sum to 1.0000
[2019-04-09 15:06:17,891] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9173
[2019-04-09 15:06:18,006] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 22.5, 26.38058495323359, 0.5433207315967854, 1.0, 1.0, 65.0, 50.65529071716428], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 22.5, 26.40869022098843, 0.4609520166978546, 1.0, 1.0, 65.0, 70.50302435272924], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.375, 0.7007241850823691, 0.6536506722326182, 1.0, 1.0, 1.0, 0.7050302435272924], 
reward next is 0.2950, 
noisyNet noise sample is [array([1.9046472], dtype=float32), -1.144745]. 
=============================================
[2019-04-09 15:06:19,914] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00084535 0.11975088 0.19478177 0.03395956 0.01489772 0.00310162
 0.20706967 0.03126019 0.07985707 0.0706144  0.24386185], sum to 1.0000
[2019-04-09 15:06:19,914] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2743
[2019-04-09 15:06:20,002] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 22.5, 25.99254153571694, 0.4701117031887718, 1.0, 1.0, 25.0, 46.57681623089898], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 219000.0000, 
sim time next is 219600.0000, 
raw observation next is [-4.5, 65.0, 139.0, 0.0, 22.5, 25.99914358679023, 0.3678602756240283, 1.0, 1.0, 25.0, 49.80336634095754], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.65, 0.4633333333333333, 0.0, 0.375, 0.6665952988991858, 0.6226200918746762, 1.0, 1.0, 0.2, 0.4980336634095754], 
reward next is 0.5020, 
noisyNet noise sample is [array([0.7471836], dtype=float32), 0.05554327]. 
=============================================
[2019-04-09 15:06:20,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00336778 0.07697603 0.08597635 0.02490427 0.01399969 0.00530229
 0.3047366  0.05327866 0.0656446  0.05739886 0.30841485], sum to 1.0000
[2019-04-09 15:06:20,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5475
[2019-04-09 15:06:20,781] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-13.65, 76.0, 0.0, 0.0, 19.0, 23.76439816124046, 0.0375486403201838, 0.0, 1.0, 65.0, 65.24338433910478], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [-13.73333333333333, 74.0, 0.0, 0.0, 19.0, 23.6920968521663, 0.02769888882564838, 0.0, 1.0, 50.0, 52.0461915622288], 
processed observation next is [1.0, 0.9565217391304348, 0.08217913204062795, 0.74, 0.0, 0.0, 0.08333333333333333, 0.4743414043471918, 0.5092329629418828, 0.0, 1.0, 0.7, 0.5204619156222879], 
reward next is 0.4795, 
noisyNet noise sample is [array([1.0817839], dtype=float32), -0.86680615]. 
=============================================
[2019-04-09 15:06:21,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00840112 0.09896883 0.13884997 0.03558984 0.02133281 0.01020347
 0.22347426 0.06994286 0.07086453 0.08057486 0.24179742], sum to 1.0000
[2019-04-09 15:06:21,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0172
[2019-04-09 15:06:21,205] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.96666666666667, 68.0, 0.0, 0.0, 19.0, 23.29924070870094, -0.09808421272082944, 0.0, 1.0, 50.0, 38.08098998080338], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 278400.0000, 
sim time next is 279000.0000, 
raw observation next is [-11.15, 68.5, 0.0, 0.0, 19.0, 23.20889340697588, -0.118799714133876, 0.0, 1.0, 20.0, 36.21120838429084], 
processed observation next is [1.0, 0.21739130434782608, 0.15373961218836565, 0.685, 0.0, 0.0, 0.08333333333333333, 0.4340744505813232, 0.460400095288708, 0.0, 1.0, 0.1, 0.36211208384290844], 
reward next is 0.6379, 
noisyNet noise sample is [array([-1.9227592], dtype=float32), 0.10649849]. 
=============================================
[2019-04-09 15:06:21,299] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.147946 ]
 [4.079199 ]
 [4.1872315]
 [4.1561065]
 [4.38528  ]], R is [[4.67771387]
 [5.25012684]
 [5.79917288]
 [6.34802437]
 [6.73944807]].
[2019-04-09 15:06:21,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00212733 0.06826066 0.1450383  0.03565587 0.0115079  0.00457574
 0.23185058 0.04555624 0.06449414 0.0761387  0.31479457], sum to 1.0000
[2019-04-09 15:06:21,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1408
[2019-04-09 15:06:21,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.2, 80.5, 0.0, 0.0, 19.0, 24.6991527593829, 0.2413120094682197, 0.0, 1.0, 30.0, 39.07085133920405], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 257400.0000, 
sim time next is 258000.0000, 
raw observation next is [-4.3, 80.0, 0.0, 0.0, 19.0, 24.65917536052671, 0.2235670884227301, 0.0, 1.0, 25.0, 39.21141864864625], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.8, 0.0, 0.0, 0.08333333333333333, 0.5549312800438925, 0.5745223628075767, 0.0, 1.0, 0.2, 0.3921141864864625], 
reward next is 0.6079, 
noisyNet noise sample is [array([-1.167215], dtype=float32), -0.8592846]. 
=============================================
[2019-04-09 15:06:21,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[5.2807612]
 [5.251256 ]
 [5.0907917]
 [5.174661 ]
 [5.2441263]], R is [[5.77148247]
 [6.32305908]
 [6.73204803]
 [7.07623625]
 [7.59950304]].
[2019-04-09 15:06:22,120] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0016012  0.07357221 0.16239256 0.01897224 0.00935777 0.00222213
 0.26290283 0.02970927 0.04433781 0.04262049 0.35231152], sum to 1.0000
[2019-04-09 15:06:22,182] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-09 15:06:22,198] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 19.0, 24.05229170612824, 0.1615139832261532, 0.0, 1.0, 65.0, 67.86537982148457], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 250200.0000, 
sim time next is 250800.0000, 
raw observation next is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 19.0, 24.10235077836746, 0.174176282905162, 0.0, 1.0, 25.0, 52.56491582965469], 
processed observation next is [1.0, 0.9130434782608695, 0.35918744228993543, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.5085292315306216, 0.5580587609683874, 0.0, 1.0, 0.2, 0.5256491582965469], 
reward next is 0.4744, 
noisyNet noise sample is [array([0.6082671], dtype=float32), 0.5668886]. 
=============================================
[2019-04-09 15:06:23,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00447915 0.11577393 0.10642068 0.02730676 0.01695573 0.00600176
 0.29364341 0.04065121 0.04844461 0.04781078 0.29251194], sum to 1.0000
[2019-04-09 15:06:23,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2233
[2019-04-09 15:06:23,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 19.0, 24.49527654259198, 0.17988750602791, 0.0, 1.0, 65.0, 81.98604630831089], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 19.0, 24.35511908736673, 0.1762986831610317, 0.0, 1.0, 25.0, 54.69607392283251], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 0.08333333333333333, 0.529593257280561, 0.5587662277203439, 0.0, 1.0, 0.2, 0.5469607392283251], 
reward next is 0.4530, 
noisyNet noise sample is [array([-0.22527178], dtype=float32), 0.49174017]. 
=============================================
[2019-04-09 15:06:23,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00575507 0.13760461 0.13205768 0.04198012 0.02029066 0.00866465
 0.20410123 0.05977298 0.06107563 0.06836041 0.260337  ], sum to 1.0000
[2019-04-09 15:06:23,878] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-09 15:06:23,906] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.683333333333334, 69.5, 0.0, 0.0, 19.0, 24.35494260590836, 0.1879761058881865, 0.0, 1.0, 65.0, 65.83692139090316], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 274200.0000, 
sim time next is 274800.0000, 
raw observation next is [-9.866666666666667, 69.0, 0.0, 0.0, 19.0, 24.40360184643068, 0.1860585122030548, 0.0, 1.0, 45.0, 52.53966470139736], 
processed observation next is [1.0, 0.17391304347826086, 0.18928901200369344, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5336334872025565, 0.5620195040676849, 0.0, 1.0, 0.6, 0.5253966470139736], 
reward next is 0.4746, 
noisyNet noise sample is [array([0.24069212], dtype=float32), -1.753723]. 
=============================================
[2019-04-09 15:06:24,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00474595 0.08503894 0.12996326 0.02812624 0.01781166 0.00646616
 0.2698585  0.04834681 0.06720506 0.07422682 0.26821062], sum to 1.0000
[2019-04-09 15:06:24,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3688
[2019-04-09 15:06:24,173] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.566666666666666, 70.33333333333334, 0.0, 0.0, 19.0, 24.1766143333646, 0.0731978876453473, 0.0, 1.0, 45.0, 34.51934907240452], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 267000.0000, 
sim time next is 267600.0000, 
raw observation next is [-7.833333333333334, 69.66666666666667, 0.0, 0.0, 19.0, 24.01517152665187, 0.04368258363544141, 0.0, 1.0, 45.0, 32.57264694096235], 
processed observation next is [1.0, 0.08695652173913043, 0.2456140350877193, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.5012642938876558, 0.5145608612118138, 0.0, 1.0, 0.6, 0.3257264694096235], 
reward next is 0.6743, 
noisyNet noise sample is [array([-1.9773122], dtype=float32), -0.75107276]. 
=============================================
[2019-04-09 15:06:24,235] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00177602 0.13908118 0.18200523 0.03107513 0.01011426 0.00279406
 0.18505542 0.04122391 0.10303225 0.05316053 0.25068203], sum to 1.0000
[2019-04-09 15:06:24,235] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7229
[2019-04-09 15:06:24,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-9.5, 44.0, 89.0, 694.5, 22.5, 25.31372349947139, 0.3888969578722593, 1.0, 1.0, 30.0, 51.85106716978927], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 306000.0000, 
sim time next is 306600.0000, 
raw observation next is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 22.5, 25.35672986462408, 0.3963436985599311, 1.0, 1.0, 60.0, 50.59460467856098], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44000000000000006, 0.30333333333333334, 0.7440147329650091, 0.375, 0.6130608220520065, 0.6321145661866437, 1.0, 1.0, 0.9, 0.5059460467856097], 
reward next is 0.4941, 
noisyNet noise sample is [array([-0.19620165], dtype=float32), -0.5687644]. 
=============================================
[2019-04-09 15:06:24,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00536382 0.11900729 0.12678649 0.04267329 0.02296609 0.01017449
 0.1498891  0.06303395 0.0900813  0.07326083 0.2967634 ], sum to 1.0000
[2019-04-09 15:06:24,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9589
[2019-04-09 15:06:24,417] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.28400188577171, -0.3369712165579153, 0.0, 1.0, 65.0, 66.23369400927419], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 360600.0000, 
sim time next is 361200.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.18258599433575, -0.3258939385702986, 0.0, 1.0, 65.0, 82.42398127538249], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.34854883286131244, 0.3913686871432338, 0.0, 1.0, 1.0, 0.8242398127538249], 
reward next is 0.1758, 
noisyNet noise sample is [array([-1.2972767], dtype=float32), 0.29114714]. 
=============================================
[2019-04-09 15:06:24,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00217996 0.07924189 0.16438122 0.01804274 0.00862107 0.00321104
 0.25593132 0.05511456 0.06788573 0.06784066 0.27754977], sum to 1.0000
[2019-04-09 15:06:24,654] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1837
[2019-04-09 15:06:24,725] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 25.13219749785968, 0.3515504452002241, 0.0, 1.0, 45.0, 40.58190840908092], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 255600.0000, 
sim time next is 256200.0000, 
raw observation next is [-4.0, 81.50000000000001, 0.0, 0.0, 19.0, 25.10635135014511, 0.3320768684827184, 0.0, 1.0, 25.0, 40.14685628321597], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.8150000000000002, 0.0, 0.0, 0.08333333333333333, 0.5921959458454259, 0.6106922894942395, 0.0, 1.0, 0.2, 0.4014685628321597], 
reward next is 0.5985, 
noisyNet noise sample is [array([-0.52616674], dtype=float32), 0.57489634]. 
=============================================
[2019-04-09 15:06:25,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00474084 0.10790201 0.14605014 0.03963426 0.01784167 0.00974865
 0.21869878 0.03813836 0.0681656  0.07284806 0.27623168], sum to 1.0000
[2019-04-09 15:06:25,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7708
[2019-04-09 15:06:25,209] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.633333333333333, 67.66666666666667, 0.0, 0.0, 19.0, 24.11694324414897, 0.07756301800599025, 0.0, 1.0, 45.0, 37.07372028860934], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 269400.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 19.0, 24.01629568760928, 0.06380697086791905, 0.0, 1.0, 65.0, 64.30312922519018], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.08333333333333333, 0.50135797396744, 0.5212689902893063, 0.0, 1.0, 1.0, 0.6430312922519018], 
reward next is 0.3570, 
noisyNet noise sample is [array([-0.83503306], dtype=float32), 0.6135755]. 
=============================================
[2019-04-09 15:06:25,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[4.3927155]
 [4.446947 ]
 [4.6715035]
 [4.6600704]
 [4.707972 ]], R is [[4.70462227]
 [5.28683901]
 [5.81233168]
 [6.34650326]
 [6.87423086]].
[2019-04-09 15:06:25,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00592462 0.09079281 0.1328066  0.03680323 0.02238198 0.01039487
 0.24973136 0.04301338 0.06864587 0.05783543 0.2816698 ], sum to 1.0000
[2019-04-09 15:06:25,405] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00193413 0.17598598 0.14256518 0.04826248 0.01557607 0.00616809
 0.17589907 0.03274685 0.0965983  0.0501347  0.2541291 ], sum to 1.0000
[2019-04-09 15:06:25,409] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3601
[2019-04-09 15:06:25,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-09 15:06:25,432] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.7, 63.0, 91.0, 447.5, 22.5, 25.02598491496374, 0.2837418693990884, 1.0, 1.0, 20.0, 40.57300529905822], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 295200.0000, 
sim time next is 295800.0000, 
raw observation next is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 22.5, 25.09589235826427, 0.2915633794163408, 1.0, 1.0, 20.0, 39.08909458546204], 
processed observation next is [1.0, 0.43478260869565216, 0.14358264081255764, 0.625, 0.2988888888888889, 0.518232044198895, 0.375, 0.5913243631886891, 0.5971877931387802, 1.0, 1.0, 0.1, 0.3908909458546204], 
reward next is 0.6091, 
noisyNet noise sample is [array([3.357991], dtype=float32), 1.3977014]. 
=============================================
[2019-04-09 15:06:25,444] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 21.63808766377327, -0.4478793509269929, 0.0, 1.0, 60.0, 61.24217288014759], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 370800.0000, 
sim time next is 371400.0000, 
raw observation next is [-16.28333333333333, 78.5, 0.0, 0.0, 22.5, 21.73003051445702, -0.4537530895526493, 0.0, 1.0, 45.0, 49.32700014440741], 
processed observation next is [1.0, 0.30434782608695654, 0.011542012927054512, 0.785, 0.0, 0.0, 0.375, 0.31083587620475167, 0.3487489701491169, 0.0, 1.0, 0.6, 0.49327000144407407], 
reward next is 0.5067, 
noisyNet noise sample is [array([0.22041555], dtype=float32), -0.1976429]. 
=============================================
[2019-04-09 15:06:25,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00451524 0.09433232 0.14711405 0.04233144 0.01259743 0.00628039
 0.21810392 0.0342082  0.07294501 0.07756252 0.29000947], sum to 1.0000
[2019-04-09 15:06:25,456] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8284
[2019-04-09 15:06:25,493] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 24.79551460064333, 0.2605875266588951, 0.0, 1.0, 30.0, 70.9687543392553], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 24.73150866169178, 0.2681409008841609, 0.0, 1.0, 65.0, 60.50003509474514], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5609590551409817, 0.5893803002947203, 0.0, 1.0, 1.0, 0.6050003509474514], 
reward next is 0.3950, 
noisyNet noise sample is [array([0.7900103], dtype=float32), 0.17800605]. 
=============================================
[2019-04-09 15:06:25,605] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00418989 0.11349942 0.13819747 0.05466434 0.0139902  0.00809745
 0.13861577 0.05446117 0.09743067 0.07247423 0.3043794 ], sum to 1.0000
[2019-04-09 15:06:25,605] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2414
[2019-04-09 15:06:25,756] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 22.5, 24.40801671441507, 0.07066288536482812, 1.0, 1.0, 45.0, 50.09922186656449], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 377400.0000, 
sim time next is 378000.0000, 
raw observation next is [-15.6, 90.0, 32.0, 607.5, 22.5, 24.57806540040795, 0.09170571031861156, 1.0, 1.0, 25.0, 47.19379534179773], 
processed observation next is [1.0, 0.391304347826087, 0.030470914127423816, 0.9, 0.10666666666666667, 0.6712707182320442, 0.375, 0.5481721167006626, 0.5305685701062038, 1.0, 1.0, 0.2, 0.4719379534179773], 
reward next is 0.5281, 
noisyNet noise sample is [array([0.9627095], dtype=float32), -0.61010635]. 
=============================================
[2019-04-09 15:06:25,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[5.0299726]
 [4.777743 ]
 [4.9048805]
 [4.7974763]
 [4.7937737]], R is [[5.59610939]
 [6.03915596]
 [6.3380599 ]
 [6.37795401]
 [6.48343229]].
[2019-04-09 15:06:26,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00098111 0.12763937 0.21799032 0.0390062  0.01787178 0.00542589
 0.20059389 0.02660583 0.08395747 0.05441143 0.22551678], sum to 1.0000
[2019-04-09 15:06:26,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0493
[2019-04-09 15:06:26,537] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.5, 44.0, 93.0, 652.1666666666666, 22.5, 25.28878034439354, 0.2818543511186213, 1.0, 1.0, 65.0, 63.16757949939593], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 307200.0000, 
sim time next is 307800.0000, 
raw observation next is [-9.5, 44.0, 95.0, 631.0, 22.5, 25.04693045838041, 0.3736345515077283, 1.0, 1.0, 20.0, 47.50899826239634], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31666666666666665, 0.6972375690607735, 0.375, 0.587244204865034, 0.6245448505025761, 1.0, 1.0, 0.1, 0.4750899826239634], 
reward next is 0.5249, 
noisyNet noise sample is [array([0.11093479], dtype=float32), -0.19074409]. 
=============================================
[2019-04-09 15:06:26,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00156206 0.10360353 0.17262064 0.02051868 0.01319377 0.00226092
 0.17109276 0.04986318 0.04890026 0.07144254 0.3449417 ], sum to 1.0000
[2019-04-09 15:06:27,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6258
[2019-04-09 15:06:27,076] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-12.8, 72.33333333333334, 0.0, 0.0, 19.0, 24.46864953476602, 0.235172225595815, 0.0, 1.0, 45.0, 52.95027359608231], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 332400.0000, 
sim time next is 333000.0000, 
raw observation next is [-12.8, 73.5, 0.0, 0.0, 19.0, 24.47037015430081, 0.2217631908267825, 0.0, 1.0, 60.0, 51.94034126206324], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.735, 0.0, 0.0, 0.08333333333333333, 0.5391975128584008, 0.5739210636089275, 0.0, 1.0, 0.9, 0.5194034126206324], 
reward next is 0.4806, 
noisyNet noise sample is [array([-2.6306317], dtype=float32), 1.0090075]. 
=============================================
[2019-04-09 15:06:27,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[6.2273326]
 [6.404928 ]
 [6.4676046]
 [6.4398184]
 [6.429491 ]], R is [[6.6806097 ]
 [7.08430099]
 [7.34695673]
 [7.67949343]
 [8.14412785]].
[2019-04-09 15:06:27,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00084778 0.12314252 0.15235098 0.02013678 0.00883589 0.00467522
 0.14644006 0.02759342 0.10367639 0.03591239 0.3763886 ], sum to 1.0000
[2019-04-09 15:06:27,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-09 15:06:27,652] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.0, 89.0, 694.5, 22.5, 25.08635429368662, 0.3519272642776243, 1.0, 1.0, 30.0, 52.5660681844596], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 306000.0000, 
sim time next is 306600.0000, 
raw observation next is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 22.5, 25.15082334444256, 0.3602896976894995, 1.0, 1.0, 45.0, 37.64722311743117], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44000000000000006, 0.30333333333333334, 0.7440147329650091, 0.375, 0.5959019453702133, 0.6200965658964998, 1.0, 1.0, 0.6, 0.37647223117431167], 
reward next is 0.6235, 
noisyNet noise sample is [array([-0.4754025], dtype=float32), -1.1027472]. 
=============================================
[2019-04-09 15:06:27,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00076764 0.13974953 0.15400298 0.02127326 0.00972097 0.00384919
 0.14446414 0.02717414 0.11261522 0.04005826 0.34632462], sum to 1.0000
[2019-04-09 15:06:27,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-09 15:06:27,742] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 22.5, 25.15082334444256, 0.3602896976894995, 1.0, 1.0, 45.0, 37.64722311743117], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 306600.0000, 
sim time next is 307200.0000, 
raw observation next is [-9.5, 44.0, 93.0, 652.1666666666666, 22.5, 25.21254841726552, 0.2672020033005206, 1.0, 1.0, 45.0, 41.52554285330135], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31, 0.7206261510128913, 0.375, 0.6010457014387933, 0.5890673344335068, 1.0, 1.0, 0.6, 0.4152554285330135], 
reward next is 0.5847, 
noisyNet noise sample is [array([-0.4754025], dtype=float32), -1.1027472]. 
=============================================
[2019-04-09 15:06:28,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00116378 0.13290475 0.1421925  0.03130737 0.01025257 0.00260239
 0.24137726 0.02348263 0.0579506  0.05773669 0.29902944], sum to 1.0000
[2019-04-09 15:06:28,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5051
[2019-04-09 15:06:28,126] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 47.0, 358.5, 22.5, 25.34277430452282, 0.4240365139032152, 1.0, 1.0, 25.0, 40.11527054459089], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 316800.0000, 
sim time next is 317400.0000, 
raw observation next is [-9.683333333333334, 43.16666666666666, 39.33333333333333, 319.0, 22.5, 24.92071864488025, 0.3711122055541494, 1.0, 1.0, 45.0, 37.77750305740419], 
processed observation next is [1.0, 0.6956521739130435, 0.19436749769159742, 0.4316666666666666, 0.1311111111111111, 0.3524861878453039, 0.375, 0.5767265537400208, 0.6237040685180498, 1.0, 1.0, 0.6, 0.3777750305740419], 
reward next is 0.6222, 
noisyNet noise sample is [array([0.03757036], dtype=float32), -0.461685]. 
=============================================
[2019-04-09 15:06:28,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0040087  0.09312015 0.14058296 0.03077995 0.01668145 0.0066465
 0.17976838 0.04153646 0.07661851 0.06891049 0.34134638], sum to 1.0000
[2019-04-09 15:06:28,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1823
[2019-04-09 15:06:28,546] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-14.3, 68.0, 0.0, 0.0, 19.0, 23.03605920433082, -0.1107520705549784, 0.0, 1.0, 65.0, 61.58128583274367], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 348000.0000, 
sim time next is 348600.0000, 
raw observation next is [-14.4, 68.5, 0.0, 0.0, 19.0, 23.09600849122353, -0.1085388268371981, 0.0, 1.0, 25.0, 55.74269672567974], 
processed observation next is [1.0, 0.0, 0.0637119113573407, 0.685, 0.0, 0.0, 0.08333333333333333, 0.4246673742686274, 0.4638203910542673, 0.0, 1.0, 0.2, 0.5574269672567974], 
reward next is 0.4426, 
noisyNet noise sample is [array([-1.1133145], dtype=float32), 0.73534846]. 
=============================================
[2019-04-09 15:06:28,605] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00166987 0.10156658 0.15776685 0.03175111 0.01384758 0.0028255
 0.20209369 0.02303406 0.08078896 0.05999141 0.32466438], sum to 1.0000
[2019-04-09 15:06:28,606] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8788
[2019-04-09 15:06:28,655] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 22.5, 25.88737422790891, 0.4709878029970975, 1.0, 1.0, 60.0, 49.59206292207973], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 300600.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 22.5, 25.87847258537825, 0.1610110798909735, 1.0, 1.0, 65.0, 65.940927481131], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 0.375, 0.6565393821148543, 0.5536703599636578, 1.0, 1.0, 1.0, 0.65940927481131], 
reward next is 0.3406, 
noisyNet noise sample is [array([-1.0392056], dtype=float32), 0.49637198]. 
=============================================
[2019-04-09 15:06:28,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00109318 0.0924774  0.2180544  0.03560038 0.00979428 0.00308856
 0.12220405 0.035179   0.10457469 0.05211405 0.32582003], sum to 1.0000
[2019-04-09 15:06:28,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3542
[2019-04-09 15:06:28,716] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 44.0, 89.0, 694.5, 22.5, 24.74833671424512, 0.2219369602618691, 1.0, 1.0, 45.0, 33.86627554371172], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 306000.0000, 
sim time next is 306600.0000, 
raw observation next is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 22.5, 24.68912096604592, 0.1125678783356796, 1.0, 1.0, 35.0, 34.98557041135535], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44000000000000006, 0.30333333333333334, 0.7440147329650091, 0.375, 0.5574267471704933, 0.5375226261118932, 1.0, 1.0, 0.4, 0.3498557041135535], 
reward next is 0.6501, 
noisyNet noise sample is [array([0.5655656], dtype=float32), -0.049103364]. 
=============================================
[2019-04-09 15:06:29,137] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00756886 0.08836128 0.14653769 0.03853532 0.02310349 0.01058625
 0.26833966 0.05680686 0.06475701 0.07332318 0.22208034], sum to 1.0000
[2019-04-09 15:06:29,138] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6520
[2019-04-09 15:06:29,198] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 21.8881901302899, -0.4152323521078849, 0.0, 1.0, 25.0, 53.52695399455131], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 369600.0000, 
sim time next is 370200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 21.8492932567765, -0.4164227049832559, 0.0, 1.0, 55.0, 49.88777132334001], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.32077443806470846, 0.36119243167224807, 0.0, 1.0, 0.8, 0.49887771323340013], 
reward next is 0.5011, 
noisyNet noise sample is [array([0.84320635], dtype=float32), -0.42888808]. 
=============================================
[2019-04-09 15:06:30,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00800285 0.0704442  0.12153804 0.04148475 0.02451289 0.01454466
 0.19638026 0.0690595  0.08208449 0.08941874 0.2825296 ], sum to 1.0000
[2019-04-09 15:06:30,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0719
[2019-04-09 15:06:30,749] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 22.21817369425695, -0.343082291944136, 0.0, 1.0, 55.0, 51.7750735667877], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 367800.0000, 
sim time next is 368400.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 22.19758390847606, -0.3459784354090985, 0.0, 1.0, 20.0, 42.46734652670028], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.34979865903967156, 0.38467385486363387, 0.0, 1.0, 0.1, 0.42467346526700284], 
reward next is 0.5753, 
noisyNet noise sample is [array([0.22857727], dtype=float32), -0.7428474]. 
=============================================
[2019-04-09 15:06:31,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00688118 0.13522445 0.1169353  0.03933708 0.01919645 0.01335513
 0.18371947 0.03654758 0.0753214  0.05992273 0.31355914], sum to 1.0000
[2019-04-09 15:06:31,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8472
[2019-04-09 15:06:31,316] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.3, 71.0, 0.0, 0.0, 19.0, 23.17079569085553, -0.1232756304579163, 0.0, 1.0, 20.0, 40.84289082025432], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 358200.0000, 
sim time next is 358800.0000, 
raw observation next is [-15.4, 71.66666666666667, 0.0, 0.0, 19.0, 23.16504967102762, -0.1316657578455042, 0.0, 1.0, 65.0, 68.16168554859205], 
processed observation next is [1.0, 0.13043478260869565, 0.03601108033240995, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.4304208059189684, 0.4561114140514986, 0.0, 1.0, 1.0, 0.6816168554859204], 
reward next is 0.3184, 
noisyNet noise sample is [array([-2.3081896], dtype=float32), -0.18149517]. 
=============================================
[2019-04-09 15:06:31,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00117824 0.11374114 0.18484965 0.035405   0.01521133 0.00396894
 0.15288977 0.03396884 0.08264232 0.07276516 0.30337963], sum to 1.0000
[2019-04-09 15:06:31,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3380
[2019-04-09 15:06:31,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00581288 0.07356011 0.12708314 0.03199908 0.02178256 0.00717843
 0.21672153 0.05781597 0.06213587 0.07918259 0.31672785], sum to 1.0000
[2019-04-09 15:06:31,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7873
[2019-04-09 15:06:31,624] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 22.5, 24.82140702172997, 0.2788001211883027, 1.0, 1.0, 65.0, 59.7011160446683], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 330600.0000, 
sim time next is 331200.0000, 
raw observation next is [-12.8, 70.0, 0.0, 0.0, 22.5, 24.76769476114464, 0.265405806176886, 0.0, 1.0, 35.0, 53.77444034712273], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7, 0.0, 0.0, 0.375, 0.56397456342872, 0.588468602058962, 0.0, 1.0, 0.4, 0.5377444034712273], 
reward next is 0.4623, 
noisyNet noise sample is [array([-1.6890236], dtype=float32), -0.573254]. 
=============================================
[2019-04-09 15:06:31,632] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 22.90860042320361, -0.1795687761379232, 0.0, 1.0, 55.0, 55.48084144543765], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 354000.0000, 
sim time next is 354600.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 22.93068524171566, -0.1743962001873374, 0.0, 1.0, 65.0, 63.23089073762289], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.41089043680963844, 0.4418679332708875, 0.0, 1.0, 1.0, 0.6323089073762289], 
reward next is 0.3677, 
noisyNet noise sample is [array([1.1339533], dtype=float32), 0.1277768]. 
=============================================
[2019-04-09 15:06:31,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00767175 0.07687561 0.14746557 0.04036184 0.01735742 0.01013855
 0.28871423 0.03370784 0.05707625 0.07831123 0.24231969], sum to 1.0000
[2019-04-09 15:06:31,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9320
[2019-04-09 15:06:31,837] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 23.34536610998325, -0.1297245927889945, 0.0, 1.0, 65.0, 68.6312078815058], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 23.13143052591609, -0.1246948363786304, 0.0, 1.0, 65.0, 69.3140022078411], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.4276192104930076, 0.4584350545404565, 0.0, 1.0, 1.0, 0.693140022078411], 
reward next is 0.3069, 
noisyNet noise sample is [array([1.2580407], dtype=float32), -0.04558686]. 
=============================================
[2019-04-09 15:06:31,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[4.6869607]
 [4.970941 ]
 [4.826685 ]
 [5.0752797]
 [4.8111196]], R is [[5.00596142]
 [5.26958942]
 [5.77293396]
 [6.28770351]
 [6.7951932 ]].
[2019-04-09 15:06:31,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00164488 0.10369957 0.15820199 0.04388991 0.01331354 0.00478317
 0.1896604  0.03062915 0.08662187 0.05606418 0.31149125], sum to 1.0000
[2019-04-09 15:06:31,944] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8932
[2019-04-09 15:06:31,973] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 22.5, 24.40926203911232, 0.1588649450906256, 1.0, 1.0, 45.0, 49.15400019758022], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 387000.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 22.5, 24.6622052605344, 0.04574403233956383, 1.0, 1.0, 65.0, 60.69707586275799], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.375, 0.5551837717111999, 0.5152480107798546, 1.0, 1.0, 1.0, 0.6069707586275799], 
reward next is 0.3930, 
noisyNet noise sample is [array([0.11680986], dtype=float32), -1.0306463]. 
=============================================
[2019-04-09 15:06:32,225] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00459791 0.11590739 0.1366021  0.04135301 0.01551505 0.00914411
 0.17610511 0.05878344 0.07066401 0.07954222 0.29178563], sum to 1.0000
[2019-04-09 15:06:32,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9323
[2019-04-09 15:06:32,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.46425251160144, -0.3146966140487719, 0.0, 1.0, 20.0, 50.40425820835167], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 361200.0000, 
sim time next is 361800.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.36974192921665, -0.3084642326014919, 0.0, 1.0, 65.0, 69.2175972541334], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.3641451607680543, 0.39717858913283605, 0.0, 1.0, 1.0, 0.6921759725413339], 
reward next is 0.3078, 
noisyNet noise sample is [array([1.6109515], dtype=float32), 1.8302251]. 
=============================================
[2019-04-09 15:06:32,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00599243 0.10863397 0.10734677 0.04880635 0.02186459 0.00792322
 0.21525156 0.06367559 0.05088833 0.06795498 0.30166224], sum to 1.0000
[2019-04-09 15:06:32,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7895
[2019-04-09 15:06:32,320] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 22.27058256519612, -0.380109675326405, 0.0, 1.0, 60.0, 55.21839847742348], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 369600.0000, 
sim time next is 370200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 22.09945836024678, -0.35017986537469, 0.0, 1.0, 65.0, 72.68763876899719], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.3416215300205649, 0.3832733782084367, 0.0, 1.0, 1.0, 0.7268763876899719], 
reward next is 0.2731, 
noisyNet noise sample is [array([-0.6786545], dtype=float32), -2.00087]. 
=============================================
[2019-04-09 15:06:32,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00771278 0.09218538 0.14021842 0.03091504 0.02163935 0.0085693
 0.22746126 0.05716728 0.06787543 0.0721378  0.2741179 ], sum to 1.0000
[2019-04-09 15:06:32,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2856
[2019-04-09 15:06:32,353] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.13668175600242, -0.3259362749670109, 0.0, 1.0, 55.0, 58.18756067775612], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 361800.0000, 
sim time next is 362400.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 22.16846585195528, -0.3310742913821964, 0.0, 1.0, 35.0, 42.00453008838685], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.3473721543296066, 0.3896419028726012, 0.0, 1.0, 0.4, 0.4200453008838685], 
reward next is 0.5800, 
noisyNet noise sample is [array([0.940523], dtype=float32), -0.73190266]. 
=============================================
[2019-04-09 15:06:32,869] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00743683 0.06756262 0.13685144 0.04298887 0.01789355 0.00994546
 0.18779543 0.05011775 0.07156987 0.08228501 0.3255532 ], sum to 1.0000
[2019-04-09 15:06:32,869] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3177
[2019-04-09 15:06:32,905] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 19.0, 22.35128957345644, -0.2972924985421296, 0.0, 1.0, 30.0, 54.97145959590313], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 19.0, 22.35402516900913, -0.3006593163420811, 0.0, 1.0, 45.0, 41.92817019277425], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.08333333333333333, 0.36283543075076086, 0.399780227885973, 0.0, 1.0, 0.6, 0.41928170192774256], 
reward next is 0.5807, 
noisyNet noise sample is [array([0.92873406], dtype=float32), 0.64004374]. 
=============================================
[2019-04-09 15:06:33,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00238536 0.07334247 0.17932503 0.01937094 0.01255492 0.0027158
 0.25475174 0.03059307 0.0527625  0.05878693 0.31341124], sum to 1.0000
[2019-04-09 15:06:33,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-09 15:06:33,698] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.94393927740722, 0.462516532872794, 0.0, 1.0, 25.0, 37.53878284693002], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 428400.0000, 
sim time next is 429000.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.87696712142736, 0.4377149833240532, 0.0, 1.0, 25.0, 38.17100581774437], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.6564139267856133, 0.6459049944413511, 0.0, 1.0, 0.2, 0.38171005817744375], 
reward next is 0.6183, 
noisyNet noise sample is [array([0.77408725], dtype=float32), 1.643922]. 
=============================================
[2019-04-09 15:06:33,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[6.354168 ]
 [6.1488943]
 [6.5064425]
 [6.3025217]
 [6.6242695]], R is [[6.55302906]
 [7.11211109]
 [7.51375008]
 [7.78633404]
 [8.32154846]].
[2019-04-09 15:06:34,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00590697 0.11310469 0.15513487 0.05395592 0.02090481 0.00493128
 0.19505611 0.05165025 0.0674304  0.06172886 0.27019584], sum to 1.0000
[2019-04-09 15:06:34,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2482
[2019-04-09 15:06:34,462] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 22.5, 23.06043458562717, -0.08191135768464271, 1.0, 1.0, 60.0, 79.18762222417298], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 376800.0000, 
sim time next is 377400.0000, 
raw observation next is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 22.5, 23.87488985990251, -0.01852915275442963, 1.0, 1.0, 25.0, 57.66799119011817], 
processed observation next is [1.0, 0.34782608695652173, 0.02539242843951994, 0.885, 0.09888888888888887, 0.6232044198895028, 0.375, 0.4895741549918758, 0.49382361574852346, 1.0, 1.0, 0.2, 0.5766799119011817], 
reward next is 0.4233, 
noisyNet noise sample is [array([0.18866387], dtype=float32), -0.062489733]. 
=============================================
[2019-04-09 15:06:34,670] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00188415 0.09993373 0.1599232  0.03786889 0.01400006 0.00227345
 0.18330875 0.02651323 0.05856809 0.04694126 0.36878514], sum to 1.0000
[2019-04-09 15:06:34,670] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7717
[2019-04-09 15:06:34,697] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.2, 38.0, 0.0, 0.0, 22.5, 25.86614761696253, 0.3823853661324105, 1.0, 1.0, 65.0, 62.62350668715462], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [-9.3, 38.66666666666666, 0.0, 0.0, 22.5, 25.15619341124166, 0.3590007837876608, 1.0, 1.0, 65.0, 55.35288175540025], 
processed observation next is [1.0, 0.7391304347826086, 0.20498614958448752, 0.38666666666666655, 0.0, 0.0, 0.375, 0.5963494509368049, 0.6196669279292203, 1.0, 1.0, 1.0, 0.5535288175540025], 
reward next is 0.4465, 
noisyNet noise sample is [array([-0.28273085], dtype=float32), 0.14818588]. 
=============================================
[2019-04-09 15:06:34,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00098599 0.1317025  0.19577967 0.0489122  0.0081773  0.00411183
 0.17184424 0.02671922 0.0790957  0.0406335  0.29203779], sum to 1.0000
[2019-04-09 15:06:34,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-09 15:06:34,794] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 22.5, 25.14868091787831, 0.150572928642317, 1.0, 1.0, 65.0, 60.75324964492584], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 388800.0000, 
sim time next is 389400.0000, 
raw observation next is [-12.61666666666667, 51.0, 58.0, 858.0, 22.5, 25.17916157990334, 0.3160192613541483, 1.0, 1.0, 65.0, 54.73270496153241], 
processed observation next is [1.0, 0.5217391304347826, 0.11311172668513378, 0.51, 0.19333333333333333, 0.9480662983425414, 0.375, 0.598263464991945, 0.6053397537847162, 1.0, 1.0, 1.0, 0.5473270496153241], 
reward next is 0.4527, 
noisyNet noise sample is [array([1.4193652], dtype=float32), -1.2324169]. 
=============================================
[2019-04-09 15:06:35,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00281173 0.09850881 0.14362653 0.03005125 0.01315745 0.00361103
 0.21667734 0.04675844 0.10322534 0.0585935  0.28297862], sum to 1.0000
[2019-04-09 15:06:35,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1934
[2019-04-09 15:06:35,678] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-10.6, 48.0, 0.0, 0.0, 19.0, 22.9873071638784, -0.134312457898526, 0.0, 1.0, 65.0, 59.74700114992913], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 423000.0000, 
sim time next is 423600.0000, 
raw observation next is [-10.6, 48.33333333333333, 0.0, 0.0, 19.0, 22.89159334989176, -0.1415328933902271, 0.0, 1.0, 25.0, 46.8241210328603], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4833333333333333, 0.0, 0.0, 0.08333333333333333, 0.4076327791576467, 0.4528223688699243, 0.0, 1.0, 0.2, 0.468241210328603], 
reward next is 0.5318, 
noisyNet noise sample is [array([0.7601181], dtype=float32), 1.4027896]. 
=============================================
[2019-04-09 15:06:36,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00109719 0.10657458 0.14837696 0.02319792 0.00812781 0.00370257
 0.17031646 0.0326116  0.06898086 0.04648807 0.39052594], sum to 1.0000
[2019-04-09 15:06:36,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1563
[2019-04-09 15:06:36,691] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.4, 39.33333333333334, 0.0, 0.0, 22.5, 24.59250311324135, 0.2339335004000342, 1.0, 1.0, 60.0, 52.85889040269941], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 409800.0000, 
sim time next is 410400.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 22.5, 24.8542628064063, 0.2651131417292561, 1.0, 1.0, 25.0, 45.2853179235663], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.375, 0.571188567200525, 0.5883710472430853, 1.0, 1.0, 0.2, 0.45285317923566304], 
reward next is 0.5471, 
noisyNet noise sample is [array([-1.170421], dtype=float32), -0.5345756]. 
=============================================
[2019-04-09 15:06:37,889] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0011539  0.10322461 0.217394   0.02904052 0.00774185 0.00386495
 0.16650173 0.03841398 0.07072292 0.05262856 0.309313  ], sum to 1.0000
[2019-04-09 15:06:37,890] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7660
[2019-04-09 15:06:37,939] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.0, 43.0, 48.0, 832.0, 22.5, 27.27886912195932, 0.74379772137228, 1.0, 1.0, 45.0, 48.69194829272916], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 397800.0000, 
sim time next is 398400.0000, 
raw observation next is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 22.5, 27.35183065932882, 0.7535749760678915, 1.0, 1.0, 65.0, 58.16070509061519], 
processed observation next is [1.0, 0.6086956521739131, 0.1902123730378578, 0.42, 0.15388888888888885, 0.8963167587476979, 0.375, 0.7793192216107349, 0.7511916586892972, 1.0, 1.0, 1.0, 0.5816070509061518], 
reward next is 0.4184, 
noisyNet noise sample is [array([0.48962462], dtype=float32), -0.047935754]. 
=============================================
[2019-04-09 15:06:37,966] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00092974 0.08022028 0.15301806 0.03793141 0.00838532 0.00308373
 0.18837538 0.03381334 0.05719968 0.04598424 0.39105892], sum to 1.0000
[2019-04-09 15:06:37,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1226
[2019-04-09 15:06:38,037] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-11.7, 51.0, 56.5, 896.0, 22.5, 26.83738424270021, 0.6553091319666823, 1.0, 1.0, 65.0, 53.94334408704653], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 392400.0000, 
sim time next is 393000.0000, 
raw observation next is [-11.5, 50.16666666666666, 56.0, 893.0, 22.5, 26.92968123539775, 0.6702338901540471, 1.0, 1.0, 25.0, 45.30777234332546], 
processed observation next is [1.0, 0.5652173913043478, 0.1440443213296399, 0.5016666666666666, 0.18666666666666668, 0.9867403314917127, 0.375, 0.7441401029498126, 0.7234112967180156, 1.0, 1.0, 0.2, 0.4530777234332546], 
reward next is 0.5469, 
noisyNet noise sample is [array([0.05755471], dtype=float32), -0.28075385]. 
=============================================
[2019-04-09 15:06:38,041] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[7.077008 ]
 [6.9802217]
 [7.1181016]
 [7.022217 ]
 [6.950993 ]], R is [[7.58000135]
 [7.96476793]
 [8.45158958]
 [8.73126125]
 [8.95600605]].
[2019-04-09 15:06:38,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00257297 0.06678745 0.13970467 0.02155605 0.01246441 0.00572614
 0.21470805 0.03977164 0.05485255 0.0619492  0.37990686], sum to 1.0000
[2019-04-09 15:06:38,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0391
[2019-04-09 15:06:38,940] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 22.90126670954211, -0.1680507901808244, 0.0, 1.0, 25.0, 45.75823778702738], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 434400.0000, 
sim time next is 435000.0000, 
raw observation next is [-11.28333333333333, 54.83333333333334, 0.0, 0.0, 19.0, 22.85062852951283, -0.1735518666278693, 0.0, 1.0, 20.0, 30.81729189967588], 
processed observation next is [1.0, 0.0, 0.1500461680517083, 0.5483333333333335, 0.0, 0.0, 0.08333333333333333, 0.40421904412606907, 0.44214937779071023, 0.0, 1.0, 0.1, 0.3081729189967588], 
reward next is 0.6918, 
noisyNet noise sample is [array([0.45545337], dtype=float32), 0.44106132]. 
=============================================
[2019-04-09 15:06:38,953] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[4.9025593]
 [5.0567403]
 [5.219954 ]
 [5.3398747]
 [5.26187  ]], R is [[5.81051826]
 [6.2948308 ]
 [6.64717102]
 [7.27709246]
 [7.88648415]].
[2019-04-09 15:06:39,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00114189 0.08160154 0.1556156  0.03188643 0.01191183 0.00301323
 0.2708098  0.04948368 0.06238973 0.0587699  0.27337644], sum to 1.0000
[2019-04-09 15:06:39,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6160
[2019-04-09 15:06:39,651] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.9166666666666667, 34.5, 44.0, 0.0, 22.5, 26.12518335530348, 0.4992914292853325, 1.0, 1.0, 25.0, 42.68006221665504], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 489000.0000, 
sim time next is 489600.0000, 
raw observation next is [1.1, 34.0, 38.0, 0.0, 22.5, 26.49175210516862, 0.5224677166090147, 1.0, 1.0, 55.0, 31.27172013042544], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.34, 0.12666666666666668, 0.0, 0.375, 0.7076460087640516, 0.6741559055363382, 1.0, 1.0, 0.8, 0.3127172013042544], 
reward next is 0.6873, 
noisyNet noise sample is [array([-0.85027385], dtype=float32), 2.230926]. 
=============================================
[2019-04-09 15:06:39,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00074876 0.10628395 0.13820945 0.02553191 0.01282428 0.00178611
 0.24793641 0.03316942 0.10688776 0.05016121 0.27646074], sum to 1.0000
[2019-04-09 15:06:39,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7447
[2019-04-09 15:06:40,040] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.2, 38.0, 0.0, 0.0, 22.5, 27.45970012370435, 0.750013784386081, 1.0, 1.0, 65.0, 56.21257406086293], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [-9.3, 38.66666666666666, 0.0, 0.0, 22.5, 27.26948356494308, 0.7475454797357951, 1.0, 1.0, 65.0, 54.72089946348294], 
processed observation next is [1.0, 0.7391304347826086, 0.20498614958448752, 0.38666666666666655, 0.0, 0.0, 0.375, 0.7724569637452566, 0.7491818265785984, 1.0, 1.0, 1.0, 0.5472089946348294], 
reward next is 0.4528, 
noisyNet noise sample is [array([0.946744], dtype=float32), -0.7064898]. 
=============================================
[2019-04-09 15:06:40,064] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00509718 0.10736582 0.14912923 0.04820429 0.0149045  0.00989304
 0.21036628 0.03355181 0.06217805 0.05882638 0.30048347], sum to 1.0000
[2019-04-09 15:06:40,064] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9231
[2019-04-09 15:06:40,098] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.7, 52.0, 0.0, 0.0, 19.0, 24.49456558695876, 0.1362783457997016, 0.0, 1.0, 65.0, 66.30116847627257], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 449400.0000, 
sim time next is 450000.0000, 
raw observation next is [-10.6, 52.0, 0.0, 0.0, 19.0, 24.47657323463953, 0.157366019024186, 0.0, 1.0, 65.0, 68.66160474012827], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.52, 0.0, 0.0, 0.08333333333333333, 0.539714436219961, 0.5524553396747286, 0.0, 1.0, 1.0, 0.6866160474012827], 
reward next is 0.3134, 
noisyNet noise sample is [array([1.471661], dtype=float32), -0.44116852]. 
=============================================
[2019-04-09 15:06:40,109] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[4.6731195]
 [4.725953 ]
 [4.8480196]
 [4.7659087]
 [5.025671 ]], R is [[5.08258963]
 [5.368752  ]
 [5.92939997]
 [6.48985815]
 [6.89277458]].
[2019-04-09 15:06:40,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00119355 0.0985858  0.12029549 0.02699642 0.01299863 0.00551823
 0.19477314 0.04912243 0.05990148 0.0580851  0.37252972], sum to 1.0000
[2019-04-09 15:06:40,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3111
[2019-04-09 15:06:41,006] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 19.0, 24.066142537922, 0.09910558957579392, 0.0, 1.0, 65.0, 63.81593825107987], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 424800.0000, 
sim time next is 425400.0000, 
raw observation next is [-10.78333333333333, 49.83333333333334, 0.0, 0.0, 19.0, 24.07261655594952, 0.0970658257253257, 0.0, 1.0, 45.0, 50.78754859716219], 
processed observation next is [1.0, 0.9565217391304348, 0.1638965835641737, 0.4983333333333334, 0.0, 0.0, 0.08333333333333333, 0.5060513796624599, 0.5323552752417752, 0.0, 1.0, 0.6, 0.5078754859716219], 
reward next is 0.4921, 
noisyNet noise sample is [array([-1.6503755], dtype=float32), -1.354111]. 
=============================================
[2019-04-09 15:06:41,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00111711 0.06011126 0.18866986 0.05067784 0.01326912 0.00425314
 0.18678263 0.03869453 0.08357868 0.0563128  0.31653303], sum to 1.0000
[2019-04-09 15:06:41,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9275
[2019-04-09 15:06:41,832] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 47.33333333333334, 0.0, 0.0, 19.0, 26.34059046683998, 0.5854060614277514, 0.0, 1.0, 50.0, 50.99321920601458], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 421800.0000, 
sim time next is 422400.0000, 
raw observation next is [-10.6, 47.66666666666667, 0.0, 0.0, 19.0, 26.33213520342916, 0.584962370232151, 0.0, 1.0, 65.0, 60.44013096330322], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47666666666666674, 0.0, 0.0, 0.08333333333333333, 0.6943446002857634, 0.6949874567440504, 0.0, 1.0, 1.0, 0.6044013096330322], 
reward next is 0.3956, 
noisyNet noise sample is [array([-0.84552044], dtype=float32), -0.73363674]. 
=============================================
[2019-04-09 15:06:42,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00232813 0.11190394 0.10954573 0.02034986 0.00966332 0.00310597
 0.2608977  0.03274525 0.05867246 0.05460557 0.33618206], sum to 1.0000
[2019-04-09 15:06:42,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0038
[2019-04-09 15:06:42,196] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.96666666666667, 50.66666666666667, 0.0, 0.0, 19.0, 26.13562474479529, 0.5057402800565857, 0.0, 1.0, 45.0, 39.62790968444344], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 426000.0000, 
sim time next is 426600.0000, 
raw observation next is [-11.15, 51.5, 0.0, 0.0, 19.0, 26.06281823190241, 0.4801896171931917, 0.0, 1.0, 20.0, 37.71563610779112], 
processed observation next is [1.0, 0.9565217391304348, 0.15373961218836565, 0.515, 0.0, 0.0, 0.08333333333333333, 0.6719015193252007, 0.6600632057310639, 0.0, 1.0, 0.1, 0.3771563610779112], 
reward next is 0.6228, 
noisyNet noise sample is [array([-0.7791063], dtype=float32), 0.6727484]. 
=============================================
[2019-04-09 15:06:42,438] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00107853 0.16329482 0.12403504 0.02961579 0.01146816 0.00258057
 0.20242652 0.02384186 0.09495762 0.04435735 0.30234382], sum to 1.0000
[2019-04-09 15:06:42,438] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1333
[2019-04-09 15:06:42,474] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 47.0, 0.0, 0.0, 19.0, 26.53832102899299, 0.6227099729988748, 0.0, 1.0, 65.0, 58.28873595306261], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 421200.0000, 
sim time next is 421800.0000, 
raw observation next is [-10.6, 47.33333333333334, 0.0, 0.0, 19.0, 26.48897168022109, 0.6103368863836942, 0.0, 1.0, 20.0, 52.78027186214954], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47333333333333344, 0.0, 0.0, 0.08333333333333333, 0.707414306685091, 0.7034456287945647, 0.0, 1.0, 0.1, 0.5278027186214954], 
reward next is 0.4722, 
noisyNet noise sample is [array([-0.227779], dtype=float32), -0.038420036]. 
=============================================
[2019-04-09 15:06:42,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00211736 0.0865521  0.1150927  0.03199119 0.00892483 0.00684154
 0.1992963  0.03966701 0.04920112 0.06096243 0.39935347], sum to 1.0000
[2019-04-09 15:06:42,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7879
[2019-04-09 15:06:42,586] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.61666666666667, 54.16666666666666, 0.0, 0.0, 19.0, 25.79532192860472, 0.3952594005296494, 0.0, 1.0, 20.0, 38.85312579875303], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 432600.0000, 
sim time next is 433200.0000, 
raw observation next is [-11.53333333333333, 54.33333333333334, 0.0, 0.0, 19.0, 25.69260681392863, 0.3892131653198166, 0.0, 1.0, 65.0, 65.32899075812409], 
processed observation next is [1.0, 0.0, 0.14312096029547564, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.6410505678273859, 0.6297377217732721, 0.0, 1.0, 1.0, 0.653289907581241], 
reward next is 0.3467, 
noisyNet noise sample is [array([0.25270534], dtype=float32), -0.39327326]. 
=============================================
[2019-04-09 15:06:42,795] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00208396 0.07130754 0.12288241 0.01536345 0.00701403 0.00191348
 0.26157022 0.03340339 0.03690754 0.05358919 0.39396477], sum to 1.0000
[2019-04-09 15:06:42,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8538
[2019-04-09 15:06:42,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00217158 0.06382512 0.18564989 0.02265585 0.01031001 0.00452126
 0.22421639 0.05069514 0.05389098 0.06500918 0.31705472], sum to 1.0000
[2019-04-09 15:06:42,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2448
[2019-04-09 15:06:42,830] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 19.0, 25.60683143281445, 0.4540557795213419, 0.0, 1.0, 65.0, 55.4993266521248], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 19.0, 25.63174092348422, 0.4697204763391298, 0.0, 1.0, 65.0, 56.75886928018033], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.08333333333333333, 0.6359784102903516, 0.6565734921130433, 0.0, 1.0, 1.0, 0.5675886928018032], 
reward next is 0.4324, 
noisyNet noise sample is [array([-0.68176633], dtype=float32), -0.52354664]. 
=============================================
[2019-04-09 15:06:42,836] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-11.45, 54.5, 0.0, 0.0, 19.0, 24.95075422583234, 0.2643314815508077, 0.0, 1.0, 30.0, 47.4543854803588], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 433800.0000, 
sim time next is 434400.0000, 
raw observation next is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 24.90782323743455, 0.2478283772932836, 0.0, 1.0, 25.0, 32.39485972602727], 
processed observation next is [1.0, 0.0, 0.14773776546629722, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.575651936452879, 0.5826094590977612, 0.0, 1.0, 0.2, 0.3239485972602727], 
reward next is 0.6761, 
noisyNet noise sample is [array([-1.8476359], dtype=float32), 1.7814745]. 
=============================================
[2019-04-09 15:06:43,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0011024  0.11537589 0.16061062 0.02309075 0.01822534 0.00435575
 0.15611596 0.02884029 0.07723539 0.05199888 0.3630488 ], sum to 1.0000
[2019-04-09 15:06:43,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9597
[2019-04-09 15:06:43,208] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 22.5, 25.48773302576272, 0.301673133572135, 1.0, 1.0, 65.0, 61.38512512097353], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 476400.0000, 
sim time next is 477000.0000, 
raw observation next is [-1.45, 26.5, 129.0, 0.0, 22.5, 25.49315550430624, 0.3231947326541711, 1.0, 1.0, 65.0, 58.01864021499912], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.265, 0.43, 0.0, 0.375, 0.6244296253588534, 0.6077315775513904, 1.0, 1.0, 1.0, 0.5801864021499912], 
reward next is 0.4198, 
noisyNet noise sample is [array([-1.4592376], dtype=float32), 2.4402928]. 
=============================================
[2019-04-09 15:06:43,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[6.9406643]
 [6.4813147]
 [6.6789904]
 [6.575363 ]
 [6.5623927]], R is [[7.08619642]
 [7.40148354]
 [7.98221779]
 [8.39966297]
 [8.71538258]].
[2019-04-09 15:06:43,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00337623 0.09812733 0.1106291  0.03584121 0.01360162 0.0048023
 0.2533932  0.03627625 0.06580973 0.05949831 0.3186448 ], sum to 1.0000
[2019-04-09 15:06:43,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6037
[2019-04-09 15:06:43,247] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 19.0, 25.93085734808143, 0.5028423017606626, 0.0, 1.0, 65.0, 48.16654445199064], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 25.93961989487484, 0.5087192310864769, 0.0, 1.0, 55.0, 47.84972597853389], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.6616349912395701, 0.6695730770288256, 0.0, 1.0, 0.8, 0.4784972597853389], 
reward next is 0.5215, 
noisyNet noise sample is [array([-0.50690734], dtype=float32), -0.9304542]. 
=============================================
[2019-04-09 15:06:43,540] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00646483 0.1405347  0.12130719 0.06094832 0.03085013 0.01029994
 0.18499498 0.03465683 0.07601524 0.05815095 0.27577686], sum to 1.0000
[2019-04-09 15:06:43,547] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2043
[2019-04-09 15:06:43,571] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.516666666666667, 82.50000000000001, 0.0, 0.0, 19.0, 25.71105667870637, 0.4944125751722834, 0.0, 1.0, 65.0, 59.19660323106127], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 533400.0000, 
sim time next is 534000.0000, 
raw observation next is [2.333333333333333, 83.0, 0.0, 0.0, 19.0, 25.77740570334477, 0.5011469918304147, 0.0, 1.0, 25.0, 44.6096456749578], 
processed observation next is [0.0, 0.17391304347826086, 0.5272391505078486, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6481171419453974, 0.6670489972768049, 0.0, 1.0, 0.2, 0.446096456749578], 
reward next is 0.5539, 
noisyNet noise sample is [array([0.07179933], dtype=float32), -0.24382593]. 
=============================================
[2019-04-09 15:06:43,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[4.286312]
 [4.341734]
 [4.14982 ]
 [4.35481 ]
 [4.263945]], R is [[4.88734341]
 [5.24650383]
 [5.57713127]
 [6.168715  ]
 [6.72566795]].
[2019-04-09 15:06:43,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0012753  0.11199589 0.11375483 0.01830528 0.01089993 0.0028651
 0.21766828 0.03269983 0.06609299 0.05255584 0.37188676], sum to 1.0000
[2019-04-09 15:06:43,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3939
[2019-04-09 15:06:43,819] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.96666666666667, 50.66666666666667, 0.0, 0.0, 19.0, 25.83454511933184, 0.4615131531698256, 0.0, 1.0, 65.0, 67.965070528489], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 426000.0000, 
sim time next is 426600.0000, 
raw observation next is [-11.15, 51.5, 0.0, 0.0, 19.0, 25.80112101333005, 0.4608042297079336, 0.0, 1.0, 45.0, 51.42227437259961], 
processed observation next is [1.0, 0.9565217391304348, 0.15373961218836565, 0.515, 0.0, 0.0, 0.08333333333333333, 0.6500934177775042, 0.6536014099026445, 0.0, 1.0, 0.6, 0.5142227437259961], 
reward next is 0.4858, 
noisyNet noise sample is [array([-0.20151442], dtype=float32), -0.38929778]. 
=============================================
[2019-04-09 15:06:44,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00077716 0.126409   0.10086995 0.03211186 0.00850859 0.0015903
 0.20669064 0.02228248 0.10608924 0.04743749 0.34723327], sum to 1.0000
[2019-04-09 15:06:44,313] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9488
[2019-04-09 15:06:44,349] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 37.0, 26.0, 0.0, 22.5, 25.84310923959899, 0.478371702681438, 1.0, 1.0, 65.0, 47.49124603435971], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 490800.0000, 
sim time next is 491400.0000, 
raw observation next is [1.1, 38.5, 20.0, 0.0, 22.5, 25.47915048518396, 0.4375201370960122, 1.0, 1.0, 65.0, 38.48906009410626], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.385, 0.06666666666666667, 0.0, 0.375, 0.6232625404319968, 0.6458400456986707, 1.0, 1.0, 1.0, 0.3848906009410626], 
reward next is 0.6151, 
noisyNet noise sample is [array([-0.85422134], dtype=float32), -0.7789209]. 
=============================================
[2019-04-09 15:06:44,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0007341  0.12632108 0.10140332 0.03196559 0.0090389  0.00156113
 0.19907537 0.02432298 0.11620064 0.05312434 0.33625254], sum to 1.0000
[2019-04-09 15:06:44,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8897
[2019-04-09 15:06:44,376] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00086944 0.09958795 0.15515997 0.03649794 0.00998555 0.00267951
 0.22217529 0.03729079 0.06031008 0.06261294 0.31283063], sum to 1.0000
[2019-04-09 15:06:44,379] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4790
[2019-04-09 15:06:44,394] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 22.5, 25.76510892607218, 0.4621932245806602, 1.0, 1.0, 25.0, 48.38084503331846], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 498600.0000, 
sim time next is 499200.0000, 
raw observation next is [0.9000000000000001, 92.0, 0.0, 0.0, 22.5, 25.77149516667449, 0.4652283172748774, 1.0, 1.0, 45.0, 35.66858667858494], 
processed observation next is [1.0, 0.782608695652174, 0.48753462603878117, 0.92, 0.0, 0.0, 0.375, 0.6476245972228742, 0.6550761057582924, 1.0, 1.0, 0.6, 0.35668586678584935], 
reward next is 0.6433, 
noisyNet noise sample is [array([-0.14094244], dtype=float32), -1.0991158]. 
=============================================
[2019-04-09 15:06:44,425] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 38.5, 20.0, 0.0, 22.5, 25.47915048518396, 0.4375201370960122, 1.0, 1.0, 65.0, 38.48906009410626], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 491400.0000, 
sim time next is 492000.0000, 
raw observation next is [1.1, 40.0, 16.66666666666667, 0.0, 22.5, 26.19687967777762, 0.497947997250311, 1.0, 1.0, 65.0, 43.80840920418686], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.4, 0.05555555555555557, 0.0, 0.375, 0.6830733064814684, 0.6659826657501037, 1.0, 1.0, 1.0, 0.4380840920418686], 
reward next is 0.5619, 
noisyNet noise sample is [array([-0.85422134], dtype=float32), -0.7789209]. 
=============================================
[2019-04-09 15:06:44,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[6.918557 ]
 [6.9141183]
 [7.0049915]
 [6.8814907]
 [6.7231116]], R is [[7.42292118]
 [7.96380138]
 [8.40925121]
 [8.79790974]
 [9.32762146]].
[2019-04-09 15:06:45,230] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00566142 0.12105129 0.13097195 0.03438308 0.02017025 0.00706086
 0.16434568 0.05091197 0.06701007 0.07136332 0.32707006], sum to 1.0000
[2019-04-09 15:06:45,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-09 15:06:45,261] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.95, 43.5, 0.0, 0.0, 19.0, 24.14023955989206, 0.09887670461006358, 0.0, 1.0, 60.0, 64.20616841399573], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [-8.766666666666666, 43.33333333333333, 0.0, 0.0, 19.0, 24.30817281421361, 0.1048864992468956, 0.0, 1.0, 55.0, 51.97122478027337], 
processed observation next is [1.0, 0.2608695652173913, 0.2197599261311173, 0.4333333333333333, 0.0, 0.0, 0.08333333333333333, 0.5256810678511341, 0.5349621664156319, 0.0, 1.0, 0.8, 0.5197122478027336], 
reward next is 0.4803, 
noisyNet noise sample is [array([2.6485696], dtype=float32), 0.38251713]. 
=============================================
[2019-04-09 15:06:45,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[5.2683625]
 [5.4527116]
 [5.1748223]
 [5.347568 ]
 [5.2084355]], R is [[5.8575902 ]
 [6.15695286]
 [6.49557209]
 [7.09251881]
 [7.61690474]].
[2019-04-09 15:06:45,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00131796 0.0684773  0.09334747 0.02521341 0.01073662 0.00236374
 0.3111436  0.03042546 0.04993332 0.05125791 0.35578313], sum to 1.0000
[2019-04-09 15:06:45,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6176
[2019-04-09 15:06:45,377] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [2.8, 92.66666666666667, 0.0, 0.0, 19.0, 24.83016214638977, 0.29128327968611, 0.0, 1.0, 65.0, 54.37216480253769], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [2.9, 93.33333333333334, 0.0, 0.0, 19.0, 24.88599434095353, 0.3035204336698865, 0.0, 1.0, 60.0, 53.72383103110308], 
processed observation next is [1.0, 0.9565217391304348, 0.5429362880886427, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.5738328617461276, 0.6011734778899621, 0.0, 1.0, 0.9, 0.5372383103110309], 
reward next is 0.4628, 
noisyNet noise sample is [array([0.18915585], dtype=float32), -1.119171]. 
=============================================
[2019-04-09 15:06:45,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00515951 0.09828833 0.14822794 0.04499771 0.02263323 0.00858108
 0.235841   0.03170257 0.07622587 0.05610049 0.27224225], sum to 1.0000
[2019-04-09 15:06:45,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5665
[2019-04-09 15:06:45,449] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 19.0, 25.24687580166371, 0.3830565108249993, 0.0, 1.0, 45.0, 44.72347545069837], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 19.0, 25.33709807976447, 0.3844313564950142, 0.0, 1.0, 50.0, 39.74041758300978], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 0.08333333333333333, 0.6114248399803724, 0.628143785498338, 0.0, 1.0, 0.7, 0.3974041758300978], 
reward next is 0.6026, 
noisyNet noise sample is [array([1.0479782], dtype=float32), -0.48882633]. 
=============================================
[2019-04-09 15:06:45,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.659881 ]
 [4.8313622]
 [4.9285846]
 [5.1507215]
 [5.058451 ]], R is [[5.14094639]
 [5.64230251]
 [5.99334383]
 [6.24850321]
 [7.18601847]].
[2019-04-09 15:06:46,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00414564 0.09896661 0.11073697 0.04593174 0.01318212 0.00615782
 0.20947355 0.04184383 0.04838626 0.05741891 0.3637565 ], sum to 1.0000
[2019-04-09 15:06:46,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2317
[2019-04-09 15:06:46,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.533333333333333, 38.83333333333334, 15.33333333333334, 0.0, 22.5, 24.78738493104404, 0.1734115879882469, 0.0, 1.0, 65.0, 58.43359478915197], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 461400.0000, 
sim time next is 462000.0000, 
raw observation next is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 22.5, 24.83637516506959, 0.2096765583248643, 1.0, 1.0, 30.0, 52.57555712655525], 
processed observation next is [1.0, 0.34782608695652173, 0.26131117266851345, 0.3766666666666667, 0.0638888888888889, 0.0, 0.375, 0.5696979304224659, 0.5698921861082881, 1.0, 1.0, 0.3, 0.5257555712655525], 
reward next is 0.4742, 
noisyNet noise sample is [array([-1.2179174], dtype=float32), 1.0555534]. 
=============================================
[2019-04-09 15:06:46,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[5.2831903]
 [5.1394877]
 [5.1646166]
 [5.045972 ]
 [5.00432  ]], R is [[5.68183088]
 [6.04067659]
 [6.54235029]
 [6.9743104 ]
 [7.23111534]].
[2019-04-09 15:06:46,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00119983 0.12016968 0.21649776 0.03805193 0.00854708 0.00229566
 0.18304132 0.03062588 0.05716163 0.04725584 0.2951535 ], sum to 1.0000
[2019-04-09 15:06:46,499] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8862
[2019-04-09 15:06:46,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00314385 0.08091252 0.11678189 0.03872536 0.01711032 0.00575623
 0.24316025 0.03396655 0.04970964 0.054896   0.35583735], sum to 1.0000
[2019-04-09 15:06:46,527] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 22.5, 26.60524228716388, 0.5080184940559042, 1.0, 1.0, 20.0, 35.5064536964664], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 476400.0000, 
sim time next is 477000.0000, 
raw observation next is [-1.45, 26.5, 129.0, 0.0, 22.5, 26.54102874741529, 0.5120086498445734, 1.0, 1.0, 65.0, 56.98863381830035], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.265, 0.43, 0.0, 0.375, 0.7117523956179408, 0.6706695499481912, 1.0, 1.0, 1.0, 0.5698863381830035], 
reward next is 0.4301, 
noisyNet noise sample is [array([-0.11841911], dtype=float32), -1.0174383]. 
=============================================
[2019-04-09 15:06:46,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-09 15:06:46,538] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[6.824847]
 [6.731057]
 [6.952801]
 [6.622858]
 [6.810747]], R is [[7.26887512]
 [7.84112215]
 [8.39611244]
 [8.9528923 ]
 [9.41913223]].
[2019-04-09 15:06:46,545] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 25.45699619984635, 0.4023638802042819, 0.0, 1.0, 65.0, 64.79048536936996], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 19.0, 25.41963161654407, 0.4085610190367288, 0.0, 1.0, 45.0, 47.37198572989923], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 0.08333333333333333, 0.6183026347120059, 0.6361870063455762, 0.0, 1.0, 0.6, 0.4737198572989923], 
reward next is 0.5263, 
noisyNet noise sample is [array([0.12695038], dtype=float32), 0.8101662]. 
=============================================
[2019-04-09 15:06:46,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[4.9961357]
 [4.9310517]
 [5.0312295]
 [4.9786687]
 [5.1203914]], R is [[5.39414883]
 [5.6923027 ]
 [6.23867798]
 [6.85497046]
 [7.44899321]].
[2019-04-09 15:06:46,893] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00079608 0.11966933 0.19911334 0.0266765  0.01200524 0.00175171
 0.25295702 0.04159437 0.07824112 0.07970171 0.18749359], sum to 1.0000
[2019-04-09 15:06:46,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-09 15:06:46,916] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.7000000000000001, 88.0, 0.0, 0.0, 22.5, 26.24171127457435, 0.5228391273370457, 1.0, 1.0, 25.0, 43.47960707342096], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 498000.0000, 
sim time next is 498600.0000, 
raw observation next is [0.8, 90.0, 0.0, 0.0, 22.5, 26.18702049246982, 0.5213134579273232, 1.0, 1.0, 20.0, 44.49303434873674], 
processed observation next is [1.0, 0.782608695652174, 0.4847645429362882, 0.9, 0.0, 0.0, 0.375, 0.6822517077058183, 0.6737711526424411, 1.0, 1.0, 0.1, 0.44493034348736743], 
reward next is 0.5551, 
noisyNet noise sample is [array([-1.2701437], dtype=float32), 1.3840805]. 
=============================================
[2019-04-09 15:06:46,998] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00848516 0.08690004 0.1333146  0.04653205 0.02482958 0.01047365
 0.21326427 0.04631198 0.07241482 0.07655095 0.280923  ], sum to 1.0000
[2019-04-09 15:06:47,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8821
[2019-04-09 15:06:47,025] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 19.0, 24.84815358856573, 0.3240241554223848, 0.0, 1.0, 60.0, 52.73230746020758], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 541200.0000, 
sim time next is 541800.0000, 
raw observation next is [0.8, 90.0, 0.0, 0.0, 19.0, 25.00995670206412, 0.3487075534440077, 0.0, 1.0, 65.0, 65.77167008165816], 
processed observation next is [0.0, 0.2608695652173913, 0.4847645429362882, 0.9, 0.0, 0.0, 0.08333333333333333, 0.5841630585053433, 0.6162358511480025, 0.0, 1.0, 1.0, 0.6577167008165816], 
reward next is 0.3423, 
noisyNet noise sample is [array([0.198566], dtype=float32), -0.4403678]. 
=============================================
[2019-04-09 15:06:47,366] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:06:47,366] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:06:47,366] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:47,367] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:06:47,370] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:47,371] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run16
[2019-04-09 15:06:47,382] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:06:47,384] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:47,393] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run16
[2019-04-09 15:06:47,423] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run16
[2019-04-09 15:07:13,602] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02374227], dtype=float32), 0.0286176]
[2019-04-09 15:07:13,603] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [11.0, 85.0, 0.0, 0.0, 19.0, 27.83743134121841, 1.150915217293883, 0.0, 1.0, 20.0, 29.79666261601907]
[2019-04-09 15:07:13,603] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:07:13,604] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00123595 0.1307242  0.12732017 0.0305034  0.0137103  0.0022369
 0.24376199 0.032136   0.06590328 0.05567091 0.2967969 ], sampled 0.035341515460995065
[2019-04-09 15:07:33,912] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02374227], dtype=float32), 0.0286176]
[2019-04-09 15:07:33,912] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-7.491901515333334, 87.64387272333335, 0.0, 0.0, 19.0, 24.64213924934418, 0.2346454061074624, 0.0, 1.0, 20.0, 50.84409042455851]
[2019-04-09 15:07:33,912] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:07:33,913] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00740493 0.10717151 0.12949553 0.05257728 0.02764776 0.01219136
 0.21266115 0.05103096 0.0744448  0.08010546 0.24526925], sampled 0.2692184575415284
[2019-04-09 15:08:09,726] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02374227], dtype=float32), 0.0286176]
[2019-04-09 15:08:09,726] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [23.7, 38.0, 253.0, 380.0, 22.5, 30.8087928584386, 1.948273570593788, 1.0, 0.0, 20.0, 0.0]
[2019-04-09 15:08:09,726] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:08:09,727] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00159041 0.18147068 0.1542536  0.0515219  0.01299442 0.00383092
 0.17384742 0.02910711 0.09294723 0.04581485 0.25262153], sampled 0.36942894379895297
[2019-04-09 15:08:14,871] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5650.9579 284413.7851 2907.9762
[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:14,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:15,052] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:20,870] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5307.6511 318869.8396 2141.1583
[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:20,891] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,005] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,609] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5401.9861 309402.5007 2538.6472
[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,629] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:21,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,631] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 150000, evaluation results [150000.0, 5401.98612964183, 309402.50067726965, 2538.6471780417414, 5650.957939237308, 284413.78513080993, 2907.9761612783977, 5307.651135180974, 318869.839623386, 2141.1582988734253]
[2019-04-09 15:08:22,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00119237 0.0816883  0.15399772 0.03766562 0.01032999 0.00257276
 0.19639307 0.03408356 0.08635473 0.07440993 0.321312  ], sum to 1.0000
[2019-04-09 15:08:22,671] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-09 15:08:22,689] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.9000000000000001, 92.0, 0.0, 0.0, 22.5, 25.64109060920757, 0.4291980614636222, 1.0, 1.0, 45.0, 48.67706372369631], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 499200.0000, 
sim time next is 499800.0000, 
raw observation next is [1.0, 94.0, 0.0, 0.0, 22.5, 25.68295326893057, 0.4234240662721604, 0.0, 1.0, 65.0, 50.84793702058344], 
processed observation next is [1.0, 0.782608695652174, 0.4903047091412743, 0.94, 0.0, 0.0, 0.375, 0.6402461057442143, 0.6411413554240535, 0.0, 1.0, 1.0, 0.5084793702058343], 
reward next is 0.4915, 
noisyNet noise sample is [array([-0.9619793], dtype=float32), 1.6765399]. 
=============================================
[2019-04-09 15:08:22,805] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00804272 0.15394549 0.13144468 0.05043444 0.03439248 0.01187031
 0.20605814 0.0498177  0.0699909  0.08651629 0.19748689], sum to 1.0000
[2019-04-09 15:08:22,811] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4270
[2019-04-09 15:08:22,828] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.08333333333333331, 91.16666666666667, 70.66666666666666, 103.6666666666667, 19.0, 25.046133300503, 0.3477179022819988, 0.0, 1.0, 20.0, 36.79457803287621], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 550200.0000, 
sim time next is 550800.0000, 
raw observation next is [0.0, 91.0, 89.0, 103.5, 19.0, 25.05090251074589, 0.3422780374285184, 0.0, 1.0, 45.0, 35.06088673299683], 
processed observation next is [0.0, 0.391304347826087, 0.46260387811634357, 0.91, 0.2966666666666667, 0.1143646408839779, 0.08333333333333333, 0.5875752092288241, 0.6140926791428395, 0.0, 1.0, 0.6, 0.35060886732996827], 
reward next is 0.6494, 
noisyNet noise sample is [array([-0.05007424], dtype=float32), 1.0815853]. 
=============================================
[2019-04-09 15:08:23,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00409594 0.13128862 0.09857984 0.03703393 0.02753064 0.00735049
 0.24120453 0.04357503 0.06962258 0.06150209 0.27821636], sum to 1.0000
[2019-04-09 15:08:23,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1403
[2019-04-09 15:08:23,403] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 83.0, 70.5, 59.0, 19.0, 25.44453819040103, 0.4339743965245835, 0.0, 1.0, 25.0, 36.43994186794979], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 576000.0000, 
sim time next is 576600.0000, 
raw observation next is [-1.283333333333333, 83.66666666666667, 60.66666666666666, 54.33333333333333, 19.0, 25.43291144401508, 0.4220037936106786, 0.0, 1.0, 25.0, 30.71251830088917], 
processed observation next is [0.0, 0.6956521739130435, 0.4270544783010157, 0.8366666666666667, 0.2022222222222222, 0.060036832412523014, 0.08333333333333333, 0.6194092870012566, 0.6406679312035596, 0.0, 1.0, 0.2, 0.3071251830088917], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.37925956], dtype=float32), 0.13565233]. 
=============================================
[2019-04-09 15:08:23,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00303117 0.09425921 0.11003425 0.04302013 0.0155905  0.00537162
 0.19506079 0.03469537 0.06023169 0.04828982 0.39041543], sum to 1.0000
[2019-04-09 15:08:23,454] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8999
[2019-04-09 15:08:23,467] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 19.0, 26.23499128639957, 0.560123577906051, 0.0, 1.0, 45.0, 43.42428498443272], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 525000.0000, 
sim time next is 525600.0000, 
raw observation next is [4.3, 88.0, 0.0, 0.0, 19.0, 26.21730940924942, 0.5600250318175521, 0.0, 1.0, 65.0, 49.62185686891674], 
processed observation next is [0.0, 0.08695652173913043, 0.5817174515235458, 0.88, 0.0, 0.0, 0.08333333333333333, 0.6847757841041183, 0.6866750106058507, 0.0, 1.0, 1.0, 0.49621856868916736], 
reward next is 0.5038, 
noisyNet noise sample is [array([-0.23789184], dtype=float32), 0.81131244]. 
=============================================
[2019-04-09 15:08:23,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00126063 0.05607579 0.08940531 0.01732538 0.0062124  0.00212876
 0.24417828 0.02784089 0.05530749 0.06150131 0.43876383], sum to 1.0000
[2019-04-09 15:08:23,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-09 15:08:23,592] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 19.0, 25.94556570825076, 0.4876458427064925, 0.0, 1.0, 30.0, 31.97852728936729], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 518400.0000, 
sim time next is 519000.0000, 
raw observation next is [4.0, 95.66666666666667, 0.0, 0.0, 19.0, 25.86843497711378, 0.4698794774242343, 0.0, 1.0, 50.0, 30.44460725542508], 
processed observation next is [0.0, 0.0, 0.5734072022160666, 0.9566666666666667, 0.0, 0.0, 0.08333333333333333, 0.6557029147594816, 0.6566264924747448, 0.0, 1.0, 0.7, 0.3044460725542508], 
reward next is 0.6956, 
noisyNet noise sample is [array([-1.3654715], dtype=float32), 1.789335]. 
=============================================
[2019-04-09 15:08:23,599] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[6.430064 ]
 [6.6999135]
 [6.741973 ]
 [6.80935  ]
 [6.7114615]], R is [[7.15254688]
 [7.76123619]
 [8.3474102 ]
 [8.90846062]
 [9.43894291]].
[2019-04-09 15:08:23,809] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00590827 0.1361321  0.13069086 0.05338239 0.02651369 0.01182401
 0.20555077 0.04044981 0.06573106 0.06279156 0.26102546], sum to 1.0000
[2019-04-09 15:08:23,809] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-09 15:08:23,827] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.2, 75.0, 0.0, 0.0, 19.0, 25.06588326711024, 0.2716830464349603, 0.0, 1.0, 55.0, 43.03958986192923], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 617400.0000, 
sim time next is 618000.0000, 
raw observation next is [-4.3, 75.0, 0.0, 0.0, 19.0, 24.94774658982193, 0.251977167330621, 0.0, 1.0, 30.0, 39.02475008401597], 
processed observation next is [0.0, 0.13043478260869565, 0.34349030470914127, 0.75, 0.0, 0.0, 0.08333333333333333, 0.5789788824851607, 0.583992389110207, 0.0, 1.0, 0.3, 0.3902475008401597], 
reward next is 0.6098, 
noisyNet noise sample is [array([0.3188717], dtype=float32), 0.6928943]. 
=============================================
[2019-04-09 15:08:23,831] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[4.3124785]
 [4.1382856]
 [4.11062  ]
 [4.24186  ]
 [4.4004545]], R is [[4.60154152]
 [5.12513018]
 [5.74035072]
 [6.33410406]
 [6.90530777]].
[2019-04-09 15:08:23,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00688462 0.10605968 0.13071358 0.0418848  0.02888746 0.00738092
 0.18064247 0.05359368 0.06825537 0.06206574 0.3136317 ], sum to 1.0000
[2019-04-09 15:08:23,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-09 15:08:23,934] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 19.0, 24.76168693351064, 0.2643580705733061, 0.0, 1.0, 65.0, 75.04807002056863], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 19.0, 24.77570060457692, 0.2753106572007378, 0.0, 1.0, 45.0, 51.67304280496789], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.08333333333333333, 0.5646417170480765, 0.5917702190669126, 0.0, 1.0, 0.6, 0.516730428049679], 
reward next is 0.4833, 
noisyNet noise sample is [array([0.04490881], dtype=float32), -0.7853588]. 
=============================================
[2019-04-09 15:08:23,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00645477 0.10952778 0.1365777  0.04562152 0.02586803 0.00682601
 0.17381205 0.05144765 0.06914511 0.06282443 0.31189495], sum to 1.0000
[2019-04-09 15:08:23,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6532
[2019-04-09 15:08:23,952] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 73.83333333333333, 0.0, 0.0, 19.0, 24.77570060457692, 0.2753106572007378, 0.0, 1.0, 45.0, 51.67304280496789], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 619800.0000, 
sim time next is 620400.0000, 
raw observation next is [-4.5, 72.66666666666667, 0.0, 0.0, 19.0, 24.81491434294206, 0.274282822172819, 0.0, 1.0, 60.0, 51.3910668124643], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.5679095285785051, 0.5914276073909397, 0.0, 1.0, 0.9, 0.513910668124643], 
reward next is 0.4861, 
noisyNet noise sample is [array([0.04490881], dtype=float32), -0.7853588]. 
=============================================
[2019-04-09 15:08:24,000] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00354063 0.09746888 0.17626075 0.03101368 0.02016906 0.00645682
 0.22174339 0.04905784 0.04719295 0.05663716 0.29045892], sum to 1.0000
[2019-04-09 15:08:24,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6485
[2019-04-09 15:08:24,030] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.65, 88.5, 0.0, 0.0, 19.0, 25.18508071913613, 0.3338887159339121, 0.0, 1.0, 65.0, 55.74212812643576], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 523800.0000, 
sim time next is 524400.0000, 
raw observation next is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 25.106375934201, 0.3412738288388977, 0.0, 1.0, 55.0, 48.48255442444308], 
processed observation next is [0.0, 0.043478260869565216, 0.5881809787626964, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.59219799451675, 0.6137579429462993, 0.0, 1.0, 0.8, 0.4848255442444308], 
reward next is 0.5152, 
noisyNet noise sample is [array([-0.17802168], dtype=float32), 0.14349635]. 
=============================================
[2019-04-09 15:08:24,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00403142 0.11624674 0.11081539 0.03843205 0.01986149 0.00432695
 0.2363164  0.04259208 0.07153803 0.10056462 0.2552749 ], sum to 1.0000
[2019-04-09 15:08:24,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3422
[2019-04-09 15:08:24,149] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 25.09044888574733, 0.3376602505985039, 0.0, 1.0, 65.0, 67.8756827122256], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [-2.8, 86.33333333333333, 0.0, 0.0, 19.0, 25.06826342876328, 0.3426145672799284, 0.0, 1.0, 45.0, 47.98736027703576], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8633333333333333, 0.0, 0.0, 0.08333333333333333, 0.5890219523969401, 0.6142048557599761, 0.0, 1.0, 0.6, 0.4798736027703576], 
reward next is 0.5201, 
noisyNet noise sample is [array([-1.4693507], dtype=float32), 0.56126434]. 
=============================================
[2019-04-09 15:08:24,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[5.088333 ]
 [4.9296083]
 [4.9422626]
 [4.794979 ]
 [4.71896  ]], R is [[5.48944044]
 [5.75578928]
 [6.27542686]
 [6.89478922]
 [7.49223185]].
[2019-04-09 15:08:24,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00691338 0.12373672 0.11341587 0.04381268 0.02966102 0.00957172
 0.18096246 0.04035851 0.08900108 0.07991021 0.28265637], sum to 1.0000
[2019-04-09 15:08:24,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8673
[2019-04-09 15:08:24,445] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.966666666666667, 84.0, 0.0, 0.0, 19.0, 25.18035671204942, 0.3603230346080986, 0.0, 1.0, 20.0, 31.64958410682573], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 535200.0000, 
sim time next is 535800.0000, 
raw observation next is [1.783333333333333, 84.5, 0.0, 0.0, 19.0, 25.15542700335326, 0.3656184988514324, 0.0, 1.0, 65.0, 59.7410002810237], 
processed observation next is [0.0, 0.17391304347826086, 0.5120036934441367, 0.845, 0.0, 0.0, 0.08333333333333333, 0.5962855836127717, 0.6218728329504775, 0.0, 1.0, 1.0, 0.597410002810237], 
reward next is 0.4026, 
noisyNet noise sample is [array([-0.7589779], dtype=float32), 0.8101871]. 
=============================================
[2019-04-09 15:08:24,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00067008 0.10138348 0.13208216 0.02212963 0.00576604 0.00120561
 0.2279504  0.02995778 0.04106056 0.05591751 0.3818767 ], sum to 1.0000
[2019-04-09 15:08:24,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2387
[2019-04-09 15:08:24,480] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 19.0, 26.01144260793161, 0.5309231470364539, 0.0, 1.0, 45.0, 40.30324870476591], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 511200.0000, 
sim time next is 511800.0000, 
raw observation next is [2.8, 92.66666666666667, 0.0, 0.0, 19.0, 26.02437394996411, 0.5298583927613016, 0.0, 1.0, 30.0, 40.40959959769769], 
processed observation next is [1.0, 0.9565217391304348, 0.5401662049861496, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6686978291636759, 0.6766194642537672, 0.0, 1.0, 0.3, 0.40409599597697693], 
reward next is 0.5959, 
noisyNet noise sample is [array([-1.4290378], dtype=float32), -0.32329464]. 
=============================================
[2019-04-09 15:08:24,498] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0042972  0.10028862 0.1304042  0.02896863 0.01659869 0.0070297
 0.2424535  0.03157919 0.07030231 0.07133792 0.29674   ], sum to 1.0000
[2019-04-09 15:08:24,503] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5948
[2019-04-09 15:08:24,519] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.466666666666667, 87.0, 0.0, 0.0, 19.0, 25.93605986351708, 0.5322337777653411, 0.0, 1.0, 65.0, 54.71215826795236], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 584400.0000, 
sim time next is 585000.0000, 
raw observation next is [-2.55, 87.0, 0.0, 0.0, 19.0, 25.97179707307663, 0.5325092558954851, 0.0, 1.0, 55.0, 46.45040504338711], 
processed observation next is [0.0, 0.782608695652174, 0.3919667590027701, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6643164227563858, 0.6775030852984951, 0.0, 1.0, 0.8, 0.46450405043387105], 
reward next is 0.5355, 
noisyNet noise sample is [array([0.26003867], dtype=float32), 0.311516]. 
=============================================
[2019-04-09 15:08:24,524] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[4.958342 ]
 [4.8892994]
 [5.193359 ]
 [5.1386333]
 [5.160108 ]], R is [[5.51037788]
 [5.90815258]
 [6.32191372]
 [6.85231733]
 [7.38131428]].
[2019-04-09 15:08:24,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00605457 0.11285877 0.08998792 0.04817792 0.02483086 0.01136385
 0.21610257 0.03601837 0.08744734 0.07161184 0.29554597], sum to 1.0000
[2019-04-09 15:08:24,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-09 15:08:24,593] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.9, 75.0, 0.0, 0.0, 19.0, 25.44899310280712, 0.3705657892665464, 0.0, 1.0, 25.0, 36.27502508931446], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 615600.0000, 
sim time next is 616200.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 19.0, 25.37938604874534, 0.3616072859137824, 0.0, 1.0, 55.0, 42.7330751960846], 
processed observation next is [0.0, 0.13043478260869565, 0.3518005540166205, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6149488373954449, 0.6205357619712608, 0.0, 1.0, 0.8, 0.42733075196084597], 
reward next is 0.5727, 
noisyNet noise sample is [array([-1.3729948], dtype=float32), -0.3960337]. 
=============================================
[2019-04-09 15:08:24,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00568652 0.11240441 0.12896287 0.06695057 0.02344377 0.0106106
 0.22415479 0.04111755 0.07636528 0.07067165 0.23963207], sum to 1.0000
[2019-04-09 15:08:24,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0416
[2019-04-09 15:08:24,889] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 19.0, 24.71915021374767, 0.2269522493362603, 0.0, 1.0, 25.0, 38.90703549779863], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 19.0, 24.69888669964251, 0.2175347895389564, 0.0, 1.0, 20.0, 37.71399051612759], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 0.08333333333333333, 0.5582405583035426, 0.5725115965129854, 0.0, 1.0, 0.1, 0.3771399051612759], 
reward next is 0.6229, 
noisyNet noise sample is [array([1.3686743], dtype=float32), -0.59386915]. 
=============================================
[2019-04-09 15:08:25,127] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0057336  0.14716166 0.1277867  0.05608687 0.03683566 0.01201404
 0.17607555 0.04984051 0.09551232 0.08085942 0.21209371], sum to 1.0000
[2019-04-09 15:08:25,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1046
[2019-04-09 15:08:25,143] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 19.0, 25.70951381104203, 0.4545411400189662, 0.0, 1.0, 20.0, 60.29288166437954], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 19.0, 25.59239167189361, 0.4518985699926546, 0.0, 1.0, 20.0, 37.54688613617569], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.632699305991134, 0.6506328566642182, 0.0, 1.0, 0.1, 0.3754688613617569], 
reward next is 0.6245, 
noisyNet noise sample is [array([-0.40940177], dtype=float32), -1.13503]. 
=============================================
[2019-04-09 15:08:25,923] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00330799 0.08678138 0.10332774 0.05868321 0.02429394 0.00597543
 0.24329036 0.03382875 0.06931207 0.06836175 0.30283743], sum to 1.0000
[2019-04-09 15:08:25,923] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3320
[2019-04-09 15:08:25,939] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 82.0, 89.0, 135.0, 19.0, 26.0800928616698, 0.5757865880532801, 0.0, 1.0, 45.0, 39.31814823449701], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 559800.0000, 
sim time next is 560400.0000, 
raw observation next is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 26.06627845162497, 0.5771925150349856, 0.0, 1.0, 45.0, 39.68943177606959], 
processed observation next is [0.0, 0.4782608695652174, 0.44228993536472766, 0.8166666666666668, 0.3194444444444444, 0.19723756906077347, 0.08333333333333333, 0.6721898709687476, 0.6923975050116619, 0.0, 1.0, 0.6, 0.3968943177606959], 
reward next is 0.6031, 
noisyNet noise sample is [array([-0.32577178], dtype=float32), 0.35959074]. 
=============================================
[2019-04-09 15:08:25,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0035954  0.124879   0.1563887  0.04800994 0.01337604 0.00639494
 0.15987062 0.04483094 0.09498132 0.05134126 0.29633182], sum to 1.0000
[2019-04-09 15:08:25,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2628
[2019-04-09 15:08:26,011] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 19.0, 25.78583073484276, 0.5589115070038004, 0.0, 1.0, 30.0, 46.39682359184906], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 19.0, 25.84627999189775, 0.5713211830978399, 0.0, 1.0, 25.0, 36.50650168618166], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4555555555555557, 0.6208103130755065, 0.08333333333333333, 0.6538566659914791, 0.6904403943659466, 0.0, 1.0, 0.2, 0.3650650168618166], 
reward next is 0.6349, 
noisyNet noise sample is [array([0.28419828], dtype=float32), -2.1565583]. 
=============================================
[2019-04-09 15:08:26,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00624643 0.14451678 0.12692662 0.06579994 0.02408643 0.00815963
 0.19645952 0.03353542 0.07711335 0.06317055 0.25398532], sum to 1.0000
[2019-04-09 15:08:26,260] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9727
[2019-04-09 15:08:26,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00307    0.09892596 0.12737928 0.04293223 0.02848659 0.00700645
 0.187585   0.04286095 0.10743269 0.09392324 0.2603976 ], sum to 1.0000
[2019-04-09 15:08:26,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3520
[2019-04-09 15:08:26,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.399999999999999, 75.0, 0.0, 0.0, 19.0, 24.92388935425444, 0.2777190808758671, 0.0, 1.0, 60.0, 50.23003051366418], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 618600.0000, 
sim time next is 619200.0000, 
raw observation next is [-4.5, 75.0, 0.0, 0.0, 19.0, 24.87372333950069, 0.2906493198506028, 0.0, 1.0, 65.0, 69.40898930293314], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.75, 0.0, 0.0, 0.08333333333333333, 0.572810278291724, 0.5968831066168676, 0.0, 1.0, 1.0, 0.6940898930293313], 
reward next is 0.3059, 
noisyNet noise sample is [array([1.4314455], dtype=float32), -0.5300366]. 
=============================================
[2019-04-09 15:08:26,307] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 87.0, 0.0, 0.0, 19.0, 26.23627651527371, 0.6020779423561863, 0.0, 1.0, 65.0, 53.69165775143806], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 580800.0000, 
sim time next is 581400.0000, 
raw observation next is [-2.0, 87.0, 0.0, 0.0, 19.0, 26.24689066221607, 0.6054484255492857, 0.0, 1.0, 45.0, 45.00639991792912], 
processed observation next is [0.0, 0.7391304347826086, 0.40720221606648205, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6872408885180059, 0.7018161418497618, 0.0, 1.0, 0.6, 0.45006399917929124], 
reward next is 0.5499, 
noisyNet noise sample is [array([0.99970764], dtype=float32), 0.12929542]. 
=============================================
[2019-04-09 15:08:26,444] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00786848 0.11115716 0.14939138 0.06545197 0.02981454 0.01274791
 0.1797935  0.03683092 0.09506783 0.08598255 0.2258937 ], sum to 1.0000
[2019-04-09 15:08:26,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0420
[2019-04-09 15:08:26,458] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.2, 75.0, 19.0, 0.0, 19.0, 24.20696995377716, 0.1286758380222326, 0.0, 1.0, 45.0, 36.37658301979029], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 635400.0000, 
sim time next is 636000.0000, 
raw observation next is [-4.1, 73.66666666666666, 38.33333333333333, 8.499999999999998, 19.0, 24.22766203313653, 0.120900088828988, 0.0, 1.0, 20.0, 37.19394420500815], 
processed observation next is [0.0, 0.34782608695652173, 0.3490304709141275, 0.7366666666666666, 0.12777777777777777, 0.009392265193370164, 0.08333333333333333, 0.5189718360947108, 0.5403000296096626, 0.0, 1.0, 0.1, 0.3719394420500815], 
reward next is 0.6281, 
noisyNet noise sample is [array([1.2231832], dtype=float32), -1.3265196]. 
=============================================
[2019-04-09 15:08:26,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.111832 ]
 [4.2069054]
 [3.9159026]
 [4.114269 ]
 [4.132381 ]], R is [[4.63640976]
 [5.22628021]
 [5.66764736]
 [5.92130899]
 [6.42022848]].
[2019-04-09 15:08:26,499] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00376774 0.10089173 0.11772946 0.03512463 0.01510908 0.00837924
 0.20946096 0.03292994 0.06516983 0.05818644 0.35325098], sum to 1.0000
[2019-04-09 15:08:26,499] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0651
[2019-04-09 15:08:26,515] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 83.0, 100.0, 73.0, 19.0, 25.20806600699857, 0.4240628073977792, 0.0, 1.0, 25.0, 46.51986014668766], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 574200.0000, 
sim time next is 574800.0000, 
raw observation next is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 19.0, 25.33308176468617, 0.4242043597838278, 0.0, 1.0, 20.0, 35.7434421312395], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3005555555555556, 0.07550644567219153, 0.08333333333333333, 0.6110901470571809, 0.641401453261276, 0.0, 1.0, 0.1, 0.357434421312395], 
reward next is 0.6426, 
noisyNet noise sample is [array([1.4101752], dtype=float32), 0.98551726]. 
=============================================
[2019-04-09 15:08:26,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00395678 0.09221602 0.16136454 0.03720092 0.02003796 0.01263803
 0.21390083 0.03712421 0.09168646 0.07692142 0.2529529 ], sum to 1.0000
[2019-04-09 15:08:26,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1254
[2019-04-09 15:08:26,807] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 80.0, 132.5, 531.0, 19.0, 25.8824034072735, 0.6006879360598788, 0.0, 1.0, 20.0, 43.52513095474536], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 568800.0000, 
sim time next is 569400.0000, 
raw observation next is [-1.2, 80.5, 130.6666666666667, 509.6666666666666, 19.0, 25.91242731704037, 0.6006134929668038, 0.0, 1.0, 25.0, 28.52276537673838], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.805, 0.4355555555555557, 0.5631675874769796, 0.08333333333333333, 0.6593689430866974, 0.7002044976556013, 0.0, 1.0, 0.2, 0.2852276537673838], 
reward next is 0.7148, 
noisyNet noise sample is [array([1.0161761], dtype=float32), -0.80029917]. 
=============================================
[2019-04-09 15:08:26,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00437461 0.13270175 0.17678104 0.03703644 0.02027097 0.00716889
 0.2016962  0.0444792  0.06652901 0.07226142 0.23670045], sum to 1.0000
[2019-04-09 15:08:26,952] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7419
[2019-04-09 15:08:26,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.3, 87.0, 0.0, 0.0, 19.0, 26.05515826203046, 0.5575857728751277, 0.0, 1.0, 50.0, 35.94414881684086], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 583200.0000, 
sim time next is 583800.0000, 
raw observation next is [-2.383333333333333, 87.0, 0.0, 0.0, 19.0, 26.03198231086378, 0.5579770153166975, 0.0, 1.0, 60.0, 47.85489136153448], 
processed observation next is [0.0, 0.782608695652174, 0.3965835641735919, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6693318592386482, 0.6859923384388992, 0.0, 1.0, 0.9, 0.4785489136153448], 
reward next is 0.5215, 
noisyNet noise sample is [array([1.6188921], dtype=float32), -0.50389016]. 
=============================================
[2019-04-09 15:08:27,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00315677 0.07848277 0.15366411 0.03495366 0.01494001 0.00554554
 0.2115174  0.03877535 0.07541914 0.05413231 0.32941294], sum to 1.0000
[2019-04-09 15:08:27,313] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1228
[2019-04-09 15:08:27,336] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 85.0, 0.0, 0.0, 19.0, 25.94397146427818, 0.5097324914828628, 0.0, 1.0, 20.0, 36.27021097638449], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 592200.0000, 
sim time next is 592800.0000, 
raw observation next is [-2.8, 84.33333333333333, 0.0, 0.0, 19.0, 25.91135568131584, 0.4898475049874989, 0.0, 1.0, 25.0, 34.51437212407809], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.6592796401096533, 0.6632825016624996, 0.0, 1.0, 0.2, 0.3451437212407809], 
reward next is 0.6549, 
noisyNet noise sample is [array([-0.88782203], dtype=float32), -0.6703156]. 
=============================================
[2019-04-09 15:08:27,439] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00499212 0.14303154 0.13431686 0.05626602 0.03068972 0.00985411
 0.21494079 0.04873873 0.0628435  0.0593987  0.23492795], sum to 1.0000
[2019-04-09 15:08:27,444] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6042
[2019-04-09 15:08:27,460] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 65.0, 94.5, 19.0, 19.0, 24.49822077583418, 0.2150025091773049, 0.0, 1.0, 60.0, 59.91685998361402], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 644400.0000, 
sim time next is 645000.0000, 
raw observation next is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 19.0, 24.55271856640437, 0.2213938334848899, 0.0, 1.0, 50.0, 44.24311575079098], 
processed observation next is [0.0, 0.4782608695652174, 0.3716528162511542, 0.6433333333333334, 0.3088888888888889, 0.027992633517495404, 0.08333333333333333, 0.5460598805336975, 0.5737979444949634, 0.0, 1.0, 0.7, 0.4424311575079098], 
reward next is 0.5576, 
noisyNet noise sample is [array([-1.1789862], dtype=float32), -0.8342437]. 
=============================================
[2019-04-09 15:08:27,470] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[4.5411644]
 [4.522961 ]
 [4.4102607]
 [4.4733825]
 [4.203651 ]], R is [[5.26219654]
 [5.61040592]
 [5.91884327]
 [6.366961  ]
 [6.68066216]].
[2019-04-09 15:08:27,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00447865 0.10759508 0.17046462 0.05421758 0.01724568 0.00558444
 0.23893115 0.04959438 0.06839123 0.06167069 0.22182646], sum to 1.0000
[2019-04-09 15:08:27,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8512
[2019-04-09 15:08:27,592] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 19.0, 25.67508985653128, 0.4901236997209146, 0.0, 1.0, 65.0, 59.69017367404417], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 596400.0000, 
sim time next is 597000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 19.0, 25.73994859802335, 0.4969839119842944, 0.0, 1.0, 65.0, 56.49304146420166], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6449957165019459, 0.6656613039947649, 0.0, 1.0, 1.0, 0.5649304146420167], 
reward next is 0.4351, 
noisyNet noise sample is [array([-0.663586], dtype=float32), -0.852263]. 
=============================================
[2019-04-09 15:08:27,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[5.080891 ]
 [5.2496324]
 [5.2405763]
 [5.0402393]
 [5.142112 ]], R is [[5.64881659]
 [5.99542665]
 [6.28258991]
 [6.73126125]
 [7.29640388]].
[2019-04-09 15:08:27,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00431654 0.09682406 0.16028202 0.03665985 0.02205801 0.00678519
 0.16914998 0.04265717 0.0773655  0.1018841  0.28201762], sum to 1.0000
[2019-04-09 15:08:27,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1676
[2019-04-09 15:08:27,845] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 19.0, 24.7168713837811, 0.1763825894400415, 0.0, 1.0, 25.0, 35.6077029743802], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 687600.0000, 
sim time next is 688200.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 19.0, 24.65292446373754, 0.1573783455630712, 0.0, 1.0, 25.0, 33.73498115543281], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5544103719781285, 0.5524594485210238, 0.0, 1.0, 0.2, 0.33734981155432814], 
reward next is 0.6627, 
noisyNet noise sample is [array([1.5811912], dtype=float32), -0.18776149]. 
=============================================
[2019-04-09 15:08:28,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00579321 0.1302545  0.15377332 0.05617605 0.0306592  0.00825863
 0.16801155 0.03703416 0.09696569 0.09886172 0.21421203], sum to 1.0000
[2019-04-09 15:08:28,038] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1773
[2019-04-09 15:08:28,051] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.1, 56.5, 18.0, 10.66666666666667, 19.0, 24.51129138260044, 0.1522864849630711, 0.0, 1.0, 45.0, 30.45882482019607], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 665400.0000, 
sim time next is 666000.0000, 
raw observation next is [-1.2, 57.0, 13.5, 8.5, 19.0, 24.44096264216096, 0.1318707546978154, 0.0, 1.0, 25.0, 29.04111122601811], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.045, 0.009392265193370166, 0.08333333333333333, 0.5367468868467468, 0.5439569182326052, 0.0, 1.0, 0.2, 0.2904111122601811], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.7868475], dtype=float32), 1.1456045]. 
=============================================
[2019-04-09 15:08:28,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[4.6083603]
 [4.516903 ]
 [4.734172 ]
 [4.676072 ]
 [4.6410933]], R is [[5.11362648]
 [5.75790215]
 [6.38072252]
 [6.98381662]
 [7.58992195]].
[2019-04-09 15:08:28,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00320199 0.09830755 0.12207436 0.03375528 0.0206715  0.01061556
 0.22009318 0.05051547 0.05181425 0.08670384 0.302247  ], sum to 1.0000
[2019-04-09 15:08:28,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7924
[2019-04-09 15:08:28,357] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 19.0, 25.31814229815211, 0.3282316595698345, 0.0, 1.0, 20.0, 49.80459619696217], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 687000.0000, 
sim time next is 687600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 19.0, 25.30164722521501, 0.3303039604258585, 0.0, 1.0, 65.0, 54.41075725759574], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6084706021012508, 0.6101013201419528, 0.0, 1.0, 1.0, 0.5441075725759574], 
reward next is 0.4559, 
noisyNet noise sample is [array([-0.22472751], dtype=float32), -0.2842173]. 
=============================================
[2019-04-09 15:08:28,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0077927  0.12836643 0.12497331 0.0626253  0.0301247  0.00783476
 0.21284784 0.06299736 0.07710194 0.06801187 0.21732372], sum to 1.0000
[2019-04-09 15:08:28,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-09 15:08:28,583] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 19.0, 24.06620825594497, 0.0972962949179792, 0.0, 1.0, 25.0, 36.03861046564801], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [-4.5, 65.0, 0.0, 0.0, 19.0, 24.08628080997127, 0.0839898787464035, 0.0, 1.0, 20.0, 34.31268640497721], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5071900674976059, 0.5279966262488012, 0.0, 1.0, 0.1, 0.34312686404977205], 
reward next is 0.6569, 
noisyNet noise sample is [array([-0.62769264], dtype=float32), 2.0055313]. 
=============================================
[2019-04-09 15:08:28,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00317846 0.11415812 0.10602089 0.02663745 0.01893128 0.00474534
 0.22909139 0.04526699 0.09063323 0.0742258  0.28711104], sum to 1.0000
[2019-04-09 15:08:28,689] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4798
[2019-04-09 15:08:28,732] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 83.0, 0.0, 0.0, 19.0, 25.27933670310034, 0.3618732132396365, 0.0, 1.0, 65.0, 59.01482221681538], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [-3.2, 83.0, 0.0, 0.0, 19.0, 25.18101371935437, 0.3605511080364264, 0.0, 1.0, 45.0, 46.19316828538393], 
processed observation next is [0.0, 0.9565217391304348, 0.37396121883656513, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5984178099461976, 0.6201837026788088, 0.0, 1.0, 0.6, 0.46193168285383934], 
reward next is 0.5381, 
noisyNet noise sample is [array([-0.2347648], dtype=float32), -1.4010249]. 
=============================================
[2019-04-09 15:08:28,742] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00976044 0.11723479 0.1449037  0.0539595  0.03686662 0.01274212
 0.24092391 0.04269526 0.06035789 0.07400725 0.20654854], sum to 1.0000
[2019-04-09 15:08:28,747] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8385
[2019-04-09 15:08:28,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[5.1508427]
 [4.932084 ]
 [4.9728904]
 [5.0821643]
 [5.120954 ]], R is [[5.71716976]
 [6.06984997]
 [6.69707346]
 [7.30210638]
 [7.88413858]].
[2019-04-09 15:08:28,775] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 75.0, 0.0, 0.0, 19.0, 24.80087306185257, 0.2503970954817572, 0.0, 1.0, 20.0, 42.41603703587482], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 615600.0000, 
sim time next is 616200.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 19.0, 24.82119327673491, 0.2630229129407147, 0.0, 1.0, 65.0, 56.25066268184608], 
processed observation next is [0.0, 0.13043478260869565, 0.3518005540166205, 0.75, 0.0, 0.0, 0.08333333333333333, 0.5684327730612425, 0.5876743043135716, 0.0, 1.0, 1.0, 0.5625066268184608], 
reward next is 0.4375, 
noisyNet noise sample is [array([-0.68986094], dtype=float32), -1.2533944]. 
=============================================
[2019-04-09 15:08:28,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00586899 0.11311702 0.16530545 0.04855324 0.02423624 0.00673077
 0.18610536 0.04374905 0.07164801 0.08934509 0.2453408 ], sum to 1.0000
[2019-04-09 15:08:28,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-09 15:08:28,799] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 24.7602526106991, 0.1970809753713451, 0.0, 1.0, 25.0, 38.42220902907354], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 24.73883530444728, 0.1824376385639771, 0.0, 1.0, 25.0, 38.28319293428073], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5615696087039401, 0.5608125461879924, 0.0, 1.0, 0.2, 0.3828319293428073], 
reward next is 0.6172, 
noisyNet noise sample is [array([-1.4988394], dtype=float32), 0.51576465]. 
=============================================
[2019-04-09 15:08:29,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0081914  0.10955289 0.13813488 0.04779343 0.03129285 0.0146092
 0.21169785 0.05303895 0.09115792 0.06813683 0.22639376], sum to 1.0000
[2019-04-09 15:08:29,088] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7154
[2019-04-09 15:08:29,107] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 19.0, 24.8160409912194, 0.2649771901089962, 0.0, 1.0, 55.0, 52.06450750306405], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 625200.0000, 
sim time next is 625800.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 19.0, 24.78984282459367, 0.2748739075502032, 0.0, 1.0, 60.0, 52.58380835691599], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.655, 0.0, 0.0, 0.08333333333333333, 0.5658202353828058, 0.5916246358500677, 0.0, 1.0, 0.9, 0.5258380835691598], 
reward next is 0.4742, 
noisyNet noise sample is [array([0.07645682], dtype=float32), 0.29844713]. 
=============================================
[2019-04-09 15:08:29,192] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00746881 0.10202135 0.11414285 0.06193547 0.02961637 0.00959671
 0.2029166  0.0537417  0.1019266  0.05840866 0.25822487], sum to 1.0000
[2019-04-09 15:08:29,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4781
[2019-04-09 15:08:29,230] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 19.0, 25.51122874826601, 0.4130340337658402, 0.0, 1.0, 45.0, 49.26963428389624], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 616200.0000, 
sim time next is 616800.0000, 
raw observation next is [-4.1, 75.0, 0.0, 0.0, 19.0, 25.56627386333375, 0.4063519811211229, 0.0, 1.0, 45.0, 41.57858071686321], 
processed observation next is [0.0, 0.13043478260869565, 0.3490304709141275, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6305228219444791, 0.6354506603737077, 0.0, 1.0, 0.6, 0.41578580716863206], 
reward next is 0.5842, 
noisyNet noise sample is [array([0.27074262], dtype=float32), 0.60676235]. 
=============================================
[2019-04-09 15:08:30,249] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00322933 0.09268313 0.16768584 0.02496835 0.014836   0.00507442
 0.25098988 0.03513239 0.06631247 0.07567766 0.2634105 ], sum to 1.0000
[2019-04-09 15:08:30,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1845
[2019-04-09 15:08:30,266] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 25.13103732638905, 0.2621488484524013, 0.0, 1.0, 25.0, 37.71874731108617], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 25.07598134460836, 0.2484421991530956, 0.0, 1.0, 25.0, 35.97794679246437], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.5896651120506965, 0.5828140663843652, 0.0, 1.0, 0.2, 0.3597794679246437], 
reward next is 0.6402, 
noisyNet noise sample is [array([-0.26416996], dtype=float32), -2.2069938]. 
=============================================
[2019-04-09 15:08:30,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00061816 0.16895363 0.12612717 0.03439793 0.00864467 0.00161608
 0.1970209  0.03733144 0.04409594 0.04888691 0.33230716], sum to 1.0000
[2019-04-09 15:08:30,793] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8570
[2019-04-09 15:08:30,822] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 22.5, 27.13068600515384, 0.7204249373646564, 1.0, 1.0, 65.0, 35.48646842862922], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 22.5, 27.12517558521504, 0.7188278233075395, 1.0, 1.0, 65.0, 35.20632779137311], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.375, 0.7604312987679199, 0.7396092744358466, 1.0, 1.0, 1.0, 0.35206327791373115], 
reward next is 0.6479, 
noisyNet noise sample is [array([-0.69568056], dtype=float32), 1.4649302]. 
=============================================
[2019-04-09 15:08:31,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00275271 0.07876762 0.13494065 0.03989637 0.01002029 0.00565278
 0.33014023 0.04279299 0.04734632 0.09147274 0.21621726], sum to 1.0000
[2019-04-09 15:08:31,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2178
[2019-04-09 15:08:31,036] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 19.0, 25.07109390926954, 0.2521869310901088, 0.0, 1.0, 25.0, 40.08208869259004], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 700800.0000, 
sim time next is 701400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 25.0402086782154, 0.239886201835586, 0.0, 1.0, 30.0, 38.16427470016166], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.58668405651795, 0.5799620672785287, 0.0, 1.0, 0.3, 0.3816427470016166], 
reward next is 0.6184, 
noisyNet noise sample is [array([-2.324025], dtype=float32), 1.0833728]. 
=============================================
[2019-04-09 15:08:31,181] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00492713 0.1420222  0.13074635 0.05068726 0.02192894 0.00725333
 0.19705549 0.04468254 0.09478951 0.10168757 0.20421977], sum to 1.0000
[2019-04-09 15:08:31,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7027
[2019-04-09 15:08:31,199] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 19.0, 24.76511553449284, 0.2780412103402562, 0.0, 1.0, 55.0, 39.02718189261214], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 19.0, 24.83081565317665, 0.2889372449177842, 0.0, 1.0, 60.0, 50.76263532975388], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.08333333333333333, 0.5692346377647208, 0.5963124149725948, 0.0, 1.0, 0.9, 0.5076263532975388], 
reward next is 0.4924, 
noisyNet noise sample is [array([0.04841799], dtype=float32), 0.20980603]. 
=============================================
[2019-04-09 15:08:31,372] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0011119  0.1467061  0.14707391 0.03776561 0.01014434 0.00190368
 0.19357617 0.02484081 0.07076358 0.05421506 0.3118988 ], sum to 1.0000
[2019-04-09 15:08:31,373] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9950
[2019-04-09 15:08:31,393] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.9, 70.66666666666666, 107.3333333333333, 52.16666666666666, 22.5, 26.09101285416931, 0.4521345626649517, 1.0, 1.0, 55.0, 44.87490043400982], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 726000.0000, 
sim time next is 726600.0000, 
raw observation next is [-1.8, 69.33333333333334, 113.6666666666667, 55.33333333333333, 22.5, 26.2167590085609, 0.4693877345127715, 1.0, 1.0, 65.0, 46.80284286083439], 
processed observation next is [1.0, 0.391304347826087, 0.41274238227146814, 0.6933333333333335, 0.378888888888889, 0.06114180478821362, 0.375, 0.6847299173800749, 0.6564625781709238, 1.0, 1.0, 1.0, 0.46802842860834387], 
reward next is 0.5320, 
noisyNet noise sample is [array([0.78041345], dtype=float32), -0.8415006]. 
=============================================
[2019-04-09 15:08:31,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00430842 0.11104116 0.17665629 0.04082017 0.02042437 0.00616688
 0.22510368 0.04956735 0.09074001 0.07056805 0.20460364], sum to 1.0000
[2019-04-09 15:08:31,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5205
[2019-04-09 15:08:31,701] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 19.0, 24.44272368924806, 0.1505819414648401, 0.0, 1.0, 55.0, 42.21419849092583], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 24.46241440493945, 0.1429106517633477, 0.0, 1.0, 45.0, 37.57593628589852], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.5385345337449543, 0.5476368839211159, 0.0, 1.0, 0.6, 0.3757593628589852], 
reward next is 0.6242, 
noisyNet noise sample is [array([2.0384178], dtype=float32), -0.56173956]. 
=============================================
[2019-04-09 15:08:31,736] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00053933 0.13129427 0.17869389 0.01458497 0.00462912 0.00096365
 0.25172806 0.01945812 0.05331856 0.0448022  0.2999878 ], sum to 1.0000
[2019-04-09 15:08:31,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2086
[2019-04-09 15:08:31,756] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 22.5, 27.49470842029307, 0.8073180815121574, 1.0, 1.0, 45.0, 27.82164470517031], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 753000.0000, 
sim time next is 753600.0000, 
raw observation next is [-3.166666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 27.47241729735174, 0.4949366489237029, 1.0, 1.0, 25.0, 53.86669881974961], 
processed observation next is [1.0, 0.7391304347826086, 0.3748845798707295, 0.5466666666666667, 0.0, 0.0, 0.375, 0.789368108112645, 0.6649788829745676, 1.0, 1.0, 0.2, 0.538666988197496], 
reward next is 0.4613, 
noisyNet noise sample is [array([0.944465], dtype=float32), 0.43249485]. 
=============================================
[2019-04-09 15:08:31,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00046363 0.15028743 0.13772658 0.0308499  0.00649191 0.00197077
 0.18200977 0.01961295 0.07401919 0.04086858 0.3556993 ], sum to 1.0000
[2019-04-09 15:08:31,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4280
[2019-04-09 15:08:31,890] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4166666666666667, 45.33333333333334, 83.0, 733.6666666666667, 22.5, 27.32050977088822, 0.8067516605424156, 1.0, 1.0, 65.0, 32.51435893794493], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 742200.0000, 
sim time next is 742800.0000, 
raw observation next is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 22.5, 27.37182212961552, 0.8177198459583761, 1.0, 1.0, 45.0, 26.27386129997782], 
processed observation next is [1.0, 0.6086956521739131, 0.4718374884579871, 0.4566666666666667, 0.27166666666666667, 0.7998158379373849, 0.375, 0.78098517746796, 0.7725732819861254, 1.0, 1.0, 0.6, 0.2627386129997782], 
reward next is 0.7373, 
noisyNet noise sample is [array([1.2519411], dtype=float32), -1.3642166]. 
=============================================
[2019-04-09 15:08:32,069] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00631034 0.12526363 0.15663376 0.03987775 0.01752629 0.00708734
 0.20939763 0.05860697 0.0812401  0.07911542 0.21894075], sum to 1.0000
[2019-04-09 15:08:32,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-09 15:08:32,089] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.8999999999999999, 55.5, 27.0, 15.0, 19.0, 25.12163054558675, 0.2762305887390562, 0.0, 1.0, 50.0, 35.39925493492228], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 664200.0000, 
sim time next is 664800.0000, 
raw observation next is [-1.0, 56.0, 22.5, 12.83333333333333, 19.0, 25.05140629489533, 0.264607766391711, 0.0, 1.0, 55.0, 43.72470154793877], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.56, 0.075, 0.014180478821362795, 0.08333333333333333, 0.5876171912412774, 0.588202588797237, 0.0, 1.0, 0.8, 0.4372470154793877], 
reward next is 0.5628, 
noisyNet noise sample is [array([-1.0687253], dtype=float32), -0.5077556]. 
=============================================
[2019-04-09 15:08:32,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00482231 0.06929632 0.14724554 0.03871392 0.0174636  0.0061795
 0.30099842 0.04594557 0.06776185 0.05105003 0.25052303], sum to 1.0000
[2019-04-09 15:08:32,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-09 15:08:32,216] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 24.87138587120931, 0.2475609428446597, 0.0, 1.0, 65.0, 62.82137035815204], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 19.0, 24.86320709833254, 0.2518653418814913, 0.0, 1.0, 30.0, 49.95707623594353], 
processed observation next is [0.0, 0.9565217391304348, 0.3568790397045245, 0.7066666666666667, 0.0, 0.0, 0.08333333333333333, 0.5719339248610451, 0.5839551139604972, 0.0, 1.0, 0.3, 0.4995707623594353], 
reward next is 0.5004, 
noisyNet noise sample is [array([-0.12847018], dtype=float32), -0.33163825]. 
=============================================
[2019-04-09 15:08:32,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[5.3309603]
 [5.1928873]
 [5.516665 ]
 [5.471876 ]
 [5.225974 ]], R is [[5.76546907]
 [6.07960033]
 [6.6930213 ]
 [7.14991045]
 [7.48398685]].
[2019-04-09 15:08:32,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00283504 0.09222653 0.18919177 0.03032036 0.01561857 0.00417648
 0.22019827 0.03829599 0.05744728 0.0556205  0.29406917], sum to 1.0000
[2019-04-09 15:08:32,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1799
[2019-04-09 15:08:32,475] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 19.0, 24.74589203545393, 0.2430613769954398, 0.0, 1.0, 25.0, 49.88706750042516], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 693600.0000, 
sim time next is 694200.0000, 
raw observation next is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 19.0, 24.86771696960099, 0.2425888510904926, 0.0, 1.0, 25.0, 38.17018050270468], 
processed observation next is [1.0, 0.0, 0.3661126500461681, 0.7183333333333333, 0.0, 0.0, 0.08333333333333333, 0.5723097474667492, 0.5808629503634976, 0.0, 1.0, 0.2, 0.3817018050270468], 
reward next is 0.6183, 
noisyNet noise sample is [array([0.5520405], dtype=float32), 0.13165319]. 
=============================================
[2019-04-09 15:08:32,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00377174 0.12431607 0.13069941 0.03590313 0.02538605 0.00773211
 0.24157749 0.03614632 0.07444885 0.07011063 0.24990821], sum to 1.0000
[2019-04-09 15:08:32,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0302
[2019-04-09 15:08:32,573] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 19.0, 25.15838997617055, 0.2756162847589711, 0.0, 1.0, 45.0, 38.47449784033206], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 679800.0000, 
sim time next is 680400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 25.12078886877695, 0.2579906863086039, 0.0, 1.0, 20.0, 36.47386353269411], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.593399072398079, 0.5859968954362013, 0.0, 1.0, 0.1, 0.3647386353269411], 
reward next is 0.6353, 
noisyNet noise sample is [array([-1.9184667], dtype=float32), 0.17675476]. 
=============================================
[2019-04-09 15:08:32,636] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1792131e-04 5.5301469e-02 1.1964778e-01 2.0187333e-02 4.5524333e-03
 1.8482326e-03 1.9152276e-01 2.4760377e-02 9.7634234e-02 2.6306426e-02
 4.5782104e-01], sum to 1.0000
[2019-04-09 15:08:32,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-09 15:08:32,672] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 22.5, 26.97973885285738, 0.8037368374515431, 1.0, 1.0, 65.0, 18.74225097350519], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 745800.0000, 
sim time next is 746400.0000, 
raw observation next is [-0.2, 46.33333333333334, 84.16666666666667, 144.8333333333333, 22.5, 27.3759951425983, 0.6665815422109481, 1.0, 1.0, 25.0, 52.2741883437585], 
processed observation next is [1.0, 0.6521739130434783, 0.4570637119113574, 0.46333333333333343, 0.28055555555555556, 0.16003683241252298, 0.375, 0.7813329285498583, 0.7221938474036493, 1.0, 1.0, 0.2, 0.522741883437585], 
reward next is 0.4773, 
noisyNet noise sample is [array([0.5167442], dtype=float32), -0.05491597]. 
=============================================
[2019-04-09 15:08:32,730] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00050206 0.0930879  0.12076391 0.0167972  0.00392482 0.00103383
 0.22515848 0.01332953 0.05632197 0.02300317 0.44607714], sum to 1.0000
[2019-04-09 15:08:32,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3916
[2019-04-09 15:08:32,767] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 82.5, 372.5, 22.5, 26.79825952727388, 0.823820398110782, 1.0, 1.0, 65.0, 40.61573905802138], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 745200.0000, 
sim time next is 745800.0000, 
raw observation next is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 22.5, 26.07247070026477, 0.726148645173384, 1.0, 1.0, 45.0, 11.98355003678449], 
processed observation next is [1.0, 0.6521739130434783, 0.4598337950138504, 0.46666666666666673, 0.27777777777777773, 0.28581952117863707, 0.375, 0.6727058916887309, 0.7420495483911279, 1.0, 1.0, 0.6, 0.11983550036784489], 
reward next is 0.8802, 
noisyNet noise sample is [array([0.27704194], dtype=float32), 0.5892353]. 
=============================================
[2019-04-09 15:08:33,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00045206 0.13501108 0.17302077 0.03630583 0.00693084 0.00130019
 0.18541603 0.02882931 0.06496058 0.03831158 0.32946175], sum to 1.0000
[2019-04-09 15:08:33,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-09 15:08:33,179] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 45.0, 84.5, 743.5, 22.5, 27.4254028471953, 0.8220944302276815, 1.0, 1.0, 65.0, 31.65004854414619], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 741600.0000, 
sim time next is 742200.0000, 
raw observation next is [0.4166666666666667, 45.33333333333334, 83.0, 733.6666666666667, 22.5, 27.42735568110109, 0.8350519338561692, 1.0, 1.0, 25.0, 27.76949392702498], 
processed observation next is [1.0, 0.6086956521739131, 0.47414589104339805, 0.4533333333333334, 0.27666666666666667, 0.810681399631676, 0.375, 0.7856129734250908, 0.778350644618723, 1.0, 1.0, 0.2, 0.2776949392702498], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.2216758], dtype=float32), -0.03742463]. 
=============================================
[2019-04-09 15:08:33,457] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00219433 0.09091048 0.09314884 0.03621911 0.01007517 0.00377485
 0.24592522 0.0364408  0.05697761 0.05929613 0.3650374 ], sum to 1.0000
[2019-04-09 15:08:33,458] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1064
[2019-04-09 15:08:33,474] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 25.48195975531358, 0.4002799158006068, 0.0, 1.0, 45.0, 48.27838230467827], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 22.5, 25.48571765593488, 0.3935380345096489, 0.0, 1.0, 25.0, 40.76862875864502], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6833333333333335, 0.0, 0.0, 0.375, 0.62380980466124, 0.6311793448365496, 0.0, 1.0, 0.2, 0.4076862875864502], 
reward next is 0.5923, 
noisyNet noise sample is [array([-1.2254189], dtype=float32), -0.26005402]. 
=============================================
[2019-04-09 15:08:33,516] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00409547 0.08667089 0.15455209 0.03068935 0.01238673 0.00705313
 0.26034495 0.04944562 0.0644258  0.06033647 0.26999953], sum to 1.0000
[2019-04-09 15:08:33,517] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-09 15:08:33,547] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.466666666666667, 75.66666666666667, 0.0, 0.0, 19.0, 24.64026316165448, 0.1803108164191418, 0.0, 1.0, 50.0, 51.67536624741327], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 708000.0000, 
sim time next is 708600.0000, 
raw observation next is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 19.0, 24.68813232301137, 0.1978287597346033, 0.0, 1.0, 65.0, 55.79231238917822], 
processed observation next is [1.0, 0.17391304347826086, 0.3965835641735919, 0.7583333333333333, 0.0, 0.0, 0.08333333333333333, 0.5573443602509475, 0.5659429199115344, 0.0, 1.0, 1.0, 0.5579231238917822], 
reward next is 0.4421, 
noisyNet noise sample is [array([0.22346821], dtype=float32), 0.16632046]. 
=============================================
[2019-04-09 15:08:33,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00178002 0.0878676  0.1296168  0.03425973 0.00903821 0.00319747
 0.21826981 0.0210952  0.06269945 0.03477257 0.39740318], sum to 1.0000
[2019-04-09 15:08:33,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9170
[2019-04-09 15:08:33,579] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 72.5, 0.0, 0.0, 19.0, 25.74582147657415, 0.4724806863892624, 0.0, 1.0, 65.0, 54.20903624743278], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 786600.0000, 
sim time next is 787200.0000, 
raw observation next is [-7.799999999999999, 73.0, 0.0, 0.0, 19.0, 25.75201015861766, 0.4729813038625585, 0.0, 1.0, 65.0, 54.02590831447582], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188372, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6460008465514718, 0.6576604346208529, 0.0, 1.0, 1.0, 0.5402590831447582], 
reward next is 0.4597, 
noisyNet noise sample is [array([-0.44160464], dtype=float32), 0.067440934]. 
=============================================
[2019-04-09 15:08:33,625] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00046141 0.1037506  0.13796172 0.01766832 0.00462189 0.00197714
 0.17766812 0.03038251 0.09141076 0.03149492 0.40260264], sum to 1.0000
[2019-04-09 15:08:33,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6103
[2019-04-09 15:08:33,643] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 26.59816852729398, 0.6624053360613917, 1.0, 1.0, 65.0, 46.62667159336808], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 760800.0000, 
sim time next is 761400.0000, 
raw observation next is [-4.45, 55.5, 0.0, 0.0, 22.5, 26.56210139584442, 0.655558205807569, 1.0, 1.0, 65.0, 49.73030800066091], 
processed observation next is [1.0, 0.8260869565217391, 0.3393351800554017, 0.555, 0.0, 0.0, 0.375, 0.7135084496537015, 0.7185194019358563, 1.0, 1.0, 1.0, 0.4973030800066091], 
reward next is 0.5027, 
noisyNet noise sample is [array([-0.00571048], dtype=float32), -0.92882925]. 
=============================================
[2019-04-09 15:08:33,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00330671 0.11282704 0.14912641 0.03128378 0.01172109 0.00380452
 0.21464224 0.03136786 0.0622524  0.08135255 0.29831544], sum to 1.0000
[2019-04-09 15:08:33,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-09 15:08:33,788] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 19.33333333333334, 0.0, 22.5, 24.97609144644836, 0.2182853751677543, 1.0, 1.0, 65.0, 59.76687956799397], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 720600.0000, 
sim time next is 721200.0000, 
raw observation next is [-2.3, 76.0, 24.16666666666667, 0.0, 22.5, 24.95543791104454, 0.2476453669260185, 1.0, 1.0, 45.0, 43.7646113045209], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.08055555555555557, 0.0, 0.375, 0.5796198259203784, 0.5825484556420062, 1.0, 1.0, 0.6, 0.43764611304520895], 
reward next is 0.5624, 
noisyNet noise sample is [array([-0.20803177], dtype=float32), 0.7045975]. 
=============================================
[2019-04-09 15:08:33,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00092317 0.14193985 0.21304095 0.02141459 0.00666363 0.00207036
 0.21775907 0.02151515 0.07899173 0.05111125 0.24457023], sum to 1.0000
[2019-04-09 15:08:33,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-09 15:08:33,947] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 22.5, 26.36167944056894, 0.4986370476148687, 1.0, 1.0, 45.0, 43.18897759103416], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 728400.0000, 
sim time next is 729000.0000, 
raw observation next is [-1.15, 67.0, 139.0, 68.0, 22.5, 26.44900349472852, 0.5155938220184683, 1.0, 1.0, 45.0, 34.24927512521695], 
processed observation next is [1.0, 0.43478260869565216, 0.4307479224376732, 0.67, 0.4633333333333333, 0.07513812154696133, 0.375, 0.7040836245607099, 0.6718646073394895, 1.0, 1.0, 0.6, 0.3424927512521695], 
reward next is 0.6575, 
noisyNet noise sample is [array([-0.5413397], dtype=float32), -0.75161767]. 
=============================================
[2019-04-09 15:08:33,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[7.3307824]
 [7.045448 ]
 [7.038034 ]
 [7.173275 ]
 [7.0287633]], R is [[8.02680206]
 [8.51464462]
 [8.96486759]
 [9.42095089]
 [9.96784306]].
[2019-04-09 15:08:33,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00186158 0.13859244 0.1586224  0.0350718  0.01372819 0.00439091
 0.20155461 0.03912149 0.04640049 0.08094236 0.2797137 ], sum to 1.0000
[2019-04-09 15:08:33,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2211
[2019-04-09 15:08:33,993] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 75.0, 16.0, 0.0, 22.5, 25.00048332675956, 0.2660106489596532, 1.0, 1.0, 20.0, 37.53796264226872], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 22.5, 25.01743045018485, 0.2537630812135114, 1.0, 1.0, 25.0, 35.3219318895713], 
processed observation next is [1.0, 0.34782608695652173, 0.2793167128347184, 0.75, 0.07111111111111112, 0.0, 0.375, 0.5847858708487376, 0.5845876937378371, 1.0, 1.0, 0.2, 0.353219318895713], 
reward next is 0.6468, 
noisyNet noise sample is [array([0.16359003], dtype=float32), -2.8967786]. 
=============================================
[2019-04-09 15:08:34,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[5.664642 ]
 [5.6876826]
 [5.4764   ]
 [5.559478 ]
 [5.3448606]], R is [[6.50984955]
 [7.0693717 ]
 [7.57091188]
 [7.99196053]
 [8.36489582]].
[2019-04-09 15:08:34,197] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00311119 0.10186751 0.10996826 0.02841564 0.01550796 0.0048019
 0.2613223  0.03852876 0.06360364 0.06446265 0.30841017], sum to 1.0000
[2019-04-09 15:08:34,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7941
[2019-04-09 15:08:34,215] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 19.0, 25.02795993108469, 0.2521831659168648, 0.0, 1.0, 45.0, 50.65610959480601], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 25.06327296934181, 0.2617895049479522, 0.0, 1.0, 65.0, 55.19920846005208], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.5886060807784842, 0.587263168315984, 0.0, 1.0, 1.0, 0.5519920846005207], 
reward next is 0.4480, 
noisyNet noise sample is [array([-0.45102215], dtype=float32), -0.53372836]. 
=============================================
[2019-04-09 15:08:34,642] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00089253 0.1610717  0.12835929 0.02926466 0.00684487 0.00122571
 0.23187038 0.0221216  0.06550322 0.05965203 0.29319397], sum to 1.0000
[2019-04-09 15:08:34,645] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7835
[2019-04-09 15:08:34,658] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 50.0, 110.0, 611.0, 22.5, 27.05827664018883, 0.7287823578100473, 1.0, 1.0, 25.0, 34.33593371126466], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 738000.0000, 
sim time next is 738600.0000, 
raw observation next is [0.5, 49.16666666666667, 103.0, 665.0, 22.5, 27.06284902966274, 0.7296554512870692, 1.0, 1.0, 65.0, 39.91364050097099], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.4916666666666667, 0.3433333333333333, 0.7348066298342542, 0.375, 0.7552374191385617, 0.7432184837623564, 1.0, 1.0, 1.0, 0.3991364050097099], 
reward next is 0.6009, 
noisyNet noise sample is [array([-1.4641486], dtype=float32), 0.18746723]. 
=============================================
[2019-04-09 15:08:34,664] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00060258 0.10088035 0.14027455 0.02943802 0.00760728 0.00093427
 0.23948939 0.01867667 0.06456338 0.03823111 0.35930237], sum to 1.0000
[2019-04-09 15:08:34,666] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1858
[2019-04-09 15:08:34,696] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.9666666666666668, 66.66666666666667, 129.8333333333333, 186.5, 22.5, 26.61007288808882, 0.5578192681573101, 1.0, 1.0, 30.0, 35.76038084507353], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 729600.0000, 
sim time next is 730200.0000, 
raw observation next is [-0.7833333333333332, 66.33333333333333, 120.6666666666667, 304.9999999999999, 22.5, 26.64612750970328, 0.5734600064897433, 1.0, 1.0, 25.0, 33.49028640822575], 
processed observation next is [1.0, 0.43478260869565216, 0.44090489381348114, 0.6633333333333333, 0.4022222222222223, 0.3370165745856352, 0.375, 0.7205106258086067, 0.6911533354965811, 1.0, 1.0, 0.2, 0.3349028640822575], 
reward next is 0.6651, 
noisyNet noise sample is [array([1.0380609], dtype=float32), -1.3773009]. 
=============================================
[2019-04-09 15:08:34,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00229036 0.08574784 0.10614278 0.02922756 0.00991414 0.00485518
 0.24514087 0.04032135 0.06287654 0.05760524 0.3558782 ], sum to 1.0000
[2019-04-09 15:08:34,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9575
[2019-04-09 15:08:34,722] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00286962 0.0876622  0.14332159 0.02594193 0.00985392 0.00364873
 0.22361524 0.04061247 0.05512809 0.06254069 0.34480545], sum to 1.0000
[2019-04-09 15:08:34,723] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 74.33333333333333, 0.0, 0.0, 19.0, 25.3144696003255, 0.3759486665502785, 0.0, 1.0, 55.0, 39.28886277161527], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 792600.0000, 
sim time next is 793200.0000, 
raw observation next is [-7.300000000000001, 73.66666666666667, 0.0, 0.0, 19.0, 25.34716284311791, 0.3741791798750729, 0.0, 1.0, 65.0, 54.11363135788979], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.7366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6122635702598259, 0.624726393291691, 0.0, 1.0, 1.0, 0.5411363135788979], 
reward next is 0.4589, 
noisyNet noise sample is [array([0.03536583], dtype=float32), 1.7934352]. 
=============================================
[2019-04-09 15:08:34,726] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0641
[2019-04-09 15:08:34,737] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 25.00355815235437, 0.1841222177082463, 0.0, 1.0, 30.0, 32.68504933707045], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 715800.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 24.8285933680219, 0.1578667240180882, 0.0, 1.0, 20.0, 28.88462200020833], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.5690494473351583, 0.5526222413393628, 0.0, 1.0, 0.1, 0.2888462200020833], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.75373805], dtype=float32), 0.51633537]. 
=============================================
[2019-04-09 15:08:34,780] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00157441 0.07719702 0.09983304 0.03535772 0.00981792 0.00352352
 0.2253617  0.03779985 0.05734061 0.04860017 0.40359405], sum to 1.0000
[2019-04-09 15:08:34,800] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1567
[2019-04-09 15:08:34,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00277925 0.09738722 0.13053204 0.02808847 0.00812254 0.00310389
 0.2420097  0.03877307 0.04570736 0.06600434 0.33749205], sum to 1.0000
[2019-04-09 15:08:34,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9931
[2019-04-09 15:08:34,813] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.799999999999999, 73.0, 0.0, 0.0, 19.0, 25.63214095757799, 0.4341857181453287, 0.0, 1.0, 20.0, 39.76257050129026], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 787200.0000, 
sim time next is 787800.0000, 
raw observation next is [-7.8, 73.5, 0.0, 0.0, 19.0, 25.59415813661418, 0.4160266430628239, 0.0, 1.0, 25.0, 37.57375354558442], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.735, 0.0, 0.0, 0.08333333333333333, 0.632846511384515, 0.6386755476876079, 0.0, 1.0, 0.2, 0.37573753545584415], 
reward next is 0.6243, 
noisyNet noise sample is [array([0.18774666], dtype=float32), -1.7665776]. 
=============================================
[2019-04-09 15:08:34,821] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.716666666666667, 71.0, 0.0, 0.0, 19.0, 25.60204514235968, 0.4461157168893237, 0.0, 1.0, 45.0, 37.39734498002508], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 19.0, 25.54908099768129, 0.4448387992128146, 0.0, 1.0, 65.0, 53.55501463608486], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6290900831401075, 0.6482795997376049, 0.0, 1.0, 1.0, 0.5355501463608486], 
reward next is 0.4644, 
noisyNet noise sample is [array([0.25952253], dtype=float32), -0.30505714]. 
=============================================
[2019-04-09 15:08:35,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00069882 0.11291043 0.12953137 0.03228404 0.00752657 0.00245639
 0.22839731 0.02475291 0.08542487 0.03289267 0.34312463], sum to 1.0000
[2019-04-09 15:08:35,085] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3545
[2019-04-09 15:08:35,100] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 22.5, 26.37143016018435, 0.4931887547622594, 1.0, 1.0, 45.0, 30.73285402091001], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 727200.0000, 
sim time next is 727800.0000, 
raw observation next is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 22.5, 26.37522609101144, 0.4897019848211832, 1.0, 1.0, 45.0, 28.35527235029343], 
processed observation next is [1.0, 0.43478260869565216, 0.4205909510618652, 0.6766666666666667, 0.421111111111111, 0.06813996316758747, 0.375, 0.6979355075842868, 0.6632339949403944, 1.0, 1.0, 0.6, 0.2835527235029343], 
reward next is 0.7164, 
noisyNet noise sample is [array([-1.1662111], dtype=float32), -1.9162846]. 
=============================================
[2019-04-09 15:08:35,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00058573 0.08225523 0.16457596 0.01771973 0.01039858 0.00169759
 0.16475847 0.02223588 0.08636105 0.04511968 0.40429208], sum to 1.0000
[2019-04-09 15:08:35,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7960
[2019-04-09 15:08:35,560] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.899999999999999, 54.0, 0.0, 0.0, 22.5, 26.5236300008626, 0.664552532112252, 1.0, 1.0, 50.0, 42.63295633872158], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 758400.0000, 
sim time next is 759000.0000, 
raw observation next is [-3.9, 53.5, 0.0, 0.0, 22.5, 26.53265409827124, 0.6537847046653504, 1.0, 1.0, 65.0, 46.93168176363153], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.535, 0.0, 0.0, 0.375, 0.7110545081892701, 0.7179282348884501, 1.0, 1.0, 1.0, 0.46931681763631533], 
reward next is 0.5307, 
noisyNet noise sample is [array([-0.10558256], dtype=float32), 1.2906018]. 
=============================================
[2019-04-09 15:08:35,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[8.076028 ]
 [8.006189 ]
 [8.12542  ]
 [7.9719954]
 [8.003225 ]], R is [[ 8.50517368]
 [ 8.99379253]
 [ 9.39426517]
 [ 9.81317711]
 [10.34680271]].
[2019-04-09 15:08:35,593] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3491137e-04 6.2313516e-02 1.4423077e-01 1.5898449e-02 4.0601692e-03
 8.3769311e-04 1.3766068e-01 3.8297962e-02 6.2822253e-02 2.7115747e-02
 5.0632781e-01], sum to 1.0000
[2019-04-09 15:08:35,594] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7536
[2019-04-09 15:08:35,619] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.4, 45.66666666666667, 82.16666666666667, 26.33333333333334, 22.5, 26.9534206825149, 0.7757076887183803, 1.0, 1.0, 45.0, 23.6049891497325], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 747600.0000, 
sim time next is 748200.0000, 
raw observation next is [-0.5, 45.33333333333333, 79.33333333333333, 21.66666666666667, 22.5, 27.34999343749529, 0.8011110757278836, 1.0, 1.0, 65.0, 26.00821115963643], 
processed observation next is [1.0, 0.6521739130434783, 0.44875346260387816, 0.4533333333333333, 0.2644444444444444, 0.023941068139963172, 0.375, 0.7791661197912741, 0.7670370252426278, 1.0, 1.0, 1.0, 0.2600821115963643], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.8152921], dtype=float32), 1.8090457]. 
=============================================
[2019-04-09 15:08:35,767] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00064232 0.12234049 0.13927147 0.0386202  0.00509937 0.00132878
 0.25998247 0.0262721  0.12286056 0.03634394 0.24723831], sum to 1.0000
[2019-04-09 15:08:35,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8335
[2019-04-09 15:08:35,805] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 26.70300029374909, 0.6094395807806198, 1.0, 1.0, 45.0, 32.47443339769328], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 22.5, 25.78024334190942, 0.5775275908101096, 1.0, 1.0, 65.0, 56.4845148628839], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 0.375, 0.6483536118257849, 0.6925091969367032, 1.0, 1.0, 1.0, 0.5648451486288389], 
reward next is 0.4352, 
noisyNet noise sample is [array([-0.00901891], dtype=float32), -0.2550345]. 
=============================================
[2019-04-09 15:08:35,830] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00047658 0.11546977 0.18547416 0.04295674 0.00750715 0.00155086
 0.15732011 0.03029155 0.06557962 0.03658951 0.35678402], sum to 1.0000
[2019-04-09 15:08:35,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0797
[2019-04-09 15:08:35,853] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 22.5, 26.78676147456855, 0.6538076954904356, 1.0, 1.0, 25.0, 39.00726695457386], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 736800.0000, 
sim time next is 737400.0000, 
raw observation next is [0.3166666666666667, 51.16666666666667, 117.0, 557.0, 22.5, 26.79054419717518, 0.6724625544844863, 1.0, 1.0, 20.0, 37.77015084477454], 
processed observation next is [1.0, 0.5217391304347826, 0.47137580794090495, 0.5116666666666667, 0.39, 0.6154696132596685, 0.375, 0.7325453497645983, 0.7241541848281621, 1.0, 1.0, 0.1, 0.3777015084477454], 
reward next is 0.6223, 
noisyNet noise sample is [array([-0.24095915], dtype=float32), 2.0966783]. 
=============================================
[2019-04-09 15:08:35,892] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00285229 0.08767795 0.17384608 0.03958213 0.01163499 0.00539587
 0.25448808 0.05587432 0.06927394 0.06314426 0.2362301 ], sum to 1.0000
[2019-04-09 15:08:35,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-09 15:08:35,909] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 24.90879344915149, 0.2860046714899125, 0.0, 1.0, 45.0, 49.28775338814762], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 22.5, 24.92040019397139, 0.2794572370087128, 0.0, 1.0, 20.0, 34.00472060394138], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6833333333333335, 0.0, 0.0, 0.375, 0.5767000161642825, 0.5931524123362376, 0.0, 1.0, 0.1, 0.3400472060394138], 
reward next is 0.6600, 
noisyNet noise sample is [array([-0.534703], dtype=float32), -1.3413416]. 
=============================================
[2019-04-09 15:08:35,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00282914 0.10617693 0.17209494 0.03826702 0.01218913 0.00357903
 0.18036364 0.0381817  0.06122354 0.05262851 0.33246642], sum to 1.0000
[2019-04-09 15:08:35,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4973
[2019-04-09 15:08:35,958] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 71.0, 0.0, 0.0, 22.5, 25.24713851949027, 0.3378571901471024, 1.0, 1.0, 65.0, 60.5479816753255], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 804600.0000, 
sim time next is 805200.0000, 
raw observation next is [-6.700000000000001, 72.33333333333334, 0.0, 0.0, 22.5, 25.21038297007159, 0.3378949275999896, 1.0, 1.0, 45.0, 49.76911387846788], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.7233333333333334, 0.0, 0.0, 0.375, 0.6008652475059657, 0.6126316425333299, 1.0, 1.0, 0.6, 0.49769113878467874], 
reward next is 0.5023, 
noisyNet noise sample is [array([0.81249475], dtype=float32), -0.5878287]. 
=============================================
[2019-04-09 15:08:36,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00043431 0.09879545 0.22602004 0.03719532 0.00621856 0.00154756
 0.15791178 0.03011154 0.1372824  0.02419968 0.28028333], sum to 1.0000
[2019-04-09 15:08:36,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1642
[2019-04-09 15:08:36,366] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.2, 46.33333333333334, 84.16666666666667, 144.8333333333333, 22.5, 27.29908047798063, 0.7832118473538583, 1.0, 1.0, 45.0, 26.63508005623991], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 746400.0000, 
sim time next is 747000.0000, 
raw observation next is [-0.3, 46.0, 85.0, 31.0, 22.5, 27.38938204960257, 0.7830193971813554, 1.0, 1.0, 25.0, 30.61659398718298], 
processed observation next is [1.0, 0.6521739130434783, 0.4542936288088643, 0.46, 0.2833333333333333, 0.03425414364640884, 0.375, 0.7824485041335475, 0.7610064657271184, 1.0, 1.0, 0.2, 0.30616593987182983], 
reward next is 0.6938, 
noisyNet noise sample is [array([-0.40249422], dtype=float32), -1.0088431]. 
=============================================
[2019-04-09 15:08:36,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[9.041945]
 [8.874877]
 [9.026065]
 [9.03594 ]
 [9.258187]], R is [[ 9.60448933]
 [10.24209404]
 [10.80286312]
 [11.4996891 ]
 [11.9983139 ]].
[2019-04-09 15:08:37,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00063211 0.13896611 0.14305808 0.04503677 0.00806333 0.0019959
 0.22206792 0.02698783 0.07872937 0.07165456 0.26280802], sum to 1.0000
[2019-04-09 15:08:37,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00117202 0.07527956 0.14228164 0.02129071 0.01096085 0.00335463
 0.20526047 0.02780193 0.05948644 0.03694097 0.4161707 ], sum to 1.0000
[2019-04-09 15:08:37,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9497
[2019-04-09 15:08:37,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-09 15:08:37,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00067449 0.08127219 0.1689444  0.02485157 0.0086315  0.00141664
 0.22273727 0.02664569 0.05591293 0.05532086 0.35359243], sum to 1.0000
[2019-04-09 15:08:37,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2605
[2019-04-09 15:08:37,129] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 22.5, 26.61235106095999, 0.6340613980093192, 1.0, 1.0, 45.0, 35.79220418584698], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 22.5, 26.41853586631181, 0.6268461521010543, 1.0, 1.0, 65.0, 62.1810744980891], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 0.375, 0.7015446555259842, 0.7089487173670181, 1.0, 1.0, 1.0, 0.6218107449808911], 
reward next is 0.3782, 
noisyNet noise sample is [array([1.3151002], dtype=float32), -0.18085301]. 
=============================================
[2019-04-09 15:08:37,131] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 73.5, 0.0, 0.0, 19.0, 25.38873211541206, 0.3717826679047834, 0.0, 1.0, 45.0, 38.04768111748378], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 19.0, 25.32980845406043, 0.3821793663570268, 0.0, 1.0, 65.0, 53.47057935258919], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6108173711717025, 0.6273931221190089, 0.0, 1.0, 1.0, 0.5347057935258919], 
reward next is 0.4653, 
noisyNet noise sample is [array([0.6911447], dtype=float32), -1.9025421]. 
=============================================
[2019-04-09 15:08:37,131] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 25.60012588800412, 0.4727114589839907, 0.0, 1.0, 65.0, 53.09783617629122], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [-3.3, 82.33333333333334, 0.0, 0.0, 19.0, 25.66939545738873, 0.4714709922376105, 0.0, 1.0, 30.0, 46.78209212819979], 
processed observation next is [1.0, 0.9565217391304348, 0.37119113573407203, 0.8233333333333335, 0.0, 0.0, 0.08333333333333333, 0.6391162881157276, 0.6571569974125369, 0.0, 1.0, 0.3, 0.4678209212819979], 
reward next is 0.5322, 
noisyNet noise sample is [array([0.50862235], dtype=float32), -1.2060008]. 
=============================================
[2019-04-09 15:08:37,332] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00043897 0.1006617  0.11958248 0.02803851 0.00860628 0.00135479
 0.19220166 0.05176553 0.08865686 0.05013172 0.35856146], sum to 1.0000
[2019-04-09 15:08:37,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2668
[2019-04-09 15:08:37,348] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 61.5, 0.0, 0.0, 19.0, 25.92968059639765, 0.5427421675592606, 0.0, 1.0, 50.0, 36.90130735141602], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [-5.8, 62.0, 0.0, 0.0, 19.0, 25.87283794493943, 0.519974494355289, 0.0, 1.0, 45.0, 32.94129126734906], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6560698287449526, 0.673324831451763, 0.0, 1.0, 0.6, 0.3294129126734906], 
reward next is 0.6706, 
noisyNet noise sample is [array([-1.2590616], dtype=float32), -0.36860338]. 
=============================================
[2019-04-09 15:08:37,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[7.8752794]
 [7.911542 ]
 [8.139737 ]
 [8.235692 ]
 [8.471269 ]], R is [[ 8.58127213]
 [ 9.12644672]
 [ 9.60025883]
 [10.10576439]
 [10.62489414]].
[2019-04-09 15:08:37,539] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00053067 0.0986767  0.12098279 0.01393684 0.0082741  0.00089578
 0.24715546 0.01324911 0.05971731 0.05054026 0.38604096], sum to 1.0000
[2019-04-09 15:08:37,539] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1258
[2019-04-09 15:08:37,553] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 86.0, 14.5, 0.0, 22.5, 26.56526153150888, 0.6392920371699179, 1.0, 1.0, 65.0, 43.94090993873743], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 838800.0000, 
sim time next is 839400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 22.5, 26.8348161486869, 0.651532547306262, 1.0, 1.0, 25.0, 31.05352692799916], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.375, 0.7362346790572417, 0.717177515768754, 1.0, 1.0, 0.2, 0.31053526927999164], 
reward next is 0.6895, 
noisyNet noise sample is [array([-0.9005586], dtype=float32), -0.93778604]. 
=============================================
[2019-04-09 15:08:38,050] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00201041 0.09277919 0.1570755  0.03468821 0.0124585  0.00514916
 0.29314694 0.04370448 0.05342145 0.06967764 0.23588857], sum to 1.0000
[2019-04-09 15:08:38,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3824
[2019-04-09 15:08:38,071] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 25.46783911512207, 0.3637869945114175, 0.0, 1.0, 25.0, 36.20126253078787], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 797400.0000, 
sim time next is 798000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 25.37588940673919, 0.3418968316147379, 0.0, 1.0, 45.0, 34.45708288887638], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6146574505615993, 0.6139656105382459, 0.0, 1.0, 0.6, 0.3445708288887638], 
reward next is 0.6554, 
noisyNet noise sample is [array([1.5420213], dtype=float32), 0.27751416]. 
=============================================
[2019-04-09 15:08:38,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[5.8848805]
 [5.885817 ]
 [5.8969483]
 [5.997516 ]
 [5.941871 ]], R is [[6.49294138]
 [7.06599903]
 [7.61447001]
 [8.13318825]
 [8.62240505]].
[2019-04-09 15:08:38,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00073965 0.13613361 0.16022913 0.02804844 0.01347523 0.0024732
 0.19667543 0.02800501 0.07702123 0.02817303 0.329026  ], sum to 1.0000
[2019-04-09 15:08:38,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4442
[2019-04-09 15:08:38,437] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 74.0, 0.0, 22.5, 26.24750562644272, 0.4761455458989949, 1.0, 1.0, 65.0, 46.87295672771281], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 813600.0000, 
sim time next is 814200.0000, 
raw observation next is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 22.5, 26.31324174135495, 0.4832715556740164, 1.0, 1.0, 45.0, 40.51702815548183], 
processed observation next is [1.0, 0.43478260869565216, 0.2987072945521699, 0.7433333333333333, 0.2611111111111111, 0.0, 0.375, 0.6927701451129126, 0.6610905185580055, 1.0, 1.0, 0.6, 0.4051702815548183], 
reward next is 0.5948, 
noisyNet noise sample is [array([0.26352432], dtype=float32), 0.1804068]. 
=============================================
[2019-04-09 15:08:38,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00052644 0.10168433 0.20195909 0.02753614 0.00667922 0.00193683
 0.20797448 0.03132706 0.07477204 0.03574907 0.30985543], sum to 1.0000
[2019-04-09 15:08:38,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1009
[2019-04-09 15:08:38,807] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 22.5, 26.46091589736032, 0.6151710918572929, 1.0, 1.0, 45.0, 43.47075368048164], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 843000.0000, 
sim time next is 843600.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 22.5, 26.44197847309356, 0.4877092202704789, 1.0, 1.0, 20.0, 49.47656853902847], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.375, 0.70349820609113, 0.6625697400901597, 1.0, 1.0, 0.1, 0.4947656853902847], 
reward next is 0.5052, 
noisyNet noise sample is [array([0.55293626], dtype=float32), 1.3244563]. 
=============================================
[2019-04-09 15:08:38,831] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00138108 0.08494981 0.1616991  0.02042352 0.00966674 0.00199162
 0.26136518 0.03658016 0.06361429 0.05300639 0.30532202], sum to 1.0000
[2019-04-09 15:08:38,835] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9202
[2019-04-09 15:08:38,855] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 19.0, 25.57859304099669, 0.4323785236220869, 0.0, 1.0, 45.0, 36.09061832955761], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 25.56945767833143, 0.4497398520970493, 0.0, 1.0, 65.0, 52.20819929792243], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.6307881398609524, 0.6499132840323497, 0.0, 1.0, 1.0, 0.5220819929792243], 
reward next is 0.4779, 
noisyNet noise sample is [array([0.67332405], dtype=float32), -0.67464006]. 
=============================================
[2019-04-09 15:08:38,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0021057  0.09833623 0.14061467 0.02657565 0.00843644 0.00389188
 0.32065883 0.03144324 0.06330049 0.03213161 0.27250528], sum to 1.0000
[2019-04-09 15:08:38,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2962
[2019-04-09 15:08:38,999] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 22.5, 25.97591480297877, 0.5306535557854962, 1.0, 1.0, 45.0, 31.99714188048884], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [0.55, 76.0, 29.0, 0.0, 22.5, 26.23723695862171, 0.5457108221275627, 1.0, 1.0, 45.0, 28.22354449899822], 
processed observation next is [1.0, 0.34782608695652173, 0.4778393351800555, 0.76, 0.09666666666666666, 0.0, 0.375, 0.6864364132184759, 0.6819036073758542, 1.0, 1.0, 0.6, 0.2822354449899822], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.03370063], dtype=float32), 0.022534702]. 
=============================================
[2019-04-09 15:08:39,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00056101 0.10644917 0.13654651 0.02639837 0.00529511 0.00092993
 0.2643023  0.02038457 0.06362516 0.06262963 0.31287822], sum to 1.0000
[2019-04-09 15:08:39,024] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2275
[2019-04-09 15:08:39,043] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 77.66666666666667, 96.33333333333334, 0.0, 22.5, 26.56191817723578, 0.598493800155822, 1.0, 1.0, 45.0, 35.66761799070859], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 823800.0000, 
sim time next is 824400.0000, 
raw observation next is [-4.5, 79.0, 95.0, 0.0, 22.5, 26.58695635971711, 0.6038156276119017, 1.0, 1.0, 25.0, 36.19237392237754], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.31666666666666665, 0.0, 0.375, 0.7155796966430925, 0.7012718758706339, 1.0, 1.0, 0.2, 0.3619237392237754], 
reward next is 0.6381, 
noisyNet noise sample is [array([0.3556296], dtype=float32), -0.23986997]. 
=============================================
[2019-04-09 15:08:39,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0023593  0.10441159 0.12160066 0.02784695 0.01415359 0.00366277
 0.24622345 0.02837253 0.03633955 0.05389686 0.3611328 ], sum to 1.0000
[2019-04-09 15:08:39,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6974
[2019-04-09 15:08:39,373] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 19.0, 25.65759529984557, 0.4463095575723723, 0.0, 1.0, 60.0, 45.51040820322396], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 19.0, 25.64432174110196, 0.4748398218960944, 0.0, 1.0, 65.0, 54.25596184980459], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6370268117584968, 0.6582799406320315, 0.0, 1.0, 1.0, 0.5425596184980459], 
reward next is 0.4574, 
noisyNet noise sample is [array([0.42057496], dtype=float32), -0.61127687]. 
=============================================
[2019-04-09 15:08:39,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[5.986177 ]
 [6.1737604]
 [6.133386 ]
 [5.984303 ]
 [6.090899 ]], R is [[6.67603588]
 [7.15417194]
 [7.72128344]
 [8.27517509]
 [8.74476528]].
[2019-04-09 15:08:39,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00050422 0.07216439 0.15092559 0.02438123 0.00494217 0.00135162
 0.2981536  0.04657384 0.03839844 0.04899901 0.3136059 ], sum to 1.0000
[2019-04-09 15:08:39,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0420
[2019-04-09 15:08:39,737] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 77.66666666666667, 96.33333333333334, 0.0, 22.5, 26.18367646153901, 0.5387728360993128, 1.0, 1.0, 20.0, 32.25745456942577], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 823800.0000, 
sim time next is 824400.0000, 
raw observation next is [-4.5, 79.0, 95.0, 0.0, 22.5, 26.29414280466183, 0.5491246750046063, 1.0, 1.0, 20.0, 33.09375575719571], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.31666666666666665, 0.0, 0.375, 0.6911785670551526, 0.6830415583348688, 1.0, 1.0, 0.1, 0.3309375575719571], 
reward next is 0.6691, 
noisyNet noise sample is [array([1.2384381], dtype=float32), -0.5777862]. 
=============================================
[2019-04-09 15:08:39,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00198689 0.09621676 0.17839935 0.04014233 0.01072081 0.00302067
 0.19012108 0.03060496 0.06623819 0.07261105 0.30993792], sum to 1.0000
[2019-04-09 15:08:39,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4236
[2019-04-09 15:08:39,772] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 74.0, 0.0, 0.0, 19.0, 25.95020610675208, 0.5070610374543748, 0.0, 1.0, 55.0, 47.29386827418358], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 19.0, 25.92937781875964, 0.4986532309870573, 0.0, 1.0, 45.0, 36.37803584565899], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.6607814848966367, 0.6662177436623524, 0.0, 1.0, 0.6, 0.3637803584565899], 
reward next is 0.6362, 
noisyNet noise sample is [array([-0.89499885], dtype=float32), 0.22109433]. 
=============================================
[2019-04-09 15:08:40,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00042677 0.07927956 0.17622596 0.01807148 0.0038335  0.00134419
 0.21576916 0.01840662 0.03486919 0.04581783 0.40595576], sum to 1.0000
[2019-04-09 15:08:40,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0376
[2019-04-09 15:08:40,144] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 22.5, 26.11493225549895, 0.5780322978919346, 1.0, 1.0, 25.0, 38.15539401554521], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 836400.0000, 
sim time next is 837000.0000, 
raw observation next is [-3.9, 84.0, 29.0, 0.0, 22.5, 25.65597306979837, 0.5343355869116662, 1.0, 1.0, 20.0, 33.64441925442516], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.84, 0.09666666666666666, 0.0, 0.375, 0.637997755816531, 0.6781118623038888, 1.0, 1.0, 0.1, 0.3364441925442516], 
reward next is 0.6636, 
noisyNet noise sample is [array([0.5278027], dtype=float32), 1.1303018]. 
=============================================
[2019-04-09 15:08:40,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[8.664981 ]
 [8.591184 ]
 [8.710179 ]
 [8.8705435]
 [8.627673 ]], R is [[ 9.07357883]
 [ 9.60128975]
 [10.05072117]
 [10.63027477]
 [11.17780209]].
[2019-04-09 15:08:40,224] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.64562201e-04 8.87992457e-02 3.65146905e-01 2.96949111e-02
 4.41715075e-03 7.82956777e-04 2.14820385e-01 1.44660035e-02
 4.91912365e-02 3.46519612e-02 1.97764739e-01], sum to 1.0000
[2019-04-09 15:08:40,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1704
[2019-04-09 15:08:40,255] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 22.5, 26.21078497608387, 0.5586344208848771, 1.0, 1.0, 45.0, 44.90751079451624], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 844200.0000, 
sim time next is 844800.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 22.5, 26.05063986135233, 0.5424180404386266, 1.0, 1.0, 25.0, 39.87047694236098], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.375, 0.6708866551126942, 0.6808060134795423, 1.0, 1.0, 0.2, 0.39870476942360983], 
reward next is 0.6013, 
noisyNet noise sample is [array([0.48077643], dtype=float32), -0.52594775]. 
=============================================
[2019-04-09 15:08:40,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00202948 0.10319381 0.15661684 0.02979233 0.01217498 0.00327951
 0.19929852 0.02290625 0.0763187  0.04698611 0.34740344], sum to 1.0000
[2019-04-09 15:08:40,287] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6911
[2019-04-09 15:08:40,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 25.93825900332476, 0.5044977948455182, 0.0, 1.0, 65.0, 49.75684429279715], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 887400.0000, 
sim time next is 888000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 25.98343905241197, 0.5186779675764537, 0.0, 1.0, 65.0, 48.82552780590976], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6652865877009976, 0.672892655858818, 0.0, 1.0, 1.0, 0.4882552780590976], 
reward next is 0.5117, 
noisyNet noise sample is [array([-1.4416411], dtype=float32), 0.92656726]. 
=============================================
[2019-04-09 15:08:40,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[6.162329 ]
 [6.339171 ]
 [6.0519834]
 [6.238435 ]
 [6.221231 ]], R is [[6.60195351]
 [7.03836536]
 [7.5323019 ]
 [7.95925379]
 [8.38521481]].
[2019-04-09 15:08:40,619] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00051307 0.13618511 0.16458902 0.02835006 0.00620707 0.00113044
 0.15418601 0.0171793  0.05433015 0.04920042 0.38812932], sum to 1.0000
[2019-04-09 15:08:40,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-09 15:08:40,649] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 22.5, 25.87494569768723, 0.5176973785319728, 1.0, 1.0, 65.0, 51.20421991098478], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 845400.0000, 
sim time next is 846000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 22.5, 25.84892673099243, 0.5204830068400979, 1.0, 1.0, 55.0, 48.48300810567233], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.86, 0.0, 0.0, 0.375, 0.6540772275827026, 0.6734943356133659, 1.0, 1.0, 0.8, 0.4848300810567233], 
reward next is 0.5152, 
noisyNet noise sample is [array([0.03303738], dtype=float32), -2.0567503]. 
=============================================
[2019-04-09 15:08:40,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[7.9468575]
 [8.275445 ]
 [8.213221 ]
 [8.249132 ]
 [8.084399 ]], R is [[ 8.62313747]
 [ 9.0248642 ]
 [ 9.50836468]
 [ 9.9730711 ]
 [10.26958656]].
[2019-04-09 15:08:40,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3684947e-04 6.6060632e-02 7.2137073e-02 9.5760049e-03 2.9094494e-03
 5.2193191e-04 2.8028151e-01 8.1225242e-03 4.5227602e-02 2.7094947e-02
 4.8793143e-01], sum to 1.0000
[2019-04-09 15:08:40,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3847
[2019-04-09 15:08:40,792] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 100.0, 0.0, 0.0, 22.5, 26.69122014292571, 0.7771923188210138, 1.0, 1.0, 45.0, 35.63111548937441], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 935400.0000, 
sim time next is 936000.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 22.5, 26.67228309214042, 0.7770046967732961, 1.0, 1.0, 45.0, 36.60258105947382], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.375, 0.7226902576783685, 0.7590015655910988, 1.0, 1.0, 0.6, 0.3660258105947382], 
reward next is 0.6340, 
noisyNet noise sample is [array([-0.8002126], dtype=float32), 2.378306]. 
=============================================
[2019-04-09 15:08:40,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[8.655625]
 [8.699382]
 [8.483148]
 [8.824267]
 [8.570083]], R is [[ 9.21535206]
 [ 9.76688766]
 [10.24263   ]
 [10.71081924]
 [11.26493835]].
[2019-04-09 15:08:40,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00043923 0.06154639 0.0914562  0.01864806 0.00356869 0.00116301
 0.30380496 0.0299379  0.04249316 0.02949259 0.4174497 ], sum to 1.0000
[2019-04-09 15:08:40,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7715
[2019-04-09 15:08:40,922] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 25.66157267929443, 0.4918326640145771, 0.0, 1.0, 65.0, 52.90030572976535], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [-3.3, 82.33333333333334, 0.0, 0.0, 19.0, 25.70378809735209, 0.4922682913349335, 0.0, 1.0, 65.0, 53.78930356638993], 
processed observation next is [1.0, 0.9565217391304348, 0.37119113573407203, 0.8233333333333335, 0.0, 0.0, 0.08333333333333333, 0.6419823414460074, 0.6640894304449778, 0.0, 1.0, 1.0, 0.5378930356638993], 
reward next is 0.4621, 
noisyNet noise sample is [array([-0.10395698], dtype=float32), -0.15838592]. 
=============================================
[2019-04-09 15:08:41,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00131029 0.09285446 0.11636268 0.04454297 0.00699776 0.00146495
 0.27711016 0.02539871 0.04824872 0.03137459 0.35433465], sum to 1.0000
[2019-04-09 15:08:41,242] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1364
[2019-04-09 15:08:41,261] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 22.5, 25.93190497575478, 0.5182891244928044, 1.0, 1.0, 45.0, 35.63448818060822], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [0.55, 76.0, 29.0, 0.0, 22.5, 26.23138376566556, 0.537577132835008, 1.0, 1.0, 25.0, 37.11872316979517], 
processed observation next is [1.0, 0.34782608695652173, 0.4778393351800555, 0.76, 0.09666666666666666, 0.0, 0.375, 0.6859486471387966, 0.6791923776116694, 1.0, 1.0, 0.2, 0.37118723169795165], 
reward next is 0.6288, 
noisyNet noise sample is [array([0.5601308], dtype=float32), -1.1603714]. 
=============================================
[2019-04-09 15:08:41,332] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00185094 0.07735261 0.13222209 0.02522402 0.00999528 0.00255858
 0.3170919  0.02079782 0.03999217 0.03937602 0.3335385 ], sum to 1.0000
[2019-04-09 15:08:41,333] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8368
[2019-04-09 15:08:41,356] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.9, 79.33333333333334, 0.0, 0.0, 19.0, 25.87106111881829, 0.4824071507824337, 0.0, 1.0, 45.0, 39.09659960506558], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 870000.0000, 
sim time next is 870600.0000, 
raw observation next is [-1.8, 79.16666666666667, 0.0, 0.0, 19.0, 25.81405879481943, 0.4762269292441655, 0.0, 1.0, 65.0, 51.63860722481], 
processed observation next is [1.0, 0.043478260869565216, 0.41274238227146814, 0.7916666666666667, 0.0, 0.0, 0.08333333333333333, 0.6511715662349525, 0.6587423097480551, 0.0, 1.0, 1.0, 0.5163860722481001], 
reward next is 0.4836, 
noisyNet noise sample is [array([-0.34286028], dtype=float32), -0.26950145]. 
=============================================
[2019-04-09 15:08:41,415] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00075148 0.06624055 0.15110229 0.01765259 0.00516902 0.00202899
 0.26681104 0.02947155 0.05161004 0.06136971 0.34779263], sum to 1.0000
[2019-04-09 15:08:41,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2328
[2019-04-09 15:08:41,433] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 19.0, 25.72559576086753, 0.4762528472260893, 0.0, 1.0, 25.0, 34.21085581454009], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 25.74570940449909, 0.4593348377344862, 0.0, 1.0, 30.0, 33.25050961147183], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.6454757837082576, 0.6531116125781621, 0.0, 1.0, 0.3, 0.3325050961147183], 
reward next is 0.6675, 
noisyNet noise sample is [array([0.3803582], dtype=float32), 0.9297043]. 
=============================================
[2019-04-09 15:08:41,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[6.787657 ]
 [6.895872 ]
 [6.990732 ]
 [7.095815 ]
 [7.0981503]], R is [[7.31351423]
 [7.89827061]
 [8.41644573]
 [8.87356377]
 [9.41043758]].
[2019-04-09 15:08:41,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00033806 0.08942457 0.17116368 0.01749915 0.00379316 0.00122777
 0.24786386 0.01826791 0.04752172 0.0673284  0.3355718 ], sum to 1.0000
[2019-04-09 15:08:41,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4775
[2019-04-09 15:08:41,517] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 84.5, 0.0, 0.0, 22.5, 24.5541284982618, 0.2994642567208526, 1.0, 1.0, 20.0, 47.64831306539296], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 847800.0000, 
sim time next is 848400.0000, 
raw observation next is [-3.566666666666666, 84.0, 0.0, 0.0, 22.5, 24.62128613228932, 0.2981892378747402, 1.0, 1.0, 45.0, 34.90901548423184], 
processed observation next is [1.0, 0.8260869565217391, 0.3638042474607572, 0.84, 0.0, 0.0, 0.375, 0.5517738443574434, 0.5993964126249134, 1.0, 1.0, 0.6, 0.34909015484231837], 
reward next is 0.6509, 
noisyNet noise sample is [array([-0.10403533], dtype=float32), 0.06384329]. 
=============================================
[2019-04-09 15:08:41,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00084468 0.0486597  0.14395095 0.02183745 0.00959334 0.00230874
 0.19418763 0.02454918 0.05983143 0.0640019  0.43023497], sum to 1.0000
[2019-04-09 15:08:41,595] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7311
[2019-04-09 15:08:41,610] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 19.0, 26.35459329752469, 0.727665195997126, 0.0, 1.0, 45.0, 45.59819216460877], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 19.0, 26.41167354053391, 0.7406237432489737, 0.0, 1.0, 65.0, 46.62636807797372], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 0.08333333333333333, 0.7009727950444926, 0.7468745810829912, 0.0, 1.0, 1.0, 0.46626368077973723], 
reward next is 0.5337, 
noisyNet noise sample is [array([0.22600219], dtype=float32), -0.37442112]. 
=============================================
[2019-04-09 15:08:41,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[7.2225113]
 [7.3596363]
 [7.2745724]
 [7.520772 ]
 [7.622119 ]], R is [[7.82394981]
 [8.28972816]
 [8.63899708]
 [9.17269516]
 [9.52971268]].
[2019-04-09 15:08:41,921] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-09 15:08:41,922] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:08:41,923] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:08:41,923] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:08:41,923] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:08:41,923] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:08:41,924] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:08:41,930] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run17
[2019-04-09 15:08:41,952] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run17
[2019-04-09 15:08:41,964] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run17
[2019-04-09 15:09:45,423] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02573408], dtype=float32), 0.030564053]
[2019-04-09 15:09:45,423] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-5.629218322666667, 81.89840846000001, 151.4045546733333, 233.143405755, 22.5, 25.28490040172193, 0.3439017098114028, 1.0, 1.0, 30.0, 40.16217185874177]
[2019-04-09 15:09:45,423] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:09:45,423] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00308336 0.14348502 0.14251302 0.05743583 0.01890882 0.00553479
 0.2027783  0.036727   0.08454196 0.06472833 0.24026358], sampled 0.863447428084791
[2019-04-09 15:10:01,811] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02573408], dtype=float32), 0.030564053]
[2019-04-09 15:10:01,811] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-7.587963344666667, 71.02796492333334, 0.0, 0.0, 19.0, 25.42111977971666, 0.4086141194405542, 0.0, 1.0, 65.0, 53.63738918359224]
[2019-04-09 15:10:01,811] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:10:01,812] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00403716 0.10372085 0.14427236 0.04408831 0.01973045 0.00675475
 0.23947471 0.03558688 0.06273874 0.06305766 0.27653813], sampled 0.49157052162391823
[2019-04-09 15:10:04,785] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02573408], dtype=float32), 0.030564053]
[2019-04-09 15:10:04,785] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [4.716666666666667, 68.0, 100.3333333333333, 295.0, 22.5, 29.21729171318441, 1.406120088483071, 1.0, 1.0, 45.0, 4.918810616499265]
[2019-04-09 15:10:04,786] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:10:04,786] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00046317 0.1347905  0.15412547 0.02858275 0.00743355 0.00134889
 0.2172297  0.01976269 0.06844695 0.04228131 0.32553503], sampled 0.6493590165708641
[2019-04-09 15:10:26,391] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02573408], dtype=float32), 0.030564053]
[2019-04-09 15:10:26,391] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [3.816666666666666, 35.33333333333334, 68.0, 491.6666666666666, 19.0, 28.074599764296, 1.148865796612659, 0.0, 1.0, 65.0, 22.61492440592576]
[2019-04-09 15:10:26,391] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:10:26,392] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00224647 0.09358768 0.16376734 0.03156067 0.01372691 0.00455828
 0.21352844 0.03280418 0.06315631 0.05708893 0.3239748 ], sampled 0.6776275191184966
[2019-04-09 15:10:29,152] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5646.7762 284944.9191 2922.9184
[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,172] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:29,280] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,002] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5299.7502 319641.3193 2179.4391
[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,022] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,138] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:35,886] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5385.9424 311005.7613 2559.3831
[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:35,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,013] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:10:36,909] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 160000, evaluation results [160000.0, 5385.942386652585, 311005.7613347398, 2559.3831010650665, 5646.776151045014, 284944.9191495295, 2922.9184319584215, 5299.750211855559, 319641.3193198704, 2179.4390787317084]
[2019-04-09 15:10:37,028] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4353375e-04 8.5060492e-02 1.4443526e-01 1.9666629e-02 2.3804924e-03
 7.0168899e-04 2.6813757e-01 1.1544145e-02 3.6938082e-02 2.3921100e-02
 4.0697104e-01], sum to 1.0000
[2019-04-09 15:10:37,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9659
[2019-04-09 15:10:37,067] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.5, 92.83333333333333, 30.0, 0.0, 22.5, 27.5373495253878, 0.8718587212304008, 1.0, 1.0, 65.0, 23.41309193367524], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 922200.0000, 
sim time next is 922800.0000, 
raw observation next is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 22.5, 27.61828716639344, 0.882778956862576, 1.0, 1.0, 65.0, 28.96267338173585], 
processed observation next is [1.0, 0.6956521739130435, 0.5900277008310251, 0.9266666666666667, 0.08, 0.0, 0.375, 0.8015239305327867, 0.7942596522875253, 1.0, 1.0, 1.0, 0.2896267338173585], 
reward next is 0.7104, 
noisyNet noise sample is [array([0.7743038], dtype=float32), 0.724896]. 
=============================================
[2019-04-09 15:10:37,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00246051 0.05900187 0.21818803 0.02376657 0.00566322 0.0025967
 0.24127245 0.04761694 0.04991625 0.06474431 0.28477305], sum to 1.0000
[2019-04-09 15:10:37,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4879
[2019-04-09 15:10:37,096] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 25.76793834884402, 0.4436314865106319, 0.0, 1.0, 45.0, 33.92526673311574], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 873600.0000, 
sim time next is 874200.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 25.65248427714172, 0.4275529596870277, 0.0, 1.0, 45.0, 32.24942840952048], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6377070230951434, 0.6425176532290092, 0.0, 1.0, 0.6, 0.3224942840952048], 
reward next is 0.6775, 
noisyNet noise sample is [array([0.10161076], dtype=float32), -1.8289536]. 
=============================================
[2019-04-09 15:10:37,187] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1769706e-04 7.3804863e-02 1.6473515e-01 2.0060431e-02 4.2707315e-03
 9.4854058e-04 2.1898818e-01 1.9788103e-02 7.4584305e-02 3.0090543e-02
 3.9241147e-01], sum to 1.0000
[2019-04-09 15:10:37,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-09 15:10:37,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.8, 93.0, 94.0, 0.0, 22.5, 27.35426460138564, 0.7996434415669268, 1.0, 1.0, 45.0, 25.49867530037248], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 913800.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 22.5, 27.39025188012001, 0.8054379176773057, 1.0, 1.0, 25.0, 25.58704362304713], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 0.375, 0.782520990010001, 0.7684793058924352, 1.0, 1.0, 0.2, 0.2558704362304713], 
reward next is 0.7441, 
noisyNet noise sample is [array([0.794862], dtype=float32), 1.3015054]. 
=============================================
[2019-04-09 15:10:37,383] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00152282 0.07181995 0.11085866 0.01429574 0.01044401 0.00307128
 0.32315844 0.03061514 0.04595052 0.06588951 0.32237384], sum to 1.0000
[2019-04-09 15:10:37,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-09 15:10:37,406] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 19.0, 25.58119510916998, 0.4132777934509033, 0.0, 1.0, 25.0, 31.33963650908392], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 868800.0000, 
sim time next is 869400.0000, 
raw observation next is [-2.0, 79.5, 0.0, 0.0, 19.0, 25.49423792048525, 0.4023511138774345, 0.0, 1.0, 25.0, 29.82891566615568], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.795, 0.0, 0.0, 0.08333333333333333, 0.6245198267071043, 0.6341170379591449, 0.0, 1.0, 0.2, 0.2982891566615568], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.2116451], dtype=float32), 0.071393706]. 
=============================================
[2019-04-09 15:10:37,515] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3016308e-04 1.1953175e-01 1.1559195e-01 1.4108581e-02 5.8808196e-03
 1.0704361e-03 1.8298887e-01 2.4265833e-02 4.8000660e-02 7.8816622e-02
 4.0941432e-01], sum to 1.0000
[2019-04-09 15:10:37,519] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-09 15:10:37,536] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.4, 99.33333333333334, 0.0, 0.0, 22.5, 26.83758238429857, 0.8027869247555616, 1.0, 1.0, 20.0, 33.47195559150734], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 931800.0000, 
sim time next is 932400.0000, 
raw observation next is [4.4, 100.0, 0.0, 0.0, 22.5, 26.83893372571415, 0.8055884737673434, 1.0, 1.0, 65.0, 53.08045023849716], 
processed observation next is [1.0, 0.8260869565217391, 0.5844875346260389, 1.0, 0.0, 0.0, 0.375, 0.7365778104761791, 0.7685294912557811, 1.0, 1.0, 1.0, 0.5308045023849716], 
reward next is 0.4692, 
noisyNet noise sample is [array([0.3904641], dtype=float32), -0.54938483]. 
=============================================
[2019-04-09 15:10:37,810] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00063613 0.05009331 0.14214534 0.01246521 0.00384456 0.0013985
 0.28053734 0.02667163 0.05890198 0.04128448 0.38202155], sum to 1.0000
[2019-04-09 15:10:37,811] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5096
[2019-04-09 15:10:37,833] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 26.61738524339353, 0.7659134679723824, 0.0, 1.0, 45.0, 37.82583651462592], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 945600.0000, 
sim time next is 946200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 26.63811468131479, 0.765790334788582, 0.0, 1.0, 50.0, 34.87379537535018], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7198428901095658, 0.7552634449295273, 0.0, 1.0, 0.7, 0.34873795375350175], 
reward next is 0.6513, 
noisyNet noise sample is [array([1.7116584], dtype=float32), 1.6095763]. 
=============================================
[2019-04-09 15:10:38,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00116892 0.13089836 0.148343   0.03711495 0.00842521 0.00246753
 0.19307148 0.02640147 0.08467911 0.05253943 0.31489047], sum to 1.0000
[2019-04-09 15:10:38,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9341
[2019-04-09 15:10:38,176] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 22.5, 27.033031759721, 0.8352571526478436, 1.0, 1.0, 20.0, 31.93608421661954], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 978600.0000, 
sim time next is 979200.0000, 
raw observation next is [9.4, 93.0, 13.5, 0.0, 22.5, 27.04941579654773, 0.8407030823418477, 1.0, 1.0, 65.0, 50.71492549977946], 
processed observation next is [1.0, 0.34782608695652173, 0.7229916897506927, 0.93, 0.045, 0.0, 0.375, 0.7541179830456443, 0.7802343607806158, 1.0, 1.0, 1.0, 0.5071492549977946], 
reward next is 0.4929, 
noisyNet noise sample is [array([1.6784803], dtype=float32), 0.6346318]. 
=============================================
[2019-04-09 15:10:38,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00093307 0.11340097 0.20495975 0.03469041 0.00648476 0.00138968
 0.20698637 0.02110502 0.0397159  0.03589959 0.33443454], sum to 1.0000
[2019-04-09 15:10:38,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7418
[2019-04-09 15:10:38,330] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 84.0, 72.16666666666667, 0.0, 22.5, 26.97016038742174, 0.6711064100119009, 1.0, 1.0, 45.0, 30.00447336436471], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [1.1, 84.0, 77.0, 0.0, 22.5, 27.01308045177869, 0.6709748026571533, 1.0, 1.0, 65.0, 41.3247665094253], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.25666666666666665, 0.0, 0.375, 0.7510900376482242, 0.7236582675523845, 1.0, 1.0, 1.0, 0.41324766509425304], 
reward next is 0.5868, 
noisyNet noise sample is [array([-0.8582339], dtype=float32), 0.8161559]. 
=============================================
[2019-04-09 15:10:38,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00058427 0.10471078 0.12463807 0.04973976 0.00811668 0.00129689
 0.27341986 0.0268421  0.06647556 0.03816872 0.30600727], sum to 1.0000
[2019-04-09 15:10:38,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-09 15:10:38,374] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 84.0, 77.0, 0.0, 22.5, 26.96621684298214, 0.6676217126466382, 1.0, 1.0, 25.0, 27.34990587956998], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 901800.0000, 
sim time next is 902400.0000, 
raw observation next is [1.1, 84.0, 80.33333333333334, 0.0, 22.5, 26.94877353160524, 0.6800621324460466, 1.0, 1.0, 20.0, 33.29538842831413], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.26777777777777784, 0.0, 0.375, 0.74573112763377, 0.7266873774820155, 1.0, 1.0, 0.1, 0.3329538842831413], 
reward next is 0.6670, 
noisyNet noise sample is [array([0.6160662], dtype=float32), -0.042933743]. 
=============================================
[2019-04-09 15:10:38,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00070237 0.09268072 0.11242475 0.02416529 0.00596616 0.00097954
 0.28689793 0.03189135 0.05639193 0.04863731 0.33926257], sum to 1.0000
[2019-04-09 15:10:38,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5154
[2019-04-09 15:10:38,421] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.16666666666667, 92.33333333333334, 54.5, 0.0, 22.5, 27.70895796828158, 0.956420622184036, 1.0, 1.0, 45.0, 23.8028007446724], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 984000.0000, 
sim time next is 984600.0000, 
raw observation next is [10.25, 92.5, 60.0, 0.0, 22.5, 27.79852749187326, 0.9685627809227796, 1.0, 1.0, 45.0, 19.5857634915705], 
processed observation next is [1.0, 0.391304347826087, 0.7465373961218837, 0.925, 0.2, 0.0, 0.375, 0.8165439576561049, 0.8228542603075932, 1.0, 1.0, 0.6, 0.195857634915705], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.2932584], dtype=float32), -0.4060539]. 
=============================================
[2019-04-09 15:10:38,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0014322  0.09546086 0.15718429 0.0274421  0.00798613 0.00218177
 0.27270305 0.03956042 0.05073767 0.05155786 0.29375365], sum to 1.0000
[2019-04-09 15:10:38,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4175
[2019-04-09 15:10:38,435] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.883333333333334, 83.0, 0.0, 0.0, 19.0, 26.94664556185909, 0.8016195092172241, 0.0, 1.0, 65.0, 39.75257895590641], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 965400.0000, 
sim time next is 966000.0000, 
raw observation next is [8.066666666666666, 83.0, 0.0, 0.0, 19.0, 26.89516141871008, 0.8008466420232069, 0.0, 1.0, 45.0, 39.66672992185351], 
processed observation next is [1.0, 0.17391304347826086, 0.6860572483841183, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7412634515591735, 0.7669488806744024, 0.0, 1.0, 0.6, 0.3966672992185351], 
reward next is 0.6033, 
noisyNet noise sample is [array([1.8011556], dtype=float32), -1.8106724]. 
=============================================
[2019-04-09 15:10:38,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[7.0473404]
 [6.732716 ]
 [6.8463163]
 [6.928891 ]
 [7.020662 ]], R is [[7.44075918]
 [7.96882582]
 [8.58993816]
 [9.17293167]
 [9.73952866]].
[2019-04-09 15:10:38,613] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00068956 0.06310344 0.17083001 0.02558658 0.00627884 0.00199895
 0.26178375 0.02174023 0.03702719 0.06636725 0.34459412], sum to 1.0000
[2019-04-09 15:10:38,619] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1876
[2019-04-09 15:10:38,635] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 19.0, 26.68737341873106, 0.7788617207839925, 0.0, 1.0, 45.0, 34.45107093926538], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 19.0, 26.71187277325271, 0.7797386154568224, 0.0, 1.0, 25.0, 34.76645094674868], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 0.08333333333333333, 0.7259893977710593, 0.7599128718189408, 0.0, 1.0, 0.2, 0.3476645094674868], 
reward next is 0.6523, 
noisyNet noise sample is [array([0.48303443], dtype=float32), -0.35844645]. 
=============================================
[2019-04-09 15:10:38,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00065123 0.12891147 0.14480338 0.03628307 0.00872739 0.00229337
 0.2574013  0.03418487 0.07526208 0.0457132  0.26576868], sum to 1.0000
[2019-04-09 15:10:38,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8007
[2019-04-09 15:10:38,859] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 22.5, 26.80481293967502, 0.6261617169326154, 1.0, 1.0, 45.0, 27.82298316306579], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 898800.0000, 
sim time next is 899400.0000, 
raw observation next is [1.1, 83.33333333333333, 57.66666666666666, 0.0, 22.5, 26.79532309349313, 0.6395311269894085, 1.0, 1.0, 65.0, 54.78758179647534], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8333333333333333, 0.19222222222222218, 0.0, 0.375, 0.7329435911244276, 0.7131770423298028, 1.0, 1.0, 1.0, 0.5478758179647534], 
reward next is 0.4521, 
noisyNet noise sample is [array([0.20912078], dtype=float32), 0.01121566]. 
=============================================
[2019-04-09 15:10:39,653] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00053312 0.07642002 0.167839   0.01698233 0.00635554 0.00099086
 0.22303577 0.02079342 0.08878227 0.03257273 0.36569494], sum to 1.0000
[2019-04-09 15:10:39,653] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3183
[2019-04-09 15:10:39,667] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 93.0, 78.0, 0.0, 22.5, 27.92231805150832, 1.003568115347703, 1.0, 1.0, 45.0, 19.78012511196519], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 986400.0000, 
sim time next is 987000.0000, 
raw observation next is [10.68333333333333, 91.83333333333333, 84.0, 0.0, 22.5, 27.97918410161238, 1.014115767438019, 1.0, 1.0, 45.0, 18.21778799494439], 
processed observation next is [1.0, 0.43478260869565216, 0.7585410895660203, 0.9183333333333333, 0.28, 0.0, 0.375, 0.8315986751343649, 0.8380385891460064, 1.0, 1.0, 0.6, 0.1821778799494439], 
reward next is 0.8178, 
noisyNet noise sample is [array([-0.77716875], dtype=float32), 0.994492]. 
=============================================
[2019-04-09 15:10:39,675] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00041032 0.12193922 0.1486592  0.03050708 0.00477288 0.00130115
 0.22680475 0.01494128 0.07772388 0.03969242 0.3332478 ], sum to 1.0000
[2019-04-09 15:10:39,675] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-09 15:10:39,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[7.9973774]
 [7.8228383]
 [8.051977 ]
 [7.7849708]
 [7.737569 ]], R is [[ 8.93389606]
 [ 9.64675522]
 [10.33298492]
 [11.0394516 ]
 [11.73164654]].
[2019-04-09 15:10:39,688] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 22.5, 27.87836429967319, 0.9736349820831328, 1.0, 1.0, 65.0, 22.54693990178323], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 22.5, 27.89098666046807, 0.9852263407963858, 1.0, 1.0, 55.0, 25.77611341896804], 
processed observation next is [1.0, 0.391304347826087, 0.7511542012927056, 0.9283333333333332, 0.24, 0.0, 0.375, 0.8242488883723391, 0.8284087802654619, 1.0, 1.0, 0.8, 0.2577611341896804], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.4112045], dtype=float32), -1.1354758]. 
=============================================
[2019-04-09 15:10:39,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0119144e-04 4.8322637e-02 1.4791290e-01 7.6609156e-03 3.7471515e-03
 6.1197439e-04 1.2584689e-01 1.8338710e-02 4.4676736e-02 3.4867890e-02
 5.6771290e-01], sum to 1.0000
[2019-04-09 15:10:39,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8489
[2019-04-09 15:10:39,785] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 98.66666666666667, 0.0, 0.0, 19.0, 26.62262232634706, 0.777543197832491, 0.0, 1.0, 60.0, 43.78699514367398], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 940800.0000, 
sim time next is 941400.0000, 
raw observation next is [5.0, 98.0, 0.0, 0.0, 19.0, 26.63547178798633, 0.7785491879441827, 0.0, 1.0, 65.0, 43.20995711653369], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.98, 0.0, 0.0, 0.08333333333333333, 0.7196226489988607, 0.7595163959813943, 0.0, 1.0, 1.0, 0.43209957116533687], 
reward next is 0.5679, 
noisyNet noise sample is [array([0.23659092], dtype=float32), -1.7005309]. 
=============================================
[2019-04-09 15:10:40,192] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.45127773e-04 1.18405014e-01 1.06672302e-01 1.84970256e-02
 5.43497130e-03 9.90662840e-04 2.23999292e-01 2.31691711e-02
 5.45186885e-02 3.29949707e-02 4.14972752e-01], sum to 1.0000
[2019-04-09 15:10:40,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1833
[2019-04-09 15:10:40,206] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.1, 93.0, 90.0, 0.0, 22.5, 27.45242570728928, 0.8330650957385598, 1.0, 1.0, 20.0, 25.41158124507749], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 916200.0000, 
sim time next is 916800.0000, 
raw observation next is [4.2, 93.0, 84.0, 0.0, 22.5, 27.48453713447161, 0.8324156310937344, 1.0, 1.0, 25.0, 24.80910976993744], 
processed observation next is [1.0, 0.6086956521739131, 0.5789473684210527, 0.93, 0.28, 0.0, 0.375, 0.7903780945393007, 0.7774718770312448, 1.0, 1.0, 0.2, 0.2480910976993744], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.37180096], dtype=float32), 1.5069029]. 
=============================================
[2019-04-09 15:10:40,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.18510536e-04 2.88697518e-02 1.64662406e-01 1.17189018e-02
 4.84766625e-03 3.06180096e-04 3.79796028e-01 1.21624051e-02
 4.60184179e-02 1.79077983e-02 3.33591908e-01], sum to 1.0000
[2019-04-09 15:10:40,270] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6896
[2019-04-09 15:10:40,300] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.62657068999572, 1.051283534984919, 0.0, 1.0, 45.0, 25.26818833499343], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1024200.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.61559835658204, 1.051202100851378, 0.0, 1.0, 65.0, 29.10385609954319], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8012998630485034, 0.8504007002837927, 0.0, 1.0, 1.0, 0.2910385609954319], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.03312564], dtype=float32), 0.58523226]. 
=============================================
[2019-04-09 15:10:40,370] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8016273e-04 7.5902089e-02 1.0903032e-01 1.3815363e-02 2.2760490e-03
 6.4785412e-04 2.9306325e-01 8.1167221e-03 4.2345259e-02 2.8736856e-02
 4.2578602e-01], sum to 1.0000
[2019-04-09 15:10:40,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8319
[2019-04-09 15:10:40,380] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 19.0, 27.62423942535763, 1.07315606030074, 0.0, 1.0, 65.0, 28.416148475449], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 19.0, 27.62121430494607, 1.072697808267464, 0.0, 1.0, 65.0, 28.35443550520961], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8017678587455057, 0.8575659360891548, 0.0, 1.0, 1.0, 0.28354435505209613], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.22295207], dtype=float32), 0.18874352]. 
=============================================
[2019-04-09 15:10:40,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.63948954e-04 5.80727234e-02 1.05022624e-01 1.65210813e-02
 5.98958181e-03 6.37177378e-04 3.49167228e-01 2.32125949e-02
 6.31002709e-02 4.42073606e-02 3.33805352e-01], sum to 1.0000
[2019-04-09 15:10:40,441] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0462
[2019-04-09 15:10:40,452] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 19.0, 27.6066992444595, 1.050854545481996, 0.0, 1.0, 45.0, 24.31367336556875], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 19.0, 27.60241046492608, 1.050627425443165, 0.0, 1.0, 20.0, 24.86142808296026], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.08333333333333333, 0.8002008720771734, 0.8502091418143882, 0.0, 1.0, 0.1, 0.2486142808296026], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.7088436], dtype=float32), 0.26750377]. 
=============================================
[2019-04-09 15:10:40,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1244748e-04 6.3156933e-02 1.4165328e-01 8.6601991e-03 4.0666903e-03
 1.3161942e-03 2.0343244e-01 1.5580605e-02 7.8685440e-02 5.9953678e-02
 4.2328209e-01], sum to 1.0000
[2019-04-09 15:10:40,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3655
[2019-04-09 15:10:40,561] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.33333333333333, 0.0, 0.0, 19.0, 27.59798268007042, 1.050312243143549, 0.0, 1.0, 45.0, 27.07242807108135], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1029000.0000, 
sim time next is 1029600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.59713541216624, 1.062493823544119, 0.0, 1.0, 45.0, 23.4365956719504], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7997612843471865, 0.8541646078480397, 0.0, 1.0, 0.6, 0.234365956719504], 
reward next is 0.7656, 
noisyNet noise sample is [array([-0.70481944], dtype=float32), -0.29984903]. 
=============================================
[2019-04-09 15:10:40,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1687581e-04 1.0756373e-01 1.1358887e-01 2.4246270e-02 2.6335237e-03
 3.2371865e-04 2.6274893e-01 1.7556973e-02 4.7690164e-02 2.4073573e-02
 3.9935741e-01], sum to 1.0000
[2019-04-09 15:10:40,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-09 15:10:40,648] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.95, 78.0, 57.0, 0.0, 22.5, 28.53902311698234, 1.145471428082194, 1.0, 1.0, 55.0, 8.294253243046972], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1006200.0000, 
sim time next is 1006800.0000, 
raw observation next is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 22.5, 28.62158340528469, 1.161238978584544, 1.0, 1.0, 45.0, 13.74471217450685], 
processed observation next is [1.0, 0.6521739130434783, 0.8818097876269622, 0.77, 0.1722222222222222, 0.0, 0.375, 0.8851319504403907, 0.8870796595281814, 1.0, 1.0, 0.6, 0.13744712174506848], 
reward next is 0.8626, 
noisyNet noise sample is [array([-0.31965238], dtype=float32), -0.08268008]. 
=============================================
[2019-04-09 15:10:40,723] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00043317 0.0964395  0.07578844 0.01010527 0.00427419 0.0009686
 0.29545027 0.01309596 0.050962   0.03803656 0.41444606], sum to 1.0000
[2019-04-09 15:10:40,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1166
[2019-04-09 15:10:40,755] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.65105301164278, 1.061079806633329, 0.0, 1.0, 65.0, 28.23680267947796], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1032600.0000, 
sim time next is 1033200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.67362362725837, 1.057840349175392, 0.0, 1.0, 25.0, 22.88937272058682], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8061353022715307, 0.8526134497251308, 0.0, 1.0, 0.2, 0.2288937272058682], 
reward next is 0.7711, 
noisyNet noise sample is [array([-0.81800324], dtype=float32), -0.33396757]. 
=============================================
[2019-04-09 15:10:40,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00061757 0.06526403 0.22781871 0.01254245 0.00555065 0.00059458
 0.321623   0.02059891 0.0389738  0.04276447 0.2636518 ], sum to 1.0000
[2019-04-09 15:10:40,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0107
[2019-04-09 15:10:40,803] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.64220339353847, 1.074700300145497, 0.0, 1.0, 65.0, 27.57038968078959], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1035600.0000, 
sim time next is 1036200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.63675145798073, 1.074343060561943, 0.0, 1.0, 20.0, 25.39388466492791], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8030626214983941, 0.8581143535206476, 0.0, 1.0, 0.1, 0.2539388466492791], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.21598394], dtype=float32), -1.1543963]. 
=============================================
[2019-04-09 15:10:40,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00049921 0.0450071  0.10858428 0.00696214 0.00287046 0.00056071
 0.35567433 0.01387891 0.03700233 0.03399009 0.39497045], sum to 1.0000
[2019-04-09 15:10:40,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-09 15:10:40,918] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.71190928711743, 1.10106929901422, 0.0, 1.0, 60.0, 20.29043890996629], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1038000.0000, 
sim time next is 1038600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.84231695110297, 1.096348836648197, 0.0, 1.0, 65.0, 20.27785473027379], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8201930792585808, 0.8654496122160656, 0.0, 1.0, 1.0, 0.20277854730273792], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.21164769], dtype=float32), 0.37863278]. 
=============================================
[2019-04-09 15:10:41,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1226531e-04 8.4738865e-02 2.3906852e-01 1.7062878e-02 2.8198706e-03
 7.3899393e-04 1.9456473e-01 1.7279766e-02 7.4198343e-02 2.6623186e-02
 3.4269258e-01], sum to 1.0000
[2019-04-09 15:10:41,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9498
[2019-04-09 15:10:41,061] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 22.5, 28.35559266559937, 1.146917156046863, 1.0, 1.0, 65.0, 17.52520749552658], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 22.5, 28.35872686043845, 1.138050216594445, 1.0, 1.0, 65.0, 18.05788986812548], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.375, 0.8632272383698707, 0.8793500721981484, 1.0, 1.0, 1.0, 0.1805788986812548], 
reward next is 0.8194, 
noisyNet noise sample is [array([-0.05926144], dtype=float32), 0.34141368]. 
=============================================
[2019-04-09 15:10:41,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00039449 0.08097746 0.15777908 0.02254172 0.00604831 0.00120684
 0.23348457 0.02195041 0.08928112 0.06417775 0.32215822], sum to 1.0000
[2019-04-09 15:10:41,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5921
[2019-04-09 15:10:41,263] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.4, 98.0, 0.0, 0.0, 22.5, 26.92949997382857, 0.8047423731902712, 1.0, 1.0, 25.0, 33.99007158706743], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 930600.0000, 
sim time next is 931200.0000, 
raw observation next is [4.4, 98.66666666666666, 0.0, 0.0, 22.5, 26.86100971140798, 0.7992176849132053, 1.0, 1.0, 55.0, 39.40718565958844], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9866666666666666, 0.0, 0.0, 0.375, 0.7384174759506651, 0.7664058949710685, 1.0, 1.0, 0.8, 0.39407185659588445], 
reward next is 0.6059, 
noisyNet noise sample is [array([0.5266861], dtype=float32), -0.08860586]. 
=============================================
[2019-04-09 15:10:41,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4200219e-04 6.3719191e-02 8.6686961e-02 1.2189723e-02 5.0567221e-03
 5.9945520e-04 1.9109951e-01 2.3159012e-02 3.8949057e-02 2.5226807e-02
 5.5307156e-01], sum to 1.0000
[2019-04-09 15:10:41,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4236
[2019-04-09 15:10:41,389] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [14.4, 79.0, 0.0, 0.0, 22.5, 27.83409009594573, 1.070676398972384, 0.0, 1.0, 65.0, 29.10834974726783], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1020600.0000, 
sim time next is 1021200.0000, 
raw observation next is [14.4, 78.33333333333333, 0.0, 0.0, 22.5, 27.75793289057625, 1.066284311441739, 1.0, 1.0, 60.0, 28.36755061677218], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7833333333333333, 0.0, 0.0, 0.375, 0.8131610742146874, 0.855428103813913, 1.0, 1.0, 0.9, 0.2836755061677218], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.83177644], dtype=float32), 0.74980086]. 
=============================================
[2019-04-09 15:10:41,503] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.52568163e-04 1.02934726e-01 8.69614184e-02 1.93340816e-02
 2.86734314e-03 5.43707283e-04 2.49599949e-01 1.64760947e-02
 5.86052611e-02 2.18189955e-02 4.40605819e-01], sum to 1.0000
[2019-04-09 15:10:41,507] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8056
[2019-04-09 15:10:41,541] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.98333333333333, 85.16666666666667, 122.0, 0.0, 22.5, 28.08139946126395, 1.07776455247282, 1.0, 1.0, 65.0, 20.46808980123091], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 997800.0000, 
sim time next is 998400.0000, 
raw observation next is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 22.5, 28.14158564937496, 1.0857672447761, 1.0, 1.0, 65.0, 16.70417453410878], 
processed observation next is [1.0, 0.5652173913043478, 0.8301015697137583, 0.8433333333333334, 0.40166666666666667, 0.0, 0.375, 0.8451321374479134, 0.8619224149253667, 1.0, 1.0, 1.0, 0.16704174534108782], 
reward next is 0.8330, 
noisyNet noise sample is [array([1.1506653], dtype=float32), -0.86744165]. 
=============================================
[2019-04-09 15:10:41,977] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00071581 0.04959066 0.1023634  0.01514145 0.00696902 0.00230882
 0.24729066 0.02631253 0.04762833 0.03577801 0.46590126], sum to 1.0000
[2019-04-09 15:10:41,977] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2990
[2019-04-09 15:10:41,990] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 19.0, 26.85798442119026, 0.8021020953197144, 0.0, 1.0, 45.0, 30.26713625132447], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 957000.0000, 
sim time next is 957600.0000, 
raw observation next is [6.6, 82.0, 0.0, 0.0, 19.0, 26.92909908759666, 0.8036069459418106, 0.0, 1.0, 45.0, 32.97680772492443], 
processed observation next is [1.0, 0.08695652173913043, 0.6454293628808865, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7440915906330551, 0.7678689819806035, 0.0, 1.0, 0.6, 0.3297680772492443], 
reward next is 0.6702, 
noisyNet noise sample is [array([0.09164459], dtype=float32), -0.84210515]. 
=============================================
[2019-04-09 15:10:42,459] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3774480e-04 9.1374762e-02 1.3348579e-01 9.9649457e-03 3.5742107e-03
 7.2230375e-04 3.1053424e-01 2.1151783e-02 4.2706165e-02 3.4714255e-02
 3.5143375e-01], sum to 1.0000
[2019-04-09 15:10:42,460] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0242
[2019-04-09 15:10:42,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.6982713153612, 1.080877370768453, 0.0, 1.0, 45.0, 26.54921138626815], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1030800.0000, 
sim time next is 1031400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.71555352794517, 1.078790775192099, 0.0, 1.0, 20.0, 21.73905937302804], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8096294606620974, 0.8595969250640331, 0.0, 1.0, 0.1, 0.2173905937302804], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.54066813], dtype=float32), -0.36281157]. 
=============================================
[2019-04-09 15:10:42,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00116183 0.05700329 0.12082927 0.02247808 0.00517799 0.00213052
 0.29636398 0.03060398 0.06744645 0.04639091 0.35041368], sum to 1.0000
[2019-04-09 15:10:42,585] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7432
[2019-04-09 15:10:42,599] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 19.0, 27.00185956323735, 0.8120537164523668, 0.0, 1.0, 25.0, 35.93983880255757], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 970200.0000, 
sim time next is 970800.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 19.0, 26.95655160856219, 0.8214351489856345, 0.0, 1.0, 65.0, 39.71775857908482], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7463793007135159, 0.7738117163285448, 0.0, 1.0, 1.0, 0.3971775857908482], 
reward next is 0.6028, 
noisyNet noise sample is [array([-1.7756317], dtype=float32), 2.3285742]. 
=============================================
[2019-04-09 15:10:42,670] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6523583e-04 8.5081987e-02 1.2299025e-01 1.1609175e-02 2.1087350e-03
 4.0039411e-04 2.1902896e-01 1.4866767e-02 5.9375476e-02 2.6247937e-02
 4.5812503e-01], sum to 1.0000
[2019-04-09 15:10:42,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3127
[2019-04-09 15:10:42,685] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 79.66666666666667, 0.0, 0.0, 22.5, 27.86082772384587, 1.050623894990125, 1.0, 1.0, 65.0, 28.21817100132829], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [14.4, 79.0, 0.0, 0.0, 22.5, 27.77819779708436, 1.045388023906996, 0.0, 1.0, 45.0, 25.3989409302532], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.79, 0.0, 0.0, 0.375, 0.8148498164236967, 0.8484626746356653, 0.0, 1.0, 0.6, 0.253989409302532], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.5732573], dtype=float32), -1.8280101]. 
=============================================
[2019-04-09 15:10:43,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00094364 0.06287666 0.11014535 0.01291935 0.00650904 0.00234881
 0.24974999 0.02846688 0.05083295 0.03297443 0.4422329 ], sum to 1.0000
[2019-04-09 15:10:43,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-09 15:10:43,030] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00096973 0.06311758 0.13782948 0.01942831 0.00629365 0.00195905
 0.3961661  0.02824437 0.04540015 0.04022378 0.26036772], sum to 1.0000
[2019-04-09 15:10:43,030] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2832
[2019-04-09 15:10:43,047] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.89007969200485, 1.091021195461962, 0.0, 1.0, 60.0, 22.48893217601822], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.91066319612754, 1.081682033439819, 0.0, 1.0, 45.0, 24.60763941897925], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.825888599677295, 0.860560677813273, 0.0, 1.0, 0.6, 0.2460763941897925], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.3405992], dtype=float32), -0.048922006]. 
=============================================
[2019-04-09 15:10:43,050] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 19.0, 27.70834268130154, 1.079094614233848, 0.0, 1.0, 25.0, 22.36329964992823], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1060800.0000, 
sim time next is 1061400.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 19.0, 27.68773153743479, 1.075685086729914, 0.0, 1.0, 45.0, 23.54206522723952], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.08333333333333333, 0.8073109614528992, 0.858561695576638, 0.0, 1.0, 0.6, 0.2354206522723952], 
reward next is 0.7646, 
noisyNet noise sample is [array([0.6669022], dtype=float32), -0.35667053]. 
=============================================
[2019-04-09 15:10:43,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00147691 0.06372593 0.13947646 0.01957615 0.00844556 0.00236143
 0.3190795  0.02396412 0.04267736 0.03180588 0.34741074], sum to 1.0000
[2019-04-09 15:10:43,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6726
[2019-04-09 15:10:43,200] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.81701086870946, 1.085072955139991, 0.0, 1.0, 45.0, 22.47076852668105], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1050600.0000, 
sim time next is 1051200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.82199687470539, 1.082641781090374, 0.0, 1.0, 65.0, 33.18640065647283], 
processed observation next is [1.0, 0.17391304347826086, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8184997395587826, 0.8608805936967913, 0.0, 1.0, 1.0, 0.33186400656472825], 
reward next is 0.6681, 
noisyNet noise sample is [array([-1.8142306], dtype=float32), -0.9838518]. 
=============================================
[2019-04-09 15:10:43,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00127857 0.09197408 0.11729018 0.02097159 0.00633437 0.00270113
 0.22153306 0.03557915 0.04097854 0.02678413 0.43457517], sum to 1.0000
[2019-04-09 15:10:43,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0704
[2019-04-09 15:10:43,325] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 19.0, 26.96920235162494, 0.8335552424124747, 0.0, 1.0, 20.0, 33.24464350290479], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 19.0, 27.06918894620872, 0.8407220754590782, 0.0, 1.0, 20.0, 26.37269608246522], 
processed observation next is [1.0, 0.2608695652173913, 0.7063711911357342, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7557657455173933, 0.7802406918196927, 0.0, 1.0, 0.1, 0.2637269608246522], 
reward next is 0.7363, 
noisyNet noise sample is [array([-1.2000057], dtype=float32), 0.6283559]. 
=============================================
[2019-04-09 15:10:43,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[6.9309196]
 [7.2288737]
 [6.8777204]
 [6.9549003]
 [7.12903  ]], R is [[ 7.77125454]
 [ 8.36109543]
 [ 8.87770081]
 [ 9.46287251]
 [10.04863071]].
[2019-04-09 15:10:43,432] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00081391 0.09701844 0.14698128 0.03751525 0.00818231 0.00089608
 0.33279616 0.02103637 0.06613273 0.04106822 0.24755922], sum to 1.0000
[2019-04-09 15:10:43,435] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0427
[2019-04-09 15:10:43,448] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.2, 83.0, 48.0, 124.0, 22.5, 28.33491160680503, 1.177004893581674, 1.0, 1.0, 60.0, 13.37443390684289], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1068600.0000, 
sim time next is 1069200.0000, 
raw observation next is [12.2, 83.0, 61.0, 151.5, 22.5, 28.32299606612062, 1.190321628813167, 1.0, 1.0, 20.0, 16.86501101786326], 
processed observation next is [1.0, 0.391304347826087, 0.8005540166204987, 0.83, 0.20333333333333334, 0.16740331491712707, 0.375, 0.8602496721767183, 0.8967738762710556, 1.0, 1.0, 0.1, 0.1686501101786326], 
reward next is 0.8313, 
noisyNet noise sample is [array([0.05059246], dtype=float32), 1.0795071]. 
=============================================
[2019-04-09 15:10:43,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00121804 0.09051108 0.07240046 0.01632307 0.00805884 0.00113559
 0.26865938 0.02169373 0.04802282 0.04665533 0.42532167], sum to 1.0000
[2019-04-09 15:10:43,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8556
[2019-04-09 15:10:43,668] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 19.0, 27.70914627966448, 1.068076173799505, 0.0, 1.0, 45.0, 22.29616922421295], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1060800.0000, 
sim time next is 1061400.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 19.0, 27.68854769187421, 1.064724847376306, 0.0, 1.0, 65.0, 27.78836675372008], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.08333333333333333, 0.8073789743228508, 0.8549082824587687, 0.0, 1.0, 1.0, 0.2778836675372008], 
reward next is 0.7221, 
noisyNet noise sample is [array([-2.156841], dtype=float32), 0.28360617]. 
=============================================
[2019-04-09 15:10:43,845] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.79793831e-04 1.08131625e-01 1.18604921e-01 2.29897983e-02
 4.32187039e-03 4.43926110e-04 2.58950800e-01 3.42049040e-02
 1.09062999e-01 6.01382591e-02 2.82971054e-01], sum to 1.0000
[2019-04-09 15:10:43,851] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-09 15:10:43,861] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [12.0, 86.0, 121.3333333333333, 0.0, 22.5, 28.19046438363553, 1.080794701504321, 1.0, 1.0, 25.0, 18.1884727219885], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 992400.0000, 
sim time next is 993000.0000, 
raw observation next is [12.1, 86.0, 122.6666666666667, 0.0, 22.5, 28.20127770078374, 1.090058209386318, 1.0, 1.0, 55.0, 17.66094067929582], 
processed observation next is [1.0, 0.4782608695652174, 0.7977839335180056, 0.86, 0.408888888888889, 0.0, 0.375, 0.8501064750653118, 0.863352736462106, 1.0, 1.0, 0.8, 0.1766094067929582], 
reward next is 0.8234, 
noisyNet noise sample is [array([-1.7748382], dtype=float32), 1.2562574]. 
=============================================
[2019-04-09 15:10:43,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[9.650564]
 [9.518623]
 [9.53822 ]
 [9.641923]
 [9.468654]], R is [[10.28619194]
 [11.00144577]
 [11.72431374]
 [12.42168427]
 [13.11831856]].
[2019-04-09 15:10:44,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00059701 0.0707202  0.15065902 0.00811972 0.00533815 0.00072407
 0.24083398 0.01920379 0.04690319 0.03889682 0.418004  ], sum to 1.0000
[2019-04-09 15:10:44,084] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3138
[2019-04-09 15:10:44,098] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 19.0, 27.96237650469391, 1.201870140114723, 0.0, 1.0, 65.0, 24.97277985001613], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [11.7, 70.16666666666667, 0.0, 0.0, 19.0, 27.96290222549072, 1.201130975573081, 0.0, 1.0, 45.0, 23.90764381499482], 
processed observation next is [1.0, 1.0, 0.7867036011080333, 0.7016666666666667, 0.0, 0.0, 0.08333333333333333, 0.8302418521242266, 0.9003769918576937, 0.0, 1.0, 0.6, 0.2390764381499482], 
reward next is 0.7609, 
noisyNet noise sample is [array([-0.3107384], dtype=float32), -0.5800641]. 
=============================================
[2019-04-09 15:10:44,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0519638e-04 9.4344981e-02 1.9882898e-01 1.5301760e-02 3.2994107e-03
 6.5896520e-04 1.9667651e-01 1.9197688e-02 3.1515010e-02 2.3397753e-02
 4.1657379e-01], sum to 1.0000
[2019-04-09 15:10:44,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-09 15:10:44,224] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.55, 61.0, 0.0, 0.0, 19.0, 28.20487692923154, 1.274050058457724, 0.0, 1.0, 65.0, 22.14323213850131], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1110600.0000, 
sim time next is 1111200.0000, 
raw observation next is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 19.0, 28.17882511438684, 1.269633579990727, 0.0, 1.0, 20.0, 19.70097896692792], 
processed observation next is [1.0, 0.8695652173913043, 0.8356417359187445, 0.6133333333333333, 0.0, 0.0, 0.08333333333333333, 0.8482354261989032, 0.9232111933302424, 0.0, 1.0, 0.1, 0.1970097896692792], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.24079089], dtype=float32), -0.098528326]. 
=============================================
[2019-04-09 15:10:44,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3549817e-04 8.0796905e-02 1.9065571e-01 2.3156775e-02 5.1454715e-03
 4.8682719e-04 2.9973513e-01 1.4188621e-02 5.3573169e-02 4.3552484e-02
 2.8857344e-01], sum to 1.0000
[2019-04-09 15:10:44,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9628
[2019-04-09 15:10:44,286] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 22.5, 28.35601487744342, 1.134968158565073, 1.0, 1.0, 20.0, 14.92636951727246], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 22.5, 28.31083158176115, 1.121260667737283, 1.0, 1.0, 65.0, 21.96431014727062], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.375, 0.8592359651467625, 0.8737535559124275, 1.0, 1.0, 1.0, 0.2196431014727062], 
reward next is 0.7804, 
noisyNet noise sample is [array([-1.4481612], dtype=float32), 0.72455025]. 
=============================================
[2019-04-09 15:10:44,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00093935 0.08670084 0.13765949 0.02157034 0.00917879 0.00219577
 0.3663304  0.01995097 0.03962941 0.05833333 0.2575113 ], sum to 1.0000
[2019-04-09 15:10:44,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-09 15:10:44,324] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.5, 77.0, 0.0, 0.0, 19.0, 27.89269315261862, 1.175909093038493, 0.0, 1.0, 25.0, 22.20670305938372], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1126800.0000, 
sim time next is 1127400.0000, 
raw observation next is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 27.87242496762738, 1.172250532636515, 0.0, 1.0, 65.0, 26.63307452520471], 
processed observation next is [0.0, 0.043478260869565216, 0.7511542012927056, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8227020806356151, 0.890750177545505, 0.0, 1.0, 1.0, 0.2663307452520471], 
reward next is 0.7337, 
noisyNet noise sample is [array([-1.6036828], dtype=float32), -1.0362581]. 
=============================================
[2019-04-09 15:10:44,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00352505 0.12359425 0.15954338 0.04247279 0.01400204 0.00530981
 0.15181284 0.02235054 0.06384238 0.04385503 0.3696919 ], sum to 1.0000
[2019-04-09 15:10:44,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-09 15:10:44,569] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.55, 78.0, 0.0, 0.0, 19.0, 27.78602391247136, 1.150306961333037, 0.0, 1.0, 65.0, 27.65735594600335], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1132200.0000, 
sim time next is 1132800.0000, 
raw observation next is [10.73333333333333, 77.66666666666666, 0.0, 0.0, 19.0, 27.78218518357008, 1.148582290494288, 0.0, 1.0, 65.0, 27.68840638725263], 
processed observation next is [0.0, 0.08695652173913043, 0.7599261311172669, 0.7766666666666666, 0.0, 0.0, 0.08333333333333333, 0.8151820986308399, 0.882860763498096, 0.0, 1.0, 1.0, 0.2768840638725263], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.40139547], dtype=float32), -0.52684116]. 
=============================================
[2019-04-09 15:10:44,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3231714e-04 4.4043746e-02 1.5226483e-01 1.5079690e-02 1.9972178e-03
 5.1882380e-04 3.3588475e-01 1.5849043e-02 2.5872111e-02 4.0119592e-02
 3.6803788e-01], sum to 1.0000
[2019-04-09 15:10:44,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6189
[2019-04-09 15:10:44,633] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.63198603410028, 1.059504524399407, 0.0, 1.0, 45.0, 27.41556924304026], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1036200.0000, 
sim time next is 1036800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.62870842769424, 1.059398888733438, 0.0, 1.0, 60.0, 29.92497282390508], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8023923689745199, 0.8531329629111459, 0.0, 1.0, 0.9, 0.29924972823905077], 
reward next is 0.7008, 
noisyNet noise sample is [array([-0.8940081], dtype=float32), -0.23570698]. 
=============================================
[2019-04-09 15:10:44,677] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7002796e-04 8.1939131e-02 1.0970828e-01 2.0638498e-02 3.1796764e-03
 1.1079025e-03 3.9602143e-01 1.7474508e-02 4.1753929e-02 2.2065552e-02
 3.0574107e-01], sum to 1.0000
[2019-04-09 15:10:44,680] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0859
[2019-04-09 15:10:44,685] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.55, 49.5, 35.0, 0.0, 22.5, 29.93031554758532, 1.528105367298691, 1.0, 0.0, 65.0, 0.1550298552136523], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1096200.0000, 
sim time next is 1096800.0000, 
raw observation next is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 22.5, 29.940978375258, 1.529665311552142, 1.0, 0.0, 25.0, 0.1549177428856766], 
processed observation next is [1.0, 0.6956521739130435, 0.968605724838412, 0.4966666666666666, 0.0977777777777778, 0.0005524861878453037, 0.375, 0.9950815312714999, 1.0098884371840473, 1.0, 0.0, 0.2, 0.0015491774288567662], 
reward next is 0.9985, 
noisyNet noise sample is [array([-0.33606154], dtype=float32), -0.36283842]. 
=============================================
[2019-04-09 15:10:44,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4629923e-04 1.2435895e-01 1.5339997e-01 1.5818471e-02 4.1632918e-03
 2.5353700e-04 3.4982419e-01 1.6746698e-02 7.1287811e-02 3.0719582e-02
 2.3318115e-01], sum to 1.0000
[2019-04-09 15:10:44,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-09 15:10:44,921] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.31666666666667, 76.0, 46.33333333333334, 0.0, 22.5, 28.57730908066201, 1.161214214529264, 1.0, 1.0, 25.0, 12.74640680897931], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [15.5, 75.0, 41.0, 0.0, 22.5, 28.60014746124332, 1.158183208018167, 1.0, 1.0, 65.0, 13.70424852213494], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.75, 0.13666666666666666, 0.0, 0.375, 0.8833456217702768, 0.8860610693393891, 1.0, 1.0, 1.0, 0.1370424852213494], 
reward next is 0.8630, 
noisyNet noise sample is [array([0.3738213], dtype=float32), -1.5567102]. 
=============================================
[2019-04-09 15:10:44,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[10.173178]
 [10.104449]
 [10.216462]
 [10.07987 ]
 [10.251379]], R is [[10.93436813]
 [11.69756031]
 [12.4315052 ]
 [13.20547581]
 [14.01767349]].
[2019-04-09 15:10:45,010] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00048505 0.13331461 0.18751305 0.0353708  0.00642926 0.0012677
 0.26923934 0.014389   0.0342975  0.02652283 0.29117087], sum to 1.0000
[2019-04-09 15:10:45,011] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0654
[2019-04-09 15:10:45,031] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 22.5, 28.88119897674652, 1.360986877522557, 1.0, 0.0, 20.0, 11.24823766766209], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 22.5, 28.78581454993341, 1.360244287151466, 1.0, 0.0, 20.0, 13.96891313663415], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.375, 0.8988178791611174, 0.953414762383822, 1.0, 0.0, 0.1, 0.1396891313663415], 
reward next is 0.8603, 
noisyNet noise sample is [array([-0.62288976], dtype=float32), 0.6612392]. 
=============================================
[2019-04-09 15:10:45,049] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[8.625491]
 [8.784655]
 [8.87706 ]
 [9.031415]
 [8.640851]], R is [[ 9.52720928]
 [10.31945515]
 [11.09770584]
 [11.83644104]
 [12.60810852]].
[2019-04-09 15:10:45,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00076631 0.05685058 0.14644782 0.02140395 0.01010438 0.00139696
 0.31591624 0.0210957  0.04511602 0.03746774 0.3434343 ], sum to 1.0000
[2019-04-09 15:10:45,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-09 15:10:45,512] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 27.79195613620081, 1.08087888043203, 0.0, 1.0, 65.0, 24.7042242392266], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 27.79801943731849, 1.079729622200391, 0.0, 1.0, 25.0, 24.41239424538009], 
processed observation next is [1.0, 0.21739130434782608, 0.8333333333333334, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.8165016197765409, 0.8599098740667971, 0.0, 1.0, 0.2, 0.2441239424538009], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.9970764], dtype=float32), 0.6886363]. 
=============================================
[2019-04-09 15:10:45,670] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1893884e-04 5.0058931e-02 8.4239282e-02 1.3721567e-02 4.1968962e-03
 5.1899138e-04 2.6130852e-01 1.4274296e-02 3.6410503e-02 4.4336431e-02
 4.9061570e-01], sum to 1.0000
[2019-04-09 15:10:45,671] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0610
[2019-04-09 15:10:45,695] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.7, 64.0, 0.0, 0.0, 19.0, 28.06799571270332, 1.240141339479374, 0.0, 1.0, 65.0, 23.61177411197712], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1116000.0000, 
sim time next is 1116600.0000, 
raw observation next is [12.61666666666667, 64.33333333333334, 0.0, 0.0, 19.0, 28.05363977314132, 1.236705890572995, 0.0, 1.0, 25.0, 20.9194733708859], 
processed observation next is [1.0, 0.9565217391304348, 0.8120960295475533, 0.6433333333333334, 0.0, 0.0, 0.08333333333333333, 0.8378033144284434, 0.912235296857665, 0.0, 1.0, 0.2, 0.209194733708859], 
reward next is 0.7908, 
noisyNet noise sample is [array([-0.7880496], dtype=float32), 0.18894051]. 
=============================================
[2019-04-09 15:10:45,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2085644e-04 8.6189948e-02 2.0185947e-01 2.8567489e-02 4.3571456e-03
 4.7349100e-04 2.1040052e-01 1.8528463e-02 4.0931735e-02 6.5259993e-02
 3.4321076e-01], sum to 1.0000
[2019-04-09 15:10:45,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3861
[2019-04-09 15:10:45,736] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 78.33333333333333, 0.0, 0.0, 22.5, 27.51097180562697, 1.007622113099669, 1.0, 1.0, 65.0, 25.32581278590607], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1021200.0000, 
sim time next is 1021800.0000, 
raw observation next is [14.4, 77.66666666666667, 0.0, 0.0, 22.5, 27.57328069599615, 1.023378853495586, 1.0, 1.0, 45.0, 30.87434809325546], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7766666666666667, 0.0, 0.0, 0.375, 0.7977733913330125, 0.8411262844985288, 1.0, 1.0, 0.6, 0.3087434809325546], 
reward next is 0.6913, 
noisyNet noise sample is [array([0.8590726], dtype=float32), -0.37024045]. 
=============================================
[2019-04-09 15:10:45,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00083281 0.06979922 0.13333917 0.01072492 0.00388933 0.00056059
 0.3643426  0.01611885 0.03585145 0.0396481  0.32489294], sum to 1.0000
[2019-04-09 15:10:45,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4019
[2019-04-09 15:10:45,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.65880728723192, 1.039222890721793, 0.0, 1.0, 45.0, 25.75735331747821], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.64902864495371, 1.038659005031776, 0.0, 1.0, 45.0, 22.60110061307223], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8040857204128091, 0.8462196683439253, 0.0, 1.0, 0.6, 0.2260110061307223], 
reward next is 0.7740, 
noisyNet noise sample is [array([1.1752195], dtype=float32), 0.89791393]. 
=============================================
[2019-04-09 15:10:46,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1264038e-04 1.3849783e-01 1.3025835e-01 2.7732164e-02 7.0213377e-03
 4.1010362e-04 2.4731402e-01 1.5889568e-02 3.9091773e-02 3.3914041e-02
 3.5975811e-01], sum to 1.0000
[2019-04-09 15:10:46,654] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0778
[2019-04-09 15:10:46,661] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 22.5, 29.24617779238517, 1.395464717821425, 1.0, 0.0, 65.0, 3.898182570990874], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1082400.0000, 
sim time next is 1083000.0000, 
raw observation next is [18.01666666666667, 57.5, 177.6666666666667, 211.3333333333333, 22.5, 29.15607319724645, 1.403814931300174, 1.0, 0.0, 20.0, 5.028108144548787], 
processed observation next is [1.0, 0.5217391304347826, 0.9616805170821794, 0.575, 0.5922222222222224, 0.23351749539594838, 0.375, 0.9296727664372041, 0.9679383104333913, 1.0, 0.0, 0.1, 0.05028108144548787], 
reward next is 0.9497, 
noisyNet noise sample is [array([-1.0585288], dtype=float32), 0.49371675]. 
=============================================
[2019-04-09 15:10:46,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[10.008055]
 [10.010739]
 [10.111234]
 [10.312107]
 [10.270926]], R is [[10.79880428]
 [11.65183449]
 [12.51847363]
 [13.39328861]
 [14.25935555]].
[2019-04-09 15:10:46,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00284382 0.0897068  0.11657482 0.04308272 0.02004382 0.00698625
 0.20595647 0.04117538 0.06722441 0.08028549 0.32612005], sum to 1.0000
[2019-04-09 15:10:46,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9001
[2019-04-09 15:10:46,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6138415e-04 8.6385056e-02 1.3449585e-01 2.3197217e-02 2.3969563e-03
 7.8409712e-04 2.3872876e-01 1.1904351e-02 6.3356489e-02 1.7243216e-02
 4.2124662e-01], sum to 1.0000
[2019-04-09 15:10:46,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1970
[2019-04-09 15:10:46,984] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.55, 64.0, 171.0, 0.0, 19.0, 28.22710029990508, 1.240811708927507, 0.0, 0.0, 40.0, 16.15257246851856], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1168200.0000, 
sim time next is 1168800.0000, 
raw observation next is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 19.0, 28.25872974765873, 1.242241793464549, 0.0, 0.0, 65.0, 17.65202545020599], 
processed observation next is [0.0, 0.5217391304347826, 0.9741458910433982, 0.6433333333333333, 0.5633333333333334, 0.0, 0.08333333333333333, 0.8548941456382275, 0.9140805978215164, 0.0, 0.0, 1.0, 0.1765202545020599], 
reward next is 0.8235, 
noisyNet noise sample is [array([0.33164975], dtype=float32), -0.31670773]. 
=============================================
[2019-04-09 15:10:46,987] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 22.5, 29.37123539922718, 1.466182238482779, 1.0, 0.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 22.5, 29.63811539625866, 1.487914875740442, 1.0, 0.0, 25.0, 0.09803448126055667], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.375, 0.9698429496882216, 0.9959716252468139, 1.0, 0.0, 0.2, 0.0009803448126055666], 
reward next is 0.9990, 
noisyNet noise sample is [array([-0.22916938], dtype=float32), -0.53501016]. 
=============================================
[2019-04-09 15:10:47,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3709352e-04 8.3477378e-02 1.3243985e-01 1.8908374e-02 5.0569046e-03
 6.4048712e-04 2.1582156e-01 1.9802807e-02 8.7128989e-02 5.2057207e-02
 3.8432932e-01], sum to 1.0000
[2019-04-09 15:10:47,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6303
[2019-04-09 15:10:47,019] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.11666666666667, 80.5, 104.6666666666667, 156.0, 22.5, 28.66057523280334, 1.265296782807049, 1.0, 1.0, 45.0, 8.043392613395977], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [13.3, 80.0, 107.0, 117.0, 22.5, 28.75281711607064, 1.274632158271179, 1.0, 1.0, 65.0, 8.629470917317718], 
processed observation next is [1.0, 0.43478260869565216, 0.8310249307479226, 0.8, 0.3566666666666667, 0.1292817679558011, 0.375, 0.8960680930058867, 0.9248773860903929, 1.0, 1.0, 1.0, 0.08629470917317718], 
reward next is 0.9137, 
noisyNet noise sample is [array([1.3759661], dtype=float32), 0.72109365]. 
=============================================
[2019-04-09 15:10:47,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2203560e-04 1.1673841e-01 1.7406295e-01 1.8946882e-02 5.5642128e-03
 5.7346153e-04 2.5269261e-01 1.6330240e-02 3.5357591e-02 4.0461466e-02
 3.3895016e-01], sum to 1.0000
[2019-04-09 15:10:47,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8200
[2019-04-09 15:10:47,096] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 22.5, 28.87469145419387, 1.287446287183821, 1.0, 1.0, 25.0, 7.598617399272522], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [14.4, 75.0, 114.0, 0.0, 22.5, 28.89906965714869, 1.293682090356471, 1.0, 1.0, 65.0, 9.755149878556853], 
processed observation next is [1.0, 0.43478260869565216, 0.8614958448753465, 0.75, 0.38, 0.0, 0.375, 0.9082558047623909, 0.9312273634521571, 1.0, 1.0, 1.0, 0.09755149878556853], 
reward next is 0.9024, 
noisyNet noise sample is [array([0.13268183], dtype=float32), 0.67423695]. 
=============================================
[2019-04-09 15:10:47,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00074827 0.06879545 0.23079106 0.02133997 0.00486716 0.00130359
 0.23330031 0.02110589 0.04778061 0.03104274 0.33892494], sum to 1.0000
[2019-04-09 15:10:47,176] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3386
[2019-04-09 15:10:47,188] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.2, 77.33333333333334, 0.0, 0.0, 19.0, 27.77997473528882, 1.050438538558345, 0.0, 1.0, 45.0, 23.5159985175908], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1052400.0000, 
sim time next is 1053000.0000, 
raw observation next is [14.1, 77.5, 0.0, 0.0, 19.0, 27.73171168902247, 1.059812060644915, 0.0, 1.0, 45.0, 24.30791974857156], 
processed observation next is [1.0, 0.17391304347826086, 0.8531855955678671, 0.775, 0.0, 0.0, 0.08333333333333333, 0.8109759740852059, 0.8532706868816383, 0.0, 1.0, 0.6, 0.2430791974857156], 
reward next is 0.7569, 
noisyNet noise sample is [array([-1.2560359], dtype=float32), 0.68973744]. 
=============================================
[2019-04-09 15:10:47,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[7.342882 ]
 [7.359134 ]
 [7.441679 ]
 [7.3885684]
 [7.549448 ]], R is [[ 7.92522478]
 [ 8.61081219]
 [ 9.31322861]
 [ 9.98712254]
 [10.63100243]].
[2019-04-09 15:10:47,588] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00121102 0.08525698 0.1453734  0.02980133 0.00429537 0.00135317
 0.29221758 0.01727907 0.06615569 0.06338601 0.29367033], sum to 1.0000
[2019-04-09 15:10:47,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2265
[2019-04-09 15:10:47,603] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.2, 83.0, 11.5, 38.0, 22.5, 27.82054092382391, 1.091380304871508, 1.0, 1.0, 45.0, 19.65427451739788], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [12.2, 83.0, 15.0, 48.33333333333334, 22.5, 27.82353397996337, 1.096378169184015, 1.0, 1.0, 25.0, 21.43355304064508], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.05, 0.05340699815837938, 0.375, 0.8186278316636141, 0.865459389728005, 1.0, 1.0, 0.2, 0.2143355304064508], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.33560374], dtype=float32), 0.03971612]. 
=============================================
[2019-04-09 15:10:47,663] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00417077 0.15411687 0.13576171 0.03936685 0.0152241  0.00445287
 0.22971603 0.03744202 0.04919334 0.0417444  0.288811  ], sum to 1.0000
[2019-04-09 15:10:47,664] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3259
[2019-04-09 15:10:47,680] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 19.0, 28.00110315463084, 1.181477318801834, 0.0, 0.0, 45.0, 19.41028627974672], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1159800.0000, 
sim time next is 1160400.0000, 
raw observation next is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 19.0, 28.01383348313911, 1.186429165783203, 0.0, 0.0, 30.0, 19.22535813330622], 
processed observation next is [0.0, 0.43478260869565216, 0.9492151431209604, 0.6633333333333334, 0.4072222222222223, 0.0, 0.08333333333333333, 0.8344861235949258, 0.8954763885944009, 0.0, 0.0, 0.3, 0.1922535813330622], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.3585107], dtype=float32), 1.2903702]. 
=============================================
[2019-04-09 15:10:48,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00046606 0.04865317 0.14815836 0.01893112 0.00561608 0.0005133
 0.32554075 0.01672493 0.04166758 0.05754971 0.33617896], sum to 1.0000
[2019-04-09 15:10:48,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1100
[2019-04-09 15:10:48,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8865729e-04 1.0499364e-01 1.6159675e-01 2.1549756e-02 4.1640010e-03
 8.7548926e-04 2.7769586e-01 1.6246744e-02 3.8863245e-02 4.5893729e-02
 3.2783219e-01], sum to 1.0000
[2019-04-09 15:10:48,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0558
[2019-04-09 15:10:48,547] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.2, 66.0, 0.0, 0.0, 19.0, 27.99988081637624, 1.211798937380572, 0.0, 1.0, 45.0, 20.5995559339125], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1119600.0000, 
sim time next is 1120200.0000, 
raw observation next is [12.1, 66.83333333333333, 0.0, 0.0, 19.0, 27.98976681086005, 1.208588751155799, 0.0, 1.0, 25.0, 21.26676628356448], 
processed observation next is [1.0, 1.0, 0.7977839335180056, 0.6683333333333333, 0.0, 0.0, 0.08333333333333333, 0.8324805675716709, 0.9028629170519329, 0.0, 1.0, 0.2, 0.21266766283564478], 
reward next is 0.7873, 
noisyNet noise sample is [array([-0.1845441], dtype=float32), 1.2586504]. 
=============================================
[2019-04-09 15:10:48,568] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 22.5, 29.28347390079857, 1.416923301790206, 1.0, 0.0, 45.0, 7.215915751909445], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1082400.0000, 
sim time next is 1083000.0000, 
raw observation next is [18.01666666666667, 57.5, 177.6666666666667, 211.3333333333333, 22.5, 29.17062735529735, 1.350121258917322, 1.0, 0.0, 20.0, 55.75333800849376], 
processed observation next is [1.0, 0.5217391304347826, 0.9616805170821794, 0.575, 0.5922222222222224, 0.23351749539594838, 0.375, 0.9308856129414457, 0.9500404196391073, 1.0, 0.0, 0.1, 0.5575333800849376], 
reward next is 0.4425, 
noisyNet noise sample is [array([-0.67577446], dtype=float32), -0.8946524]. 
=============================================
[2019-04-09 15:10:48,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.036179]
 [9.307673]
 [9.186282]
 [9.634469]
 [9.441005]], R is [[ 9.55710983]
 [10.38938046]
 [11.23626328]
 [12.09964943]
 [12.95420933]].
[2019-04-09 15:10:48,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5004200e-04 9.4888322e-02 1.4922270e-01 1.2843648e-02 3.5776154e-03
 5.4228061e-04 2.1677743e-01 1.0603332e-02 3.9368629e-02 7.7612877e-02
 3.9441317e-01], sum to 1.0000
[2019-04-09 15:10:48,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4586
[2019-04-09 15:10:48,808] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.8, 54.0, 155.5, 0.0, 22.5, 29.62073324459391, 1.479790938284315, 1.0, 0.0, 45.0, 0.6592060120764741], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1087200.0000, 
sim time next is 1087800.0000, 
raw observation next is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 22.5, 29.67076239688395, 1.486380605529732, 1.0, 0.0, 45.0, 0.3174728977151515], 
processed observation next is [1.0, 0.6086956521739131, 0.9861495844875346, 0.5316666666666666, 0.501111111111111, 0.0, 0.375, 0.9725635330736626, 0.9954602018432439, 1.0, 0.0, 0.6, 0.0031747289771515154], 
reward next is 0.9968, 
noisyNet noise sample is [array([0.60485834], dtype=float32), 0.286028]. 
=============================================
[2019-04-09 15:10:48,823] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00358462 0.10083353 0.15566649 0.03359352 0.01487948 0.00645109
 0.2930456  0.02950517 0.06020166 0.04286253 0.2593763 ], sum to 1.0000
[2019-04-09 15:10:48,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8380
[2019-04-09 15:10:48,841] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 28.21334375297498, 1.241997863093405, 0.0, 0.0, 45.0, 17.35489248457533], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.20450533652419, 1.239009584973183, 0.0, 0.0, 65.0, 21.34172261770152], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.8503754447103491, 0.913003194991061, 0.0, 0.0, 1.0, 0.2134172261770152], 
reward next is 0.7866, 
noisyNet noise sample is [array([-2.0714467], dtype=float32), 0.875467]. 
=============================================
[2019-04-09 15:10:48,854] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00379371 0.11597512 0.15692383 0.0362196  0.01622467 0.00605999
 0.26428887 0.03272384 0.06868995 0.04804687 0.2510535 ], sum to 1.0000
[2019-04-09 15:10:48,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8099
[2019-04-09 15:10:48,869] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.20450533652419, 1.239009584973183, 0.0, 0.0, 65.0, 21.34172261770152], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1220400.0000, 
sim time next is 1221000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.19078811717775, 1.242266538470015, 0.0, 0.0, 65.0, 22.05657124105902], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.8492323430981458, 0.9140888461566717, 0.0, 0.0, 1.0, 0.22056571241059023], 
reward next is 0.7794, 
noisyNet noise sample is [array([-2.0714467], dtype=float32), 0.875467]. 
=============================================
[2019-04-09 15:10:48,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[5.2339425]
 [5.2981725]
 [5.1318927]
 [5.3320932]
 [5.351818 ]], R is [[5.83966446]
 [6.56785059]
 [7.32862282]
 [8.03653908]
 [8.73927689]].
[2019-04-09 15:10:48,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0018636  0.0981769  0.10438504 0.03004822 0.01290369 0.00299408
 0.2660253  0.04305672 0.04792683 0.04784505 0.34477454], sum to 1.0000
[2019-04-09 15:10:48,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1028
[2019-04-09 15:10:48,978] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 19.0, 27.90323773861824, 1.153332969198501, 0.0, 1.0, 45.0, 17.12873161822097], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1154400.0000, 
sim time next is 1155000.0000, 
raw observation next is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 19.0, 27.94545261966594, 1.156623015575897, 0.0, 1.0, 25.0, 22.56717756760487], 
processed observation next is [0.0, 0.34782608695652173, 0.879039704524469, 0.765, 0.1611111111111111, 0.0, 0.08333333333333333, 0.8287877183054949, 0.8855410051919655, 0.0, 1.0, 0.2, 0.22567177567604868], 
reward next is 0.7743, 
noisyNet noise sample is [array([0.78399795], dtype=float32), -0.22337987]. 
=============================================
[2019-04-09 15:10:48,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[6.4076757]
 [6.39198  ]
 [6.5291963]
 [6.4204144]
 [6.3891697]], R is [[ 7.25763512]
 [ 8.01377201]
 [ 8.7094717 ]
 [ 9.38636398]
 [10.02067471]].
[2019-04-09 15:10:49,139] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00321339 0.11812115 0.11954716 0.06117674 0.02232483 0.00427945
 0.1576015  0.02647244 0.07081214 0.06516187 0.3512894 ], sum to 1.0000
[2019-04-09 15:10:49,139] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-09 15:10:49,151] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.31219607505058, 1.26311685170706, 0.0, 0.0, 65.0, 19.54175631241429], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1194000.0000, 
sim time next is 1194600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.30816610811072, 1.262528802421695, 0.0, 0.0, 45.0, 20.12995332345767], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8590138423425598, 0.920842934140565, 0.0, 0.0, 0.6, 0.2012995332345767], 
reward next is 0.7987, 
noisyNet noise sample is [array([-0.77600974], dtype=float32), -1.178299]. 
=============================================
[2019-04-09 15:10:49,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3232424e-04 1.3611014e-01 1.9347222e-01 2.0555271e-02 3.6857068e-03
 1.3149540e-03 1.9151220e-01 3.2857113e-02 4.6035074e-02 3.5208564e-02
 3.3891642e-01], sum to 1.0000
[2019-04-09 15:10:49,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5485
[2019-04-09 15:10:49,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1446549e-04 9.5181122e-02 1.0613455e-01 1.8772522e-02 6.0201697e-03
 7.0071552e-04 3.0447647e-01 1.0545616e-02 7.4669309e-02 3.6327276e-02
 3.4685770e-01], sum to 1.0000
[2019-04-09 15:10:49,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9869
[2019-04-09 15:10:49,456] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [16.63333333333333, 52.0, 0.0, 0.0, 22.5, 28.82929850290434, 1.377495844632384, 1.0, 0.0, 20.0, 9.3220374153484], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [16.36666666666667, 52.5, 0.0, 0.0, 22.5, 28.91156181932771, 1.386299097278877, 1.0, 0.0, 50.0, 7.418388415749289], 
processed observation next is [1.0, 0.7391304347826086, 0.9159741458910436, 0.525, 0.0, 0.0, 0.375, 0.909296818277309, 0.9620996990929589, 1.0, 0.0, 0.7, 0.0741838841574929], 
reward next is 0.9258, 
noisyNet noise sample is [array([0.11034825], dtype=float32), -0.8679162]. 
=============================================
[2019-04-09 15:10:49,479] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.288122]
 [9.410631]
 [9.480732]
 [9.755537]
 [9.426908]], R is [[10.32941437]
 [11.13289928]
 [11.94032288]
 [12.70748234]
 [13.00756264]].
[2019-04-09 15:10:49,481] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 57.0, 0.0, 0.0, 22.5, 28.69255180811846, 1.344142738487586, 1.0, 0.0, 60.0, 14.72566431480733], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1105200.0000, 
sim time next is 1105800.0000, 
raw observation next is [14.8, 57.5, 0.0, 0.0, 22.5, 28.73672715827121, 1.321741038208435, 1.0, 1.0, 65.0, 16.18257255278498], 
processed observation next is [1.0, 0.8260869565217391, 0.8725761772853187, 0.575, 0.0, 0.0, 0.375, 0.8947272631892677, 0.9405803460694783, 1.0, 1.0, 1.0, 0.1618257255278498], 
reward next is 0.8382, 
noisyNet noise sample is [array([-0.20384757], dtype=float32), 0.7240973]. 
=============================================
[2019-04-09 15:10:49,517] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00413612 0.1028696  0.1204517  0.039683   0.01957192 0.00754023
 0.22785938 0.05412609 0.08477368 0.05554765 0.2834406 ], sum to 1.0000
[2019-04-09 15:10:49,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1308
[2019-04-09 15:10:49,531] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.17701572382604, 1.238410847347998, 0.0, 0.0, 45.0, 19.38850549723259], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1222800.0000, 
sim time next is 1223400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.17950145629905, 1.237612227331143, 0.0, 0.0, 25.0, 18.50576499372487], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.8482917880249209, 0.912537409110381, 0.0, 0.0, 0.2, 0.1850576499372487], 
reward next is 0.8149, 
noisyNet noise sample is [array([1.5890187], dtype=float32), -0.72155756]. 
=============================================
[2019-04-09 15:10:49,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00633176 0.13062397 0.1535589  0.0627269  0.02846817 0.00657403
 0.20501561 0.03535409 0.06207284 0.06031807 0.2489557 ], sum to 1.0000
[2019-04-09 15:10:49,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1175
[2019-04-09 15:10:49,730] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.12504102672521, 1.238061373989676, 0.0, 0.0, 45.0, 19.72001713876968], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1228200.0000, 
sim time next is 1228800.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.12651240006208, 1.237140557271046, 0.0, 0.0, 20.0, 19.38273378509473], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8438760333385066, 0.9123801857570154, 0.0, 0.0, 0.1, 0.1938273378509473], 
reward next is 0.8062, 
noisyNet noise sample is [array([-0.264224], dtype=float32), -0.3067381]. 
=============================================
[2019-04-09 15:10:49,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00121284 0.07594508 0.12115737 0.02618835 0.00915659 0.00210483
 0.185969   0.02570591 0.04796834 0.07316178 0.43142992], sum to 1.0000
[2019-04-09 15:10:49,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-09 15:10:49,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0037326  0.14731349 0.12588677 0.04124335 0.01525903 0.0075483
 0.17947549 0.0291955  0.04999943 0.05157145 0.34877458], sum to 1.0000
[2019-04-09 15:10:49,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-09 15:10:49,886] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.08333333333333, 78.66666666666667, 0.0, 0.0, 19.0, 27.82411420031768, 1.158508366132959, 0.0, 1.0, 65.0, 27.30755296465545], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1129800.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 19.0, 27.81037370563823, 1.155524504199045, 0.0, 1.0, 65.0, 27.52065980767717], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.08333333333333333, 0.8175311421365192, 0.885174834733015, 0.0, 1.0, 1.0, 0.2752065980767717], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.31634432], dtype=float32), 0.33960646]. 
=============================================
[2019-04-09 15:10:49,897] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.14491859478494, 1.233197981071943, 0.0, 0.0, 25.0, 19.60331597034887], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.13237673545378, 1.233048255366892, 0.0, 0.0, 65.0, 22.92978763922619], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8443647279544816, 0.9110160851222974, 0.0, 0.0, 1.0, 0.2292978763922619], 
reward next is 0.7707, 
noisyNet noise sample is [array([-0.5395805], dtype=float32), -0.3128107]. 
=============================================
[2019-04-09 15:10:49,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00248581 0.08689101 0.14552546 0.03734239 0.01233345 0.00388504
 0.24916507 0.02780793 0.06729873 0.04513147 0.32213366], sum to 1.0000
[2019-04-09 15:10:49,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1324
[2019-04-09 15:10:49,955] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.1, 64.33333333333334, 0.0, 0.0, 19.0, 28.34022100587033, 1.272928443179484, 0.0, 0.0, 30.0, 17.52305291974406], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [18.0, 65.0, 0.0, 0.0, 19.0, 28.37313106935234, 1.269437326924041, 0.0, 0.0, 25.0, 14.1472597570838], 
processed observation next is [0.0, 0.782608695652174, 0.9612188365650972, 0.65, 0.0, 0.0, 0.08333333333333333, 0.864427589112695, 0.923145775641347, 0.0, 0.0, 0.2, 0.141472597570838], 
reward next is 0.8585, 
noisyNet noise sample is [array([0.09321455], dtype=float32), 0.3059466]. 
=============================================
[2019-04-09 15:10:50,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00231086 0.12004372 0.12672278 0.03753396 0.01470583 0.00436411
 0.17900819 0.04494523 0.06919161 0.0437301  0.35744357], sum to 1.0000
[2019-04-09 15:10:50,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7778
[2019-04-09 15:10:50,087] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.10454183485118, 1.236146598811083, 0.0, 0.0, 65.0, 22.34616599491574], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1235400.0000, 
sim time next is 1236000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.12604107027755, 1.234335668599267, 0.0, 0.0, 65.0, 21.0696125185739], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8438367558564627, 0.9114452228664224, 0.0, 0.0, 1.0, 0.21069612518573902], 
reward next is 0.7893, 
noisyNet noise sample is [array([-1.2241762], dtype=float32), 1.9937115]. 
=============================================
[2019-04-09 15:10:50,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[6.043955 ]
 [5.9035053]
 [5.845176 ]
 [6.0186954]
 [5.852328 ]], R is [[6.50625706]
 [7.21773291]
 [7.95153713]
 [8.67578983]
 [9.39454365]].
[2019-04-09 15:10:50,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00103048 0.09072059 0.13163754 0.01744343 0.00784192 0.00160912
 0.3494967  0.03380221 0.03514351 0.05372536 0.27754912], sum to 1.0000
[2019-04-09 15:10:50,422] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0205
[2019-04-09 15:10:50,441] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.5, 77.0, 0.0, 0.0, 19.0, 27.88481046560555, 1.182916337812617, 0.0, 1.0, 45.0, 23.05564052512733], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1126800.0000, 
sim time next is 1127400.0000, 
raw observation next is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 27.8646726671606, 1.179181488320595, 0.0, 1.0, 20.0, 23.14491820402463], 
processed observation next is [0.0, 0.043478260869565216, 0.7511542012927056, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8220560555967168, 0.8930604961068651, 0.0, 1.0, 0.1, 0.2314491820402463], 
reward next is 0.7686, 
noisyNet noise sample is [array([0.99321496], dtype=float32), 0.9669558]. 
=============================================
[2019-04-09 15:10:50,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00050694 0.06841041 0.09769078 0.01515922 0.00319248 0.00062467
 0.29161277 0.01629939 0.03070232 0.01794492 0.45785606], sum to 1.0000
[2019-04-09 15:10:50,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2655
[2019-04-09 15:10:50,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.61666666666667, 64.33333333333334, 0.0, 0.0, 19.0, 28.05740279619076, 1.227546942730307, 0.0, 1.0, 65.0, 23.62561120619432], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1116600.0000, 
sim time next is 1117200.0000, 
raw observation next is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 19.0, 28.04389087117424, 1.224084424663834, 0.0, 1.0, 45.0, 20.83975232267919], 
processed observation next is [1.0, 0.9565217391304348, 0.8097876269621421, 0.6466666666666667, 0.0, 0.0, 0.08333333333333333, 0.8369909059311867, 0.9080281415546114, 0.0, 1.0, 0.6, 0.2083975232267919], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.630717], dtype=float32), 0.44160965]. 
=============================================
[2019-04-09 15:10:50,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00069595 0.07576676 0.09792226 0.02342951 0.00884285 0.00142374
 0.23669632 0.02073619 0.06078943 0.04210311 0.4315939 ], sum to 1.0000
[2019-04-09 15:10:50,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3576
[2019-04-09 15:10:50,948] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.53333333333333, 100.0, 29.66666666666666, 0.0, 19.0, 28.17647513398901, 1.286035998557371, 0.0, 1.0, 30.0, 17.23810566276653], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 19.0, 28.17031152105742, 1.283651461028954, 0.0, 1.0, 45.0, 17.92156590894738], 
processed observation next is [0.0, 0.6956521739130435, 0.8301015697137583, 1.0, 0.08111111111111109, 0.0, 0.08333333333333333, 0.8475259600881184, 0.9278838203429847, 0.0, 1.0, 0.6, 0.1792156590894738], 
reward next is 0.8208, 
noisyNet noise sample is [array([-1.0678647], dtype=float32), -2.6183026]. 
=============================================
[2019-04-09 15:10:51,365] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00139788 0.11325993 0.18007278 0.03645754 0.00896388 0.0019362
 0.26741123 0.0228976  0.09085707 0.05321394 0.22353195], sum to 1.0000
[2019-04-09 15:10:51,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3465
[2019-04-09 15:10:51,383] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 59.66666666666667, 0.0, 19.0, 28.19488423768016, 1.28999549148061, 0.0, 1.0, 25.0, 18.90310260543686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1264200.0000, 
sim time next is 1264800.0000, 
raw observation next is [13.8, 100.0, 55.33333333333333, 0.0, 19.0, 28.18803966155907, 1.289219139154602, 0.0, 1.0, 45.0, 16.56720653578166], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.18444444444444444, 0.0, 0.08333333333333333, 0.8490033051299225, 0.9297397130515339, 0.0, 1.0, 0.6, 0.16567206535781662], 
reward next is 0.8343, 
noisyNet noise sample is [array([0.4163809], dtype=float32), -0.38937822]. 
=============================================
[2019-04-09 15:10:51,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00206893 0.11981947 0.13959977 0.03311143 0.01700876 0.00385799
 0.20635347 0.03139042 0.0606655  0.06606408 0.32006022], sum to 1.0000
[2019-04-09 15:10:51,401] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4843
[2019-04-09 15:10:51,421] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.75, 66.0, 130.0, 0.0, 19.0, 28.02810604069695, 1.184538586498609, 0.0, 0.0, 25.0, 20.72092857326744], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1161000.0000, 
sim time next is 1161600.0000, 
raw observation next is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 19.0, 28.04358982518939, 1.195102655784386, 0.0, 0.0, 45.0, 17.6447114431929], 
processed observation next is [0.0, 0.43478260869565216, 0.9593721144967682, 0.6566666666666666, 0.45, 0.0, 0.08333333333333333, 0.8369658187657825, 0.8983675519281288, 0.0, 0.0, 0.6, 0.176447114431929], 
reward next is 0.8236, 
noisyNet noise sample is [array([-1.6298867], dtype=float32), -0.38412327]. 
=============================================
[2019-04-09 15:10:51,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00495576 0.09532191 0.14415695 0.03059929 0.01631809 0.00716614
 0.21551357 0.03889173 0.08021094 0.0641634  0.30270225], sum to 1.0000
[2019-04-09 15:10:51,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-09 15:10:51,546] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.08333333333333, 95.5, 0.0, 0.0, 19.0, 28.14182873312935, 1.23717308676831, 0.0, 0.0, 65.0, 22.55891027828284], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1227000.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.13392295555178, 1.236502819098171, 0.0, 0.0, 20.0, 21.16064147345548], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8444935796293148, 0.9121676063660571, 0.0, 0.0, 0.1, 0.2116064147345548], 
reward next is 0.7884, 
noisyNet noise sample is [array([-1.79579], dtype=float32), 0.15530859]. 
=============================================
[2019-04-09 15:10:51,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00153182 0.09825046 0.13054855 0.0233532  0.01261701 0.0033651
 0.30173793 0.02306563 0.05566372 0.0331118  0.31675476], sum to 1.0000
[2019-04-09 15:10:51,568] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3815
[2019-04-09 15:10:51,580] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.16666666666667, 82.5, 21.0, 0.3333333333333333, 19.0, 27.77945063706575, 1.128656121622365, 0.0, 1.0, 25.0, 21.48715900987117], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1152600.0000, 
sim time next is 1153200.0000, 
raw observation next is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 19.0, 27.77742977778541, 1.13041258556724, 0.0, 1.0, 45.0, 23.29811874174218], 
processed observation next is [0.0, 0.34782608695652173, 0.840258541089566, 0.81, 0.08666666666666667, 0.00018416206261510123, 0.08333333333333333, 0.8147858148154509, 0.8768041951890799, 0.0, 1.0, 0.6, 0.2329811874174218], 
reward next is 0.7670, 
noisyNet noise sample is [array([0.17919676], dtype=float32), 0.5635745]. 
=============================================
[2019-04-09 15:10:51,689] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00253598 0.10024694 0.13083488 0.0343429  0.01677555 0.00391917
 0.23154402 0.02170027 0.05940898 0.03677231 0.36191902], sum to 1.0000
[2019-04-09 15:10:51,694] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9138
[2019-04-09 15:10:51,716] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 19.0, 27.79937084573848, 1.120559682809231, 0.0, 1.0, 65.0, 26.64703445930206], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1151400.0000, 
sim time next is 1152000.0000, 
raw observation next is [12.7, 84.0, 16.0, 0.5, 19.0, 27.7917752429811, 1.120553810752444, 0.0, 1.0, 65.0, 27.22843481531286], 
processed observation next is [0.0, 0.34782608695652173, 0.8144044321329641, 0.84, 0.05333333333333334, 0.0005524861878453039, 0.08333333333333333, 0.8159812702484249, 0.8735179369174814, 0.0, 1.0, 1.0, 0.2722843481531286], 
reward next is 0.7277, 
noisyNet noise sample is [array([-2.2765656], dtype=float32), 0.7202715]. 
=============================================
[2019-04-09 15:10:51,723] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[5.906481 ]
 [6.0065384]
 [5.649025 ]
 [6.048435 ]
 [5.8310757]], R is [[6.54186392]
 [7.20997477]
 [7.88002205]
 [8.54153156]
 [9.18275166]].
[2019-04-09 15:10:51,933] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00141585 0.07541357 0.11534811 0.03149691 0.0082988  0.00216069
 0.32452363 0.02572552 0.04543783 0.04895172 0.3212274 ], sum to 1.0000
[2019-04-09 15:10:51,935] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6610
[2019-04-09 15:10:51,968] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 19.0, 27.5611335401218, 1.144343015894536, 0.0, 1.0, 65.0, 37.59710241159269], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1288800.0000, 
sim time next is 1289400.0000, 
raw observation next is [5.500000000000001, 100.0, 0.0, 0.0, 19.0, 27.5533310529008, 1.141066193904656, 0.0, 1.0, 60.0, 37.59629116407513], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7961109210750665, 0.8803553979682187, 0.0, 1.0, 0.9, 0.37596291164075135], 
reward next is 0.6240, 
noisyNet noise sample is [array([0.6500163], dtype=float32), 1.4635538]. 
=============================================
[2019-04-09 15:10:52,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00211391 0.11233509 0.14258493 0.02842857 0.01066374 0.00342755
 0.25107354 0.03278665 0.06107384 0.07688268 0.27862945], sum to 1.0000
[2019-04-09 15:10:52,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8811
[2019-04-09 15:10:52,039] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.3, 65.0, 120.0, 0.0, 19.0, 28.38013951872666, 1.275615950072213, 0.0, 0.0, 30.0, 16.79275856682571], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1176000.0000, 
sim time next is 1176600.0000, 
raw observation next is [18.3, 65.0, 112.0, 0.0, 19.0, 28.36901571603526, 1.276151244056718, 0.0, 0.0, 65.0, 17.68719239444497], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.37333333333333335, 0.0, 0.08333333333333333, 0.8640846430029384, 0.925383748018906, 0.0, 0.0, 1.0, 0.1768719239444497], 
reward next is 0.8231, 
noisyNet noise sample is [array([-0.48742858], dtype=float32), -0.088796124]. 
=============================================
[2019-04-09 15:10:52,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00129142 0.11795731 0.14914396 0.02430681 0.00808353 0.00136949
 0.22110462 0.03360894 0.04279293 0.05213913 0.34820187], sum to 1.0000
[2019-04-09 15:10:52,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1462
[2019-04-09 15:10:52,070] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 19.0, 27.58006117201642, 1.138245774381735, 0.0, 1.0, 30.0, 26.37628276527578], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1290600.0000, 
sim time next is 1291200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.57203400062653, 1.133122022289422, 0.0, 1.0, 25.0, 28.81760372441871], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7976695000522108, 0.8777073407631407, 0.0, 1.0, 0.2, 0.2881760372441871], 
reward next is 0.7118, 
noisyNet noise sample is [array([-0.95827734], dtype=float32), -0.8257116]. 
=============================================
[2019-04-09 15:10:52,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00212818 0.09840453 0.1764899  0.02508057 0.01405889 0.00415415
 0.2590211  0.02132043 0.04679372 0.05258033 0.29996818], sum to 1.0000
[2019-04-09 15:10:52,086] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00230013 0.0938263  0.1447805  0.02483594 0.01161366 0.0036213
 0.2526304  0.03655985 0.05777028 0.07740226 0.2946594 ], sum to 1.0000
[2019-04-09 15:10:52,087] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9948
[2019-04-09 15:10:52,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1297
[2019-04-09 15:10:52,098] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.35, 71.0, 83.0, 0.0, 19.0, 27.97116663464907, 1.163131135690139, 0.0, 0.0, 65.0, 22.52515201359003], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1157400.0000, 
sim time next is 1158000.0000, 
raw observation next is [16.63333333333333, 69.66666666666667, 90.83333333333333, 0.0, 19.0, 27.97507086387202, 1.166690571899676, 0.0, 0.0, 65.0, 23.6778289962714], 
processed observation next is [0.0, 0.391304347826087, 0.9233610341643583, 0.6966666666666668, 0.30277777777777776, 0.0, 0.08333333333333333, 0.8312559053226684, 0.888896857299892, 0.0, 0.0, 1.0, 0.23677828996271402], 
reward next is 0.7632, 
noisyNet noise sample is [array([0.3825902], dtype=float32), 1.3260826]. 
=============================================
[2019-04-09 15:10:52,105] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.38333333333334, 64.66666666666667, 96.0, 0.0, 19.0, 28.39966122791586, 1.28081681023019, 0.0, 0.0, 65.0, 15.5334470210306], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1177800.0000, 
sim time next is 1178400.0000, 
raw observation next is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 19.0, 28.40146313838784, 1.279324159665895, 0.0, 0.0, 25.0, 16.28026133390833], 
processed observation next is [0.0, 0.6521739130434783, 0.9741458910433982, 0.6433333333333334, 0.29333333333333333, 0.0, 0.08333333333333333, 0.8667885948656533, 0.9264413865552984, 0.0, 0.0, 0.2, 0.1628026133390833], 
reward next is 0.8372, 
noisyNet noise sample is [array([-0.48742858], dtype=float32), -0.088796124]. 
=============================================
[2019-04-09 15:10:52,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[6.7251887]
 [6.534112 ]
 [6.735811 ]
 [6.5449066]
 [6.555792 ]], R is [[7.23043394]
 [7.93287802]
 [8.63325882]
 [9.30194283]
 [9.98734283]].
[2019-04-09 15:10:52,199] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00062816 0.09252591 0.17942806 0.02155177 0.00902879 0.00144993
 0.25674638 0.01968512 0.04445252 0.04163188 0.33287147], sum to 1.0000
[2019-04-09 15:10:52,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8473
[2019-04-09 15:10:52,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 19.0, 27.55154813539449, 1.1225357654913, 0.0, 1.0, 65.0, 32.81334877202649], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 19.0, 27.53257492356278, 1.11789700424025, 0.0, 1.0, 65.0, 34.41364493269861], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.08333333333333333, 0.7943812436302317, 0.8726323347467501, 0.0, 1.0, 1.0, 0.3441364493269861], 
reward next is 0.6559, 
noisyNet noise sample is [array([0.37003], dtype=float32), -0.71597904]. 
=============================================
[2019-04-09 15:10:52,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00065623 0.07161143 0.1720737  0.01944256 0.00597057 0.00283313
 0.38817656 0.01514995 0.03866809 0.06211447 0.2233033 ], sum to 1.0000
[2019-04-09 15:10:52,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8964
[2019-04-09 15:10:52,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00076949 0.06991718 0.12751336 0.02643575 0.00733211 0.00167789
 0.28514275 0.02856372 0.0508143  0.05111092 0.35072246], sum to 1.0000
[2019-04-09 15:10:52,395] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-09 15:10:52,398] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.1, 94.5, 0.0, 0.0, 19.0, 27.46209284288979, 1.098604365383782, 0.0, 1.0, 20.0, 27.05277927016297], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1297800.0000, 
sim time next is 1298400.0000, 
raw observation next is [4.0, 94.0, 0.0, 0.0, 19.0, 27.46432840571574, 1.097716244446925, 0.0, 1.0, 65.0, 35.18217822798752], 
processed observation next is [1.0, 0.0, 0.5734072022160666, 0.94, 0.0, 0.0, 0.08333333333333333, 0.788694033809645, 0.8659054148156416, 0.0, 1.0, 1.0, 0.3518217822798752], 
reward next is 0.6482, 
noisyNet noise sample is [array([0.42600754], dtype=float32), 0.4583655]. 
=============================================
[2019-04-09 15:10:52,407] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 19.0, 27.60648623221744, 1.159407472429204, 0.0, 1.0, 65.0, 47.279388515451], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1285800.0000, 
sim time next is 1286400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.59838166517277, 1.155645837170121, 0.0, 1.0, 30.0, 33.04731404665839], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7998651387643975, 0.885215279056707, 0.0, 1.0, 0.3, 0.33047314046658394], 
reward next is 0.6695, 
noisyNet noise sample is [array([0.26614365], dtype=float32), 0.8664005]. 
=============================================
[2019-04-09 15:10:52,595] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00132207 0.16037524 0.09898245 0.02789762 0.00974354 0.00266147
 0.26048625 0.03220326 0.08410014 0.05695033 0.26527762], sum to 1.0000
[2019-04-09 15:10:52,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-09 15:10:52,610] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 19.0, 28.36842579974692, 1.278840532545258, 0.0, 0.0, 65.0, 17.69327378536876], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 19.0, 28.36384647895136, 1.285140006849527, 0.0, 0.0, 55.0, 15.75037347706339], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.08333333333333333, 0.8636538732459466, 0.9283800022831756, 0.0, 0.0, 0.8, 0.1575037347706339], 
reward next is 0.8425, 
noisyNet noise sample is [array([0.16152957], dtype=float32), 0.7095166]. 
=============================================
[2019-04-09 15:10:52,803] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00136926 0.09065258 0.10332952 0.0314382  0.01420276 0.0023165
 0.32562757 0.01774475 0.06327448 0.03983132 0.3102131 ], sum to 1.0000
[2019-04-09 15:10:52,803] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7376
[2019-04-09 15:10:52,815] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.0, 98.66666666666666, 100.0, 0.0, 19.0, 28.18469254275702, 1.286412956613125, 0.0, 1.0, 45.0, 16.97943117819374], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1255200.0000, 
sim time next is 1255800.0000, 
raw observation next is [13.9, 99.33333333333334, 99.0, 0.0, 19.0, 28.18299819745597, 1.286970415555066, 0.0, 1.0, 65.0, 19.64021569584483], 
processed observation next is [0.0, 0.5217391304347826, 0.847645429362881, 0.9933333333333334, 0.33, 0.0, 0.08333333333333333, 0.8485831831213307, 0.9289901385183553, 0.0, 1.0, 1.0, 0.19640215695844832], 
reward next is 0.8036, 
noisyNet noise sample is [array([1.0774164], dtype=float32), -0.13619134]. 
=============================================
[2019-04-09 15:10:52,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00102871 0.08589609 0.13582087 0.02299525 0.01453243 0.00118834
 0.3031625  0.03479017 0.06579398 0.06226974 0.27252188], sum to 1.0000
[2019-04-09 15:10:52,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-09 15:10:52,998] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [13.8, 100.0, 59.66666666666667, 0.0, 19.0, 28.19923506995737, 1.291379693434767, 0.0, 1.0, 45.0, 17.48882921601823], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1264200.0000, 
sim time next is 1264800.0000, 
raw observation next is [13.8, 100.0, 55.33333333333333, 0.0, 19.0, 28.19232819696233, 1.290594242440431, 0.0, 1.0, 50.0, 17.03349441588254], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.18444444444444444, 0.0, 0.08333333333333333, 0.8493606830801941, 0.930198080813477, 0.0, 1.0, 0.7, 0.1703349441588254], 
reward next is 0.8297, 
noisyNet noise sample is [array([0.35969654], dtype=float32), -1.2233416]. 
=============================================
[2019-04-09 15:10:53,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00089656 0.07651884 0.18334126 0.03587584 0.01119241 0.00199217
 0.27599937 0.0249024  0.07387307 0.04890821 0.2664999 ], sum to 1.0000
[2019-04-09 15:10:53,317] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5115
[2019-04-09 15:10:53,330] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 19.0, 28.18685205856244, 1.291567281708185, 0.0, 1.0, 45.0, 20.93721090739884], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 19.0, 28.18340481536899, 1.290939301272948, 0.0, 1.0, 45.0, 15.7814295324625], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 0.08333333333333333, 0.848617067947416, 0.9303131004243159, 0.0, 1.0, 0.6, 0.157814295324625], 
reward next is 0.8422, 
noisyNet noise sample is [array([0.14414003], dtype=float32), 1.3649497]. 
=============================================
[2019-04-09 15:10:53,340] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[7.63392  ]
 [7.6799903]
 [7.6976857]
 [7.827665 ]
 [7.612264 ]], R is [[ 8.66218376]
 [ 9.36618996]
 [10.07176018]
 [10.79628944]
 [11.53850079]].
[2019-04-09 15:10:53,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00309874 0.08803742 0.10124781 0.02724915 0.01372991 0.00473679
 0.22919263 0.02707725 0.06940156 0.06910031 0.36712846], sum to 1.0000
[2019-04-09 15:10:53,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2249
[2019-04-09 15:10:53,409] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 19.0, 28.15188645421642, 1.269320052012123, 0.0, 0.0, 25.0, 16.3424328115548], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [14.9, 100.0, 76.66666666666667, 0.0, 19.0, 28.15674414542503, 1.270495445269492, 0.0, 1.0, 35.0, 17.07291463238484], 
processed observation next is [0.0, 0.43478260869565216, 0.8753462603878118, 1.0, 0.2555555555555556, 0.0, 0.08333333333333333, 0.8463953454520858, 0.9234984817564973, 0.0, 1.0, 0.4, 0.1707291463238484], 
reward next is 0.8293, 
noisyNet noise sample is [array([-0.8083308], dtype=float32), -0.26757458]. 
=============================================
[2019-04-09 15:10:53,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00329455 0.07831663 0.11418206 0.02505171 0.01141966 0.0043464
 0.21178624 0.02941771 0.06687437 0.07676347 0.37854722], sum to 1.0000
[2019-04-09 15:10:53,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3796
[2019-04-09 15:10:53,462] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.8, 100.0, 77.33333333333333, 0.0, 19.0, 28.15919520026673, 1.271405364500349, 0.0, 1.0, 45.0, 16.97340640672295], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1246800.0000, 
sim time next is 1247400.0000, 
raw observation next is [14.7, 100.0, 78.0, 0.0, 19.0, 28.16006008164721, 1.272308575237508, 0.0, 1.0, 45.0, 16.99157355255816], 
processed observation next is [0.0, 0.43478260869565216, 0.8698060941828256, 1.0, 0.26, 0.0, 0.08333333333333333, 0.8466716734706008, 0.9241028584125027, 0.0, 1.0, 0.6, 0.16991573552558162], 
reward next is 0.8301, 
noisyNet noise sample is [array([-0.8083308], dtype=float32), -0.26757458]. 
=============================================
[2019-04-09 15:10:53,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00059007 0.05372774 0.10608014 0.01585772 0.00472259 0.00135208
 0.2358634  0.01961596 0.05413656 0.03196976 0.47608402], sum to 1.0000
[2019-04-09 15:10:53,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2112
[2019-04-09 15:10:53,610] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.366666666666667, 92.0, 0.0, 0.0, 19.0, 27.44526956281902, 1.046455010573212, 0.0, 1.0, 65.0, 33.67427995479731], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1309200.0000, 
sim time next is 1309800.0000, 
raw observation next is [2.283333333333333, 92.0, 0.0, 0.0, 19.0, 27.41827262194685, 1.037057012503247, 0.0, 1.0, 45.0, 33.79044276323805], 
processed observation next is [1.0, 0.13043478260869565, 0.5258541089566021, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7848560518289043, 0.8456856708344157, 0.0, 1.0, 0.6, 0.3379044276323805], 
reward next is 0.6621, 
noisyNet noise sample is [array([0.2658284], dtype=float32), 1.0660266]. 
=============================================
[2019-04-09 15:10:54,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00392698 0.10576781 0.12132201 0.03824867 0.02097751 0.00650129
 0.23948008 0.04106946 0.05969334 0.04251074 0.32050216], sum to 1.0000
[2019-04-09 15:10:54,069] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0599
[2019-04-09 15:10:54,078] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 19.0, 28.23757036521881, 1.243547885775211, 0.0, 0.0, 65.0, 22.16078990622979], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 28.21120082926448, 1.243454463431648, 0.0, 0.0, 20.0, 19.89708855744798], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.08333333333333333, 0.8509334024387067, 0.9144848211438826, 0.0, 0.0, 0.1, 0.19897088557447978], 
reward next is 0.8010, 
noisyNet noise sample is [array([0.48149195], dtype=float32), 1.1852657]. 
=============================================
[2019-04-09 15:10:54,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00321943 0.12293426 0.12662852 0.0501052  0.02066867 0.00420055
 0.27189615 0.04136429 0.05754794 0.06226194 0.23917308], sum to 1.0000
[2019-04-09 15:10:54,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0045
[2019-04-09 15:10:54,214] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.13168666905131, 1.241749807971575, 0.0, 0.0, 65.0, 22.58000730041816], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1227600.0000, 
sim time next is 1228200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.12281625103549, 1.242623341639667, 0.0, 0.0, 20.0, 21.91978522396435], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8435680209196242, 0.9142077805465556, 0.0, 0.0, 0.1, 0.2191978522396435], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.3659771], dtype=float32), -0.72315013]. 
=============================================
[2019-04-09 15:10:54,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00078618 0.06312103 0.1272881  0.01634551 0.0027601  0.00086574
 0.22990409 0.02264084 0.05052125 0.04680078 0.43896642], sum to 1.0000
[2019-04-09 15:10:54,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6059
[2019-04-09 15:10:54,714] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 22.5, 27.48775229157087, 1.022825825839459, 1.0, 1.0, 55.0, 25.02393644798413], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 22.5, 27.70723173591888, 1.029906833889618, 1.0, 1.0, 65.0, 25.62011108328848], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.375, 0.8089359779932401, 0.8433022779632061, 1.0, 1.0, 1.0, 0.2562011108328848], 
reward next is 0.7438, 
noisyNet noise sample is [array([2.0960398], dtype=float32), 3.1753843]. 
=============================================
[2019-04-09 15:10:54,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00062449 0.07890141 0.14202887 0.01435779 0.00509216 0.00168257
 0.2744586  0.01324917 0.02732551 0.02530268 0.41697675], sum to 1.0000
[2019-04-09 15:10:54,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-09 15:10:54,732] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.300000000000001, 95.5, 0.0, 0.0, 19.0, 27.44090320444245, 1.099458470492513, 0.0, 1.0, 55.0, 33.87138023840609], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1296600.0000, 
sim time next is 1297200.0000, 
raw observation next is [4.200000000000001, 95.0, 0.0, 0.0, 19.0, 27.42301910688191, 1.103526353524545, 0.0, 1.0, 65.0, 35.89564978926816], 
processed observation next is [1.0, 0.0, 0.5789473684210527, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7852515922401592, 0.8678421178415151, 0.0, 1.0, 1.0, 0.35895649789268164], 
reward next is 0.6410, 
noisyNet noise sample is [array([-0.4516666], dtype=float32), -1.1547734]. 
=============================================
[2019-04-09 15:10:55,204] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-09 15:10:55,204] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:10:55,204] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:10:55,205] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:10:55,205] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:10:55,207] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:10:55,207] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:10:55,211] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run18
[2019-04-09 15:10:55,235] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run18
[2019-04-09 15:10:55,250] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run18
[2019-04-09 15:12:37,546] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02772048], dtype=float32), 0.032541934]
[2019-04-09 15:12:37,546] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-2.319153283, 80.14332691, 0.0, 0.0, 19.0, 26.85612724106504, 0.7856176060211405, 0.0, 1.0, 25.0, 39.09464196676188]
[2019-04-09 15:12:37,546] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:12:37,547] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00114597 0.09123684 0.12490251 0.02487213 0.01013523 0.00243423
 0.29337677 0.02362941 0.04846986 0.04461693 0.33518007], sampled 0.2767487691585323
[2019-04-09 15:12:50,489] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5646.2822 284965.6190 2930.1226
[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,509] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:50,619] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:51,426] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.02772048], dtype=float32), 0.032541934]
[2019-04-09 15:12:51,427] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [2.0, 52.0, 0.0, 0.0, 19.0, 27.7686173720228, 1.071235529364727, 0.0, 1.0, 20.0, 26.69604643991951]
[2019-04-09 15:12:51,427] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:12:51,427] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.00064764 0.1032216  0.14202665 0.02341286 0.00791053 0.0015907
 0.27070376 0.02144194 0.05156832 0.04278451 0.33469146], sampled 0.6988846521949221
[2019-04-09 15:12:57,293] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5292.7839 320321.6085 2208.3326
[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,328] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,436] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,720] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5389.8516 310624.0227 2600.6386
[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:57,848] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:12:58,743] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 170000, evaluation results [170000.0, 5389.851560922522, 310624.0226647662, 2600.6386039638546, 5646.282151639472, 284965.6190298881, 2930.1225721946557, 5292.783915222806, 320321.60847771785, 2208.332551918241]
[2019-04-09 15:12:58,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00125629 0.06691458 0.11166319 0.02604227 0.00684573 0.00140183
 0.3814429  0.02378939 0.04805109 0.06615482 0.26643792], sum to 1.0000
[2019-04-09 15:12:58,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-09 15:12:58,888] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.533333333333334, 92.0, 0.0, 0.0, 19.0, 27.44341995988463, 1.050281584893112, 0.0, 1.0, 65.0, 36.28202576797182], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1308000.0000, 
sim time next is 1308600.0000, 
raw observation next is [2.45, 92.0, 0.0, 0.0, 19.0, 27.42876932727187, 1.056793503091292, 0.0, 1.0, 65.0, 36.72962204865622], 
processed observation next is [1.0, 0.13043478260869565, 0.5304709141274239, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7857307772726557, 0.8522645010304307, 0.0, 1.0, 1.0, 0.3672962204865622], 
reward next is 0.6327, 
noisyNet noise sample is [array([0.175908], dtype=float32), 0.84087586]. 
=============================================
[2019-04-09 15:12:59,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00398854 0.09779537 0.12416878 0.0313457  0.02138351 0.00555686
 0.17276368 0.03782192 0.05767272 0.05734333 0.39015958], sum to 1.0000
[2019-04-09 15:12:59,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9475
[2019-04-09 15:12:59,028] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.13846372665643, 1.234908865102781, 0.0, 0.0, 45.0, 20.18723883173001], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1223400.0000, 
sim time next is 1224000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.14790081339796, 1.234757074170179, 0.0, 0.0, 65.0, 22.0759537099375], 
processed observation next is [0.0, 0.17391304347826086, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.8456584011164967, 0.9115856913900596, 0.0, 0.0, 1.0, 0.22075953709937501], 
reward next is 0.7792, 
noisyNet noise sample is [array([0.11420161], dtype=float32), 0.3558954]. 
=============================================
[2019-04-09 15:12:59,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[5.361988 ]
 [5.3507915]
 [5.30627  ]
 [5.4598265]
 [5.2324476]], R is [[6.07862616]
 [6.81596756]
 [7.52275562]
 [8.2374239 ]
 [8.81511307]].
[2019-04-09 15:12:59,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00321666 0.15140729 0.13228253 0.04810362 0.01831905 0.00432843
 0.22002906 0.03143527 0.06220296 0.07034597 0.25832912], sum to 1.0000
[2019-04-09 15:12:59,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7896
[2019-04-09 15:12:59,099] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 98.66666666666666, 35.66666666666666, 0.0, 19.0, 28.12341290667124, 1.247490365917534, 0.0, 0.0, 45.0, 18.53083163527065], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1240800.0000, 
sim time next is 1241400.0000, 
raw observation next is [15.0, 99.33333333333334, 43.33333333333333, 0.0, 19.0, 28.1228162550506, 1.249847385402998, 0.0, 0.0, 65.0, 21.33842805095318], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9933333333333334, 0.14444444444444443, 0.0, 0.08333333333333333, 0.8435680212542168, 0.9166157951343327, 0.0, 0.0, 1.0, 0.2133842805095318], 
reward next is 0.7866, 
noisyNet noise sample is [array([0.98481554], dtype=float32), 1.6944357]. 
=============================================
[2019-04-09 15:12:59,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0006989  0.10146678 0.07503112 0.01828883 0.00706437 0.0011009
 0.20260334 0.02184112 0.04626757 0.02655644 0.49908063], sum to 1.0000
[2019-04-09 15:12:59,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4353
[2019-04-09 15:12:59,242] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 19.0, 27.39790685689475, 1.02245824998434, 0.0, 1.0, 65.0, 35.97317816835226], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1312800.0000, 
sim time next is 1313400.0000, 
raw observation next is [1.7, 92.0, 0.0, 0.0, 19.0, 27.32553473035154, 1.027351590786379, 0.0, 1.0, 55.0, 36.20435627129808], 
processed observation next is [1.0, 0.17391304347826086, 0.5096952908587258, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7771278941959615, 0.8424505302621265, 0.0, 1.0, 0.8, 0.3620435627129808], 
reward next is 0.6380, 
noisyNet noise sample is [array([0.34573546], dtype=float32), 0.830084]. 
=============================================
[2019-04-09 15:12:59,490] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00151734 0.09951515 0.14984895 0.02566387 0.00625098 0.00402125
 0.16889428 0.03119212 0.05023375 0.05811236 0.40475005], sum to 1.0000
[2019-04-09 15:12:59,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9160
[2019-04-09 15:12:59,504] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 98.0, 95.0, 0.0, 19.0, 28.17348721093201, 1.280223737184896, 0.0, 1.0, 45.0, 17.23870161536323], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [14.4, 97.33333333333334, 96.0, 0.0, 19.0, 28.1863002500281, 1.281018583360871, 0.0, 1.0, 65.0, 19.00462773186162], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9733333333333334, 0.32, 0.0, 0.08333333333333333, 0.8488583541690083, 0.9270061944536238, 0.0, 1.0, 1.0, 0.1900462773186162], 
reward next is 0.8100, 
noisyNet noise sample is [array([1.1835308], dtype=float32), -0.39284375]. 
=============================================
[2019-04-09 15:12:59,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00085649 0.05879458 0.12133397 0.02097889 0.00589239 0.00146785
 0.3400558  0.01595728 0.03101811 0.07607184 0.32757282], sum to 1.0000
[2019-04-09 15:12:59,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6699
[2019-04-09 15:12:59,594] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 19.0, 28.1859626040817, 1.289449111546365, 0.0, 1.0, 20.0, 22.72668735424848], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 19.0, 28.18251622118753, 1.288832341770895, 0.0, 1.0, 60.0, 20.24680402842522], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 0.08333333333333333, 0.8485430184322942, 0.9296107805902983, 0.0, 1.0, 0.9, 0.2024680402842522], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.08967961], dtype=float32), 0.88062817]. 
=============================================
[2019-04-09 15:12:59,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[7.391119 ]
 [7.3485155]
 [7.2737365]
 [7.2830224]
 [7.342714 ]], R is [[ 8.17593575]
 [ 8.86690903]
 [ 9.43987656]
 [10.17275143]
 [10.89582729]].
[2019-04-09 15:12:59,794] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0667531e-04 5.9989564e-02 1.7238025e-01 1.7117953e-02 3.2995092e-03
 2.6772276e-04 2.8560430e-01 1.0166890e-02 2.6945638e-02 3.6871094e-02
 3.8725027e-01], sum to 1.0000
[2019-04-09 15:12:59,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5332032e-04 1.3499500e-01 1.9069901e-01 1.9419838e-02 3.9296006e-03
 8.6714455e-04 1.8579628e-01 1.8355342e-02 5.5768855e-02 2.9755864e-02
 3.6015984e-01], sum to 1.0000
[2019-04-09 15:12:59,797] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3790
[2019-04-09 15:12:59,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7294
[2019-04-09 15:12:59,809] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 22.5, 27.99466167295997, 1.109667267361699, 1.0, 1.0, 65.0, 27.73904469866289], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 22.5, 27.96052230897789, 1.120206159482478, 1.0, 1.0, 45.0, 26.33854737936655], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 0.375, 0.8300435257481574, 0.873402053160826, 1.0, 1.0, 0.6, 0.2633854737936655], 
reward next is 0.7366, 
noisyNet noise sample is [array([-2.6415262], dtype=float32), 2.5049644]. 
=============================================
[2019-04-09 15:12:59,817] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 22.5, 27.99248066269162, 1.081364897863665, 1.0, 1.0, 20.0, 22.93478183614226], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1332600.0000, 
sim time next is 1333200.0000, 
raw observation next is [0.7000000000000001, 92.0, 92.5, 0.0, 22.5, 28.02893381074295, 1.085659432463003, 1.0, 1.0, 65.0, 25.97326179776399], 
processed observation next is [1.0, 0.43478260869565216, 0.4819944598337951, 0.92, 0.30833333333333335, 0.0, 0.375, 0.835744484228579, 0.8618864774876677, 1.0, 1.0, 1.0, 0.2597326179776399], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.38539276], dtype=float32), -0.3936376]. 
=============================================
[2019-04-09 15:12:59,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9273664e-05 5.6302845e-02 1.4455763e-01 1.5424196e-02 3.2859792e-03
 4.9827743e-04 3.3247769e-01 1.1179262e-02 6.5656327e-02 1.6715053e-02
 3.5381350e-01], sum to 1.0000
[2019-04-09 15:12:59,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6615
[2019-04-09 15:12:59,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00142941 0.09507799 0.1252658  0.0336644  0.01400859 0.00184422
 0.32703802 0.01976174 0.08119418 0.05306584 0.24764982], sum to 1.0000
[2019-04-09 15:12:59,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2932
[2019-04-09 15:12:59,909] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 19.0, 28.12822249898492, 1.273887440526756, 0.0, 1.0, 65.0, 21.87947909637429], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1270200.0000, 
sim time next is 1270800.0000, 
raw observation next is [12.2, 100.0, 9.5, 0.0, 19.0, 28.11103672738536, 1.270436750734171, 0.0, 1.0, 45.0, 22.83135040059707], 
processed observation next is [0.0, 0.7391304347826086, 0.8005540166204987, 1.0, 0.03166666666666667, 0.0, 0.08333333333333333, 0.8425863939487801, 0.9234789169113903, 0.0, 1.0, 0.6, 0.2283135040059707], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.34297043], dtype=float32), 1.0604563]. 
=============================================
[2019-04-09 15:12:59,919] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.44860735914891, 1.040849704693253, 1.0, 1.0, 65.0, 37.54448987469767], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.37088634250747, 1.045777418854311, 1.0, 1.0, 65.0, 36.50946024006456], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.780907195208956, 0.848592472951437, 1.0, 1.0, 1.0, 0.3650946024006456], 
reward next is 0.6349, 
noisyNet noise sample is [array([-0.30749288], dtype=float32), 1.6113815]. 
=============================================
[2019-04-09 15:13:00,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00061279 0.08202394 0.29877064 0.02486016 0.00468519 0.00131456
 0.23366854 0.01732003 0.05468984 0.05499121 0.22706306], sum to 1.0000
[2019-04-09 15:13:00,080] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2668
[2019-04-09 15:13:00,111] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 92.0, 36.0, 0.0, 22.5, 27.79083773531677, 1.035462100217061, 1.0, 1.0, 45.0, 29.01041648196541], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1329000.0000, 
sim time next is 1329600.0000, 
raw observation next is [0.5, 92.0, 40.5, 0.0, 22.5, 27.75187110911536, 1.05161858162118, 1.0, 1.0, 25.0, 27.86157410389971], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.135, 0.0, 0.375, 0.8126559257596133, 0.85053952720706, 1.0, 1.0, 0.2, 0.2786157410389971], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.9899685], dtype=float32), 0.88956875]. 
=============================================
[2019-04-09 15:13:01,183] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1863014e-04 7.9110898e-02 1.1175527e-01 1.5921628e-02 3.8762700e-03
 1.4747794e-03 4.8460633e-01 1.6981235e-02 2.7699461e-02 2.7592819e-02
 2.3056273e-01], sum to 1.0000
[2019-04-09 15:13:01,183] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0333
[2019-04-09 15:13:01,196] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 19.0, 27.39603888485185, 1.039789632416457, 0.0, 1.0, 45.0, 32.97113831325577], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 19.0, 27.37338744730311, 1.036597706837056, 0.0, 1.0, 20.0, 31.2602589799814], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7811156206085924, 0.8455325689456853, 0.0, 1.0, 0.1, 0.31260258979981403], 
reward next is 0.6874, 
noisyNet noise sample is [array([-1.3508972], dtype=float32), 0.9849355]. 
=============================================
[2019-04-09 15:13:01,212] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[8.108723 ]
 [7.943467 ]
 [8.1298685]
 [8.018407 ]
 [8.203483 ]], R is [[ 8.56719685]
 [ 9.15181446]
 [ 9.74027729]
 [10.36019516]
 [10.93433094]].
[2019-04-09 15:13:01,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00060313 0.06189991 0.1277196  0.0231428  0.0049756  0.00083305
 0.1783576  0.02229355 0.04381981 0.02783024 0.5085247 ], sum to 1.0000
[2019-04-09 15:13:01,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5358
[2019-04-09 15:13:01,629] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 92.0, 0.0, 0.0, 19.0, 27.38890590814184, 1.062769003490811, 0.0, 1.0, 45.0, 30.32491176974132], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [2.9, 92.0, 0.0, 0.0, 19.0, 27.41986249229931, 1.065223956182171, 0.0, 1.0, 65.0, 35.37289888030354], 
processed observation next is [1.0, 0.08695652173913043, 0.5429362880886427, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7849885410249424, 0.8550746520607237, 0.0, 1.0, 1.0, 0.3537289888030354], 
reward next is 0.6463, 
noisyNet noise sample is [array([0.97420746], dtype=float32), -1.1683855]. 
=============================================
[2019-04-09 15:13:01,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00078592 0.09334241 0.22493348 0.03550994 0.0045973  0.0020111
 0.22243683 0.02075326 0.03445754 0.0427833  0.31838897], sum to 1.0000
[2019-04-09 15:13:01,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4568
[2019-04-09 15:13:01,812] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.45, 92.0, 0.0, 0.0, 19.0, 27.42778864596633, 1.057410576925563, 0.0, 1.0, 25.0, 32.12721707644187], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1308600.0000, 
sim time next is 1309200.0000, 
raw observation next is [2.366666666666667, 92.0, 0.0, 0.0, 19.0, 27.4476823753965, 1.048112005612671, 0.0, 1.0, 45.0, 28.954135420501], 
processed observation next is [1.0, 0.13043478260869565, 0.528162511542013, 0.92, 0.0, 0.0, 0.08333333333333333, 0.787306864616375, 0.8493706685375569, 0.0, 1.0, 0.6, 0.28954135420501], 
reward next is 0.7105, 
noisyNet noise sample is [array([0.41116026], dtype=float32), -0.21573204]. 
=============================================
[2019-04-09 15:13:01,886] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00067892 0.08361253 0.07608865 0.01990363 0.00562852 0.00234431
 0.3460016  0.0124281  0.03974451 0.03788397 0.37568516], sum to 1.0000
[2019-04-09 15:13:01,888] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8727
[2019-04-09 15:13:01,904] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 19.0, 27.42231378105971, 1.103364944685041, 0.0, 1.0, 65.0, 35.90228007160329], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 19.0, 27.46093364811191, 1.098270059297419, 0.0, 1.0, 65.0, 33.26988399120193], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 0.08333333333333333, 0.7884111373426593, 0.8660900197658062, 0.0, 1.0, 1.0, 0.3326988399120193], 
reward next is 0.6673, 
noisyNet noise sample is [array([1.036216], dtype=float32), -0.7474188]. 
=============================================
[2019-04-09 15:13:01,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.94524740e-04 5.20670265e-02 1.33163705e-01 1.58993844e-02
 3.83350067e-03 6.25715242e-04 4.24127579e-01 1.24199055e-02
 2.35161055e-02 2.76279077e-02 3.06424707e-01], sum to 1.0000
[2019-04-09 15:13:01,922] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-09 15:13:01,941] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 27.08950380312026, 0.9808957508670436, 0.0, 1.0, 65.0, 38.3923404011414], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1375200.0000, 
sim time next is 1375800.0000, 
raw observation next is [0.4166666666666667, 95.83333333333333, 0.0, 0.0, 19.0, 27.07564871412705, 0.9345751017115789, 0.0, 1.0, 45.0, 42.17885627424068], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.9583333333333333, 0.0, 0.0, 0.08333333333333333, 0.7563040595105873, 0.8115250339038597, 0.0, 1.0, 0.6, 0.4217885627424068], 
reward next is 0.5782, 
noisyNet noise sample is [array([-0.24211183], dtype=float32), 0.65743846]. 
=============================================
[2019-04-09 15:13:02,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00125859 0.10606765 0.07748199 0.01937122 0.00329841 0.00178288
 0.27772963 0.03114287 0.0625423  0.0384883  0.3808362 ], sum to 1.0000
[2019-04-09 15:13:02,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3662
[2019-04-09 15:13:02,136] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 22.5, 26.75609105456122, 0.8125253408310021, 1.0, 1.0, 25.0, 42.44157910720117], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1408800.0000, 
sim time next is 1409400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 22.5, 26.83199934262807, 0.8142378639466109, 0.0, 1.0, 45.0, 30.12178445137027], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.375, 0.7359999452190058, 0.771412621315537, 0.0, 1.0, 0.6, 0.3012178445137027], 
reward next is 0.6988, 
noisyNet noise sample is [array([0.84254473], dtype=float32), 1.5585631]. 
=============================================
[2019-04-09 15:13:02,464] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.24297458e-05 1.36480704e-01 1.30670592e-01 2.23369300e-02
 3.52592906e-03 2.41765782e-04 2.72944450e-01 1.00742625e-02
 3.84614319e-02 2.26744227e-02 3.62507135e-01], sum to 1.0000
[2019-04-09 15:13:02,468] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0963
[2019-04-09 15:13:02,488] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 118.6666666666667, 0.0, 22.5, 28.1332258472345, 1.111850158621952, 1.0, 1.0, 45.0, 20.33894115045792], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1336200.0000, 
sim time next is 1336800.0000, 
raw observation next is [1.1, 92.0, 122.8333333333333, 0.0, 22.5, 28.14632055843714, 1.115765336290562, 1.0, 1.0, 20.0, 21.8114183441274], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.40944444444444433, 0.0, 0.375, 0.8455267132030949, 0.8719217787635207, 1.0, 1.0, 0.1, 0.218114183441274], 
reward next is 0.7819, 
noisyNet noise sample is [array([1.2457418], dtype=float32), -0.81224734]. 
=============================================
[2019-04-09 15:13:02,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7293743e-04 1.0498884e-01 2.2505219e-01 1.2348618e-02 3.2815004e-03
 1.0502008e-03 3.0701846e-01 2.2543304e-02 4.0399656e-02 3.1928856e-02
 2.5121549e-01], sum to 1.0000
[2019-04-09 15:13:02,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6428
[2019-04-09 15:13:02,955] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 117.6666666666667, 0.0, 22.5, 28.12276516910219, 1.123994251234327, 1.0, 1.0, 45.0, 22.66073723373642], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1339800.0000, 
sim time next is 1340400.0000, 
raw observation next is [1.1, 92.0, 115.3333333333333, 0.0, 22.5, 28.12044716185245, 1.117673279969945, 1.0, 1.0, 45.0, 22.86182791870819], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.3844444444444443, 0.0, 0.375, 0.8433705968210375, 0.8725577599899816, 1.0, 1.0, 0.6, 0.2286182791870819], 
reward next is 0.7714, 
noisyNet noise sample is [array([-0.75321853], dtype=float32), -0.73238987]. 
=============================================
[2019-04-09 15:13:03,118] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00050786 0.05465272 0.0961441  0.0157106  0.00347097 0.00137045
 0.29091492 0.01473402 0.02089737 0.02433865 0.47725835], sum to 1.0000
[2019-04-09 15:13:03,122] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9621
[2019-04-09 15:13:03,150] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.77002808056763, 0.8184621892905527, 0.0, 1.0, 25.0, 33.52176309922266], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1407600.0000, 
sim time next is 1408200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 22.5, 26.83312828192088, 0.8179285156229258, 0.0, 1.0, 65.0, 39.04045458873151], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.375, 0.7360940234934068, 0.7726428385409753, 0.0, 1.0, 1.0, 0.39040454588731505], 
reward next is 0.6096, 
noisyNet noise sample is [array([-1.430261], dtype=float32), -1.2910284]. 
=============================================
[2019-04-09 15:13:03,704] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0324257e-04 9.4338946e-02 8.8656798e-02 2.0266216e-02 1.4593727e-03
 3.9985325e-04 2.1863450e-01 1.2029450e-02 4.3081768e-02 3.5049472e-02
 4.8598048e-01], sum to 1.0000
[2019-04-09 15:13:03,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0569
[2019-04-09 15:13:03,741] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.7000000000000001, 92.0, 92.5, 0.0, 22.5, 28.03133776997557, 1.08627574305651, 1.0, 1.0, 65.0, 25.94786779323825], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1333200.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 22.5, 28.05559782604098, 1.09165179988162, 1.0, 1.0, 20.0, 26.25405504353828], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.375, 0.837966485503415, 0.8638839332938734, 1.0, 1.0, 0.1, 0.2625405504353828], 
reward next is 0.7375, 
noisyNet noise sample is [array([-0.26610565], dtype=float32), 1.0231493]. 
=============================================
[2019-04-09 15:13:03,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.22209555e-04 9.97671187e-02 9.96608511e-02 1.51144955e-02
 2.59929243e-03 6.00149040e-04 3.21440428e-01 1.06636900e-02
 4.14671302e-02 4.49904278e-02 3.63374263e-01], sum to 1.0000
[2019-04-09 15:13:03,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8432
[2019-04-09 15:13:03,758] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 27.03823228439438, 0.8803682122289361, 0.0, 1.0, 65.0, 40.75098595719628], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1384800.0000, 
sim time next is 1385400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 27.06525871545972, 0.8701234822896984, 0.0, 1.0, 65.0, 38.78726431508464], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.75543822628831, 0.7900411607632328, 0.0, 1.0, 1.0, 0.3878726431508464], 
reward next is 0.6121, 
noisyNet noise sample is [array([0.35367042], dtype=float32), 0.13461302]. 
=============================================
[2019-04-09 15:13:03,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9253921e-05 6.5201007e-02 2.3387626e-01 1.1053286e-02 4.7318451e-03
 2.9338221e-04 1.7088082e-01 8.7209987e-03 6.4990759e-02 1.8712932e-02
 4.2145953e-01], sum to 1.0000
[2019-04-09 15:13:03,910] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3958
[2019-04-09 15:13:03,931] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 22.5, 28.15745021416955, 1.119641250434274, 1.0, 1.0, 65.0, 24.93857328629275], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1337400.0000, 
sim time next is 1338000.0000, 
raw observation next is [1.1, 92.0, 124.6666666666667, 0.0, 22.5, 28.16724785574546, 1.114088956536271, 1.0, 1.0, 45.0, 23.28209972285388], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4155555555555557, 0.0, 0.375, 0.8472706546454551, 0.8713629855120905, 1.0, 1.0, 0.6, 0.23282099722853877], 
reward next is 0.7672, 
noisyNet noise sample is [array([1.3799412], dtype=float32), -0.8072476]. 
=============================================
[2019-04-09 15:13:03,950] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[10.506298]
 [10.161409]
 [10.508934]
 [10.113564]
 [10.257282]], R is [[10.89990044]
 [11.54151535]
 [12.18898773]
 [12.81468678]
 [13.4420042 ]].
[2019-04-09 15:13:04,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6118089e-05 5.3563993e-02 1.3924775e-01 9.3303192e-03 2.4824713e-03
 2.4666148e-04 4.9164984e-01 9.3297698e-03 2.1519376e-02 1.8987540e-02
 2.5357625e-01], sum to 1.0000
[2019-04-09 15:13:04,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0505
[2019-04-09 15:13:04,054] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4979609e-04 6.2958956e-02 9.9467069e-02 9.6995495e-03 2.4578085e-03
 1.7333253e-04 2.2999673e-01 1.6900340e-02 2.9755352e-02 2.9910181e-02
 5.1853085e-01], sum to 1.0000
[2019-04-09 15:13:04,055] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1757
[2019-04-09 15:13:04,060] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8, 94.5, 18.0, 0.0, 22.5, 28.15106321041299, 1.123894200932481, 1.0, 1.0, 45.0, 29.78461605500399], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1355400.0000, 
sim time next is 1356000.0000, 
raw observation next is [0.7000000000000001, 95.0, 15.0, 0.0, 22.5, 28.01971471996933, 1.134167903049421, 1.0, 1.0, 20.0, 29.2672016545043], 
processed observation next is [1.0, 0.6956521739130435, 0.4819944598337951, 0.95, 0.05, 0.0, 0.375, 0.8349762266641108, 0.8780559676831402, 1.0, 1.0, 0.1, 0.292672016545043], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.81860626], dtype=float32), 1.1951152]. 
=============================================
[2019-04-09 15:13:04,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.2857438e-05 7.2250962e-02 7.8835078e-02 1.3861434e-02 1.8928156e-03
 2.8218364e-04 4.8037764e-01 9.6504083e-03 2.3682674e-02 2.7202006e-02
 2.9189190e-01], sum to 1.0000
[2019-04-09 15:13:04,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2013
[2019-04-09 15:13:04,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[10.5633335]
 [10.704593 ]
 [10.689347 ]
 [10.682527 ]
 [10.625062 ]], R is [[11.03870201]
 [11.63046932]
 [12.24168873]
 [12.88766766]
 [13.54067612]].
[2019-04-09 15:13:04,089] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 22.5, 27.99493489741594, 1.109782309237979, 1.0, 1.0, 45.0, 25.3594823271711], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 22.5, 27.96080099872992, 1.120323010411179, 1.0, 1.0, 45.0, 25.14833946705446], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 0.375, 0.83006674989416, 0.873441003470393, 1.0, 1.0, 0.6, 0.2514833946705446], 
reward next is 0.7485, 
noisyNet noise sample is [array([2.5785415], dtype=float32), -0.008141108]. 
=============================================
[2019-04-09 15:13:04,091] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.19720502327532, 1.011353486211229, 1.0, 1.0, 20.0, 30.96439669768285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.17030097400218, 1.00720673745883, 1.0, 1.0, 45.0, 32.87644883539944], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7641917478335151, 0.8357355791529434, 1.0, 1.0, 0.6, 0.32876448835399436], 
reward next is 0.6712, 
noisyNet noise sample is [array([-1.4821658], dtype=float32), 0.79463196]. 
=============================================
[2019-04-09 15:13:04,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[10.888381 ]
 [11.039079 ]
 [10.9602785]
 [10.992837 ]
 [10.793347 ]], R is [[11.11467552]
 [11.69388485]
 [12.19971561]
 [12.69567585]
 [13.28344917]].
[2019-04-09 15:13:04,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.06968058e-05 7.09779635e-02 7.63107166e-02 1.44709125e-02
 1.90710253e-03 3.02588916e-04 2.21687019e-01 1.07539613e-02
 3.10087148e-02 2.46706158e-02 5.47839642e-01], sum to 1.0000
[2019-04-09 15:13:04,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5320
[2019-04-09 15:13:04,239] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 122.3333333333333, 0.0, 22.5, 28.11789884028083, 1.119933258058918, 1.0, 1.0, 65.0, 27.68584556656687], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 22.5, 28.1265626174587, 1.121044223902401, 1.0, 1.0, 65.0, 25.31464433874187], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.375, 0.8438802181215582, 0.873681407967467, 1.0, 1.0, 1.0, 0.2531464433874187], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.66777915], dtype=float32), -0.7625162]. 
=============================================
[2019-04-09 15:13:04,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00070809 0.07691759 0.19696963 0.02340635 0.00919021 0.00138101
 0.25941148 0.02371611 0.03966016 0.03334007 0.3352993 ], sum to 1.0000
[2019-04-09 15:13:04,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7989
[2019-04-09 15:13:04,426] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.93087276824191, 0.8330951716265015, 0.0, 1.0, 65.0, 38.93899283210249], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1397400.0000, 
sim time next is 1398000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.92288523628326, 0.830659002357674, 0.0, 1.0, 65.0, 40.37586299983671], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7435737696902717, 0.7768863341192246, 0.0, 1.0, 1.0, 0.40375862999836715], 
reward next is 0.5962, 
noisyNet noise sample is [array([1.8686581], dtype=float32), 0.7449229]. 
=============================================
[2019-04-09 15:13:04,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[8.372972]
 [8.218677]
 [8.441711]
 [8.444675]
 [8.360843]], R is [[ 8.83657742]
 [ 9.35882187]
 [ 9.84146118]
 [10.41402626]
 [10.93352032]].
[2019-04-09 15:13:04,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5139318e-04 8.4539205e-02 1.4936508e-01 1.5914677e-02 2.6662180e-03
 3.5219511e-04 3.4424052e-01 9.9955294e-03 2.9110655e-02 2.7364476e-02
 3.3630002e-01], sum to 1.0000
[2019-04-09 15:13:04,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8027
[2019-04-09 15:13:04,500] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 90.0, 0.0, 22.5, 27.62653900821001, 0.9725432938724925, 1.0, 1.0, 45.0, 25.01337580219356], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1423800.0000, 
sim time next is 1424400.0000, 
raw observation next is [0.0, 95.0, 91.0, 0.0, 22.5, 27.65798859108013, 0.9740328831480078, 1.0, 1.0, 65.0, 26.85321587128864], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30333333333333334, 0.0, 0.375, 0.8048323825900109, 0.8246776277160026, 1.0, 1.0, 1.0, 0.2685321587128864], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.62298596], dtype=float32), 1.7429326]. 
=============================================
[2019-04-09 15:13:04,629] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4414421e-04 6.2123794e-02 1.9720399e-01 2.2993762e-02 3.5725692e-03
 6.6302600e-04 2.4840760e-01 2.1278819e-02 3.8352318e-02 2.8650539e-02
 3.7660944e-01], sum to 1.0000
[2019-04-09 15:13:04,632] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9186
[2019-04-09 15:13:04,646] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.38840981049188, 1.037955319589395, 1.0, 1.0, 20.0, 31.59338932640697], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1364400.0000, 
sim time next is 1365000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.34554347549358, 1.041205155139376, 1.0, 1.0, 65.0, 36.16273685506101], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7787952896244651, 0.8470683850464585, 1.0, 1.0, 1.0, 0.3616273685506101], 
reward next is 0.6384, 
noisyNet noise sample is [array([1.2535979], dtype=float32), -0.71033156]. 
=============================================
[2019-04-09 15:13:04,661] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6916385e-04 9.7545423e-02 1.3333692e-01 1.3364475e-02 2.4355529e-03
 3.1613087e-04 2.4246192e-01 9.2891250e-03 8.1202425e-02 3.9683547e-02
 3.8019535e-01], sum to 1.0000
[2019-04-09 15:13:04,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1824
[2019-04-09 15:13:04,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[10.638894]
 [10.763599]
 [10.943486]
 [11.038289]
 [10.88687 ]], R is [[11.59164715]
 [12.15979671]
 [12.67310333]
 [13.22062874]
 [13.77568054]].
[2019-04-09 15:13:04,684] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 92.0, 63.33333333333334, 0.0, 22.5, 27.77247508339757, 0.9860833679468942, 1.0, 1.0, 45.0, 27.7731352034152], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1435800.0000, 
sim time next is 1436400.0000, 
raw observation next is [1.1, 92.0, 59.0, 0.0, 22.5, 27.81005488385932, 0.9919528454063372, 1.0, 1.0, 25.0, 19.86347924618236], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19666666666666666, 0.0, 0.375, 0.8175045736549432, 0.8306509484687791, 1.0, 1.0, 0.2, 0.1986347924618236], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.27663794], dtype=float32), -1.0094236]. 
=============================================
[2019-04-09 15:13:04,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5618480e-04 1.0208463e-01 1.5294462e-01 6.5482040e-03 1.9192199e-03
 4.3807499e-04 2.4372248e-01 9.0833688e-03 3.0130854e-02 3.5226554e-02
 4.1774571e-01], sum to 1.0000
[2019-04-09 15:13:04,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0210
[2019-04-09 15:13:04,716] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 100.1666666666667, 0.0, 22.5, 28.04575642060369, 1.130065129762864, 1.0, 1.0, 45.0, 25.36432477116936], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [1.1, 92.0, 94.33333333333333, 0.0, 22.5, 28.08690354003903, 1.136526690872776, 1.0, 1.0, 20.0, 21.29610606574303], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.3144444444444444, 0.0, 0.375, 0.8405752950032527, 0.8788422302909252, 1.0, 1.0, 0.1, 0.21296106065743028], 
reward next is 0.7870, 
noisyNet noise sample is [array([0.77750844], dtype=float32), -0.68150485]. 
=============================================
[2019-04-09 15:13:04,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7536673e-04 9.3167245e-02 9.0994239e-02 1.0789025e-02 3.5503283e-03
 6.0714490e-04 3.5329780e-01 1.0329875e-02 4.2523682e-02 2.9927680e-02
 3.6463767e-01], sum to 1.0000
[2019-04-09 15:13:04,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8458
[2019-04-09 15:13:04,753] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 27.00661619435591, 0.9060287092206828, 0.0, 1.0, 50.0, 30.89139488364586], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 19.0, 26.99510319989021, 0.9050311851202318, 0.0, 1.0, 65.0, 36.81622104649971], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 0.08333333333333333, 0.7495919333241842, 0.8016770617067439, 0.0, 1.0, 1.0, 0.36816221046499714], 
reward next is 0.6318, 
noisyNet noise sample is [array([0.03520064], dtype=float32), -0.9676593]. 
=============================================
[2019-04-09 15:13:04,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8995444e-04 1.6463993e-01 1.3993640e-01 2.0988638e-02 2.3733634e-03
 2.9587842e-04 2.6468199e-01 7.6148487e-03 2.9092744e-02 2.4828164e-02
 3.4535807e-01], sum to 1.0000
[2019-04-09 15:13:04,868] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8728
[2019-04-09 15:13:04,880] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 91.0, 0.0, 22.5, 27.65841711684594, 0.9743822591440741, 1.0, 1.0, 20.0, 23.05396166595996], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1424400.0000, 
sim time next is 1425000.0000, 
raw observation next is [0.0, 95.0, 92.0, 0.0, 22.5, 27.66352449598358, 0.981616097724468, 1.0, 1.0, 65.0, 33.84778238983486], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30666666666666664, 0.0, 0.375, 0.8052937079986316, 0.827205365908156, 1.0, 1.0, 1.0, 0.33847782389834863], 
reward next is 0.6615, 
noisyNet noise sample is [array([0.10497691], dtype=float32), -1.5939325]. 
=============================================
[2019-04-09 15:13:04,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[10.11538 ]
 [10.186238]
 [10.050174]
 [ 9.968894]
 [10.032636]], R is [[10.78232956]
 [11.44396687]
 [12.06791019]
 [12.70223618]
 [13.31349659]].
[2019-04-09 15:13:04,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2771779e-04 5.5860970e-02 1.2946674e-01 1.1309369e-02 1.1635965e-03
 3.0716666e-04 2.2674249e-01 1.1874915e-02 2.0309942e-02 3.7934352e-02
 5.0490278e-01], sum to 1.0000
[2019-04-09 15:13:04,992] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7578
[2019-04-09 15:13:05,008] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.4166666666666667, 95.83333333333333, 0.0, 0.0, 19.0, 27.07777673737442, 0.8976396739748101, 0.0, 1.0, 20.0, 42.71249549690962], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1375800.0000, 
sim time next is 1376400.0000, 
raw observation next is [0.3333333333333334, 95.66666666666666, 0.0, 0.0, 19.0, 27.05526693299237, 0.9000310203859714, 0.0, 1.0, 65.0, 48.33514340926545], 
processed observation next is [1.0, 0.9565217391304348, 0.4718374884579871, 0.9566666666666666, 0.0, 0.0, 0.08333333333333333, 0.7546055777493642, 0.8000103401286571, 0.0, 1.0, 1.0, 0.48335143409265446], 
reward next is 0.5166, 
noisyNet noise sample is [array([1.6869236], dtype=float32), -0.13720988]. 
=============================================
[2019-04-09 15:13:05,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.83168493e-04 5.07730097e-02 1.33886799e-01 1.21973315e-02
 1.35472917e-03 4.25850070e-04 2.23694965e-01 1.32725332e-02
 2.04407163e-02 3.75270247e-02 5.06243944e-01], sum to 1.0000
[2019-04-09 15:13:05,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8369
[2019-04-09 15:13:05,049] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333334, 95.66666666666666, 0.0, 0.0, 19.0, 27.05526693299237, 0.9000310203859714, 0.0, 1.0, 65.0, 48.33514340926545], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1376400.0000, 
sim time next is 1377000.0000, 
raw observation next is [0.25, 95.5, 0.0, 0.0, 19.0, 26.98758162523244, 0.8959441777302488, 0.0, 1.0, 65.0, 44.08624531961215], 
processed observation next is [1.0, 0.9565217391304348, 0.46952908587257625, 0.955, 0.0, 0.0, 0.08333333333333333, 0.7489651354360367, 0.7986480592434163, 0.0, 1.0, 1.0, 0.44086245319612155], 
reward next is 0.5591, 
noisyNet noise sample is [array([1.6869236], dtype=float32), -0.13720988]. 
=============================================
[2019-04-09 15:13:05,075] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[ 9.792115]
 [10.056256]
 [ 9.835732]
 [10.124827]
 [10.11434 ]], R is [[10.16839123]
 [10.5833559 ]
 [11.05039787]
 [11.60735226]
 [12.18321228]].
[2019-04-09 15:13:05,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8679095e-05 4.8887271e-02 2.1882296e-01 9.7159119e-03 2.2969253e-03
 2.0375766e-04 2.4130102e-01 1.5741581e-02 3.1480778e-02 2.5612624e-02
 4.0584844e-01], sum to 1.0000
[2019-04-09 15:13:05,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-09 15:13:05,247] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 67.66666666666667, 0.0, 22.5, 27.09573468817008, 0.997866701851403, 1.0, 1.0, 45.0, 27.90034779844071], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1435200.0000, 
sim time next is 1435800.0000, 
raw observation next is [1.1, 92.0, 63.33333333333334, 0.0, 22.5, 26.57106675329173, 0.8957306039703871, 1.0, 1.0, 65.0, 7.256575409791125], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.21111111111111114, 0.0, 0.375, 0.7142555627743109, 0.798576867990129, 1.0, 1.0, 1.0, 0.07256575409791126], 
reward next is 0.9274, 
noisyNet noise sample is [array([0.35933307], dtype=float32), 0.94802713]. 
=============================================
[2019-04-09 15:13:05,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.4524279e-05 6.2543415e-02 9.5525682e-02 4.8429002e-03 2.0530000e-03
 1.4036421e-04 2.8543535e-01 1.1751621e-02 2.0533673e-02 2.3076758e-02
 4.9400267e-01], sum to 1.0000
[2019-04-09 15:13:05,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0763
[2019-04-09 15:13:05,388] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.67593105385992, 1.082264524656209, 1.0, 1.0, 65.0, 31.18713937752099], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1361400.0000, 
sim time next is 1362000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.71030478599859, 1.063780057679715, 1.0, 1.0, 25.0, 31.51958524325386], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.8091920654998827, 0.8545933525599049, 1.0, 1.0, 0.2, 0.3151958524325386], 
reward next is 0.6848, 
noisyNet noise sample is [array([0.02128049], dtype=float32), -0.36238685]. 
=============================================
[2019-04-09 15:13:05,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[10.81459 ]
 [10.618341]
 [10.590782]
 [10.793934]
 [10.81782 ]], R is [[11.2292614 ]
 [11.80509758]
 [12.43462849]
 [13.03579521]
 [13.62248516]].
[2019-04-09 15:13:05,732] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00067799 0.09342349 0.12000768 0.01156626 0.00756535 0.00188169
 0.2645475  0.01960033 0.05291925 0.04868525 0.3791253 ], sum to 1.0000
[2019-04-09 15:13:05,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9799
[2019-04-09 15:13:05,754] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 19.0, 26.93135747117278, 0.8355078428529504, 0.0, 1.0, 65.0, 38.67925455002123], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1476000.0000, 
sim time next is 1476600.0000, 
raw observation next is [2.2, 92.33333333333334, 0.0, 0.0, 19.0, 26.93183743342638, 0.8419778537955741, 0.0, 1.0, 65.0, 36.65853911137616], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9233333333333335, 0.0, 0.0, 0.08333333333333333, 0.7443197861188651, 0.7806592845985247, 0.0, 1.0, 1.0, 0.3665853911137616], 
reward next is 0.6334, 
noisyNet noise sample is [array([1.575514], dtype=float32), -1.0267988]. 
=============================================
[2019-04-09 15:13:06,262] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4047109e-04 8.1333682e-02 1.8646544e-01 8.1693176e-03 1.4659559e-03
 3.4965805e-04 2.9512474e-01 1.5870964e-02 6.8630077e-02 2.6923809e-02
 3.1552589e-01], sum to 1.0000
[2019-04-09 15:13:06,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8930
[2019-04-09 15:13:06,275] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5797790e-04 5.5563487e-02 1.5630428e-01 1.3255597e-02 2.4816596e-03
 4.8576584e-04 2.3572271e-01 1.9340470e-02 4.2838454e-02 2.4000620e-02
 4.4964898e-01], sum to 1.0000
[2019-04-09 15:13:06,283] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.2197501560999, 0.9361859540299284, 1.0, 1.0, 65.0, 34.56309886992565], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1451400.0000, 
sim time next is 1452000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.2121183326531, 0.9290299943710821, 1.0, 1.0, 65.0, 33.14568373694115], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7676765277210919, 0.8096766647903607, 1.0, 1.0, 1.0, 0.3314568373694115], 
reward next is 0.6685, 
noisyNet noise sample is [array([-0.9976329], dtype=float32), -0.97453934]. 
=============================================
[2019-04-09 15:13:06,286] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1671
[2019-04-09 15:13:06,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.636104 ]
 [10.377101 ]
 [10.6665745]
 [10.705132 ]
 [10.770901 ]], R is [[11.13581467]
 [11.67882633]
 [12.23020554]
 [12.84306622]
 [13.41672802]].
[2019-04-09 15:13:06,306] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 19.0, 26.85932430116723, 0.8486716973872858, 0.0, 1.0, 65.0, 40.01313693305592], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1464600.0000, 
sim time next is 1465200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.85148664464535, 0.8502176416308207, 0.0, 1.0, 25.0, 35.24315980997038], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7376238870537793, 0.7834058805436069, 0.0, 1.0, 0.2, 0.3524315980997038], 
reward next is 0.6476, 
noisyNet noise sample is [array([0.777537], dtype=float32), -0.30358598]. 
=============================================
[2019-04-09 15:13:06,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00070727 0.08814026 0.1850099  0.01882766 0.00527662 0.00091604
 0.2296995  0.02556006 0.02961945 0.0648304  0.35141277], sum to 1.0000
[2019-04-09 15:13:06,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1665
[2019-04-09 15:13:06,359] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.6, 100.0, 12.0, 0.0, 22.5, 26.8406492728298, 0.8243732687518994, 1.0, 1.0, 55.0, 35.16664005655385], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1411800.0000, 
sim time next is 1412400.0000, 
raw observation next is [-0.6, 100.0, 15.0, 0.0, 22.5, 26.83586812326574, 0.865351918192745, 1.0, 1.0, 25.0, 26.51060783463633], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.05, 0.0, 0.375, 0.7363223436054783, 0.7884506393975816, 1.0, 1.0, 0.2, 0.2651060783463633], 
reward next is 0.7349, 
noisyNet noise sample is [array([-0.7916158], dtype=float32), 1.7378013]. 
=============================================
[2019-04-09 15:13:07,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00039102 0.06400879 0.20482112 0.02020292 0.0040201  0.00051135
 0.33192417 0.0224924  0.03756304 0.02957369 0.28449145], sum to 1.0000
[2019-04-09 15:13:07,031] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1154
[2019-04-09 15:13:07,046] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.7, 100.0, 37.33333333333334, 0.0, 22.5, 27.37831933487996, 0.8733318222548436, 1.0, 1.0, 25.0, 23.18285939175997], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1501800.0000, 
sim time next is 1502400.0000, 
raw observation next is [1.8, 100.0, 42.16666666666667, 0.0, 22.5, 27.39582020409919, 0.8951714964668015, 1.0, 1.0, 25.0, 25.60787647154763], 
processed observation next is [1.0, 0.391304347826087, 0.5124653739612189, 1.0, 0.14055555555555557, 0.0, 0.375, 0.782985017008266, 0.7983904988222671, 1.0, 1.0, 0.2, 0.2560787647154763], 
reward next is 0.7439, 
noisyNet noise sample is [array([-1.6191735], dtype=float32), -1.3796531]. 
=============================================
[2019-04-09 15:13:08,143] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7897385e-05 1.4391267e-01 1.2319084e-01 1.8678397e-02 2.7456812e-03
 1.4707186e-04 2.5566474e-01 1.7695846e-02 6.3368142e-02 3.7883535e-02
 3.3665520e-01], sum to 1.0000
[2019-04-09 15:13:08,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3100
[2019-04-09 15:13:08,162] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 91.33333333333334, 5.999999999999998, 0.0, 22.5, 27.82654791057722, 0.9812791721794918, 1.0, 1.0, 45.0, 22.48174815755416], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1444200.0000, 
sim time next is 1444800.0000, 
raw observation next is [1.1, 90.66666666666667, 0.0, 0.0, 22.5, 27.85388197695928, 0.9381602771035258, 1.0, 1.0, 45.0, 40.94304991379384], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.9066666666666667, 0.0, 0.0, 0.375, 0.8211568314132732, 0.8127200923678419, 1.0, 1.0, 0.6, 0.40943049913793844], 
reward next is 0.5906, 
noisyNet noise sample is [array([-0.5629648], dtype=float32), 1.8768191]. 
=============================================
[2019-04-09 15:13:08,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00058899 0.10731205 0.1714853  0.0380732  0.00285538 0.00104519
 0.22395825 0.03139121 0.03783461 0.04585919 0.33959654], sum to 1.0000
[2019-04-09 15:13:08,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9644
[2019-04-09 15:13:08,363] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333334, 97.33333333333333, 0.0, 0.0, 19.0, 26.87907566682561, 0.8130967033927465, 0.0, 1.0, 55.0, 36.72579827208217], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1491600.0000, 
sim time next is 1492200.0000, 
raw observation next is [1.65, 98.0, 0.0, 0.0, 19.0, 26.94566916511018, 0.8153314694015631, 0.0, 1.0, 65.0, 36.19190288200917], 
processed observation next is [1.0, 0.2608695652173913, 0.5083102493074793, 0.98, 0.0, 0.0, 0.08333333333333333, 0.7454724304258482, 0.7717771564671877, 0.0, 1.0, 1.0, 0.3619190288200917], 
reward next is 0.6381, 
noisyNet noise sample is [array([1.844785], dtype=float32), -1.5133451]. 
=============================================
[2019-04-09 15:13:08,455] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1801980e-05 7.9203002e-02 1.0420490e-01 1.6353397e-02 2.9060838e-03
 2.2435178e-04 4.1087097e-01 1.2651710e-02 3.6297999e-02 3.8454656e-02
 2.9876110e-01], sum to 1.0000
[2019-04-09 15:13:08,458] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3488
[2019-04-09 15:13:08,492] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 63.33333333333334, 0.0, 22.5, 26.53148553353826, 0.8911101607073989, 1.0, 1.0, 45.0, 7.835113955997516], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1435800.0000, 
sim time next is 1436400.0000, 
raw observation next is [1.1, 92.0, 59.0, 0.0, 22.5, 27.67973685070578, 0.9752328643696672, 1.0, 1.0, 45.0, 26.57379798935327], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19666666666666666, 0.0, 0.375, 0.8066447375588149, 0.8250776214565557, 1.0, 1.0, 0.6, 0.2657379798935327], 
reward next is 0.7343, 
noisyNet noise sample is [array([-1.251918], dtype=float32), -0.36941934]. 
=============================================
[2019-04-09 15:13:08,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4617619e-04 4.3642938e-02 1.3723101e-01 1.4798940e-02 3.5814722e-03
 4.0445986e-04 3.8479662e-01 1.2826119e-02 2.8660955e-02 2.3894502e-02
 3.5001680e-01], sum to 1.0000
[2019-04-09 15:13:08,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1645
[2019-04-09 15:13:08,784] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 19.0, 27.79767995495008, 1.123583614966323, 0.0, 1.0, 65.0, 25.44163312196769], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1547400.0000, 
sim time next is 1548000.0000, 
raw observation next is [6.6, 76.0, 0.0, 0.0, 19.0, 27.76377682729495, 1.118432556651367, 0.0, 1.0, 25.0, 24.18630932969019], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.76, 0.0, 0.0, 0.08333333333333333, 0.8136480689412459, 0.8728108522171224, 0.0, 1.0, 0.2, 0.2418630932969019], 
reward next is 0.7581, 
noisyNet noise sample is [array([-0.34334853], dtype=float32), -0.8223099]. 
=============================================
[2019-04-09 15:13:08,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[9.593807]
 [9.360268]
 [9.768027]
 [9.553633]
 [9.749065]], R is [[ 9.85791302]
 [10.50491714]
 [11.17659187]
 [11.80821991]
 [12.47556591]].
[2019-04-09 15:13:09,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4780575e-05 8.7223619e-02 1.4016619e-01 2.0717978e-02 2.5467367e-03
 5.9324951e-04 2.0720439e-01 1.2985744e-02 6.7078426e-02 2.7649233e-02
 4.3375960e-01], sum to 1.0000
[2019-04-09 15:13:09,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5673
[2019-04-09 15:13:09,213] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 78.0, 0.0, 22.5, 27.77064975990095, 0.993922704863543, 1.0, 1.0, 45.0, 23.81151713934463], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [1.1, 92.0, 75.0, 0.0, 22.5, 27.78286292298631, 0.8358699895395558, 1.0, 1.0, 65.0, 60.71691240344431], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.25, 0.0, 0.375, 0.8152385769155259, 0.7786233298465186, 1.0, 1.0, 1.0, 0.6071691240344431], 
reward next is 0.3928, 
noisyNet noise sample is [array([0.58653796], dtype=float32), 0.8782755]. 
=============================================
[2019-04-09 15:13:09,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[11.07628  ]
 [11.138652 ]
 [11.011295 ]
 [10.8756275]
 [11.10858  ]], R is [[11.53697491]
 [12.1834898 ]
 [12.8214016 ]
 [13.41612816]
 [14.06504917]].
[2019-04-09 15:13:09,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00044926 0.05361823 0.12628913 0.01461964 0.00305534 0.00067882
 0.3830654  0.01618384 0.0215134  0.03646015 0.3440668 ], sum to 1.0000
[2019-04-09 15:13:09,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5566
[2019-04-09 15:13:09,304] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.41265818948946, 1.022492097767491, 0.0, 1.0, 45.0, 26.36218871910862], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1558200.0000, 
sim time next is 1558800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.48881574156568, 1.006804285437922, 0.0, 1.0, 45.0, 32.33513907939852], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7907346451304734, 0.8356014284793073, 0.0, 1.0, 0.6, 0.3233513907939852], 
reward next is 0.6766, 
noisyNet noise sample is [array([0.14415617], dtype=float32), 0.55763555]. 
=============================================
[2019-04-09 15:13:09,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.35689425e-05 7.20689073e-02 1.20368555e-01 1.77463461e-02
 1.97236263e-03 3.51704861e-04 4.28758174e-01 9.23862681e-03
 3.18369046e-02 2.16658581e-02 2.95909017e-01], sum to 1.0000
[2019-04-09 15:13:09,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0276
[2019-04-09 15:13:09,404] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 22.5, 27.90447531012763, 0.9825628717328784, 1.0, 1.0, 25.0, 24.22834834519356], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 22.5, 27.79671388533468, 0.9921523212151225, 1.0, 1.0, 20.0, 26.28434932029999], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.375, 0.8163928237778899, 0.8307174404050408, 1.0, 1.0, 0.1, 0.26284349320299993], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.76239663], dtype=float32), 0.7199326]. 
=============================================
[2019-04-09 15:13:09,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00046542 0.0544106  0.1786235  0.03259442 0.00437106 0.00125563
 0.28518704 0.01669795 0.03764802 0.04624965 0.34249666], sum to 1.0000
[2019-04-09 15:13:09,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0007
[2019-04-09 15:13:09,540] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 19.0, 27.3765651888944, 0.9961580854133265, 0.0, 1.0, 45.0, 29.22439704127196], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 19.0, 27.37973343511591, 0.996090704604769, 0.0, 1.0, 20.0, 27.89289592477853], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 0.08333333333333333, 0.7816444529263258, 0.8320302348682563, 0.0, 1.0, 0.1, 0.27892895924778527], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.8278337], dtype=float32), 0.540187]. 
=============================================
[2019-04-09 15:13:09,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[8.532619]
 [8.662331]
 [8.879936]
 [8.749571]
 [8.782415]], R is [[ 9.14857197]
 [ 9.76484203]
 [10.35439968]
 [10.91790295]
 [11.56469345]].
[2019-04-09 15:13:09,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4612789e-05 8.3302073e-02 7.4684829e-02 8.5607432e-03 1.1728486e-03
 2.0125230e-04 3.6486861e-01 6.7282761e-03 5.0805144e-02 3.3199959e-02
 3.7643158e-01], sum to 1.0000
[2019-04-09 15:13:09,822] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6975
[2019-04-09 15:13:09,871] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 22.5, 28.33886341839797, 1.180028085175352, 1.0, 1.0, 45.0, 7.651108140267252], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 22.5, 28.48618149017074, 1.112667143850162, 1.0, 1.0, 65.0, 58.19163872124425], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.375, 0.8738484575142283, 0.8708890479500541, 1.0, 1.0, 1.0, 0.5819163872124425], 
reward next is 0.4181, 
noisyNet noise sample is [array([0.49745628], dtype=float32), -0.50281626]. 
=============================================
[2019-04-09 15:13:09,878] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[10.835647]
 [10.548033]
 [10.721039]
 [10.576942]
 [10.613609]], R is [[11.08272457]
 [11.89538574]
 [12.67178535]
 [13.38476944]
 [14.15129089]].
[2019-04-09 15:13:09,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8658775e-04 7.7800177e-02 1.0319741e-01 1.4347155e-02 2.5160639e-03
 7.3769287e-04 3.3481073e-01 1.1582048e-02 4.4559304e-02 3.1565275e-02
 3.7859744e-01], sum to 1.0000
[2019-04-09 15:13:09,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6636
[2019-04-09 15:13:09,917] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.38643830717037, 1.015122354885696, 0.0, 1.0, 20.0, 28.01751097816433], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1557600.0000, 
sim time next is 1558200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.41234068299037, 1.022440593180701, 0.0, 1.0, 65.0, 32.43528035171598], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7843617235825308, 0.8408135310602337, 0.0, 1.0, 1.0, 0.32435280351715984], 
reward next is 0.6756, 
noisyNet noise sample is [array([-0.16332395], dtype=float32), -0.6437823]. 
=============================================
[2019-04-09 15:13:10,120] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4534367e-04 7.8976385e-02 1.0815429e-01 1.8171322e-02 2.8065247e-03
 5.6260329e-04 2.1186106e-01 2.0451315e-02 1.9383375e-02 1.7676182e-02
 5.2181154e-01], sum to 1.0000
[2019-04-09 15:13:10,123] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-09 15:13:10,139] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 19.0, 27.83028451654397, 1.140869699356456, 0.0, 1.0, 25.0, 20.73666619776701], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1543200.0000, 
sim time next is 1543800.0000, 
raw observation next is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 19.0, 27.8065780309622, 1.137533483905468, 0.0, 1.0, 45.0, 21.46891282384209], 
processed observation next is [1.0, 0.8695652173913043, 0.6735918744228995, 0.7383333333333334, 0.0, 0.0, 0.08333333333333333, 0.8172148359135166, 0.8791778279684893, 0.0, 1.0, 0.6, 0.2146891282384209], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.8446493], dtype=float32), 1.3883438]. 
=============================================
[2019-04-09 15:13:10,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8067130e-04 5.6596342e-02 1.4906384e-01 2.8526781e-02 2.2883045e-03
 7.2350097e-04 3.2468265e-01 1.3593401e-02 4.7300339e-02 2.7655248e-02
 3.4938896e-01], sum to 1.0000
[2019-04-09 15:13:10,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1836
[2019-04-09 15:13:10,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.1, 100.0, 55.66666666666666, 0.0, 22.5, 27.70209298092278, 0.9106530597855206, 1.0, 1.0, 25.0, 22.34057513274005], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1504200.0000, 
sim time next is 1504800.0000, 
raw observation next is [2.2, 100.0, 60.0, 0.0, 22.5, 27.72282867553266, 0.9055779708717875, 1.0, 1.0, 25.0, 21.26845095539424], 
processed observation next is [1.0, 0.43478260869565216, 0.5235457063711911, 1.0, 0.2, 0.0, 0.375, 0.8102357229610551, 0.8018593236239292, 1.0, 1.0, 0.2, 0.21268450955394239], 
reward next is 0.7873, 
noisyNet noise sample is [array([-0.38068005], dtype=float32), 0.37548277]. 
=============================================
[2019-04-09 15:13:10,290] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5787626e-04 9.2852280e-02 9.7095057e-02 9.8227868e-03 2.0790934e-03
 4.1020109e-04 3.7167719e-01 8.2782013e-03 4.1072261e-02 1.4417406e-02
 3.6213759e-01], sum to 1.0000
[2019-04-09 15:13:10,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-09 15:13:10,302] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.1, 50.33333333333334, 80.33333333333333, 328.0, 22.5, 28.09716728622229, 1.239559999666833, 1.0, 1.0, 60.0, 1.180630081961136], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1525800.0000, 
sim time next is 1526400.0000, 
raw observation next is [12.2, 50.0, 82.0, 253.0, 22.5, 28.6150325348333, 1.264300785178674, 1.0, 1.0, 45.0, 1.691975541546581], 
processed observation next is [1.0, 0.6956521739130435, 0.8005540166204987, 0.5, 0.2733333333333333, 0.2795580110497238, 0.375, 0.8845860445694417, 0.921433595059558, 1.0, 1.0, 0.6, 0.01691975541546581], 
reward next is 0.9831, 
noisyNet noise sample is [array([0.465282], dtype=float32), 1.649826]. 
=============================================
[2019-04-09 15:13:10,312] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3796834e-05 5.6360245e-02 2.0030418e-01 8.6656325e-03 1.8032095e-03
 1.4079316e-04 3.6993638e-01 7.9231849e-03 3.7846483e-02 3.7094381e-02
 2.7987170e-01], sum to 1.0000
[2019-04-09 15:13:10,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2667
[2019-04-09 15:13:10,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.41666666666667, 58.33333333333334, 30.33333333333333, 13.33333333333333, 22.5, 28.5689977304659, 1.297770106568912, 1.0, 1.0, 25.0, 11.41881835041921], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1530600.0000, 
sim time next is 1531200.0000, 
raw observation next is [10.33333333333333, 58.66666666666667, 0.0, 0.0, 22.5, 28.20752864069239, 1.166435749146566, 1.0, 1.0, 35.0, 10.06006859605274], 
processed observation next is [1.0, 0.7391304347826086, 0.7488457987072946, 0.5866666666666667, 0.0, 0.0, 0.375, 0.850627386724366, 0.8888119163821887, 1.0, 1.0, 0.4, 0.1006006859605274], 
reward next is 0.8994, 
noisyNet noise sample is [array([-0.18577227], dtype=float32), 0.101585716]. 
=============================================
[2019-04-09 15:13:10,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3880759e-04 4.9728751e-02 9.8723263e-02 2.1887803e-02 1.9513145e-03
 1.2471535e-03 2.8474957e-01 1.7170267e-02 2.0678228e-02 2.5069052e-02
 4.7835580e-01], sum to 1.0000
[2019-04-09 15:13:10,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-09 15:13:10,390] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.416666666666667, 79.5, 0.0, 0.0, 19.0, 27.36850231336678, 0.9391998188197267, 0.0, 1.0, 65.0, 33.2711784586571], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1579800.0000, 
sim time next is 1580400.0000, 
raw observation next is [5.5, 79.0, 0.0, 0.0, 19.0, 27.3398808201134, 0.9367967320668819, 0.0, 1.0, 65.0, 33.94613311858325], 
processed observation next is [1.0, 0.30434782608695654, 0.6149584487534627, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7783234016761167, 0.8122655773556273, 0.0, 1.0, 1.0, 0.3394613311858325], 
reward next is 0.6605, 
noisyNet noise sample is [array([1.857878], dtype=float32), -2.1827877]. 
=============================================
[2019-04-09 15:13:10,452] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8629084e-04 3.9191153e-02 1.6390364e-01 1.2312475e-02 3.2831635e-03
 5.7793653e-04 3.1044495e-01 1.6789617e-02 4.5394342e-02 2.8333886e-02
 3.7958255e-01], sum to 1.0000
[2019-04-09 15:13:10,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2404
[2019-04-09 15:13:10,465] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.866666666666667, 80.0, 0.0, 0.0, 19.0, 27.57344602283671, 1.060255922085324, 0.0, 1.0, 65.0, 32.4117551823082], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [5.683333333333334, 81.0, 0.0, 0.0, 19.0, 27.54008800737384, 1.051729769193157, 0.0, 1.0, 45.0, 29.59131975811138], 
processed observation next is [1.0, 0.9565217391304348, 0.6200369344413666, 0.81, 0.0, 0.0, 0.08333333333333333, 0.79500733394782, 0.8505765897310523, 0.0, 1.0, 0.6, 0.2959131975811138], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.19234338], dtype=float32), -0.64416784]. 
=============================================
[2019-04-09 15:13:10,480] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[10.075796]
 [10.043995]
 [10.331269]
 [10.469633]
 [10.456856]], R is [[10.56526375]
 [11.13549328]
 [11.73893166]
 [12.28054333]
 [12.84978294]].
[2019-04-09 15:13:10,747] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.3206348e-04 3.8121115e-02 1.4096244e-01 7.0388759e-03 2.5880036e-03
 4.4439148e-04 2.5574344e-01 1.6964581e-02 2.7422929e-02 4.7108121e-02
 4.6337405e-01], sum to 1.0000
[2019-04-09 15:13:10,752] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2365
[2019-04-09 15:13:10,772] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.34359132605472, 1.012702944187182, 0.0, 1.0, 25.0, 32.860257962726], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1556400.0000, 
sim time next is 1557000.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.34482955107927, 1.016241393514548, 0.0, 1.0, 45.0, 27.75040654492566], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7787357959232724, 0.838747131171516, 0.0, 1.0, 0.6, 0.2775040654492566], 
reward next is 0.7225, 
noisyNet noise sample is [array([-1.7181293], dtype=float32), 1.9867803]. 
=============================================
[2019-04-09 15:13:10,792] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[ 9.807618]
 [10.094203]
 [10.087105]
 [10.25166 ]
 [10.00703 ]], R is [[10.53485298]
 [11.1009016 ]
 [11.64219666]
 [12.2081852 ]
 [12.79212379]].
[2019-04-09 15:13:10,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2605418e-04 1.3312837e-01 1.9851144e-01 1.2570045e-02 1.6588646e-03
 5.2611355e-04 2.3472585e-01 9.4548780e-03 3.1861488e-02 4.0588289e-02
 3.3674863e-01], sum to 1.0000
[2019-04-09 15:13:10,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9818
[2019-04-09 15:13:11,004] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.283333333333333, 73.16666666666667, 0.0, 0.0, 19.0, 27.93104374443953, 1.147106339818017, 0.0, 1.0, 60.0, 26.68251595523886], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1541400.0000, 
sim time next is 1542000.0000, 
raw observation next is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 19.0, 27.89830564846008, 1.140038319029234, 0.0, 1.0, 65.0, 23.87397396014601], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.8248588040383401, 0.8800127730097446, 0.0, 1.0, 1.0, 0.2387397396014601], 
reward next is 0.7613, 
noisyNet noise sample is [array([-0.03498667], dtype=float32), 0.7202015]. 
=============================================
[2019-04-09 15:13:11,026] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[11.837412]
 [11.827877]
 [11.800494]
 [11.981497]
 [12.021805]], R is [[12.34783173]
 [12.95752907]
 [13.63005543]
 [14.29329395]
 [14.98023701]].
[2019-04-09 15:13:11,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00049962 0.05050156 0.1802774  0.01273118 0.0035934  0.00080201
 0.31447116 0.01951571 0.03913379 0.02162887 0.35684523], sum to 1.0000
[2019-04-09 15:13:11,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0806
[2019-04-09 15:13:11,049] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 22.5, 26.88545686907344, 0.7924824573056709, 1.0, 1.0, 65.0, 38.95400230661856], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1496400.0000, 
sim time next is 1497000.0000, 
raw observation next is [1.1, 100.0, 5.999999999999998, 0.0, 22.5, 26.84555309438505, 0.7950243508192276, 1.0, 1.0, 25.0, 36.16957384497525], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.019999999999999993, 0.0, 0.375, 0.7371294245320875, 0.7650081169397426, 1.0, 1.0, 0.2, 0.36169573844975245], 
reward next is 0.6383, 
noisyNet noise sample is [array([0.16506359], dtype=float32), -0.39146596]. 
=============================================
[2019-04-09 15:13:11,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[8.8274975]
 [8.782482 ]
 [8.62161  ]
 [8.848216 ]
 [8.551538 ]], R is [[ 9.33141327]
 [ 9.84855938]
 [10.37943459]
 [10.92137241]
 [11.39158916]].
[2019-04-09 15:13:11,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.01772729e-04 8.50666389e-02 2.23825112e-01 1.35822585e-02
 1.84842967e-03 6.49524154e-04 1.45105705e-01 1.76328495e-02
 6.68377802e-02 3.33894081e-02 4.11760539e-01], sum to 1.0000
[2019-04-09 15:13:11,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4308
[2019-04-09 15:13:11,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 100.0, 18.0, 0.0, 22.5, 27.09446006375812, 0.8295134395309639, 1.0, 1.0, 65.0, 26.16790289363428], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1499400.0000, 
sim time next is 1500000.0000, 
raw observation next is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 22.5, 27.26039691486326, 0.8380320852923221, 1.0, 1.0, 45.0, 28.20736265677323], 
processed observation next is [1.0, 0.34782608695652173, 0.502308402585411, 1.0, 0.0761111111111111, 0.0, 0.375, 0.7716997429052718, 0.7793440284307741, 1.0, 1.0, 0.6, 0.2820736265677323], 
reward next is 0.7179, 
noisyNet noise sample is [array([-1.6366537], dtype=float32), -0.072296165]. 
=============================================
[2019-04-09 15:13:11,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[9.503252]
 [9.216632]
 [9.259258]
 [9.247697]
 [8.766268]], R is [[ 9.9248724 ]
 [10.56394482]
 [11.15842438]
 [11.66265488]
 [12.15619564]].
[2019-04-09 15:13:11,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.32670757e-04 7.87087083e-02 1.33912191e-01 1.39366845e-02
 2.74195895e-03 2.23967669e-04 2.51266330e-01 1.81269068e-02
 5.03709540e-02 2.73387264e-02 4.23240930e-01], sum to 1.0000
[2019-04-09 15:13:11,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-09 15:13:11,864] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 96.0, 80.5, 354.0, 22.5, 27.8025129768252, 0.9847456477682967, 1.0, 1.0, 45.0, 19.29650428387399], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1508400.0000, 
sim time next is 1509000.0000, 
raw observation next is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 22.5, 27.88802295682698, 1.002149922478085, 1.0, 1.0, 65.0, 20.65669768269504], 
processed observation next is [1.0, 0.4782608695652174, 0.559095106186519, 0.955, 0.27666666666666667, 0.521546961325967, 0.375, 0.824001913068915, 0.8340499741593618, 1.0, 1.0, 1.0, 0.20656697682695038], 
reward next is 0.7934, 
noisyNet noise sample is [array([-0.90893316], dtype=float32), 0.0024664954]. 
=============================================
[2019-04-09 15:13:11,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[10.186958]
 [10.305867]
 [ 9.925229]
 [ 9.85587 ]
 [ 9.833276]], R is [[10.99041653]
 [11.68754768]
 [12.34650612]
 [12.96543312]
 [13.5972929 ]].
[2019-04-09 15:13:12,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7089651e-05 9.2772014e-02 8.4100418e-02 1.6714992e-02 1.1655081e-03
 3.4464561e-04 2.0379776e-01 1.0958202e-02 2.6984580e-02 2.2187604e-02
 5.4087722e-01], sum to 1.0000
[2019-04-09 15:13:12,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1693
[2019-04-09 15:13:12,041] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 22.5, 28.14615124448392, 1.078108422141266, 1.0, 1.0, 45.0, 16.23629707704994], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1511400.0000, 
sim time next is 1512000.0000, 
raw observation next is [4.4, 93.0, 94.0, 704.0, 22.5, 28.17936008795144, 1.091098481282113, 1.0, 1.0, 25.0, 15.49220733810255], 
processed observation next is [1.0, 0.5217391304347826, 0.5844875346260389, 0.93, 0.31333333333333335, 0.7779005524861878, 0.375, 0.8482800073292868, 0.8636994937607043, 1.0, 1.0, 0.2, 0.15492207338102548], 
reward next is 0.8451, 
noisyNet noise sample is [array([0.45036867], dtype=float32), -0.15581174]. 
=============================================
[2019-04-09 15:13:12,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[10.882438]
 [10.927903]
 [10.907461]
 [10.740079]
 [10.919936]], R is [[11.8124218 ]
 [12.53193474]
 [13.24048233]
 [13.94251823]
 [14.62311554]].
[2019-04-09 15:13:12,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4287832e-05 9.2259444e-02 1.6195720e-01 1.1716817e-02 1.1078531e-03
 1.8994045e-04 2.4167399e-01 1.0490989e-02 2.2580933e-02 1.3648127e-02
 4.4428042e-01], sum to 1.0000
[2019-04-09 15:13:12,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-09 15:13:12,075] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 22.5, 27.88466950755618, 1.109490251873385, 1.0, 1.0, 65.0, 13.74390841198825], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1515000.0000, 
sim time next is 1515600.0000, 
raw observation next is [7.2, 73.0, 92.5, 700.5, 22.5, 28.09561152529709, 1.121393751169845, 1.0, 1.0, 65.0, 11.26812043396021], 
processed observation next is [1.0, 0.5652173913043478, 0.662049861495845, 0.73, 0.30833333333333335, 0.7740331491712708, 0.375, 0.8413009604414242, 0.8737979170566149, 1.0, 1.0, 1.0, 0.1126812043396021], 
reward next is 0.8873, 
noisyNet noise sample is [array([0.9327747], dtype=float32), 0.9332887]. 
=============================================
[2019-04-09 15:13:12,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0005715  0.06881105 0.20258401 0.02366257 0.00357745 0.00146689
 0.2515758  0.01434307 0.03421332 0.03632307 0.36287132], sum to 1.0000
[2019-04-09 15:13:12,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0242
[2019-04-09 15:13:12,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.8573067474574, 0.805225989103089, 0.0, 1.0, 45.0, 37.64675452118797], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.86349162705188, 0.8094092266856195, 0.0, 1.0, 20.0, 32.33078271654336], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7386243022543232, 0.7698030755618732, 0.0, 1.0, 0.1, 0.3233078271654336], 
reward next is 0.6767, 
noisyNet noise sample is [array([0.80229956], dtype=float32), 0.5456609]. 
=============================================
[2019-04-09 15:13:12,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00064567 0.05751253 0.10676494 0.01881717 0.00327541 0.00125756
 0.39019638 0.03905087 0.03782811 0.02657592 0.31807548], sum to 1.0000
[2019-04-09 15:13:12,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-09 15:13:12,181] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 19.0, 27.39677693502652, 0.9805341318971005, 0.0, 1.0, 45.0, 31.71701678981422], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1566600.0000, 
sim time next is 1567200.0000, 
raw observation next is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 19.0, 27.43547959543662, 0.9703253224775912, 0.0, 1.0, 45.0, 23.99528758169037], 
processed observation next is [1.0, 0.13043478260869565, 0.5863342566943676, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7862899662863851, 0.8234417741591971, 0.0, 1.0, 0.6, 0.2399528758169037], 
reward next is 0.7600, 
noisyNet noise sample is [array([-0.08886789], dtype=float32), 0.35485986]. 
=============================================
[2019-04-09 15:13:12,303] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1946822e-04 7.6194182e-02 1.5302384e-01 2.5766470e-02 2.2096110e-03
 7.4748212e-04 3.1978992e-01 1.8712118e-02 4.9278386e-02 2.5378680e-02
 3.2867974e-01], sum to 1.0000
[2019-04-09 15:13:12,303] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2654
[2019-04-09 15:13:12,322] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 22.5, 27.78181235804929, 1.029153624596431, 1.0, 1.0, 45.0, 21.85931598271389], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1588200.0000, 
sim time next is 1588800.0000, 
raw observation next is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 22.5, 27.86230823148968, 1.051672846356099, 1.0, 1.0, 45.0, 19.11384760786317], 
processed observation next is [1.0, 0.391304347826087, 0.6555863342566944, 0.7333333333333334, 0.34, 0.13167587476979745, 0.375, 0.8218590192908067, 0.850557615452033, 1.0, 1.0, 0.6, 0.1911384760786317], 
reward next is 0.8089, 
noisyNet noise sample is [array([1.6961215], dtype=float32), 0.78903073]. 
=============================================
[2019-04-09 15:13:12,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00056907 0.08530712 0.0815107  0.01728721 0.00234152 0.00056541
 0.4032595  0.01063485 0.03361759 0.03865503 0.32625204], sum to 1.0000
[2019-04-09 15:13:12,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0281139e-04 8.0683380e-02 1.5753947e-01 1.9228481e-02 2.4597391e-03
 1.5910077e-04 2.3244886e-01 2.1817444e-02 8.1424847e-02 3.0025328e-02
 3.7411055e-01], sum to 1.0000
[2019-04-09 15:13:12,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2377
[2019-04-09 15:13:12,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0990
[2019-04-09 15:13:12,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 19.0, 26.92478385486877, 0.777514783242483, 0.0, 1.0, 45.0, 29.35563214120616], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 22.5, 26.74138371717518, 0.7582252463501139, 0.0, 1.0, 45.0, 26.61130858449548], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.375, 0.7284486430979316, 0.7527417487833713, 0.0, 1.0, 0.6, 0.2661130858449548], 
reward next is 0.7339, 
noisyNet noise sample is [array([-0.4446363], dtype=float32), 0.18609625]. 
=============================================
[2019-04-09 15:13:12,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [11.33333333333333, 53.83333333333334, 75.66666666666667, 601.3333333333334, 22.5, 28.85750465938154, 1.259467775263301, 1.0, 1.0, 25.0, 2.853612093136555], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1522200.0000, 
sim time next is 1522800.0000, 
raw observation next is [11.6, 52.0, 76.0, 570.5, 22.5, 28.88655794166571, 1.267507536646383, 1.0, 1.0, 25.0, 2.740783248257239], 
processed observation next is [1.0, 0.6521739130434783, 0.7839335180055402, 0.52, 0.25333333333333335, 0.6303867403314917, 0.375, 0.9072131618054758, 0.922502512215461, 1.0, 1.0, 0.2, 0.027407832482572392], 
reward next is 0.9726, 
noisyNet noise sample is [array([-0.5674991], dtype=float32), 0.6273719]. 
=============================================
[2019-04-09 15:13:12,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4926814e-04 8.8148974e-02 2.0615442e-01 1.1687046e-02 1.1891883e-03
 2.2322696e-04 1.9115096e-01 1.4164063e-02 2.1655893e-02 2.2363108e-02
 4.4301385e-01], sum to 1.0000
[2019-04-09 15:13:12,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6965
[2019-04-09 15:13:12,431] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.4, 93.0, 94.0, 704.0, 22.5, 27.79235262312314, 1.082665983628871, 1.0, 1.0, 45.0, 8.721889157868741], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1512000.0000, 
sim time next is 1512600.0000, 
raw observation next is [4.866666666666667, 89.66666666666667, 96.0, 702.6666666666667, 22.5, 28.08542941472846, 1.082200648790505, 1.0, 1.0, 65.0, 17.40276337833479], 
processed observation next is [1.0, 0.5217391304347826, 0.5974145891043399, 0.8966666666666667, 0.32, 0.7764272559852671, 0.375, 0.8404524512273716, 0.860733549596835, 1.0, 1.0, 1.0, 0.1740276337833479], 
reward next is 0.8260, 
noisyNet noise sample is [array([-0.79247135], dtype=float32), 1.602305]. 
=============================================
[2019-04-09 15:13:12,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.5817526e-05 7.3420793e-02 1.1653892e-01 1.0207572e-02 1.6112424e-03
 2.0336219e-04 2.7681229e-01 1.3851978e-02 3.3443112e-02 2.1925835e-02
 4.5189899e-01], sum to 1.0000
[2019-04-09 15:13:12,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9545
[2019-04-09 15:13:12,704] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 19.0, 27.92070826510229, 1.153129167405173, 0.0, 1.0, 45.0, 22.18198677194066], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1628400.0000, 
sim time next is 1629000.0000, 
raw observation next is [7.45, 75.0, 0.0, 0.0, 19.0, 27.88573774953101, 1.149177683619936, 0.0, 1.0, 65.0, 25.46721286258878], 
processed observation next is [1.0, 0.8695652173913043, 0.6689750692520776, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8238114791275842, 0.883059227873312, 0.0, 1.0, 1.0, 0.2546721286258878], 
reward next is 0.7453, 
noisyNet noise sample is [array([-0.29943618], dtype=float32), -0.54394025]. 
=============================================
[2019-04-09 15:13:12,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[10.665476 ]
 [10.897152 ]
 [10.9977045]
 [10.83238  ]
 [10.887456 ]], R is [[11.76160622]
 [12.42217064]
 [13.05006027]
 [13.6874361 ]
 [14.35939217]].
[2019-04-09 15:13:12,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00046784 0.09323413 0.16633382 0.01734029 0.0038236  0.00163658
 0.27785638 0.02663994 0.04075405 0.03082421 0.34108913], sum to 1.0000
[2019-04-09 15:13:12,786] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8575
[2019-04-09 15:13:12,801] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.166666666666666, 81.0, 0.0, 0.0, 22.5, 27.38157377773264, 0.9263233190762402, 1.0, 1.0, 45.0, 31.53616333117326], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [5.083333333333334, 81.5, 13.0, 15.0, 22.5, 27.3284017605305, 0.9256602825322884, 1.0, 1.0, 65.0, 35.46947492970705], 
processed observation next is [1.0, 0.30434782608695654, 0.6034164358264081, 0.815, 0.043333333333333335, 0.016574585635359115, 0.375, 0.7773668133775417, 0.8085534275107628, 1.0, 1.0, 1.0, 0.35469474929707046], 
reward next is 0.6453, 
noisyNet noise sample is [array([-1.3525279], dtype=float32), -1.1959262]. 
=============================================
[2019-04-09 15:13:12,839] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.47371630e-05 7.18655512e-02 1.23477064e-01 9.88193322e-03
 2.26008007e-03 1.61688775e-04 2.90291101e-01 1.27205793e-02
 2.71074697e-02 3.12031768e-02 4.30946678e-01], sum to 1.0000
[2019-04-09 15:13:12,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7637
[2019-04-09 15:13:12,852] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.35, 54.0, 87.0, 28.0, 22.5, 29.08196954583133, 1.271239200275754, 1.0, 1.0, 20.0, 2.250393549427711], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1528200.0000, 
sim time next is 1528800.0000, 
raw observation next is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 22.5, 29.05884420433985, 1.28064847550286, 1.0, 1.0, 65.0, 10.34131443313226], 
processed observation next is [1.0, 0.6956521739130435, 0.7691597414589106, 0.5533333333333332, 0.24277777777777776, 0.026887661141804794, 0.375, 0.921570350361654, 0.92688282516762, 1.0, 1.0, 1.0, 0.10341314433132259], 
reward next is 0.8966, 
noisyNet noise sample is [array([1.0345197], dtype=float32), -0.08128774]. 
=============================================
[2019-04-09 15:13:13,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6317985e-05 1.2260138e-01 1.6077715e-01 1.7760333e-02 7.6095451e-04
 2.3613537e-04 2.2350848e-01 4.8231739e-03 3.9319657e-02 1.3384781e-02
 4.1675165e-01], sum to 1.0000
[2019-04-09 15:13:13,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8420
[2019-04-09 15:13:13,042] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.666666666666666, 71.33333333333334, 90.0, 700.6666666666666, 22.5, 28.17597884532648, 1.135104986026234, 1.0, 1.0, 65.0, 15.63188510958999], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1516200.0000, 
sim time next is 1516800.0000, 
raw observation next is [8.133333333333333, 69.66666666666667, 87.5, 700.8333333333334, 22.5, 28.18347416900833, 0.9845517318522257, 1.0, 1.0, 65.0, 48.18957795083571], 
processed observation next is [1.0, 0.5652173913043478, 0.687903970452447, 0.6966666666666668, 0.2916666666666667, 0.774401473296501, 0.375, 0.8486228474173609, 0.8281839106174086, 1.0, 1.0, 1.0, 0.4818957795083571], 
reward next is 0.5181, 
noisyNet noise sample is [array([0.6312733], dtype=float32), 0.33436942]. 
=============================================
[2019-04-09 15:13:13,157] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.48204626e-04 1.02091536e-01 1.23763323e-01 1.69912875e-02
 2.12926511e-03 3.43506719e-04 3.14494491e-01 1.40502993e-02
 3.79534774e-02 3.20805945e-02 3.55854094e-01], sum to 1.0000
[2019-04-09 15:13:13,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2454
[2019-04-09 15:13:13,170] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.1, 77.66666666666667, 0.0, 0.0, 19.0, 27.8088807649215, 1.153558812202496, 0.0, 1.0, 25.0, 22.28546678453719], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1631400.0000, 
sim time next is 1632000.0000, 
raw observation next is [7.0, 79.33333333333334, 0.0, 0.0, 19.0, 27.79179138228103, 1.150375877409741, 0.0, 1.0, 40.0, 23.61424636876641], 
processed observation next is [1.0, 0.9130434782608695, 0.6565096952908588, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.8159826151900859, 0.883458625803247, 0.0, 1.0, 0.5, 0.2361424636876641], 
reward next is 0.7639, 
noisyNet noise sample is [array([-0.22278892], dtype=float32), -0.73197573]. 
=============================================
[2019-04-09 15:13:13,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[11.161662]
 [11.115071]
 [11.431344]
 [11.353069]
 [11.718011]], R is [[11.63044834]
 [12.29128933]
 [12.92619896]
 [13.53218174]
 [14.15019131]].
[2019-04-09 15:13:13,229] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1778628e-04 8.8652655e-02 1.4687550e-01 6.5448089e-03 2.7813090e-03
 4.5355217e-04 2.3535149e-01 1.0097843e-02 4.5751460e-02 1.3936421e-02
 4.4943717e-01], sum to 1.0000
[2019-04-09 15:13:13,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6569
[2019-04-09 15:13:13,232] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.34838065e-05 1.44984558e-01 1.02231801e-01 1.34713305e-02
 1.91403122e-03 1.39669413e-04 3.66420686e-01 9.85956285e-03
 6.24978431e-02 1.85321141e-02 2.79864937e-01], sum to 1.0000
[2019-04-09 15:13:13,232] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1969
[2019-04-09 15:13:13,245] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 22.5, 28.97217319706765, 1.288518864051393, 1.0, 1.0, 55.0, 9.868582511660618], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1609800.0000, 
sim time next is 1610400.0000, 
raw observation next is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 22.5, 29.00511493522096, 1.295360150985742, 1.0, 1.0, 25.0, 7.791707656591578], 
processed observation next is [1.0, 0.6521739130434783, 0.840258541089566, 0.4966666666666667, 0.29722222222222217, 0.0, 0.375, 0.9170929112684133, 0.9317867169952473, 1.0, 1.0, 0.2, 0.07791707656591579], 
reward next is 0.9221, 
noisyNet noise sample is [array([0.41275543], dtype=float32), 0.051727593]. 
=============================================
[2019-04-09 15:13:13,255] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.983333333333333, 66.83333333333334, 171.6666666666667, 104.0, 22.5, 28.33156435930104, 1.12066618691448, 1.0, 1.0, 45.0, 13.49325779541434], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1591800.0000, 
sim time next is 1592400.0000, 
raw observation next is [8.266666666666667, 65.66666666666667, 185.8333333333333, 96.0, 22.5, 28.37344923474414, 1.134190663775871, 1.0, 1.0, 65.0, 17.10350620183377], 
processed observation next is [1.0, 0.43478260869565216, 0.6915974145891045, 0.6566666666666667, 0.6194444444444442, 0.10607734806629834, 0.375, 0.864454102895345, 0.878063554591957, 1.0, 1.0, 1.0, 0.1710350620183377], 
reward next is 0.8290, 
noisyNet noise sample is [array([0.12243583], dtype=float32), -2.058104]. 
=============================================
[2019-04-09 15:13:13,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.11678935e-04 7.96291307e-02 1.52961075e-01 1.00563262e-02
 3.56189022e-03 4.09736356e-04 3.55521530e-01 1.56940352e-02
 4.44041267e-02 3.56939696e-02 3.01956475e-01], sum to 1.0000
[2019-04-09 15:13:13,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4662
[2019-04-09 15:13:13,435] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 19.0, 27.80122173685928, 1.134589822839631, 0.0, 1.0, 65.0, 24.85040216142686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 19.0, 27.78375351766714, 1.148629828108134, 0.0, 1.0, 45.0, 20.86052313259488], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.08333333333333333, 0.8153127931389283, 0.882876609369378, 0.0, 1.0, 0.6, 0.20860523132594883], 
reward next is 0.7914, 
noisyNet noise sample is [array([1.4851875], dtype=float32), -0.7859619]. 
=============================================
[2019-04-09 15:13:13,438] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.74451296e-05 1.08219735e-01 1.38844162e-01 1.24303857e-02
 1.16516592e-03 1.98990208e-04 3.88807058e-01 4.99382429e-03
 3.13875228e-02 2.10447647e-02 2.92850912e-01], sum to 1.0000
[2019-04-09 15:13:13,439] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4214
[2019-04-09 15:13:13,456] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 22.5, 28.66815707515625, 1.321390014045585, 1.0, 1.0, 45.0, 11.60360597761578], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1613400.0000, 
sim time next is 1614000.0000, 
raw observation next is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 22.5, 28.31611452569772, 1.254718017378959, 1.0, 1.0, 45.0, 2.534826981616936], 
processed observation next is [1.0, 0.6956521739130435, 0.8208679593721148, 0.52, 0.18222222222222223, 0.03406998158379374, 0.375, 0.85967621047481, 0.9182393391263197, 1.0, 1.0, 0.6, 0.02534826981616936], 
reward next is 0.9747, 
noisyNet noise sample is [array([1.1106259], dtype=float32), 0.950386]. 
=============================================
[2019-04-09 15:13:13,461] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[12.0329  ]
 [11.981501]
 [11.968599]
 [11.871573]
 [12.217592]], R is [[12.95661068]
 [13.71100807]
 [14.04540062]
 [14.83148003]
 [15.59852982]].
[2019-04-09 15:13:14,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9554518e-05 7.2512865e-02 2.1463501e-01 6.7877872e-03 1.8686862e-03
 2.4124318e-04 3.0271721e-01 1.6343495e-02 3.9483435e-02 2.6126789e-02
 3.1924394e-01], sum to 1.0000
[2019-04-09 15:13:14,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-09 15:13:14,072] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 22.5, 28.86993449446449, 1.269526622827471, 1.0, 1.0, 45.0, 7.995195204435539], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1609800.0000, 
sim time next is 1610400.0000, 
raw observation next is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 22.5, 28.96358159432363, 1.285076529741461, 1.0, 1.0, 20.0, 8.14402153012878], 
processed observation next is [1.0, 0.6521739130434783, 0.840258541089566, 0.4966666666666667, 0.29722222222222217, 0.0, 0.375, 0.9136317995269693, 0.9283588432471537, 1.0, 1.0, 0.1, 0.08144021530128781], 
reward next is 0.9186, 
noisyNet noise sample is [array([-1.1316642], dtype=float32), 0.3094595]. 
=============================================
[2019-04-09 15:13:14,156] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2617331e-04 1.1399118e-01 1.1566455e-01 1.3063688e-02 3.5702465e-03
 1.4010549e-04 2.9594037e-01 8.4437560e-03 3.2039892e-02 2.6265880e-02
 3.9075419e-01], sum to 1.0000
[2019-04-09 15:13:14,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9460882e-04 4.7679488e-02 8.3150260e-02 1.0925798e-02 1.6682786e-03
 6.0689606e-04 3.1152123e-01 1.1867857e-02 2.0547044e-02 1.9535091e-02
 4.9220347e-01], sum to 1.0000
[2019-04-09 15:13:14,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0826
[2019-04-09 15:13:14,161] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8437
[2019-04-09 15:13:14,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 22.5, 29.07314482136384, 1.281487327434922, 1.0, 1.0, 60.0, 2.456982780303036], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 22.5, 29.07963613884466, 1.288423469716977, 1.0, 1.0, 20.0, 7.701724697313023], 
processed observation next is [1.0, 0.6956521739130435, 0.7613111726685133, 0.5666666666666668, 0.19555555555555557, 0.022836095764272566, 0.375, 0.9233030115703883, 0.9294744899056591, 1.0, 1.0, 0.1, 0.07701724697313023], 
reward next is 0.9230, 
noisyNet noise sample is [array([1.9221616], dtype=float32), 0.6624058]. 
=============================================
[2019-04-09 15:13:14,172] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 81.50000000000001, 0.0, 0.0, 19.0, 27.42375035021049, 1.009393630752558, 0.0, 1.0, 45.0, 30.96136859362497], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1559400.0000, 
sim time next is 1560000.0000, 
raw observation next is [5.0, 81.0, 0.0, 0.0, 19.0, 27.45510828202559, 1.00821939078288, 0.0, 1.0, 65.0, 30.06178278654504], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.81, 0.0, 0.0, 0.08333333333333333, 0.7879256901687993, 0.8360731302609601, 0.0, 1.0, 1.0, 0.30061782786545044], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.96459216], dtype=float32), -0.24835376]. 
=============================================
[2019-04-09 15:13:14,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[ 9.8152  ]
 [ 9.818131]
 [ 9.908071]
 [10.069039]
 [ 9.955916]], R is [[10.42000866]
 [11.00619507]
 [11.58188057]
 [12.14154625]
 [12.74454308]].
[2019-04-09 15:13:14,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0992751e-05 9.7934857e-02 1.5973166e-01 2.7107229e-02 2.0330839e-03
 1.8416371e-04 2.2637926e-01 7.1297958e-03 3.3529658e-02 2.8862562e-02
 4.1702673e-01], sum to 1.0000
[2019-04-09 15:13:14,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4613
[2019-04-09 15:13:14,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.91666666666667, 55.16666666666666, 17.33333333333333, 12.33333333333333, 22.5, 28.90947197155882, 1.266904918374878, 1.0, 1.0, 65.0, 9.946729381779086], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1617000.0000, 
sim time next is 1617600.0000, 
raw observation next is [11.63333333333333, 56.33333333333334, 0.0, 0.0, 22.5, 28.93460763645277, 1.256091218993938, 1.0, 1.0, 45.0, 12.99162183305009], 
processed observation next is [1.0, 0.7391304347826086, 0.7848568790397045, 0.5633333333333335, 0.0, 0.0, 0.375, 0.911217303037731, 0.9186970729979794, 1.0, 1.0, 0.6, 0.1299162183305009], 
reward next is 0.8701, 
noisyNet noise sample is [array([-0.85851604], dtype=float32), 1.1488909]. 
=============================================
[2019-04-09 15:13:14,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.10220775e-04 4.80417646e-02 1.10281982e-01 1.81656126e-02
 2.18913262e-03 3.42379499e-04 2.18111917e-01 1.43990349e-02
 2.73683481e-02 3.98823656e-02 5.21107137e-01], sum to 1.0000
[2019-04-09 15:13:14,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-09 15:13:14,584] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 22.5, 28.289203127907, 1.188845756818604, 1.0, 1.0, 20.0, 16.31366936299577], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1537200.0000, 
sim time next is 1537800.0000, 
raw observation next is [9.033333333333333, 63.0, 0.0, 0.0, 22.5, 28.1940140005695, 1.185912703797901, 1.0, 1.0, 20.0, 17.88052802402555], 
processed observation next is [1.0, 0.8260869565217391, 0.7128347183748848, 0.63, 0.0, 0.0, 0.375, 0.8495011667141249, 0.8953042345993003, 1.0, 1.0, 0.1, 0.17880528024025552], 
reward next is 0.8212, 
noisyNet noise sample is [array([0.40387538], dtype=float32), 1.2844464]. 
=============================================
[2019-04-09 15:13:14,594] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.14294918e-04 1.02969594e-01 1.20396741e-01 1.03635890e-02
 1.93550019e-03 1.06555759e-03 1.76619664e-01 1.27091547e-02
 2.85992622e-02 2.84401253e-02 5.16486466e-01], sum to 1.0000
[2019-04-09 15:13:14,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3173
[2019-04-09 15:13:14,607] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 86.0, 0.0, 0.0, 19.0, 27.38157367786164, 0.9795629924001138, 0.0, 1.0, 65.0, 35.39149375277588], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1566000.0000, 
sim time next is 1566600.0000, 
raw observation next is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 19.0, 27.39293640770106, 0.9819517121048027, 0.0, 1.0, 45.0, 30.30925547367158], 
processed observation next is [1.0, 0.13043478260869565, 0.5854108956602032, 0.8583333333333334, 0.0, 0.0, 0.08333333333333333, 0.782744700641755, 0.8273172373682676, 0.0, 1.0, 0.6, 0.3030925547367158], 
reward next is 0.6969, 
noisyNet noise sample is [array([-0.16923028], dtype=float32), -0.6218334]. 
=============================================
[2019-04-09 15:13:14,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7590013e-04 8.0840409e-02 7.1876489e-02 1.6756270e-02 2.1605096e-03
 5.4006046e-04 2.8593391e-01 1.4430157e-02 5.4602806e-02 3.2877509e-02
 4.3960595e-01], sum to 1.0000
[2019-04-09 15:13:14,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6093
[2019-04-09 15:13:14,665] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 96.33333333333333, 0.0, 0.0, 19.0, 27.63234677788814, 1.049756002814735, 0.0, 1.0, 65.0, 35.9284153538896], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [6.9, 96.5, 0.0, 0.0, 19.0, 27.69379659653742, 1.048199023313978, 0.0, 1.0, 65.0, 26.84162604529658], 
processed observation next is [1.0, 0.08695652173913043, 0.6537396121883658, 0.965, 0.0, 0.0, 0.08333333333333333, 0.8078163830447851, 0.8493996744379926, 0.0, 1.0, 1.0, 0.2684162604529658], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.4637294], dtype=float32), -0.61811614]. 
=============================================
[2019-04-09 15:13:14,673] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7403685e-05 5.8650218e-02 1.0760829e-01 8.3995536e-03 8.1391848e-04
 2.2941965e-04 3.0491430e-01 6.5961643e-03 1.6514277e-02 1.7524028e-02
 4.7866243e-01], sum to 1.0000
[2019-04-09 15:13:14,680] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-09 15:13:14,693] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.416666666666667, 77.0, 0.0, 0.0, 19.0, 27.73544090042995, 1.07003187389995, 0.0, 1.0, 65.0, 31.97250570222501], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1548600.0000, 
sim time next is 1549200.0000, 
raw observation next is [6.233333333333333, 78.0, 0.0, 0.0, 19.0, 27.69668919571589, 1.074656830864625, 0.0, 1.0, 65.0, 34.09556795191147], 
processed observation next is [1.0, 0.9565217391304348, 0.6352723915050786, 0.78, 0.0, 0.0, 0.08333333333333333, 0.8080574329763243, 0.8582189436215417, 0.0, 1.0, 1.0, 0.3409556795191147], 
reward next is 0.6590, 
noisyNet noise sample is [array([0.4648641], dtype=float32), 0.5513003]. 
=============================================
[2019-04-09 15:13:14,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6056238e-04 6.9936089e-02 9.5047876e-02 1.4807831e-02 2.9516993e-03
 7.3603040e-04 2.9729244e-01 1.9612564e-02 3.5567809e-02 3.0439382e-02
 4.3324780e-01], sum to 1.0000
[2019-04-09 15:13:14,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0841
[2019-04-09 15:13:14,843] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.416666666666667, 82.00000000000001, 0.0, 0.0, 19.0, 27.5383103876415, 1.057530298513355, 0.0, 1.0, 45.0, 28.41312983779477], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1552200.0000, 
sim time next is 1552800.0000, 
raw observation next is [5.333333333333334, 82.0, 0.0, 0.0, 19.0, 27.576043358358, 1.044254161922058, 0.0, 1.0, 25.0, 24.207851335413], 
processed observation next is [1.0, 1.0, 0.6103416435826409, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7980036131964999, 0.848084720640686, 0.0, 1.0, 0.2, 0.24207851335413], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.23077694], dtype=float32), -0.05418574]. 
=============================================
[2019-04-09 15:13:14,877] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.57306616e-04 8.02678764e-02 1.07429676e-01 1.25008523e-02
 2.33872188e-03 8.72420962e-04 3.61548454e-01 1.80951301e-02
 2.34536566e-02 2.33962536e-02 3.69839698e-01], sum to 1.0000
[2019-04-09 15:13:14,883] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-09 15:13:14,895] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 27.41630038979337, 0.9637280397274363, 0.0, 1.0, 45.0, 29.27968402455185], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1570800.0000, 
sim time next is 1571400.0000, 
raw observation next is [4.65, 84.5, 0.0, 0.0, 19.0, 27.3841898207468, 0.9556675126474997, 0.0, 1.0, 45.0, 29.28833044594238], 
processed observation next is [1.0, 0.17391304347826086, 0.5914127423822716, 0.845, 0.0, 0.0, 0.08333333333333333, 0.7820158183955668, 0.8185558375491665, 0.0, 1.0, 0.6, 0.2928833044594238], 
reward next is 0.7071, 
noisyNet noise sample is [array([-0.8381551], dtype=float32), 1.3912358]. 
=============================================
[2019-04-09 15:13:14,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00054847 0.06935459 0.13456272 0.02066614 0.00380364 0.00103068
 0.33103618 0.01445604 0.02972476 0.02873218 0.36608467], sum to 1.0000
[2019-04-09 15:13:14,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6046
[2019-04-09 15:13:14,942] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 22.5, 27.49160319586129, 1.000420125808174, 0.0, 1.0, 65.0, 32.6371231326849], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 22.5, 27.48380443804206, 0.9952957995204527, 1.0, 1.0, 55.0, 29.33207050794525], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.375, 0.790317036503505, 0.8317652665068175, 1.0, 1.0, 0.8, 0.2933207050794525], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.7072637], dtype=float32), 1.0354946]. 
=============================================
[2019-04-09 15:13:14,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[9.202323 ]
 [9.0576315]
 [9.245281 ]
 [9.2124815]
 [9.115198 ]], R is [[ 9.89462185]
 [10.46930408]
 [11.031003  ]
 [11.64907455]
 [12.22587585]].
[2019-04-09 15:13:15,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00049447 0.14192735 0.11073008 0.01532735 0.00256771 0.00087508
 0.24644648 0.01716326 0.03351428 0.03532132 0.39563262], sum to 1.0000
[2019-04-09 15:13:15,166] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8528
[2019-04-09 15:13:15,191] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.25, 80.5, 0.0, 0.0, 19.0, 27.40452163504949, 0.9467780180574402, 0.0, 1.0, 65.0, 34.07050377798288], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1578600.0000, 
sim time next is 1579200.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 19.0, 27.39311870965751, 0.940174675549415, 0.0, 1.0, 65.0, 31.71217513477908], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 0.8, 0.0, 0.0, 0.08333333333333333, 0.7827598924714593, 0.8133915585164716, 0.0, 1.0, 1.0, 0.3171217513477908], 
reward next is 0.6829, 
noisyNet noise sample is [array([-0.05273551], dtype=float32), 0.8682332]. 
=============================================
[2019-04-09 15:13:15,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.2657367e-05 4.8777312e-02 2.3032825e-01 2.0996919e-02 2.1465789e-03
 3.2078594e-04 2.1350661e-01 1.3872129e-02 2.5571954e-02 2.7859859e-02
 4.1655698e-01], sum to 1.0000
[2019-04-09 15:13:15,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4995
[2019-04-09 15:13:15,231] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 75.0, 0.0, 0.0, 19.0, 27.88188431568608, 1.167094036460999, 0.0, 1.0, 65.0, 25.89084643725895], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1629000.0000, 
sim time next is 1629600.0000, 
raw observation next is [7.366666666666667, 75.33333333333333, 0.0, 0.0, 19.0, 27.85285573304431, 1.162727705623436, 0.0, 1.0, 45.0, 27.1083796905066], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7533333333333333, 0.0, 0.0, 0.08333333333333333, 0.8210713110870259, 0.8875759018744787, 0.0, 1.0, 0.6, 0.271083796905066], 
reward next is 0.7289, 
noisyNet noise sample is [array([2.261269], dtype=float32), -1.1779625]. 
=============================================
[2019-04-09 15:13:16,176] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00045419 0.09481675 0.18446173 0.01614893 0.00263857 0.00072834
 0.290145   0.0206286  0.03598492 0.02432454 0.3296684 ], sum to 1.0000
[2019-04-09 15:13:16,176] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1589
[2019-04-09 15:13:16,198] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.866666666666667, 92.0, 0.0, 0.0, 22.5, 27.50621632018816, 0.9837337884251508, 1.0, 1.0, 55.0, 26.40284639836208], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1669200.0000, 
sim time next is 1669800.0000, 
raw observation next is [3.583333333333333, 92.0, 10.66666666666666, 0.0, 22.5, 27.49470192654807, 0.9824118743837617, 1.0, 1.0, 25.0, 29.82538518160525], 
processed observation next is [1.0, 0.30434782608695654, 0.5618651892890121, 0.92, 0.035555555555555535, 0.0, 0.375, 0.7912251605456726, 0.8274706247945872, 1.0, 1.0, 0.2, 0.29825385181605246], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.86778843], dtype=float32), -0.6037281]. 
=============================================
[2019-04-09 15:13:16,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0553117e-04 6.5063566e-02 1.2568586e-01 7.1474328e-03 2.0425741e-03
 6.2576687e-04 5.2723259e-01 1.9017439e-02 3.9806843e-02 2.9056590e-02
 1.8401588e-01], sum to 1.0000
[2019-04-09 15:13:16,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1967
[2019-04-09 15:13:16,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.433333333333334, 92.0, 0.0, 0.0, 22.5, 27.48632470580637, 0.982230114295497, 1.0, 1.0, 65.0, 33.22179031486046], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1668000.0000, 
sim time next is 1668600.0000, 
raw observation next is [4.15, 92.0, 0.0, 0.0, 22.5, 27.46109104670842, 0.9883920242379026, 1.0, 1.0, 30.0, 30.5185224389504], 
processed observation next is [1.0, 0.30434782608695654, 0.5775623268698062, 0.92, 0.0, 0.0, 0.375, 0.7884242538923685, 0.8294640080793009, 1.0, 1.0, 0.3, 0.30518522438950396], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.21100438], dtype=float32), 0.66746724]. 
=============================================
[2019-04-09 15:13:17,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4355364e-04 5.3087212e-02 1.0382051e-01 2.4197506e-02 2.8646365e-03
 6.9004763e-04 2.4037811e-01 1.9360662e-02 4.0576022e-02 2.7127272e-02
 4.8765442e-01], sum to 1.0000
[2019-04-09 15:13:17,301] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0440
[2019-04-09 15:13:17,312] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 27.67175647996056, 1.047897556503681, 0.0, 1.0, 25.0, 28.46207235558963], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1648800.0000, 
sim time next is 1649400.0000, 
raw observation next is [7.1, 96.16666666666666, 0.0, 0.0, 19.0, 27.66400912436316, 1.040399525819766, 0.0, 1.0, 20.0, 24.0595935635111], 
processed observation next is [1.0, 0.08695652173913043, 0.6592797783933518, 0.9616666666666666, 0.0, 0.0, 0.08333333333333333, 0.8053340936969301, 0.8467998419399221, 0.0, 1.0, 0.1, 0.240595935635111], 
reward next is 0.7594, 
noisyNet noise sample is [array([0.57806665], dtype=float32), 1.1333958]. 
=============================================
[2019-04-09 15:13:17,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6972956e-05 8.2067929e-02 8.6996160e-02 6.0771825e-03 1.4439407e-03
 3.2553112e-04 4.2760849e-01 6.3628131e-03 2.1930331e-02 1.8736972e-02
 3.4839368e-01], sum to 1.0000
[2019-04-09 15:13:17,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8205
[2019-04-09 15:13:17,335] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 22.5, 28.57037557016952, 1.198763445159791, 1.0, 1.0, 45.0, 11.72576752835972], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1596000.0000, 
sim time next is 1596600.0000, 
raw observation next is [10.5, 59.0, 216.0, 249.0, 22.5, 28.64996264880627, 1.218449649535311, 1.0, 1.0, 65.0, 11.51933605141919], 
processed observation next is [1.0, 0.4782608695652174, 0.7534626038781165, 0.59, 0.72, 0.2751381215469613, 0.375, 0.8874968874005225, 0.906149883178437, 1.0, 1.0, 1.0, 0.11519336051419189], 
reward next is 0.8848, 
noisyNet noise sample is [array([0.91228205], dtype=float32), -0.5289399]. 
=============================================
[2019-04-09 15:13:17,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0251218e-05 8.9630157e-02 2.1674861e-01 1.2830827e-02 1.9378146e-03
 5.8173371e-04 2.5496545e-01 1.7085342e-02 5.0037857e-02 1.8795602e-02
 3.3729631e-01], sum to 1.0000
[2019-04-09 15:13:17,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4528
[2019-04-09 15:13:17,367] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 22.5, 28.09385328676185, 1.072368775175937, 1.0, 1.0, 65.0, 28.87144293776355], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1678200.0000, 
sim time next is 1678800.0000, 
raw observation next is [1.366666666666667, 92.0, 63.83333333333333, 0.0, 22.5, 28.05983097602707, 1.076978669519584, 1.0, 1.0, 45.0, 28.87334572280797], 
processed observation next is [1.0, 0.43478260869565216, 0.5004616805170823, 0.92, 0.21277777777777776, 0.0, 0.375, 0.8383192480022558, 0.8589928898398614, 1.0, 1.0, 0.6, 0.2887334572280797], 
reward next is 0.7113, 
noisyNet noise sample is [array([0.5262536], dtype=float32), -0.29801133]. 
=============================================
[2019-04-09 15:13:17,414] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-09 15:13:17,426] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:13:17,426] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:13:17,427] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:13:17,428] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:13:17,428] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:13:17,428] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:13:17,431] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run19
[2019-04-09 15:13:17,442] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run19
[2019-04-09 15:13:17,457] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run19
[2019-04-09 15:13:23,449] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.02970162], dtype=float32), 0.034481127]
[2019-04-09 15:13:23,449] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [7.7, 93.0, 82.0, 0.0, 19.0, 24.78809648451846, 0.2937678855394704, 0.0, 1.0, 25.0, 34.88538841648732]
[2019-04-09 15:13:23,449] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:13:23,450] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.00144281 0.11090256 0.14571983 0.03131885 0.01124565 0.00209856
 0.27729917 0.02918827 0.05316081 0.06137361 0.27624983], sampled 0.7664667231842287
[2019-04-09 15:14:49,254] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5642.5382 285366.1079 2944.4380
[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,295] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:49,472] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,769] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5289.9449 320621.5743 2233.2845
[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,826] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5388.0574 310794.2560 2614.1511
[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,869] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:59,976] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:15:00,869] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 180000, evaluation results [180000.0, 5388.057439590693, 310794.2560409327, 2614.1511067728484, 5642.538150727319, 285366.1079337692, 2944.438028350936, 5289.944938474029, 320621.57434615906, 2233.284531936788]
[2019-04-09 15:15:00,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9291767e-04 4.6106648e-02 9.9047542e-02 7.5767622e-03 2.8133364e-03
 2.5287608e-04 3.6490500e-01 1.7372640e-02 3.8241185e-02 4.4503678e-02
 3.7898746e-01], sum to 1.0000
[2019-04-09 15:15:00,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5104
[2019-04-09 15:15:00,901] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.08333333333333331, 94.5, 0.0, 0.0, 19.0, 26.94259001086922, 0.8624107965910097, 0.0, 1.0, 45.0, 36.95646787817405], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1723800.0000, 
sim time next is 1724400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.91271801814038, 0.8535907708689735, 0.0, 1.0, 45.0, 38.47272841056085], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7427265015116985, 0.7845302569563245, 0.0, 1.0, 0.6, 0.38472728410560847], 
reward next is 0.6153, 
noisyNet noise sample is [array([0.7398217], dtype=float32), -1.1362396]. 
=============================================
[2019-04-09 15:15:00,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0822356e-04 4.4293329e-02 1.1057305e-01 7.3668840e-03 2.4178752e-03
 2.9054846e-04 3.4285179e-01 1.6542561e-02 3.5379462e-02 4.3589991e-02
 3.9648634e-01], sum to 1.0000
[2019-04-09 15:15:00,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2319
[2019-04-09 15:15:00,949] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.91271801814038, 0.8535907708689735, 0.0, 1.0, 45.0, 38.47272841056085], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1724400.0000, 
sim time next is 1725000.0000, 
raw observation next is [0.08333333333333333, 94.5, 0.0, 0.0, 19.0, 26.86622959775844, 0.8458014112332625, 0.0, 1.0, 45.0, 37.06112204615576], 
processed observation next is [1.0, 1.0, 0.4649122807017544, 0.945, 0.0, 0.0, 0.08333333333333333, 0.73885246647987, 0.7819338037444208, 0.0, 1.0, 0.6, 0.3706112204615576], 
reward next is 0.6294, 
noisyNet noise sample is [array([0.7398217], dtype=float32), -1.1362396]. 
=============================================
[2019-04-09 15:15:00,966] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[10.662718]
 [10.59742 ]
 [10.958712]
 [10.698413]
 [11.185743]], R is [[11.18371296]
 [11.68714809]
 [12.20071125]
 [12.68706799]
 [13.17957401]].
[2019-04-09 15:15:01,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1208532e-04 5.3478111e-02 2.0840609e-01 1.4515243e-02 2.0123408e-03
 3.5721762e-04 2.3879166e-01 7.4650832e-03 3.2768913e-02 4.7001787e-02
 3.9509141e-01], sum to 1.0000
[2019-04-09 15:15:01,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2484
[2019-04-09 15:15:01,039] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 22.5, 28.02207633024093, 1.188690138930907, 1.0, 1.0, 65.0, 22.50261932194636], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 22.5, 28.00062613560438, 1.181736128561773, 1.0, 1.0, 25.0, 22.79560915756749], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.375, 0.8333855113003651, 0.8939120428539242, 1.0, 1.0, 0.2, 0.2279560915756749], 
reward next is 0.7720, 
noisyNet noise sample is [array([1.1056361], dtype=float32), -0.6644393]. 
=============================================
[2019-04-09 15:15:01,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.21697080e-05 8.76805559e-02 2.73469150e-01 1.15753105e-02
 8.34189705e-04 9.19374579e-05 2.45004818e-01 1.15827806e-02
 4.71196547e-02 1.09412223e-02 3.11668158e-01], sum to 1.0000
[2019-04-09 15:15:01,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4352
[2019-04-09 15:15:01,336] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 22.5, 29.16061598503559, 1.291612758083857, 1.0, 1.0, 25.0, 8.169149754510148], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 22.5, 29.08764548528926, 1.294091779874618, 1.0, 1.0, 45.0, 10.76871330178961], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 0.375, 0.9239704571074384, 0.9313639266248727, 1.0, 1.0, 0.6, 0.1076871330178961], 
reward next is 0.8923, 
noisyNet noise sample is [array([-0.668903], dtype=float32), -0.16455556]. 
=============================================
[2019-04-09 15:15:01,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6109828e-05 3.7840243e-02 1.3800588e-01 1.0880355e-02 1.7681092e-03
 2.1698569e-04 5.3301263e-01 7.4524069e-03 1.6140832e-02 1.9473033e-02
 2.3516341e-01], sum to 1.0000
[2019-04-09 15:15:01,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2762
[2019-04-09 15:15:01,475] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 22.5, 27.4294719452458, 1.153237339737326, 0.0, 1.0, 45.0, 12.82313768820081], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1622400.0000, 
sim time next is 1623000.0000, 
raw observation next is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 22.5, 27.86362775193827, 1.181500600300129, 1.0, 1.0, 45.0, 17.6523414151368], 
processed observation next is [1.0, 0.782608695652174, 0.7280701754385965, 0.6516666666666666, 0.0, 0.0, 0.375, 0.8219689793281892, 0.8938335334333763, 1.0, 1.0, 0.6, 0.176523414151368], 
reward next is 0.8235, 
noisyNet noise sample is [array([-1.1168764], dtype=float32), -0.61173546]. 
=============================================
[2019-04-09 15:15:01,483] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[11.835737]
 [11.819619]
 [11.946156]
 [11.837062]
 [11.907355]], R is [[12.46348095]
 [13.21061516]
 [13.85276794]
 [14.05999565]
 [14.80149078]].
[2019-04-09 15:15:01,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0991508e-04 1.1257795e-01 1.2237014e-01 9.3726646e-03 2.5170289e-03
 4.0792243e-04 3.0815896e-01 1.1807796e-02 1.9146638e-02 3.4027562e-02
 3.7950337e-01], sum to 1.0000
[2019-04-09 15:15:01,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1844
[2019-04-09 15:15:01,551] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00068625 0.08396302 0.06847477 0.014398   0.00274731 0.00072952
 0.32041025 0.01621503 0.05248877 0.02474563 0.4151414 ], sum to 1.0000
[2019-04-09 15:15:01,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-09 15:15:01,555] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.1, 87.16666666666667, 0.0, 0.0, 19.0, 27.56050042680423, 1.063671930665559, 0.0, 1.0, 45.0, 29.24818944176021], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1642200.0000, 
sim time next is 1642800.0000, 
raw observation next is [7.0, 88.33333333333334, 0.0, 0.0, 19.0, 27.53137260855031, 1.071690954581596, 0.0, 1.0, 25.0, 28.40976908640974], 
processed observation next is [1.0, 0.0, 0.6565096952908588, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.7942810507125259, 0.8572303181938654, 0.0, 1.0, 0.2, 0.28409769086409736], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.05553456], dtype=float32), -0.46214637]. 
=============================================
[2019-04-09 15:15:01,577] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.3, 91.33333333333334, 0.0, 0.0, 19.0, 26.76054920996501, 0.8110181915787068, 0.0, 1.0, 65.0, 45.69893211490643], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1734000.0000, 
sim time next is 1734600.0000, 
raw observation next is [0.25, 91.16666666666667, 0.0, 0.0, 19.0, 26.75364995408543, 0.8072503085745509, 0.0, 1.0, 55.0, 45.46585066965093], 
processed observation next is [0.0, 0.043478260869565216, 0.46952908587257625, 0.9116666666666667, 0.0, 0.0, 0.08333333333333333, 0.7294708295071191, 0.769083436191517, 0.0, 1.0, 0.8, 0.4546585066965093], 
reward next is 0.5453, 
noisyNet noise sample is [array([-1.8620691], dtype=float32), 1.6056844]. 
=============================================
[2019-04-09 15:15:02,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1293918e-05 7.0544891e-02 1.4923315e-01 9.6929334e-03 8.2351238e-04
 8.9899506e-05 2.1444698e-01 1.2820469e-02 2.1290351e-02 1.7683830e-02
 5.0329268e-01], sum to 1.0000
[2019-04-09 15:15:02,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9366
[2019-04-09 15:15:02,153] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 79.33333333333334, 0.0, 0.0, 19.0, 27.79861808337805, 1.132588611159087, 0.0, 1.0, 65.0, 26.83868036152392], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1632000.0000, 
sim time next is 1632600.0000, 
raw observation next is [6.9, 81.0, 0.0, 0.0, 19.0, 27.78275286896362, 1.133160877383153, 0.0, 1.0, 65.0, 26.95180951141646], 
processed observation next is [1.0, 0.9130434782608695, 0.6537396121883658, 0.81, 0.0, 0.0, 0.08333333333333333, 0.8152294057469683, 0.877720292461051, 0.0, 1.0, 1.0, 0.2695180951141646], 
reward next is 0.7305, 
noisyNet noise sample is [array([-1.0983515], dtype=float32), 1.1990839]. 
=============================================
[2019-04-09 15:15:02,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00171617 0.11739083 0.14534447 0.04666859 0.00983326 0.00363889
 0.26697952 0.03359764 0.04894118 0.06044234 0.26544708], sum to 1.0000
[2019-04-09 15:15:02,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-09 15:15:02,335] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 91.0, 0.0, 0.0, 19.0, 26.33652880529494, 0.7395289659444897, 0.0, 1.0, 45.0, 34.95653049611266], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1738800.0000, 
sim time next is 1739400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 0.0, 0.0, 19.0, 26.3285033851074, 0.7263839048210438, 0.0, 1.0, 25.0, 33.37068051077001], 
processed observation next is [0.0, 0.13043478260869565, 0.4598337950138504, 0.9033333333333334, 0.0, 0.0, 0.08333333333333333, 0.69404194875895, 0.7421279682736812, 0.0, 1.0, 0.2, 0.33370680510770007], 
reward next is 0.6663, 
noisyNet noise sample is [array([-1.2570782], dtype=float32), -0.6235828]. 
=============================================
[2019-04-09 15:15:02,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00155237 0.11643483 0.15724595 0.03057683 0.01139898 0.0039328
 0.29323354 0.0204247  0.05031977 0.05004966 0.26483056], sum to 1.0000
[2019-04-09 15:15:02,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8610
[2019-04-09 15:15:02,417] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 89.0, 0.0, 0.0, 19.0, 26.23857367991134, 0.7087320357319261, 0.0, 1.0, 65.0, 59.55408515152928], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1740600.0000, 
sim time next is 1741200.0000, 
raw observation next is [-0.4, 88.33333333333334, 0.0, 0.0, 19.0, 26.19825945629681, 0.7337486851085625, 0.0, 1.0, 65.0, 76.8570078951417], 
processed observation next is [0.0, 0.13043478260869565, 0.45152354570637127, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.6831882880247342, 0.7445828950361876, 0.0, 1.0, 1.0, 0.768570078951417], 
reward next is 0.2314, 
noisyNet noise sample is [array([-1.422127], dtype=float32), -0.6453903]. 
=============================================
[2019-04-09 15:15:02,478] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00127872 0.1410922  0.1097164  0.02584657 0.00938655 0.00235632
 0.31210816 0.026135   0.04364629 0.02770987 0.30072397], sum to 1.0000
[2019-04-09 15:15:02,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-09 15:15:02,507] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 87.0, 0.0, 0.0, 19.0, 26.52339847009317, 0.7348246764286817, 0.0, 1.0, 65.0, 50.82856606685737], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1752600.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 26.51575139173026, 0.7350506125513229, 0.0, 1.0, 45.0, 46.32752330746547], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7096459493108549, 0.7450168708504409, 0.0, 1.0, 0.6, 0.4632752330746547], 
reward next is 0.5367, 
noisyNet noise sample is [array([1.4649086], dtype=float32), -0.19457783]. 
=============================================
[2019-04-09 15:15:02,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00114456 0.07058392 0.1285997  0.02440431 0.01047649 0.00338309
 0.3413961  0.02828384 0.0485609  0.04817944 0.29498765], sum to 1.0000
[2019-04-09 15:15:02,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5967
[2019-04-09 15:15:02,590] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 86.33333333333333, 0.0, 0.0, 19.0, 26.37912982196196, 0.7486116792397927, 0.0, 1.0, 25.0, 49.11066190778694], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1743000.0000, 
sim time next is 1743600.0000, 
raw observation next is [-0.6, 85.66666666666667, 0.0, 0.0, 19.0, 26.42918375295235, 0.7496043632203211, 0.0, 1.0, 50.0, 39.65870496848517], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.702431979412696, 0.7498681210734404, 0.0, 1.0, 0.7, 0.3965870496848517], 
reward next is 0.6034, 
noisyNet noise sample is [array([-0.54153997], dtype=float32), 1.3575703]. 
=============================================
[2019-04-09 15:15:02,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.60912702e-04 6.39558062e-02 1.09790735e-01 1.92751829e-02
 2.11562682e-03 9.28070338e-04 4.56534624e-01 1.60844680e-02
 3.11650839e-02 2.31316239e-02 2.76657820e-01], sum to 1.0000
[2019-04-09 15:15:02,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5940
[2019-04-09 15:15:02,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5205800e-04 4.5455560e-02 8.9751415e-02 6.7238235e-03 1.8598839e-03
 2.2369332e-04 3.1117862e-01 9.1138324e-03 1.7096089e-02 2.9899323e-02
 4.8854575e-01], sum to 1.0000
[2019-04-09 15:15:02,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-09 15:15:02,634] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.5452617429331, 1.021645990506998, 0.0, 1.0, 45.0, 22.55010397507293], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1657200.0000, 
sim time next is 1657800.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.59254818953288, 1.016764015814032, 0.0, 1.0, 45.0, 25.62165104199567], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7993790157944067, 0.8389213386046773, 0.0, 1.0, 0.6, 0.2562165104199567], 
reward next is 0.7438, 
noisyNet noise sample is [array([2.9767537], dtype=float32), 0.8469573]. 
=============================================
[2019-04-09 15:15:02,635] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 88.33333333333334, 0.0, 0.0, 19.0, 27.53048255090941, 1.06530363202421, 0.0, 1.0, 45.0, 27.96292326086653], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1642800.0000, 
sim time next is 1643400.0000, 
raw observation next is [6.9, 89.5, 0.0, 0.0, 19.0, 27.58332713779793, 1.065912451829298, 0.0, 1.0, 65.0, 29.41231337616219], 
processed observation next is [1.0, 0.0, 0.6537396121883658, 0.895, 0.0, 0.0, 0.08333333333333333, 0.7986105948164942, 0.855304150609766, 0.0, 1.0, 1.0, 0.2941231337616219], 
reward next is 0.7059, 
noisyNet noise sample is [array([-0.09862818], dtype=float32), 1.216672]. 
=============================================
[2019-04-09 15:15:02,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.55154046e-05 1.01299204e-01 1.24818504e-01 1.22664794e-02
 1.99443707e-03 1.94210661e-04 2.65892625e-01 9.08117648e-03
 2.97882445e-02 2.45945938e-02 4.30044979e-01], sum to 1.0000
[2019-04-09 15:15:02,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5234
[2019-04-09 15:15:02,768] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 22.5, 27.07957345834084, 0.9198640048902939, 1.0, 1.0, 65.0, 53.82766990652787], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 22.5, 27.07617514174167, 0.9207405993789042, 1.0, 1.0, 55.0, 39.41805316803574], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.375, 0.7563479284784723, 0.8069135331263014, 1.0, 1.0, 0.8, 0.3941805316803574], 
reward next is 0.6058, 
noisyNet noise sample is [array([-0.69791085], dtype=float32), 0.03283127]. 
=============================================
[2019-04-09 15:15:03,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9619527e-04 4.4561211e-02 1.8287170e-01 7.1993130e-03 2.2692804e-03
 7.5426319e-04 3.6127308e-01 1.6657541e-02 2.4487309e-02 6.9640547e-02
 2.8998947e-01], sum to 1.0000
[2019-04-09 15:15:03,293] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3638
[2019-04-09 15:15:03,308] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.53229924169256, 1.010790270103557, 0.0, 1.0, 65.0, 32.91577429464841], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.47872606604524, 1.015545460313706, 0.0, 1.0, 45.0, 32.02936512165378], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7898938388371034, 0.838515153437902, 0.0, 1.0, 0.6, 0.3202936512165378], 
reward next is 0.6797, 
noisyNet noise sample is [array([-0.5066424], dtype=float32), 0.52203083]. 
=============================================
[2019-04-09 15:15:03,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00061882 0.08032662 0.08637881 0.01365756 0.0031343  0.00086192
 0.49054253 0.02262076 0.03729419 0.0293668  0.23519766], sum to 1.0000
[2019-04-09 15:15:03,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1442
[2019-04-09 15:15:03,560] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.52914780846868, 1.025926969405707, 0.0, 1.0, 60.0, 28.57407529156534], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.47565004885374, 1.030688394946335, 0.0, 1.0, 25.0, 30.66909832032934], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7896375040711451, 0.8435627983154449, 0.0, 1.0, 0.2, 0.3066909832032934], 
reward next is 0.6933, 
noisyNet noise sample is [array([-2.3782127], dtype=float32), -1.3705736]. 
=============================================
[2019-04-09 15:15:03,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00168376 0.10527723 0.11072556 0.02652735 0.01188782 0.00291165
 0.3144931  0.02355121 0.0645742  0.04581048 0.29255763], sum to 1.0000
[2019-04-09 15:15:03,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1569
[2019-04-09 15:15:03,813] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4069737e-05 1.5421064e-01 1.3385199e-01 1.2338962e-02 2.4606201e-03
 1.3989398e-04 2.7375016e-01 9.5624216e-03 4.2763628e-02 3.3787977e-02
 3.3707958e-01], sum to 1.0000
[2019-04-09 15:15:03,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5170
[2019-04-09 15:15:03,823] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 87.0, 97.0, 0.0, 19.0, 26.34685690652202, 0.6971004989763409, 0.0, 1.0, 45.0, 49.75710277556752], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [-2.3, 87.0, 100.6666666666667, 0.0, 19.0, 26.35977882822083, 0.6964480681725257, 0.0, 1.0, 25.0, 40.40879012334683], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.33555555555555566, 0.0, 0.08333333333333333, 0.6966482356850691, 0.7321493560575085, 0.0, 1.0, 0.2, 0.4040879012334683], 
reward next is 0.5959, 
noisyNet noise sample is [array([1.4013295], dtype=float32), 0.7239209]. 
=============================================
[2019-04-09 15:15:03,859] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 22.5, 27.98233519440594, 1.082759982689848, 1.0, 1.0, 65.0, 32.050037663949], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1687200.0000, 
sim time next is 1687800.0000, 
raw observation next is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 22.5, 27.8931002318862, 0.9331001486629189, 1.0, 1.0, 45.0, 71.09540121959347], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8733333333333334, 0.348888888888889, 0.0, 0.375, 0.8244250193238502, 0.8110333828876396, 1.0, 1.0, 0.6, 0.7109540121959347], 
reward next is 0.2890, 
noisyNet noise sample is [array([-0.24434207], dtype=float32), -0.30296907]. 
=============================================
[2019-04-09 15:15:03,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8558058e-04 8.7877035e-02 1.0793148e-01 7.6604099e-03 2.2460432e-03
 3.9306970e-04 1.9967887e-01 1.2185863e-02 2.3468075e-02 2.8886791e-02
 5.2938682e-01], sum to 1.0000
[2019-04-09 15:15:03,943] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0426
[2019-04-09 15:15:03,962] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00232967 0.08495793 0.18920727 0.01964514 0.00828405 0.00210966
 0.2788517  0.01821131 0.0327675  0.02839204 0.33524367], sum to 1.0000
[2019-04-09 15:15:03,962] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8674
[2019-04-09 15:15:03,965] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.083333333333334, 92.83333333333334, 0.0, 0.0, 19.0, 27.47604309443808, 0.9965710443600565, 0.0, 1.0, 25.0, 32.11236447544769], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1666200.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 19.0, 27.45959109568917, 0.9951064327442363, 0.0, 1.0, 20.0, 26.2036207984784], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7882992579740975, 0.8317021442480788, 0.0, 1.0, 0.1, 0.262036207984784], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.71025515], dtype=float32), -0.25116023]. 
=============================================
[2019-04-09 15:15:03,978] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 19.0, 26.56845977376637, 0.7430997892341425, 0.0, 1.0, 45.0, 41.03216941809386], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 19.0, 26.53864566275512, 0.7409930626631276, 0.0, 1.0, 65.0, 52.3938661033344], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7115538052295932, 0.7469976875543759, 0.0, 1.0, 1.0, 0.5239386610333441], 
reward next is 0.4761, 
noisyNet noise sample is [array([1.4696608], dtype=float32), -0.832786]. 
=============================================
[2019-04-09 15:15:04,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0292452e-05 1.0234544e-01 1.8210289e-01 1.4993946e-02 3.3721323e-03
 1.6609348e-04 2.2582531e-01 8.1382608e-03 2.2115234e-02 2.3868039e-02
 4.1703230e-01], sum to 1.0000
[2019-04-09 15:15:04,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5424
[2019-04-09 15:15:04,201] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.1, 88.0, 83.33333333333334, 0.0, 22.5, 28.08007786531397, 1.085996037013681, 1.0, 1.0, 45.0, 23.37538569201884], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1693200.0000, 
sim time next is 1693800.0000, 
raw observation next is [1.1, 88.0, 80.0, 0.0, 22.5, 28.10295888404761, 1.084460017791319, 1.0, 1.0, 55.0, 22.39399238540929], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.26666666666666666, 0.0, 0.375, 0.8419132403373008, 0.8614866725971063, 1.0, 1.0, 0.8, 0.22393992385409292], 
reward next is 0.7761, 
noisyNet noise sample is [array([-0.4481656], dtype=float32), -1.2918108]. 
=============================================
[2019-04-09 15:15:04,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9553424e-04 1.4132045e-01 2.0107126e-01 1.8382620e-02 2.1586092e-03
 5.1472237e-04 2.4704248e-01 1.2958088e-02 3.9424881e-02 2.1385698e-02
 3.1544569e-01], sum to 1.0000
[2019-04-09 15:15:04,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-09 15:15:04,241] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 92.0, 15.5, 0.0, 22.5, 27.47396781963272, 0.9926403828959961, 1.0, 1.0, 45.0, 30.12964584606928], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1670400.0000, 
sim time next is 1671000.0000, 
raw observation next is [3.116666666666667, 92.0, 20.33333333333334, 0.0, 22.5, 27.5063401339299, 0.9886184453751584, 1.0, 1.0, 45.0, 28.07841530732082], 
processed observation next is [1.0, 0.34782608695652173, 0.5489381348107111, 0.92, 0.0677777777777778, 0.0, 0.375, 0.792195011160825, 0.8295394817917194, 1.0, 1.0, 0.6, 0.2807841530732082], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.5952125], dtype=float32), 0.35972127]. 
=============================================
[2019-04-09 15:15:04,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.962455]
 [9.760733]
 [9.687315]
 [9.543619]
 [9.53484 ]], R is [[10.72532845]
 [11.31677914]
 [11.90553951]
 [12.51630878]
 [13.091609  ]].
[2019-04-09 15:15:04,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8393878e-05 1.1006406e-01 1.8970585e-01 8.1441235e-03 1.8700685e-03
 1.3504486e-04 2.3591764e-01 2.4343976e-03 2.2631817e-02 1.8873585e-02
 4.1017497e-01], sum to 1.0000
[2019-04-09 15:15:04,485] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8062
[2019-04-09 15:15:04,507] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.35, 84.5, 53.0, 0.0, 22.5, 27.91769039455133, 1.045628152848061, 1.0, 1.0, 20.0, 25.15489821602549], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1697400.0000, 
sim time next is 1698000.0000, 
raw observation next is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 22.5, 27.96660459250521, 1.061489466992395, 1.0, 1.0, 65.0, 29.00925569056871], 
processed observation next is [1.0, 0.6521739130434783, 0.502308402585411, 0.8333333333333335, 0.16388888888888892, 0.0, 0.375, 0.8305503827087675, 0.8538298223307983, 1.0, 1.0, 1.0, 0.29009255690568714], 
reward next is 0.7099, 
noisyNet noise sample is [array([0.55839384], dtype=float32), -1.0868247]. 
=============================================
[2019-04-09 15:15:04,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00085933 0.11456782 0.13064446 0.03643496 0.00733309 0.00163275
 0.25560278 0.01809794 0.05661486 0.05130339 0.32690862], sum to 1.0000
[2019-04-09 15:15:04,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1925
[2019-04-09 15:15:04,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[12.278653]
 [12.290188]
 [12.217938]
 [12.332531]
 [12.312457]], R is [[12.93244267]
 [13.55156994]
 [14.09949207]
 [14.85883808]
 [15.387537  ]].
[2019-04-09 15:15:04,530] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 19.0, 26.31068075888377, 0.69366413778907, 0.0, 1.0, 65.0, 57.39846004405188], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1779600.0000, 
sim time next is 1780200.0000, 
raw observation next is [-2.8, 85.0, 99.0, 0.0, 19.0, 26.3330436979133, 0.6964699244740095, 0.0, 1.0, 25.0, 47.41356179475716], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.85, 0.33, 0.0, 0.08333333333333333, 0.6944203081594417, 0.7321566414913364, 0.0, 1.0, 0.2, 0.4741356179475716], 
reward next is 0.5259, 
noisyNet noise sample is [array([1.8690976], dtype=float32), -0.5030907]. 
=============================================
[2019-04-09 15:15:04,599] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00067984 0.12445957 0.1332616  0.0389757  0.00705872 0.00116419
 0.24868941 0.01776775 0.05662885 0.05833828 0.3129761 ], sum to 1.0000
[2019-04-09 15:15:04,601] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8807
[2019-04-09 15:15:04,627] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 86.33333333333333, 88.00000000000001, 0.0, 19.0, 26.37754640293984, 0.6974262021325118, 0.0, 1.0, 65.0, 53.42086205616756], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1781400.0000, 
sim time next is 1782000.0000, 
raw observation next is [-2.8, 87.0, 82.5, 0.0, 19.0, 26.3865658193259, 0.6975124578762615, 0.0, 1.0, 65.0, 52.81418076109007], 
processed observation next is [0.0, 0.6521739130434783, 0.38504155124653744, 0.87, 0.275, 0.0, 0.08333333333333333, 0.6988804849438249, 0.7325041526254205, 0.0, 1.0, 1.0, 0.5281418076109007], 
reward next is 0.4719, 
noisyNet noise sample is [array([1.8690976], dtype=float32), -0.5030907]. 
=============================================
[2019-04-09 15:15:04,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7908350e-05 9.7173072e-02 1.7808753e-01 5.3867269e-03 7.8948535e-04
 1.8856551e-04 2.4533378e-01 7.8879073e-03 2.3066964e-02 1.2821752e-02
 4.2920631e-01], sum to 1.0000
[2019-04-09 15:15:04,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7063
[2019-04-09 15:15:04,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[8.099438 ]
 [8.158198 ]
 [8.0285425]
 [7.7828555]
 [8.1745825]], R is [[8.25764751]
 [8.64086246]
 [9.13798809]
 [9.57247257]
 [9.90276337]].
[2019-04-09 15:15:04,682] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 28.10361685409598, 0.9246267774648854, 1.0, 1.0, 65.0, 74.92616291054377], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [1.35, 84.5, 30.0, 0.0, 22.5, 27.33401644002521, 1.043825300421184, 1.0, 1.0, 65.0, 36.24015380390581], 
processed observation next is [1.0, 0.6956521739130435, 0.5000000000000001, 0.845, 0.1, 0.0, 0.375, 0.7778347033354341, 0.8479417668070613, 1.0, 1.0, 1.0, 0.3624015380390581], 
reward next is 0.6376, 
noisyNet noise sample is [array([-0.2906103], dtype=float32), 1.19915]. 
=============================================
[2019-04-09 15:15:04,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[12.182117]
 [12.344587]
 [12.183195]
 [12.422571]
 [12.214136]], R is [[12.96433735]
 [13.08543205]
 [13.6876545 ]
 [14.28007984]
 [14.87788677]].
[2019-04-09 15:15:04,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00101628 0.08716679 0.19613174 0.02691705 0.00614489 0.00179824
 0.20079699 0.01402858 0.03926875 0.0344601  0.3922706 ], sum to 1.0000
[2019-04-09 15:15:04,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4186
[2019-04-09 15:15:04,935] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 19.0, 26.56537007575569, 0.7458306430379329, 0.0, 1.0, 20.0, 43.38918162193561], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 19.0, 26.55340317063343, 0.7407141036105998, 0.0, 1.0, 45.0, 41.73223584888218], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7127835975527859, 0.7469047012035333, 0.0, 1.0, 0.6, 0.4173223584888218], 
reward next is 0.5827, 
noisyNet noise sample is [array([1.43798], dtype=float32), -1.1128631]. 
=============================================
[2019-04-09 15:15:04,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00118164 0.09132883 0.18240047 0.03176675 0.00734914 0.00229912
 0.20057236 0.01514096 0.04611486 0.03321318 0.38863266], sum to 1.0000
[2019-04-09 15:15:04,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3237
[2019-04-09 15:15:05,003] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 87.0, 0.0, 0.0, 19.0, 26.53139826807192, 0.7388868642319942, 0.0, 1.0, 65.0, 51.49682962654347], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1751400.0000, 
sim time next is 1752000.0000, 
raw observation next is [-1.533333333333333, 87.0, 0.0, 0.0, 19.0, 26.51676397605839, 0.736967590658136, 0.0, 1.0, 45.0, 49.26351434415432], 
processed observation next is [0.0, 0.2608695652173913, 0.42012927054478305, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7097303313381991, 0.7456558635527121, 0.0, 1.0, 0.6, 0.49263514344154324], 
reward next is 0.5074, 
noisyNet noise sample is [array([1.43798], dtype=float32), -1.1128631]. 
=============================================
[2019-04-09 15:15:05,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[7.140213 ]
 [7.3114505]
 [7.343417 ]
 [7.3217745]
 [7.3829174]], R is [[7.53262281]
 [7.94232845]
 [8.44558239]
 [8.92723465]
 [9.41268539]].
[2019-04-09 15:15:05,054] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00094721 0.09946363 0.16011746 0.02627101 0.00868925 0.00247028
 0.3322546  0.02391506 0.06880631 0.03695272 0.24011247], sum to 1.0000
[2019-04-09 15:15:05,059] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1125
[2019-04-09 15:15:05,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1598134e-04 4.6460126e-02 1.6496502e-01 1.2922780e-02 3.0699708e-03
 3.9521404e-04 3.2702166e-01 2.1207592e-02 4.2752117e-02 1.9869039e-02
 3.6102054e-01], sum to 1.0000
[2019-04-09 15:15:05,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-09 15:15:05,076] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 19.0, 26.4655707381287, 0.714118776733662, 0.0, 1.0, 20.0, 40.86794686053439], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 19.0, 26.44139440494833, 0.7070004319257728, 0.0, 1.0, 25.0, 39.2297596605361], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.08333333333333333, 0.7034495337456942, 0.7356668106419243, 0.0, 1.0, 0.2, 0.39229759660536095], 
reward next is 0.6077, 
noisyNet noise sample is [array([-0.58011657], dtype=float32), 0.68652767]. 
=============================================
[2019-04-09 15:15:05,273] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.79725222999495, 0.8325420712851322, 0.0, 1.0, 25.0, 44.47734540190935], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1728000.0000, 
sim time next is 1728600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.79069686266066, 0.8302755395788668, 0.0, 1.0, 45.0, 37.75729369137434], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7325580718883883, 0.7767585131929556, 0.0, 1.0, 0.6, 0.37757293691374344], 
reward next is 0.6224, 
noisyNet noise sample is [array([0.8808028], dtype=float32), -0.77156687]. 
=============================================
[2019-04-09 15:15:05,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5182537e-04 6.6119000e-02 2.3053777e-01 7.2516883e-03 1.8070274e-03
 3.4272767e-04 1.6947113e-01 9.6748853e-03 2.9884119e-02 1.4216300e-02
 4.7054353e-01], sum to 1.0000
[2019-04-09 15:15:05,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4795
[2019-04-09 15:15:05,308] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 85.33333333333334, 103.0, 0.0, 22.5, 28.13065761717311, 1.095595933867759, 1.0, 1.0, 45.0, 25.51128919173217], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 22.5, 28.05437289390225, 1.090873163576541, 1.0, 1.0, 50.0, 27.02920828682116], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.375, 0.8378644078251876, 0.863624387858847, 1.0, 1.0, 0.7, 0.2702920828682116], 
reward next is 0.7297, 
noisyNet noise sample is [array([0.09903412], dtype=float32), 0.15707275]. 
=============================================
[2019-04-09 15:15:05,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6632682e-05 1.2695438e-01 1.3095233e-01 2.3313396e-02 8.6392439e-04
 1.5862293e-04 2.0344159e-01 1.4667199e-02 2.7081402e-02 1.8160278e-02
 4.5437029e-01], sum to 1.0000
[2019-04-09 15:15:05,646] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7498
[2019-04-09 15:15:05,665] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 88.0, 66.5, 0.0, 22.5, 28.0524207208828, 1.069163858779551, 1.0, 1.0, 25.0, 23.02421109312398], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1695600.0000, 
sim time next is 1696200.0000, 
raw observation next is [1.183333333333333, 86.83333333333334, 62.0, 0.0, 22.5, 28.03945011644582, 1.070631821350471, 1.0, 1.0, 65.0, 27.55592936655989], 
processed observation next is [1.0, 0.6521739130434783, 0.49538319482917825, 0.8683333333333334, 0.20666666666666667, 0.0, 0.375, 0.8366208430371517, 0.8568772737834903, 1.0, 1.0, 1.0, 0.2755592936655989], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.01314979], dtype=float32), 0.03177727]. 
=============================================
[2019-04-09 15:15:05,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00099821 0.0557184  0.12705806 0.0231829  0.00451828 0.00101163
 0.24184254 0.0216751  0.02511881 0.05487582 0.4440002 ], sum to 1.0000
[2019-04-09 15:15:05,676] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7021
[2019-04-09 15:15:05,690] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.25, 91.16666666666667, 0.0, 0.0, 19.0, 26.76688712712154, 0.8066978931661147, 0.0, 1.0, 25.0, 43.73505353773042], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [0.2, 91.0, 0.0, 0.0, 19.0, 26.75081268846286, 0.8042518860150111, 0.0, 1.0, 65.0, 47.39138707081948], 
processed observation next is [0.0, 0.08695652173913043, 0.46814404432132967, 0.91, 0.0, 0.0, 0.08333333333333333, 0.7292343907052384, 0.7680839620050036, 0.0, 1.0, 1.0, 0.47391387070819485], 
reward next is 0.5261, 
noisyNet noise sample is [array([-1.1348407], dtype=float32), 0.44681504]. 
=============================================
[2019-04-09 15:15:06,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00177789 0.07262469 0.17311366 0.02543359 0.00849792 0.00169234
 0.2276876  0.02372799 0.05312061 0.04800786 0.36431587], sum to 1.0000
[2019-04-09 15:15:06,192] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4832
[2019-04-09 15:15:06,207] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 85.0, 0.0, 0.0, 19.0, 26.48106993712237, 0.7400507562763338, 0.0, 1.0, 45.0, 38.47781732544434], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1744200.0000, 
sim time next is 1744800.0000, 
raw observation next is [-0.6, 84.33333333333333, 0.0, 0.0, 19.0, 26.44853781495399, 0.7247211273979762, 0.0, 1.0, 45.0, 34.20859534115841], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.7040448179128326, 0.7415737091326587, 0.0, 1.0, 0.6, 0.3420859534115841], 
reward next is 0.6579, 
noisyNet noise sample is [array([2.013093], dtype=float32), 0.7633522]. 
=============================================
[2019-04-09 15:15:06,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00074051 0.10099307 0.20332907 0.02104123 0.00723126 0.00228677
 0.2063261  0.02736243 0.04855501 0.03823048 0.34390405], sum to 1.0000
[2019-04-09 15:15:06,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8033
[2019-04-09 15:15:06,409] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 19.0, 26.49810842915671, 0.7140851334943704, 0.0, 1.0, 45.0, 41.03622033858655], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [-2.716666666666667, 83.0, 123.6666666666667, 0.0, 19.0, 26.50942308829434, 0.7125541185336467, 0.0, 1.0, 65.0, 50.9449260684133], 
processed observation next is [0.0, 0.5217391304347826, 0.3873499538319483, 0.83, 0.4122222222222223, 0.0, 0.08333333333333333, 0.7091185906911951, 0.7375180395112156, 0.0, 1.0, 1.0, 0.509449260684133], 
reward next is 0.4906, 
noisyNet noise sample is [array([0.0853053], dtype=float32), -0.9550178]. 
=============================================
[2019-04-09 15:15:06,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00074498 0.10965223 0.20073697 0.02378101 0.00724067 0.002193
 0.19575116 0.02534058 0.05043534 0.03468783 0.34943622], sum to 1.0000
[2019-04-09 15:15:06,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2972
[2019-04-09 15:15:06,489] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 83.0, 121.3333333333333, 0.0, 19.0, 26.51630078822954, 0.7134851018579106, 0.0, 1.0, 65.0, 50.45604256586267], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1775400.0000, 
sim time next is 1776000.0000, 
raw observation next is [-2.8, 83.0, 120.1666666666667, 0.0, 19.0, 26.51034679159779, 0.7121720051191777, 0.0, 1.0, 25.0, 47.83774882451589], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.40055555555555566, 0.0, 0.08333333333333333, 0.7091955659664825, 0.7373906683730592, 0.0, 1.0, 0.2, 0.4783774882451589], 
reward next is 0.5216, 
noisyNet noise sample is [array([0.0853053], dtype=float32), -0.9550178]. 
=============================================
[2019-04-09 15:15:06,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[8.343492]
 [8.18851 ]
 [8.351869]
 [8.373982]
 [8.025076]], R is [[ 8.87312222]
 [ 9.27983093]
 [ 9.69260311]
 [10.08622837]
 [10.57500362]].
[2019-04-09 15:15:06,971] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00085696 0.10497122 0.16933113 0.02520094 0.00852425 0.00270289
 0.32588297 0.0228733  0.03021237 0.03531805 0.27412593], sum to 1.0000
[2019-04-09 15:15:06,975] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4462
[2019-04-09 15:15:06,997] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.06666666666666668, 91.0, 0.0, 0.0, 19.0, 26.74858497040041, 0.7860130294670217, 0.0, 1.0, 20.0, 38.06320673575623], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1737600.0000, 
sim time next is 1738200.0000, 
raw observation next is [0.03333333333333333, 91.0, 0.0, 0.0, 19.0, 26.73378950080915, 0.7829834893701345, 0.0, 1.0, 45.0, 41.35087132088251], 
processed observation next is [0.0, 0.08695652173913043, 0.463527239150508, 0.91, 0.0, 0.0, 0.08333333333333333, 0.7278157917340957, 0.7609944964567115, 0.0, 1.0, 0.6, 0.4135087132088251], 
reward next is 0.5865, 
noisyNet noise sample is [array([0.18872292], dtype=float32), -0.43875605]. 
=============================================
[2019-04-09 15:15:07,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0007617  0.11532039 0.1612394  0.0174724  0.00929766 0.0020409
 0.27170068 0.01550575 0.04328632 0.04165333 0.32172152], sum to 1.0000
[2019-04-09 15:15:07,252] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7843
[2019-04-09 15:15:07,283] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 80.0, 0.0, 0.0, 19.0, 25.62969930292731, 0.4728549533028019, 0.0, 1.0, 65.0, 66.9643804344312], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1813200.0000, 
sim time next is 1813800.0000, 
raw observation next is [-5.0, 79.5, 0.0, 0.0, 19.0, 25.58221726846234, 0.4924188377614389, 0.0, 1.0, 65.0, 67.36839194904138], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.795, 0.0, 0.0, 0.08333333333333333, 0.6318514390385284, 0.6641396125871463, 0.0, 1.0, 1.0, 0.6736839194904138], 
reward next is 0.3263, 
noisyNet noise sample is [array([0.28710982], dtype=float32), -0.9247035]. 
=============================================
[2019-04-09 15:15:07,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00152905 0.15539637 0.1851201  0.0296979  0.00881761 0.00166545
 0.21233307 0.01945323 0.0391384  0.05736027 0.2894886 ], sum to 1.0000
[2019-04-09 15:15:07,303] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5658
[2019-04-09 15:15:07,343] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 19.0, 26.1525937903304, 0.6146493633621227, 0.0, 1.0, 20.0, 44.2010487205627], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1791600.0000, 
sim time next is 1792200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 26.12360397939898, 0.6022143383417603, 0.0, 1.0, 45.0, 41.33222668965806], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.6769669982832482, 0.7007381127805868, 0.0, 1.0, 0.6, 0.4133222668965806], 
reward next is 0.5867, 
noisyNet noise sample is [array([-0.28594053], dtype=float32), 2.3181865]. 
=============================================
[2019-04-09 15:15:07,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00332046 0.09576099 0.11982907 0.02430847 0.01854943 0.00344136
 0.2507042  0.02731465 0.04866651 0.04317168 0.36493313], sum to 1.0000
[2019-04-09 15:15:07,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1260
[2019-04-09 15:15:07,408] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 24.02275260908281, 0.1171073956424197, 0.0, 1.0, 65.0, 66.63621082500076], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1841400.0000, 
sim time next is 1842000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 24.02055821442611, 0.1223567953239521, 0.0, 1.0, 25.0, 53.21616601506474], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5017131845355092, 0.5407855984413174, 0.0, 1.0, 0.2, 0.5321616601506475], 
reward next is 0.4678, 
noisyNet noise sample is [array([-0.13926286], dtype=float32), -0.45705196]. 
=============================================
[2019-04-09 15:15:07,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[6.4754405]
 [6.5881057]
 [6.613108 ]
 [6.5851493]
 [6.388494 ]], R is [[6.92396879]
 [7.18836689]
 [7.72584343]
 [8.26569271]
 [8.67206955]].
[2019-04-09 15:15:07,797] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00048893 0.09115011 0.15765451 0.02740194 0.00702014 0.00106856
 0.32071355 0.01067784 0.03280283 0.03530136 0.31572026], sum to 1.0000
[2019-04-09 15:15:07,797] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1378
[2019-04-09 15:15:07,817] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.399999999999999, 82.83333333333334, 0.0, 0.0, 19.0, 26.11259105947056, 0.603408565606108, 0.0, 1.0, 65.0, 55.60152390370936], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1795800.0000, 
sim time next is 1796400.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 19.0, 26.10260414763959, 0.6019648369869985, 0.0, 1.0, 25.0, 52.21614508094301], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6752170123032991, 0.7006549456623329, 0.0, 1.0, 0.2, 0.5221614508094301], 
reward next is 0.4778, 
noisyNet noise sample is [array([-0.36026657], dtype=float32), -0.5262566]. 
=============================================
[2019-04-09 15:15:07,918] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00119548 0.08709537 0.13908175 0.02292579 0.01010208 0.00253637
 0.3497086  0.01987875 0.03734692 0.0260825  0.30404642], sum to 1.0000
[2019-04-09 15:15:07,918] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-09 15:15:07,932] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 88.33333333333334, 0.0, 0.0, 19.0, 26.66727812132729, 0.780176584116044, 0.0, 1.0, 45.0, 40.50522557053871], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1741200.0000, 
sim time next is 1741800.0000, 
raw observation next is [-0.5, 87.66666666666666, 0.0, 0.0, 19.0, 26.64780324443308, 0.7735596299943093, 0.0, 1.0, 45.0, 38.88925352086045], 
processed observation next is [0.0, 0.13043478260869565, 0.44875346260387816, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.7206502703694234, 0.7578532099981031, 0.0, 1.0, 0.6, 0.3888925352086045], 
reward next is 0.6111, 
noisyNet noise sample is [array([-0.6961786], dtype=float32), -0.54017276]. 
=============================================
[2019-04-09 15:15:08,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00166066 0.10468978 0.11914198 0.03849714 0.00997082 0.00318537
 0.32862008 0.02629513 0.04944612 0.08281635 0.23567654], sum to 1.0000
[2019-04-09 15:15:08,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6037
[2019-04-09 15:15:08,693] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 84.33333333333334, 58.5, 0.0, 19.0, 26.22159147588252, 0.6762154148522477, 0.0, 1.0, 55.0, 44.57844280664108], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1761600.0000, 
sim time next is 1762200.0000, 
raw observation next is [-2.0, 85.0, 65.0, 0.0, 19.0, 26.19838594139011, 0.6649199187293612, 0.0, 1.0, 45.0, 38.91900601552371], 
processed observation next is [0.0, 0.391304347826087, 0.40720221606648205, 0.85, 0.21666666666666667, 0.0, 0.08333333333333333, 0.6831988284491759, 0.7216399729097871, 0.0, 1.0, 0.6, 0.38919006015523716], 
reward next is 0.6108, 
noisyNet noise sample is [array([-1.0239601], dtype=float32), 0.6466745]. 
=============================================
[2019-04-09 15:15:08,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00140583 0.07049184 0.12325131 0.02654318 0.00749135 0.00282914
 0.31636265 0.01559662 0.049673   0.05392457 0.3324305 ], sum to 1.0000
[2019-04-09 15:15:08,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4063
[2019-04-09 15:15:08,761] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 85.0, 0.0, 0.0, 19.0, 26.57955782829417, 0.7650677630469603, 0.0, 1.0, 45.0, 39.12704334811354], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1744200.0000, 
sim time next is 1744800.0000, 
raw observation next is [-0.6, 84.33333333333333, 0.0, 0.0, 19.0, 26.54431263559184, 0.7536880970170298, 0.0, 1.0, 45.0, 37.24839466944856], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.7120260529659866, 0.7512293656723433, 0.0, 1.0, 0.6, 0.3724839466944856], 
reward next is 0.6275, 
noisyNet noise sample is [array([0.6094296], dtype=float32), -1.3247646]. 
=============================================
[2019-04-09 15:15:08,785] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00203487 0.13757776 0.1721376  0.05233102 0.01254607 0.00357743
 0.23003978 0.02115212 0.05263077 0.04477436 0.2711982 ], sum to 1.0000
[2019-04-09 15:15:08,786] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0862
[2019-04-09 15:15:08,802] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 19.0, 24.96069907932952, 0.3190012518011587, 0.0, 1.0, 45.0, 41.82283339063584], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1834200.0000, 
sim time next is 1834800.0000, 
raw observation next is [-6.199999999999999, 79.0, 0.0, 0.0, 19.0, 25.0019212606924, 0.3029447552394158, 0.0, 1.0, 20.0, 39.43862598871581], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.5834934383910332, 0.6009815850798053, 0.0, 1.0, 0.1, 0.39438625988715814], 
reward next is 0.6056, 
noisyNet noise sample is [array([-3.081726], dtype=float32), -0.45543578]. 
=============================================
[2019-04-09 15:15:08,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00125484 0.13478395 0.12710029 0.01898964 0.0116849  0.00219236
 0.213218   0.01451877 0.08891163 0.05932655 0.32801905], sum to 1.0000
[2019-04-09 15:15:08,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6848
[2019-04-09 15:15:08,827] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 87.0, 81.0, 0.0, 19.0, 26.13110075087287, 0.6402481223776063, 0.0, 1.0, 25.0, 33.0521472253247], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1764000.0000, 
sim time next is 1764600.0000, 
raw observation next is [-2.3, 87.0, 86.33333333333333, 0.0, 19.0, 26.06284914180408, 0.6456602357829425, 0.0, 1.0, 65.0, 61.70875092788546], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.28777777777777774, 0.0, 0.08333333333333333, 0.67190409515034, 0.7152200785943141, 0.0, 1.0, 1.0, 0.6170875092788546], 
reward next is 0.3829, 
noisyNet noise sample is [array([-0.56211287], dtype=float32), -1.4916297]. 
=============================================
[2019-04-09 15:15:09,072] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00159538 0.09143019 0.13981253 0.03729307 0.01658358 0.00367968
 0.29030544 0.01923712 0.0544104  0.04622076 0.2994319 ], sum to 1.0000
[2019-04-09 15:15:09,072] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7173
[2019-04-09 15:15:09,091] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 24.7233756705251, 0.2512856993103332, 0.0, 1.0, 25.0, 40.79525270152003], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1840200.0000, 
sim time next is 1840800.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 24.68604202386479, 0.2522604792838439, 0.0, 1.0, 65.0, 68.34023261839772], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5571701686553991, 0.584086826427948, 0.0, 1.0, 1.0, 0.6834023261839772], 
reward next is 0.3166, 
noisyNet noise sample is [array([-0.94426626], dtype=float32), 1.3953841]. 
=============================================
[2019-04-09 15:15:09,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0021976  0.10069014 0.11621214 0.03036991 0.01295915 0.00223259
 0.27919352 0.026293   0.04670206 0.05380178 0.32934806], sum to 1.0000
[2019-04-09 15:15:09,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8920
[2019-04-09 15:15:09,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00098126 0.1133796  0.11807253 0.02799998 0.00835748 0.00167313
 0.3407648  0.02610775 0.04401964 0.05360676 0.26503706], sum to 1.0000
[2019-04-09 15:15:09,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1240
[2019-04-09 15:15:09,587] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 79.0, 167.0, 70.0, 19.0, 25.40265614163828, 0.4124226325001081, 0.0, 1.0, 20.0, 42.91692949148087], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1867200.0000, 
sim time next is 1867800.0000, 
raw observation next is [-4.5, 81.0, 148.0, 56.00000000000001, 19.0, 25.40722454811331, 0.4035403805301216, 0.0, 1.0, 25.0, 40.46295312648952], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.81, 0.49333333333333335, 0.06187845303867404, 0.08333333333333333, 0.6172687123427757, 0.6345134601767072, 0.0, 1.0, 0.2, 0.40462953126489526], 
reward next is 0.5954, 
noisyNet noise sample is [array([-0.32383642], dtype=float32), -1.8197942]. 
=============================================
[2019-04-09 15:15:09,602] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 19.0, 25.57778742941535, 0.4590296704607503, 0.0, 1.0, 65.0, 61.08519736417079], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1828800.0000, 
sim time next is 1829400.0000, 
raw observation next is [-6.2, 82.33333333333334, 0.0, 0.0, 19.0, 25.63081291879339, 0.4584887414624654, 0.0, 1.0, 65.0, 59.68637320761363], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.8233333333333335, 0.0, 0.0, 0.08333333333333333, 0.6359010765661157, 0.6528295804874885, 0.0, 1.0, 1.0, 0.5968637320761363], 
reward next is 0.4031, 
noisyNet noise sample is [array([-0.3561463], dtype=float32), -1.8729184]. 
=============================================
[2019-04-09 15:15:09,758] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00114217 0.07665308 0.14865033 0.03350825 0.00800766 0.00196119
 0.42511916 0.02273716 0.05726239 0.03709353 0.18786505], sum to 1.0000
[2019-04-09 15:15:09,759] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-09 15:15:09,786] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 19.0, 25.49158857569235, 0.5591778699662973, 0.0, 1.0, 20.0, 46.45915178654692], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1770600.0000, 
sim time next is 1771200.0000, 
raw observation next is [-2.3, 83.0, 122.5, 0.0, 19.0, 25.55549604260655, 0.559681397940839, 0.0, 1.0, 45.0, 34.37656396674795], 
processed observation next is [0.0, 0.5217391304347826, 0.3988919667590028, 0.83, 0.4083333333333333, 0.0, 0.08333333333333333, 0.6296246702172125, 0.6865604659802796, 0.0, 1.0, 0.6, 0.34376563966747953], 
reward next is 0.6562, 
noisyNet noise sample is [array([-1.7673297], dtype=float32), 0.43028188]. 
=============================================
[2019-04-09 15:15:10,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00060702 0.08231786 0.13321003 0.02776513 0.003225   0.00122043
 0.25947773 0.01914452 0.06250067 0.04070099 0.36983058], sum to 1.0000
[2019-04-09 15:15:10,251] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0749
[2019-04-09 15:15:10,264] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 83.0, 115.6666666666667, 0.0, 19.0, 25.35570640092751, 0.5066950233830053, 0.0, 1.0, 30.0, 41.70020575789388], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1777200.0000, 
sim time next is 1777800.0000, 
raw observation next is [-2.8, 83.0, 112.3333333333333, 0.0, 19.0, 25.38347460415143, 0.5009831640716627, 0.0, 1.0, 20.0, 26.93885365330651], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.37444444444444436, 0.0, 0.08333333333333333, 0.6152895503459526, 0.6669943880238876, 0.0, 1.0, 0.1, 0.2693885365330651], 
reward next is 0.7306, 
noisyNet noise sample is [array([-1.6841862], dtype=float32), 1.526771]. 
=============================================
[2019-04-09 15:15:10,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00093639 0.14949986 0.1780677  0.0290879  0.00569538 0.00147431
 0.23941088 0.02335391 0.04598436 0.0611462  0.26534313], sum to 1.0000
[2019-04-09 15:15:10,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6069
[2019-04-09 15:15:10,529] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 19.0, 26.00171981646627, 0.5924796888961685, 0.0, 1.0, 20.0, 34.12965377667093], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 19.0, 25.925894345041, 0.573917618773098, 0.0, 1.0, 50.0, 37.86517254383065], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 0.08333333333333333, 0.6604911954200835, 0.691305872924366, 0.0, 1.0, 0.7, 0.37865172543830655], 
reward next is 0.6213, 
noisyNet noise sample is [array([-0.4905331], dtype=float32), 0.554483]. 
=============================================
[2019-04-09 15:15:10,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00117883 0.06922273 0.21178508 0.03499541 0.00670394 0.00199303
 0.270554   0.02784231 0.04992495 0.04566848 0.28013125], sum to 1.0000
[2019-04-09 15:15:10,617] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4432
[2019-04-09 15:15:10,643] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 19.0, 26.1880809155925, 0.6196330074150972, 0.0, 1.0, 45.0, 42.9105174129995], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1791600.0000, 
sim time next is 1792200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 26.16561002191193, 0.6096082388312626, 0.0, 1.0, 45.0, 42.42870627652951], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.6804675018259942, 0.7032027462770875, 0.0, 1.0, 0.6, 0.4242870627652951], 
reward next is 0.5757, 
noisyNet noise sample is [array([-0.08889392], dtype=float32), -1.3811079]. 
=============================================
[2019-04-09 15:15:10,676] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00100313 0.09836496 0.1309532  0.01928887 0.00638949 0.00243393
 0.29582456 0.01304281 0.04417265 0.03509425 0.35343212], sum to 1.0000
[2019-04-09 15:15:10,676] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0711
[2019-04-09 15:15:10,689] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.8, 80.5, 0.0, 0.0, 19.0, 25.77427928791612, 0.5170932574406405, 0.0, 1.0, 45.0, 50.2284425754126], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1819800.0000, 
sim time next is 1820400.0000, 
raw observation next is [-5.866666666666667, 81.33333333333333, 0.0, 0.0, 19.0, 25.80995039053367, 0.5118003474205103, 0.0, 1.0, 20.0, 44.59402482603808], 
processed observation next is [0.0, 0.043478260869565216, 0.30009233610341646, 0.8133333333333332, 0.0, 0.0, 0.08333333333333333, 0.650829199211139, 0.6706001158068368, 0.0, 1.0, 0.1, 0.4459402482603808], 
reward next is 0.5541, 
noisyNet noise sample is [array([-0.9696764], dtype=float32), -1.0322042]. 
=============================================
[2019-04-09 15:15:11,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00072728 0.06981274 0.11375632 0.02183192 0.00523511 0.00124385
 0.20176944 0.02102374 0.03264648 0.02478157 0.5071715 ], sum to 1.0000
[2019-04-09 15:15:11,449] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5912
[2019-04-09 15:15:11,466] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 19.0, 23.94666838716446, 0.02010567412423507, 0.0, 1.0, 45.0, 48.83158415028607], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1918200.0000, 
sim time next is 1918800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 19.0, 23.97012506219912, 0.02634377711413829, 0.0, 1.0, 65.0, 63.04621471680824], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.08333333333333333, 0.49751042184992667, 0.5087812590380462, 0.0, 1.0, 1.0, 0.6304621471680824], 
reward next is 0.3695, 
noisyNet noise sample is [array([1.0951211], dtype=float32), -0.24383971]. 
=============================================
[2019-04-09 15:15:11,589] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00097882 0.09781697 0.16766028 0.03063471 0.00677008 0.00188022
 0.25202218 0.02045589 0.0515448  0.03444888 0.33578715], sum to 1.0000
[2019-04-09 15:15:11,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9090
[2019-04-09 15:15:11,621] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 19.0, 25.02960334710644, 0.3497164185579648, 0.0, 1.0, 20.0, 37.68526598276876], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1810800.0000, 
sim time next is 1811400.0000, 
raw observation next is [-5.0, 81.50000000000001, 0.0, 0.0, 19.0, 24.99510342686282, 0.3513550443884728, 0.0, 1.0, 65.0, 65.25252118113893], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8150000000000002, 0.0, 0.0, 0.08333333333333333, 0.5829252855719016, 0.617118348129491, 0.0, 1.0, 1.0, 0.6525252118113893], 
reward next is 0.3475, 
noisyNet noise sample is [array([0.02762634], dtype=float32), 0.2617867]. 
=============================================
[2019-04-09 15:15:11,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00184784 0.13089244 0.11292149 0.03192929 0.00820832 0.0034384
 0.26672634 0.01941329 0.04549516 0.04455009 0.33457732], sum to 1.0000
[2019-04-09 15:15:11,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1344
[2019-04-09 15:15:11,810] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 19.0, 24.38853989668863, 0.2134069025698625, 0.0, 1.0, 55.0, 47.69246395305053], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1821600.0000, 
sim time next is 1822200.0000, 
raw observation next is [-6.033333333333333, 83.66666666666667, 0.0, 0.0, 19.0, 24.38769786445614, 0.2096006609880222, 0.0, 1.0, 25.0, 41.94139169152494], 
processed observation next is [0.0, 0.08695652173913043, 0.29547553093259465, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.532308155371345, 0.5698668869960074, 0.0, 1.0, 0.2, 0.41941391691524943], 
reward next is 0.5806, 
noisyNet noise sample is [array([0.06063993], dtype=float32), -0.63391155]. 
=============================================
[2019-04-09 15:15:11,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00149124 0.09939577 0.10248628 0.02556296 0.00815039 0.00152814
 0.41705644 0.02489934 0.03210087 0.07407049 0.2132581 ], sum to 1.0000
[2019-04-09 15:15:11,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2981
[2019-04-09 15:15:11,958] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 19.0, 24.02676679132256, 0.09554964402497808, 0.0, 1.0, 55.0, 54.16822305365708], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1899000.0000, 
sim time next is 1899600.0000, 
raw observation next is [-7.3, 81.0, 0.0, 0.0, 19.0, 24.01915392254946, 0.09223247306474015, 0.0, 1.0, 45.0, 39.26286979442045], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.08333333333333333, 0.501596160212455, 0.5307441576882467, 0.0, 1.0, 0.6, 0.3926286979442045], 
reward next is 0.6074, 
noisyNet noise sample is [array([1.0371577], dtype=float32), -1.2009416]. 
=============================================
[2019-04-09 15:15:11,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00086812 0.07524558 0.15188766 0.01661015 0.00727206 0.0010159
 0.40714967 0.02419883 0.03696914 0.06580507 0.21297787], sum to 1.0000
[2019-04-09 15:15:11,972] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-09 15:15:11,988] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 25.43429205480948, 0.4517299354570337, 0.0, 1.0, 25.0, 52.51474474772292], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1806000.0000, 
sim time next is 1806600.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 25.45850746892511, 0.4400462888562189, 0.0, 1.0, 25.0, 37.10353409576737], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6215422890770924, 0.6466820962854063, 0.0, 1.0, 0.2, 0.3710353409576737], 
reward next is 0.6290, 
noisyNet noise sample is [array([-0.9755632], dtype=float32), -1.7057322]. 
=============================================
[2019-04-09 15:15:12,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00055229 0.07270264 0.12158375 0.01413857 0.0054859  0.00112969
 0.34819335 0.01243236 0.0556188  0.04772851 0.32043412], sum to 1.0000
[2019-04-09 15:15:12,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8343
[2019-04-09 15:15:12,262] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 83.33333333333334, 0.0, 0.0, 19.0, 25.55766249177034, 0.4604331600629936, 0.0, 1.0, 65.0, 63.8295011191238], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1809600.0000, 
sim time next is 1810200.0000, 
raw observation next is [-5.0, 82.66666666666667, 0.0, 0.0, 19.0, 25.49099140140354, 0.4782786369112935, 0.0, 1.0, 65.0, 75.81529914844685], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6242492834502951, 0.6594262123037645, 0.0, 1.0, 1.0, 0.7581529914844686], 
reward next is 0.2418, 
noisyNet noise sample is [array([-0.65934783], dtype=float32), 0.25715986]. 
=============================================
[2019-04-09 15:15:12,270] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00089784 0.07726394 0.14204131 0.02172272 0.0099361  0.0009823
 0.37122655 0.02208031 0.04815009 0.03673692 0.26896197], sum to 1.0000
[2019-04-09 15:15:12,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1559
[2019-04-09 15:15:12,292] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 19.0, 25.46243229461645, 0.4123723410070253, 0.0, 1.0, 65.0, 56.21156608596091], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 19.0, 25.50857036502606, 0.4142228049633316, 0.0, 1.0, 25.0, 50.50114454322188], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 0.08333333333333333, 0.625714197085505, 0.6380742683211106, 0.0, 1.0, 0.2, 0.5050114454322189], 
reward next is 0.4950, 
noisyNet noise sample is [array([0.50318325], dtype=float32), 0.42082202]. 
=============================================
[2019-04-09 15:15:12,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00225972 0.13644254 0.1513861  0.05070883 0.01140608 0.00204417
 0.23821443 0.0342332  0.06561877 0.03877736 0.26890892], sum to 1.0000
[2019-04-09 15:15:12,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-09 15:15:12,443] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 19.0, 25.29225032311183, 0.3901454062555312, 0.0, 1.0, 45.0, 42.07470993574503], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [-6.1, 85.0, 0.0, 0.0, 19.0, 25.26172192543396, 0.3906342618170786, 0.0, 1.0, 65.0, 63.10977630599179], 
processed observation next is [0.0, 0.08695652173913043, 0.29362880886426596, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6051434937861634, 0.6302114206056929, 0.0, 1.0, 1.0, 0.6310977630599178], 
reward next is 0.3689, 
noisyNet noise sample is [array([0.2781752], dtype=float32), 0.41838259]. 
=============================================
[2019-04-09 15:15:12,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00161758 0.09174919 0.13657548 0.02363902 0.00695477 0.00251553
 0.30264392 0.01914142 0.0571581  0.0571563  0.3008487 ], sum to 1.0000
[2019-04-09 15:15:12,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6933
[2019-04-09 15:15:12,514] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 19.0, 24.59926329546618, 0.2184202659776715, 0.0, 1.0, 60.0, 54.36251582997723], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1882800.0000, 
sim time next is 1883400.0000, 
raw observation next is [-4.683333333333334, 83.5, 0.0, 0.0, 19.0, 24.5117389092695, 0.2332544730394674, 0.0, 1.0, 60.0, 71.14247702736874], 
processed observation next is [0.0, 0.8260869565217391, 0.3328716528162512, 0.835, 0.0, 0.0, 0.08333333333333333, 0.5426449091057917, 0.5777514910131558, 0.0, 1.0, 0.9, 0.7114247702736874], 
reward next is 0.2886, 
noisyNet noise sample is [array([-1.5065346], dtype=float32), 0.6243608]. 
=============================================
[2019-04-09 15:15:12,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00116228 0.07730439 0.13424963 0.02445269 0.00726173 0.00171755
 0.36612594 0.02814728 0.03164778 0.04126553 0.28666523], sum to 1.0000
[2019-04-09 15:15:12,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-09 15:15:12,622] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 24.14397907804796, 0.04777245282893846, 0.0, 1.0, 65.0, 64.10119920158758], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1911600.0000, 
sim time next is 1912200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 19.0, 24.04787550920427, 0.05275287593419675, 0.0, 1.0, 45.0, 47.45669421273321], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5039896257670226, 0.5175842919780655, 0.0, 1.0, 0.6, 0.47456694212733214], 
reward next is 0.5254, 
noisyNet noise sample is [array([-2.784634], dtype=float32), -0.04298703]. 
=============================================
[2019-04-09 15:15:12,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00087595 0.09102237 0.13410202 0.02286724 0.01047367 0.00128318
 0.2851987  0.0145281  0.0395579  0.04600934 0.3540815 ], sum to 1.0000
[2019-04-09 15:15:12,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-09 15:15:12,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0011037  0.07639203 0.11557313 0.01621888 0.00547487 0.00245109
 0.50122595 0.02156038 0.03318414 0.01664314 0.21017262], sum to 1.0000
[2019-04-09 15:15:12,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8099
[2019-04-09 15:15:12,782] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.3, 73.0, 184.0, 81.0, 19.0, 25.62064672800081, 0.4811657340240718, 0.0, 1.0, 65.0, 58.314581774204], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1855800.0000, 
sim time next is 1856400.0000, 
raw observation next is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 19.0, 25.67510457504711, 0.4867085559494401, 0.0, 1.0, 65.0, 57.52320602391838], 
processed observation next is [0.0, 0.4782608695652174, 0.31855955678670367, 0.7233333333333333, 0.5777777777777776, 0.07458563535911603, 0.08333333333333333, 0.6395920479205927, 0.6622361853164801, 0.0, 1.0, 1.0, 0.5752320602391837], 
reward next is 0.4248, 
noisyNet noise sample is [array([0.44658673], dtype=float32), 1.5337677]. 
=============================================
[2019-04-09 15:15:12,788] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 25.5691550483947, 0.4773577800792269, 0.0, 1.0, 65.0, 58.43498158988948], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1819200.0000, 
sim time next is 1819800.0000, 
raw observation next is [-5.8, 80.5, 0.0, 0.0, 19.0, 25.61137216236664, 0.4826842990956055, 0.0, 1.0, 65.0, 60.60868832192372], 
processed observation next is [0.0, 0.043478260869565216, 0.30193905817174516, 0.805, 0.0, 0.0, 0.08333333333333333, 0.6342810135305532, 0.6608947663652018, 0.0, 1.0, 1.0, 0.6060868832192372], 
reward next is 0.3939, 
noisyNet noise sample is [array([-0.35918388], dtype=float32), 1.9660898]. 
=============================================
[2019-04-09 15:15:13,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00060577 0.09274756 0.11537863 0.02607051 0.00747479 0.00140724
 0.36454773 0.01888437 0.04211422 0.05249152 0.27827758], sum to 1.0000
[2019-04-09 15:15:13,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-09 15:15:13,512] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.50000000000001, 0.0, 0.0, 19.0, 24.74761975321068, 0.1922501580623579, 0.0, 1.0, 45.0, 33.78837563727626], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1897800.0000, 
sim time next is 1898400.0000, 
raw observation next is [-7.300000000000001, 80.0, 0.0, 0.0, 19.0, 24.62263686793743, 0.1648647547786837, 0.0, 1.0, 45.0, 32.21533842453353], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.8, 0.0, 0.0, 0.08333333333333333, 0.5518864056614525, 0.5549549182595612, 0.0, 1.0, 0.6, 0.3221533842453353], 
reward next is 0.6778, 
noisyNet noise sample is [array([-1.8976786], dtype=float32), -0.005457264]. 
=============================================
[2019-04-09 15:15:14,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00097308 0.11638703 0.1512619  0.02256433 0.00632896 0.00138514
 0.311266   0.02190717 0.04104908 0.04548488 0.28139246], sum to 1.0000
[2019-04-09 15:15:14,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4214
[2019-04-09 15:15:14,536] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 24.93034507441562, 0.2639788978790213, 0.0, 1.0, 65.0, 61.98808654292957], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1916400.0000, 
sim time next is 1917000.0000, 
raw observation next is [-8.65, 80.0, 0.0, 0.0, 19.0, 24.95686452251113, 0.2710545175063927, 0.0, 1.0, 45.0, 52.18011786769216], 
processed observation next is [1.0, 0.17391304347826086, 0.22299168975069253, 0.8, 0.0, 0.0, 0.08333333333333333, 0.5797387102092607, 0.5903515058354643, 0.0, 1.0, 0.6, 0.5218011786769217], 
reward next is 0.4782, 
noisyNet noise sample is [array([1.1781312], dtype=float32), 1.9136735]. 
=============================================
[2019-04-09 15:15:14,543] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[8.894902]
 [8.797568]
 [8.946765]
 [8.950901]
 [9.092716]], R is [[ 9.15853214]
 [ 9.44706631]
 [ 9.84093571]
 [10.30936432]
 [10.76855755]].
[2019-04-09 15:15:14,698] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00040962 0.06381916 0.09637465 0.02078721 0.00411965 0.00100551
 0.39547974 0.01396072 0.06267054 0.03817003 0.30320323], sum to 1.0000
[2019-04-09 15:15:14,703] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9354
[2019-04-09 15:15:14,731] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 82.00000000000001, 0.0, 0.0, 19.0, 24.97128954604303, 0.2443761846559263, 0.0, 1.0, 65.0, 68.07486246778957], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1919400.0000, 
sim time next is 1920000.0000, 
raw observation next is [-8.900000000000002, 82.0, 0.0, 0.0, 19.0, 24.95291359906606, 0.2619822413888539, 0.0, 1.0, 65.0, 65.96693236900526], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.82, 0.0, 0.0, 0.08333333333333333, 0.5794094665888384, 0.5873274137962846, 0.0, 1.0, 1.0, 0.6596693236900526], 
reward next is 0.3403, 
noisyNet noise sample is [array([-0.5195285], dtype=float32), 1.0687374]. 
=============================================
[2019-04-09 15:15:14,735] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[8.635644]
 [8.462033]
 [8.645133]
 [8.759313]
 [8.569235]], R is [[ 8.94401169]
 [ 9.1738224 ]
 [ 9.65114784]
 [10.14969635]
 [10.64224243]].
[2019-04-09 15:15:14,827] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.15453973e-05 8.25805590e-02 1.92203328e-01 3.22268866e-02
 1.48219836e-03 1.06387364e-04 1.84224904e-01 1.19353626e-02
 3.83654274e-02 5.66368215e-02 4.00176615e-01], sum to 1.0000
[2019-04-09 15:15:14,828] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3708
[2019-04-09 15:15:14,870] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 65.0, 228.6666666666667, 6.0, 22.5, 26.7096344923742, 0.604069002381795, 1.0, 1.0, 65.0, 41.61795041684216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1944600.0000, 
sim time next is 1945200.0000, 
raw observation next is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 22.5, 26.73096935898186, 0.594965385974637, 1.0, 1.0, 45.0, 43.45161890558668], 
processed observation next is [1.0, 0.5217391304347826, 0.3342566943674977, 0.65, 0.7594444444444443, 0.0055248618784530384, 0.375, 0.7275807799151549, 0.6983217953248789, 1.0, 1.0, 0.6, 0.43451618905586675], 
reward next is 0.5655, 
noisyNet noise sample is [array([-0.478231], dtype=float32), -0.1198371]. 
=============================================
[2019-04-09 15:15:15,075] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00076336 0.06358758 0.11514471 0.02613414 0.00545836 0.00154596
 0.30977795 0.01601855 0.05329303 0.06420285 0.34407347], sum to 1.0000
[2019-04-09 15:15:15,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8122
[2019-04-09 15:15:15,091] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 19.0, 25.06680175856873, 0.3032329472132204, 0.0, 1.0, 30.0, 33.95259358300716], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 19.0, 25.01348492152241, 0.3022359125147167, 0.0, 1.0, 65.0, 62.13134106271143], 
processed observation next is [0.0, 0.7391304347826086, 0.3287165281625116, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5844570767935341, 0.6007453041715722, 0.0, 1.0, 1.0, 0.6213134106271143], 
reward next is 0.3787, 
noisyNet noise sample is [array([-1.3347685], dtype=float32), -1.0607313]. 
=============================================
[2019-04-09 15:15:15,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[8.020521]
 [8.025064]
 [7.953396]
 [8.221029]
 [7.879257]], R is [[ 8.26572609]
 [ 8.84354305]
 [ 9.37041759]
 [ 9.83627796]
 [10.37553787]].
[2019-04-09 15:15:15,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00092803 0.1788925  0.09357494 0.03007403 0.00898896 0.00176475
 0.39057544 0.02002781 0.03752116 0.0704203  0.16723202], sum to 1.0000
[2019-04-09 15:15:15,770] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3313
[2019-04-09 15:15:15,794] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 50.5, 0.0, 19.0, 24.804830195544, 0.2468711172855984, 0.0, 1.0, 20.0, 39.64122178911823], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1872000.0000, 
sim time next is 1872600.0000, 
raw observation next is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 19.0, 24.74098787864891, 0.2293728611897493, 0.0, 1.0, 45.0, 31.22328183341095], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7633333333333333, 0.14444444444444443, 0.0, 0.08333333333333333, 0.5617489898874091, 0.5764576203965831, 0.0, 1.0, 0.6, 0.3122328183341095], 
reward next is 0.6878, 
noisyNet noise sample is [array([0.02074501], dtype=float32), 1.1917065]. 
=============================================
[2019-04-09 15:15:16,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5584319e-05 6.1541185e-02 1.5111096e-01 1.3731269e-02 1.9488940e-03
 1.5730130e-04 2.5252202e-01 5.2201808e-03 4.5494769e-02 2.2050418e-02
 4.4618738e-01], sum to 1.0000
[2019-04-09 15:15:16,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9386
[2019-04-09 15:15:16,237] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 22.5, 26.73193035260459, 0.628525376778004, 1.0, 1.0, 20.0, 39.28215532615657], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1948200.0000, 
sim time next is 1948800.0000, 
raw observation next is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 22.5, 26.70441009770872, 0.6499201921154741, 1.0, 1.0, 65.0, 39.78271442113304], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.64, 0.5066666666666667, 0.000736648250460405, 0.375, 0.7253675081423934, 0.7166400640384913, 1.0, 1.0, 1.0, 0.3978271442113304], 
reward next is 0.6022, 
noisyNet noise sample is [array([2.4211917], dtype=float32), -1.1255051]. 
=============================================
[2019-04-09 15:15:16,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3739361e-05 6.9179900e-02 1.3515730e-01 1.4714859e-02 2.0953484e-03
 1.5047159e-04 2.8074557e-01 5.5708964e-03 5.0758142e-02 2.1721378e-02
 4.1987240e-01], sum to 1.0000
[2019-04-09 15:15:16,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4242
[2019-04-09 15:15:16,249] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 22.5, 26.70441009770872, 0.6499201921154741, 1.0, 1.0, 65.0, 39.78271442113304], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1948800.0000, 
sim time next is 1949400.0000, 
raw observation next is [-3.65, 63.5, 137.0, 0.0, 22.5, 26.83710088794917, 0.6574809907769782, 1.0, 1.0, 45.0, 30.81809156327294], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.635, 0.45666666666666667, 0.0, 0.375, 0.7364250739957642, 0.7191603302589927, 1.0, 1.0, 0.6, 0.3081809156327294], 
reward next is 0.6918, 
noisyNet noise sample is [array([2.4211917], dtype=float32), -1.1255051]. 
=============================================
[2019-04-09 15:15:16,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00051749 0.07892414 0.16009992 0.03328592 0.00739264 0.00123472
 0.21887746 0.00954855 0.03009376 0.03595307 0.4240723 ], sum to 1.0000
[2019-04-09 15:15:16,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0102
[2019-04-09 15:15:16,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.8, 80.33333333333334, 0.0, 0.0, 19.0, 25.3284986751846, 0.3653509949157687, 0.0, 1.0, 65.0, 57.35116293948616], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1891200.0000, 
sim time next is 1891800.0000, 
raw observation next is [-5.9, 79.0, 0.0, 0.0, 19.0, 25.31827070063353, 0.3633729966744881, 0.0, 1.0, 20.0, 51.9361623175279], 
processed observation next is [0.0, 0.9130434782608695, 0.2991689750692521, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6098558917194609, 0.6211243322248293, 0.0, 1.0, 0.1, 0.519361623175279], 
reward next is 0.4806, 
noisyNet noise sample is [array([1.1594312], dtype=float32), -0.395075]. 
=============================================
[2019-04-09 15:15:16,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00095814 0.06555165 0.14838786 0.02099417 0.00672077 0.00163856
 0.37150118 0.02491324 0.04141548 0.04720201 0.27071694], sum to 1.0000
[2019-04-09 15:15:16,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8351
[2019-04-09 15:15:16,592] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.9, 78.0, 0.0, 0.0, 19.0, 24.51536777288663, 0.1471518946558056, 0.0, 1.0, 20.0, 53.1424029575611], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1908600.0000, 
sim time next is 1909200.0000, 
raw observation next is [-8.0, 78.0, 0.0, 0.0, 19.0, 24.51110887113892, 0.1501125907987438, 0.0, 1.0, 65.0, 58.85337945541875], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5425924059282433, 0.5500375302662479, 0.0, 1.0, 1.0, 0.5885337945541874], 
reward next is 0.4115, 
noisyNet noise sample is [array([-0.6164087], dtype=float32), -0.96876633]. 
=============================================
[2019-04-09 15:15:16,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00076964 0.04122324 0.15600647 0.01055272 0.00370005 0.0013937
 0.21886551 0.014426   0.02476594 0.06040397 0.4678927 ], sum to 1.0000
[2019-04-09 15:15:16,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1648
[2019-04-09 15:15:16,835] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.55, 76.5, 0.0, 0.0, 19.0, 24.63771085309931, 0.2149356273248505, 0.0, 1.0, 65.0, 63.98487342486693], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1906200.0000, 
sim time next is 1906800.0000, 
raw observation next is [-7.633333333333333, 77.0, 0.0, 0.0, 19.0, 24.71770359303884, 0.2259385415485656, 0.0, 1.0, 25.0, 50.8773621646876], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5598086327532368, 0.5753128471828551, 0.0, 1.0, 0.2, 0.508773621646876], 
reward next is 0.4912, 
noisyNet noise sample is [array([0.2694689], dtype=float32), -0.49672297]. 
=============================================
[2019-04-09 15:15:18,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00043592 0.07835172 0.19291249 0.01834433 0.00294362 0.00114986
 0.29937777 0.02513856 0.03435236 0.03688809 0.31010526], sum to 1.0000
[2019-04-09 15:15:18,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2253
[2019-04-09 15:15:18,229] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 22.5, 25.0692884090856, 0.2713682053007436, 1.0, 1.0, 65.0, 55.75573251803761], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1927800.0000, 
sim time next is 1928400.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 22.5, 25.18514437991447, 0.2719814901164601, 1.0, 1.0, 25.0, 49.5693615051091], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.375, 0.5987620316595391, 0.5906604967054867, 1.0, 1.0, 0.2, 0.495693615051091], 
reward next is 0.5043, 
noisyNet noise sample is [array([0.81984615], dtype=float32), 1.5734901]. 
=============================================
[2019-04-09 15:15:18,527] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00048283 0.12758638 0.13925098 0.0125856  0.00291868 0.00073865
 0.22746433 0.01382596 0.04467534 0.02616513 0.4043061 ], sum to 1.0000
[2019-04-09 15:15:18,528] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-09 15:15:18,547] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 25.63252970747715, 0.4208713545246534, 1.0, 1.0, 65.0, 52.81431792942767], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2014200.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 25.59905459454583, 0.4202723300654578, 1.0, 1.0, 65.0, 55.48173690916356], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.6332545495454859, 0.6400907766884859, 1.0, 1.0, 1.0, 0.5548173690916356], 
reward next is 0.4452, 
noisyNet noise sample is [array([1.1879661], dtype=float32), 0.6199353]. 
=============================================
[2019-04-09 15:15:18,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.66117316e-04 1.11554995e-01 1.55793577e-01 1.22024706e-02
 2.39334325e-03 3.79502570e-04 2.83771455e-01 7.79103255e-03
 3.85431387e-02 2.72578187e-02 3.60146523e-01], sum to 1.0000
[2019-04-09 15:15:18,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8184617e-05 2.1380866e-02 9.6265078e-02 1.0120941e-02 1.8232476e-03
 4.3155547e-04 2.4857347e-01 1.7757818e-02 4.0964268e-02 3.2157950e-02
 5.3042662e-01], sum to 1.0000
[2019-04-09 15:15:18,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5080
[2019-04-09 15:15:18,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-09 15:15:18,760] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.933333333333334, 85.5, 55.66666666666667, 0.0, 22.5, 25.84317234512633, 0.4425567842593756, 1.0, 1.0, 65.0, 66.57334306046808], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2020200.0000, 
sim time next is 2020800.0000, 
raw observation next is [-5.866666666666666, 85.0, 62.33333333333333, 0.0, 22.5, 25.89124883117064, 0.4623116672447646, 1.0, 1.0, 45.0, 45.79046564175747], 
processed observation next is [1.0, 0.391304347826087, 0.30009233610341646, 0.85, 0.20777777777777776, 0.0, 0.375, 0.6576040692642199, 0.6541038890815881, 1.0, 1.0, 0.6, 0.4579046564175747], 
reward next is 0.5421, 
noisyNet noise sample is [array([2.0143104], dtype=float32), -1.345257]. 
=============================================
[2019-04-09 15:15:18,770] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 25.50306974806502, 0.407007637153372, 0.0, 1.0, 45.0, 33.51565952040999], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 25.39348317680087, 0.3810057199215774, 0.0, 1.0, 45.0, 31.85049432820002], 
processed observation next is [1.0, 0.0, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.6161235980667392, 0.6270019066405258, 0.0, 1.0, 0.6, 0.3185049432820002], 
reward next is 0.6815, 
noisyNet noise sample is [array([0.76596427], dtype=float32), -0.70237786]. 
=============================================
[2019-04-09 15:15:18,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8336348e-05 7.4838601e-02 9.8322116e-02 7.7642957e-03 1.4512407e-03
 1.1828928e-04 1.7124748e-01 7.4414890e-03 2.4056843e-02 2.2610888e-02
 5.9211046e-01], sum to 1.0000
[2019-04-09 15:15:18,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4442
[2019-04-09 15:15:18,851] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 22.5, 26.5382816197174, 0.6169063298070957, 1.0, 1.0, 65.0, 61.57594432556865], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2050200.0000, 
sim time next is 2050800.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 22.5, 26.47851949764507, 0.6641127005662892, 1.0, 1.0, 65.0, 45.37476672074347], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.375, 0.7065432914704225, 0.721370900188763, 1.0, 1.0, 1.0, 0.45374766720743476], 
reward next is 0.5463, 
noisyNet noise sample is [array([-1.1667072], dtype=float32), -1.5668796]. 
=============================================
[2019-04-09 15:15:19,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5141336e-05 4.5627337e-02 2.2309342e-01 8.8410312e-03 1.4171988e-03
 1.0678216e-04 3.4413686e-01 1.4922806e-02 6.0473055e-02 2.0587351e-02
 2.8074899e-01], sum to 1.0000
[2019-04-09 15:15:19,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3590
[2019-04-09 15:15:19,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2298500e-05 5.4012503e-02 1.5988766e-01 8.9361342e-03 1.0009165e-03
 2.3262241e-04 2.5334176e-01 1.2613505e-02 3.3247828e-02 2.1183189e-02
 4.5551145e-01], sum to 1.0000
[2019-04-09 15:15:19,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4552
[2019-04-09 15:15:19,078] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.35, 68.5, 30.0, 0.0, 22.5, 26.54599191272146, 0.6337312227189206, 1.0, 1.0, 25.0, 31.95415491336244], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1960200.0000, 
sim time next is 1960800.0000, 
raw observation next is [-3.533333333333333, 70.66666666666667, 25.83333333333333, 0.3333333333333333, 22.5, 26.85895474387993, 0.6506759054892423, 1.0, 1.0, 45.0, 30.12594452030897], 
processed observation next is [1.0, 0.6956521739130435, 0.36472760849492153, 0.7066666666666667, 0.0861111111111111, 0.00036832412523020257, 0.375, 0.7382462286566609, 0.7168919684964141, 1.0, 1.0, 0.6, 0.30125944520308967], 
reward next is 0.6987, 
noisyNet noise sample is [array([-0.05041571], dtype=float32), 0.40857807]. 
=============================================
[2019-04-09 15:15:19,090] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 66.66666666666667, 230.3333333333333, 8.0, 22.5, 26.65456235062809, 0.6028332003589493, 1.0, 1.0, 65.0, 41.631133176491], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1943400.0000, 
sim time next is 1944000.0000, 
raw observation next is [-5.0, 65.0, 229.5, 7.0, 22.5, 26.72078721869324, 0.6122768574539291, 1.0, 1.0, 45.0, 39.87596091336536], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.765, 0.0077348066298342545, 0.375, 0.7267322682244366, 0.7040922858179764, 1.0, 1.0, 0.6, 0.3987596091336536], 
reward next is 0.6012, 
noisyNet noise sample is [array([0.31575865], dtype=float32), -0.1615767]. 
=============================================
[2019-04-09 15:15:19,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[11.812157]
 [11.555986]
 [11.806614]
 [11.675871]
 [11.658112]], R is [[12.36781883]
 [12.82782936]
 [13.32646751]
 [13.86811829]
 [14.3126173 ]].
[2019-04-09 15:15:19,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4249718e-05 6.3598588e-02 1.0302626e-01 1.1306977e-02 1.4022290e-03
 2.6909102e-04 4.0257150e-01 6.1482308e-03 3.8755041e-02 1.2148467e-02
 3.6071929e-01], sum to 1.0000
[2019-04-09 15:15:19,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1699
[2019-04-09 15:15:19,558] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.66666666666667, 0.0, 0.0, 19.0, 25.76882071724209, 0.5662502668011687, 0.0, 1.0, 45.0, 46.29610677986698], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2064000.0000, 
sim time next is 2064600.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 19.0, 25.77912809855377, 0.5637346415649984, 0.0, 1.0, 30.0, 34.06042236894537], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6482606748794808, 0.6879115471883329, 0.0, 1.0, 0.3, 0.3406042236894537], 
reward next is 0.6594, 
noisyNet noise sample is [array([-0.14040077], dtype=float32), -1.7861537]. 
=============================================
[2019-04-09 15:15:19,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9154800e-05 7.2211310e-02 1.7329903e-01 5.6507187e-03 6.4362242e-04
 1.8655002e-04 3.2874039e-01 1.5294085e-02 1.8083753e-02 1.2686012e-02
 3.7310541e-01], sum to 1.0000
[2019-04-09 15:15:19,578] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4705
[2019-04-09 15:15:19,593] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 62.0, 112.0, 0.0, 22.5, 27.03219717479161, 0.6865542175127025, 1.0, 1.0, 65.0, 37.96378632697277], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 22.5, 27.05930294425439, 0.6910489823052918, 1.0, 1.0, 45.0, 33.37195182154957], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 0.375, 0.7549419120211992, 0.7303496607684306, 1.0, 1.0, 0.6, 0.3337195182154957], 
reward next is 0.6663, 
noisyNet noise sample is [array([-0.41177943], dtype=float32), 0.9369732]. 
=============================================
[2019-04-09 15:15:19,599] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.40794320e-05 1.16422616e-01 1.21451192e-01 1.15986355e-02
 1.31576986e-03 1.99943606e-04 3.40450287e-01 8.89487378e-03
 2.89654490e-02 2.52273139e-02 3.45429927e-01], sum to 1.0000
[2019-04-09 15:15:19,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3649
[2019-04-09 15:15:19,639] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 59.33333333333334, 0.0, 22.5, 26.83007868509236, 0.6319892879726084, 1.0, 1.0, 65.0, 40.87988511477102], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1957800.0000, 
sim time next is 1958400.0000, 
raw observation next is [-2.8, 62.0, 52.0, 0.0, 22.5, 26.7880147267161, 0.6345718710550884, 1.0, 1.0, 45.0, 40.47447082651114], 
processed observation next is [1.0, 0.6956521739130435, 0.38504155124653744, 0.62, 0.17333333333333334, 0.0, 0.375, 0.7323345605596749, 0.7115239570183628, 1.0, 1.0, 0.6, 0.4047447082651114], 
reward next is 0.5953, 
noisyNet noise sample is [array([0.12661797], dtype=float32), 0.15185708]. 
=============================================
[2019-04-09 15:15:19,645] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2995824e-05 1.9433725e-01 1.4249809e-01 1.3332668e-02 1.3785621e-03
 1.1725837e-04 2.5913042e-01 8.8377073e-03 3.5289634e-02 1.1041760e-02
 3.3399361e-01], sum to 1.0000
[2019-04-09 15:15:19,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4748
[2019-04-09 15:15:19,671] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 22.5, 26.45769131269878, 0.517819723461993, 1.0, 1.0, 25.0, 45.34798311899706], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [-4.45, 65.0, 227.0, 4.0, 22.5, 26.34644348583012, 0.5234634589417863, 1.0, 1.0, 45.0, 40.12413411322719], 
processed observation next is [1.0, 0.5217391304347826, 0.3393351800554017, 0.65, 0.7566666666666667, 0.004419889502762431, 0.375, 0.6955369571525102, 0.6744878196472621, 1.0, 1.0, 0.6, 0.40124134113227194], 
reward next is 0.5988, 
noisyNet noise sample is [array([-0.16115929], dtype=float32), 0.90349936]. 
=============================================
[2019-04-09 15:15:20,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0061164e-05 5.2092414e-02 1.0317766e-01 4.3668258e-03 1.1280315e-03
 1.7536161e-04 2.9072332e-01 9.8573770e-03 7.6980689e-03 1.5741531e-02
 5.1499933e-01], sum to 1.0000
[2019-04-09 15:15:20,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-09 15:15:20,311] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 19.0, 26.02174884068642, 0.6093314018409481, 0.0, 1.0, 45.0, 43.77933331183106], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2059800.0000, 
sim time next is 2060400.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 26.04618906052124, 0.6081152276921522, 0.0, 1.0, 45.0, 40.15879168231555], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6705157550434366, 0.702705075897384, 0.0, 1.0, 0.6, 0.40158791682315553], 
reward next is 0.5984, 
noisyNet noise sample is [array([-0.8783917], dtype=float32), -0.20924683]. 
=============================================
[2019-04-09 15:15:20,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8448736e-05 2.9417653e-02 1.0798493e-01 5.1383264e-03 9.0231065e-04
 4.2688949e-05 3.3445200e-01 4.6092612e-03 3.2542128e-02 2.3475908e-02
 4.6139634e-01], sum to 1.0000
[2019-04-09 15:15:20,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0031
[2019-04-09 15:15:20,433] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.0, 17.0, 0.0, 22.5, 26.86082096086049, 0.6823980164038238, 1.0, 1.0, 65.0, 57.7051911200434], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2048400.0000, 
sim time next is 2049000.0000, 
raw observation next is [-3.9, 82.00000000000001, 12.0, 0.0, 22.5, 26.8996243881994, 0.5779024103077667, 1.0, 1.0, 20.0, 56.42457704458251], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8200000000000002, 0.04, 0.0, 0.375, 0.7416353656832833, 0.6926341367692556, 1.0, 1.0, 0.1, 0.5642457704458251], 
reward next is 0.4358, 
noisyNet noise sample is [array([-0.07158875], dtype=float32), -1.4195464]. 
=============================================
[2019-04-09 15:15:20,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[12.495608]
 [12.746572]
 [12.285101]
 [12.481972]
 [12.488956]], R is [[12.83887959]
 [13.13343906]
 [13.67188168]
 [14.14957809]
 [14.56773663]].
[2019-04-09 15:15:20,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.26571564e-05 7.03500360e-02 1.06698774e-01 9.10159480e-03
 2.00663204e-03 1.95558576e-04 2.41989017e-01 1.26411393e-02
 3.68852392e-02 3.04484889e-02 4.89620894e-01], sum to 1.0000
[2019-04-09 15:15:20,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0648
[2019-04-09 15:15:20,524] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 78.83333333333333, 0.0, 0.0, 19.0, 25.82200241056768, 0.5459967515630618, 0.0, 1.0, 65.0, 53.10833338043485], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1975800.0000, 
sim time next is 1976400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 19.0, 25.84043192078541, 0.549354521578141, 0.0, 1.0, 20.0, 44.33451090843768], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6533693267321175, 0.6831181738593802, 0.0, 1.0, 0.1, 0.4433451090843768], 
reward next is 0.5567, 
noisyNet noise sample is [array([1.9464469], dtype=float32), -1.7326769]. 
=============================================
[2019-04-09 15:15:20,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00054807 0.08777101 0.12565337 0.01506073 0.00369194 0.00134377
 0.3075366  0.01962275 0.02411657 0.03418665 0.38046852], sum to 1.0000
[2019-04-09 15:15:20,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2117
[2019-04-09 15:15:20,861] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 25.04044387442917, 0.3367273515559693, 0.0, 1.0, 65.0, 58.81883239289786], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2002800.0000, 
sim time next is 2003400.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 19.0, 25.10844850044659, 0.3426275426581016, 0.0, 1.0, 25.0, 42.74414730034804], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5923707083705491, 0.6142091808860338, 0.0, 1.0, 0.2, 0.4274414730034804], 
reward next is 0.5726, 
noisyNet noise sample is [array([0.84951437], dtype=float32), -0.22368063]. 
=============================================
[2019-04-09 15:15:21,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1320796e-05 1.1048712e-01 1.8361963e-01 1.3766480e-02 1.7402779e-03
 1.1257218e-04 3.2393780e-01 7.3570604e-03 4.6228535e-02 1.2783190e-02
 2.9992595e-01], sum to 1.0000
[2019-04-09 15:15:21,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3750
[2019-04-09 15:15:21,196] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.233333333333333, 79.0, 0.0, 0.0, 22.5, 26.13314328376531, 0.5731524272590504, 1.0, 1.0, 45.0, 38.79452944498605], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1971600.0000, 
sim time next is 1972200.0000, 
raw observation next is [-5.416666666666667, 81.0, 0.0, 0.0, 22.5, 26.01719710974505, 0.5582541979131076, 0.0, 1.0, 45.0, 37.11444677160009], 
processed observation next is [1.0, 0.8260869565217391, 0.3125577100646353, 0.81, 0.0, 0.0, 0.375, 0.668099759145421, 0.6860847326377025, 0.0, 1.0, 0.6, 0.37114446771600096], 
reward next is 0.6289, 
noisyNet noise sample is [array([0.98696643], dtype=float32), -0.6511993]. 
=============================================
[2019-04-09 15:15:21,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.42629246e-05 1.05762236e-01 1.64870277e-01 1.23319374e-02
 1.83557032e-03 9.68329332e-05 3.43377739e-01 6.96377968e-03
 4.59906422e-02 1.21085914e-02 3.06628138e-01], sum to 1.0000
[2019-04-09 15:15:21,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0551
[2019-04-09 15:15:21,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 25.90518402454956, 0.5404624477439478, 0.0, 1.0, 45.0, 35.40895201430653], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 19.0, 25.798347310304, 0.5197417081861931, 0.0, 1.0, 20.0, 33.15317431011514], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.8216666666666668, 0.0, 0.0, 0.08333333333333333, 0.6498622758586666, 0.6732472360620644, 0.0, 1.0, 0.1, 0.3315317431011514], 
reward next is 0.6685, 
noisyNet noise sample is [array([0.98696643], dtype=float32), -0.6511993]. 
=============================================
[2019-04-09 15:15:21,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8689290e-04 5.8595926e-02 1.1654745e-01 8.4716016e-03 1.3619633e-03
 5.5943633e-04 3.1456852e-01 5.2426835e-03 2.2466633e-02 2.3119695e-02
 4.4887912e-01], sum to 1.0000
[2019-04-09 15:15:21,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0520
[2019-04-09 15:15:21,279] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 19.0, 25.56734497104415, 0.4439043949438279, 0.0, 1.0, 25.0, 37.49404977791011], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1992000.0000, 
sim time next is 1992600.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 19.0, 25.59823345641669, 0.42498800970619, 0.0, 1.0, 20.0, 36.97183240294934], 
processed observation next is [1.0, 0.043478260869565216, 0.2991689750692521, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6331861213680575, 0.6416626699020633, 0.0, 1.0, 0.1, 0.3697183240294934], 
reward next is 0.6303, 
noisyNet noise sample is [array([-0.740689], dtype=float32), 0.6043097]. 
=============================================
[2019-04-09 15:15:21,484] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3344891e-04 6.4760648e-02 1.5534216e-01 9.2609916e-03 1.7350786e-03
 1.4374222e-04 2.7871042e-01 7.4827662e-03 1.4571155e-02 2.4786720e-02
 4.4307294e-01], sum to 1.0000
[2019-04-09 15:15:21,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4674
[2019-04-09 15:15:21,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9805612e-04 5.7823863e-02 1.6824141e-01 1.0874944e-02 3.9862739e-03
 7.4885762e-04 2.2037362e-01 4.5858878e-03 2.1288669e-02 1.5373176e-02
 4.9650523e-01], sum to 1.0000
[2019-04-09 15:15:21,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2474
[2019-04-09 15:15:21,507] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 19.0, 25.66221841442363, 0.4760124940382818, 0.0, 1.0, 65.0, 55.35892545601856], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1989000.0000, 
sim time next is 1989600.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 19.0, 25.68790490479924, 0.4740868010064958, 0.0, 1.0, 45.0, 46.03892496366571], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.6406587420666033, 0.658028933668832, 0.0, 1.0, 0.6, 0.46038924963665706], 
reward next is 0.5396, 
noisyNet noise sample is [array([0.13933545], dtype=float32), 0.4869064]. 
=============================================
[2019-04-09 15:15:21,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6402390e-04 5.0379209e-02 1.6475198e-01 1.1997457e-02 3.7651297e-03
 8.3740469e-04 2.2091077e-01 2.2608515e-02 4.8267998e-02 3.4922108e-02
 4.4119546e-01], sum to 1.0000
[2019-04-09 15:15:21,522] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 24.77128856090625, 0.2765886806615749, 0.0, 1.0, 65.0, 60.77791004424139], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1993800.0000, 
sim time next is 1994400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.73453315608324, 0.2788526312661589, 0.0, 1.0, 45.0, 47.78678398012985], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5612110963402701, 0.5929508770887196, 0.0, 1.0, 0.6, 0.4778678398012985], 
reward next is 0.5221, 
noisyNet noise sample is [array([1.6774182], dtype=float32), -2.0515037]. 
=============================================
[2019-04-09 15:15:21,523] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4031
[2019-04-09 15:15:21,540] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 25.47425178325025, 0.3942771014829248, 0.0, 1.0, 50.0, 43.00472425244957], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 25.51155894693364, 0.3883883584265606, 0.0, 1.0, 45.0, 41.33765768939226], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6259632455778034, 0.6294627861421869, 0.0, 1.0, 0.6, 0.4133765768939226], 
reward next is 0.5866, 
noisyNet noise sample is [array([-0.47767016], dtype=float32), -1.4896003]. 
=============================================
[2019-04-09 15:15:21,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[9.319444]
 [9.199752]
 [9.301297]
 [9.383355]
 [9.197964]], R is [[ 9.48182774]
 [ 9.95696259]
 [10.40275097]
 [10.74938107]
 [11.07435226]].
[2019-04-09 15:15:21,720] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-09 15:15:21,720] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:15:21,721] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:21,723] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:15:21,724] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:15:21,727] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:21,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:21,739] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run20
[2019-04-09 15:15:21,759] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run20
[2019-04-09 15:15:21,771] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run20
[2019-04-09 15:15:57,257] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.0316843], dtype=float32), 0.03644791]
[2019-04-09 15:15:57,257] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [1.0310862335, 92.94311367833335, 0.0, 0.0, 19.0, 27.01035639084585, 0.8767539454215685, 0.0, 1.0, 65.0, 45.30175801960691]
[2019-04-09 15:15:57,257] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:15:57,258] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [2.6222738e-04 7.0345767e-02 1.4146046e-01 1.4575977e-02 3.1532070e-03
 5.5858458e-04 3.2819903e-01 1.2492998e-02 2.8946672e-02 2.8946975e-02
 3.7105808e-01], sampled 0.5868063460359466
[2019-04-09 15:16:32,044] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.0316843], dtype=float32), 0.03644791]
[2019-04-09 15:16:32,044] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-2.0, 71.0, 0.0, 0.0, 22.5, 27.84920064740245, 1.024403845492899, 1.0, 1.0, 65.0, 29.87171199397169]
[2019-04-09 15:16:32,045] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:16:32,046] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [1.0972169e-04 9.7458154e-02 1.5246084e-01 1.6285133e-02 2.4354432e-03
 3.3827050e-04 2.7698240e-01 8.4288502e-03 3.3182073e-02 2.0968491e-02
 3.9135054e-01], sampled 0.7662081408683865
[2019-04-09 15:16:33,226] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.0316843], dtype=float32), 0.03644791]
[2019-04-09 15:16:33,226] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-1.7, 75.0, 0.0, 0.0, 22.5, 27.72551218363471, 1.054060094057769, 0.0, 1.0, 55.0, 27.09860301218572]
[2019-04-09 15:16:33,226] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:16:33,227] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [8.5543135e-05 8.8541843e-02 1.5338446e-01 1.3132962e-02 2.0864054e-03
 2.5295897e-04 2.9482988e-01 8.9814821e-03 3.1557132e-02 2.1150004e-02
 3.8599738e-01], sampled 0.15673745873420242
[2019-04-09 15:16:47,203] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5640.3450 285603.4683 2947.4151
[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,223] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:47,339] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,751] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5389.8370 310616.2995 2553.3247
[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,771] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:52,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,335] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5290.0347 320596.5316 2230.2610
[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,355] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:53,471] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:54,358] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 190000, evaluation results [190000.0, 5389.837004573489, 310616.29954265227, 2553.324686959085, 5640.345043761132, 285603.46830882033, 2947.4150638626434, 5290.034684087191, 320596.53159128, 2230.261023333229]
[2019-04-09 15:16:54,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2552622e-04 6.7284524e-02 1.0650440e-01 2.6952604e-02 1.8426118e-03
 6.8025157e-04 3.3846152e-01 7.0994417e-03 1.6986802e-02 3.7011411e-02
 3.9695096e-01], sum to 1.0000
[2019-04-09 15:16:54,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7607
[2019-04-09 15:16:54,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9088981e-04 5.0982460e-02 6.6429161e-02 9.3305130e-03 2.4522580e-03
 4.0081827e-04 2.8801218e-01 1.0639174e-02 1.2065707e-02 2.2707522e-02
 5.3678936e-01], sum to 1.0000
[2019-04-09 15:16:54,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7180
[2019-04-09 15:16:54,427] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.383333333333333, 79.50000000000001, 48.33333333333334, 24.66666666666667, 22.5, 25.53334725799439, 0.4228094035415887, 1.0, 1.0, 65.0, 55.57267344422296], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2103000.0000, 
sim time next is 2103600.0000, 
raw observation next is [-7.466666666666667, 80.0, 60.16666666666666, 30.83333333333334, 22.5, 25.48832581317284, 0.4586168988503374, 1.0, 1.0, 65.0, 54.7447332518825], 
processed observation next is [1.0, 0.34782608695652173, 0.25577100646352724, 0.8, 0.20055555555555551, 0.03406998158379374, 0.375, 0.6240271510977365, 0.6528722996167792, 1.0, 1.0, 1.0, 0.547447332518825], 
reward next is 0.4526, 
noisyNet noise sample is [array([0.03749501], dtype=float32), 0.14919704]. 
=============================================
[2019-04-09 15:16:54,440] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7940428e-04 5.3677600e-02 1.1767271e-01 1.0546345e-02 3.5843905e-03
 4.3970239e-04 3.2512456e-01 8.3550978e-03 2.3432598e-02 2.9468687e-02
 4.2741889e-01], sum to 1.0000
[2019-04-09 15:16:54,440] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-09 15:16:54,442] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.72205772646313, 0.3056890537021335, 0.0, 1.0, 65.0, 64.10798925179941], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 24.71909271225426, 0.3306644065694185, 0.0, 1.0, 65.0, 65.88412589511333], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5599243926878549, 0.6102214688564728, 0.0, 1.0, 1.0, 0.6588412589511333], 
reward next is 0.3412, 
noisyNet noise sample is [array([0.22411557], dtype=float32), -0.33716637]. 
=============================================
[2019-04-09 15:16:54,461] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 25.76231810708425, 0.4558511263989302, 0.0, 1.0, 60.0, 46.90544947332473], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 25.78091350942834, 0.4570680421320755, 0.0, 1.0, 65.0, 51.97109162563724], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6484094591190285, 0.6523560140440252, 0.0, 1.0, 1.0, 0.5197109162563724], 
reward next is 0.4803, 
noisyNet noise sample is [array([-0.1915948], dtype=float32), 1.7292058]. 
=============================================
[2019-04-09 15:16:54,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4113223e-04 8.6325504e-02 1.3109101e-01 1.7907003e-02 2.2094727e-03
 5.3349516e-04 2.2229561e-01 1.1671894e-02 1.9067528e-02 2.6401868e-02
 4.8215550e-01], sum to 1.0000
[2019-04-09 15:16:54,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8365
[2019-04-09 15:16:54,543] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 25.44529855956687, 0.3788020002886492, 1.0, 1.0, 65.0, 53.68716075594358], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2014200.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 25.43110332268959, 0.3795723453731745, 1.0, 1.0, 65.0, 55.66084013263752], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.6192586102241325, 0.6265241151243915, 1.0, 1.0, 1.0, 0.5566084013263751], 
reward next is 0.4434, 
noisyNet noise sample is [array([-0.1258538], dtype=float32), 1.4108413]. 
=============================================
[2019-04-09 15:16:54,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8590669e-04 6.0226358e-02 1.5279746e-01 2.2393448e-02 3.6745139e-03
 4.9857062e-04 3.9512774e-01 1.6220797e-02 4.4312533e-02 2.2247393e-02
 2.8221533e-01], sum to 1.0000
[2019-04-09 15:16:54,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5465
[2019-04-09 15:16:54,663] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 22.5, 25.49680594646191, 0.38666011434002, 1.0, 1.0, 45.0, 46.78899628395869], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 22.5, 25.48326842712288, 0.4247120275090769, 1.0, 1.0, 45.0, 39.30940227844921], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 0.375, 0.62360570226024, 0.641570675836359, 1.0, 1.0, 0.6, 0.3930940227844921], 
reward next is 0.6069, 
noisyNet noise sample is [array([0.48380005], dtype=float32), 0.42874387]. 
=============================================
[2019-04-09 15:16:54,704] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00049484 0.05988061 0.12846027 0.00947662 0.00216623 0.00064262
 0.3987191  0.01772103 0.02799904 0.04746372 0.3069759 ], sum to 1.0000
[2019-04-09 15:16:54,710] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3422
[2019-04-09 15:16:54,722] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 19.0, 25.50918310940673, 0.4740707912213581, 0.0, 1.0, 20.0, 51.62951430743392], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2077800.0000, 
sim time next is 2078400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 25.55635703627795, 0.4629251085060036, 0.0, 1.0, 45.0, 37.84362665394064], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.08333333333333333, 0.6296964196898293, 0.6543083695020012, 0.0, 1.0, 0.6, 0.3784362665394064], 
reward next is 0.6216, 
noisyNet noise sample is [array([0.7282101], dtype=float32), -1.7366923]. 
=============================================
[2019-04-09 15:16:54,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.95583141e-05 3.32772098e-02 5.45807481e-02 6.32259808e-03
 9.83686303e-04 1.12869115e-04 3.96558106e-01 9.01743863e-03
 1.55156348e-02 1.36766071e-02 4.69925553e-01], sum to 1.0000
[2019-04-09 15:16:54,827] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0105
[2019-04-09 15:16:54,851] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 22.5, 25.89383779776528, 0.5582758890493821, 0.0, 1.0, 45.0, 46.1564717963052], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2059200.0000, 
sim time next is 2059800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 25.87040404697162, 0.5529302553641897, 0.0, 1.0, 20.0, 39.33958428754932], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6558670039143015, 0.6843100851213966, 0.0, 1.0, 0.1, 0.39339584287549323], 
reward next is 0.6066, 
noisyNet noise sample is [array([-0.5123582], dtype=float32), 2.9662206]. 
=============================================
[2019-04-09 15:16:55,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00046106 0.12338091 0.16863945 0.01602528 0.00372913 0.00051026
 0.34972444 0.01795667 0.02727791 0.06544695 0.22684795], sum to 1.0000
[2019-04-09 15:16:55,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5191
[2019-04-09 15:16:55,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5547039e-05 4.7968388e-02 7.0784763e-02 7.4339984e-03 1.3619986e-03
 1.7156966e-04 4.8336047e-01 8.7493313e-03 1.8894419e-02 1.4530150e-02
 3.4666935e-01], sum to 1.0000
[2019-04-09 15:16:55,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8223
[2019-04-09 15:16:55,301] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 25.09212661701148, 0.3019565955309769, 0.0, 1.0, 65.0, 61.93693946303008], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2000400.0000, 
sim time next is 2001000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.99233344058384, 0.2978135463173104, 0.0, 1.0, 45.0, 48.60116443442795], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5826944533819868, 0.5992711821057701, 0.0, 1.0, 0.6, 0.4860116443442795], 
reward next is 0.5140, 
noisyNet noise sample is [array([0.0188393], dtype=float32), -0.6784045]. 
=============================================
[2019-04-09 15:16:55,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.772393 ]
 [9.5061865]
 [9.482647 ]
 [9.508125 ]
 [9.805065 ]], R is [[ 9.900877  ]
 [10.18249893]
 [10.71069527]
 [11.24437523]
 [11.7579546 ]].
[2019-04-09 15:16:55,316] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.399999999999999, 85.33333333333334, 0.0, 0.0, 19.0, 25.33595158267685, 0.4569327763205757, 0.0, 1.0, 65.0, 54.36129376480726], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2069400.0000, 
sim time next is 2070000.0000, 
raw observation next is [-4.5, 86.0, 0.0, 0.0, 19.0, 25.36966666232937, 0.459909715688092, 0.0, 1.0, 65.0, 57.22368588432967], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6141388885274474, 0.6533032385626973, 0.0, 1.0, 1.0, 0.5722368588432967], 
reward next is 0.4278, 
noisyNet noise sample is [array([-0.15112503], dtype=float32), -1.8064386]. 
=============================================
[2019-04-09 15:16:55,341] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[11.298012]
 [11.083177]
 [11.270104]
 [11.551539]
 [11.719995]], R is [[11.56564903]
 [11.9063797 ]
 [12.29709816]
 [12.54436684]
 [12.7887373 ]].
[2019-04-09 15:16:55,345] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.42752377e-04 1.01421945e-01 9.22580808e-02 1.80135649e-02
 2.25311751e-03 2.96817452e-04 2.77986467e-01 1.36219803e-02
 2.45306343e-02 2.21443977e-02 4.47130203e-01], sum to 1.0000
[2019-04-09 15:16:55,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1617
[2019-04-09 15:16:55,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 22.5, 25.80627362678722, 0.4335752493233826, 1.0, 1.0, 45.0, 41.47255749145194], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2018400.0000, 
sim time next is 2019000.0000, 
raw observation next is [-6.033333333333333, 86.16666666666667, 42.33333333333333, 0.0, 22.5, 25.97980734684007, 0.4458172111447167, 1.0, 1.0, 25.0, 38.59987347998747], 
processed observation next is [1.0, 0.34782608695652173, 0.29547553093259465, 0.8616666666666667, 0.1411111111111111, 0.0, 0.375, 0.6649839455700057, 0.6486057370482389, 1.0, 1.0, 0.2, 0.38599873479987473], 
reward next is 0.6140, 
noisyNet noise sample is [array([1.2921511], dtype=float32), -0.25629953]. 
=============================================
[2019-04-09 15:16:55,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[11.145813 ]
 [10.824851 ]
 [11.0767355]
 [10.753638 ]
 [10.594522 ]], R is [[11.64015579]
 [12.10902882]
 [12.52731609]
 [12.91002464]
 [13.23434734]].
[2019-04-09 15:16:55,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00043637 0.09113731 0.11449957 0.03600825 0.00282362 0.00068596
 0.29373655 0.01972094 0.02278987 0.04304308 0.37511837], sum to 1.0000
[2019-04-09 15:16:55,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0498
[2019-04-09 15:16:55,477] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 19.0, 25.27844658923482, 0.3626051070458361, 0.0, 1.0, 65.0, 54.28285432017034], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 25.31740347926089, 0.3698748493379351, 0.0, 1.0, 20.0, 48.04012309590354], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6097836232717407, 0.6232916164459784, 0.0, 1.0, 0.1, 0.48040123095903536], 
reward next is 0.5196, 
noisyNet noise sample is [array([-0.7002596], dtype=float32), 1.1866502]. 
=============================================
[2019-04-09 15:16:55,523] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00084001 0.09002939 0.2692655  0.01811927 0.00402534 0.00094769
 0.19804068 0.02783487 0.02712385 0.04143963 0.32233384], sum to 1.0000
[2019-04-09 15:16:55,535] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2277
[2019-04-09 15:16:55,552] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 19.0, 25.62691287420898, 0.4610653314031269, 0.0, 1.0, 65.0, 50.39554421284627], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2092800.0000, 
sim time next is 2093400.0000, 
raw observation next is [-6.45, 85.0, 0.0, 0.0, 19.0, 25.71606550326212, 0.457694625007975, 0.0, 1.0, 25.0, 49.37804938856689], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6430054586051765, 0.6525648750026584, 0.0, 1.0, 0.2, 0.4937804938856689], 
reward next is 0.5062, 
noisyNet noise sample is [array([0.17687368], dtype=float32), 2.0707004]. 
=============================================
[2019-04-09 15:16:55,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1906932e-04 3.3764716e-02 1.0411656e-01 6.4313626e-03 9.5586461e-04
 2.2103591e-04 3.6789891e-01 1.4770826e-02 2.0384993e-02 1.1572654e-02
 4.3976402e-01], sum to 1.0000
[2019-04-09 15:16:55,572] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1984
[2019-04-09 15:16:55,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 86.83333333333334, 0.0, 0.0, 19.0, 25.69770059952151, 0.4996790602196232, 0.0, 1.0, 45.0, 39.59558867939272], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2070600.0000, 
sim time next is 2071200.0000, 
raw observation next is [-4.5, 87.66666666666667, 0.0, 0.0, 19.0, 25.65408799342553, 0.4830979772672641, 0.0, 1.0, 45.0, 38.8429846167581], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6378406661187942, 0.661032659089088, 0.0, 1.0, 0.6, 0.388429846167581], 
reward next is 0.6116, 
noisyNet noise sample is [array([0.84362096], dtype=float32), -2.809709]. 
=============================================
[2019-04-09 15:16:56,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.70817067e-05 9.08824876e-02 1.82320565e-01 9.67693515e-03
 1.95180741e-03 1.83808530e-04 3.47533107e-01 1.06418375e-02
 4.23208065e-02 2.42455751e-02 2.90196091e-01], sum to 1.0000
[2019-04-09 15:16:56,698] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4013
[2019-04-09 15:16:56,714] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 22.5, 26.85879039330401, 0.7068940599939145, 1.0, 1.0, 45.0, 32.35624326879481], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 22.5, 26.90392807462874, 0.7117868148540154, 1.0, 1.0, 45.0, 31.87483046246872], 
processed observation next is [1.0, 0.4782608695652174, 0.27146814404432135, 0.6766666666666667, 0.8977777777777781, 0.11767955801104972, 0.375, 0.7419940062190618, 0.7372622716180052, 1.0, 1.0, 0.6, 0.3187483046246872], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.26245576], dtype=float32), -0.2585157]. 
=============================================
[2019-04-09 15:16:56,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5874636e-05 7.2890401e-02 1.2016352e-01 1.3663308e-02 1.3271024e-03
 1.5027575e-04 4.8520228e-01 1.7513594e-02 2.2989467e-02 2.3856079e-02
 2.4217813e-01], sum to 1.0000
[2019-04-09 15:16:56,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5924
[2019-04-09 15:16:56,955] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 22.5, 26.49638537139224, 0.6129264755969125, 1.0, 1.0, 20.0, 40.11135080654366], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2032800.0000, 
sim time next is 2033400.0000, 
raw observation next is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 22.5, 26.521503716283, 0.5101219490143714, 1.0, 1.0, 45.0, 46.6316572700937], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7833333333333334, 0.511111111111111, 0.0, 0.375, 0.71012530969025, 0.6700406496714572, 1.0, 1.0, 0.6, 0.46631657270093696], 
reward next is 0.5337, 
noisyNet noise sample is [array([-0.8670308], dtype=float32), 0.5845828]. 
=============================================
[2019-04-09 15:16:57,226] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2364898e-04 4.9046721e-02 1.1931733e-01 1.1254354e-02 1.0511156e-03
 1.3173227e-04 3.6573973e-01 8.3242562e-03 1.2350112e-02 2.6515823e-02
 4.0614519e-01], sum to 1.0000
[2019-04-09 15:16:57,226] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-09 15:16:57,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 82.66666666666667, 0.0, 0.0, 19.0, 26.05465465829597, 0.5660066604160982, 0.0, 1.0, 25.0, 51.2734112607205], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2067000.0000, 
sim time next is 2067600.0000, 
raw observation next is [-4.1, 83.33333333333334, 0.0, 0.0, 19.0, 26.05284026873341, 0.5666459189643892, 0.0, 1.0, 45.0, 45.88047422086863], 
processed observation next is [1.0, 0.9565217391304348, 0.3490304709141275, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.6710700223944507, 0.6888819729881298, 0.0, 1.0, 0.6, 0.4588047422086863], 
reward next is 0.5412, 
noisyNet noise sample is [array([-0.60727566], dtype=float32), -1.5937163]. 
=============================================
[2019-04-09 15:16:57,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0502330e-04 4.7151331e-02 2.2421077e-01 1.9884480e-02 1.7795119e-03
 2.9742569e-04 2.3660788e-01 1.5447513e-02 4.1562892e-02 1.5452605e-02
 3.9750060e-01], sum to 1.0000
[2019-04-09 15:16:57,770] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1786
[2019-04-09 15:16:57,822] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 22.5, 25.90947846217799, 0.5092160418905919, 1.0, 1.0, 65.0, 50.70008685504604], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2105400.0000, 
sim time next is 2106000.0000, 
raw observation next is [-7.8, 82.0, 123.0, 77.5, 22.5, 26.05581114959286, 0.5328654822705327, 1.0, 1.0, 25.0, 38.92360640596097], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.41, 0.0856353591160221, 0.375, 0.671317595799405, 0.6776218274235108, 1.0, 1.0, 0.2, 0.3892360640596097], 
reward next is 0.6108, 
noisyNet noise sample is [array([-0.35268465], dtype=float32), 1.2253401]. 
=============================================
[2019-04-09 15:16:57,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.501493]
 [10.108128]
 [10.052565]
 [ 9.978698]
 [ 9.918852]], R is [[10.78944588]
 [11.17455101]
 [11.64216518]
 [12.02388191]
 [12.45894051]].
[2019-04-09 15:16:57,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6656479e-04 7.0260957e-02 8.7220438e-02 7.8805340e-03 1.9715847e-03
 2.7902893e-04 4.1050151e-01 8.2869781e-03 1.5101376e-02 2.9560694e-02
 3.6877039e-01], sum to 1.0000
[2019-04-09 15:16:57,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8897
[2019-04-09 15:16:57,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.64287550e-05 1.21524997e-01 1.05417944e-01 1.00626284e-02
 1.33769924e-03 1.64484343e-04 3.40569019e-01 1.28011825e-02
 2.71542426e-02 2.00634282e-02 3.60857934e-01], sum to 1.0000
[2019-04-09 15:16:57,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2168
[2019-04-09 15:16:57,915] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 19.0, 25.64249752561938, 0.4690793937020099, 0.0, 1.0, 65.0, 53.42997680312536], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 25.6667521706576, 0.467814150316006, 0.0, 1.0, 65.0, 55.2882510175189], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.6388960142214666, 0.6559380501053353, 0.0, 1.0, 1.0, 0.5528825101751891], 
reward next is 0.4471, 
noisyNet noise sample is [array([2.4967146], dtype=float32), 1.3277961]. 
=============================================
[2019-04-09 15:16:57,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[10.7936535]
 [10.428123 ]
 [10.571798 ]
 [10.561926 ]
 [10.755789 ]], R is [[10.73120022]
 [11.08958817]
 [11.45374775]
 [11.79597187]
 [12.1909008 ]].
[2019-04-09 15:16:57,929] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 22.5, 26.61107690355525, 0.6414041320994854, 1.0, 1.0, 25.0, 32.28752695399169], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [-7.3, 75.0, 250.5, 80.5, 22.5, 26.62943260157227, 0.6442116930563272, 1.0, 1.0, 25.0, 31.34378141979316], 
processed observation next is [1.0, 0.4782608695652174, 0.26038781163434904, 0.75, 0.835, 0.08895027624309393, 0.375, 0.7191193834643558, 0.7147372310187757, 1.0, 1.0, 0.2, 0.3134378141979316], 
reward next is 0.6866, 
noisyNet noise sample is [array([1.5128132], dtype=float32), -1.4108644]. 
=============================================
[2019-04-09 15:16:57,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1430976e-04 5.1737923e-02 1.6824071e-01 1.4196793e-02 1.4384023e-03
 2.9308515e-04 3.1256530e-01 2.0195495e-02 9.4410805e-03 1.7047910e-02
 4.0472898e-01], sum to 1.0000
[2019-04-09 15:16:57,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4159
[2019-04-09 15:16:57,952] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.00000000000001, 0.0, 0.0, 19.0, 25.77853553434258, 0.5107408145875065, 0.0, 1.0, 65.0, 53.87164364583509], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [-7.300000000000001, 82.0, 0.0, 0.0, 19.0, 25.74988965026484, 0.5046535955064965, 0.0, 1.0, 45.0, 48.70792757094035], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.64582413752207, 0.6682178651688321, 0.0, 1.0, 0.6, 0.4870792757094035], 
reward next is 0.5129, 
noisyNet noise sample is [array([0.1271895], dtype=float32), 0.021377724]. 
=============================================
[2019-04-09 15:16:58,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3659878e-05 1.4470950e-01 1.7066637e-01 1.5396586e-02 1.9719519e-03
 5.1615993e-04 3.2022932e-01 1.7553210e-02 4.0229514e-02 2.8270172e-02
 2.6039362e-01], sum to 1.0000
[2019-04-09 15:16:58,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3163
[2019-04-09 15:16:58,055] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 22.5, 26.62740663568403, 0.6628294086968554, 1.0, 1.0, 40.0, 27.44351315434541], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 22.5, 26.63882578323201, 0.6612711437520423, 1.0, 1.0, 30.0, 26.60346776982454], 
processed observation next is [1.0, 0.4782608695652174, 0.27146814404432135, 0.6766666666666667, 0.8977777777777781, 0.11767955801104972, 0.375, 0.7199021486026677, 0.7204237145840141, 1.0, 1.0, 0.3, 0.2660346776982454], 
reward next is 0.7340, 
noisyNet noise sample is [array([-0.86150616], dtype=float32), -0.1972285]. 
=============================================
[2019-04-09 15:16:58,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00038288 0.09912797 0.11614475 0.01558472 0.00263018 0.00103559
 0.3400668  0.01810195 0.03204333 0.0284231  0.34645864], sum to 1.0000
[2019-04-09 15:16:58,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6508
[2019-04-09 15:16:58,176] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 25.82892706423443, 0.4740963240230268, 0.0, 1.0, 65.0, 49.25924608827856], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2170200.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 25.85218196621626, 0.4766952816740462, 0.0, 1.0, 60.0, 52.33780122697004], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6543484971846883, 0.6588984272246821, 0.0, 1.0, 0.9, 0.5233780122697005], 
reward next is 0.4766, 
noisyNet noise sample is [array([-0.6498817], dtype=float32), -1.9139607]. 
=============================================
[2019-04-09 15:16:58,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0936869e-05 2.4559008e-02 1.0998714e-01 9.9289026e-03 1.3360091e-03
 5.4063185e-05 4.4368738e-01 5.2427659e-03 2.6496176e-02 1.4352879e-02
 3.6430484e-01], sum to 1.0000
[2019-04-09 15:16:58,246] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0778
[2019-04-09 15:16:58,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000413   0.07806625 0.11670003 0.01634805 0.00231857 0.00076883
 0.33310482 0.01304782 0.02057765 0.02733796 0.39131704], sum to 1.0000
[2019-04-09 15:16:58,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0256
[2019-04-09 15:16:58,262] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 84.66666666666667, 0.0, 0.0, 19.0, 26.17363674890665, 0.6252351347521762, 0.0, 1.0, 65.0, 48.32494191242846], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2064000.0000, 
sim time next is 2064600.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 19.0, 26.16541993779682, 0.6234154580897179, 0.0, 1.0, 25.0, 44.10510358654279], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6804516614830683, 0.7078051526965726, 0.0, 1.0, 0.2, 0.4410510358654279], 
reward next is 0.5589, 
noisyNet noise sample is [array([-0.8502867], dtype=float32), 1.3957605]. 
=============================================
[2019-04-09 15:16:58,267] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 19.0, 25.56010918490654, 0.4446590759803062, 0.0, 1.0, 65.0, 64.91848844262674], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 19.0, 25.52505890862878, 0.4386457925346149, 0.0, 1.0, 20.0, 51.82866918563906], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.08333333333333333, 0.6270882423857316, 0.646215264178205, 0.0, 1.0, 0.1, 0.5182866918563905], 
reward next is 0.4817, 
noisyNet noise sample is [array([-0.9475401], dtype=float32), -1.3232437]. 
=============================================
[2019-04-09 15:16:58,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3113861e-05 8.2542285e-02 1.7051597e-01 7.4481387e-03 7.1203447e-04
 8.4809035e-05 3.3242881e-01 2.9744310e-03 4.1713782e-02 1.1190034e-02
 3.5034651e-01], sum to 1.0000
[2019-04-09 15:16:58,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9374
[2019-04-09 15:16:58,453] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 22.5, 27.00092370134556, 0.7170353022592776, 1.0, 1.0, 20.0, 34.39491653885148], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 22.5, 26.94133619193794, 0.6156990423697316, 1.0, 1.0, 65.0, 74.06367394545192], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.375, 0.7451113493281616, 0.7052330141232438, 1.0, 1.0, 1.0, 0.7406367394545192], 
reward next is 0.2594, 
noisyNet noise sample is [array([0.19772223], dtype=float32), -0.37929118]. 
=============================================
[2019-04-09 15:16:58,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.58065502e-05 1.13444120e-01 9.98017415e-02 1.20606627e-02
 8.96654965e-04 4.42416000e-04 3.14500183e-01 1.24264145e-02
 2.02797540e-02 1.63186044e-02 4.09733593e-01], sum to 1.0000
[2019-04-09 15:16:58,479] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1990
[2019-04-09 15:16:58,480] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.74883874e-04 6.73567429e-02 9.98516381e-02 1.48385353e-02
 2.40799249e-03 3.68326728e-04 4.80229467e-01 1.12693785e-02
 2.11472735e-02 2.72406805e-02 2.75015146e-01], sum to 1.0000
[2019-04-09 15:16:58,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4665
[2019-04-09 15:16:58,491] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 91.00000000000001, 0.0, 0.0, 19.0, 25.3525229965243, 0.4255236017140843, 0.0, 1.0, 55.0, 42.97699422905386], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2074200.0000, 
sim time next is 2074800.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.0, 25.32570377105489, 0.4248620486566366, 0.0, 1.0, 25.0, 37.55760797192308], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.08333333333333333, 0.6104753142545741, 0.6416206828855455, 0.0, 1.0, 0.2, 0.37557607971923085], 
reward next is 0.6244, 
noisyNet noise sample is [array([0.33374], dtype=float32), -2.2028453]. 
=============================================
[2019-04-09 15:16:58,505] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 91.0, 0.0, 0.0, 19.0, 25.50281682412096, 0.4439665070432064, 0.0, 1.0, 45.0, 39.20513017253514], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [-5.7, 90.33333333333334, 0.0, 0.0, 19.0, 25.59222025099118, 0.4091687902054508, 0.0, 1.0, 45.0, 41.13396084513048], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.9033333333333334, 0.0, 0.0, 0.08333333333333333, 0.6326850209159316, 0.6363895967351503, 0.0, 1.0, 0.6, 0.4113396084513048], 
reward next is 0.5887, 
noisyNet noise sample is [array([0.85329694], dtype=float32), -2.7119608]. 
=============================================
[2019-04-09 15:16:58,672] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.63886063e-05 1.05727874e-01 8.20203051e-02 7.88443163e-03
 1.39508932e-03 2.13530278e-04 2.69903421e-01 7.00791180e-03
 1.96369048e-02 1.47450957e-02 4.91419077e-01], sum to 1.0000
[2019-04-09 15:16:58,672] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8769
[2019-04-09 15:16:58,688] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.11036829842836, 0.6046361999642794, 0.0, 1.0, 20.0, 35.40306854622506], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2148000.0000, 
sim time next is 2148600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.07284765305067, 0.602101365999404, 0.0, 1.0, 65.0, 48.06610601871883], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6727373044208891, 0.7007004553331346, 0.0, 1.0, 1.0, 0.4806610601871883], 
reward next is 0.5193, 
noisyNet noise sample is [array([-1.0626979], dtype=float32), 1.404079]. 
=============================================
[2019-04-09 15:16:58,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3934788e-05 4.7020745e-02 1.9615506e-01 8.4236078e-03 8.2058436e-04
 1.3687856e-04 4.0404931e-01 5.3057917e-03 2.1708835e-02 1.1031073e-02
 3.0532420e-01], sum to 1.0000
[2019-04-09 15:16:58,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4345
[2019-04-09 15:16:59,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00051118 0.13435955 0.14239588 0.01555549 0.00198475 0.00080571
 0.24171785 0.01730641 0.03754359 0.04613941 0.36168006], sum to 1.0000
[2019-04-09 15:16:59,023] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8578
[2019-04-09 15:16:59,039] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.7, 90.33333333333334, 0.0, 0.0, 19.0, 25.38170920830778, 0.4056450948228913, 0.0, 1.0, 65.0, 63.39259587262602], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2088600.0000, 
sim time next is 2089200.0000, 
raw observation next is [-5.8, 89.66666666666667, 0.0, 0.0, 19.0, 25.36771717017763, 0.4136470788347663, 0.0, 1.0, 65.0, 60.8165363566478], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6139764308481359, 0.6378823596115888, 0.0, 1.0, 1.0, 0.608165363566478], 
reward next is 0.3918, 
noisyNet noise sample is [array([0.3963357], dtype=float32), 0.48882034]. 
=============================================
[2019-04-09 15:16:59,040] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 22.5, 26.99678660014926, 0.530096558226436, 1.0, 1.0, 65.0, 73.55073892673997], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 22.5, 26.24479556125075, 0.6557278583107905, 1.0, 1.0, 45.0, 47.76378187200482], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.375, 0.6870662967708959, 0.7185759527702635, 1.0, 1.0, 0.6, 0.4776378187200482], 
reward next is 0.5224, 
noisyNet noise sample is [array([-0.8999548], dtype=float32), -0.4381454]. 
=============================================
[2019-04-09 15:16:59,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[12.507222 ]
 [12.148121 ]
 [12.376337 ]
 [12.2098465]
 [12.377201 ]], R is [[12.82090378]
 [12.95718765]
 [13.4203434 ]
 [13.97062778]
 [14.43416977]].
[2019-04-09 15:16:59,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4948622e-04 5.1494882e-02 9.2585675e-02 7.9662679e-03 1.5519173e-03
 2.4971546e-04 3.3593214e-01 1.4744568e-02 3.8257353e-02 2.3440773e-02
 4.3362728e-01], sum to 1.0000
[2019-04-09 15:16:59,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3699
[2019-04-09 15:16:59,146] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 19.0, 25.22443194972142, 0.407186755793604, 0.0, 1.0, 30.0, 35.54386040177685], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2071800.0000, 
sim time next is 2072400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 25.17419720450612, 0.38733943286671, 0.0, 1.0, 45.0, 33.78200003421692], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.08333333333333333, 0.5978497670421768, 0.6291131442889033, 0.0, 1.0, 0.6, 0.3378200003421692], 
reward next is 0.6622, 
noisyNet noise sample is [array([-1.2171159], dtype=float32), -0.47842604]. 
=============================================
[2019-04-09 15:16:59,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4914189e-04 5.8261737e-02 9.8330140e-02 2.4898678e-02 2.3988618e-03
 9.4496924e-04 3.5569414e-01 1.8362785e-02 2.0024661e-02 1.2910294e-02
 4.0792456e-01], sum to 1.0000
[2019-04-09 15:16:59,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3512
[2019-04-09 15:16:59,593] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 87.66666666666666, 0.0, 0.0, 19.0, 25.64950350133988, 0.4666798020242873, 0.0, 1.0, 45.0, 41.96388025390741], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2079600.0000, 
sim time next is 2080200.0000, 
raw observation next is [-4.5, 86.83333333333333, 0.0, 0.0, 19.0, 25.59625175051048, 0.4501266937073296, 0.0, 1.0, 45.0, 39.98536697088131], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8683333333333333, 0.0, 0.0, 0.08333333333333333, 0.6330209792092066, 0.6500422312357765, 0.0, 1.0, 0.6, 0.3998536697088131], 
reward next is 0.6001, 
noisyNet noise sample is [array([0.11325107], dtype=float32), -1.3783481]. 
=============================================
[2019-04-09 15:16:59,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8747097e-05 4.5922380e-02 8.4332086e-02 1.3221404e-02 1.3088008e-03
 1.6365596e-04 3.1940201e-01 6.2553203e-03 1.5955029e-02 1.1437352e-02
 5.0194317e-01], sum to 1.0000
[2019-04-09 15:16:59,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-09 15:16:59,698] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.75, 71.0, 117.0, 0.0, 22.5, 26.68961169182457, 0.6136781807823798, 1.0, 1.0, 65.0, 41.96476221129715], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2197800.0000, 
sim time next is 2198400.0000, 
raw observation next is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 22.5, 26.73484743093893, 0.622481687768206, 1.0, 1.0, 65.0, 41.46886563857802], 
processed observation next is [1.0, 0.43478260869565216, 0.3333333333333333, 0.71, 0.40444444444444433, 0.0, 0.375, 0.7279039525782443, 0.7074938959227354, 1.0, 1.0, 1.0, 0.41468865638578023], 
reward next is 0.5853, 
noisyNet noise sample is [array([1.2775778], dtype=float32), 0.76619923]. 
=============================================
[2019-04-09 15:17:00,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00043913 0.09879423 0.1261202  0.01235229 0.00224832 0.00047529
 0.38011006 0.02252067 0.0404957  0.0400943  0.27634975], sum to 1.0000
[2019-04-09 15:17:00,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1939
[2019-04-09 15:17:00,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8951337e-05 7.1721531e-02 1.9168839e-01 1.2111839e-02 2.7666350e-03
 6.0967432e-05 3.6145136e-01 6.9340887e-03 4.2840026e-02 2.0006727e-02
 2.9035941e-01], sum to 1.0000
[2019-04-09 15:17:00,641] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7842
[2019-04-09 15:17:00,660] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.3, 79.0, 36.5, 18.5, 22.5, 24.58377485782328, 0.2668443316671065, 1.0, 1.0, 45.0, 41.82495646286904], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2102400.0000, 
sim time next is 2103000.0000, 
raw observation next is [-7.383333333333333, 79.50000000000001, 48.33333333333334, 24.66666666666667, 22.5, 24.70260095939241, 0.2593656466425988, 1.0, 1.0, 25.0, 31.56580439467701], 
processed observation next is [1.0, 0.34782608695652173, 0.25807940904893817, 0.7950000000000002, 0.16111111111111115, 0.027255985267034995, 0.375, 0.5585500799493675, 0.586455215547533, 1.0, 1.0, 0.2, 0.3156580439467701], 
reward next is 0.6843, 
noisyNet noise sample is [array([0.9397524], dtype=float32), -2.5903242]. 
=============================================
[2019-04-09 15:17:00,662] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.366666666666667, 64.0, 150.6666666666667, 111.6666666666667, 22.5, 26.79470013038541, 0.6994419742770305, 1.0, 1.0, 65.0, 40.80956880753374], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2119200.0000, 
sim time next is 2119800.0000, 
raw observation next is [-6.283333333333333, 64.0, 150.3333333333333, 89.33333333333334, 22.5, 26.86258319713593, 0.6996860905339853, 1.0, 1.0, 65.0, 36.22459443657829], 
processed observation next is [1.0, 0.5217391304347826, 0.288550323176362, 0.64, 0.501111111111111, 0.0987108655616943, 0.375, 0.7385485997613275, 0.7332286968446619, 1.0, 1.0, 1.0, 0.36224594436578295], 
reward next is 0.6378, 
noisyNet noise sample is [array([1.4192436], dtype=float32), -1.3409443]. 
=============================================
[2019-04-09 15:17:00,665] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0240587e-04 8.6651497e-02 1.4899834e-01 1.6404314e-02 1.4974519e-03
 1.7254932e-04 1.7772892e-01 5.5940994e-03 3.0743763e-02 1.5652452e-02
 5.1645422e-01], sum to 1.0000
[2019-04-09 15:17:00,666] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8225
[2019-04-09 15:17:00,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.938982]
 [9.991607]
 [9.714631]
 [9.73973 ]
 [9.657989]], R is [[10.95045662]
 [11.42270279]
 [11.7278614 ]
 [12.30852318]
 [12.86773968]].
[2019-04-09 15:17:00,686] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 22.5, 26.59378848741245, 0.6243200903426372, 1.0, 1.0, 20.0, 37.50880059741966], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2110800.0000, 
sim time next is 2111400.0000, 
raw observation next is [-7.55, 78.5, 208.0, 60.0, 22.5, 26.64268167574293, 0.6292188651625593, 1.0, 1.0, 65.0, 40.52804341432311], 
processed observation next is [1.0, 0.43478260869565216, 0.25346260387811637, 0.785, 0.6933333333333334, 0.06629834254143646, 0.375, 0.7202234729785776, 0.709739621720853, 1.0, 1.0, 1.0, 0.40528043414323106], 
reward next is 0.5947, 
noisyNet noise sample is [array([0.6677786], dtype=float32), -0.13464889]. 
=============================================
[2019-04-09 15:17:00,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1166367e-05 7.7963024e-02 1.1572241e-01 2.4441278e-02 1.3693986e-03
 1.6932287e-04 4.8010239e-01 7.0328573e-03 2.6267873e-02 1.6514637e-02
 2.5034565e-01], sum to 1.0000
[2019-04-09 15:17:00,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4391
[2019-04-09 15:17:01,025] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 22.5, 26.50971395180009, 0.590056152181921, 1.0, 1.0, 45.0, 34.71574685549166], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 22.5, 26.50898833937143, 0.5988565907979415, 1.0, 1.0, 45.0, 34.69847631363921], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 0.375, 0.709082361614286, 0.6996188635993139, 1.0, 1.0, 0.6, 0.3469847631363921], 
reward next is 0.6530, 
noisyNet noise sample is [array([0.32544246], dtype=float32), -1.7779205]. 
=============================================
[2019-04-09 15:17:01,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[12.322777]
 [12.079856]
 [11.649126]
 [11.435497]
 [11.558491]], R is [[12.6542654 ]
 [13.18056583]
 [13.69700718]
 [14.14431858]
 [14.55910587]].
[2019-04-09 15:17:01,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6739431e-05 5.4459635e-02 1.5576401e-01 1.2907399e-02 2.0611146e-03
 1.2912760e-04 2.6673269e-01 6.5343957e-03 4.5670260e-02 1.5354913e-02
 4.4034970e-01], sum to 1.0000
[2019-04-09 15:17:01,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7806
[2019-04-09 15:17:01,323] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 22.5, 26.53577824786943, 0.6692328265462507, 1.0, 1.0, 45.0, 34.54222794459871], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2121600.0000, 
sim time next is 2122200.0000, 
raw observation next is [-5.9, 66.0, 149.0, 0.0, 22.5, 26.67598634233178, 0.6766493220366551, 1.0, 1.0, 65.0, 52.57350212665438], 
processed observation next is [1.0, 0.5652173913043478, 0.2991689750692521, 0.66, 0.49666666666666665, 0.0, 0.375, 0.7229988618609816, 0.7255497740122183, 1.0, 1.0, 1.0, 0.5257350212665438], 
reward next is 0.4743, 
noisyNet noise sample is [array([0.3723949], dtype=float32), 0.2661138]. 
=============================================
[2019-04-09 15:17:01,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9686362e-05 6.8656877e-02 1.4882785e-01 3.9716503e-03 7.5943617e-04
 7.5926589e-05 2.5743172e-01 5.0641010e-03 2.6973605e-02 1.1553968e-02
 4.7665513e-01], sum to 1.0000
[2019-04-09 15:17:01,974] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8440
[2019-04-09 15:17:01,987] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 22.5, 26.54633918871939, 0.6666362634945263, 1.0, 1.0, 45.0, 30.00934043590298], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2138400.0000, 
sim time next is 2139000.0000, 
raw observation next is [-5.0, 71.5, 0.0, 0.0, 22.5, 26.70368594237265, 0.6722082851096096, 1.0, 1.0, 65.0, 41.92510093291433], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.715, 0.0, 0.0, 0.375, 0.7253071618643876, 0.7240694283698699, 1.0, 1.0, 1.0, 0.41925100932914333], 
reward next is 0.5807, 
noisyNet noise sample is [array([0.1673783], dtype=float32), 0.8043696]. 
=============================================
[2019-04-09 15:17:01,997] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[13.379824]
 [13.233397]
 [13.418645]
 [13.3112  ]
 [13.223385]], R is [[13.61505413]
 [14.17881012]
 [14.61756897]
 [15.08925438]
 [15.29249001]].
[2019-04-09 15:17:02,267] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1364644e-05 6.0275622e-02 1.1783610e-01 1.1298483e-02 1.0076796e-03
 1.5500367e-04 2.3823249e-01 8.2360161e-03 3.4522496e-02 1.9911714e-02
 5.0850296e-01], sum to 1.0000
[2019-04-09 15:17:02,270] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4877
[2019-04-09 15:17:02,284] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.916666666666667, 70.5, 0.0, 0.0, 22.5, 26.12458651938132, 0.6436390456196355, 1.0, 1.0, 65.0, 38.56760163165331], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2137800.0000, 
sim time next is 2138400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 22.5, 26.46606904927152, 0.660952665907261, 1.0, 1.0, 60.0, 31.92631978470858], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.71, 0.0, 0.0, 0.375, 0.7055057541059601, 0.7203175553024203, 1.0, 1.0, 0.9, 0.3192631978470858], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.03028545], dtype=float32), -1.8245971]. 
=============================================
[2019-04-09 15:17:02,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.36857105e-05 7.67485201e-02 1.12930164e-01 1.40117267e-02
 1.33498816e-03 1.06476866e-04 3.47745061e-01 6.02095155e-03
 2.58194134e-02 1.31798200e-02 4.02009159e-01], sum to 1.0000
[2019-04-09 15:17:02,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-09 15:17:02,506] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 83.0, 0.0, 0.0, 19.0, 25.88651989138947, 0.545574184153942, 0.0, 1.0, 25.0, 34.54406812490416], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [-6.516666666666667, 83.0, 0.0, 0.0, 19.0, 25.81661007181292, 0.5296669952758964, 0.0, 1.0, 45.0, 32.13817380640238], 
processed observation next is [1.0, 0.9130434782608695, 0.2820867959372115, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6513841726510767, 0.6765556650919655, 0.0, 1.0, 0.6, 0.32138173806402376], 
reward next is 0.6786, 
noisyNet noise sample is [array([0.31761843], dtype=float32), 0.38603055]. 
=============================================
[2019-04-09 15:17:02,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1383325e-04 7.1638152e-02 1.1921283e-01 7.9427706e-03 1.2907152e-03
 6.2781543e-04 4.7002712e-01 8.7003876e-03 2.6043620e-02 1.6076865e-02
 2.7822584e-01], sum to 1.0000
[2019-04-09 15:17:02,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4601
[2019-04-09 15:17:02,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.11106555e-05 1.61948934e-01 9.16197225e-02 1.22566484e-02
 2.67985673e-03 1.11101595e-04 2.65040964e-01 1.19880103e-02
 2.61537749e-02 3.23179699e-02 3.95831943e-01], sum to 1.0000
[2019-04-09 15:17:02,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5269
[2019-04-09 15:17:02,625] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 25.56690408302013, 0.4569578590912932, 0.0, 1.0, 65.0, 57.62598254726794], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2253600.0000, 
sim time next is 2254200.0000, 
raw observation next is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 19.0, 25.5694186323877, 0.4626041620049579, 0.0, 1.0, 45.0, 49.50586822907368], 
processed observation next is [1.0, 0.08695652173913043, 0.25807940904893817, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6307848860323082, 0.654201387334986, 0.0, 1.0, 0.6, 0.4950586822907368], 
reward next is 0.5049, 
noisyNet noise sample is [array([0.9077695], dtype=float32), 1.1359774]. 
=============================================
[2019-04-09 15:17:02,634] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 72.5, 0.0, 0.0, 22.5, 26.54039846568814, 0.6614183528741787, 1.0, 1.0, 20.0, 43.41799735239876], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2140200.0000, 
sim time next is 2140800.0000, 
raw observation next is [-5.0, 73.0, 0.0, 0.0, 22.5, 26.47131173343704, 0.6571975806726256, 1.0, 1.0, 45.0, 38.61884283813983], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.73, 0.0, 0.0, 0.375, 0.7059426444530867, 0.7190658602242085, 1.0, 1.0, 0.6, 0.3861884283813983], 
reward next is 0.6138, 
noisyNet noise sample is [array([-1.5896715], dtype=float32), -2.3752372]. 
=============================================
[2019-04-09 15:17:02,767] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.4291409e-05 5.2179150e-02 1.8132351e-01 1.4763958e-02 1.2864777e-03
 1.0803710e-04 2.1543360e-01 1.5929313e-02 4.9727779e-02 2.0689949e-02
 4.4847387e-01], sum to 1.0000
[2019-04-09 15:17:02,770] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5295
[2019-04-09 15:17:02,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4305015e-05 6.6006973e-02 1.8177409e-01 1.1257371e-02 8.0806314e-04
 8.0067446e-05 2.4764208e-01 8.2472432e-03 3.9561689e-02 3.0305017e-02
 4.1425306e-01], sum to 1.0000
[2019-04-09 15:17:02,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-09 15:17:02,789] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 68.0, 135.5, 0.0, 22.5, 26.18567053783958, 0.5283599918849285, 1.0, 1.0, 65.0, 61.08571701212047], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2203200.0000, 
sim time next is 2203800.0000, 
raw observation next is [-3.816666666666666, 67.5, 133.0, 0.0, 22.5, 26.18724787413349, 0.5570702276175942, 1.0, 1.0, 65.0, 71.28035244841348], 
processed observation next is [1.0, 0.5217391304347826, 0.3568790397045245, 0.675, 0.44333333333333336, 0.0, 0.375, 0.6822706561777908, 0.6856900758725314, 1.0, 1.0, 1.0, 0.7128035244841349], 
reward next is 0.2872, 
noisyNet noise sample is [array([-0.9523462], dtype=float32), 1.2773486]. 
=============================================
[2019-04-09 15:17:02,812] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 22.5, 26.55483546830512, 0.6859094781741945, 1.0, 1.0, 65.0, 45.07202972708668], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2224800.0000, 
sim time next is 2225400.0000, 
raw observation next is [-4.516666666666667, 68.33333333333334, 0.0, 0.0, 22.5, 26.61753906708882, 0.5019818612779597, 1.0, 1.0, 55.0, 58.54605027535231], 
processed observation next is [1.0, 0.782608695652174, 0.337488457987073, 0.6833333333333335, 0.0, 0.0, 0.375, 0.718128255590735, 0.6673272870926533, 1.0, 1.0, 0.8, 0.5854605027535231], 
reward next is 0.4145, 
noisyNet noise sample is [array([-0.7057521], dtype=float32), 0.28643796]. 
=============================================
[2019-04-09 15:17:02,946] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6851933e-04 1.5576136e-01 1.2481901e-01 1.7873088e-02 2.0695496e-03
 4.1313615e-04 3.2000148e-01 1.3643636e-02 2.7926669e-02 1.2160017e-02
 3.2506356e-01], sum to 1.0000
[2019-04-09 15:17:02,946] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0313
[2019-04-09 15:17:02,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 19.0, 25.663056641037, 0.4419152363119887, 0.0, 1.0, 45.0, 44.96417270918571], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2261400.0000, 
sim time next is 2262000.0000, 
raw observation next is [-8.566666666666666, 88.33333333333334, 0.0, 0.0, 19.0, 25.63345454105254, 0.4315083698203908, 0.0, 1.0, 20.0, 41.99925543723276], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.6361212117543783, 0.6438361232734636, 0.0, 1.0, 0.1, 0.41999255437232763], 
reward next is 0.5800, 
noisyNet noise sample is [array([-1.3889799], dtype=float32), -0.22256003]. 
=============================================
[2019-04-09 15:17:02,965] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7622623e-04 6.4134233e-02 1.1394220e-01 1.5674215e-02 1.6213439e-03
 2.1623443e-04 4.6762198e-01 9.8321643e-03 2.1963295e-02 2.0610545e-02
 2.8420761e-01], sum to 1.0000
[2019-04-09 15:17:02,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2042
[2019-04-09 15:17:02,977] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[ 9.952091]
 [10.131262]
 [ 9.848533]
 [ 9.818881]
 [ 9.897667]], R is [[10.34316158]
 [10.79008865]
 [11.21948147]
 [11.53927708]
 [11.82885361]].
[2019-04-09 15:17:02,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 81.50000000000001, 0.0, 0.0, 19.0, 25.61425407852084, 0.460672215506738, 0.0, 1.0, 45.0, 40.0767984428475], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [-7.300000000000001, 81.0, 0.0, 0.0, 19.0, 25.55105504551387, 0.4531822053530737, 0.0, 1.0, 45.0, 37.98272794022076], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.08333333333333333, 0.6292545871261558, 0.6510607351176912, 0.0, 1.0, 0.6, 0.3798272794022076], 
reward next is 0.6202, 
noisyNet noise sample is [array([1.3117999], dtype=float32), 0.06691668]. 
=============================================
[2019-04-09 15:17:03,011] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2608652e-04 4.2724773e-02 1.0158892e-01 1.0736525e-02 1.6877231e-03
 2.0918594e-04 4.7022179e-01 1.5037287e-02 1.9192891e-02 3.2680873e-02
 3.0579391e-01], sum to 1.0000
[2019-04-09 15:17:03,011] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0904
[2019-04-09 15:17:03,029] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.300000000000001, 82.0, 0.0, 0.0, 19.0, 25.74409676684577, 0.5031027591847864, 0.0, 1.0, 65.0, 56.27750208953085], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2157600.0000, 
sim time next is 2158200.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 25.73403024136158, 0.4986686311659455, 0.0, 1.0, 25.0, 47.30827877209997], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.644502520113465, 0.6662228770553152, 0.0, 1.0, 0.2, 0.47308278772099965], 
reward next is 0.5269, 
noisyNet noise sample is [array([-0.08492662], dtype=float32), 0.71319747]. 
=============================================
[2019-04-09 15:17:03,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5592875e-05 4.4777635e-02 9.5814586e-02 1.4796865e-02 1.3056500e-03
 1.1545767e-04 4.2530444e-01 7.0986184e-03 3.2803748e-02 1.5336599e-02
 3.6260071e-01], sum to 1.0000
[2019-04-09 15:17:03,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5052
[2019-04-09 15:17:03,214] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 70.5, 128.0, 0.0, 22.5, 26.59727639002854, 0.6298632767496947, 1.0, 1.0, 65.0, 45.93663079436174], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2211000.0000, 
sim time next is 2211600.0000, 
raw observation next is [-3.9, 70.0, 124.0, 0.0, 22.5, 26.80290790595783, 0.6405913009903929, 1.0, 1.0, 45.0, 33.4608745497217], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.7, 0.41333333333333333, 0.0, 0.375, 0.7335756588298192, 0.7135304336634642, 1.0, 1.0, 0.6, 0.33460874549721703], 
reward next is 0.6654, 
noisyNet noise sample is [array([-0.5614989], dtype=float32), -0.80458826]. 
=============================================
[2019-04-09 15:17:03,534] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8717258e-04 7.4947961e-02 1.6606966e-01 1.0283900e-02 1.3913466e-03
 3.0944150e-04 3.2625696e-01 1.5937939e-02 2.8531801e-02 3.0457610e-02
 3.4562618e-01], sum to 1.0000
[2019-04-09 15:17:03,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0780
[2019-04-09 15:17:03,548] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 22.5, 25.13196947671327, 0.2994786417542012, 1.0, 1.0, 65.0, 63.13693731109363], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 22.5, 25.05825543621907, 0.3017706253801778, 1.0, 1.0, 25.0, 50.59139194150971], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.375, 0.5881879530182559, 0.6005902084600593, 1.0, 1.0, 0.2, 0.5059139194150971], 
reward next is 0.4941, 
noisyNet noise sample is [array([-0.20834927], dtype=float32), 0.27929056]. 
=============================================
[2019-04-09 15:17:03,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[9.625801]
 [9.176977]
 [9.437129]
 [9.385769]
 [9.624849]], R is [[ 9.82749844]
 [10.09785366]
 [10.63479233]
 [11.1466608 ]
 [11.63343906]].
[2019-04-09 15:17:03,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00047703 0.10789251 0.11018609 0.01058964 0.00276896 0.00106894
 0.42412436 0.02605314 0.03781364 0.02927554 0.24975018], sum to 1.0000
[2019-04-09 15:17:03,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5195
[2019-04-09 15:17:03,744] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 19.0, 25.1874130850847, 0.3066590214727338, 0.0, 1.0, 45.0, 30.72252358957362], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 19.0, 25.07674164318761, 0.2816718846886762, 0.0, 1.0, 45.0, 29.2782921374485], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5897284702656341, 0.5938906282295587, 0.0, 1.0, 0.6, 0.292782921374485], 
reward next is 0.7072, 
noisyNet noise sample is [array([1.2800927], dtype=float32), 0.4731956]. 
=============================================
[2019-04-09 15:17:03,769] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.85162091e-05 5.71502596e-02 1.03142805e-01 6.61355397e-03
 1.83808932e-03 1.41856217e-04 4.32926804e-01 8.20828788e-03
 1.49706267e-02 2.78361849e-02 3.47122967e-01], sum to 1.0000
[2019-04-09 15:17:03,770] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0316
[2019-04-09 15:17:03,796] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.9, 73.0, 0.0, 0.0, 19.0, 25.77933071517934, 0.5045043521132827, 0.0, 1.0, 65.0, 56.63829732445721], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2241000.0000, 
sim time next is 2241600.0000, 
raw observation next is [-6.0, 73.66666666666667, 0.0, 0.0, 19.0, 25.73037945270315, 0.4979497313893338, 0.0, 1.0, 20.0, 51.35884409677761], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.7366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6441982877252626, 0.6659832437964446, 0.0, 1.0, 0.1, 0.5135884409677761], 
reward next is 0.4864, 
noisyNet noise sample is [array([-0.58500314], dtype=float32), -1.0471895]. 
=============================================
[2019-04-09 15:17:03,814] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.57967190e-04 8.50599483e-02 1.03629045e-01 1.45000676e-02
 2.65809032e-03 7.98365683e-04 3.12652141e-01 1.48722194e-02
 3.44052278e-02 2.78082807e-02 4.03258681e-01], sum to 1.0000
[2019-04-09 15:17:03,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4918
[2019-04-09 15:17:03,827] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 91.0, 0.0, 0.0, 19.0, 24.97599796815263, 0.298462625698023, 0.0, 1.0, 65.0, 56.9431511944594], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2269800.0000, 
sim time next is 2270400.0000, 
raw observation next is [-9.3, 91.0, 0.0, 0.0, 19.0, 25.04799677212832, 0.2950764732854867, 0.0, 1.0, 45.0, 50.45921304811156], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.91, 0.0, 0.0, 0.08333333333333333, 0.5873330643440268, 0.5983588244284955, 0.0, 1.0, 0.6, 0.5045921304811156], 
reward next is 0.4954, 
noisyNet noise sample is [array([1.3640578], dtype=float32), -2.3967748]. 
=============================================
[2019-04-09 15:17:03,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9652229e-04 8.0189221e-02 1.3129890e-01 9.3591483e-03 1.3221272e-03
 4.2968750e-04 3.8083711e-01 9.1806734e-03 1.6614977e-02 1.3794347e-02
 3.5677731e-01], sum to 1.0000
[2019-04-09 15:17:03,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3858
[2019-04-09 15:17:03,946] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 77.5, 0.0, 0.0, 19.0, 25.36174890338054, 0.4362941425490246, 0.0, 1.0, 65.0, 65.24932610263441], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2247000.0000, 
sim time next is 2247600.0000, 
raw observation next is [-6.700000000000001, 77.0, 0.0, 0.0, 19.0, 25.39187911770039, 0.4476812899248213, 0.0, 1.0, 45.0, 50.41870041253895], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6159899264750326, 0.649227096641607, 0.0, 1.0, 0.6, 0.5041870041253895], 
reward next is 0.4958, 
noisyNet noise sample is [array([-0.19857624], dtype=float32), -0.74592096]. 
=============================================
[2019-04-09 15:17:03,954] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1015792e-05 1.6634595e-01 1.6981119e-01 2.5158105e-02 9.5370464e-04
 2.2487724e-04 2.1222998e-01 8.8506527e-03 2.8744522e-02 1.5519042e-02
 3.7213099e-01], sum to 1.0000
[2019-04-09 15:17:03,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1910
[2019-04-09 15:17:03,977] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 22.5, 26.29179207576713, 0.6292152189126013, 1.0, 1.0, 25.0, 31.66237371105647], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2224800.0000, 
sim time next is 2225400.0000, 
raw observation next is [-4.516666666666667, 68.33333333333334, 0.0, 0.0, 22.5, 26.34070710129367, 0.6222453743500882, 1.0, 1.0, 25.0, 32.62044477532062], 
processed observation next is [1.0, 0.782608695652174, 0.337488457987073, 0.6833333333333335, 0.0, 0.0, 0.375, 0.6950589251078059, 0.7074151247833628, 1.0, 1.0, 0.2, 0.32620444775320623], 
reward next is 0.6738, 
noisyNet noise sample is [array([0.4920176], dtype=float32), 0.2738316]. 
=============================================
[2019-04-09 15:17:04,014] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3187678e-04 1.0263317e-01 1.9139108e-01 2.8670391e-02 1.7130623e-03
 7.6007214e-04 3.0739743e-01 9.4017303e-03 3.6979206e-02 4.1751020e-02
 2.7907094e-01], sum to 1.0000
[2019-04-09 15:17:04,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9979
[2019-04-09 15:17:04,034] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 75.0, 21.5, 131.0, 22.5, 25.17680038803782, 0.3426842192694626, 1.0, 1.0, 45.0, 34.62751065773299], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [-5.600000000000001, 75.0, 28.0, 174.6666666666667, 22.5, 25.23358871094991, 0.3507859617404123, 1.0, 1.0, 65.0, 62.13559101304075], 
processed observation next is [1.0, 0.34782608695652173, 0.3074792243767313, 0.75, 0.09333333333333334, 0.1930018416206262, 0.375, 0.6027990592458258, 0.6169286539134707, 1.0, 1.0, 1.0, 0.6213559101304075], 
reward next is 0.3786, 
noisyNet noise sample is [array([-1.1540697], dtype=float32), -0.13664001]. 
=============================================
[2019-04-09 15:17:04,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.89934051e-05 4.90633547e-02 2.31521070e-01 3.91611131e-03
 1.04376767e-03 8.52122175e-05 2.85849899e-01 9.11781192e-03
 2.08523124e-02 1.14196865e-02 3.87101740e-01], sum to 1.0000
[2019-04-09 15:17:04,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5644
[2019-04-09 15:17:04,258] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.516666666666667, 68.33333333333334, 0.0, 0.0, 22.5, 26.65124861974486, 0.6861674975499276, 1.0, 1.0, 65.0, 42.44157569094999], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2225400.0000, 
sim time next is 2226000.0000, 
raw observation next is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 22.5, 26.64996812915524, 0.6841986512015922, 1.0, 1.0, 45.0, 39.85523902508204], 
processed observation next is [1.0, 0.782608695652174, 0.3370267774699908, 0.6866666666666668, 0.0, 0.0, 0.375, 0.7208306774296034, 0.7280662170671973, 1.0, 1.0, 0.6, 0.3985523902508204], 
reward next is 0.6014, 
noisyNet noise sample is [array([0.47044054], dtype=float32), 1.494853]. 
=============================================
[2019-04-09 15:17:04,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[13.96878 ]
 [14.049053]
 [13.902661]
 [13.979593]
 [13.992494]], R is [[14.18420029]
 [14.61794281]
 [15.03279018]
 [15.43436718]
 [15.79974651]].
[2019-04-09 15:17:04,285] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7267409e-04 6.8425640e-02 1.1670897e-01 9.2365993e-03 1.9905330e-03
 3.4937606e-04 5.5898279e-01 1.0161796e-02 1.9027743e-02 1.5525418e-02
 1.9941850e-01], sum to 1.0000
[2019-04-09 15:17:04,297] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-09 15:17:04,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3143512e-04 5.9191048e-02 1.2590526e-01 1.9163195e-02 1.7687337e-03
 3.9773309e-04 3.8415462e-01 1.0426878e-02 1.6252270e-02 1.8140875e-02
 3.6446792e-01], sum to 1.0000
[2019-04-09 15:17:04,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9847
[2019-04-09 15:17:04,314] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 76.16666666666667, 0.0, 0.0, 19.0, 25.46231842777864, 0.416863441335919, 0.0, 1.0, 45.0, 39.76168203604349], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2250600.0000, 
sim time next is 2251200.0000, 
raw observation next is [-6.9, 77.33333333333334, 0.0, 0.0, 19.0, 25.41165152078356, 0.4002026489916079, 0.0, 1.0, 45.0, 39.48188378384139], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.6176376267319634, 0.6334008829972027, 0.0, 1.0, 0.6, 0.39481883783841387], 
reward next is 0.6052, 
noisyNet noise sample is [array([0.38996914], dtype=float32), -1.7475563]. 
=============================================
[2019-04-09 15:17:04,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 87.0, 73.0, 27.5, 22.5, 25.26420753827392, 0.3605692148584853, 1.0, 1.0, 65.0, 61.4927556404978], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2278800.0000, 
sim time next is 2279400.0000, 
raw observation next is [-8.116666666666667, 85.5, 82.33333333333334, 31.33333333333334, 22.5, 25.4902495347307, 0.3812589673166428, 1.0, 1.0, 65.0, 51.74632391385659], 
processed observation next is [1.0, 0.391304347826087, 0.23776546629732226, 0.855, 0.2744444444444445, 0.03462246777163905, 0.375, 0.6241874612275584, 0.627086322438881, 1.0, 1.0, 1.0, 0.517463239138566], 
reward next is 0.4825, 
noisyNet noise sample is [array([-0.50212747], dtype=float32), 0.38283122]. 
=============================================
[2019-04-09 15:17:04,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.07212642e-04 6.90977052e-02 1.21607594e-01 1.53276026e-02
 1.20948756e-03 7.51754967e-04 3.07669044e-01 1.45362429e-02
 2.90912893e-02 4.64874730e-02 3.93914580e-01], sum to 1.0000
[2019-04-09 15:17:04,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2936
[2019-04-09 15:17:04,408] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 22.5, 25.11512920446941, 0.3075296900811222, 1.0, 1.0, 45.0, 40.91814402804451], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2187000.0000, 
sim time next is 2187600.0000, 
raw observation next is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 22.5, 25.1215279115164, 0.3160409001266133, 1.0, 1.0, 45.0, 39.75955384993565], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.02833333333333333, 0.04825046040515653, 0.375, 0.5934606592930333, 0.6053469667088711, 1.0, 1.0, 0.6, 0.39759553849935647], 
reward next is 0.6024, 
noisyNet noise sample is [array([0.1836771], dtype=float32), -0.5357843]. 
=============================================
[2019-04-09 15:17:04,437] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9915764e-04 1.0571005e-01 1.2589736e-01 1.8020706e-02 2.7519399e-03
 8.7205117e-04 3.7815732e-01 1.2041717e-02 4.1563798e-02 3.5112809e-02
 2.7967310e-01], sum to 1.0000
[2019-04-09 15:17:04,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2139
[2019-04-09 15:17:04,477] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 75.0, 21.5, 131.0, 22.5, 25.19629431206474, 0.3427622245520771, 1.0, 1.0, 45.0, 35.72329126139297], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [-5.600000000000001, 75.0, 28.0, 174.6666666666667, 22.5, 25.24848934242138, 0.3207487973706787, 1.0, 1.0, 25.0, 36.60539309246644], 
processed observation next is [1.0, 0.34782608695652173, 0.3074792243767313, 0.75, 0.09333333333333334, 0.1930018416206262, 0.375, 0.604040778535115, 0.6069162657902262, 1.0, 1.0, 0.2, 0.36605393092466443], 
reward next is 0.6339, 
noisyNet noise sample is [array([0.26580417], dtype=float32), -0.7799712]. 
=============================================
[2019-04-09 15:17:04,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.79459603e-04 5.08615486e-02 1.20150104e-01 7.77852954e-03
 3.23845469e-03 5.21674054e-04 4.13659930e-01 2.10912731e-02
 3.67322676e-02 5.21145947e-02 2.93672115e-01], sum to 1.0000
[2019-04-09 15:17:04,557] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0712
[2019-04-09 15:17:04,587] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 76.5, 0.0, 0.0, 19.0, 25.21376758494206, 0.4046990065393171, 0.0, 1.0, 45.0, 49.02228397602438], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2248200.0000, 
sim time next is 2248800.0000, 
raw observation next is [-6.700000000000001, 76.0, 0.0, 0.0, 19.0, 25.32434216885994, 0.3922559496589941, 0.0, 1.0, 25.0, 37.97531508003711], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6103618474049949, 0.6307519832196647, 0.0, 1.0, 0.2, 0.37975315080037114], 
reward next is 0.6202, 
noisyNet noise sample is [array([1.9961452], dtype=float32), 0.7877293]. 
=============================================
[2019-04-09 15:17:04,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00047114 0.10356668 0.20730005 0.01948793 0.00297816 0.00099994
 0.26042783 0.01432122 0.0289716  0.03876034 0.3227151 ], sum to 1.0000
[2019-04-09 15:17:04,660] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1290
[2019-04-09 15:17:04,674] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.9, 77.0, 0.0, 0.0, 19.0, 24.83979706434442, 0.2752068183978776, 0.0, 1.0, 25.0, 46.5187924836909], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2183400.0000, 
sim time next is 2184000.0000, 
raw observation next is [-5.8, 76.33333333333334, 0.0, 0.0, 19.0, 24.82942987167334, 0.2686158926535127, 0.0, 1.0, 25.0, 31.47101457890092], 
processed observation next is [1.0, 0.2608695652173913, 0.30193905817174516, 0.7633333333333334, 0.0, 0.0, 0.08333333333333333, 0.5691191559727784, 0.5895386308845042, 0.0, 1.0, 0.2, 0.3147101457890092], 
reward next is 0.6853, 
noisyNet noise sample is [array([0.41105622], dtype=float32), -0.43199438]. 
=============================================
[2019-04-09 15:17:04,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[9.572343]
 [9.719834]
 [9.52225 ]
 [9.463043]
 [9.783712]], R is [[10.25596428]
 [10.68821716]
 [10.98868942]
 [11.5649519 ]
 [12.11948586]].
[2019-04-09 15:17:04,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8658407e-04 6.4311191e-02 1.2371788e-01 8.3881505e-03 1.5113627e-03
 2.1742473e-04 2.5901303e-01 1.5472495e-02 2.3120455e-02 4.6889883e-02
 4.5717162e-01], sum to 1.0000
[2019-04-09 15:17:04,857] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-09 15:17:04,871] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 75.0, 61.33333333333333, 325.0, 22.5, 26.0691821072466, 0.5040280890028611, 1.0, 1.0, 45.0, 35.52237001800866], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2191800.0000, 
sim time next is 2192400.0000, 
raw observation next is [-5.6, 75.0, 71.5, 356.5, 22.5, 26.11333691214365, 0.5183833070357148, 1.0, 1.0, 65.0, 57.52775151562], 
processed observation next is [1.0, 0.391304347826087, 0.30747922437673136, 0.75, 0.23833333333333334, 0.3939226519337017, 0.375, 0.6761114093453043, 0.6727944356785716, 1.0, 1.0, 1.0, 0.5752775151562], 
reward next is 0.4247, 
noisyNet noise sample is [array([1.0664737], dtype=float32), -0.6781449]. 
=============================================
[2019-04-09 15:17:05,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5008881e-05 5.5673011e-02 2.4243006e-01 7.7813389e-03 1.5644075e-03
 3.7192320e-05 3.4854159e-01 5.4153977e-03 2.6029577e-02 2.4354873e-02
 2.8815761e-01], sum to 1.0000
[2019-04-09 15:17:05,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5450
[2019-04-09 15:17:05,248] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.2, 69.5, 35.0, 0.0, 22.5, 27.1226484116479, 0.7192540029448203, 1.0, 1.0, 25.0, 34.53530130397436], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2219400.0000, 
sim time next is 2220000.0000, 
raw observation next is [-4.3, 70.0, 29.66666666666667, 0.0, 22.5, 27.1164671241974, 0.5813621925278999, 1.0, 1.0, 45.0, 43.01520760195283], 
processed observation next is [1.0, 0.6956521739130435, 0.34349030470914127, 0.7, 0.0988888888888889, 0.0, 0.375, 0.7597055936831166, 0.6937873975093, 1.0, 1.0, 0.6, 0.4301520760195283], 
reward next is 0.5698, 
noisyNet noise sample is [array([0.31318906], dtype=float32), -0.7408674]. 
=============================================
[2019-04-09 15:17:05,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[13.523603]
 [13.587643]
 [13.665102]
 [13.720864]
 [13.678503]], R is [[14.23675823]
 [14.74903774]
 [15.26444244]
 [15.78537178]
 [16.2726059 ]].
[2019-04-09 15:17:05,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00035902 0.09266092 0.18630527 0.010415   0.00323662 0.00038964
 0.3009343  0.01039422 0.03478573 0.02476378 0.33575553], sum to 1.0000
[2019-04-09 15:17:05,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9491
[2019-04-09 15:17:05,448] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 22.5, 24.88496713105721, 0.2843678345362605, 1.0, 1.0, 45.0, 40.75918037802825], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2275800.0000, 
sim time next is 2276400.0000, 
raw observation next is [-9.133333333333333, 89.66666666666667, 38.0, 16.66666666666667, 22.5, 24.93560501160405, 0.3337755669269446, 1.0, 1.0, 65.0, 54.60475386044354], 
processed observation next is [1.0, 0.34782608695652173, 0.20960295475530935, 0.8966666666666667, 0.12666666666666668, 0.018416206261510134, 0.375, 0.5779670843003375, 0.6112585223089816, 1.0, 1.0, 1.0, 0.5460475386044353], 
reward next is 0.4540, 
noisyNet noise sample is [array([-0.26008505], dtype=float32), -0.20342554]. 
=============================================
[2019-04-09 15:17:05,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4934914e-05 8.8689588e-02 1.9190131e-01 1.1905882e-02 4.7312977e-04
 1.3907449e-04 3.0749363e-01 6.4278515e-03 1.6161667e-02 2.3534475e-02
 3.5322839e-01], sum to 1.0000
[2019-04-09 15:17:05,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-09 15:17:05,738] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.55, 69.0, 0.0, 0.0, 22.5, 24.33016859885161, 0.3504677952967432, 1.0, 1.0, 45.0, 31.74911089261261], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2226600.0000, 
sim time next is 2227200.0000, 
raw observation next is [-4.566666666666666, 69.33333333333333, 0.0, 0.0, 22.5, 24.64348155993723, 0.3677157153905342, 1.0, 1.0, 45.0, 33.04785460477917], 
processed observation next is [1.0, 0.782608695652174, 0.3361034164358265, 0.6933333333333332, 0.0, 0.0, 0.375, 0.5536234633281024, 0.622571905130178, 1.0, 1.0, 0.6, 0.3304785460477917], 
reward next is 0.6695, 
noisyNet noise sample is [array([1.5820267], dtype=float32), -0.37532365]. 
=============================================
[2019-04-09 15:17:06,009] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4808372e-05 5.8460534e-02 1.0213507e-01 7.7190758e-03 7.8118051e-04
 8.5808773e-05 2.9825649e-01 5.1397798e-03 2.0050058e-02 1.2540330e-02
 4.9480686e-01], sum to 1.0000
[2019-04-09 15:17:06,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2540
[2019-04-09 15:17:06,054] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 69.0, 55.33333333333333, 47.49999999999999, 22.5, 27.13978155865614, 0.5538913279213039, 1.0, 1.0, 45.0, 64.04719540068466], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2218800.0000, 
sim time next is 2219400.0000, 
raw observation next is [-4.2, 69.5, 35.0, 0.0, 22.5, 26.39223420768297, 0.6680929800280855, 1.0, 1.0, 45.0, 46.33826422132406], 
processed observation next is [1.0, 0.6956521739130435, 0.34626038781163443, 0.695, 0.11666666666666667, 0.0, 0.375, 0.6993528506402477, 0.7226976600093619, 1.0, 1.0, 0.6, 0.4633826422132406], 
reward next is 0.5366, 
noisyNet noise sample is [array([-0.6958315], dtype=float32), 1.292228]. 
=============================================
[2019-04-09 15:17:06,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7392658e-05 7.0005178e-02 1.4244382e-01 4.7967122e-03 1.6433096e-03
 7.9575257e-05 4.0363565e-01 9.1777463e-03 3.2370284e-02 1.4931138e-02
 3.2088917e-01], sum to 1.0000
[2019-04-09 15:17:06,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7822
[2019-04-09 15:17:06,257] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.583333333333333, 69.66666666666667, 0.0, 0.0, 22.5, 26.48330988867085, 0.6543566947654091, 1.0, 1.0, 65.0, 48.40668016032046], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2227800.0000, 
sim time next is 2228400.0000, 
raw observation next is [-4.6, 70.0, 0.0, 0.0, 22.5, 26.48156535189337, 0.6414611573699821, 0.0, 1.0, 65.0, 46.58816152282192], 
processed observation next is [1.0, 0.8260869565217391, 0.33518005540166207, 0.7, 0.0, 0.0, 0.375, 0.7067971126577808, 0.713820385789994, 0.0, 1.0, 1.0, 0.4658816152282192], 
reward next is 0.5341, 
noisyNet noise sample is [array([-0.1854447], dtype=float32), 0.25821406]. 
=============================================
[2019-04-09 15:17:06,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3351003e-05 2.8854802e-02 5.0179586e-02 5.8872350e-03 8.5054233e-04
 2.1239978e-04 5.0841540e-01 5.0237090e-03 2.3231888e-02 2.0050956e-02
 3.5725006e-01], sum to 1.0000
[2019-04-09 15:17:06,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8380
[2019-04-09 15:17:06,399] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 59.0, 0.0, 0.0, 19.0, 26.28656474491033, 0.5987727914775044, 0.0, 1.0, 65.0, 47.05396026744656], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2329200.0000, 
sim time next is 2329800.0000, 
raw observation next is [-2.3, 60.0, 0.0, 0.0, 19.0, 26.27610786579005, 0.5964188121448245, 0.0, 1.0, 65.0, 47.18775260952864], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.6, 0.0, 0.0, 0.08333333333333333, 0.6896756554825041, 0.6988062707149415, 0.0, 1.0, 1.0, 0.4718775260952864], 
reward next is 0.5281, 
noisyNet noise sample is [array([0.5404956], dtype=float32), 1.3800714]. 
=============================================
[2019-04-09 15:17:06,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5944198e-05 1.1554872e-01 1.5890898e-01 1.0598670e-02 1.6418094e-03
 1.4577189e-04 3.3814514e-01 7.9943016e-03 3.4320764e-02 2.2629656e-02
 3.1001028e-01], sum to 1.0000
[2019-04-09 15:17:06,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6560
[2019-04-09 15:17:06,431] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 22.5, 27.05873915554008, 0.7213781624479937, 1.0, 1.0, 25.0, 28.08943564574671], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 22.5, 27.07443261064947, 0.7288266113921683, 1.0, 1.0, 55.0, 29.52075850162256], 
processed observation next is [1.0, 0.5652173913043478, 0.44090489381348114, 0.46, 0.6255555555555558, 0.07292817679558011, 0.375, 0.7562027175541225, 0.7429422037973894, 1.0, 1.0, 0.8, 0.29520758501622557], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.06126569], dtype=float32), -0.6906937]. 
=============================================
[2019-04-09 15:17:06,452] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6881458e-05 1.4070074e-01 8.2058176e-02 1.8766655e-02 5.2939134e-04
 1.6921152e-04 4.8084256e-01 4.6163700e-03 5.0223321e-02 2.4475509e-02
 1.9757117e-01], sum to 1.0000
[2019-04-09 15:17:06,458] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0613
[2019-04-09 15:17:06,479] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 51.0, 241.5, 71.5, 22.5, 26.69572421011773, 0.6724255116411372, 1.0, 1.0, 65.0, 36.66025031652315], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 22.5, 26.78394265969021, 0.6795425430291239, 1.0, 1.0, 20.0, 34.74951328292933], 
processed observation next is [1.0, 0.5652173913043478, 0.4205909510618652, 0.5, 0.7822222222222224, 0.07808471454880295, 0.375, 0.731995221640851, 0.726514181009708, 1.0, 1.0, 0.1, 0.3474951328292933], 
reward next is 0.6525, 
noisyNet noise sample is [array([-0.23190957], dtype=float32), 1.0356458]. 
=============================================
[2019-04-09 15:17:06,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9171151e-05 5.9284709e-02 1.5866195e-01 9.2290016e-03 9.3357748e-04
 6.5906308e-05 4.2220852e-01 7.2737327e-03 4.9375795e-02 9.6877906e-03
 2.8324986e-01], sum to 1.0000
[2019-04-09 15:17:06,912] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6728
[2019-04-09 15:17:06,934] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 44.0, 89.5, 21.0, 22.5, 26.65332889793065, 0.7540406216921638, 1.0, 1.0, 45.0, 34.66681430321257], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2304000.0000, 
sim time next is 2304600.0000, 
raw observation next is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 22.5, 26.17515975999536, 0.6830289100585117, 1.0, 1.0, 55.0, 16.44839861539692], 
processed observation next is [1.0, 0.6956521739130435, 0.4598337950138504, 0.4483333333333334, 0.24444444444444444, 0.015469613259668509, 0.375, 0.6812633133329467, 0.7276763033528373, 1.0, 1.0, 0.8, 0.1644839861539692], 
reward next is 0.8355, 
noisyNet noise sample is [array([-0.302312], dtype=float32), 0.17933033]. 
=============================================
[2019-04-09 15:17:06,985] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7147670e-05 7.2384238e-02 1.6798271e-01 9.5637133e-03 1.9874193e-03
 1.5507238e-04 3.6430308e-01 1.4103572e-02 1.8787311e-02 1.3313128e-02
 3.3734259e-01], sum to 1.0000
[2019-04-09 15:17:06,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9644
[2019-04-09 15:17:06,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1376428e-05 9.7549483e-02 1.6360222e-01 6.1904876e-03 1.0081398e-03
 8.3797117e-05 3.1808320e-01 1.2581081e-02 2.7510487e-02 1.8476475e-02
 3.5485321e-01], sum to 1.0000
[2019-04-09 15:17:07,000] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 139.5, 44.5, 22.5, 25.93072225621015, 0.452230237124939, 1.0, 1.0, 65.0, 52.789357963145], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2282400.0000, 
sim time next is 2283000.0000, 
raw observation next is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 22.5, 26.06371260907833, 0.4748963415505774, 1.0, 1.0, 65.0, 48.26401098516941], 
processed observation next is [1.0, 0.43478260869565216, 0.2848568790397045, 0.7633333333333333, 0.5077777777777777, 0.05119705340699815, 0.375, 0.6719760507565274, 0.6582987805168591, 1.0, 1.0, 1.0, 0.48264010985169414], 
reward next is 0.5174, 
noisyNet noise sample is [array([0.10250562], dtype=float32), -0.3415546]. 
=============================================
[2019-04-09 15:17:07,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0952
[2019-04-09 15:17:07,005] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.83476571e-06 3.51407640e-02 2.01428965e-01 6.48728199e-03
 8.40672699e-04 4.75836750e-05 5.20109534e-01 3.64092714e-03
 1.83533449e-02 1.24003645e-02 2.01542810e-01], sum to 1.0000
[2019-04-09 15:17:07,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3381
[2019-04-09 15:17:07,015] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[11.85222 ]
 [11.737676]
 [11.525111]
 [11.383964]
 [11.320251]], R is [[12.41142559]
 [12.75941753]
 [13.14340305]
 [13.57207489]
 [13.90562916]].
[2019-04-09 15:17:07,025] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 70.0, 8.333333333333332, 0.0, 22.5, 26.92661728424947, 0.686366903001281, 1.0, 1.0, 25.0, 36.85427603568038], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [-4.5, 69.5, 0.0, 0.0, 22.5, 26.94564286519415, 0.6735777089011612, 1.0, 1.0, 45.0, 46.19368505801549], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.695, 0.0, 0.0, 0.375, 0.745470238766179, 0.7245259029670538, 1.0, 1.0, 0.6, 0.4619368505801549], 
reward next is 0.5381, 
noisyNet noise sample is [array([0.85166204], dtype=float32), 0.61328095]. 
=============================================
[2019-04-09 15:17:07,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.100000000000001, 68.5, 0.0, 0.0, 19.0, 25.83372271829558, 0.5553023112448074, 0.0, 1.0, 50.0, 35.79795869626857], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [-5.2, 69.0, 0.0, 0.0, 19.0, 25.80261776534612, 0.5472979848743601, 0.0, 1.0, 20.0, 38.45652562235848], 
processed observation next is [1.0, 0.9130434782608695, 0.31855955678670367, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6502181471121767, 0.6824326616247868, 0.0, 1.0, 0.1, 0.3845652562235848], 
reward next is 0.6154, 
noisyNet noise sample is [array([-1.0032992], dtype=float32), 0.32206693]. 
=============================================
[2019-04-09 15:17:07,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[13.969862]
 [14.025669]
 [14.174715]
 [13.985699]
 [14.290791]], R is [[14.49944496]
 [14.98590755]
 [15.41156387]
 [15.89889526]
 [16.2824173 ]].
[2019-04-09 15:17:07,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7858847e-05 1.1251063e-01 1.1842006e-01 1.4386970e-02 8.2758174e-04
 1.9266135e-04 2.9457951e-01 6.3808681e-03 2.8372485e-02 2.8652947e-02
 3.9558852e-01], sum to 1.0000
[2019-04-09 15:17:07,325] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2267
[2019-04-09 15:17:07,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 22.5, 26.46641750639657, 0.5544651329897589, 1.0, 1.0, 25.0, 34.68672146769443], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 22.5, 26.51129466970326, 0.5636693427257237, 1.0, 1.0, 25.0, 33.73921422903237], 
processed observation next is [1.0, 0.43478260869565216, 0.29270544783010155, 0.7466666666666667, 0.5505555555555557, 0.05322283609576428, 0.375, 0.7092745558086051, 0.6878897809085746, 1.0, 1.0, 0.2, 0.3373921422903237], 
reward next is 0.6626, 
noisyNet noise sample is [array([1.1305301], dtype=float32), 0.3035845]. 
=============================================
[2019-04-09 15:17:07,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7205001e-05 5.4086931e-02 2.0571150e-01 8.5034398e-03 1.5685468e-03
 7.4171941e-05 4.2736673e-01 8.0300188e-03 1.0564388e-02 2.6222400e-02
 2.5781465e-01], sum to 1.0000
[2019-04-09 15:17:07,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-09 15:17:07,606] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 70.5, 0.0, 0.0, 19.0, 25.97848834519546, 0.5777078944864721, 0.0, 1.0, 20.0, 48.44620502728193], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2232600.0000, 
sim time next is 2233200.0000, 
raw observation next is [-5.0, 70.0, 0.0, 0.0, 19.0, 25.97831421142928, 0.5733950401509209, 0.0, 1.0, 45.0, 37.43942801677454], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6648595176191066, 0.691131680050307, 0.0, 1.0, 0.6, 0.3743942801677454], 
reward next is 0.6256, 
noisyNet noise sample is [array([0.72174406], dtype=float32), -0.27624413]. 
=============================================
[2019-04-09 15:17:08,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9711118e-05 9.4630465e-02 1.5405701e-01 8.0254599e-03 7.5970840e-04
 7.9235746e-05 3.2939106e-01 8.8092927e-03 1.4393498e-02 1.9276695e-02
 3.7052786e-01], sum to 1.0000
[2019-04-09 15:17:08,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4534
[2019-04-09 15:17:08,163] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 26.59706220495295, 0.6857947681973197, 0.0, 1.0, 65.0, 41.92397060625231], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2317200.0000, 
sim time next is 2317800.0000, 
raw observation next is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 22.5, 26.54749481234004, 0.6825271792319496, 1.0, 1.0, 45.0, 40.12021258307395], 
processed observation next is [1.0, 0.8260869565217391, 0.4178208679593721, 0.5566666666666668, 0.0, 0.0, 0.375, 0.7122912343616701, 0.7275090597439832, 1.0, 1.0, 0.6, 0.4012021258307395], 
reward next is 0.5988, 
noisyNet noise sample is [array([-1.2873708], dtype=float32), 1.1465344]. 
=============================================
[2019-04-09 15:17:08,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4013278e-05 5.2256774e-02 1.5622054e-01 9.0222443e-03 1.0764938e-03
 1.7791348e-04 3.8689151e-01 1.1654550e-02 2.9515034e-02 1.9959742e-02
 3.3316115e-01], sum to 1.0000
[2019-04-09 15:17:08,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3901
[2019-04-09 15:17:08,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4230398e-04 9.3842261e-02 1.0883027e-01 1.2795026e-02 2.2105372e-03
 4.0398847e-04 3.3505097e-01 1.8164204e-02 5.9008207e-02 4.4617549e-02
 3.2483470e-01], sum to 1.0000
[2019-04-09 15:17:08,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6706
[2019-04-09 15:17:08,440] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 25.4204421511795, 0.4569852542403868, 0.0, 1.0, 45.0, 52.79652884062456], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 19.0, 25.40877676320246, 0.4437267574097835, 0.0, 1.0, 45.0, 38.19963985531982], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.08333333333333333, 0.6173980636002051, 0.6479089191365945, 0.0, 1.0, 0.6, 0.3819963985531982], 
reward next is 0.6180, 
noisyNet noise sample is [array([-0.5876288], dtype=float32), -0.014483275]. 
=============================================
[2019-04-09 15:17:08,470] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 19.0, 25.14632395314162, 0.2907193925552714, 0.0, 1.0, 50.0, 35.94522249296577], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 25.0152617906962, 0.2792802744176111, 0.0, 1.0, 60.0, 59.89617829926055], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5846051492246834, 0.5930934248058704, 0.0, 1.0, 0.9, 0.5989617829926055], 
reward next is 0.4010, 
noisyNet noise sample is [array([-0.40280774], dtype=float32), -0.13791056]. 
=============================================
[2019-04-09 15:17:08,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9696113e-04 3.6163852e-02 1.6075747e-01 1.6689546e-02 1.8961721e-03
 5.6718098e-04 3.2629427e-01 1.7111221e-02 3.1182077e-02 1.9850129e-02
 3.8929102e-01], sum to 1.0000
[2019-04-09 15:17:08,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3465
[2019-04-09 15:17:08,903] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 19.0, 24.77990027150684, 0.2671507645226812, 0.0, 1.0, 25.0, 50.97514066695258], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 24.83136569458542, 0.259356054116741, 0.0, 1.0, 45.0, 38.88807460176398], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.5692804745487848, 0.5864520180389137, 0.0, 1.0, 0.6, 0.38888074601763983], 
reward next is 0.6111, 
noisyNet noise sample is [array([0.5553518], dtype=float32), 1.3156632]. 
=============================================
[2019-04-09 15:17:08,971] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[10.397992]
 [10.321273]
 [10.249497]
 [10.363868]
 [10.054568]], R is [[10.83862591]
 [11.22048855]
 [11.5493288 ]
 [11.9165926 ]
 [12.14819622]].
[2019-04-09 15:17:09,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5872970e-05 6.2569015e-02 1.0028341e-01 1.1710194e-02 1.4221867e-03
 1.4292658e-04 3.1104460e-01 8.6280853e-03 4.7691196e-02 1.3479413e-02
 4.4299322e-01], sum to 1.0000
[2019-04-09 15:17:09,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9028
[2019-04-09 15:17:09,084] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 51.0, 0.0, 0.0, 22.5, 27.19674015728397, 0.6183436404662346, 1.0, 1.0, 65.0, 72.05496764172926], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2310000.0000, 
sim time next is 2310600.0000, 
raw observation next is [-1.1, 51.5, 0.0, 0.0, 22.5, 26.46378815073183, 0.7433193068664972, 1.0, 1.0, 65.0, 36.6111195472044], 
processed observation next is [1.0, 0.7391304347826086, 0.4321329639889197, 0.515, 0.0, 0.0, 0.375, 0.7053156792276525, 0.7477731022888324, 1.0, 1.0, 1.0, 0.366111195472044], 
reward next is 0.6339, 
noisyNet noise sample is [array([-0.583333], dtype=float32), -0.1253034]. 
=============================================
[2019-04-09 15:17:09,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00044647 0.17690621 0.10369133 0.02534805 0.00369275 0.00092901
 0.30446598 0.01953111 0.03615187 0.02793238 0.30090478], sum to 1.0000
[2019-04-09 15:17:09,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-09 15:17:09,317] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.8999999999999999, 50.5, 252.0, 424.0, 19.0, 25.49385116513562, 0.5245187243703139, 0.0, 1.0, 20.0, 42.03476981917982], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2377800.0000, 
sim time next is 2378400.0000, 
raw observation next is [-0.8, 51.66666666666667, 241.8333333333333, 353.3333333333334, 19.0, 25.69009303611434, 0.5422176523438322, 0.0, 1.0, 60.0, 45.02165187230288], 
processed observation next is [0.0, 0.5217391304347826, 0.4404432132963989, 0.5166666666666667, 0.806111111111111, 0.39042357274401485, 0.08333333333333333, 0.6408410863428617, 0.6807392174479441, 0.0, 1.0, 0.9, 0.4502165187230288], 
reward next is 0.5498, 
noisyNet noise sample is [array([-0.97012234], dtype=float32), -1.1148891]. 
=============================================
[2019-04-09 15:17:09,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00045975 0.195301   0.10749172 0.03024399 0.00395753 0.00087394
 0.29167926 0.01948982 0.03780863 0.02744198 0.2852524 ], sum to 1.0000
[2019-04-09 15:17:09,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-09 15:17:09,396] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.8, 51.66666666666667, 241.8333333333333, 353.3333333333334, 19.0, 25.69009303611434, 0.5422176523438322, 0.0, 1.0, 60.0, 45.02165187230288], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2378400.0000, 
sim time next is 2379000.0000, 
raw observation next is [-0.7, 52.83333333333333, 231.6666666666667, 282.6666666666667, 19.0, 25.81991374854824, 0.5508442647570589, 0.0, 1.0, 20.0, 42.04345130320613], 
processed observation next is [0.0, 0.5217391304347826, 0.443213296398892, 0.5283333333333333, 0.7722222222222224, 0.3123388581952118, 0.08333333333333333, 0.6516594790456868, 0.6836147549190197, 0.0, 1.0, 0.1, 0.4204345130320613], 
reward next is 0.5796, 
noisyNet noise sample is [array([-0.97012234], dtype=float32), -1.1148891]. 
=============================================
[2019-04-09 15:17:09,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[9.820765]
 [9.809256]
 [9.70129 ]
 [9.315918]
 [9.354386]], R is [[10.06849194]
 [10.51759052]
 [10.99206734]
 [11.30667686]
 [11.83844757]].
[2019-04-09 15:17:09,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00060731 0.08188016 0.08505843 0.01934825 0.00375044 0.00085234
 0.35485595 0.02305524 0.03053515 0.03945561 0.360601  ], sum to 1.0000
[2019-04-09 15:17:09,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-09 15:17:09,857] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.2, 86.66666666666667, 0.0, 0.0, 19.0, 25.01717700006198, 0.3126636876146618, 0.0, 1.0, 50.0, 43.44828645907882], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2259600.0000, 
sim time next is 2260200.0000, 
raw observation next is [-8.3, 86.83333333333333, 0.0, 0.0, 19.0, 25.03157084348883, 0.3053285475375049, 0.0, 1.0, 45.0, 42.48799090344579], 
processed observation next is [1.0, 0.13043478260869565, 0.23268698060941828, 0.8683333333333333, 0.0, 0.0, 0.08333333333333333, 0.5859642369574024, 0.6017761825125016, 0.0, 1.0, 0.6, 0.42487990903445794], 
reward next is 0.5751, 
noisyNet noise sample is [array([-0.8418014], dtype=float32), -0.97946507]. 
=============================================
[2019-04-09 15:17:10,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3922676e-04 1.5056315e-01 1.8102664e-01 1.7037710e-02 1.9485014e-03
 5.0283002e-04 3.4985471e-01 1.3836995e-02 3.4763932e-02 2.0377157e-02
 2.2994918e-01], sum to 1.0000
[2019-04-09 15:17:10,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5377
[2019-04-09 15:17:10,041] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.583333333333334, 87.66666666666666, 63.66666666666666, 23.66666666666666, 22.5, 25.08787170578459, 0.2611222092247522, 1.0, 1.0, 20.0, 31.33660513939842], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2278200.0000, 
sim time next is 2278800.0000, 
raw observation next is [-8.4, 87.0, 73.0, 27.5, 22.5, 25.08873719686301, 0.2606161279647385, 1.0, 1.0, 45.0, 29.80181979657274], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.87, 0.24333333333333335, 0.03038674033149171, 0.375, 0.5907280997385843, 0.5868720426549129, 1.0, 1.0, 0.6, 0.2980181979657274], 
reward next is 0.7020, 
noisyNet noise sample is [array([0.9446335], dtype=float32), -0.5454171]. 
=============================================
[2019-04-09 15:17:10,157] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9638235e-04 9.8540336e-02 1.4874862e-01 3.1937834e-02 5.1729050e-03
 1.1332002e-03 2.0482534e-01 1.6615469e-02 3.9807517e-02 2.7554618e-02
 4.2526779e-01], sum to 1.0000
[2019-04-09 15:17:10,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7022
[2019-04-09 15:17:10,181] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.1, 48.16666666666667, 223.6666666666667, 384.6666666666666, 19.0, 25.87843596894277, 0.5662085859994039, 0.0, 1.0, 65.0, 52.88439376998787], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [-1.0, 49.33333333333334, 237.8333333333333, 404.3333333333334, 19.0, 26.02655581348694, 0.5890285937628316, 0.0, 1.0, 65.0, 49.65348782831287], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.4933333333333334, 0.7927777777777776, 0.44677716390423583, 0.08333333333333333, 0.6688796511239117, 0.6963428645876105, 0.0, 1.0, 1.0, 0.4965348782831287], 
reward next is 0.5035, 
noisyNet noise sample is [array([-0.2966967], dtype=float32), 0.2504147]. 
=============================================
[2019-04-09 15:17:10,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.08405137e-05 1.20126344e-01 1.08768255e-01 1.58171356e-02
 1.13041105e-03 1.57526840e-04 2.83012062e-01 8.27697851e-03
 2.77164821e-02 2.09783316e-02 4.13965613e-01], sum to 1.0000
[2019-04-09 15:17:10,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0368
[2019-04-09 15:17:10,503] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.95, 56.83333333333334, 228.3333333333333, 86.0, 22.5, 26.64473166036334, 0.6262942119464524, 1.0, 1.0, 65.0, 40.98544956779965], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 22.5, 26.69029764202113, 0.6394563425552791, 1.0, 1.0, 20.0, 39.1579337155118], 
processed observation next is [1.0, 0.5217391304347826, 0.38781163434903054, 0.5566666666666668, 0.8172222222222224, 0.08839779005524862, 0.375, 0.7241914701684274, 0.7131521141850931, 1.0, 1.0, 0.1, 0.391579337155118], 
reward next is 0.6084, 
noisyNet noise sample is [array([2.5087056], dtype=float32), -2.1471746]. 
=============================================
[2019-04-09 15:17:10,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3701587e-05 1.7459424e-01 1.6426569e-01 9.4201341e-03 1.0131268e-03
 2.0446665e-04 3.0927563e-01 9.2557054e-03 2.2086019e-02 1.2216816e-02
 2.9764441e-01], sum to 1.0000
[2019-04-09 15:17:10,534] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-09 15:17:10,553] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 63.0, 161.0, 110.0, 22.5, 26.72673549778482, 0.6209005808739824, 1.0, 1.0, 45.0, 34.8400441582405], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2287800.0000, 
sim time next is 2288400.0000, 
raw observation next is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 22.5, 26.75974759970664, 0.6266362189860829, 1.0, 1.0, 45.0, 32.4480840184308], 
processed observation next is [1.0, 0.4782608695652174, 0.3573407202216067, 0.6133333333333333, 0.5927777777777776, 0.11491712707182321, 0.375, 0.72997896664222, 0.7088787396620276, 1.0, 1.0, 0.6, 0.324480840184308], 
reward next is 0.6755, 
noisyNet noise sample is [array([-2.2394235], dtype=float32), 0.43502316]. 
=============================================
[2019-04-09 15:17:10,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5757880e-06 4.9181785e-02 1.4521031e-01 9.0729427e-03 2.2912359e-04
 2.3277880e-05 3.5318041e-01 4.6251495e-03 2.1724673e-02 9.5090372e-03
 4.0723568e-01], sum to 1.0000
[2019-04-09 15:17:10,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6544
[2019-04-09 15:17:10,710] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 22.5, 27.15877203130262, 0.732502160706014, 1.0, 1.0, 45.0, 29.61461897923813], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2299200.0000, 
sim time next is 2299800.0000, 
raw observation next is [0.8166666666666668, 43.33333333333334, 126.6666666666667, 54.0, 22.5, 27.19150370859545, 0.7338947725978274, 1.0, 1.0, 65.0, 30.10756504017807], 
processed observation next is [1.0, 0.6086956521739131, 0.4852262234533703, 0.4333333333333334, 0.42222222222222233, 0.05966850828729282, 0.375, 0.7659586423829543, 0.7446315908659424, 1.0, 1.0, 1.0, 0.3010756504017807], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.21726042], dtype=float32), -1.404342]. 
=============================================
[2019-04-09 15:17:11,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0073861e-05 1.4975767e-01 1.3083820e-01 9.7410101e-03 6.7596050e-04
 1.1578308e-04 2.4445567e-01 7.8651588e-03 2.2481844e-02 2.8747862e-02
 4.0528074e-01], sum to 1.0000
[2019-04-09 15:17:11,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5260
[2019-04-09 15:17:11,676] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 22.5, 26.90089338067995, 0.7110069216444318, 1.0, 1.0, 25.0, 27.31160259453114], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 22.5, 26.94779193076246, 0.7107513943510715, 1.0, 1.0, 45.0, 32.25231838889835], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.375, 0.7456493275635383, 0.7369171314503572, 1.0, 1.0, 0.6, 0.32252318388898354], 
reward next is 0.6775, 
noisyNet noise sample is [array([0.7755388], dtype=float32), 0.6161228]. 
=============================================
[2019-04-09 15:17:11,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9485354e-05 7.4953832e-02 6.0166318e-02 6.8585654e-03 4.7710686e-04
 6.6353561e-05 1.9925843e-01 2.6681300e-03 1.9140925e-02 2.6224826e-02
 6.1015600e-01], sum to 1.0000
[2019-04-09 15:17:11,813] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1215
[2019-04-09 15:17:11,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2888441e-05 9.0197518e-02 9.8755211e-02 1.7447084e-02 1.7189013e-03
 2.1332299e-04 2.2167574e-01 1.1333313e-02 2.6315371e-02 2.3348659e-02
 5.0892198e-01], sum to 1.0000
[2019-04-09 15:17:11,874] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.2, 53.66666666666666, 0.0, 0.0, 22.5, 26.81189927368144, 0.7064155280093308, 1.0, 1.0, 65.0, 37.42351305637372], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2314200.0000, 
sim time next is 2314800.0000, 
raw observation next is [-1.2, 54.0, 0.0, 0.0, 22.5, 26.78806996645603, 0.7056732984104465, 1.0, 1.0, 25.0, 33.65757396129004], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.54, 0.0, 0.0, 0.375, 0.7323391638713357, 0.7352244328034822, 1.0, 1.0, 0.2, 0.3365757396129004], 
reward next is 0.6634, 
noisyNet noise sample is [array([0.04205304], dtype=float32), -0.078554]. 
=============================================
[2019-04-09 15:17:11,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8295
[2019-04-09 15:17:11,922] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.1, 63.0, 161.0, 110.0, 22.5, 25.61851983749426, 0.3950949423142168, 1.0, 1.0, 60.0, 54.8993694604785], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2287800.0000, 
sim time next is 2288400.0000, 
raw observation next is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 22.5, 25.64962373281122, 0.4294217700598233, 1.0, 1.0, 65.0, 70.14454877266287], 
processed observation next is [1.0, 0.4782608695652174, 0.3573407202216067, 0.6133333333333333, 0.5927777777777776, 0.11491712707182321, 0.375, 0.6374686444009351, 0.6431405900199411, 1.0, 1.0, 1.0, 0.7014454877266287], 
reward next is 0.2986, 
noisyNet noise sample is [array([0.3030227], dtype=float32), -0.732722]. 
=============================================
[2019-04-09 15:17:13,342] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5790541e-05 3.6702830e-02 2.3193501e-01 4.2003770e-03 1.2556458e-03
 1.6796774e-04 2.8376934e-01 8.5203256e-03 2.4259934e-02 2.2724127e-02
 3.8642862e-01], sum to 1.0000
[2019-04-09 15:17:13,342] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2833
[2019-04-09 15:17:13,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00253258 0.12015714 0.18112953 0.03614387 0.0080812  0.0034816
 0.23840359 0.02466966 0.05508491 0.05834657 0.27196935], sum to 1.0000
[2019-04-09 15:17:13,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-09 15:17:13,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 22.5, 26.82937807662899, 0.7282991667493982, 1.0, 1.0, 65.0, 37.61652632892388], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2315400.0000, 
sim time next is 2316000.0000, 
raw observation next is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 26.78806330018252, 0.7166804231676044, 0.0, 1.0, 45.0, 34.40584620177594], 
processed observation next is [1.0, 0.8260869565217391, 0.42474607571560485, 0.5466666666666667, 0.0, 0.0, 0.375, 0.7323386083485435, 0.7388934743892014, 0.0, 1.0, 0.6, 0.34405846201775936], 
reward next is 0.6559, 
noisyNet noise sample is [array([-0.9499934], dtype=float32), -1.7674804]. 
=============================================
[2019-04-09 15:17:13,392] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 24.72002797805454, 0.1892456814249107, 0.0, 1.0, 25.0, 32.36386142734847], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2428800.0000, 
sim time next is 2429400.0000, 
raw observation next is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 24.61642656344706, 0.1817084197606726, 0.0, 1.0, 65.0, 60.19843992127555], 
processed observation next is [0.0, 0.08695652173913043, 0.24884579870729456, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.5513688802872551, 0.5605694732535575, 0.0, 1.0, 1.0, 0.6019843992127555], 
reward next is 0.3980, 
noisyNet noise sample is [array([0.50185627], dtype=float32), 0.71707714]. 
=============================================
[2019-04-09 15:17:13,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[13.706119]
 [13.866292]
 [13.872529]
 [13.979781]
 [13.691325]], R is [[14.37971783]
 [14.85975552]
 [15.33117199]
 [15.857481  ]
 [16.39834213]].
[2019-04-09 15:17:13,926] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.33569318e-05 7.43158609e-02 1.70038566e-01 1.50685115e-02
 1.05013489e-03 2.21446811e-04 1.72509283e-01 9.77094937e-03
 2.12992784e-02 1.61978900e-02 5.19434750e-01], sum to 1.0000
[2019-04-09 15:17:13,936] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3152
[2019-04-09 15:17:13,960] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.25170932359954, 0.5927749004130773, 0.0, 1.0, 45.0, 38.17326950153051], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2331000.0000, 
sim time next is 2331600.0000, 
raw observation next is [-2.3, 63.0, 0.0, 0.0, 19.0, 26.23306094084204, 0.5846790753515886, 0.0, 1.0, 45.0, 36.57119326384711], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6860884117368368, 0.6948930251171962, 0.0, 1.0, 0.6, 0.36571193263847107], 
reward next is 0.6343, 
noisyNet noise sample is [array([-2.8156008e-05], dtype=float32), 0.59559596]. 
=============================================
[2019-04-09 15:17:14,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.001131   0.08791647 0.14897981 0.03927261 0.01036861 0.00098554
 0.30712909 0.0245857  0.03818636 0.03311788 0.3083269 ], sum to 1.0000
[2019-04-09 15:17:14,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0297
[2019-04-09 15:17:14,191] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.55, 54.0, 0.0, 0.0, 19.0, 25.30558923103552, 0.3362941131919445, 0.0, 1.0, 30.0, 42.38715259990421], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2428200.0000, 
sim time next is 2428800.0000, 
raw observation next is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 25.29522076667398, 0.3189204659666244, 0.0, 1.0, 20.0, 41.58687391272314], 
processed observation next is [0.0, 0.08695652173913043, 0.2511542012927055, 0.5433333333333333, 0.0, 0.0, 0.08333333333333333, 0.6079350638894985, 0.6063068219888748, 0.0, 1.0, 0.1, 0.41586873912723143], 
reward next is 0.5841, 
noisyNet noise sample is [array([0.56474245], dtype=float32), -0.39539927]. 
=============================================
[2019-04-09 15:17:14,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4786917e-05 1.2273804e-01 2.4501134e-01 4.7880760e-03 1.0889071e-03
 1.5471694e-04 3.0143121e-01 7.2674262e-03 2.6216939e-02 1.1703223e-02
 2.7956530e-01], sum to 1.0000
[2019-04-09 15:17:14,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1125
[2019-04-09 15:17:14,332] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 54.0, 0.0, 0.0, 22.5, 26.241415073555, 0.6160550840618861, 1.0, 1.0, 45.0, 39.26761544979474], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2314800.0000, 
sim time next is 2315400.0000, 
raw observation next is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 22.5, 26.26593299095893, 0.6130656428007148, 1.0, 1.0, 20.0, 26.55632430173877], 
processed observation next is [1.0, 0.8260869565217391, 0.4270544783010157, 0.5433333333333333, 0.0, 0.0, 0.375, 0.6888277492465775, 0.7043552142669048, 1.0, 1.0, 0.1, 0.2655632430173877], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.17689961], dtype=float32), -0.81300443]. 
=============================================
[2019-04-09 15:17:14,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00077292 0.05892827 0.17545736 0.02552623 0.00636434 0.00127442
 0.30377024 0.01306167 0.04540465 0.03180193 0.337638  ], sum to 1.0000
[2019-04-09 15:17:14,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8695
[2019-04-09 15:17:14,377] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 19.0, 25.88308010261267, 0.5128220566884322, 0.0, 1.0, 65.0, 53.38257913697502], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2403600.0000, 
sim time next is 2404200.0000, 
raw observation next is [-3.233333333333333, 42.16666666666666, 0.0, 0.0, 19.0, 25.91980649340827, 0.5205955688852716, 0.0, 1.0, 65.0, 56.95130101101206], 
processed observation next is [0.0, 0.8260869565217391, 0.3730378578024008, 0.4216666666666666, 0.0, 0.0, 0.08333333333333333, 0.6599838744506892, 0.6735318562950905, 0.0, 1.0, 1.0, 0.5695130101101207], 
reward next is 0.4305, 
noisyNet noise sample is [array([0.5284697], dtype=float32), 0.7438062]. 
=============================================
[2019-04-09 15:17:15,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00087819 0.1737141  0.14765306 0.03028665 0.00699078 0.00162015
 0.27594453 0.0202975  0.04276142 0.04315512 0.25669852], sum to 1.0000
[2019-04-09 15:17:15,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9277
[2019-04-09 15:17:15,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 67.0, 121.0, 360.0, 19.0, 25.40095775581703, 0.446703745634764, 0.0, 1.0, 45.0, 46.9702848885157], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2367000.0000, 
sim time next is 2367600.0000, 
raw observation next is [-3.0, 66.33333333333333, 124.0, 375.0, 19.0, 25.41815268852663, 0.4685150563267809, 0.0, 1.0, 65.0, 58.44283309479046], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.6633333333333333, 0.41333333333333333, 0.4143646408839779, 0.08333333333333333, 0.6181793907105524, 0.6561716854422603, 0.0, 1.0, 1.0, 0.5844283309479046], 
reward next is 0.4156, 
noisyNet noise sample is [array([0.04581105], dtype=float32), -0.06381863]. 
=============================================
[2019-04-09 15:17:15,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00064564 0.12344243 0.15555546 0.01881497 0.01041579 0.00141145
 0.30435133 0.01968667 0.05108324 0.03317786 0.28141516], sum to 1.0000
[2019-04-09 15:17:15,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9675
[2019-04-09 15:17:15,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 44.66666666666667, 0.0, 0.0, 19.0, 25.80654945721997, 0.4499083796946321, 0.0, 1.0, 25.0, 39.82845965065471], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2420400.0000, 
sim time next is 2421000.0000, 
raw observation next is [-5.9, 45.5, 0.0, 0.0, 19.0, 25.83236779277955, 0.4443616534769232, 0.0, 1.0, 65.0, 53.23841087879487], 
processed observation next is [0.0, 0.0, 0.2991689750692521, 0.455, 0.0, 0.0, 0.08333333333333333, 0.6526973160649625, 0.6481205511589744, 0.0, 1.0, 1.0, 0.5323841087879487], 
reward next is 0.4676, 
noisyNet noise sample is [array([0.3737458], dtype=float32), 0.27593955]. 
=============================================
[2019-04-09 15:17:15,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[9.187182]
 [9.130673]
 [9.203859]
 [9.571019]
 [9.309067]], R is [[ 9.3792572 ]
 [ 9.88717937]
 [10.28593349]
 [10.63559532]
 [11.11278629]].
[2019-04-09 15:17:16,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.000898   0.09226597 0.11161849 0.02047327 0.00499137 0.00120563
 0.28825536 0.01473692 0.03463383 0.03841294 0.39250827], sum to 1.0000
[2019-04-09 15:17:16,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8809
[2019-04-09 15:17:16,273] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.00973828578505, 0.5168093517968629, 0.0, 1.0, 25.0, 45.26467346004534], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 19.0, 25.99395486495435, 0.5141298614558899, 0.0, 1.0, 65.0, 50.42730835604418], 
processed observation next is [0.0, 0.13043478260869565, 0.3965835641735919, 0.625, 0.0, 0.0, 0.08333333333333333, 0.6661629054128625, 0.6713766204852966, 0.0, 1.0, 1.0, 0.5042730835604418], 
reward next is 0.4957, 
noisyNet noise sample is [array([0.70207804], dtype=float32), -0.7505093]. 
=============================================
[2019-04-09 15:17:16,337] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2560839e-04 7.0751823e-02 1.9755614e-01 2.3476413e-02 5.4494650e-03
 1.6083207e-03 2.7161011e-01 6.6163694e-03 2.5764219e-02 2.3190029e-02
 3.7365150e-01], sum to 1.0000
[2019-04-09 15:17:16,337] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-09 15:17:16,378] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 46.33333333333334, 0.0, 0.0, 19.0, 26.06144791334146, 0.4871600435467192, 0.0, 1.0, 25.0, 40.78674569482533], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2421600.0000, 
sim time next is 2422200.0000, 
raw observation next is [-6.1, 47.16666666666666, 0.0, 0.0, 19.0, 26.00386700583563, 0.4818656759733653, 0.0, 1.0, 65.0, 54.15058666759864], 
processed observation next is [0.0, 0.0, 0.29362880886426596, 0.47166666666666657, 0.0, 0.0, 0.08333333333333333, 0.6669889171529692, 0.6606218919911218, 0.0, 1.0, 1.0, 0.5415058666759864], 
reward next is 0.4585, 
noisyNet noise sample is [array([-1.6077011], dtype=float32), -1.2073072]. 
=============================================
[2019-04-09 15:17:16,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1466896e-04 1.1484716e-01 1.2432188e-01 1.3876631e-02 2.8154890e-03
 5.4535060e-04 3.0773562e-01 1.1113741e-02 2.3970040e-02 2.6912335e-02
 3.7354702e-01], sum to 1.0000
[2019-04-09 15:17:16,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6064
[2019-04-09 15:17:16,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.3, 46.0, 79.0, 58.0, 19.0, 26.64380650006563, 0.6729216581305854, 0.0, 1.0, 65.0, 44.57199688884297], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2392200.0000, 
sim time next is 2392800.0000, 
raw observation next is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 19.0, 26.63592107992829, 0.6699315875554098, 0.0, 1.0, 60.0, 44.88851166542831], 
processed observation next is [0.0, 0.6956521739130435, 0.45152354570637127, 0.4566666666666667, 0.2227777777777778, 0.056353591160221, 0.08333333333333333, 0.7196600899940243, 0.7233105291851366, 0.0, 1.0, 0.9, 0.4488851166542831], 
reward next is 0.5511, 
noisyNet noise sample is [array([0.79514045], dtype=float32), -0.30234432]. 
=============================================
[2019-04-09 15:17:16,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00145179 0.07290678 0.09326696 0.02978196 0.01183016 0.0016919
 0.28940588 0.01861487 0.03391226 0.04749163 0.3996457 ], sum to 1.0000
[2019-04-09 15:17:16,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6955
[2019-04-09 15:17:16,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.9, 56.0, 0.0, 0.0, 19.0, 25.61410883501397, 0.3952664086049915, 0.0, 1.0, 20.0, 48.38909495341366], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2430600.0000, 
sim time next is 2431200.0000, 
raw observation next is [-8.0, 57.0, 0.0, 0.0, 19.0, 25.59625544398626, 0.3899698235367666, 0.0, 1.0, 65.0, 56.37324689236937], 
processed observation next is [0.0, 0.13043478260869565, 0.24099722991689754, 0.57, 0.0, 0.0, 0.08333333333333333, 0.6330212869988548, 0.6299899411789222, 0.0, 1.0, 1.0, 0.5637324689236938], 
reward next is 0.4363, 
noisyNet noise sample is [array([-1.111411], dtype=float32), 0.067961775]. 
=============================================
[2019-04-09 15:17:16,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00142714 0.1278119  0.15833224 0.02477439 0.00710651 0.00153729
 0.30027384 0.01912611 0.03848671 0.03372365 0.2874002 ], sum to 1.0000
[2019-04-09 15:17:16,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6278
[2019-04-09 15:17:16,793] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 19.0, 25.51576030009755, 0.4121990029559285, 0.0, 1.0, 20.0, 31.84751569702246], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2347200.0000, 
sim time next is 2347800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 19.0, 25.48364891520918, 0.3921327850535603, 0.0, 1.0, 50.0, 30.35988640725158], 
processed observation next is [0.0, 0.17391304347826086, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.623637409600765, 0.6307109283511868, 0.0, 1.0, 0.7, 0.3035988640725158], 
reward next is 0.6964, 
noisyNet noise sample is [array([1.8655577], dtype=float32), 0.7485366]. 
=============================================
[2019-04-09 15:17:16,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00140847 0.11667805 0.14822644 0.03206117 0.00613623 0.00091596
 0.26471463 0.02406119 0.05474226 0.03339873 0.31765693], sum to 1.0000
[2019-04-09 15:17:16,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8414
[2019-04-09 15:17:16,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.9, 65.66666666666667, 127.0, 390.0, 19.0, 25.73926613753375, 0.5112730475267261, 0.0, 1.0, 45.0, 35.60107581451837], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2368200.0000, 
sim time next is 2368800.0000, 
raw observation next is [-2.8, 65.0, 130.0, 405.0, 19.0, 25.72975805403442, 0.5233139410307579, 0.0, 1.0, 65.0, 60.9292551225267], 
processed observation next is [0.0, 0.43478260869565216, 0.38504155124653744, 0.65, 0.43333333333333335, 0.44751381215469616, 0.08333333333333333, 0.6441465045028684, 0.674437980343586, 0.0, 1.0, 1.0, 0.6092925512252669], 
reward next is 0.3907, 
noisyNet noise sample is [array([2.271418], dtype=float32), 0.7095894]. 
=============================================
[2019-04-09 15:17:17,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00157657 0.12992561 0.23501456 0.02467312 0.00614401 0.00124238
 0.31436923 0.02445303 0.02958069 0.03038869 0.20263217], sum to 1.0000
[2019-04-09 15:17:17,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1174
[2019-04-09 15:17:17,769] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 19.0, 25.69483121004547, 0.4771041547657004, 0.0, 1.0, 65.0, 63.13354000480773], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2355000.0000, 
sim time next is 2355600.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 19.0, 25.74756842400264, 0.4794868844897506, 0.0, 1.0, 20.0, 49.49894803663066], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.08333333333333333, 0.64563070200022, 0.6598289614965835, 0.0, 1.0, 0.1, 0.4949894803663066], 
reward next is 0.5050, 
noisyNet noise sample is [array([-0.6125172], dtype=float32), 2.0195093]. 
=============================================
[2019-04-09 15:17:17,853] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-09 15:17:17,863] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:17:17,864] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:17:17,867] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run21
[2019-04-09 15:17:17,909] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:17:17,910] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:17:17,911] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:17:17,910] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:17:17,928] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run21
[2019-04-09 15:17:17,955] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run21
[2019-04-09 15:18:56,448] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.03366499], dtype=float32), 0.03843295]
[2019-04-09 15:18:56,448] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-4.358826001, 53.26371238, 0.0, 0.0, 19.0, 26.47857113418928, 0.6265708348618343, 0.0, 1.0, 25.0, 39.48191372962103]
[2019-04-09 15:18:56,448] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:18:56,450] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.00048712 0.08028915 0.14424257 0.01991181 0.0043047  0.00092492
 0.33969188 0.01593553 0.03154922 0.03093443 0.3317287 ], sampled 0.15434288523856088
[2019-04-09 15:18:57,426] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.03366499], dtype=float32), 0.03843295]
[2019-04-09 15:18:57,426] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-4.0, 30.0, 0.0, 0.0, 19.0, 27.0251129858685, 0.8092190784334172, 0.0, 1.0, 65.0, 37.47137346908941]
[2019-04-09 15:18:57,426] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:18:57,427] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [1.1818434e-04 9.7986959e-02 1.4171165e-01 1.7546017e-02 2.9464078e-03
 3.6084809e-04 3.2732576e-01 8.7909782e-03 3.1520229e-02 1.9283559e-02
 3.5240942e-01], sampled 0.34991859305714035
[2019-04-09 15:19:05,276] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5644.4543 285159.1596 2954.2083
[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,297] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:05,405] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:11,918] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5282.9291 321330.0569 2261.3090
[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:11,938] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:12,045] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,406] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5388.9965 310700.3530 2610.0850
[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,426] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:13,532] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:14,427] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 200000, evaluation results [200000.0, 5388.996469644555, 310700.35303554457, 2610.08496487979, 5644.4542594258255, 285159.15957237454, 2954.2083201081887, 5282.929080250879, 321330.05692616326, 2261.3089591256376]
[2019-04-09 15:19:14,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.92796234e-04 1.15716815e-01 1.82355002e-01 2.30289903e-02
 3.25958827e-03 6.35945064e-04 1.82634592e-01 1.69580504e-02
 4.44765128e-02 2.98182722e-02 4.00823444e-01], sum to 1.0000
[2019-04-09 15:19:14,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9502
[2019-04-09 15:19:14,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9326208e-04 6.9456249e-02 9.9415779e-02 1.2173095e-02 4.0796963e-03
 4.7883682e-04 5.5291438e-01 1.6969584e-02 2.5151387e-02 2.7298378e-02
 1.9156936e-01], sum to 1.0000
[2019-04-09 15:19:14,463] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6345
[2019-04-09 15:19:14,468] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 54.0, 221.5, 212.0, 19.0, 26.15338933390644, 0.6002136248032072, 0.0, 1.0, 25.0, 38.53115420484463], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2379600.0000, 
sim time next is 2380200.0000, 
raw observation next is [-0.5, 53.66666666666666, 211.3333333333333, 141.3333333333333, 19.0, 26.18440226348808, 0.6000200739353799, 0.0, 1.0, 65.0, 50.74535956485665], 
processed observation next is [0.0, 0.5652173913043478, 0.44875346260387816, 0.5366666666666666, 0.7044444444444443, 0.15616942909760584, 0.08333333333333333, 0.6820335219573401, 0.7000066913117933, 0.0, 1.0, 1.0, 0.5074535956485665], 
reward next is 0.4925, 
noisyNet noise sample is [array([2.6532311], dtype=float32), 0.23127007]. 
=============================================
[2019-04-09 15:19:14,480] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 19.0, 26.3513251482501, 0.5739291316642117, 0.0, 1.0, 45.0, 41.41547961543075], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 26.31480270726732, 0.5646030352033188, 0.0, 1.0, 45.0, 39.73791775087729], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.6929002256056099, 0.6882010117344396, 0.0, 1.0, 0.6, 0.3973791775087729], 
reward next is 0.6026, 
noisyNet noise sample is [array([0.08646889], dtype=float32), 0.6953901]. 
=============================================
[2019-04-09 15:19:14,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00045346 0.05695266 0.13493423 0.01418029 0.00214283 0.00052111
 0.31439498 0.01143958 0.01677256 0.02471689 0.4234914 ], sum to 1.0000
[2019-04-09 15:19:14,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5272
[2019-04-09 15:19:14,765] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.3, 25.33333333333333, 41.0, 239.6666666666667, 19.0, 26.92141771761032, 0.6921086624211732, 0.0, 1.0, 25.0, 29.20726172422592], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2479200.0000, 
sim time next is 2479800.0000, 
raw observation next is [3.3, 25.16666666666667, 34.0, 200.3333333333334, 19.0, 26.94312015863641, 0.6929692051276422, 0.0, 1.0, 50.0, 30.02677967993773], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2516666666666667, 0.11333333333333333, 0.2213627992633518, 0.08333333333333333, 0.7452600132197009, 0.7309897350425474, 0.0, 1.0, 0.7, 0.30026779679937726], 
reward next is 0.6997, 
noisyNet noise sample is [array([-1.4823004], dtype=float32), 0.9772133]. 
=============================================
[2019-04-09 15:19:14,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0003654  0.08515171 0.13781445 0.02965576 0.00263893 0.00057479
 0.33655873 0.01436577 0.01936338 0.02550763 0.34800342], sum to 1.0000
[2019-04-09 15:19:14,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9067
[2019-04-09 15:19:14,865] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 19.0, 25.86241817704293, 0.5266647177891427, 0.0, 1.0, 30.0, 24.60654835543407], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2388000.0000, 
sim time next is 2388600.0000, 
raw observation next is [0.0, 47.0, 86.0, 341.0, 19.0, 25.77761196936735, 0.5347930948207992, 0.0, 1.0, 65.0, 51.37151455421456], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.2866666666666667, 0.37679558011049724, 0.08333333333333333, 0.6481343307806124, 0.6782643649402664, 0.0, 1.0, 1.0, 0.5137151455421456], 
reward next is 0.4863, 
noisyNet noise sample is [array([1.6332881], dtype=float32), -0.572126]. 
=============================================
[2019-04-09 15:19:14,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.72214592e-04 1.09559156e-01 1.32265151e-01 1.81422345e-02
 5.71029866e-03 8.64597096e-04 3.96680534e-01 1.75587405e-02
 4.12466824e-02 3.85736935e-02 2.39026695e-01], sum to 1.0000
[2019-04-09 15:19:14,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6304
[2019-04-09 15:19:14,947] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.4, 43.0, 0.0, 0.0, 19.0, 26.34538348171473, 0.5901096989820992, 0.0, 1.0, 45.0, 47.31674272691344], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2401200.0000, 
sim time next is 2401800.0000, 
raw observation next is [-2.566666666666666, 42.83333333333334, 0.0, 0.0, 19.0, 26.34772584247046, 0.5853581346154678, 0.0, 1.0, 45.0, 38.10956230968298], 
processed observation next is [0.0, 0.8260869565217391, 0.39150507848568794, 0.42833333333333345, 0.0, 0.0, 0.08333333333333333, 0.6956438202058717, 0.695119378205156, 0.0, 1.0, 0.6, 0.38109562309682976], 
reward next is 0.6189, 
noisyNet noise sample is [array([0.6162447], dtype=float32), -1.750651]. 
=============================================
[2019-04-09 15:19:14,965] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0008233  0.14374535 0.13833918 0.02658051 0.00648182 0.00218783
 0.3204938  0.02635625 0.02755883 0.02919569 0.27823746], sum to 1.0000
[2019-04-09 15:19:14,971] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3231
[2019-04-09 15:19:14,987] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 19.0, 24.5436773487458, 0.2183102028805218, 0.0, 1.0, 65.0, 75.75743166274124], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2450400.0000, 
sim time next is 2451000.0000, 
raw observation next is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 19.0, 24.57531662511709, 0.2434083145072252, 0.0, 1.0, 45.0, 49.2151791357124], 
processed observation next is [0.0, 0.34782608695652173, 0.25023084025854114, 0.5133333333333334, 0.15666666666666668, 0.5513812154696133, 0.08333333333333333, 0.5479430520930908, 0.5811361048357417, 0.0, 1.0, 0.6, 0.49215179135712395], 
reward next is 0.5078, 
noisyNet noise sample is [array([0.42272004], dtype=float32), 0.256751]. 
=============================================
[2019-04-09 15:19:14,997] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[7.9936376]
 [7.7164717]
 [7.5485845]
 [7.652072 ]
 [7.7948613]], R is [[8.26032829]
 [8.42015076]
 [8.74214745]
 [9.33889484]
 [9.91234398]].
[2019-04-09 15:19:15,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00069308 0.11532249 0.20229399 0.03023394 0.00695217 0.00108639
 0.2586857  0.03664762 0.0465596  0.03977659 0.2617485 ], sum to 1.0000
[2019-04-09 15:19:15,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0970
[2019-04-09 15:19:15,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 40.66666666666667, 73.5, 758.3333333333333, 19.0, 25.32622851616141, 0.4028569977707879, 0.0, 1.0, 45.0, 46.82132427625582], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2456400.0000, 
sim time next is 2457000.0000, 
raw observation next is [-3.95, 39.5, 76.0, 777.0, 19.0, 25.46074895671757, 0.4188073428230912, 0.0, 1.0, 20.0, 44.0367760728028], 
processed observation next is [0.0, 0.43478260869565216, 0.3531855955678671, 0.395, 0.25333333333333335, 0.8585635359116022, 0.08333333333333333, 0.6217290797264642, 0.639602447607697, 0.0, 1.0, 0.1, 0.440367760728028], 
reward next is 0.5596, 
noisyNet noise sample is [array([0.554262], dtype=float32), -0.45189384]. 
=============================================
[2019-04-09 15:19:15,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[8.530177 ]
 [8.3476095]
 [8.356618 ]
 [8.254483 ]
 [8.193154 ]], R is [[ 9.15960598]
 [ 9.59979725]
 [ 9.93983459]
 [10.2688942 ]
 [10.73600483]].
[2019-04-09 15:19:15,193] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00169019 0.07393382 0.17673743 0.03854212 0.00609396 0.00125081
 0.34879032 0.02073703 0.0367284  0.03451829 0.26097763], sum to 1.0000
[2019-04-09 15:19:15,200] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2273
[2019-04-09 15:19:15,226] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 19.0, 25.0662877276912, 0.2954552684849276, 0.0, 1.0, 65.0, 54.83859955686913], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2450400.0000, 
sim time next is 2451000.0000, 
raw observation next is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 19.0, 25.03817267469462, 0.3050754972693192, 0.0, 1.0, 25.0, 50.46293620424758], 
processed observation next is [0.0, 0.34782608695652173, 0.25023084025854114, 0.5133333333333334, 0.15666666666666668, 0.5513812154696133, 0.08333333333333333, 0.5865143895578852, 0.6016918324231064, 0.0, 1.0, 0.2, 0.5046293620424759], 
reward next is 0.4954, 
noisyNet noise sample is [array([0.605009], dtype=float32), -0.5110592]. 
=============================================
[2019-04-09 15:19:15,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[8.379528 ]
 [8.1561985]
 [8.092096 ]
 [8.313621 ]
 [8.091508 ]], R is [[ 8.90701485]
 [ 9.26955891]
 [ 9.78645706]
 [10.27832985]
 [10.67063808]].
[2019-04-09 15:19:16,046] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2320084e-04 1.0722215e-01 1.6437937e-01 2.0613084e-02 2.3927148e-03
 1.2260586e-03 3.1734073e-01 1.4386567e-02 4.1327275e-02 5.0386891e-02
 2.8050202e-01], sum to 1.0000
[2019-04-09 15:19:16,050] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1348
[2019-04-09 15:19:16,067] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.383333333333333, 27.0, 81.0, 800.0, 19.0, 26.54012358041607, 0.6537262127781232, 0.0, 1.0, 65.0, 39.69911454222176], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2470200.0000, 
sim time next is 2470800.0000, 
raw observation next is [2.566666666666667, 27.0, 79.5, 792.0, 19.0, 26.59636376823096, 0.6633265529182376, 0.0, 1.0, 25.0, 36.90313003978153], 
processed observation next is [0.0, 0.6086956521739131, 0.5337026777469991, 0.27, 0.265, 0.8751381215469614, 0.08333333333333333, 0.7163636473525802, 0.7211088509727458, 0.0, 1.0, 0.2, 0.3690313003978153], 
reward next is 0.6310, 
noisyNet noise sample is [array([0.27683112], dtype=float32), -0.25694188]. 
=============================================
[2019-04-09 15:19:16,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00166759 0.12739018 0.15099011 0.02812783 0.00968802 0.00173043
 0.28184548 0.02273001 0.04754408 0.03135933 0.296927  ], sum to 1.0000
[2019-04-09 15:19:16,218] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.1815059e-04 6.3369595e-02 1.5533201e-01 2.0777980e-02 3.9010821e-03
 9.3891745e-04 1.9065674e-01 1.9316876e-02 4.8718706e-02 3.8128484e-02
 4.5844147e-01], sum to 1.0000
[2019-04-09 15:19:16,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8647
[2019-04-09 15:19:16,219] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3406
[2019-04-09 15:19:16,243] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 19.0, 26.78083367964581, 0.7026401129666686, 0.0, 1.0, 65.0, 36.47458903030788], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 19.0, 26.81474776018483, 0.709544896735873, 0.0, 1.0, 45.0, 31.6644523058425], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.08333333333333333, 0.7345623133487358, 0.7365149655786243, 0.0, 1.0, 0.6, 0.316644523058425], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.2874398], dtype=float32), 1.0110031]. 
=============================================
[2019-04-09 15:19:16,246] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.65, 60.5, 0.0, 0.0, 19.0, 24.94130014945545, 0.2530079282499922, 0.0, 1.0, 65.0, 62.70327740657339], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 24.95434034532871, 0.2599919472058057, 0.0, 1.0, 65.0, 61.65477123444765], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.5795283621107258, 0.5866639824019352, 0.0, 1.0, 1.0, 0.6165477123444765], 
reward next is 0.3835, 
noisyNet noise sample is [array([-1.5901434], dtype=float32), -0.30138353]. 
=============================================
[2019-04-09 15:19:16,389] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00052795 0.12774011 0.16333412 0.02299156 0.0020425  0.00068727
 0.27544326 0.01280294 0.04306449 0.02125649 0.3301093 ], sum to 1.0000
[2019-04-09 15:19:16,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4020
[2019-04-09 15:19:16,417] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 42.33333333333333, 0.0, 0.0, 19.0, 25.22099676944164, 0.2917888956897691, 0.0, 1.0, 25.0, 28.15419544880216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2418000.0000, 
sim time next is 2418600.0000, 
raw observation next is [-5.5, 42.66666666666667, 0.0, 0.0, 19.0, 25.06386535335576, 0.2614440515868361, 0.0, 1.0, 45.0, 34.78379014602825], 
processed observation next is [0.0, 1.0, 0.3102493074792244, 0.4266666666666667, 0.0, 0.0, 0.08333333333333333, 0.5886554461129799, 0.587148017195612, 0.0, 1.0, 0.6, 0.34783790146028254], 
reward next is 0.6522, 
noisyNet noise sample is [array([0.47515348], dtype=float32), 0.631296]. 
=============================================
[2019-04-09 15:19:16,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4325646e-04 6.1014410e-02 1.7648520e-01 1.8346550e-02 3.5727900e-03
 4.7342677e-04 2.9509819e-01 9.1456696e-03 2.6417250e-02 2.1876549e-02
 3.8722676e-01], sum to 1.0000
[2019-04-09 15:19:16,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9768
[2019-04-09 15:19:16,752] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.733333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 26.25114031677661, 0.5476056962006534, 0.0, 1.0, 45.0, 40.9463190506906], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [-2.9, 42.5, 0.0, 0.0, 19.0, 26.22635321288875, 0.5445199105965007, 0.0, 1.0, 65.0, 51.27316071789395], 
processed observation next is [0.0, 0.8260869565217391, 0.38227146814404434, 0.425, 0.0, 0.0, 0.08333333333333333, 0.6855294344073958, 0.6815066368655002, 0.0, 1.0, 1.0, 0.5127316071789395], 
reward next is 0.4873, 
noisyNet noise sample is [array([-1.403865], dtype=float32), 0.08126531]. 
=============================================
[2019-04-09 15:19:16,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[9.722564]
 [9.772904]
 [9.901819]
 [9.910908]
 [9.938579]], R is [[10.47732353]
 [10.96308708]
 [11.35953903]
 [11.73752117]
 [12.20740318]].
[2019-04-09 15:19:16,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4165231e-04 1.3996448e-01 1.2234516e-01 2.0955615e-02 3.6226516e-03
 1.5828087e-03 2.7096742e-01 1.9412018e-02 3.1599235e-02 2.2453263e-02
 3.6675572e-01], sum to 1.0000
[2019-04-09 15:19:16,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4722
[2019-04-09 15:19:16,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 19.0, 26.16047405894232, 0.5618805951698196, 0.0, 1.0, 65.0, 44.9288483306743], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2465400.0000, 
sim time next is 2466000.0000, 
raw observation next is [1.6, 28.0, 88.5, 838.5, 19.0, 26.23009494240658, 0.5739397420449069, 0.0, 1.0, 45.0, 40.11862551040657], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.28, 0.295, 0.9265193370165746, 0.08333333333333333, 0.6858412452005483, 0.6913132473483024, 0.0, 1.0, 0.6, 0.4011862551040657], 
reward next is 0.5988, 
noisyNet noise sample is [array([-0.81900334], dtype=float32), 0.35933322]. 
=============================================
[2019-04-09 15:19:16,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00102549 0.11736579 0.13675816 0.04047032 0.00546585 0.0019117
 0.23346949 0.01755354 0.05549124 0.03413341 0.3563551 ], sum to 1.0000
[2019-04-09 15:19:16,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7850
[2019-04-09 15:19:16,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[9.383795]
 [9.640799]
 [9.498809]
 [9.558782]
 [9.406554]], R is [[10.13537788]
 [10.58473587]
 [11.06207752]
 [11.4981308 ]
 [11.95992565]].
[2019-04-09 15:19:16,863] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 54.0, 40.0, 416.0, 19.0, 24.8209928790051, 0.2460305282543161, 0.0, 1.0, 25.0, 41.2322596646747], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2449800.0000, 
sim time next is 2450400.0000, 
raw observation next is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 19.0, 24.84530638139733, 0.2551382769681762, 0.0, 1.0, 65.0, 55.53689771184563], 
processed observation next is [0.0, 0.34782608695652173, 0.24007386888273316, 0.5266666666666666, 0.145, 0.505524861878453, 0.08333333333333333, 0.5704421984497774, 0.5850460923227254, 0.0, 1.0, 1.0, 0.5553689771184563], 
reward next is 0.4446, 
noisyNet noise sample is [array([-0.5361436], dtype=float32), -1.1589243]. 
=============================================
[2019-04-09 15:19:17,214] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00055714 0.09424146 0.19472304 0.01779576 0.00231817 0.00052623
 0.2974     0.01185281 0.04565054 0.02787962 0.30705515], sum to 1.0000
[2019-04-09 15:19:17,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9330
[2019-04-09 15:19:17,231] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 27.0, 70.0, 732.0, 19.0, 26.70548767287974, 0.6802519166795808, 0.0, 1.0, 25.0, 35.37737907159615], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2473200.0000, 
sim time next is 2473800.0000, 
raw observation next is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 19.0, 26.73716830513423, 0.6869992878942918, 0.0, 1.0, 65.0, 36.89639350301812], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2683333333333333, 0.22444444444444442, 0.7896869244935543, 0.08333333333333333, 0.7280973587611858, 0.7289997626314305, 0.0, 1.0, 1.0, 0.36896393503018116], 
reward next is 0.6310, 
noisyNet noise sample is [array([0.25600186], dtype=float32), 2.2707813]. 
=============================================
[2019-04-09 15:19:17,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00040349 0.14494152 0.19300184 0.01763974 0.00228348 0.00142362
 0.21734366 0.02029918 0.04288672 0.04256864 0.31720817], sum to 1.0000
[2019-04-09 15:19:17,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4835
[2019-04-09 15:19:17,389] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 26.16666666666667, 57.33333333333333, 546.3333333333334, 19.0, 26.85516671493547, 0.7038344878233994, 0.0, 1.0, 65.0, 35.75736585339337], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2476200.0000, 
sim time next is 2476800.0000, 
raw observation next is [3.3, 26.0, 55.0, 479.5, 19.0, 26.87852526267006, 0.7037516618135086, 0.0, 1.0, 45.0, 33.63566252201191], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.26, 0.18333333333333332, 0.5298342541436464, 0.08333333333333333, 0.7398771052225049, 0.7345838872711695, 0.0, 1.0, 0.6, 0.3363566252201191], 
reward next is 0.6636, 
noisyNet noise sample is [array([0.7414454], dtype=float32), 0.32494366]. 
=============================================
[2019-04-09 15:19:17,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00137938 0.0895356  0.1149913  0.0224516  0.0054228  0.00139388
 0.3132711  0.01816631 0.0355933  0.02931958 0.36847508], sum to 1.0000
[2019-04-09 15:19:17,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-09 15:19:17,713] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 25.05228764206011, 0.2488237404546676, 0.0, 1.0, 45.0, 35.84179548885889], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2439600.0000, 
sim time next is 2440200.0000, 
raw observation next is [-8.816666666666666, 60.16666666666666, 0.0, 0.0, 19.0, 24.94353537390589, 0.2241874621901788, 0.0, 1.0, 45.0, 34.16972782496629], 
processed observation next is [0.0, 0.21739130434782608, 0.21837488457987075, 0.6016666666666666, 0.0, 0.0, 0.08333333333333333, 0.5786279478254907, 0.5747291540633929, 0.0, 1.0, 0.6, 0.3416972782496629], 
reward next is 0.6583, 
noisyNet noise sample is [array([-1.4446324], dtype=float32), 0.6697362]. 
=============================================
[2019-04-09 15:19:17,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00044992 0.12395281 0.15173486 0.02870401 0.00311129 0.00072954
 0.2792363  0.01779483 0.02749672 0.02669661 0.34009314], sum to 1.0000
[2019-04-09 15:19:17,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4425
[2019-04-09 15:19:17,919] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 44.66666666666667, 0.0, 0.0, 19.0, 25.68940074779385, 0.4346414377070604, 0.0, 1.0, 65.0, 64.07132476106818], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2420400.0000, 
sim time next is 2421000.0000, 
raw observation next is [-5.9, 45.5, 0.0, 0.0, 19.0, 25.70402344769466, 0.4347755276080696, 0.0, 1.0, 45.0, 50.94014846816344], 
processed observation next is [0.0, 0.0, 0.2991689750692521, 0.455, 0.0, 0.0, 0.08333333333333333, 0.642001953974555, 0.6449251758693565, 0.0, 1.0, 0.6, 0.5094014846816344], 
reward next is 0.4906, 
noisyNet noise sample is [array([1.1574174], dtype=float32), -0.9127633]. 
=============================================
[2019-04-09 15:19:17,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[9.366629 ]
 [9.50662  ]
 [9.7828245]
 [9.767253 ]
 [9.894487 ]], R is [[ 9.46666622]
 [ 9.73128605]
 [10.26876068]
 [10.78293896]
 [11.29183674]].
[2019-04-09 15:19:18,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00098943 0.1068516  0.22775026 0.03344366 0.00522175 0.00179993
 0.26373878 0.02207148 0.04358896 0.05028999 0.24425416], sum to 1.0000
[2019-04-09 15:19:18,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4852
[2019-04-09 15:19:18,400] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 19.0, 24.58220529311563, 0.1765711889936943, 0.0, 1.0, 55.0, 42.54732289362426], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [-7.3, 50.0, 50.5, 540.5, 19.0, 24.53486482777289, 0.1771947228209735, 0.0, 1.0, 45.0, 37.04404310650912], 
processed observation next is [0.0, 0.391304347826087, 0.26038781163434904, 0.5, 0.16833333333333333, 0.5972375690607735, 0.08333333333333333, 0.5445720689810741, 0.5590649076069912, 0.0, 1.0, 0.6, 0.3704404310650912], 
reward next is 0.6296, 
noisyNet noise sample is [array([0.5727881], dtype=float32), -0.8260036]. 
=============================================
[2019-04-09 15:19:19,043] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0011751  0.08752004 0.10762448 0.03053397 0.00721957 0.00230038
 0.3016217  0.04723942 0.05661102 0.08462162 0.27353272], sum to 1.0000
[2019-04-09 15:19:19,043] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4672
[2019-04-09 15:19:19,066] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.65, 60.5, 0.0, 0.0, 19.0, 24.93967829043049, 0.2662031439190902, 0.0, 1.0, 25.0, 40.36873371190827], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 24.93954910961824, 0.2507212161574456, 0.0, 1.0, 45.0, 40.29162197730986], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.5782957591348534, 0.5835737387191485, 0.0, 1.0, 0.6, 0.4029162197730986], 
reward next is 0.5971, 
noisyNet noise sample is [array([1.629119], dtype=float32), 0.030187676]. 
=============================================
[2019-04-09 15:19:19,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00105681 0.10184985 0.1065577  0.03674459 0.00769956 0.00248588
 0.29203513 0.04703584 0.06374926 0.08539081 0.25539458], sum to 1.0000
[2019-04-09 15:19:19,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8653
[2019-04-09 15:19:19,119] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.816666666666666, 60.16666666666666, 0.0, 0.0, 19.0, 24.90350236181281, 0.2340610363333084, 0.0, 1.0, 45.0, 38.30946030765621], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2440200.0000, 
sim time next is 2440800.0000, 
raw observation next is [-8.9, 60.0, 0.0, 0.0, 19.0, 24.85408467718432, 0.2350004434901266, 0.0, 1.0, 65.0, 65.69058237092592], 
processed observation next is [0.0, 0.2608695652173913, 0.21606648199445982, 0.6, 0.0, 0.0, 0.08333333333333333, 0.5711737230986934, 0.5783334811633756, 0.0, 1.0, 1.0, 0.6569058237092592], 
reward next is 0.3431, 
noisyNet noise sample is [array([1.629119], dtype=float32), 0.030187676]. 
=============================================
[2019-04-09 15:19:19,123] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00049938 0.07053234 0.15325041 0.02154867 0.001076   0.0004144
 0.4133897  0.01667178 0.0289953  0.02546294 0.2681591 ], sum to 1.0000
[2019-04-09 15:19:19,124] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5044
[2019-04-09 15:19:19,137] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 26.41624584113411, 0.5261018705322712, 0.0, 1.0, 45.0, 38.14556802561085], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2522400.0000, 
sim time next is 2523000.0000, 
raw observation next is [-2.2, 55.66666666666666, 0.0, 0.0, 19.0, 26.41507767716733, 0.5189514307212075, 0.0, 1.0, 45.0, 37.01161227967766], 
processed observation next is [1.0, 0.17391304347826086, 0.4016620498614959, 0.5566666666666665, 0.0, 0.0, 0.08333333333333333, 0.7012564730972773, 0.6729838102404025, 0.0, 1.0, 0.6, 0.3701161227967766], 
reward next is 0.6299, 
noisyNet noise sample is [array([-0.62870026], dtype=float32), 1.4531392]. 
=============================================
[2019-04-09 15:19:19,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[10.480066]
 [10.600096]
 [10.772011]
 [10.59316 ]
 [10.544371]], R is [[11.18723202]
 [11.69390392]
 [12.19448662]
 [12.66186523]
 [13.06914425]].
[2019-04-09 15:19:19,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0009074  0.16080916 0.1405883  0.03217369 0.00629291 0.00173294
 0.29417068 0.01887777 0.05540222 0.02537229 0.26367265], sum to 1.0000
[2019-04-09 15:19:19,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9596
[2019-04-09 15:19:19,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.99651928e-04 4.75279279e-02 1.01507194e-01 1.35051562e-02
 1.91446277e-03 4.63753997e-04 4.83055472e-01 5.65738976e-03
 3.02223582e-02 2.49229465e-02 2.91023701e-01], sum to 1.0000
[2019-04-09 15:19:19,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4143
[2019-04-09 15:19:19,443] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 59.5, 0.0, 0.0, 19.0, 24.11906685827401, 0.08580700245544466, 0.0, 1.0, 65.0, 69.5126176074061], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2446200.0000, 
sim time next is 2446800.0000, 
raw observation next is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 19.0, 24.09132555078682, 0.1346928884307756, 0.0, 1.0, 65.0, 69.52668171538173], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.59, 0.030555555555555548, 0.11344383057090243, 0.08333333333333333, 0.5076104625655683, 0.5448976294769252, 0.0, 1.0, 1.0, 0.6952668171538172], 
reward next is 0.3047, 
noisyNet noise sample is [array([-0.02264193], dtype=float32), 1.0128572]. 
=============================================
[2019-04-09 15:19:19,454] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 19.0, 26.41130798414865, 0.52271899474897, 0.0, 1.0, 45.0, 35.21687032114666], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2512800.0000, 
sim time next is 2513400.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.34914339764295, 0.5198062281684709, 0.0, 1.0, 45.0, 32.04563774851794], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.39, 0.0, 0.0, 0.08333333333333333, 0.6957619498035793, 0.6732687427228236, 0.0, 1.0, 0.6, 0.3204563774851794], 
reward next is 0.6795, 
noisyNet noise sample is [array([0.43146855], dtype=float32), 0.20218076]. 
=============================================
[2019-04-09 15:19:19,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00040615 0.09605289 0.16003981 0.0146596  0.00599499 0.00067829
 0.27655235 0.02491694 0.04926386 0.03270403 0.33873117], sum to 1.0000
[2019-04-09 15:19:19,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0936
[2019-04-09 15:19:19,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1221667e-05 1.0489656e-01 2.2193095e-01 4.1448181e-03 4.0331436e-04
 3.1485026e-05 3.7334347e-01 2.4194699e-03 2.0670969e-02 2.2388689e-02
 2.4974900e-01], sum to 1.0000
[2019-04-09 15:19:19,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1304
[2019-04-09 15:19:19,509] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.55, 29.0, 0.0, 0.0, 19.0, 26.69161810329936, 0.62250285415, 0.0, 1.0, 25.0, 32.76295130022243], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [0.3666666666666668, 29.33333333333334, 0.0, 0.0, 19.0, 26.64572313107158, 0.6092299837859357, 0.0, 1.0, 20.0, 30.60845334102231], 
processed observation next is [0.0, 0.782608695652174, 0.4727608494921515, 0.2933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7204769275892984, 0.7030766612619787, 0.0, 1.0, 0.1, 0.3060845334102231], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.05850419], dtype=float32), 0.9783169]. 
=============================================
[2019-04-09 15:19:19,510] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.7833333333333333, 37.33333333333334, 0.0, 0.0, 22.5, 27.19715396291829, 0.8318517792395195, 1.0, 1.0, 45.0, 28.42219629441469], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2574600.0000, 
sim time next is 2575200.0000, 
raw observation next is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 22.5, 27.20132620009833, 0.8184077935661568, 0.0, 1.0, 25.0, 30.20005651170938], 
processed observation next is [1.0, 0.8260869565217391, 0.43582640812557716, 0.3866666666666667, 0.0, 0.0, 0.375, 0.7667771833415274, 0.7728025978553856, 0.0, 1.0, 0.2, 0.30200056511709383], 
reward next is 0.6980, 
noisyNet noise sample is [array([-0.13745777], dtype=float32), -2.1481855]. 
=============================================
[2019-04-09 15:19:19,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00150545 0.09429077 0.0806745  0.05342776 0.00822722 0.00228202
 0.31360042 0.01984554 0.0563747  0.04727677 0.3224948 ], sum to 1.0000
[2019-04-09 15:19:19,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8191
[2019-04-09 15:19:19,685] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2403995e-04 5.4776374e-02 1.6575947e-01 1.5722696e-02 4.6908059e-03
 1.0297884e-03 3.2856885e-01 1.1949325e-02 3.7412327e-02 3.6516737e-02
 3.4324959e-01], sum to 1.0000
[2019-04-09 15:19:19,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9753
[2019-04-09 15:19:19,690] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 19.0, 24.79747713737814, 0.2379320889386655, 0.0, 1.0, 20.0, 39.84617120462988], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2450400.0000, 
sim time next is 2451000.0000, 
raw observation next is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 19.0, 24.78098329271888, 0.2532311178678597, 0.0, 1.0, 65.0, 55.52687169604992], 
processed observation next is [0.0, 0.34782608695652173, 0.25023084025854114, 0.5133333333333334, 0.15666666666666668, 0.5513812154696133, 0.08333333333333333, 0.5650819410599066, 0.5844103726226199, 0.0, 1.0, 1.0, 0.5552687169604993], 
reward next is 0.4447, 
noisyNet noise sample is [array([-0.8200256], dtype=float32), 1.2643898]. 
=============================================
[2019-04-09 15:19:19,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[7.75503  ]
 [7.902757 ]
 [7.8452725]
 [8.045282 ]
 [8.115819 ]], R is [[ 8.34250832]
 [ 8.86062241]
 [ 9.3693037 ]
 [ 9.78489113]
 [10.05469131]].
[2019-04-09 15:19:19,699] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.933333333333334, 25.5, 20.0, 121.6666666666667, 19.0, 26.50480493497012, 0.5799144397568335, 0.0, 1.0, 20.0, 19.25222183048136], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2481000.0000, 
sim time next is 2481600.0000, 
raw observation next is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 19.0, 26.39578075967629, 0.5717986302622635, 0.0, 1.0, 65.0, 47.99649809309961], 
processed observation next is [0.0, 0.7391304347826086, 0.5337026777469991, 0.26, 0.043333333333333335, 0.09097605893186003, 0.08333333333333333, 0.6996483966396907, 0.6905995434207545, 0.0, 1.0, 1.0, 0.4799649809309961], 
reward next is 0.5200, 
noisyNet noise sample is [array([0.5283639], dtype=float32), 0.38793957]. 
=============================================
[2019-04-09 15:19:19,909] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8829582e-04 4.0707961e-02 1.8160540e-01 1.6832152e-02 2.9565375e-03
 3.7062605e-04 3.8012001e-01 2.5536159e-02 1.8778494e-02 3.6001492e-02
 2.9680285e-01], sum to 1.0000
[2019-04-09 15:19:19,912] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5974
[2019-04-09 15:19:19,927] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.63462757206403, 0.5691804659418179, 0.0, 1.0, 65.0, 38.8290258987961], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2507400.0000, 
sim time next is 2508000.0000, 
raw observation next is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 26.68730183957449, 0.5672718488578813, 0.0, 1.0, 45.0, 44.91342494179946], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3933333333333333, 0.0, 0.0, 0.08333333333333333, 0.723941819964541, 0.6890906162859604, 0.0, 1.0, 0.6, 0.4491342494179946], 
reward next is 0.5509, 
noisyNet noise sample is [array([-1.675858], dtype=float32), -1.30166]. 
=============================================
[2019-04-09 15:19:19,932] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.290543]
 [ 9.969474]
 [10.174038]
 [10.231149]
 [10.137079]], R is [[10.66623688]
 [11.17128468]
 [11.64572906]
 [12.13235378]
 [12.60566425]].
[2019-04-09 15:19:19,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00047311 0.09342868 0.13858289 0.02007563 0.00398199 0.0008201
 0.32215184 0.01916687 0.02421573 0.03764782 0.3394554 ], sum to 1.0000
[2019-04-09 15:19:19,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1780
[2019-04-09 15:19:19,964] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 25.75850830505688, 0.4563086362051715, 0.0, 1.0, 45.0, 45.06102175986778], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2460000.0000, 
sim time next is 2460600.0000, 
raw observation next is [-1.45, 33.5, 86.0, 829.0, 19.0, 25.80741075666954, 0.4673428348154642, 0.0, 1.0, 25.0, 42.33470143038873], 
processed observation next is [0.0, 0.4782608695652174, 0.422437673130194, 0.335, 0.2866666666666667, 0.9160220994475138, 0.08333333333333333, 0.650617563055795, 0.655780944938488, 0.0, 1.0, 0.2, 0.4233470143038873], 
reward next is 0.5767, 
noisyNet noise sample is [array([0.96553516], dtype=float32), 0.7523641]. 
=============================================
[2019-04-09 15:19:20,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7776073e-04 4.3543305e-02 1.4109017e-01 1.1612099e-02 2.6381419e-03
 4.5786620e-04 2.7918130e-01 1.0848587e-02 1.9927006e-02 1.7681422e-02
 4.7274232e-01], sum to 1.0000
[2019-04-09 15:19:20,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8192
[2019-04-09 15:19:20,168] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 33.66666666666666, 0.0, 0.0, 19.0, 26.51381823765053, 0.5713200265509265, 0.0, 1.0, 45.0, 46.46883089277236], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2497800.0000, 
sim time next is 2498400.0000, 
raw observation next is [-1.2, 33.0, 0.0, 0.0, 19.0, 26.50239544167858, 0.5712450779691751, 0.0, 1.0, 65.0, 47.73701111197482], 
processed observation next is [0.0, 0.9565217391304348, 0.42936288088642666, 0.33, 0.0, 0.0, 0.08333333333333333, 0.708532953473215, 0.6904150259897252, 0.0, 1.0, 1.0, 0.47737011111974825], 
reward next is 0.5226, 
noisyNet noise sample is [array([0.6254404], dtype=float32), -0.5302893]. 
=============================================
[2019-04-09 15:19:20,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00062678 0.09428836 0.15112415 0.02486098 0.00771696 0.00155661
 0.3735798  0.02017278 0.04263161 0.03258854 0.25085345], sum to 1.0000
[2019-04-09 15:19:20,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8474
[2019-04-09 15:19:20,272] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 19.0, 26.26535368869666, 0.5200401211085143, 0.0, 1.0, 35.0, 26.72378348753415], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 19.0, 26.17364040072383, 0.5169925063748643, 0.0, 1.0, 65.0, 54.84976962891186], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.08333333333333333, 0.6811367000603191, 0.6723308354582881, 0.0, 1.0, 1.0, 0.5484976962891186], 
reward next is 0.4515, 
noisyNet noise sample is [array([0.7830026], dtype=float32), 0.4662678]. 
=============================================
[2019-04-09 15:19:20,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00045171 0.0861085  0.09001359 0.02293234 0.00296891 0.00073116
 0.3986383  0.01485259 0.04199688 0.02534894 0.31595707], sum to 1.0000
[2019-04-09 15:19:20,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3278
[2019-04-09 15:19:20,302] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.45966105359378, 0.5541434293984663, 0.0, 1.0, 65.0, 45.84378029687922], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2513400.0000, 
sim time next is 2514000.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 19.0, 26.53221281362751, 0.542234000308555, 0.0, 1.0, 45.0, 43.67095554919543], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7110177344689591, 0.6807446667695184, 0.0, 1.0, 0.6, 0.43670955549195434], 
reward next is 0.5633, 
noisyNet noise sample is [array([0.17627196], dtype=float32), 0.343949]. 
=============================================
[2019-04-09 15:19:20,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.572737]
 [10.651098]
 [10.52252 ]
 [10.73023 ]
 [10.431728]], R is [[11.13901043]
 [11.5691824 ]
 [11.98272228]
 [12.41298389]
 [12.93437386]].
[2019-04-09 15:19:20,389] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4014442e-05 1.0943325e-01 1.0804882e-01 1.6457124e-02 1.0195627e-03
 5.9344613e-05 1.7455640e-01 8.4868725e-03 2.6721690e-02 1.0062669e-02
 5.4513025e-01], sum to 1.0000
[2019-04-09 15:19:20,389] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3749
[2019-04-09 15:19:20,405] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.466666666666667, 36.0, 220.3333333333333, 30.16666666666666, 22.5, 27.6127162391746, 0.7882355958169575, 1.0, 1.0, 45.0, 23.95612742681278], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2550000.0000, 
sim time next is 2550600.0000, 
raw observation next is [1.65, 34.5, 218.0, 22.0, 22.5, 27.60905912080964, 0.7852674590868158, 1.0, 1.0, 45.0, 24.52041339340334], 
processed observation next is [1.0, 0.5217391304347826, 0.5083102493074793, 0.345, 0.7266666666666667, 0.02430939226519337, 0.375, 0.8007549267341366, 0.7617558196956052, 1.0, 1.0, 0.6, 0.24520413393403342], 
reward next is 0.7548, 
noisyNet noise sample is [array([0.24121886], dtype=float32), -0.018875508]. 
=============================================
[2019-04-09 15:19:20,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7905920e-05 6.2774919e-02 7.2393745e-02 1.6745994e-02 5.1862479e-04
 1.5238157e-04 2.6525649e-01 6.4296257e-03 1.6193463e-02 7.7293962e-03
 5.5177748e-01], sum to 1.0000
[2019-04-09 15:19:20,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2505
[2019-04-09 15:19:20,523] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 22.5, 27.88189729679584, 0.9073012183977472, 1.0, 1.0, 45.0, 21.09288029880765], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2559000.0000, 
sim time next is 2559600.0000, 
raw observation next is [3.3, 29.0, 136.5, 313.0, 22.5, 27.84957477298212, 0.9144706423215249, 1.0, 1.0, 45.0, 21.51116901581934], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.455, 0.34585635359116024, 0.375, 0.82079789774851, 0.8048235474405083, 1.0, 1.0, 0.6, 0.21511169015819342], 
reward next is 0.7849, 
noisyNet noise sample is [array([-0.7775362], dtype=float32), -0.80995584]. 
=============================================
[2019-04-09 15:19:20,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4219222e-04 6.7584626e-02 1.5998845e-01 1.3740560e-02 3.1526238e-03
 4.5625234e-04 3.1074005e-01 1.4748348e-02 3.0111192e-02 2.8302727e-02
 3.7083295e-01], sum to 1.0000
[2019-04-09 15:19:20,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3908
[2019-04-09 15:19:20,949] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.4142972396586, 0.602980265281958, 0.0, 1.0, 65.0, 50.93114173001073], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2602800.0000, 
sim time next is 2603400.0000, 
raw observation next is [-5.100000000000001, 74.66666666666667, 0.0, 0.0, 19.0, 26.41107036846165, 0.5999453033509855, 0.0, 1.0, 45.0, 45.94720586595313], 
processed observation next is [1.0, 0.13043478260869565, 0.32132963988919666, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7009225307051375, 0.6999817677836618, 0.0, 1.0, 0.6, 0.4594720586595313], 
reward next is 0.5405, 
noisyNet noise sample is [array([-1.1745543], dtype=float32), 0.99968547]. 
=============================================
[2019-04-09 15:19:21,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1261095e-04 5.3938568e-02 9.1818810e-02 1.3287535e-02 1.7896193e-03
 3.6009194e-04 3.8455644e-01 6.3305087e-03 3.7230626e-02 2.8738482e-02
 3.8173670e-01], sum to 1.0000
[2019-04-09 15:19:21,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-09 15:19:21,044] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.2, 75.33333333333334, 0.0, 0.0, 19.0, 26.28565552006649, 0.5595414285755104, 0.0, 1.0, 45.0, 34.45185649451193], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2604000.0000, 
sim time next is 2604600.0000, 
raw observation next is [-5.3, 76.0, 0.0, 0.0, 19.0, 26.18589733379483, 0.5295745448311912, 0.0, 1.0, 25.0, 32.85030699380223], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6821581111495693, 0.6765248482770637, 0.0, 1.0, 0.2, 0.32850306993802225], 
reward next is 0.6715, 
noisyNet noise sample is [array([0.3334439], dtype=float32), -1.4061507]. 
=============================================
[2019-04-09 15:19:21,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00053237 0.05909665 0.21344173 0.01103931 0.00340277 0.0008118
 0.3592394  0.01204194 0.0389764  0.02464766 0.27676997], sum to 1.0000
[2019-04-09 15:19:21,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4811
[2019-04-09 15:19:21,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2616943e-05 3.8037609e-02 3.0168080e-01 5.2169035e-03 1.0166557e-03
 5.9450998e-05 3.2780278e-01 3.1830615e-03 1.8347234e-02 1.5742382e-02
 2.8890046e-01], sum to 1.0000
[2019-04-09 15:19:21,162] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 19.0, 26.252275132908, 0.5048787811623937, 0.0, 1.0, 65.0, 58.642280419012], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.2196960827649, 0.5149135285084973, 0.0, 1.0, 45.0, 46.93004752987375], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.08333333333333333, 0.6849746735637418, 0.6716378428361658, 0.0, 1.0, 0.6, 0.4693004752987375], 
reward next is 0.5307, 
noisyNet noise sample is [array([-1.1051646], dtype=float32), -2.2374184]. 
=============================================
[2019-04-09 15:19:21,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0488
[2019-04-09 15:19:21,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[9.718488]
 [9.608326]
 [9.794976]
 [9.744725]
 [9.68322 ]], R is [[10.12147141]
 [10.43383408]
 [10.98025036]
 [11.49521542]
 [11.99036407]].
[2019-04-09 15:19:21,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00057116 0.11773358 0.20096636 0.01937405 0.00485417 0.00086369
 0.32405734 0.01113616 0.05430821 0.02462614 0.24150917], sum to 1.0000
[2019-04-09 15:19:21,179] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9967
[2019-04-09 15:19:21,180] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 22.5, 27.47608178766372, 0.8629203891902727, 1.0, 1.0, 45.0, 26.56169157808327], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2571600.0000, 
sim time next is 2572200.0000, 
raw observation next is [-0.04999999999999999, 35.5, 0.0, 0.0, 22.5, 27.38270572490021, 0.8473339081262528, 1.0, 1.0, 25.0, 29.97878498899255], 
processed observation next is [1.0, 0.782608695652174, 0.461218836565097, 0.355, 0.0, 0.0, 0.375, 0.781892143741684, 0.7824446360420843, 1.0, 1.0, 0.2, 0.2997878498899255], 
reward next is 0.7002, 
noisyNet noise sample is [array([-0.48137408], dtype=float32), 0.42803723]. 
=============================================
[2019-04-09 15:19:21,193] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 19.0, 26.51628071454434, 0.5816916242971456, 0.0, 1.0, 45.0, 44.40863349088381], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 19.0, 26.53336570963159, 0.5879083164158405, 0.0, 1.0, 65.0, 45.37290185590373], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.08333333333333333, 0.7111138091359658, 0.6959694388052803, 0.0, 1.0, 1.0, 0.45372901855903724], 
reward next is 0.5463, 
noisyNet noise sample is [array([0.12659661], dtype=float32), -0.5448275]. 
=============================================
[2019-04-09 15:19:21,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9849846e-04 7.5244710e-02 1.7959203e-01 8.6654937e-03 1.6350267e-03
 5.3658261e-04 5.0279683e-01 1.1719245e-02 2.4354821e-02 2.3193514e-02
 1.7206329e-01], sum to 1.0000
[2019-04-09 15:19:21,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6870
[2019-04-09 15:19:21,460] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 19.0, 26.08676737070879, 0.4736506433820487, 0.0, 1.0, 65.0, 60.23471346604254], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.04557956463244, 0.4948194928927094, 0.0, 1.0, 45.0, 43.67787463570536], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.08333333333333333, 0.67046496371937, 0.6649398309642365, 0.0, 1.0, 0.6, 0.4367787463570536], 
reward next is 0.5632, 
noisyNet noise sample is [array([-0.46138278], dtype=float32), 0.23013347]. 
=============================================
[2019-04-09 15:19:21,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[10.487963]
 [10.290287]
 [10.219177]
 [10.655008]
 [10.407597]], R is [[11.0400877 ]
 [11.32733917]
 [11.88881493]
 [12.42698288]
 [12.95377922]].
[2019-04-09 15:19:21,532] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9393551e-04 9.7018726e-02 1.1693090e-01 1.0217920e-02 4.4443365e-03
 3.9652249e-04 3.3761340e-01 8.1327409e-03 1.7981276e-02 1.2610365e-02
 3.9425990e-01], sum to 1.0000
[2019-04-09 15:19:21,536] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2023
[2019-04-09 15:19:21,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.17389745e-05 6.87381476e-02 2.02107713e-01 6.39122957e-03
 5.64362446e-04 2.66755069e-05 2.99621224e-01 3.98978451e-03
 1.18630333e-02 7.72360340e-03 3.98962468e-01], sum to 1.0000
[2019-04-09 15:19:21,540] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2835
[2019-04-09 15:19:21,555] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.7833333333333333, 35.5, 0.0, 0.0, 19.0, 26.48158301650101, 0.5587962010379476, 0.0, 1.0, 45.0, 37.22704837812564], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2502600.0000, 
sim time next is 2503200.0000, 
raw observation next is [-0.9666666666666667, 36.0, 0.0, 0.0, 19.0, 26.46240827650675, 0.5510931938229889, 0.0, 1.0, 25.0, 35.88917038135637], 
processed observation next is [0.0, 1.0, 0.43582640812557716, 0.36, 0.0, 0.0, 0.08333333333333333, 0.7052006897088958, 0.6836977312743296, 0.0, 1.0, 0.2, 0.3588917038135637], 
reward next is 0.6411, 
noisyNet noise sample is [array([-1.7031364], dtype=float32), -0.98433864]. 
=============================================
[2019-04-09 15:19:21,563] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 22.5, 27.42144935493277, 0.8350734810570436, 1.0, 1.0, 65.0, 21.80919898140626], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 22.5, 27.63459308416269, 0.8622365431135558, 1.0, 1.0, 45.0, 21.47254258790524], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.375, 0.8028827570135576, 0.7874121810378519, 1.0, 1.0, 0.6, 0.2147254258790524], 
reward next is 0.7853, 
noisyNet noise sample is [array([-1.4810262], dtype=float32), 1.1462783]. 
=============================================
[2019-04-09 15:19:21,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2345643e-04 9.4118819e-02 1.4584422e-01 1.5288805e-02 1.1085719e-03
 1.4606511e-04 2.9017839e-01 7.4165221e-03 2.1658663e-02 1.8231567e-02
 4.0588489e-01], sum to 1.0000
[2019-04-09 15:19:21,932] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-09 15:19:21,947] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.299999999999999, 79.0, 30.33333333333333, 5.333333333333334, 22.5, 25.49212653053435, 0.3884309797599546, 1.0, 1.0, 65.0, 60.82479020138068], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2620200.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 42.0, 4.0, 22.5, 25.39076777526744, 0.4092899256912115, 1.0, 1.0, 65.0, 77.2282761366375], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.14, 0.004419889502762431, 0.375, 0.6158973146056201, 0.6364299752304038, 1.0, 1.0, 1.0, 0.7722827613663751], 
reward next is 0.2277, 
noisyNet noise sample is [array([-0.87840813], dtype=float32), 0.25885323]. 
=============================================
[2019-04-09 15:19:22,242] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4292652e-04 6.4955644e-02 8.7115094e-02 9.5808776e-03 1.7146887e-03
 4.4935470e-04 4.5585880e-01 1.0226651e-02 1.7111178e-02 2.4486303e-02
 3.2825840e-01], sum to 1.0000
[2019-04-09 15:19:22,248] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8965
[2019-04-09 15:19:22,265] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 19.0, 26.26731167208361, 0.4708815976011697, 0.0, 1.0, 45.0, 32.49611845954158], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2527200.0000, 
sim time next is 2527800.0000, 
raw observation next is [-2.383333333333333, 56.5, 0.0, 0.0, 19.0, 26.17795145764153, 0.4728415608655328, 0.0, 1.0, 65.0, 60.29105259945117], 
processed observation next is [1.0, 0.2608695652173913, 0.3965835641735919, 0.565, 0.0, 0.0, 0.08333333333333333, 0.6814959548034608, 0.6576138536218442, 0.0, 1.0, 1.0, 0.6029105259945117], 
reward next is 0.3971, 
noisyNet noise sample is [array([0.5213247], dtype=float32), -0.5796269]. 
=============================================
[2019-04-09 15:19:22,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.56170775e-05 1.12910345e-01 9.31699425e-02 1.32866232e-02
 7.09297659e-04 9.09470327e-05 3.73581022e-01 4.80834162e-03
 2.37299073e-02 2.19904687e-02 3.55647504e-01], sum to 1.0000
[2019-04-09 15:19:22,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2576
[2019-04-09 15:19:22,917] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 22.5, 27.23410175930562, 0.6599712559520723, 1.0, 1.0, 45.0, 27.65323115276708], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2541600.0000, 
sim time next is 2542200.0000, 
raw observation next is [-1.1, 48.66666666666667, 134.0, 41.0, 22.5, 27.27048455336127, 0.6632093881920332, 1.0, 1.0, 45.0, 27.1365677216504], 
processed observation next is [1.0, 0.43478260869565216, 0.4321329639889197, 0.4866666666666667, 0.44666666666666666, 0.045303867403314914, 0.375, 0.7725403794467726, 0.7210697960640111, 1.0, 1.0, 0.6, 0.271365677216504], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.1110371], dtype=float32), 0.8626073]. 
=============================================
[2019-04-09 15:19:22,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2376625e-05 7.3829822e-02 1.3948379e-01 1.3004809e-02 2.3827181e-04
 7.2778450e-05 2.6868877e-01 5.6420779e-03 1.7820345e-02 1.0840857e-02
 4.7033599e-01], sum to 1.0000
[2019-04-09 15:19:22,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-09 15:19:22,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2318719e-05 6.7104548e-02 1.3706437e-01 1.6075192e-02 1.0622643e-03
 3.8569648e-05 3.6488608e-01 9.4241416e-03 2.2987861e-02 2.9257627e-02
 3.5205707e-01], sum to 1.0000
[2019-04-09 15:19:22,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4152
[2019-04-09 15:19:22,959] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.066666666666667, 48.0, 0.0, 0.0, 19.0, 26.91914085005332, 0.785752401178662, 0.0, 1.0, 45.0, 35.1941459634977], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2578800.0000, 
sim time next is 2579400.0000, 
raw observation next is [-2.25, 50.0, 0.0, 0.0, 19.0, 26.8883900850973, 0.7824918608575052, 0.0, 1.0, 45.0, 33.31453102086977], 
processed observation next is [1.0, 0.8695652173913043, 0.40027700831024937, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7406991737581082, 0.760830620285835, 0.0, 1.0, 0.6, 0.3331453102086977], 
reward next is 0.6669, 
noisyNet noise sample is [array([1.8177407], dtype=float32), -1.132503]. 
=============================================
[2019-04-09 15:19:22,962] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 22.5, 27.35864602347611, 0.7007969878109542, 1.0, 1.0, 45.0, 26.57845375568228], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [-0.6, 47.0, 182.5, 58.0, 22.5, 27.35004684821349, 0.7138015413325025, 1.0, 1.0, 65.0, 32.19595514912338], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.47, 0.6083333333333333, 0.06408839779005525, 0.375, 0.7791705706844576, 0.7379338471108342, 1.0, 1.0, 1.0, 0.3219595514912338], 
reward next is 0.6780, 
noisyNet noise sample is [array([0.9025853], dtype=float32), 0.8299831]. 
=============================================
[2019-04-09 15:19:23,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.19733083e-05 6.51210099e-02 1.46603420e-01 1.26348436e-02
 6.54569885e-04 1.02628626e-04 2.83087194e-01 8.29290971e-03
 1.43677993e-02 1.51968012e-02 4.53916758e-01], sum to 1.0000
[2019-04-09 15:19:23,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3207
[2019-04-09 15:19:23,075] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 53.66666666666667, 121.8333333333333, 30.5, 22.5, 27.02382766713971, 0.6277209082164914, 1.0, 1.0, 55.0, 28.74326637100713], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2539200.0000, 
sim time next is 2539800.0000, 
raw observation next is [-2.0, 52.5, 136.0, 33.0, 22.5, 27.09509846068909, 0.6442881849489445, 1.0, 1.0, 45.0, 28.30638001478331], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.525, 0.4533333333333333, 0.036464088397790057, 0.375, 0.7579248717240908, 0.7147627283163148, 1.0, 1.0, 0.6, 0.2830638001478331], 
reward next is 0.7169, 
noisyNet noise sample is [array([-1.6606787], dtype=float32), 0.8285406]. 
=============================================
[2019-04-09 15:19:23,492] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1913726e-05 5.8725592e-02 1.6959891e-01 1.3999779e-02 1.7680475e-03
 4.5391040e-05 3.9700118e-01 7.5952793e-03 3.0423086e-02 2.2039497e-02
 2.9879129e-01], sum to 1.0000
[2019-04-09 15:19:23,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8401
[2019-04-09 15:19:23,504] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 39.0, 225.0, 46.5, 22.5, 27.55883374197642, 0.7825642666804207, 1.0, 1.0, 20.0, 25.54979908038907], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2548800.0000, 
sim time next is 2549400.0000, 
raw observation next is [1.283333333333333, 37.5, 222.6666666666667, 38.33333333333333, 22.5, 27.60384336894415, 0.7911459289406163, 1.0, 1.0, 65.0, 27.01624699861352], 
processed observation next is [1.0, 0.5217391304347826, 0.4981532779316713, 0.375, 0.7422222222222223, 0.04235727440147329, 0.375, 0.8003202807453459, 0.7637153096468721, 1.0, 1.0, 1.0, 0.2701624699861352], 
reward next is 0.7298, 
noisyNet noise sample is [array([0.71818566], dtype=float32), -1.8870466]. 
=============================================
[2019-04-09 15:19:24,046] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6999094e-05 5.0236467e-02 1.1168276e-01 6.4326809e-03 4.8207617e-04
 2.3756109e-05 4.3117633e-01 4.1661751e-03 1.6612591e-02 4.9286699e-03
 3.7424150e-01], sum to 1.0000
[2019-04-09 15:19:24,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3199
[2019-04-09 15:19:24,063] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 22.5, 27.05169317953533, 0.8058939935555163, 0.0, 1.0, 65.0, 36.7569612573575], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 22.5, 27.00573132314357, 0.797886562384375, 0.0, 1.0, 45.0, 34.3624003679306], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.44, 0.0, 0.0, 0.375, 0.7504776102619642, 0.7659621874614584, 0.0, 1.0, 0.6, 0.34362400367930596], 
reward next is 0.6564, 
noisyNet noise sample is [array([0.14348985], dtype=float32), -0.33086264]. 
=============================================
[2019-04-09 15:19:24,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4814523e-05 4.0231626e-02 9.7062066e-02 7.5826235e-03 5.9433369e-04
 3.5655616e-05 5.2692026e-01 5.5384156e-03 8.0525205e-03 8.5869543e-03
 3.0538076e-01], sum to 1.0000
[2019-04-09 15:19:24,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3273
[2019-04-09 15:19:24,170] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 59.0, 0.0, 0.0, 22.5, 27.0010147803677, 0.7884516046554125, 1.0, 1.0, 65.0, 38.48471935526081], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [-1.2, 60.0, 0.0, 0.0, 22.5, 26.99674346472823, 0.7914351108446218, 1.0, 1.0, 45.0, 36.4929012968609], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6, 0.0, 0.0, 0.375, 0.7497286220606858, 0.7638117036148739, 1.0, 1.0, 0.6, 0.364929012968609], 
reward next is 0.6351, 
noisyNet noise sample is [array([-0.28452817], dtype=float32), -0.01075275]. 
=============================================
[2019-04-09 15:19:25,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1835760e-05 1.0537410e-01 1.1342377e-01 4.9414346e-03 4.5132649e-04
 8.4878520e-05 4.6779701e-01 6.1100121e-03 1.4788664e-02 1.8251717e-02
 2.6874533e-01], sum to 1.0000
[2019-04-09 15:19:25,168] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-09 15:19:25,188] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.716666666666666, 58.5, 0.0, 0.0, 19.0, 26.58137976049828, 0.6762538227330056, 0.0, 1.0, 45.0, 39.53787508612759], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2587800.0000, 
sim time next is 2588400.0000, 
raw observation next is [-3.9, 59.0, 0.0, 0.0, 19.0, 26.53333522435461, 0.6692032963976985, 0.0, 1.0, 65.0, 49.10720886445439], 
processed observation next is [1.0, 1.0, 0.3545706371191136, 0.59, 0.0, 0.0, 0.08333333333333333, 0.7111112686962174, 0.7230677654658995, 0.0, 1.0, 1.0, 0.49107208864454394], 
reward next is 0.5089, 
noisyNet noise sample is [array([-0.1153729], dtype=float32), -1.2082325]. 
=============================================
[2019-04-09 15:19:25,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4174460e-05 5.1294565e-02 1.5966016e-01 4.7867266e-03 4.0741180e-04
 6.3188883e-05 5.1456445e-01 8.7441914e-03 1.3205315e-02 1.0971701e-02
 2.3625806e-01], sum to 1.0000
[2019-04-09 15:19:25,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1737
[2019-04-09 15:19:25,633] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 59.5, 0.0, 0.0, 19.0, 26.51094971400614, 0.6732858356426649, 0.0, 1.0, 45.0, 40.85620943216624], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2589000.0000, 
sim time next is 2589600.0000, 
raw observation next is [-4.1, 60.0, 0.0, 0.0, 19.0, 26.48508637417236, 0.6656979192268792, 0.0, 1.0, 25.0, 40.55823096942763], 
processed observation next is [1.0, 1.0, 0.3490304709141275, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7070905311810302, 0.7218993064089597, 0.0, 1.0, 0.2, 0.40558230969427633], 
reward next is 0.5944, 
noisyNet noise sample is [array([-0.3471436], dtype=float32), 1.1276907]. 
=============================================
[2019-04-09 15:19:25,770] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3837380e-04 5.6549639e-02 1.6419147e-01 9.8833749e-03 1.1309008e-03
 3.3388581e-04 4.6380818e-01 6.5831407e-03 1.0902070e-02 1.2402897e-02
 2.7407607e-01], sum to 1.0000
[2019-04-09 15:19:25,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5937
[2019-04-09 15:19:25,789] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 19.0, 26.35580884908069, 0.6211252386319439, 0.0, 1.0, 25.0, 46.71775478186819], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.45294303400623, 0.6069981655633387, 0.0, 1.0, 45.0, 41.17978315536493], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7044119195005191, 0.7023327218544463, 0.0, 1.0, 0.6, 0.4117978315536493], 
reward next is 0.5882, 
noisyNet noise sample is [array([-1.3965122], dtype=float32), 1.2455317]. 
=============================================
[2019-04-09 15:19:25,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5864846e-04 8.4747985e-02 1.3452041e-01 1.1762934e-02 4.5204270e-03
 4.3372138e-04 3.4064570e-01 2.0517230e-02 2.3417657e-02 4.9821004e-02
 3.2945430e-01], sum to 1.0000
[2019-04-09 15:19:25,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9934
[2019-04-09 15:19:25,994] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 25.94663432818793, 0.5207787771632447, 0.0, 1.0, 65.0, 57.39699322923234], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [-6.616666666666667, 78.83333333333333, 0.0, 0.0, 19.0, 25.95715774596167, 0.5194633305389763, 0.0, 1.0, 65.0, 56.74700466239561], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.7883333333333333, 0.0, 0.0, 0.08333333333333333, 0.6630964788301391, 0.673154443512992, 0.0, 1.0, 1.0, 0.5674700466239561], 
reward next is 0.4325, 
noisyNet noise sample is [array([0.22877188], dtype=float32), -0.9505377]. 
=============================================
[2019-04-09 15:19:26,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[11.634436 ]
 [11.576636 ]
 [11.461076 ]
 [11.341912 ]
 [11.5031805]], R is [[11.87748623]
 [12.18474197]
 [12.51890182]
 [12.88302803]
 [13.21070385]].
[2019-04-09 15:19:26,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6071662e-04 3.8976178e-02 1.7844947e-01 9.4160773e-03 1.9694972e-03
 6.2468630e-04 3.0552557e-01 1.4717381e-02 2.5391039e-02 1.4895838e-02
 4.0987355e-01], sum to 1.0000
[2019-04-09 15:19:26,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0581
[2019-04-09 15:19:26,055] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.2, 75.33333333333334, 0.0, 0.0, 19.0, 26.05593756926145, 0.5423008098148329, 0.0, 1.0, 25.0, 48.82909031242281], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2604000.0000, 
sim time next is 2604600.0000, 
raw observation next is [-5.3, 76.0, 0.0, 0.0, 19.0, 26.02710895056033, 0.5240798095806541, 0.0, 1.0, 25.0, 33.72572048911212], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6689257458800274, 0.674693269860218, 0.0, 1.0, 0.2, 0.3372572048911212], 
reward next is 0.6627, 
noisyNet noise sample is [array([1.1634778], dtype=float32), 0.3571101]. 
=============================================
[2019-04-09 15:19:26,195] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3472195e-04 5.5069868e-02 3.1110427e-01 6.0686627e-03 2.9723169e-03
 3.7769799e-04 3.2234702e-01 1.7365770e-02 1.7238347e-02 3.1527806e-02
 2.3579353e-01], sum to 1.0000
[2019-04-09 15:19:26,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2751
[2019-04-09 15:19:26,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 19.0, 25.83734727890139, 0.5234626693086983, 0.0, 1.0, 45.0, 47.88039680659622], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2607600.0000, 
sim time next is 2608200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 19.0, 25.94292070730884, 0.5238504943331176, 0.0, 1.0, 25.0, 38.26346159179659], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.805, 0.0, 0.0, 0.08333333333333333, 0.6619100589424033, 0.6746168314443725, 0.0, 1.0, 0.2, 0.38263461591796594], 
reward next is 0.6174, 
noisyNet noise sample is [array([1.8730958], dtype=float32), 0.5277704]. 
=============================================
[2019-04-09 15:19:26,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2664687e-05 3.1152770e-02 1.9453193e-01 4.3797283e-03 8.5125235e-04
 1.1471102e-04 3.8498870e-01 6.7503597e-03 1.9625347e-02 1.3769082e-02
 3.4378344e-01], sum to 1.0000
[2019-04-09 15:19:26,368] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-09 15:19:26,381] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 57.0, 0.0, 0.0, 19.0, 26.77920038527606, 0.7061957344265068, 0.0, 1.0, 45.0, 40.79238961817762], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2586000.0000, 
sim time next is 2586600.0000, 
raw observation next is [-3.35, 57.5, 0.0, 0.0, 19.0, 26.70898445108812, 0.6985809958149624, 0.0, 1.0, 45.0, 39.99288696778097], 
processed observation next is [1.0, 0.9565217391304348, 0.3698060941828255, 0.575, 0.0, 0.0, 0.08333333333333333, 0.7257487042573434, 0.7328603319383208, 0.0, 1.0, 0.6, 0.39992886967780966], 
reward next is 0.6001, 
noisyNet noise sample is [array([0.2609319], dtype=float32), -1.6100793]. 
=============================================
[2019-04-09 15:19:26,642] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0665302e-05 8.9913748e-02 1.1418078e-01 5.6851436e-03 5.2160537e-04
 6.3437539e-05 2.7513319e-01 1.0497728e-02 1.5172233e-02 8.2655353e-03
 4.8055586e-01], sum to 1.0000
[2019-04-09 15:19:26,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7229
[2019-04-09 15:19:26,682] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 60.5, 0.0, 0.0, 22.5, 27.04805571662605, 0.78867325481324, 1.0, 1.0, 25.0, 36.32606108848491], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [-1.2, 61.00000000000001, 0.0, 0.0, 22.5, 27.00590822764575, 0.7777383780011785, 0.0, 1.0, 65.0, 41.17360864408421], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6100000000000001, 0.0, 0.0, 0.375, 0.7504923523038126, 0.7592461260003929, 0.0, 1.0, 1.0, 0.41173608644084214], 
reward next is 0.5883, 
noisyNet noise sample is [array([0.15997857], dtype=float32), -0.6887701]. 
=============================================
[2019-04-09 15:19:26,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7251416e-05 6.0922872e-02 1.4004886e-01 1.3243791e-02 3.2595190e-04
 4.8250451e-05 2.2628827e-01 5.7026097e-03 2.1468047e-02 1.0175512e-02
 5.2175862e-01], sum to 1.0000
[2019-04-09 15:19:26,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-09 15:19:26,872] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.633333333333334, 64.0, 142.1666666666667, 244.3333333333333, 22.5, 27.00368905699287, 0.6996625563421647, 1.0, 1.0, 45.0, 34.4720712920292], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2629200.0000, 
sim time next is 2629800.0000, 
raw observation next is [-4.45, 63.5, 152.0, 275.0, 22.5, 27.05747136602527, 0.7078220289591416, 1.0, 1.0, 65.0, 38.54056663508049], 
processed observation next is [1.0, 0.43478260869565216, 0.3393351800554017, 0.635, 0.5066666666666667, 0.30386740331491713, 0.375, 0.7547892805021057, 0.7359406763197138, 1.0, 1.0, 1.0, 0.38540566635080487], 
reward next is 0.6146, 
noisyNet noise sample is [array([0.83056664], dtype=float32), -1.8822054]. 
=============================================
[2019-04-09 15:19:26,905] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6469282e-05 4.8729047e-02 8.6728923e-02 8.0899009e-03 7.2886603e-04
 2.5739300e-04 4.7249445e-01 6.4147217e-03 1.9466128e-02 2.3196746e-02
 3.3383739e-01], sum to 1.0000
[2019-04-09 15:19:26,905] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8032
[2019-04-09 15:19:26,940] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.199999999999999, 78.33333333333334, 53.66666666666667, 2.666666666666666, 22.5, 25.27094108101802, 0.3687076092851794, 1.0, 1.0, 45.0, 36.27372992635008], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2621400.0000, 
sim time next is 2622000.0000, 
raw observation next is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 22.5, 25.28279758940194, 0.4167469018527592, 1.0, 1.0, 65.0, 66.21316523102017], 
processed observation next is [1.0, 0.34782608695652173, 0.26592797783933514, 0.7766666666666667, 0.21777777777777782, 0.00147329650092081, 0.375, 0.6068997991168285, 0.6389156339509198, 1.0, 1.0, 1.0, 0.6621316523102017], 
reward next is 0.3379, 
noisyNet noise sample is [array([0.983416], dtype=float32), -1.3731211]. 
=============================================
[2019-04-09 15:19:26,948] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[12.318704]
 [12.020772]
 [12.010626]
 [11.644108]
 [11.732692]], R is [[12.71410275]
 [13.22422409]
 [13.5750742 ]
 [13.78994274]
 [14.27666283]].
[2019-04-09 15:19:27,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4195046e-05 7.5283960e-02 1.4768028e-01 6.5088384e-03 8.0174801e-04
 1.5253010e-04 4.9722055e-01 3.5235172e-03 2.0852158e-02 1.1723789e-02
 2.3616835e-01], sum to 1.0000
[2019-04-09 15:19:27,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8765
[2019-04-09 15:19:27,028] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 19.0, 25.63772201405335, 0.5539685905482198, 0.0, 1.0, 65.0, 67.03287537165001], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 19.0, 25.79689657582355, 0.5578508351840329, 0.0, 1.0, 65.0, 55.45245845517192], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.08333333333333333, 0.6497413813186291, 0.6859502783946776, 0.0, 1.0, 1.0, 0.5545245845517193], 
reward next is 0.4455, 
noisyNet noise sample is [array([-0.7370103], dtype=float32), -1.8303045]. 
=============================================
[2019-04-09 15:19:27,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.03250191e-05 5.51915206e-02 1.77580789e-01 1.78551096e-02
 9.56434582e-04 2.11643739e-04 3.53617698e-01 1.54880285e-02
 2.28141136e-02 7.95734301e-03 3.48267019e-01], sum to 1.0000
[2019-04-09 15:19:27,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8818
[2019-04-09 15:19:27,169] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 22.5, 26.57311836744172, 0.5820101790208007, 1.0, 1.0, 65.0, 49.49067417856312], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 22.5, 26.6085603197393, 0.5984591781467443, 1.0, 1.0, 65.0, 45.57978637811272], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.375, 0.7173800266449417, 0.6994863927155816, 1.0, 1.0, 1.0, 0.45579786378112724], 
reward next is 0.5442, 
noisyNet noise sample is [array([-0.6364175], dtype=float32), -0.053539902]. 
=============================================
[2019-04-09 15:19:27,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.42266845e-05 3.63756418e-02 1.37799919e-01 4.50906483e-03
 6.29215094e-04 2.91994202e-05 5.84891677e-01 4.28972719e-03
 1.16173308e-02 9.86730307e-03 2.09976673e-01], sum to 1.0000
[2019-04-09 15:19:27,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3966
[2019-04-09 15:19:27,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 45.0, 216.0, 130.0, 22.5, 27.65573575742304, 0.8747281553640969, 1.0, 1.0, 45.0, 28.10841779541206], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2644200.0000, 
sim time next is 2644800.0000, 
raw observation next is [0.5, 45.66666666666667, 205.8333333333333, 142.6666666666667, 22.5, 27.68817250109119, 0.8836660558880731, 1.0, 1.0, 45.0, 25.94240885714589], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.4566666666666667, 0.686111111111111, 0.15764272559852677, 0.375, 0.807347708424266, 0.794555351962691, 1.0, 1.0, 0.6, 0.2594240885714589], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.2707111], dtype=float32), 0.3079578]. 
=============================================
[2019-04-09 15:19:28,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.33495062e-06 8.63765553e-02 1.42994821e-01 5.15636010e-03
 5.80872409e-04 1.00227684e-04 2.23419294e-01 2.58691236e-03
 9.69343353e-03 6.97499793e-03 5.22109151e-01], sum to 1.0000
[2019-04-09 15:19:28,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0323
[2019-04-09 15:19:28,048] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 22.5, 27.46919603676094, 0.8490015924438122, 1.0, 1.0, 65.0, 25.03942429260309], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2730000.0000, 
sim time next is 2730600.0000, 
raw observation next is [-4.4, 55.0, 102.0, 733.0, 22.5, 27.51089205112619, 0.8596914686704183, 1.0, 1.0, 20.0, 21.8684431198922], 
processed observation next is [1.0, 0.6086956521739131, 0.3407202216066482, 0.55, 0.34, 0.8099447513812155, 0.375, 0.7925743375938493, 0.7865638228901394, 1.0, 1.0, 0.1, 0.21868443119892197], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.6640384], dtype=float32), 0.40245867]. 
=============================================
[2019-04-09 15:19:28,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5955521e-06 5.4730184e-02 7.9220757e-02 7.1990401e-03 7.1355136e-04
 1.5841391e-05 6.2088263e-01 3.5925587e-03 1.3704554e-02 3.3473349e-03
 2.1658897e-01], sum to 1.0000
[2019-04-09 15:19:28,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6161
[2019-04-09 15:19:28,297] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 47.0, 185.5, 168.0, 22.5, 27.44937842674997, 0.8531078367188091, 1.0, 1.0, 45.0, 31.36047709828971], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2646000.0000, 
sim time next is 2646600.0000, 
raw observation next is [0.5, 47.5, 175.3333333333333, 180.6666666666667, 22.5, 27.6284091772015, 0.8626599930282671, 1.0, 1.0, 45.0, 23.13240590079066], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.475, 0.5844444444444443, 0.19963167587476985, 0.375, 0.8023674314334585, 0.7875533310094224, 1.0, 1.0, 0.6, 0.2313240590079066], 
reward next is 0.7687, 
noisyNet noise sample is [array([1.3308394], dtype=float32), 0.93143827]. 
=============================================
[2019-04-09 15:19:28,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9822173e-05 3.5760812e-02 6.4356960e-02 8.6196847e-03 3.7610749e-04
 5.4254026e-05 5.3763485e-01 1.5516402e-03 1.2436269e-02 9.8614646e-03
 3.2932809e-01], sum to 1.0000
[2019-04-09 15:19:28,752] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7618
[2019-04-09 15:19:28,768] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 63.66666666666667, 0.0, 0.0, 19.0, 26.81542443954994, 0.7686886526464471, 0.0, 1.0, 45.0, 35.91458997567024], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2665200.0000, 
sim time next is 2665800.0000, 
raw observation next is [-1.2, 64.0, 0.0, 0.0, 19.0, 26.78609320245445, 0.7654148264366611, 0.0, 1.0, 65.0, 44.35261246261958], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.64, 0.0, 0.0, 0.08333333333333333, 0.7321744335378707, 0.755138275478887, 0.0, 1.0, 1.0, 0.44352612462619584], 
reward next is 0.5565, 
noisyNet noise sample is [array([1.3693229], dtype=float32), -0.2216529]. 
=============================================
[2019-04-09 15:19:28,771] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9576287e-06 6.6708922e-02 1.7003646e-01 3.9245784e-03 5.4555410e-04
 3.9202630e-05 4.1433492e-01 3.4977782e-03 3.9738085e-02 8.5837878e-03
 2.9258472e-01], sum to 1.0000
[2019-04-09 15:19:28,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1533
[2019-04-09 15:19:28,791] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 50.0, 75.0, 124.0, 22.5, 27.69951364074943, 0.8699530218424365, 1.0, 1.0, 55.0, 30.80397352001977], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2651400.0000, 
sim time next is 2652000.0000, 
raw observation next is [0.5, 50.0, 63.66666666666666, 117.0, 22.5, 27.72305612670458, 0.873411832828849, 1.0, 1.0, 25.0, 24.54228983711395], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.2122222222222222, 0.1292817679558011, 0.375, 0.8102546772253817, 0.7911372776096163, 1.0, 1.0, 0.2, 0.2454228983711395], 
reward next is 0.7546, 
noisyNet noise sample is [array([-1.3777303], dtype=float32), -0.8426732]. 
=============================================
[2019-04-09 15:19:28,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[15.110159]
 [14.961488]
 [15.037193]
 [14.770507]
 [15.109299]], R is [[15.58044434]
 [16.11660004]
 [16.68453789]
 [17.26465607]
 [17.98405838]].
[2019-04-09 15:19:29,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0349828e-05 7.4462757e-02 1.3729915e-01 1.6859747e-03 1.8320176e-04
 3.2149368e-05 4.5781928e-01 1.5345492e-03 1.2054163e-02 4.7764149e-03
 3.1014213e-01], sum to 1.0000
[2019-04-09 15:19:29,083] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0946
[2019-04-09 15:19:29,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 56.5, 0.0, 0.0, 22.5, 26.8455921851618, 0.7080926823739929, 1.0, 1.0, 25.0, 63.82168010244309], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 22.5, 26.59539201406488, 0.6893035206824484, 1.0, 1.0, 25.0, 51.17311183012255], 
processed observation next is [1.0, 0.782608695652174, 0.33333333333333337, 0.5733333333333333, 0.0, 0.0, 0.375, 0.7162826678387401, 0.7297678402274829, 1.0, 1.0, 0.2, 0.5117311183012255], 
reward next is 0.4883, 
noisyNet noise sample is [array([-0.04644806], dtype=float32), 1.7066936]. 
=============================================
[2019-04-09 15:19:29,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3184333e-06 8.8569820e-02 1.3867564e-01 1.8716418e-03 1.6242027e-04
 2.6227443e-05 4.6193621e-01 1.2907267e-03 1.2708029e-02 4.5904741e-03
 2.9016149e-01], sum to 1.0000
[2019-04-09 15:19:29,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-09 15:19:29,168] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 58.16666666666667, 0.0, 0.0, 22.5, 26.49033826910141, 0.6762135519131589, 1.0, 1.0, 45.0, 47.98309875034967], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2746200.0000, 
sim time next is 2746800.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.45624094458754, 0.6550076872186915, 1.0, 1.0, 45.0, 45.68274221808849], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.7046867453822951, 0.7183358957395639, 1.0, 1.0, 0.6, 0.4568274221808849], 
reward next is 0.5432, 
noisyNet noise sample is [array([-0.04644806], dtype=float32), 1.7066936]. 
=============================================
[2019-04-09 15:19:29,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1053596e-06 1.7252308e-01 2.0402183e-01 1.7599840e-02 5.4056430e-04
 6.8822766e-05 2.4746938e-01 1.2800992e-02 2.7229981e-02 1.9588407e-02
 2.9815000e-01], sum to 1.0000
[2019-04-09 15:19:29,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7523
[2019-04-09 15:19:29,191] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 22.5, 26.91754211031698, 0.728273128796321, 1.0, 1.0, 25.0, 33.16264326385606], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2637600.0000, 
sim time next is 2638200.0000, 
raw observation next is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 22.5, 27.05461166990659, 0.7466367475573747, 1.0, 1.0, 65.0, 40.45206674776775], 
processed observation next is [1.0, 0.5217391304347826, 0.43813481071098803, 0.4816666666666667, 0.7266666666666667, 0.18600368324125224, 0.375, 0.7545509724922157, 0.7488789158524582, 1.0, 1.0, 1.0, 0.4045206674776775], 
reward next is 0.5955, 
noisyNet noise sample is [array([0.36168078], dtype=float32), 0.50945985]. 
=============================================
[2019-04-09 15:19:29,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.45061495e-04 1.22321464e-01 1.31810442e-01 1.18492786e-02
 1.42252562e-03 4.36597242e-04 3.22997302e-01 1.32571030e-02
 2.92861741e-02 3.12669575e-02 3.35207105e-01], sum to 1.0000
[2019-04-09 15:19:29,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-09 15:19:29,213] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 19.0, 25.05937452309499, 0.383749160489725, 0.0, 1.0, 20.0, 51.10416656115668], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2692800.0000, 
sim time next is 2693400.0000, 
raw observation next is [-15.0, 89.66666666666667, 0.0, 0.0, 19.0, 25.0625194342903, 0.3782970239576181, 0.0, 1.0, 45.0, 39.59897841705029], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5885432861908585, 0.6260990079858727, 0.0, 1.0, 0.6, 0.3959897841705029], 
reward next is 0.6040, 
noisyNet noise sample is [array([0.44971836], dtype=float32), 0.2861382]. 
=============================================
[2019-04-09 15:19:29,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5046709e-05 4.2049017e-02 7.7387244e-02 6.0200058e-03 5.7016901e-04
 8.4012390e-05 3.6448002e-01 3.8385515e-03 1.7840732e-02 1.3302479e-02
 4.7438270e-01], sum to 1.0000
[2019-04-09 15:19:29,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-09 15:19:29,482] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 19.0, 26.14175746748568, 0.6079362924014219, 0.0, 1.0, 65.0, 58.65914271744102], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 19.0, 26.20722540387155, 0.6021737869218389, 0.0, 1.0, 65.0, 53.18454949764693], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.08333333333333333, 0.6839354503226293, 0.700724595640613, 0.0, 1.0, 1.0, 0.5318454949764693], 
reward next is 0.4682, 
noisyNet noise sample is [array([1.3814509], dtype=float32), 0.81288767]. 
=============================================
[2019-04-09 15:19:29,872] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1370939e-05 4.0902384e-02 6.5570891e-02 8.9371884e-03 1.8781949e-04
 4.6974983e-05 5.2910078e-01 2.5803852e-03 4.6088821e-03 1.5977003e-02
 3.3207631e-01], sum to 1.0000
[2019-04-09 15:19:29,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3698
[2019-04-09 15:19:29,889] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.83333333333334, 0.0, 0.0, 19.0, 26.21375006909306, 0.6390623482792316, 0.0, 1.0, 45.0, 38.18660766215043], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2757000.0000, 
sim time next is 2757600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.19334735150987, 0.6290350663195226, 0.0, 1.0, 45.0, 39.09261072736265], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6827789459591559, 0.7096783554398408, 0.0, 1.0, 0.6, 0.3909261072736265], 
reward next is 0.6091, 
noisyNet noise sample is [array([0.49129283], dtype=float32), -0.04022879]. 
=============================================
[2019-04-09 15:19:29,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2307702e-05 4.8162624e-02 8.0186099e-02 1.1293199e-02 2.6706222e-04
 7.4066280e-05 4.9664006e-01 3.2880246e-03 6.0655256e-03 2.3232751e-02
 3.3076829e-01], sum to 1.0000
[2019-04-09 15:19:29,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0495
[2019-04-09 15:19:30,017] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.8251467202541, 0.4908595458888024, 0.0, 1.0, 45.0, 32.62695982638036], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2759400.0000, 
sim time next is 2760000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.62032460239227, 0.4555490244834635, 0.0, 1.0, 25.0, 30.44432195685545], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6350270501993558, 0.6518496748278212, 0.0, 1.0, 0.2, 0.3044432195685545], 
reward next is 0.6956, 
noisyNet noise sample is [array([0.49129283], dtype=float32), -0.04022879]. 
=============================================
[2019-04-09 15:19:30,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[13.550045]
 [13.869125]
 [14.134945]
 [13.86687 ]
 [14.481885]], R is [[13.92635441]
 [14.46082115]
 [14.94161129]
 [15.38020039]
 [15.83547211]].
[2019-04-09 15:19:30,223] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.78435286e-05 6.19426966e-02 1.08837135e-01 1.30920382e-02
 8.58106825e-04 9.26324574e-05 4.90783155e-01 4.08253539e-03
 2.02806760e-02 1.19975675e-02 2.87985563e-01], sum to 1.0000
[2019-04-09 15:19:30,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1278
[2019-04-09 15:19:30,236] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 19.0, 26.18283094382959, 0.6093452507063767, 0.0, 1.0, 45.0, 41.87508960832113], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 19.0, 26.17655918611987, 0.5899612318493114, 0.0, 1.0, 25.0, 38.82543876979202], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.08333333333333333, 0.6813799321766559, 0.6966537439497705, 0.0, 1.0, 0.2, 0.3882543876979202], 
reward next is 0.6117, 
noisyNet noise sample is [array([-0.43460023], dtype=float32), -0.36866826]. 
=============================================
[2019-04-09 15:19:30,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9292789e-06 7.6845713e-02 1.0162540e-01 3.2305629e-03 4.9351261e-04
 2.4015542e-05 3.8295293e-01 3.4919956e-03 2.3251083e-02 6.5809526e-03
 4.0149489e-01], sum to 1.0000
[2019-04-09 15:19:30,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1186
[2019-04-09 15:19:30,456] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 58.0, 0.0, 0.0, 22.5, 27.03977629555112, 0.7986052352860505, 1.0, 1.0, 65.0, 39.22468638610193], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2659200.0000, 
sim time next is 2659800.0000, 
raw observation next is [-1.1, 59.0, 0.0, 0.0, 22.5, 27.02536075106218, 0.7945126068194335, 1.0, 1.0, 25.0, 33.6412451334341], 
processed observation next is [1.0, 0.782608695652174, 0.4321329639889197, 0.59, 0.0, 0.0, 0.375, 0.7521133959218483, 0.7648375356064778, 1.0, 1.0, 0.2, 0.33641245133434095], 
reward next is 0.6636, 
noisyNet noise sample is [array([-0.9842441], dtype=float32), 0.03459678]. 
=============================================
[2019-04-09 15:19:30,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2062322e-04 7.3697448e-02 2.0851985e-01 7.3574502e-03 1.1528498e-03
 3.8962354e-04 4.0206307e-01 8.1621828e-03 2.0007960e-02 1.2023040e-02
 2.6650581e-01], sum to 1.0000
[2019-04-09 15:19:30,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8738
[2019-04-09 15:19:30,644] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.58655987201785, 0.2194585512336578, 0.0, 1.0, 45.0, 31.77016813657463], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.46766792790658, 0.2254448931098109, 0.0, 1.0, 65.0, 56.03668616061159], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5389723273255482, 0.5751482977032704, 0.0, 1.0, 1.0, 0.5603668616061159], 
reward next is 0.4396, 
noisyNet noise sample is [array([0.51675236], dtype=float32), 1.4643253]. 
=============================================
[2019-04-09 15:19:30,686] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9882615e-06 1.4174822e-01 6.0811039e-02 2.4214187e-03 9.2142762e-04
 6.0070361e-05 4.9496865e-01 6.0698926e-03 1.2692349e-02 1.9506462e-02
 2.6079148e-01], sum to 1.0000
[2019-04-09 15:19:30,686] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4888
[2019-04-09 15:19:30,723] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 22.5, 26.57216400807377, 0.6553907797740101, 1.0, 1.0, 45.0, 42.90418091270416], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2717400.0000, 
sim time next is 2718000.0000, 
raw observation next is [-9.0, 64.0, 114.5, 727.5, 22.5, 26.15262730922668, 0.613470416709241, 1.0, 1.0, 45.0, 29.36784512600515], 
processed observation next is [1.0, 0.4782608695652174, 0.21329639889196678, 0.64, 0.38166666666666665, 0.8038674033149171, 0.375, 0.6793856091022233, 0.7044901389030803, 1.0, 1.0, 0.6, 0.2936784512600515], 
reward next is 0.7063, 
noisyNet noise sample is [array([0.0681913], dtype=float32), 0.5953198]. 
=============================================
[2019-04-09 15:19:30,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[15.071489]
 [15.182278]
 [14.966359]
 [14.475592]
 [14.162074]], R is [[15.54786491]
 [15.96334457]
 [16.29423714]
 [16.75422287]
 [17.17056084]].
[2019-04-09 15:19:30,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8321849e-05 8.5474081e-02 1.2142813e-01 1.3050696e-02 1.4778577e-03
 1.6177206e-04 3.3570352e-01 6.5753139e-03 1.0249555e-02 1.2824156e-02
 4.1295666e-01], sum to 1.0000
[2019-04-09 15:19:30,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-09 15:19:30,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.57136118607947, 0.4262726263311296, 0.0, 1.0, 45.0, 37.8398600243204], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.57274154807871, 0.4362728182509155, 0.0, 1.0, 65.0, 54.51882601803998], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6310617956732258, 0.6454242727503051, 0.0, 1.0, 1.0, 0.5451882601803998], 
reward next is 0.4548, 
noisyNet noise sample is [array([1.4434544], dtype=float32), 0.9695729]. 
=============================================
[2019-04-09 15:19:31,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.77028215e-04 8.15441012e-02 1.18254654e-01 2.16696411e-02
 1.95813971e-03 5.75059850e-04 3.24402183e-01 1.64780878e-02
 1.51322335e-02 3.30085568e-02 3.86700273e-01], sum to 1.0000
[2019-04-09 15:19:31,024] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5258
[2019-04-09 15:19:31,039] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 19.0, 25.03930454000974, 0.3533708732340257, 0.0, 1.0, 20.0, 49.92956132496901], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2779800.0000, 
sim time next is 2780400.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 25.3209338728816, 0.3662457150559157, 0.0, 1.0, 45.0, 43.12640923998265], 
processed observation next is [1.0, 0.17391304347826086, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.08333333333333333, 0.6100778227401333, 0.6220819050186386, 0.0, 1.0, 0.6, 0.4312640923998265], 
reward next is 0.5687, 
noisyNet noise sample is [array([0.19854161], dtype=float32), 0.94436914]. 
=============================================
[2019-04-09 15:19:31,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2109597e-04 8.5369907e-02 1.4254293e-01 1.0262763e-02 1.8895922e-03
 3.1559390e-04 2.2820804e-01 6.9261128e-03 1.4954028e-02 1.4110811e-02
 4.9529913e-01], sum to 1.0000
[2019-04-09 15:19:31,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4494
[2019-04-09 15:19:31,167] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 19.0, 25.47310432387312, 0.4384779253429782, 0.0, 1.0, 25.0, 41.0318181310908], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2692800.0000, 
sim time next is 2693400.0000, 
raw observation next is [-15.0, 89.66666666666667, 0.0, 0.0, 19.0, 25.42652670430946, 0.4227890967026028, 0.0, 1.0, 45.0, 40.77219111116359], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6188772253591216, 0.6409296989008676, 0.0, 1.0, 0.6, 0.4077219111116359], 
reward next is 0.5923, 
noisyNet noise sample is [array([0.6202768], dtype=float32), 0.4558607]. 
=============================================
[2019-04-09 15:19:31,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7628811e-04 4.2104591e-02 1.4001532e-01 8.1353346e-03 1.7139055e-03
 4.1083293e-04 3.2980549e-01 1.3542831e-02 1.0739604e-02 3.0171160e-02
 4.2318472e-01], sum to 1.0000
[2019-04-09 15:19:31,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2890
[2019-04-09 15:19:31,400] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 22.5, 25.1368555051663, 0.3295005424385225, 1.0, 1.0, 45.0, 45.573414507496], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2705400.0000, 
sim time next is 2706000.0000, 
raw observation next is [-15.0, 83.0, 13.33333333333333, 54.99999999999999, 22.5, 25.19097543172995, 0.3264473587046604, 1.0, 1.0, 45.0, 43.15845091275979], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.04444444444444443, 0.060773480662983416, 0.375, 0.5992479526441624, 0.6088157862348867, 1.0, 1.0, 0.6, 0.4315845091275979], 
reward next is 0.5684, 
noisyNet noise sample is [array([-0.09156025], dtype=float32), -0.06489491]. 
=============================================
[2019-04-09 15:19:31,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[11.199054]
 [11.258631]
 [11.17852 ]
 [11.341665]
 [11.172025]], R is [[12.09411907]
 [12.51744366]
 [12.79384422]
 [13.1113739 ]
 [13.49764252]].
[2019-04-09 15:19:31,923] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7676575e-05 2.3420218e-02 9.9014163e-02 6.2511773e-03 4.1331904e-04
 4.2666437e-05 3.7109253e-01 4.0412322e-03 1.2081572e-02 3.9089680e-02
 4.4453579e-01], sum to 1.0000
[2019-04-09 15:19:31,927] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3592
[2019-04-09 15:19:31,963] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.37651411871436, 0.6169889418719797, 0.0, 1.0, 65.0, 60.32019812001623], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2758800.0000, 
sim time next is 2759400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.30872465205232, 0.6114059463029536, 0.0, 1.0, 45.0, 52.52828205237702], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6923937210043599, 0.7038019821009845, 0.0, 1.0, 0.6, 0.5252828205237702], 
reward next is 0.4747, 
noisyNet noise sample is [array([-0.36924794], dtype=float32), -0.062877886]. 
=============================================
[2019-04-09 15:19:32,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1511168e-05 5.0885744e-02 9.5568612e-02 7.0319446e-03 7.0426089e-04
 1.0366650e-04 3.6923179e-01 5.1488136e-03 2.0188615e-02 1.0595241e-02
 4.4049987e-01], sum to 1.0000
[2019-04-09 15:19:32,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4282
[2019-04-09 15:19:32,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 87.0, 80.0, 330.0, 22.5, 25.23784894220461, 0.3710339903982356, 1.0, 1.0, 25.0, 33.72379816024858], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2709000.0000, 
sim time next is 2709600.0000, 
raw observation next is [-14.33333333333333, 88.33333333333334, 82.83333333333334, 377.0, 22.5, 25.42012904827896, 0.3854751891325333, 1.0, 1.0, 45.0, 31.45196441030282], 
processed observation next is [1.0, 0.34782608695652173, 0.0655586334256695, 0.8833333333333334, 0.27611111111111114, 0.4165745856353591, 0.375, 0.6183440873565799, 0.6284917297108444, 1.0, 1.0, 0.6, 0.31451964410302824], 
reward next is 0.6855, 
noisyNet noise sample is [array([0.24371974], dtype=float32), 2.0021553]. 
=============================================
[2019-04-09 15:19:32,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.80423795e-05 6.84099123e-02 8.56472626e-02 5.66418609e-03
 2.66527437e-04 5.36571060e-05 4.45319504e-01 2.98680109e-03
 1.89264975e-02 1.14628095e-02 3.61244798e-01], sum to 1.0000
[2019-04-09 15:19:32,280] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2010
[2019-04-09 15:19:32,306] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 56.5, 106.6666666666667, 769.6666666666666, 22.5, 27.01655034980302, 0.7689129081294427, 1.0, 1.0, 45.0, 23.30401995454731], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2728200.0000, 
sim time next is 2728800.0000, 
raw observation next is [-4.8, 56.0, 105.5, 760.5, 22.5, 27.19490689329676, 0.8001445447387328, 1.0, 1.0, 45.0, 25.47698780141098], 
processed observation next is [1.0, 0.6086956521739131, 0.3296398891966759, 0.56, 0.3516666666666667, 0.8403314917127072, 0.375, 0.7662422411080634, 0.7667148482462443, 1.0, 1.0, 0.6, 0.25476987801410983], 
reward next is 0.7452, 
noisyNet noise sample is [array([1.7144455], dtype=float32), 0.13308443]. 
=============================================
[2019-04-09 15:19:32,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5953409e-04 7.8566782e-02 1.1971527e-01 1.2427180e-02 2.0974742e-03
 2.6062719e-04 3.9254072e-01 1.1959613e-02 1.7124988e-02 2.5485078e-02
 3.3966270e-01], sum to 1.0000
[2019-04-09 15:19:32,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7428
[2019-04-09 15:19:32,901] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 19.0, 26.13192825554265, 0.5155986326522397, 0.0, 1.0, 20.0, 39.96837656703605], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 26.10987955163533, 0.5085180974682554, 0.0, 1.0, 65.0, 50.10654968737125], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.6758232959696109, 0.6695060324894184, 0.0, 1.0, 1.0, 0.5010654968737125], 
reward next is 0.4989, 
noisyNet noise sample is [array([-0.6082119], dtype=float32), 0.042646162]. 
=============================================
[2019-04-09 15:19:32,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2199949e-05 6.7981258e-02 2.2406159e-01 6.7671095e-03 4.2006015e-04
 4.6746449e-05 2.1382156e-01 7.9528447e-03 1.0158540e-02 9.7541166e-03
 4.5902407e-01], sum to 1.0000
[2019-04-09 15:19:32,907] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9833
[2019-04-09 15:19:32,956] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666667, 55.66666666666667, 104.3333333333333, 751.3333333333334, 22.5, 27.30873894660999, 0.81013060529113, 1.0, 1.0, 30.0, 28.7837495013814], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2729400.0000, 
sim time next is 2730000.0000, 
raw observation next is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 22.5, 27.35297224813285, 0.8218974177560795, 1.0, 1.0, 65.0, 25.85592782167313], 
processed observation next is [1.0, 0.6086956521739131, 0.3370267774699908, 0.5533333333333335, 0.343888888888889, 0.8200736648250462, 0.375, 0.7794143540110708, 0.7739658059186931, 1.0, 1.0, 1.0, 0.2585592782167313], 
reward next is 0.7414, 
noisyNet noise sample is [array([-0.4194904], dtype=float32), -0.4194323]. 
=============================================
[2019-04-09 15:19:32,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[15.581201 ]
 [15.621818 ]
 [15.69012  ]
 [15.3803835]
 [15.553069 ]], R is [[16.19179344]
 [16.74203682]
 [17.34282875]
 [17.8687439 ]
 [18.53167343]].
[2019-04-09 15:19:33,191] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1615406e-06 6.6260472e-02 1.6106802e-01 1.3875063e-02 4.4612522e-04
 3.6374091e-05 3.9604992e-01 4.3139607e-03 2.1259869e-02 1.5131300e-02
 3.2155269e-01], sum to 1.0000
[2019-04-09 15:19:33,193] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2031
[2019-04-09 15:19:33,219] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.833333333333334, 64.0, 113.6666666666667, 745.3333333333334, 22.5, 26.19524957425933, 0.6324272032806005, 1.0, 1.0, 25.0, 28.67637512116349], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2718600.0000, 
sim time next is 2719200.0000, 
raw observation next is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 22.5, 26.56258006127935, 0.6691252212261084, 1.0, 1.0, 25.0, 33.68867595145396], 
processed observation next is [1.0, 0.4782608695652174, 0.22253000923361033, 0.64, 0.376111111111111, 0.8432780847145489, 0.375, 0.7135483384399457, 0.7230417404087027, 1.0, 1.0, 0.2, 0.3368867595145396], 
reward next is 0.6631, 
noisyNet noise sample is [array([0.21498722], dtype=float32), -0.3498415]. 
=============================================
[2019-04-09 15:19:33,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6423782e-05 6.0663525e-02 1.4537691e-01 7.4683079e-03 2.0478341e-04
 8.7529465e-05 4.3161178e-01 7.1368581e-03 1.1517856e-02 1.1629625e-02
 3.2427630e-01], sum to 1.0000
[2019-04-09 15:19:33,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3299
[2019-04-09 15:19:33,367] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.11491416092148, 0.5813393043666518, 0.0, 1.0, 65.0, 58.18331759373925], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.11449511229052, 0.5774796304660419, 0.0, 1.0, 45.0, 49.17072857387184], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.67620792602421, 0.6924932101553473, 0.0, 1.0, 0.6, 0.4917072857387184], 
reward next is 0.5083, 
noisyNet noise sample is [array([-0.9991862], dtype=float32), 0.90944314]. 
=============================================
[2019-04-09 15:19:33,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.15014243e-05 5.85299246e-02 1.67318523e-01 9.50368308e-03
 1.18518295e-03 1.57660019e-04 3.79452646e-01 1.20544145e-02
 2.69576274e-02 2.96015926e-02 3.15147221e-01], sum to 1.0000
[2019-04-09 15:19:33,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4814
[2019-04-09 15:19:33,483] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 64.0, 0.0, 0.0, 22.5, 24.36025995706037, 0.1925548233942579, 1.0, 1.0, 65.0, 73.20189868946551], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2791800.0000, 
sim time next is 2792400.0000, 
raw observation next is [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 22.5, 24.3921414670496, 0.2083813165580836, 1.0, 1.0, 45.0, 61.90897475125571], 
processed observation next is [1.0, 0.30434782608695654, 0.28716528162511545, 0.64, 0.06, 0.038121546961325956, 0.375, 0.5326784555874667, 0.5694604388526946, 1.0, 1.0, 0.6, 0.6190897475125571], 
reward next is 0.3809, 
noisyNet noise sample is [array([0.20468915], dtype=float32), 1.0377923]. 
=============================================
[2019-04-09 15:19:33,655] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0348723e-04 4.1496299e-02 1.1088495e-01 1.0000364e-02 9.6044381e-04
 2.8172467e-04 3.2465732e-01 7.4350811e-03 3.4372896e-02 1.9936956e-02
 4.4987053e-01], sum to 1.0000
[2019-04-09 15:19:33,656] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-09 15:19:33,686] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5824573e-04 4.6885021e-02 1.0251501e-01 8.3107427e-03 9.3063765e-04
 2.3599286e-04 3.6634299e-01 2.0298403e-02 1.6769933e-02 1.4839176e-02
 4.2271388e-01], sum to 1.0000
[2019-04-09 15:19:33,686] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-09 15:19:33,687] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.58840479969894, 0.4117857840606562, 0.0, 1.0, 65.0, 74.79877080651], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2787600.0000, 
sim time next is 2788200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.62050740058071, 0.3887494482825313, 0.0, 1.0, 25.0, 65.32275683835987], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6350422833817259, 0.6295831494275105, 0.0, 1.0, 0.2, 0.6532275683835986], 
reward next is 0.3468, 
noisyNet noise sample is [array([0.5462359], dtype=float32), 1.0071844]. 
=============================================
[2019-04-09 15:19:33,713] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.22750964092045, 0.3234455432287655, 0.0, 1.0, 45.0, 43.29573883250288], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2787000.0000, 
sim time next is 2787600.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.1467983373783, 0.3230455844210396, 0.0, 1.0, 65.0, 71.80075798574552], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.5955665281148583, 0.6076818614736799, 0.0, 1.0, 1.0, 0.7180075798574552], 
reward next is 0.2820, 
noisyNet noise sample is [array([-0.80512124], dtype=float32), -1.6713483]. 
=============================================
[2019-04-09 15:19:33,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0411316e-05 1.7620678e-01 1.6648903e-01 1.3564046e-02 1.1942781e-03
 1.2396448e-04 2.6562077e-01 1.2131700e-02 1.3224713e-02 2.0159895e-02
 3.3122441e-01], sum to 1.0000
[2019-04-09 15:19:33,872] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5251
[2019-04-09 15:19:33,899] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.0, 91.0, 88.5, 471.0, 22.5, 25.73589764917801, 0.4485986316272605, 1.0, 1.0, 65.0, 53.82229611625591], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2710800.0000, 
sim time next is 2711400.0000, 
raw observation next is [-13.66666666666667, 88.50000000000001, 91.33333333333333, 518.0, 22.5, 25.71936528836743, 0.4654769792615431, 1.0, 1.0, 45.0, 44.01853918642547], 
processed observation next is [1.0, 0.391304347826087, 0.08402585410895651, 0.8850000000000001, 0.3044444444444444, 0.5723756906077349, 0.375, 0.643280440697286, 0.6551589930871811, 1.0, 1.0, 0.6, 0.4401853918642547], 
reward next is 0.5598, 
noisyNet noise sample is [array([-0.37903938], dtype=float32), -0.1646178]. 
=============================================
[2019-04-09 15:19:34,554] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2715357e-04 5.6855399e-02 1.8201485e-01 6.2299818e-03 1.8793164e-03
 1.8924833e-04 3.9235389e-01 1.5648698e-02 3.1733349e-02 1.8726557e-02
 2.9424158e-01], sum to 1.0000
[2019-04-09 15:19:34,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6122
[2019-04-09 15:19:34,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.97848311657491, 0.4902700962673525, 0.0, 1.0, 25.0, 34.04285648760474], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2774400.0000, 
sim time next is 2775000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.92760548035708, 0.4561038244914723, 0.0, 1.0, 45.0, 35.03493121934924], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6606337900297566, 0.6520346081638241, 0.0, 1.0, 0.6, 0.3503493121934924], 
reward next is 0.6497, 
noisyNet noise sample is [array([-0.42277652], dtype=float32), 1.813778]. 
=============================================
[2019-04-09 15:19:34,585] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[11.724299]
 [11.816024]
 [12.060811]
 [12.297225]
 [12.29885 ]], R is [[12.25008011]
 [12.78715134]
 [13.29527855]
 [13.73923969]
 [14.15808964]].
[2019-04-09 15:19:34,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6340318e-04 9.9906720e-02 1.2078017e-01 1.2807654e-02 1.3916199e-03
 3.1071127e-04 2.9581812e-01 1.1047547e-02 3.3295918e-02 2.2624375e-02
 4.0185380e-01], sum to 1.0000
[2019-04-09 15:19:34,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9060
[2019-04-09 15:19:34,675] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.63345082727414, 0.4236386453326341, 0.0, 1.0, 65.0, 58.32420100297911], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2776200.0000, 
sim time next is 2776800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.53795914488665, 0.4411431344070988, 0.0, 1.0, 65.0, 74.943381502851], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6281632620738874, 0.647047711469033, 0.0, 1.0, 1.0, 0.7494338150285099], 
reward next is 0.2506, 
noisyNet noise sample is [array([1.8052627], dtype=float32), -0.95115817]. 
=============================================
[2019-04-09 15:19:34,783] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:19:34,784] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:19:34,784] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:19:34,784] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:19:34,785] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:19:34,785] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:19:34,786] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:19:34,791] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run22
[2019-04-09 15:19:34,810] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run22
[2019-04-09 15:19:34,821] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run22
[2019-04-09 15:21:00,301] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.03564351], dtype=float32), 0.040361874]
[2019-04-09 15:21:00,302] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-6.5, 56.5, 195.3333333333333, 451.3333333333333, 22.5, 26.3864430916524, 0.6465703172670499, 1.0, 1.0, 65.0, 51.97718817663566]
[2019-04-09 15:21:00,302] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:21:00,303] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [4.3666303e-05 1.2323445e-01 1.6533755e-01 1.6440157e-02 1.3280405e-03
 1.5367892e-04 2.9536396e-01 6.9418475e-03 2.9163569e-02 1.7771743e-02
 3.4422138e-01], sampled 0.8189213701777401
[2019-04-09 15:21:21,487] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5643.0003 285316.1694 2939.7668
[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,522] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:21,644] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:27,917] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5286.9824 320883.4565 2245.6156
[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:27,948] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:28,054] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,779] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5393.0721 310292.7903 2584.1352
[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:21:30,803] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 210000, evaluation results [210000.0, 5393.07209727898, 310292.790272101, 2584.135152212322, 5643.000276529517, 285316.16940595245, 2939.7667942067524, 5286.9823677453105, 320883.4565256822, 2245.6155651459867]
[2019-04-09 15:21:30,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3683132e-05 3.6912721e-02 1.2772928e-01 3.3957302e-03 5.9326174e-04
 9.5898686e-05 3.7155056e-01 9.5018577e-03 8.8844178e-03 8.8101542e-03
 4.3247238e-01], sum to 1.0000
[2019-04-09 15:21:30,857] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1142
[2019-04-09 15:21:30,873] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.08879602376062, 0.5759367377139317, 0.0, 1.0, 45.0, 44.50920975663738], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.03125535519192, 0.557729435733412, 0.0, 1.0, 25.0, 41.35627206023516], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6692712795993266, 0.6859098119111374, 0.0, 1.0, 0.2, 0.4135627206023516], 
reward next is 0.5864, 
noisyNet noise sample is [array([-2.2919073], dtype=float32), -0.9442766]. 
=============================================
[2019-04-09 15:21:31,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7211408e-05 1.0537438e-01 1.5272789e-01 1.6759399e-02 9.5058081e-04
 9.4019495e-05 2.3523019e-01 7.0296810e-03 1.7928051e-02 2.0319231e-02
 4.4353938e-01], sum to 1.0000
[2019-04-09 15:21:31,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2136
[2019-04-09 15:21:31,187] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.1149965506355, 0.3524739205242057, 0.0, 1.0, 45.0, 33.92033395674], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.97503555704522, 0.3254136598984704, 0.0, 1.0, 25.0, 28.45041651320586], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5812529630871017, 0.6084712199661567, 0.0, 1.0, 0.2, 0.28450416513205856], 
reward next is 0.7155, 
noisyNet noise sample is [array([-0.13096155], dtype=float32), -0.67195404]. 
=============================================
[2019-04-09 15:21:31,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2158073e-05 3.0432250e-02 1.2615661e-01 6.7624138e-03 1.0871097e-03
 1.3515544e-04 2.5246751e-01 8.0851829e-03 1.2479482e-02 1.5911017e-02
 5.4643112e-01], sum to 1.0000
[2019-04-09 15:21:31,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1517
[2019-04-09 15:21:31,441] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.56884403026361, 0.4390333458186697, 0.0, 1.0, 45.0, 32.07758851271856], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2767200.0000, 
sim time next is 2767800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.5596634455525, 0.4206598439867773, 0.0, 1.0, 25.0, 30.33677043016411], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6299719537960415, 0.6402199479955925, 0.0, 1.0, 0.2, 0.30336770430164106], 
reward next is 0.6966, 
noisyNet noise sample is [array([2.1163676], dtype=float32), -0.8832887]. 
=============================================
[2019-04-09 15:21:31,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3472574e-05 8.3760776e-02 1.1529278e-01 4.5522335e-03 1.1317809e-03
 3.1280251e-05 5.0192261e-01 7.4432548e-03 2.1945752e-02 2.3841962e-02
 2.4006414e-01], sum to 1.0000
[2019-04-09 15:21:31,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8450
[2019-04-09 15:21:31,530] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 63.16666666666667, 0.0, 0.0, 19.0, 26.24910730296408, 0.6465313505934489, 0.0, 1.0, 45.0, 40.58341231727422], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 19.0, 26.20637258194478, 0.6436384185512228, 0.0, 1.0, 65.0, 66.39514652450111], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.6838643818287317, 0.7145461395170742, 0.0, 1.0, 1.0, 0.6639514652450111], 
reward next is 0.3360, 
noisyNet noise sample is [array([-1.631594], dtype=float32), 1.3576627]. 
=============================================
[2019-04-09 15:21:31,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4457032e-05 5.5120066e-02 1.2919568e-01 1.2164209e-02 1.0078541e-03
 1.6717614e-04 2.7827999e-01 5.9786667e-03 7.8592850e-03 1.1266898e-02
 4.9891570e-01], sum to 1.0000
[2019-04-09 15:21:31,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4727
[2019-04-09 15:21:31,729] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 25.97387625241249, 0.5126809267826153, 0.0, 1.0, 65.0, 53.87290446098002], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2773200.0000, 
sim time next is 2773800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.00917517561052, 0.5217255442516141, 0.0, 1.0, 45.0, 45.49581830673296], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6674312646342099, 0.673908514750538, 0.0, 1.0, 0.6, 0.45495818306732955], 
reward next is 0.5450, 
noisyNet noise sample is [array([-0.9248365], dtype=float32), 2.3250659]. 
=============================================
[2019-04-09 15:21:31,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.4286581e-05 7.7039115e-02 1.6502079e-01 1.1071365e-02 1.1539534e-03
 1.3713581e-04 2.3590280e-01 8.4215589e-03 1.0696585e-02 2.9145459e-02
 4.6131685e-01], sum to 1.0000
[2019-04-09 15:21:31,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2879
[2019-04-09 15:21:31,924] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.5684595035066, 0.3911968652079096, 0.0, 1.0, 25.0, 68.33906243157342], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2789400.0000, 
sim time next is 2790000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.46984428676688, 0.3594619449874372, 0.0, 1.0, 45.0, 56.54677646202532], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6224870238972399, 0.6198206483291457, 0.0, 1.0, 0.6, 0.5654677646202532], 
reward next is 0.4345, 
noisyNet noise sample is [array([0.45218116], dtype=float32), -1.6815928]. 
=============================================
[2019-04-09 15:21:31,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[12.417688]
 [12.271995]
 [12.028468]
 [11.790503]
 [11.832145]], R is [[12.59834194]
 [12.78896809]
 [12.88728142]
 [13.02363873]
 [13.19025707]].
[2019-04-09 15:21:32,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6477061e-06 9.2277437e-02 1.4544418e-01 8.7453332e-03 5.3291878e-04
 5.1540072e-05 2.0893678e-01 3.8120241e-03 3.2155663e-02 1.8939292e-02
 4.8909914e-01], sum to 1.0000
[2019-04-09 15:21:32,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-09 15:21:32,036] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 52.0, 3.0, 53.0, 22.5, 27.01517863768764, 0.7123124528935559, 1.0, 1.0, 65.0, 69.92517198781418], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 22.5, 26.2890630737046, 0.7193675050105539, 1.0, 1.0, 65.0, 63.29378131136747], 
processed observation next is [1.0, 0.7391304347826086, 0.3610341643582641, 0.5266666666666667, 0.0, 0.0, 0.375, 0.69075525614205, 0.7397891683368513, 1.0, 1.0, 1.0, 0.6329378131136747], 
reward next is 0.3671, 
noisyNet noise sample is [array([0.04827603], dtype=float32), 0.4379459]. 
=============================================
[2019-04-09 15:21:32,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[16.101768]
 [16.055958]
 [16.228365]
 [16.275534]
 [16.251892]], R is [[16.31090546]
 [16.44854546]
 [16.82014084]
 [17.15859604]
 [17.52072144]].
[2019-04-09 15:21:32,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4718411e-04 5.8757540e-02 6.4041428e-02 2.1012656e-02 1.7315813e-03
 5.1249651e-04 3.6039519e-01 1.6918851e-02 3.2695130e-02 2.0878039e-02
 4.2290989e-01], sum to 1.0000
[2019-04-09 15:21:32,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1757
[2019-04-09 15:21:32,257] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 19.0, 26.02145881593145, 0.4919538982167839, 0.0, 1.0, 45.0, 39.95284389470958], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 25.99314352955428, 0.4762490902503176, 0.0, 1.0, 45.0, 37.76861324595829], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.6660952941295234, 0.6587496967501059, 0.0, 1.0, 0.6, 0.37768613245958294], 
reward next is 0.6223, 
noisyNet noise sample is [array([-0.23409024], dtype=float32), -0.29425743]. 
=============================================
[2019-04-09 15:21:32,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0096038e-05 5.3332698e-02 8.6139269e-02 8.4144901e-03 1.8159565e-04
 5.3922056e-05 4.9680781e-01 3.6286986e-03 1.1467806e-02 7.2379438e-03
 3.3270565e-01], sum to 1.0000
[2019-04-09 15:21:32,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9520
[2019-04-09 15:21:32,717] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 22.5, 26.54412608477072, 0.5955437370027993, 1.0, 1.0, 45.0, 39.28083904273419], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2884200.0000, 
sim time next is 2884800.0000, 
raw observation next is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 22.5, 26.50862610225732, 0.6104310486146624, 1.0, 1.0, 65.0, 65.90238814275601], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.9533333333333335, 0.19444444444444442, 0.028729281767955788, 0.375, 0.70905217518811, 0.7034770162048875, 1.0, 1.0, 1.0, 0.6590238814275601], 
reward next is 0.3410, 
noisyNet noise sample is [array([0.53482306], dtype=float32), -1.7817689]. 
=============================================
[2019-04-09 15:21:32,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6741554e-06 8.7715812e-02 1.3942082e-01 3.5043487e-03 1.7952772e-04
 2.4478693e-05 4.3679506e-01 3.2431646e-03 1.4541861e-02 6.8681701e-03
 3.0770409e-01], sum to 1.0000
[2019-04-09 15:21:32,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3230
[2019-04-09 15:21:32,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.933333333333334, 24.16666666666666, 102.6666666666667, 0.0, 22.5, 27.70215264094377, 0.8345511746813151, 1.0, 1.0, 65.0, 55.6720843301032], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2819400.0000, 
sim time next is 2820000.0000, 
raw observation next is [6.866666666666667, 24.33333333333333, 98.83333333333333, 0.0, 22.5, 27.71949263762318, 0.8397151280982023, 1.0, 1.0, 45.0, 44.49091739823243], 
processed observation next is [1.0, 0.6521739130434783, 0.6528162511542014, 0.2433333333333333, 0.32944444444444443, 0.0, 0.375, 0.8099577198019317, 0.7799050426994008, 1.0, 1.0, 0.6, 0.4449091739823243], 
reward next is 0.5551, 
noisyNet noise sample is [array([-0.7952144], dtype=float32), -0.5634429]. 
=============================================
[2019-04-09 15:21:32,893] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[16.630194]
 [16.619808]
 [16.512005]
 [16.447716]
 [16.748426]], R is [[17.04675484]
 [17.31956673]
 [17.79478455]
 [18.25086212]
 [18.74934959]].
[2019-04-09 15:21:33,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5217324e-05 1.3341603e-01 8.5278422e-02 6.7261383e-03 5.0790614e-04
 4.3117783e-05 4.4995910e-01 4.0724953e-03 1.4841886e-02 1.2678535e-02
 2.9246119e-01], sum to 1.0000
[2019-04-09 15:21:33,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9816
[2019-04-09 15:21:33,276] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 22.5, 27.16507829907484, 0.7352742721259874, 1.0, 1.0, 20.0, 34.65116442970469], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2803200.0000, 
sim time next is 2803800.0000, 
raw observation next is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 22.5, 27.18583494085175, 0.4855875534137032, 1.0, 1.0, 65.0, 57.54594970131019], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.5083333333333334, 0.5255555555555557, 0.6552486187845303, 0.375, 0.7654862450709791, 0.6618625178045677, 1.0, 1.0, 1.0, 0.575459497013102], 
reward next is 0.4245, 
noisyNet noise sample is [array([-0.37489504], dtype=float32), 2.425254]. 
=============================================
[2019-04-09 15:21:33,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5731466e-05 6.6364326e-02 1.6017224e-01 2.6921200e-02 8.1629894e-04
 1.2217574e-04 3.7317619e-01 1.0202949e-02 3.5118077e-02 1.5425364e-02
 3.1165546e-01], sum to 1.0000
[2019-04-09 15:21:33,440] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8145
[2019-04-09 15:21:33,458] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 22.5, 26.17893272888241, 0.5267819442666425, 1.0, 1.0, 65.0, 54.44990470326123], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 22.5, 26.40048965981653, 0.5658700125895274, 1.0, 1.0, 65.0, 46.46419634048413], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 0.375, 0.7000408049847108, 0.6886233375298425, 1.0, 1.0, 1.0, 0.4646419634048413], 
reward next is 0.5354, 
noisyNet noise sample is [array([0.72293663], dtype=float32), -0.6546673]. 
=============================================
[2019-04-09 15:21:33,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1351578e-05 6.4504616e-02 1.4911543e-01 2.7604526e-02 7.7609008e-04
 1.1349134e-04 3.8373771e-01 9.0154670e-03 3.5348125e-02 1.3400398e-02
 3.1636271e-01], sum to 1.0000
[2019-04-09 15:21:33,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3685
[2019-04-09 15:21:33,511] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 163.0, 370.5, 22.5, 26.40048965981653, 0.5658700125895274, 1.0, 1.0, 65.0, 46.46419634048413], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2800800.0000, 
sim time next is 2801400.0000, 
raw observation next is [-2.666666666666667, 54.16666666666667, 166.6666666666667, 416.3333333333333, 22.5, 26.59109153983466, 0.5984021675564031, 1.0, 1.0, 45.0, 40.76542089233182], 
processed observation next is [1.0, 0.43478260869565216, 0.38873499538319484, 0.5416666666666667, 0.5555555555555557, 0.460036832412523, 0.375, 0.7159242949862218, 0.6994673891854677, 1.0, 1.0, 0.6, 0.40765420892331816], 
reward next is 0.5923, 
noisyNet noise sample is [array([0.72293663], dtype=float32), -0.6546673]. 
=============================================
[2019-04-09 15:21:33,646] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3578955e-06 4.2759292e-02 8.2802676e-02 2.4060670e-03 2.8167971e-04
 2.2143437e-05 4.5808688e-01 1.6536639e-03 1.9413853e-02 8.9972075e-03
 3.8357010e-01], sum to 1.0000
[2019-04-09 15:21:33,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2198
[2019-04-09 15:21:33,686] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 22.5, 26.97698409722854, 0.7602487712669149, 1.0, 1.0, 25.0, 37.8577886128843], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2803800.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 149.5, 635.5, 22.5, 26.66320931760949, 0.7192644236149647, 1.0, 1.0, 45.0, 22.47762039272776], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.5, 0.49833333333333335, 0.7022099447513812, 0.375, 0.7219341098007908, 0.7397548078716549, 1.0, 1.0, 0.6, 0.22477620392727762], 
reward next is 0.7752, 
noisyNet noise sample is [array([-1.3789134], dtype=float32), 0.749958]. 
=============================================
[2019-04-09 15:21:33,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2192046e-06 4.6238787e-02 1.2102015e-01 1.9804202e-03 3.4342645e-04
 3.9977917e-06 5.0881320e-01 5.0666323e-03 8.6867511e-03 4.9244263e-03
 3.0291793e-01], sum to 1.0000
[2019-04-09 15:21:33,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3626
[2019-04-09 15:21:33,936] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 165.8333333333333, 0.0, 22.5, 27.57330783269478, 0.8737763514731146, 1.0, 1.0, 45.0, 26.8368621355804], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2899200.0000, 
sim time next is 2899800.0000, 
raw observation next is [2.0, 100.0, 164.0, 0.0, 22.5, 27.58762627919138, 0.713444849231439, 1.0, 1.0, 45.0, 48.35897822739543], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5466666666666666, 0.0, 0.375, 0.7989688565992816, 0.737814949743813, 1.0, 1.0, 0.6, 0.4835897822739543], 
reward next is 0.5164, 
noisyNet noise sample is [array([0.18952991], dtype=float32), 0.19341412]. 
=============================================
[2019-04-09 15:21:34,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7308062e-06 5.7573367e-02 7.9741865e-02 6.1909831e-03 1.7337492e-04
 1.2352773e-05 5.5027789e-01 2.7088039e-03 2.8557330e-02 4.7015515e-03
 2.7005669e-01], sum to 1.0000
[2019-04-09 15:21:34,747] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-09 15:21:34,767] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 24.5, 95.0, 0.0, 22.5, 27.58239109745421, 0.8057929684626427, 1.0, 1.0, 25.0, 35.63363458821979], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2820600.0000, 
sim time next is 2821200.0000, 
raw observation next is [6.733333333333333, 24.66666666666666, 91.0, 12.66666666666666, 22.5, 27.66752352790503, 0.8384305446831374, 1.0, 1.0, 45.0, 44.02650264355104], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.24666666666666662, 0.30333333333333334, 0.013996316758747691, 0.375, 0.8056269606587524, 0.7794768482277125, 1.0, 1.0, 0.6, 0.4402650264355104], 
reward next is 0.5597, 
noisyNet noise sample is [array([0.22022371], dtype=float32), -1.684494]. 
=============================================
[2019-04-09 15:21:34,792] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1408672e-05 3.9414797e-02 9.2466205e-02 1.1050530e-02 1.1034912e-03
 5.9761613e-05 3.6627516e-01 1.1019650e-02 2.2280196e-02 1.1339751e-02
 4.4496915e-01], sum to 1.0000
[2019-04-09 15:21:34,795] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8130
[2019-04-09 15:21:34,813] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 22.5, 26.45927875634978, 0.6275414421130862, 0.0, 1.0, 65.0, 70.35126812827069], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2877000.0000, 
sim time next is 2877600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 22.5, 26.46128593874508, 0.6246585962817978, 1.0, 1.0, 65.0, 58.20889492135456], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.375, 0.7051071615620899, 0.7082195320939326, 1.0, 1.0, 1.0, 0.5820889492135456], 
reward next is 0.4179, 
noisyNet noise sample is [array([1.0734617], dtype=float32), -1.3188186]. 
=============================================
[2019-04-09 15:21:35,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4851037e-06 3.8803987e-02 3.0034953e-01 7.4448641e-03 4.9453712e-04
 3.4080895e-05 3.0402738e-01 3.5274939e-03 1.0106487e-02 7.5066169e-03
 3.2770154e-01], sum to 1.0000
[2019-04-09 15:21:35,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1472
[2019-04-09 15:21:35,050] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [6.0, 28.0, 38.0, 61.0, 22.5, 27.72043078356774, 0.8476373843844379, 1.0, 1.0, 20.0, 24.87887639328875], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2826000.0000, 
sim time next is 2826600.0000, 
raw observation next is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 22.5, 27.85283961537027, 0.8587320903985859, 1.0, 1.0, 25.0, 29.1328653459093], 
processed observation next is [1.0, 0.7391304347826086, 0.6241920590951062, 0.2833333333333334, 0.08999999999999997, 0.061878453038674036, 0.375, 0.8210699679475226, 0.7862440301328619, 1.0, 1.0, 0.2, 0.291328653459093], 
reward next is 0.7087, 
noisyNet noise sample is [array([-1.563271], dtype=float32), -0.81687045]. 
=============================================
[2019-04-09 15:21:35,224] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5931236e-06 6.1920580e-02 7.3689513e-02 1.8814898e-03 2.7631182e-04
 1.4304107e-05 5.2233362e-01 7.9566417e-03 2.3844080e-02 7.6470566e-03
 3.0043378e-01], sum to 1.0000
[2019-04-09 15:21:35,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3640
[2019-04-09 15:21:35,248] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 89.66666666666667, 0.0, 0.0, 22.5, 27.01638119420536, 0.8640249044385953, 1.0, 1.0, 45.0, 30.67599503259948], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2920200.0000, 
sim time next is 2920800.0000, 
raw observation next is [-1.0, 87.33333333333334, 0.0, 0.0, 22.5, 26.98390298729796, 0.8448905546765083, 0.0, 1.0, 45.0, 29.94576816728884], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8733333333333334, 0.0, 0.0, 0.375, 0.74865858227483, 0.7816301848921694, 0.0, 1.0, 0.6, 0.2994576816728884], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.35178643], dtype=float32), -0.81266415]. 
=============================================
[2019-04-09 15:21:35,252] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1403047e-05 6.9514051e-02 2.1729410e-01 5.3943018e-03 1.0894560e-03
 5.9201604e-05 4.0144014e-01 5.1241294e-03 8.5412730e-03 1.6436284e-02
 2.7509561e-01], sum to 1.0000
[2019-04-09 15:21:35,252] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6339
[2019-04-09 15:21:35,268] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 22.5, 27.18052948245468, 0.7493607095832585, 1.0, 1.0, 45.0, 45.6295165100105], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2884800.0000, 
sim time next is 2885400.0000, 
raw observation next is [0.5, 96.5, 49.0, 0.0, 22.5, 27.23016027383953, 0.7624065833182055, 1.0, 1.0, 45.0, 41.86021143658514], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.965, 0.16333333333333333, 0.0, 0.375, 0.7691800228199609, 0.7541355277727352, 1.0, 1.0, 0.6, 0.4186021143658514], 
reward next is 0.5814, 
noisyNet noise sample is [array([-0.57746446], dtype=float32), -0.2349169]. 
=============================================
[2019-04-09 15:21:35,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2486640e-06 3.9211083e-02 6.3922741e-02 3.7470970e-03 2.8475988e-04
 4.2147138e-05 5.5378240e-01 2.3815839e-03 1.5824633e-02 3.6828308e-03
 3.1711647e-01], sum to 1.0000
[2019-04-09 15:21:35,849] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4601
[2019-04-09 15:21:35,861] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 22.5, 27.80057128425292, 0.8363480413573589, 1.0, 1.0, 45.0, 35.88152518898124], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2827200.0000, 
sim time next is 2827800.0000, 
raw observation next is [5.5, 29.0, 5.0, 46.0, 22.5, 27.67899878094201, 0.8300288553011618, 1.0, 1.0, 45.0, 33.99015973835446], 
processed observation next is [1.0, 0.7391304347826086, 0.6149584487534627, 0.29, 0.016666666666666666, 0.05082872928176796, 0.375, 0.8065832317451674, 0.7766762851003873, 1.0, 1.0, 0.6, 0.3399015973835446], 
reward next is 0.6601, 
noisyNet noise sample is [array([1.0495925], dtype=float32), 1.138979]. 
=============================================
[2019-04-09 15:21:36,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1802703e-06 4.5538153e-02 2.3935793e-01 4.7637839e-03 1.5245553e-04
 2.1907428e-05 2.1068078e-01 3.0419859e-03 3.0098114e-02 1.2901279e-02
 4.5344141e-01], sum to 1.0000
[2019-04-09 15:21:36,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-09 15:21:36,174] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 100.0, 88.33333333333334, 0.0, 22.5, 27.62385319057181, 0.898980498202234, 1.0, 1.0, 25.0, 27.89828350525804], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [2.0, 100.0, 87.5, 0.0, 22.5, 27.64515933581587, 0.9067352217791177, 1.0, 1.0, 65.0, 44.97066429013786], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2916666666666667, 0.0, 0.375, 0.8037632779846557, 0.8022450739263726, 1.0, 1.0, 1.0, 0.44970664290137863], 
reward next is 0.5503, 
noisyNet noise sample is [array([1.8570337], dtype=float32), 0.21671407]. 
=============================================
[2019-04-09 15:21:36,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0153472e-05 4.7538549e-02 1.3711335e-01 2.8039438e-03 4.9798755e-04
 5.4396027e-05 2.4635851e-01 3.4119745e-03 5.7585728e-03 7.1200114e-03
 5.4933256e-01], sum to 1.0000
[2019-04-09 15:21:36,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3181
[2019-04-09 15:21:36,225] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.65219980979761, 0.7098172268805697, 0.0, 1.0, 65.0, 48.54063679912944], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2842800.0000, 
sim time next is 2843400.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 26.65413295521195, 0.7103230248225448, 0.0, 1.0, 45.0, 42.06167889753301], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.7211777462676624, 0.7367743416075149, 0.0, 1.0, 0.6, 0.42061678897533006], 
reward next is 0.5794, 
noisyNet noise sample is [array([0.36804372], dtype=float32), 2.556468]. 
=============================================
[2019-04-09 15:21:36,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4585799e-05 7.4276745e-02 1.6172929e-01 4.1915271e-03 1.1599159e-03
 1.5061218e-04 4.6751997e-01 4.4968198e-03 1.4852905e-02 7.8254994e-03
 2.6375207e-01], sum to 1.0000
[2019-04-09 15:21:36,937] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1932
[2019-04-09 15:21:36,955] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 26.3523207300404, 0.606506550869575, 0.0, 1.0, 65.0, 72.80266447564775], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 19.0, 26.31195321455578, 0.6078061863606887, 0.0, 1.0, 25.0, 59.07068687479691], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.08333333333333333, 0.6926627678796482, 0.7026020621202296, 0.0, 1.0, 0.2, 0.5907068687479691], 
reward next is 0.4093, 
noisyNet noise sample is [array([-0.19614962], dtype=float32), -0.14434126]. 
=============================================
[2019-04-09 15:21:37,154] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1522584e-06 5.9386041e-02 7.7858947e-02 7.2559859e-03 1.1008232e-04
 5.2465775e-06 5.6144881e-01 3.3609159e-03 1.1843095e-02 3.9331554e-03
 2.7479252e-01], sum to 1.0000
[2019-04-09 15:21:37,155] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-09 15:21:37,182] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 22.5, 27.385959506259, 0.9017851356827343, 1.0, 1.0, 65.0, 37.36069702679085], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2916000.0000, 
sim time next is 2916600.0000, 
raw observation next is [0.6666666666666667, 92.83333333333333, 0.0, 0.0, 22.5, 27.28969499453703, 0.9102075125356773, 1.0, 1.0, 45.0, 34.0532503755669], 
processed observation next is [1.0, 0.782608695652174, 0.4810710987996307, 0.9283333333333332, 0.0, 0.0, 0.375, 0.7741412495447525, 0.8034025041785591, 1.0, 1.0, 0.6, 0.34053250375566896], 
reward next is 0.6595, 
noisyNet noise sample is [array([0.3275009], dtype=float32), -0.8918127]. 
=============================================
[2019-04-09 15:21:37,512] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3638735e-05 3.9606392e-02 1.2323065e-01 4.4457749e-03 3.1211160e-04
 7.6350574e-05 4.4159892e-01 6.5069627e-03 1.6342094e-02 7.8323213e-03
 3.6001477e-01], sum to 1.0000
[2019-04-09 15:21:37,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5148
[2019-04-09 15:21:37,531] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 19.0, 26.42983257628266, 0.6186167488673964, 0.0, 1.0, 65.0, 58.06142870652835], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2854800.0000, 
sim time next is 2855400.0000, 
raw observation next is [1.0, 73.16666666666667, 0.0, 0.0, 19.0, 26.46257622266424, 0.6208921136639896, 0.0, 1.0, 45.0, 50.0297920708859], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7316666666666667, 0.0, 0.0, 0.08333333333333333, 0.7052146852220199, 0.7069640378879966, 0.0, 1.0, 0.6, 0.5002979207088589], 
reward next is 0.4997, 
noisyNet noise sample is [array([-2.0373914], dtype=float32), 0.19681603]. 
=============================================
[2019-04-09 15:21:37,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0593991e-06 3.9479174e-02 1.0376080e-01 4.2476272e-03 6.4912002e-04
 4.4077355e-05 3.9650866e-01 4.8604771e-03 5.2946652e-03 6.1146640e-03
 4.3903258e-01], sum to 1.0000
[2019-04-09 15:21:37,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6141
[2019-04-09 15:21:37,633] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 22.5, 27.07845643720606, 0.7243789492379843, 1.0, 1.0, 45.0, 41.8534309336553], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2884200.0000, 
sim time next is 2884800.0000, 
raw observation next is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 22.5, 27.12589422544661, 0.7379610886639614, 1.0, 1.0, 65.0, 53.25999061803529], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.9533333333333335, 0.19444444444444442, 0.028729281767955788, 0.375, 0.7604911854538843, 0.7459870295546538, 1.0, 1.0, 1.0, 0.5325999061803529], 
reward next is 0.4674, 
noisyNet noise sample is [array([-1.9154587], dtype=float32), -0.06162223]. 
=============================================
[2019-04-09 15:21:37,634] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9008529e-05 4.1665424e-02 1.0938111e-01 5.0481125e-03 1.6091280e-03
 7.6754317e-05 4.9230960e-01 6.9527156e-03 1.6850352e-02 2.1101668e-02
 3.0493617e-01], sum to 1.0000
[2019-04-09 15:21:37,637] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9682
[2019-04-09 15:21:37,660] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 26.15668523059644, 0.5657778995952997, 0.0, 1.0, 45.0, 59.11410281099274], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 26.21467537969176, 0.582710905756375, 0.0, 1.0, 65.0, 66.6641504332343], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.6845562816409799, 0.6942369685854582, 0.0, 1.0, 1.0, 0.6666415043323429], 
reward next is 0.3334, 
noisyNet noise sample is [array([0.5885036], dtype=float32), -0.055687934]. 
=============================================
[2019-04-09 15:21:38,128] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5698920e-04 7.4629165e-02 1.3712022e-01 2.4569176e-02 1.7071157e-03
 3.8292498e-04 3.6282906e-01 1.0022046e-02 2.0891644e-02 3.7288658e-02
 3.3040294e-01], sum to 1.0000
[2019-04-09 15:21:38,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-09 15:21:38,151] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.0, 71.0, 166.0, 78.0, 19.0, 25.29635819007094, 0.4679214077769825, 0.0, 1.0, 45.0, 36.69823168734007], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2973600.0000, 
sim time next is 2974200.0000, 
raw observation next is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 19.0, 25.27428589284127, 0.472864705658449, 0.0, 1.0, 60.0, 55.40903792514224], 
processed observation next is [0.0, 0.43478260869565216, 0.3564173591874424, 0.7, 0.5666666666666667, 0.06629834254143646, 0.08333333333333333, 0.6061904910701058, 0.6576215685528163, 0.0, 1.0, 0.9, 0.5540903792514225], 
reward next is 0.4459, 
noisyNet noise sample is [array([1.091878], dtype=float32), 1.4798288]. 
=============================================
[2019-04-09 15:21:38,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1963578e-05 3.3171110e-02 6.4828634e-02 4.5641968e-03 4.7313893e-04
 3.9879509e-05 2.2065932e-01 1.6524506e-03 9.5108617e-03 7.5100041e-03
 6.5757853e-01], sum to 1.0000
[2019-04-09 15:21:38,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-09 15:21:38,397] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.39012809392466, 0.6984815858016041, 0.0, 1.0, 65.0, 51.72204434298344], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.37805386430983, 0.7000491663774658, 0.0, 1.0, 65.0, 51.53983167245936], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6981711553591525, 0.733349722125822, 0.0, 1.0, 1.0, 0.5153983167245936], 
reward next is 0.4846, 
noisyNet noise sample is [array([-0.10473016], dtype=float32), 0.4672677]. 
=============================================
[2019-04-09 15:21:38,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3472544e-06 3.2681625e-02 4.3294799e-02 2.2717097e-03 9.5139338e-05
 1.4989423e-05 3.2803455e-01 9.2758395e-04 5.8242162e-03 2.0139886e-03
 5.8483499e-01], sum to 1.0000
[2019-04-09 15:21:38,426] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-09 15:21:38,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 22.5, 26.86786244332422, 0.8394267388761912, 0.0, 1.0, 45.0, 38.48652482196157], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [-1.0, 79.16666666666667, 0.0, 0.0, 19.0, 26.821399476751, 0.8354937318574812, 1.0, 1.0, 45.0, 37.52314508413605], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.7916666666666667, 0.0, 0.0, 0.08333333333333333, 0.7351166230625834, 0.7784979106191604, 1.0, 1.0, 0.6, 0.37523145084136045], 
reward next is 0.6248, 
noisyNet noise sample is [array([-0.96075106], dtype=float32), 0.9037127]. 
=============================================
[2019-04-09 15:21:38,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.73803539e-06 4.03609797e-02 1.03083074e-01 5.33895893e-03
 2.60933622e-04 1.62016731e-05 3.09587359e-01 1.69712282e-03
 9.70130228e-03 8.94861016e-03 5.21002769e-01], sum to 1.0000
[2019-04-09 15:21:38,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4733
[2019-04-09 15:21:38,614] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 19.0, 26.78657777311947, 0.827329355748974, 0.0, 1.0, 65.0, 41.68828008554751], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2926200.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 19.0, 26.77310654648242, 0.8226800108707607, 0.0, 1.0, 60.0, 38.89401808596315], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7310922122068684, 0.7742266702902536, 0.0, 1.0, 0.9, 0.38894018085963145], 
reward next is 0.6111, 
noisyNet noise sample is [array([1.9338112], dtype=float32), 1.0463728]. 
=============================================
[2019-04-09 15:21:38,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4040685e-06 8.0863163e-02 2.1569577e-01 3.7012482e-03 6.3571142e-04
 4.4416840e-05 4.3475780e-01 2.8364791e-03 1.2882993e-02 1.0563952e-02
 2.3801008e-01], sum to 1.0000
[2019-04-09 15:21:38,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8397
[2019-04-09 15:21:38,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 22.5, 27.24969865089848, 0.7573604393122347, 1.0, 1.0, 45.0, 37.40534950464202], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 22.5, 27.28445617683629, 0.7647246740402801, 1.0, 1.0, 65.0, 40.99017680756837], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 0.375, 0.7737046814030242, 0.7549082246800934, 1.0, 1.0, 1.0, 0.4099017680756837], 
reward next is 0.5901, 
noisyNet noise sample is [array([0.33560652], dtype=float32), 0.60918194]. 
=============================================
[2019-04-09 15:21:38,888] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.14094518e-06 4.57641669e-02 1.11235246e-01 1.91987504e-03
 8.93415854e-05 9.70242854e-06 2.71143585e-01 2.38433923e-03
 1.78360809e-02 4.72569466e-03 5.44888794e-01], sum to 1.0000
[2019-04-09 15:21:38,890] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6304
[2019-04-09 15:21:38,932] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 100.0, 173.1666666666667, 0.0, 22.5, 27.3351063751582, 0.8266906095487067, 1.0, 1.0, 20.0, 27.99194918801757], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2896800.0000, 
sim time next is 2897400.0000, 
raw observation next is [1.833333333333333, 100.0, 171.3333333333333, 0.0, 22.5, 27.38994606465814, 0.6859568538803767, 1.0, 1.0, 65.0, 70.24215559162924], 
processed observation next is [1.0, 0.5217391304347826, 0.5133887349953832, 1.0, 0.5711111111111109, 0.0, 0.375, 0.7824955053881784, 0.7286522846267922, 1.0, 1.0, 1.0, 0.7024215559162924], 
reward next is 0.2976, 
noisyNet noise sample is [array([-1.7215459], dtype=float32), -0.5692786]. 
=============================================
[2019-04-09 15:21:38,972] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3347816e-05 7.2249964e-02 1.2455075e-01 6.6993725e-03 3.0353718e-04
 3.7521750e-05 4.2130837e-01 5.6160116e-03 9.7098788e-03 9.6113523e-03
 3.4987983e-01], sum to 1.0000
[2019-04-09 15:21:38,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0894
[2019-04-09 15:21:39,008] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.833333333333333, 93.0, 70.00000000000001, 113.0, 22.5, 26.06468797746277, 0.5149273634388519, 1.0, 1.0, 45.0, 39.94822876802274], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2880600.0000, 
sim time next is 2881200.0000, 
raw observation next is [1.666666666666667, 93.0, 87.5, 134.5, 22.5, 25.94814841269611, 0.5497857372634852, 1.0, 1.0, 25.0, 41.26797100310974], 
processed observation next is [1.0, 0.34782608695652173, 0.5087719298245615, 0.93, 0.2916666666666667, 0.14861878453038674, 0.375, 0.6623457010580092, 0.6832619124211617, 1.0, 1.0, 0.2, 0.41267971003109744], 
reward next is 0.5873, 
noisyNet noise sample is [array([0.89007586], dtype=float32), 0.8265202]. 
=============================================
[2019-04-09 15:21:39,058] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.85719465e-06 9.48682353e-02 1.12744376e-01 3.48439184e-03
 1.35882612e-04 1.74799079e-05 3.44937384e-01 4.26060241e-03
 1.14681041e-02 2.63974350e-02 4.01681304e-01], sum to 1.0000
[2019-04-09 15:21:39,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5420
[2019-04-09 15:21:39,098] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 98.83333333333334, 116.3333333333333, 0.0, 22.5, 27.38829430579684, 0.8050961314276969, 1.0, 1.0, 65.0, 40.56964015336804], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2893800.0000, 
sim time next is 2894400.0000, 
raw observation next is [1.0, 100.0, 131.0, 0.0, 22.5, 27.40913538957788, 0.8112713708797702, 1.0, 1.0, 45.0, 36.85689028635176], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 1.0, 0.43666666666666665, 0.0, 0.375, 0.7840946157981566, 0.7704237902932567, 1.0, 1.0, 0.6, 0.36856890286351757], 
reward next is 0.6314, 
noisyNet noise sample is [array([-0.5203273], dtype=float32), -1.1771286]. 
=============================================
[2019-04-09 15:21:39,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3251135e-04 6.0579993e-02 1.0844335e-01 1.9111564e-02 1.9018791e-03
 4.3324003e-04 4.1704780e-01 1.4138958e-02 1.7939541e-02 2.7050525e-02
 3.3302057e-01], sum to 1.0000
[2019-04-09 15:21:39,216] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7607
[2019-04-09 15:21:39,231] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 19.0, 26.17546073504151, 0.6278401619413919, 0.0, 1.0, 65.0, 53.9782704969469], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2958600.0000, 
sim time next is 2959200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.16997352328821, 0.6262713820208765, 0.0, 1.0, 65.0, 53.6476706181274], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6808311269406842, 0.7087571273402921, 0.0, 1.0, 1.0, 0.536476706181274], 
reward next is 0.4635, 
noisyNet noise sample is [array([0.999037], dtype=float32), 2.3176436]. 
=============================================
[2019-04-09 15:21:39,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4711178e-05 2.0252766e-02 1.7069018e-01 3.2875359e-03 1.2495829e-03
 1.3097926e-04 4.0798560e-01 4.6482505e-03 8.7244054e-03 4.1912895e-02
 3.4108311e-01], sum to 1.0000
[2019-04-09 15:21:39,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-09 15:21:39,332] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.23891751060555, 0.6828227533804929, 0.0, 1.0, 45.0, 35.86548271920747], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2940600.0000, 
sim time next is 2941200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.25682019252632, 0.6649701724108711, 0.0, 1.0, 45.0, 34.16774549679612], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6880683493771933, 0.721656724136957, 0.0, 1.0, 0.6, 0.3416774549679612], 
reward next is 0.6583, 
noisyNet noise sample is [array([0.24260947], dtype=float32), 0.44453478]. 
=============================================
[2019-04-09 15:21:39,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.67899327e-06 4.35226932e-02 1.12965524e-01 3.42789060e-03
 2.41015499e-04 3.43702450e-05 2.11996898e-01 1.72497204e-03
 7.05702696e-03 1.39705781e-02 6.05056345e-01], sum to 1.0000
[2019-04-09 15:21:39,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5141234e-05 6.8064153e-02 1.4764367e-01 7.2948672e-03 3.3556097e-04
 1.6813896e-05 5.1577622e-01 3.0830465e-03 8.9030890e-03 1.3440424e-02
 2.3542698e-01], sum to 1.0000
[2019-04-09 15:21:39,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2951
[2019-04-09 15:21:39,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1960
[2019-04-09 15:21:39,390] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 22.5, 26.85898689523438, 0.6978821867476893, 1.0, 1.0, 45.0, 47.53706508354311], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2882400.0000, 
sim time next is 2883000.0000, 
raw observation next is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 22.5, 26.95228229068692, 0.6973514990188598, 1.0, 1.0, 25.0, 46.67234594925444], 
processed observation next is [1.0, 0.34782608695652173, 0.49492151431209613, 0.93, 0.2877777777777778, 0.11491712707182321, 0.375, 0.74602352422391, 0.7324504996729533, 1.0, 1.0, 0.2, 0.4667234594925444], 
reward next is 0.5333, 
noisyNet noise sample is [array([0.38979384], dtype=float32), 1.1529496]. 
=============================================
[2019-04-09 15:21:39,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[15.80675 ]
 [15.560736]
 [15.622442]
 [15.103073]
 [14.772255]], R is [[16.1869545 ]
 [16.54971504]
 [16.87425423]
 [17.20012856]
 [17.35002708]].
[2019-04-09 15:21:39,400] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 90.0, 0.0, 22.5, 27.59309962745986, 0.8874084992358927, 1.0, 1.0, 65.0, 29.46440018659855], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2903400.0000, 
sim time next is 2904000.0000, 
raw observation next is [2.0, 100.0, 89.16666666666666, 0.0, 22.5, 27.60728920157365, 0.7455324449740192, 1.0, 1.0, 45.0, 68.11423946442738], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29722222222222217, 0.0, 0.375, 0.8006074334644708, 0.7485108149913398, 1.0, 1.0, 0.6, 0.6811423946442738], 
reward next is 0.3189, 
noisyNet noise sample is [array([-0.2904275], dtype=float32), 0.1697995]. 
=============================================
[2019-04-09 15:21:39,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[16.958582]
 [17.010174]
 [16.695353]
 [16.93073 ]
 [16.705889]], R is [[17.13032722]
 [17.66438103]
 [18.16592026]
 [18.6839447 ]
 [19.21012115]].
[2019-04-09 15:21:39,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2872288e-04 6.3511796e-02 1.9131209e-01 3.3069622e-02 1.2985382e-03
 1.0834720e-03 3.5033393e-01 1.0780640e-02 1.9745940e-02 2.3657240e-02
 3.0487800e-01], sum to 1.0000
[2019-04-09 15:21:39,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7211
[2019-04-09 15:21:39,442] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 25.86443747421292, 0.5669403373861207, 0.0, 1.0, 25.0, 49.13018752054826], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2958000.0000, 
sim time next is 2958600.0000, 
raw observation next is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 19.0, 25.84075909820803, 0.5742907771428397, 0.0, 1.0, 65.0, 55.7829572892583], 
processed observation next is [0.0, 0.21739130434782608, 0.3564173591874424, 0.7816666666666667, 0.0, 0.0, 0.08333333333333333, 0.6533965915173358, 0.6914302590476132, 0.0, 1.0, 1.0, 0.557829572892583], 
reward next is 0.4422, 
noisyNet noise sample is [array([0.8322376], dtype=float32), -0.4799843]. 
=============================================
[2019-04-09 15:21:39,964] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.17108786e-04 1.15422294e-01 1.59453824e-01 1.12454752e-02
 3.30329640e-03 2.04231212e-04 3.35550487e-01 1.19179152e-02
 2.95152608e-02 2.08294690e-02 3.12440574e-01], sum to 1.0000
[2019-04-09 15:21:39,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1471
[2019-04-09 15:21:39,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2302296e-04 7.3583446e-02 1.3573241e-01 5.9978380e-03 1.8972461e-03
 2.2354274e-04 4.5262492e-01 5.9595173e-03 2.4628188e-02 1.5266275e-02
 2.8386360e-01], sum to 1.0000
[2019-04-09 15:21:39,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6157
[2019-04-09 15:21:39,979] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.64195057067784, 0.7017030807995986, 0.0, 1.0, 45.0, 30.35139919026899], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.56161978462715, 0.6799918559309414, 0.0, 1.0, 25.0, 28.88031763293324], 
processed observation next is [0.0, 0.8260869565217391, 0.3979686057248385, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.713468315385596, 0.7266639519769805, 0.0, 1.0, 0.2, 0.2888031763293324], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.49679682], dtype=float32), 0.43639103]. 
=============================================
[2019-04-09 15:21:39,988] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.25, 65.0, 0.0, 0.0, 19.0, 26.49809482138197, 0.6514903631055806, 0.0, 1.0, 45.0, 38.5308401505139], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3011400.0000, 
sim time next is 3012000.0000, 
raw observation next is [-3.333333333333333, 65.0, 0.0, 0.0, 19.0, 26.4776883512133, 0.6449206649550269, 0.0, 1.0, 45.0, 36.91271988598826], 
processed observation next is [0.0, 0.8695652173913043, 0.37026777469990774, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7064740292677749, 0.714973554985009, 0.0, 1.0, 0.6, 0.3691271988598826], 
reward next is 0.6309, 
noisyNet noise sample is [array([-0.40211844], dtype=float32), 1.0421587]. 
=============================================
[2019-04-09 15:21:39,996] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[11.325377]
 [11.594746]
 [11.636972]
 [11.787895]
 [11.64626 ]], R is [[11.95318604]
 [12.44834614]
 [12.94063663]
 [13.3962307 ]
 [13.80366707]].
[2019-04-09 15:21:40,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2378501e-05 7.1377590e-02 1.6186853e-01 3.2452962e-03 3.2960341e-04
 2.0783615e-05 3.6720914e-01 8.4126461e-03 1.4082687e-02 8.6747250e-03
 3.6476663e-01], sum to 1.0000
[2019-04-09 15:21:40,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1366
[2019-04-09 15:21:40,422] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 96.5, 87.0, 0.0, 22.5, 27.429849511839, 0.8060732654234477, 1.0, 1.0, 65.0, 38.71528369285496], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2892600.0000, 
sim time next is 2893200.0000, 
raw observation next is [1.0, 97.66666666666666, 101.6666666666667, 0.0, 22.5, 27.46098940873856, 0.8123199837411258, 1.0, 1.0, 25.0, 34.43778533556131], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9766666666666666, 0.338888888888889, 0.0, 0.375, 0.7884157840615466, 0.7707733279137087, 1.0, 1.0, 0.2, 0.3443778533556131], 
reward next is 0.6556, 
noisyNet noise sample is [array([-0.33540684], dtype=float32), 0.6983907]. 
=============================================
[2019-04-09 15:21:40,426] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2121547e-04 6.3252978e-02 1.4162900e-01 1.1878093e-02 1.1462580e-03
 2.0541447e-04 3.1197152e-01 1.4314112e-02 2.6739981e-02 2.0664508e-02
 4.0807688e-01], sum to 1.0000
[2019-04-09 15:21:40,430] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7364
[2019-04-09 15:21:40,463] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 19.0, 25.88510248976176, 0.6087425646250604, 0.0, 1.0, 25.0, 40.0895848173821], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [-3.0, 65.0, 230.0, 197.3333333333333, 19.0, 25.90196555842416, 0.6305512658802804, 0.0, 1.0, 65.0, 62.23094954402683], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7666666666666667, 0.21804788213627987, 0.08333333333333333, 0.6584971298686799, 0.7101837552934268, 0.0, 1.0, 1.0, 0.6223094954402683], 
reward next is 0.3777, 
noisyNet noise sample is [array([0.31695327], dtype=float32), -0.73763597]. 
=============================================
[2019-04-09 15:21:40,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5030622e-05 7.3989078e-02 1.3355146e-01 5.5157109e-03 3.8790019e-04
 4.5895376e-05 2.6135045e-01 2.5070696e-03 7.4831895e-03 1.1779512e-02
 5.0337476e-01], sum to 1.0000
[2019-04-09 15:21:40,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-09 15:21:40,828] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 19.0, 26.63272501609893, 0.7557198632423291, 0.0, 1.0, 65.0, 52.1166924325984], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2931600.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 19.0, 26.56817864082858, 0.7517411117195335, 0.0, 1.0, 45.0, 44.65335414558549], 
processed observation next is [1.0, 0.9565217391304348, 0.4210526315789474, 0.815, 0.0, 0.0, 0.08333333333333333, 0.7140148867357151, 0.7505803705731778, 0.0, 1.0, 0.6, 0.4465335414558549], 
reward next is 0.5535, 
noisyNet noise sample is [array([-0.6201528], dtype=float32), -0.5958178]. 
=============================================
[2019-04-09 15:21:40,994] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00036696 0.06907462 0.30024454 0.02026751 0.00278714 0.0008561
 0.32175717 0.01862901 0.03076227 0.035097   0.20015769], sum to 1.0000
[2019-04-09 15:21:40,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3040
[2019-04-09 15:21:41,001] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8083129e-05 9.1789611e-02 1.4120212e-01 2.3775950e-02 2.3026601e-03
 4.9855874e-04 2.9866281e-01 7.0042866e-03 2.4773283e-02 2.7925156e-02
 3.8197750e-01], sum to 1.0000
[2019-04-09 15:21:41,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5336
[2019-04-09 15:21:41,018] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 101.0, 751.0, 19.0, 25.71630536730506, 0.6230312751119812, 0.0, 1.0, 55.0, 38.49701108464183], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [-2.0, 60.0, 98.0, 737.0, 19.0, 25.75772794310167, 0.631739190708296, 0.0, 1.0, 45.0, 36.25540075905092], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.32666666666666666, 0.8143646408839779, 0.08333333333333333, 0.6464773285918058, 0.7105797302360987, 0.0, 1.0, 0.6, 0.3625540075905092], 
reward next is 0.6374, 
noisyNet noise sample is [array([-0.16368926], dtype=float32), 0.60739243]. 
=============================================
[2019-04-09 15:21:41,021] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 24.69226386581117, 0.3413832812499386, 0.0, 1.0, 20.0, 26.39872946146991], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2948400.0000, 
sim time next is 2949000.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 24.61271614866573, 0.3186834172083677, 0.0, 1.0, 25.0, 25.61520732893014], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.5510596790554775, 0.6062278057361226, 0.0, 1.0, 0.2, 0.2561520732893014], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.5111368], dtype=float32), 0.9214715]. 
=============================================
[2019-04-09 15:21:41,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[ 9.934901 ]
 [10.1750555]
 [10.579665 ]
 [10.646152 ]
 [11.036849 ]], R is [[10.24033737]
 [10.87394619]
 [11.4888134 ]
 [11.94747639]
 [12.428792  ]].
[2019-04-09 15:21:41,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2701053e-05 9.7057439e-02 1.5902905e-01 6.4541544e-03 8.4680715e-04
 6.9866583e-05 2.0080870e-01 3.1314348e-03 1.1692766e-02 2.2188818e-02
 4.9867827e-01], sum to 1.0000
[2019-04-09 15:21:41,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5028
[2019-04-09 15:21:41,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2576724e-04 6.6554710e-02 2.3208031e-01 2.1408604e-02 9.8415010e-04
 4.9479993e-04 2.5357819e-01 7.4513820e-03 2.0721737e-02 3.0643433e-02
 3.6595684e-01], sum to 1.0000
[2019-04-09 15:21:41,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4053
[2019-04-09 15:21:41,041] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 19.0, 26.76234426905777, 0.8129768783717273, 0.0, 1.0, 25.0, 34.57254115796204], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 19.0, 26.82390947994421, 0.8213492411134736, 0.0, 1.0, 65.0, 37.32472995205271], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.08333333333333333, 0.7353257899953508, 0.773783080371158, 0.0, 1.0, 1.0, 0.37324729952052704], 
reward next is 0.6268, 
noisyNet noise sample is [array([-0.05117992], dtype=float32), -0.07788119]. 
=============================================
[2019-04-09 15:21:41,058] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 66.0, 0.0, 0.0, 19.0, 26.25103513509499, 0.5865992090003886, 0.0, 1.0, 30.0, 44.46656860204681], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3024600.0000, 
sim time next is 3025200.0000, 
raw observation next is [-4.333333333333334, 67.0, 0.0, 0.0, 19.0, 26.25879843058181, 0.5803124045574249, 0.0, 1.0, 45.0, 35.37896419325729], 
processed observation next is [0.0, 0.0, 0.3425669436749769, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6882332025484841, 0.6934374681858083, 0.0, 1.0, 0.6, 0.3537896419325729], 
reward next is 0.6462, 
noisyNet noise sample is [array([-0.94518167], dtype=float32), -1.6800134]. 
=============================================
[2019-04-09 15:21:41,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.48427646e-07 4.56048138e-02 1.20678574e-01 4.17714939e-03
 1.14020448e-04 2.57474330e-05 4.94968534e-01 9.43358114e-04
 6.61010854e-03 6.35078363e-03 3.20526004e-01], sum to 1.0000
[2019-04-09 15:21:41,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6565
[2019-04-09 15:21:41,099] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 88.33333333333334, 0.0, 22.5, 27.64012929237602, 0.9009190541366525, 1.0, 1.0, 45.0, 25.13943349232186], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [2.0, 100.0, 87.5, 0.0, 22.5, 27.64605506287928, 0.9068051892752308, 1.0, 1.0, 45.0, 25.97684844258298], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2916666666666667, 0.0, 0.375, 0.8038379219066067, 0.802268396425077, 1.0, 1.0, 0.6, 0.2597684844258298], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.46598476], dtype=float32), -0.4420681]. 
=============================================
[2019-04-09 15:21:41,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1472370e-04 3.7420705e-02 1.5513998e-01 1.6057508e-02 1.2534226e-03
 3.3563277e-04 4.3588498e-01 8.7727439e-03 3.7174381e-02 1.5396708e-02
 2.9244921e-01], sum to 1.0000
[2019-04-09 15:21:41,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1917
[2019-04-09 15:21:41,309] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 19.0, 25.66233656316597, 0.5505501487751095, 0.0, 1.0, 60.0, 51.07477012630094], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [-3.666666666666667, 69.0, 174.0, 42.0, 19.0, 25.73089764420486, 0.5551577120832565, 0.0, 1.0, 25.0, 46.27896327546868], 
processed observation next is [0.0, 0.43478260869565216, 0.3610341643582641, 0.69, 0.58, 0.04640883977900553, 0.08333333333333333, 0.6442414703504049, 0.6850525706944189, 0.0, 1.0, 0.2, 0.4627896327546868], 
reward next is 0.5372, 
noisyNet noise sample is [array([-0.38518447], dtype=float32), -0.4144761]. 
=============================================
[2019-04-09 15:21:41,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.49437440e-04 1.11128688e-01 1.02610566e-01 1.73679069e-02
 3.18173412e-03 4.26553568e-04 4.38338578e-01 1.24231046e-02
 3.08114681e-02 1.80717688e-02 2.65290201e-01], sum to 1.0000
[2019-04-09 15:21:41,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6961
[2019-04-09 15:21:41,607] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 19.0, 25.64604782897053, 0.5144955677921134, 0.0, 1.0, 45.0, 29.00576108789978], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2946000.0000, 
sim time next is 2946600.0000, 
raw observation next is [-2.5, 84.5, 0.0, 0.0, 19.0, 25.49845512870828, 0.5040850912270719, 0.0, 1.0, 55.0, 50.40499898097675], 
processed observation next is [0.0, 0.08695652173913043, 0.39335180055401664, 0.845, 0.0, 0.0, 0.08333333333333333, 0.62487126072569, 0.6680283637423573, 0.0, 1.0, 0.8, 0.5040499898097675], 
reward next is 0.4960, 
noisyNet noise sample is [array([0.3687598], dtype=float32), 0.24963368]. 
=============================================
[2019-04-09 15:21:41,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5052038e-05 3.6518119e-02 1.5784840e-01 7.4642198e-03 1.2945819e-03
 1.5682132e-04 1.7481999e-01 5.8385171e-03 2.5974397e-02 6.9657699e-03
 5.8307409e-01], sum to 1.0000
[2019-04-09 15:21:41,689] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4478
[2019-04-09 15:21:41,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.14339858157495, 0.678350424673806, 0.0, 1.0, 65.0, 67.4162101450331], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2943600.0000, 
sim time next is 2944200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.20681731480598, 0.6813224979962712, 0.0, 1.0, 65.0, 54.1583654231894], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6839014429004985, 0.7271074993320904, 0.0, 1.0, 1.0, 0.541583654231894], 
reward next is 0.4584, 
noisyNet noise sample is [array([0.74500954], dtype=float32), 0.7723682]. 
=============================================
[2019-04-09 15:21:41,867] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7754356e-04 1.1305612e-01 1.2231059e-01 1.6540570e-02 3.7295062e-03
 6.5433246e-04 3.9691836e-01 1.6668875e-02 2.5597529e-02 2.9586691e-02
 2.7455989e-01], sum to 1.0000
[2019-04-09 15:21:41,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-09 15:21:41,889] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 19.0, 24.87975205120915, 0.3310328688389152, 0.0, 1.0, 65.0, 62.95497922959183], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 19.0, 25.01972153235152, 0.3430701270160208, 0.0, 1.0, 25.0, 46.19258227912055], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.5849767943626268, 0.6143567090053402, 0.0, 1.0, 0.2, 0.46192582279120553], 
reward next is 0.5381, 
noisyNet noise sample is [array([1.2545375], dtype=float32), 0.36128396]. 
=============================================
[2019-04-09 15:21:41,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00051643 0.10934568 0.13181846 0.01656579 0.00465586 0.00080933
 0.38613847 0.02000119 0.02488733 0.03575382 0.2695077 ], sum to 1.0000
[2019-04-09 15:21:41,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7705
[2019-04-09 15:21:41,934] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 73.5, 0.0, 0.0, 19.0, 25.1596727215266, 0.3460061802252503, 0.0, 1.0, 45.0, 40.46505292007658], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3047400.0000, 
sim time next is 3048000.0000, 
raw observation next is [-6.0, 74.66666666666666, 0.0, 0.0, 19.0, 25.22125108947114, 0.359058212745041, 0.0, 1.0, 65.0, 53.98289019567077], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7466666666666666, 0.0, 0.0, 0.08333333333333333, 0.6017709241225949, 0.6196860709150137, 0.0, 1.0, 1.0, 0.5398289019567077], 
reward next is 0.4602, 
noisyNet noise sample is [array([1.2545375], dtype=float32), 0.36128396]. 
=============================================
[2019-04-09 15:21:41,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[ 9.9218855]
 [10.187042 ]
 [10.2579365]
 [ 9.814068 ]
 [ 9.927946 ]], R is [[10.29757881]
 [10.78995228]
 [11.22012711]
 [11.47837639]
 [11.71565723]].
[2019-04-09 15:21:42,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2852497e-04 6.7564458e-02 2.0148072e-01 1.8988239e-02 2.9057318e-03
 5.0640991e-04 4.3659741e-01 9.4146840e-03 2.5420172e-02 3.6660258e-02
 2.0013340e-01], sum to 1.0000
[2019-04-09 15:21:42,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4656
[2019-04-09 15:21:42,260] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.54308268651934, 0.2863421967623929, 0.0, 1.0, 65.0, 70.90342540907383], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2963400.0000, 
sim time next is 2964000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.43003653358442, 0.2752617470598752, 0.0, 1.0, 45.0, 53.46461144387731], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5358363777987017, 0.591753915686625, 0.0, 1.0, 0.6, 0.5346461144387731], 
reward next is 0.4654, 
noisyNet noise sample is [array([0.23773211], dtype=float32), 1.7474059]. 
=============================================
[2019-04-09 15:21:42,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[9.872228]
 [9.749938]
 [9.928517]
 [9.726361]
 [9.784154]], R is [[10.28750706]
 [10.47559834]
 [10.89768124]
 [11.37703228]
 [11.77063179]].
[2019-04-09 15:21:42,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00048899 0.10641139 0.11504202 0.01174993 0.00436273 0.00039554
 0.32560658 0.01943546 0.0391887  0.02264037 0.35467827], sum to 1.0000
[2019-04-09 15:21:42,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3328
[2019-04-09 15:21:42,307] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.0, 25.54883457949435, 0.438342825323881, 0.0, 1.0, 45.0, 37.15797110368877], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 19.0, 25.62421036254295, 0.4438872706952753, 0.0, 1.0, 45.0, 39.09165884222306], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 0.08333333333333333, 0.6353508635452458, 0.6479624235650917, 0.0, 1.0, 0.6, 0.3909165884222306], 
reward next is 0.6091, 
noisyNet noise sample is [array([0.73688215], dtype=float32), -1.5518473]. 
=============================================
[2019-04-09 15:21:42,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00052953 0.12283571 0.18670665 0.01839192 0.00470728 0.00072383
 0.33855796 0.02923845 0.03158068 0.02103382 0.24569418], sum to 1.0000
[2019-04-09 15:21:42,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2566
[2019-04-09 15:21:42,637] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 19.0, 26.06396220808625, 0.5977374676534949, 0.0, 1.0, 20.0, 36.50981285009125], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 19.0, 26.0008119515364, 0.575604292548061, 0.0, 1.0, 25.0, 34.66781982208725], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.08333333333333333, 0.6667343292946999, 0.6918680975160204, 0.0, 1.0, 0.2, 0.34667819822087254], 
reward next is 0.6533, 
noisyNet noise sample is [array([-0.06828842], dtype=float32), -0.75892216]. 
=============================================
[2019-04-09 15:21:42,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00053193 0.13933189 0.19352043 0.01981208 0.0048445  0.00069582
 0.31765386 0.02395188 0.03233187 0.01947844 0.2478474 ], sum to 1.0000
[2019-04-09 15:21:42,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9844
[2019-04-09 15:21:42,712] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 19.0, 25.91275829889439, 0.5751225729352404, 0.0, 1.0, 65.0, 62.34320365614073], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2957400.0000, 
sim time next is 2958000.0000, 
raw observation next is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 25.87253818402591, 0.5896930090970584, 0.0, 1.0, 65.0, 63.21466081011226], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.6560448486688258, 0.696564336365686, 0.0, 1.0, 1.0, 0.6321466081011226], 
reward next is 0.3679, 
noisyNet noise sample is [array([-0.06828842], dtype=float32), -0.75892216]. 
=============================================
[2019-04-09 15:21:42,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[10.122089]
 [10.069888]
 [10.120665]
 [10.3297  ]
 [10.239722]], R is [[10.52087021]
 [10.79222965]
 [11.33762932]
 [11.8591547 ]
 [12.3482666 ]].
[2019-04-09 15:21:43,102] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5412063e-04 1.0497185e-01 1.3066404e-01 2.5219353e-02 1.9407513e-03
 4.1740073e-04 2.6372164e-01 5.1571685e-03 2.6344702e-02 2.9324235e-02
 4.1208473e-01], sum to 1.0000
[2019-04-09 15:21:43,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0251
[2019-04-09 15:21:43,135] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 19.0, 26.04807996288033, 0.6216336928862963, 0.0, 1.0, 65.0, 56.86266540250595], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [-3.666666666666667, 69.0, 174.0, 42.0, 19.0, 26.07033530823129, 0.624046782274435, 0.0, 1.0, 20.0, 50.87962331443306], 
processed observation next is [0.0, 0.43478260869565216, 0.3610341643582641, 0.69, 0.58, 0.04640883977900553, 0.08333333333333333, 0.6725279423526075, 0.7080155940914783, 0.0, 1.0, 0.1, 0.5087962331443306], 
reward next is 0.4912, 
noisyNet noise sample is [array([-0.22144856], dtype=float32), -1.5123553]. 
=============================================
[2019-04-09 15:21:43,242] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3469972e-05 6.2697843e-02 1.9998057e-01 1.2953527e-02 8.4172760e-04
 3.2033664e-04 3.2107362e-01 1.2486604e-02 1.9123714e-02 2.7020609e-02
 3.4344795e-01], sum to 1.0000
[2019-04-09 15:21:43,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0323
[2019-04-09 15:21:43,256] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 46.0, 109.0, 806.0, 19.0, 26.63966084470267, 0.7186557939783119, 0.0, 1.0, 25.0, 34.25871545869565], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3072600.0000, 
sim time next is 3073200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 107.3333333333333, 800.8333333333334, 19.0, 26.66779215915389, 0.7260168077746557, 0.0, 1.0, 20.0, 30.83054531370516], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.3577777777777777, 0.8848987108655617, 0.08333333333333333, 0.7223160132628242, 0.7420056025915519, 0.0, 1.0, 0.1, 0.3083054531370516], 
reward next is 0.6917, 
noisyNet noise sample is [array([-0.5161666], dtype=float32), -0.21062323]. 
=============================================
[2019-04-09 15:21:43,269] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2133555e-04 6.4381748e-02 1.8423502e-01 3.1190190e-02 4.4495114e-03
 7.0748519e-04 2.9969329e-01 8.5197128e-03 2.8280640e-02 2.3806063e-02
 3.5451502e-01], sum to 1.0000
[2019-04-09 15:21:43,269] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3935
[2019-04-09 15:21:43,316] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.16117759767927, 0.5737778078721808, 0.0, 1.0, 65.0, 47.25464784335146], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3028800.0000, 
sim time next is 3029400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.16076406054852, 0.5705929020215535, 0.0, 1.0, 45.0, 45.61979499029648], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6800636717123766, 0.6901976340071845, 0.0, 1.0, 0.6, 0.45619794990296475], 
reward next is 0.5438, 
noisyNet noise sample is [array([-1.193273], dtype=float32), 0.9303516]. 
=============================================
[2019-04-09 15:21:43,612] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0342952e-04 1.2077982e-01 1.7324719e-01 2.0405751e-02 3.1937319e-03
 3.9071316e-04 3.2167631e-01 8.9960294e-03 2.9564966e-02 2.5318986e-02
 2.9612303e-01], sum to 1.0000
[2019-04-09 15:21:43,613] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4186
[2019-04-09 15:21:43,628] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 19.0, 24.65742303681726, 0.38605256246442, 0.0, 1.0, 65.0, 51.08393889709775], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [-3.0, 65.0, 145.5, 745.5, 19.0, 24.64528355324794, 0.3984793679640301, 0.0, 1.0, 45.0, 38.0291782894986], 
processed observation next is [0.0, 0.5652173913043478, 0.3795013850415513, 0.65, 0.485, 0.8237569060773481, 0.08333333333333333, 0.5537736294373282, 0.63282645598801, 0.0, 1.0, 0.6, 0.38029178289498605], 
reward next is 0.6197, 
noisyNet noise sample is [array([-0.1562449], dtype=float32), 1.0984969]. 
=============================================
[2019-04-09 15:21:43,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8557764e-05 7.8206040e-02 1.4580454e-01 1.6780075e-02 1.3224767e-03
 1.4984229e-04 2.1149842e-01 4.3748557e-03 2.4623048e-02 2.4794588e-02
 4.9234757e-01], sum to 1.0000
[2019-04-09 15:21:43,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-09 15:21:43,799] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 19.0, 26.54569530267563, 0.7582049882184081, 0.0, 1.0, 65.0, 42.73375997590081], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 19.0, 26.62243004660541, 0.7688294894497393, 0.0, 1.0, 45.0, 34.8135471170116], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.08333333333333333, 0.7185358372171174, 0.7562764964832464, 0.0, 1.0, 0.6, 0.348135471170116], 
reward next is 0.6519, 
noisyNet noise sample is [array([1.2463889], dtype=float32), -0.54511195]. 
=============================================
[2019-04-09 15:21:43,843] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4433654e-05 1.3431959e-01 1.5471454e-01 1.0277928e-02 1.0549774e-03
 2.1969686e-04 1.5030941e-01 5.2245697e-03 1.7477347e-02 2.1061940e-02
 5.0525558e-01], sum to 1.0000
[2019-04-09 15:21:43,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9563
[2019-04-09 15:21:43,860] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 27.10266195506298, 0.79796305906968, 0.0, 1.0, 65.0, 34.82452704561454], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 27.08809511612624, 0.7925260812945482, 0.0, 1.0, 65.0, 34.94880344001834], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.08333333333333333, 0.7573412596771867, 0.764175360431516, 0.0, 1.0, 1.0, 0.3494880344001834], 
reward next is 0.6505, 
noisyNet noise sample is [array([-0.92439294], dtype=float32), 0.21794109]. 
=============================================
[2019-04-09 15:21:43,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3250830e-05 5.3827945e-02 2.3050426e-01 8.1847096e-03 7.3746446e-04
 1.2071410e-04 2.9339463e-01 8.2228994e-03 1.5573802e-02 2.1694779e-02
 3.6765552e-01], sum to 1.0000
[2019-04-09 15:21:43,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6071
[2019-04-09 15:21:43,974] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.5, 57.5, 6.0, 99.0, 19.0, 26.76273709373487, 0.7349079644506472, 0.0, 1.0, 25.0, 34.21954946732063], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3000600.0000, 
sim time next is 3001200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 26.73975899326276, 0.733822907599313, 0.0, 1.0, 55.0, 36.56188924541103], 
processed observation next is [0.0, 0.7391304347826086, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.7283132494385635, 0.7446076358664376, 0.0, 1.0, 0.8, 0.36561889245411033], 
reward next is 0.6344, 
noisyNet noise sample is [array([-1.6405221], dtype=float32), 0.9887434]. 
=============================================
[2019-04-09 15:21:43,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.22008925e-04 8.11509490e-02 8.78300741e-02 1.91427264e-02
 1.62349304e-03 1.67092148e-04 3.24055612e-01 1.37044843e-02
 3.16248797e-02 1.36767766e-02 4.26901907e-01], sum to 1.0000
[2019-04-09 15:21:43,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4197
[2019-04-09 15:21:44,031] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 218.5, 487.5, 19.0, 25.67576773293499, 0.5795828436612126, 0.0, 1.0, 45.0, 47.06788647574974], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [-3.0, 65.0, 206.0, 555.3333333333334, 19.0, 25.79423163303007, 0.5909005287452157, 0.0, 1.0, 45.0, 44.11792616474342], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6866666666666666, 0.6136279926335175, 0.08333333333333333, 0.6495193027525058, 0.6969668429150718, 0.0, 1.0, 0.6, 0.4411792616474342], 
reward next is 0.5588, 
noisyNet noise sample is [array([-1.739153], dtype=float32), 0.40867746]. 
=============================================
[2019-04-09 15:21:44,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2927124e-04 5.8406126e-02 1.2292633e-01 9.8586818e-03 1.0939009e-03
 1.2921960e-04 3.5166276e-01 4.0203412e-03 2.0207392e-02 2.2451945e-02
 4.0911406e-01], sum to 1.0000
[2019-04-09 15:21:44,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1969
[2019-04-09 15:21:44,350] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.4120688747725, 0.6284722341550474, 0.0, 1.0, 25.0, 40.23341774184154], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.40914025979637, 0.6256483123234301, 0.0, 1.0, 45.0, 38.91590230369377], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7007616883163642, 0.7085494374411434, 0.0, 1.0, 0.6, 0.3891590230369377], 
reward next is 0.6108, 
noisyNet noise sample is [array([1.5336993], dtype=float32), -3.1772006]. 
=============================================
[2019-04-09 15:21:44,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8866468e-04 1.3401636e-01 1.5986519e-01 3.0967409e-02 3.2633427e-03
 3.2157238e-04 2.8884524e-01 9.5484145e-03 3.1753324e-02 2.7499560e-02
 3.1363094e-01], sum to 1.0000
[2019-04-09 15:21:44,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9676
[2019-04-09 15:21:44,463] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 158.0, 114.0, 19.0, 24.95238913532722, 0.3893386509995094, 0.0, 1.0, 25.0, 36.10139785762584], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [-4.0, 71.0, 162.0, 96.00000000000001, 19.0, 24.92978211045396, 0.3920199119921599, 0.0, 1.0, 65.0, 63.12282816852259], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.54, 0.10607734806629836, 0.08333333333333333, 0.57748184253783, 0.6306733039973866, 0.0, 1.0, 1.0, 0.6312282816852259], 
reward next is 0.3688, 
noisyNet noise sample is [array([-1.6455632], dtype=float32), -0.22028813]. 
=============================================
[2019-04-09 15:21:44,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[10.285152]
 [10.193014]
 [10.421501]
 [10.189985]
 [10.263529]], R is [[10.28053284]
 [10.81671333]
 [11.32429504]
 [11.82799625]
 [12.18968868]].
[2019-04-09 15:21:44,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.33035908e-04 9.01132450e-02 1.35682374e-01 1.69714894e-02
 1.55199214e-03 3.65208427e-04 3.58083218e-01 1.45731745e-02
 2.59479545e-02 2.92119794e-02 3.27366382e-01], sum to 1.0000
[2019-04-09 15:21:44,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8723
[2019-04-09 15:21:44,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 19.0, 24.99684545892354, 0.4706263184099511, 0.0, 1.0, 65.0, 73.71588502863847], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [-3.666666666666667, 69.0, 174.0, 42.0, 19.0, 25.17113951261886, 0.4989924696930274, 0.0, 1.0, 65.0, 63.14329078733508], 
processed observation next is [0.0, 0.43478260869565216, 0.3610341643582641, 0.69, 0.58, 0.04640883977900553, 0.08333333333333333, 0.5975949593849051, 0.6663308232310091, 0.0, 1.0, 1.0, 0.6314329078733508], 
reward next is 0.3686, 
noisyNet noise sample is [array([0.4085752], dtype=float32), -0.62626904]. 
=============================================
[2019-04-09 15:21:44,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4458565e-04 1.2411801e-01 1.2251053e-01 1.2278371e-02 1.9832875e-03
 1.3356411e-04 4.5519206e-01 6.3674049e-03 1.4336332e-02 1.1909006e-02
 2.5102684e-01], sum to 1.0000
[2019-04-09 15:21:44,641] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-09 15:21:44,659] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 19.0, 26.331749161806, 0.5949716903945151, 0.0, 1.0, 45.0, 43.9783843795573], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3025800.0000, 
sim time next is 3026400.0000, 
raw observation next is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 26.31862896297311, 0.5884659278117605, 0.0, 1.0, 45.0, 35.86881564891089], 
processed observation next is [0.0, 0.0, 0.33333333333333337, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6932190802477592, 0.6961553092705869, 0.0, 1.0, 0.6, 0.3586881564891089], 
reward next is 0.6413, 
noisyNet noise sample is [array([0.4110613], dtype=float32), -0.59575385]. 
=============================================
[2019-04-09 15:21:44,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1793617e-04 7.4093267e-02 3.5075184e-02 6.0864803e-03 1.1935488e-03
 1.5175008e-04 4.8111260e-01 4.2599100e-03 1.5851809e-02 1.8185286e-02
 3.6387217e-01], sum to 1.0000
[2019-04-09 15:21:44,726] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6360
[2019-04-09 15:21:44,755] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.833333333333333, 65.0, 0.0, 0.0, 19.0, 26.44420425134313, 0.6360242715799561, 0.0, 1.0, 45.0, 37.66856592769611], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3015600.0000, 
sim time next is 3016200.0000, 
raw observation next is [-3.916666666666667, 65.0, 0.0, 0.0, 19.0, 26.43140547824433, 0.6336448767834333, 0.0, 1.0, 60.0, 44.11769555523623], 
processed observation next is [0.0, 0.9130434782608695, 0.3541089566020314, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7026171231870274, 0.7112149589278111, 0.0, 1.0, 0.9, 0.4411769555523623], 
reward next is 0.5588, 
noisyNet noise sample is [array([1.2158142], dtype=float32), 0.7462244]. 
=============================================
[2019-04-09 15:21:45,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5008971e-04 1.2695408e-01 1.8367583e-01 1.7342081e-02 1.6061579e-03
 1.7530739e-04 3.4189692e-01 1.3429919e-02 2.8300392e-02 1.4001088e-02
 2.7246809e-01], sum to 1.0000
[2019-04-09 15:21:45,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5024
[2019-04-09 15:21:45,030] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 19.0, 25.82093576553587, 0.581637427028804, 0.0, 1.0, 60.0, 57.30492956183371], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2976000.0000, 
sim time next is 2976600.0000, 
raw observation next is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 19.0, 25.87380960682424, 0.5935872610810864, 0.0, 1.0, 45.0, 43.85814747962556], 
processed observation next is [0.0, 0.43478260869565216, 0.3748845798707295, 0.66, 0.68, 0.12228360957642719, 0.08333333333333333, 0.6561508005686866, 0.6978624203603622, 0.0, 1.0, 0.6, 0.4385814747962556], 
reward next is 0.5614, 
noisyNet noise sample is [array([-2.1311135], dtype=float32), -1.2516575]. 
=============================================
[2019-04-09 15:21:45,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1624239e-05 1.0095909e-01 1.7755686e-01 1.3377588e-02 1.2325962e-03
 1.6094663e-04 4.4306827e-01 1.2196398e-02 4.0708434e-02 2.3285126e-02
 1.8737312e-01], sum to 1.0000
[2019-04-09 15:21:45,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3003
[2019-04-09 15:21:45,137] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 64.16666666666667, 133.6666666666667, 763.6666666666666, 19.0, 26.12471211906666, 0.6493398254825856, 0.0, 1.0, 25.0, 28.620171740542], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2985000.0000, 
sim time next is 2985600.0000, 
raw observation next is [-2.666666666666667, 63.33333333333334, 121.8333333333333, 781.8333333333334, 19.0, 26.08375269868595, 0.6428017990224549, 0.0, 1.0, 45.0, 26.99994172821163], 
processed observation next is [0.0, 0.5652173913043478, 0.38873499538319484, 0.6333333333333334, 0.406111111111111, 0.8639042357274402, 0.08333333333333333, 0.6736460582238291, 0.7142672663408183, 0.0, 1.0, 0.6, 0.2699994172821163], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.07811732], dtype=float32), -0.11304742]. 
=============================================
[2019-04-09 15:21:46,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4542331e-05 8.5935280e-02 2.7156982e-01 7.7667413e-03 9.8213018e-04
 2.1735250e-04 2.4497965e-01 1.3988216e-02 1.7878972e-02 4.2393044e-02
 3.1421426e-01], sum to 1.0000
[2019-04-09 15:21:46,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-09 15:21:46,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6790060e-05 8.7565511e-02 1.6326694e-01 1.1465244e-02 9.7965728e-04
 1.6199639e-04 4.5554388e-01 6.9211069e-03 2.5110941e-02 1.6196385e-02
 2.3272152e-01], sum to 1.0000
[2019-04-09 15:21:46,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8908
[2019-04-09 15:21:46,094] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 27.01702743257409, 0.7723859332181754, 0.0, 1.0, 65.0, 35.35877610154657], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 27.00554737297755, 0.7675283378574357, 0.0, 1.0, 35.0, 34.55011500096062], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.08333333333333333, 0.7504622810814624, 0.7558427792858119, 0.0, 1.0, 0.4, 0.34550115000960624], 
reward next is 0.6545, 
noisyNet noise sample is [array([-0.7622868], dtype=float32), -0.30443695]. 
=============================================
[2019-04-09 15:21:46,110] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.58506147942015, 0.7026675146556816, 0.0, 1.0, 45.0, 38.31581753838535], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.60476557464441, 0.7006803616231662, 0.0, 1.0, 65.0, 43.94190858304699], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.08333333333333333, 0.7170637978870342, 0.7335601205410555, 0.0, 1.0, 1.0, 0.4394190858304699], 
reward next is 0.5606, 
noisyNet noise sample is [array([-1.4585952], dtype=float32), 0.4536259]. 
=============================================
[2019-04-09 15:21:46,204] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0320816e-04 5.4542862e-02 1.3675761e-01 8.5978555e-03 8.8729034e-04
 2.0454361e-04 3.6507064e-01 9.0265078e-03 1.0244755e-02 2.4435714e-02
 3.9012903e-01], sum to 1.0000
[2019-04-09 15:21:46,210] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5421
[2019-04-09 15:21:46,238] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 19.0, 26.58878760481438, 0.6852934290966216, 0.0, 1.0, 45.0, 42.05810597685706], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.5790860790713, 0.6816135626695345, 0.0, 1.0, 45.0, 36.38400906497046], 
processed observation next is [0.0, 0.8260869565217391, 0.3979686057248385, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.7149238399226082, 0.7272045208898449, 0.0, 1.0, 0.6, 0.36384009064970463], 
reward next is 0.6362, 
noisyNet noise sample is [array([-1.1523027], dtype=float32), -0.104380034]. 
=============================================
[2019-04-09 15:21:46,331] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9042685e-05 6.5714084e-02 1.7960863e-01 9.4223944e-03 6.9612474e-04
 1.1357130e-04 5.0131691e-01 1.0587454e-02 5.5826292e-03 1.0798041e-02
 2.1606110e-01], sum to 1.0000
[2019-04-09 15:21:46,332] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2794
[2019-04-09 15:21:46,362] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 19.0, 27.04158550841221, 0.7704601419688681, 0.0, 1.0, 25.0, 31.87175600966911], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [-0.5, 80.33333333333334, 0.0, 0.0, 19.0, 27.00151988510299, 0.7641958678902411, 0.0, 1.0, 45.0, 35.03366831803906], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7501266570919158, 0.7547319559634137, 0.0, 1.0, 0.6, 0.35033668318039063], 
reward next is 0.6497, 
noisyNet noise sample is [array([-0.07828586], dtype=float32), -2.108853]. 
=============================================
[2019-04-09 15:21:46,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7087738e-04 1.3316089e-01 1.9228448e-01 1.2802730e-02 4.4984315e-03
 4.6255183e-04 2.5288451e-01 1.0842579e-02 3.6882438e-02 3.7658773e-02
 3.1825170e-01], sum to 1.0000
[2019-04-09 15:21:46,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9530
[2019-04-09 15:21:46,384] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 91.0, 497.0, 19.0, 26.03921960949709, 0.5516579273381277, 0.0, 1.0, 65.0, 48.42072466556096], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3056400.0000, 
sim time next is 3057000.0000, 
raw observation next is [-5.666666666666667, 58.16666666666667, 93.66666666666667, 543.0, 19.0, 26.07173429124756, 0.563286550634139, 0.0, 1.0, 65.0, 46.63885895257874], 
processed observation next is [0.0, 0.391304347826087, 0.30563250230840255, 0.5816666666666667, 0.31222222222222223, 0.6, 0.08333333333333333, 0.6726445242706299, 0.687762183544713, 0.0, 1.0, 1.0, 0.4663885895257874], 
reward next is 0.5336, 
noisyNet noise sample is [array([0.66490215], dtype=float32), -0.06098307]. 
=============================================
[2019-04-09 15:21:46,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.867056]
 [11.034672]
 [10.976344]
 [10.917793]
 [10.515469]], R is [[11.4486208 ]
 [11.8499279 ]
 [12.23224354]
 [12.65079117]
 [13.09276295]].
[2019-04-09 15:21:46,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6071892e-04 9.3417183e-02 1.0809186e-01 1.0266924e-02 1.9735456e-03
 6.3816283e-04 3.2310125e-01 1.1185092e-02 2.4025146e-02 2.5595577e-02
 4.0144449e-01], sum to 1.0000
[2019-04-09 15:21:46,411] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1120
[2019-04-09 15:21:46,440] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 19.0, 26.08073585996079, 0.5278904571931786, 0.0, 1.0, 45.0, 34.99561324365742], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3033000.0000, 
sim time next is 3033600.0000, 
raw observation next is [-5.666666666666667, 75.0, 0.0, 0.0, 19.0, 26.00974346532878, 0.5071945779003101, 0.0, 1.0, 25.0, 33.2007648905198], 
processed observation next is [0.0, 0.08695652173913043, 0.30563250230840255, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6674786221107315, 0.6690648593001033, 0.0, 1.0, 0.2, 0.332007648905198], 
reward next is 0.6680, 
noisyNet noise sample is [array([-0.4848646], dtype=float32), 0.06253221]. 
=============================================
[2019-04-09 15:21:46,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5871076e-04 6.9740161e-02 1.5843180e-01 1.6176317e-02 2.5765477e-03
 5.6785601e-04 4.0739584e-01 8.0277920e-03 3.1855162e-02 2.0271692e-02
 2.8459805e-01], sum to 1.0000
[2019-04-09 15:21:46,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-09 15:21:46,614] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.47379524714162, 0.3734635172722946, 0.0, 1.0, 45.0, 32.43330966934788], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3042000.0000, 
sim time next is 3042600.0000, 
raw observation next is [-6.0, 75.83333333333334, 0.0, 0.0, 19.0, 25.38066136697742, 0.3700826043718302, 0.0, 1.0, 65.0, 60.22301530106334], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7583333333333334, 0.0, 0.0, 0.08333333333333333, 0.6150551139147851, 0.6233608681239434, 0.0, 1.0, 1.0, 0.6022301530106334], 
reward next is 0.3978, 
noisyNet noise sample is [array([0.07768157], dtype=float32), -0.86928195]. 
=============================================
[2019-04-09 15:21:46,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4040386e-05 5.9096739e-02 4.6135221e-02 2.5998214e-03 2.9318727e-04
 4.1046249e-05 7.0090693e-01 7.6884702e-03 7.1856757e-03 1.3729309e-02
 1.6230950e-01], sum to 1.0000
[2019-04-09 15:21:46,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-09 15:21:46,710] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 19.0, 26.76654312043737, 0.6480091518763867, 0.0, 1.0, 45.0, 48.22807634621806], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3133800.0000, 
sim time next is 3134400.0000, 
raw observation next is [5.333333333333334, 100.0, 0.0, 0.0, 19.0, 26.76776090796243, 0.6491855283558287, 0.0, 1.0, 45.0, 44.85948375651362], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7306467423302024, 0.7163951761186095, 0.0, 1.0, 0.6, 0.4485948375651362], 
reward next is 0.5514, 
noisyNet noise sample is [array([-0.05959177], dtype=float32), -0.09779348]. 
=============================================
[2019-04-09 15:21:46,725] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4045778e-05 4.8056278e-02 1.0128055e-01 7.7322354e-03 9.9315017e-04
 7.5296099e-05 3.7028271e-01 4.9545402e-03 1.4963345e-02 1.8876994e-02
 4.3276089e-01], sum to 1.0000
[2019-04-09 15:21:46,728] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6289
[2019-04-09 15:21:46,745] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.87945418162287, 0.7323377636121468, 0.0, 1.0, 65.0, 43.16685515857331], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3094800.0000, 
sim time next is 3095400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.91345216521866, 0.729822193996244, 0.0, 1.0, 65.0, 40.8545087410061], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7427876804348884, 0.7432740646654147, 0.0, 1.0, 1.0, 0.408545087410061], 
reward next is 0.5915, 
noisyNet noise sample is [array([0.29572788], dtype=float32), -1.7298651]. 
=============================================
[2019-04-09 15:21:47,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3475994e-04 9.5502712e-02 1.5586667e-01 1.6996456e-02 2.5123174e-03
 3.6194178e-04 2.8947684e-01 1.9734666e-02 2.6476035e-02 3.2085385e-02
 3.6065230e-01], sum to 1.0000
[2019-04-09 15:21:47,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4948
[2019-04-09 15:21:47,048] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 19.0, 25.55529303685576, 0.436090718738904, 0.0, 1.0, 20.0, 44.9433645074372], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3045000.0000, 
sim time next is 3045600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 25.60169299321497, 0.4343474477171202, 0.0, 1.0, 45.0, 41.12526425696075], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6334744161012477, 0.6447824825723734, 0.0, 1.0, 0.6, 0.4112526425696075], 
reward next is 0.5887, 
noisyNet noise sample is [array([0.23817915], dtype=float32), 0.17412154]. 
=============================================
[2019-04-09 15:21:47,058] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4009409e-04 8.1706256e-02 1.0673113e-01 2.7737154e-02 4.5957514e-03
 5.9778738e-04 4.4749629e-01 2.8845178e-02 3.7599862e-02 3.5504814e-02
 2.2894564e-01], sum to 1.0000
[2019-04-09 15:21:47,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2420
[2019-04-09 15:21:47,078] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.84329673094548, 0.4842437605509476, 0.0, 1.0, 45.0, 47.66652931199864], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3039000.0000, 
sim time next is 3039600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.83387239239497, 0.4755178543448957, 0.0, 1.0, 45.0, 37.35480792644201], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6528226993662475, 0.6585059514482986, 0.0, 1.0, 0.6, 0.3735480792644201], 
reward next is 0.6265, 
noisyNet noise sample is [array([1.4387776], dtype=float32), -0.9385109]. 
=============================================
[2019-04-09 15:21:47,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1666869e-04 1.6933997e-01 2.0556615e-01 2.5824966e-02 2.4766237e-03
 6.5153494e-04 2.6022980e-01 9.3808910e-03 2.7653949e-02 2.5534913e-02
 2.7312452e-01], sum to 1.0000
[2019-04-09 15:21:47,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7285
[2019-04-09 15:21:47,194] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 19.0, 25.61317015197259, 0.5011635264182499, 0.0, 1.0, 20.0, 40.33191615204926], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 19.0, 25.74978809118165, 0.5216429693304541, 0.0, 1.0, 65.0, 47.49598598838175], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.08333333333333333, 0.6458156742651374, 0.673880989776818, 0.0, 1.0, 1.0, 0.47495985988381745], 
reward next is 0.5250, 
noisyNet noise sample is [array([0.65059], dtype=float32), 1.2957722]. 
=============================================
[2019-04-09 15:21:47,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7324745e-04 1.2532178e-01 1.5996669e-01 2.1936296e-02 2.9268484e-03
 7.3014683e-04 2.1613419e-01 1.6863093e-02 2.0167677e-02 2.0127371e-02
 4.1555268e-01], sum to 1.0000
[2019-04-09 15:21:47,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6419
[2019-04-09 15:21:47,250] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 19.0, 24.36827298445853, 0.2356387743731749, 0.0, 1.0, 65.0, 67.01191862741437], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 19.0, 24.51523115599583, 0.2909431341179766, 0.0, 1.0, 65.0, 62.01476403084775], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.08333333333333333, 0.5429359296663193, 0.5969810447059922, 0.0, 1.0, 1.0, 0.6201476403084775], 
reward next is 0.3799, 
noisyNet noise sample is [array([0.65184087], dtype=float32), 1.5588368]. 
=============================================
[2019-04-09 15:21:47,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4190298e-05 9.1787510e-02 1.1588611e-01 1.2901425e-02 1.2285903e-03
 1.2786311e-04 3.1115961e-01 4.3380912e-03 4.2070009e-02 1.0934924e-02
 4.0947172e-01], sum to 1.0000
[2019-04-09 15:21:47,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7674
[2019-04-09 15:21:47,294] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 19.0, 27.17978021901796, 0.8330989557096314, 0.0, 1.0, 20.0, 31.75610784784053], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3082200.0000, 
sim time next is 3082800.0000, 
raw observation next is [0.6666666666666667, 50.66666666666667, 61.5, 517.1666666666666, 19.0, 27.17780938418825, 0.8304527293833149, 0.0, 1.0, 65.0, 32.53093549155374], 
processed observation next is [0.0, 0.6956521739130435, 0.4810710987996307, 0.5066666666666667, 0.205, 0.5714548802946593, 0.08333333333333333, 0.7648174486823542, 0.7768175764611049, 0.0, 1.0, 1.0, 0.3253093549155374], 
reward next is 0.6747, 
noisyNet noise sample is [array([-0.73610365], dtype=float32), -0.2688153]. 
=============================================
[2019-04-09 15:21:47,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.82705015e-04 6.48771897e-02 1.04506016e-01 1.25674987e-02
 1.52654236e-03 4.00146178e-04 4.49747413e-01 6.08215481e-03
 2.25920696e-02 1.41811892e-02 3.23337048e-01], sum to 1.0000
[2019-04-09 15:21:47,314] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8608
[2019-04-09 15:21:47,315] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00062109 0.07215746 0.15699437 0.0154148  0.00270584 0.00064906
 0.39164624 0.0208045  0.03147956 0.03127478 0.2762524 ], sum to 1.0000
[2019-04-09 15:21:47,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-09 15:21:47,331] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 25.78290442405278, 0.4494340534595536, 0.0, 1.0, 45.0, 28.10991442852549], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3027600.0000, 
sim time next is 3028200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 25.65540890138671, 0.4478240221060852, 0.0, 1.0, 65.0, 56.24275912245128], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6379507417822259, 0.6492746740353618, 0.0, 1.0, 1.0, 0.5624275912245128], 
reward next is 0.4376, 
noisyNet noise sample is [array([0.07539991], dtype=float32), 0.16066137]. 
=============================================
[2019-04-09 15:21:47,335] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 75.83333333333334, 0.0, 0.0, 19.0, 25.61387382410435, 0.4308703529260482, 0.0, 1.0, 45.0, 34.27356059749303], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3042600.0000, 
sim time next is 3043200.0000, 
raw observation next is [-6.0, 74.66666666666667, 0.0, 0.0, 19.0, 25.58713250107904, 0.4376860262606381, 0.0, 1.0, 65.0, 52.07588172642092], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6322610417565867, 0.6458953420868794, 0.0, 1.0, 1.0, 0.5207588172642091], 
reward next is 0.4792, 
noisyNet noise sample is [array([0.0009777], dtype=float32), 0.38327286]. 
=============================================
[2019-04-09 15:21:47,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1442243e-04 7.4630037e-02 1.8263400e-01 1.1803570e-02 1.6457262e-03
 2.4128296e-04 2.3230936e-01 8.9533729e-03 2.4163753e-02 1.8828677e-02
 4.4467571e-01], sum to 1.0000
[2019-04-09 15:21:47,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1676
[2019-04-09 15:21:47,714] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 19.0, 27.20390792091327, 0.838155023114089, 0.0, 1.0, 45.0, 28.86120625021636], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3082200.0000, 
sim time next is 3082800.0000, 
raw observation next is [0.6666666666666667, 50.66666666666667, 61.5, 517.1666666666666, 19.0, 27.19294892860525, 0.8321175243426456, 0.0, 1.0, 25.0, 27.58043357038807], 
processed observation next is [0.0, 0.6956521739130435, 0.4810710987996307, 0.5066666666666667, 0.205, 0.5714548802946593, 0.08333333333333333, 0.7660790773837709, 0.7773725081142152, 0.0, 1.0, 0.2, 0.2758043357038807], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.9754112], dtype=float32), -0.1576903]. 
=============================================
[2019-04-09 15:21:48,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0589939e-04 1.1048750e-01 1.6135958e-01 1.9140979e-02 3.0516030e-03
 5.3410686e-04 3.0562276e-01 2.7769918e-02 3.0641392e-02 2.6952453e-02
 3.1413379e-01], sum to 1.0000
[2019-04-09 15:21:48,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9357
[2019-04-09 15:21:48,136] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333333, 57.33333333333334, 96.33333333333333, 589.0000000000001, 19.0, 25.64849862523756, 0.4937522484994306, 0.0, 1.0, 20.0, 38.16488798696076], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3057600.0000, 
sim time next is 3058200.0000, 
raw observation next is [-5.0, 56.5, 99.0, 635.0, 19.0, 25.66364836932163, 0.5184393849946041, 0.0, 1.0, 65.0, 49.56060048033646], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.565, 0.33, 0.7016574585635359, 0.08333333333333333, 0.6386373641101359, 0.6728131283315347, 0.0, 1.0, 1.0, 0.4956060048033646], 
reward next is 0.5044, 
noisyNet noise sample is [array([-0.5668816], dtype=float32), -0.05673692]. 
=============================================
[2019-04-09 15:21:48,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9354948e-05 5.1843707e-02 1.0039297e-01 5.6481389e-03 9.7947032e-04
 1.3108579e-04 5.7025987e-01 4.0152534e-03 1.1732343e-02 6.1195586e-03
 2.4881822e-01], sum to 1.0000
[2019-04-09 15:21:48,319] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6145
[2019-04-09 15:21:48,359] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.79871906307979, 0.6907043241907783, 0.0, 1.0, 65.0, 43.71981384748482], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.78498026151389, 0.6917145002741601, 0.0, 1.0, 45.0, 38.70823795613849], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7320816884594908, 0.7305715000913867, 0.0, 1.0, 0.6, 0.3870823795613849], 
reward next is 0.6129, 
noisyNet noise sample is [array([0.33784956], dtype=float32), -0.9934266]. 
=============================================
[2019-04-09 15:21:48,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.33225825e-05 4.75705266e-02 1.38215378e-01 1.11814365e-02
 1.52305036e-03 2.23044379e-04 4.54168826e-01 1.58480015e-02
 3.11427265e-02 1.71661768e-02 2.82887489e-01], sum to 1.0000
[2019-04-09 15:21:48,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9113
[2019-04-09 15:21:48,473] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 19.0, 26.52533666209804, 0.7037038389991287, 0.0, 1.0, 50.0, 33.72470333328778], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [-1.5, 46.0, 109.0, 806.0, 19.0, 26.5727826383161, 0.7121806103064944, 0.0, 1.0, 65.0, 38.28851143898125], 
processed observation next is [0.0, 0.5652173913043478, 0.4210526315789474, 0.46, 0.36333333333333334, 0.8906077348066298, 0.08333333333333333, 0.7143985531930083, 0.7373935367688315, 0.0, 1.0, 1.0, 0.38288511438981254], 
reward next is 0.6171, 
noisyNet noise sample is [array([-0.5231673], dtype=float32), 1.6354078]. 
=============================================
[2019-04-09 15:21:49,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6829066e-05 7.2747938e-02 9.1499828e-02 8.5282624e-03 7.9952477e-04
 1.9927554e-04 3.9604646e-01 8.2325386e-03 1.1745250e-02 1.5244209e-02
 3.9486977e-01], sum to 1.0000
[2019-04-09 15:21:49,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0939
[2019-04-09 15:21:49,260] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 19.0, 26.79951298222344, 0.6974167037750405, 0.0, 1.0, 20.0, 43.51253539060716], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 19.0, 26.78786177017471, 0.699001256615165, 0.0, 1.0, 25.0, 35.70749296466479], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.7323218141812259, 0.7330004188717217, 0.0, 1.0, 0.2, 0.3570749296466479], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.35638124], dtype=float32), 0.89422345]. 
=============================================
[2019-04-09 15:21:49,434] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1709750e-06 8.2167156e-02 2.1561934e-01 2.2351798e-03 1.2755544e-04
 1.8368166e-05 1.5034033e-01 1.7993974e-03 1.6575446e-02 7.1305558e-03
 5.2398449e-01], sum to 1.0000
[2019-04-09 15:21:49,435] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1775
[2019-04-09 15:21:49,446] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.0, 100.0, 109.0, 755.8333333333334, 22.5, 28.30335040754366, 1.045297877819672, 1.0, 1.0, 65.0, 15.36005952142872], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3148800.0000, 
sim time next is 3149400.0000, 
raw observation next is [7.0, 100.0, 110.0, 765.6666666666666, 22.5, 28.35133999786628, 1.061457619817784, 1.0, 1.0, 25.0, 12.94613713056124], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36666666666666664, 0.8460405156537752, 0.375, 0.8626116664888567, 0.853819206605928, 1.0, 1.0, 0.2, 0.12946137130561242], 
reward next is 0.8705, 
noisyNet noise sample is [array([0.65645623], dtype=float32), -1.0185254]. 
=============================================
[2019-04-09 15:21:49,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4341700e-06 1.4726744e-02 3.7754655e-02 1.4014880e-03 3.5055430e-04
 2.5117422e-05 2.0122674e-01 5.1008244e-03 6.8917368e-03 7.2816419e-03
 7.2523201e-01], sum to 1.0000
[2019-04-09 15:21:49,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-09 15:21:49,856] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.4, 100.0, 0.0, 0.0, 19.0, 26.72567666070593, 0.6439241172094002, 0.0, 1.0, 65.0, 61.56219260896761], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3123600.0000, 
sim time next is 3124200.0000, 
raw observation next is [2.5, 100.0, 0.0, 0.0, 19.0, 26.68001247908698, 0.6465920928669374, 0.0, 1.0, 65.0, 61.20975815425529], 
processed observation next is [1.0, 0.13043478260869565, 0.5318559556786704, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7233343732572483, 0.7155306976223125, 0.0, 1.0, 1.0, 0.6120975815425529], 
reward next is 0.3879, 
noisyNet noise sample is [array([-1.1170651], dtype=float32), 0.005953693]. 
=============================================
[2019-04-09 15:21:49,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2866925e-05 4.9064111e-02 1.6185927e-01 6.5080561e-03 5.8746844e-04
 1.6361258e-04 3.2158172e-01 7.4364142e-03 1.5180433e-02 2.5352551e-02
 4.1217354e-01], sum to 1.0000
[2019-04-09 15:21:49,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5654
[2019-04-09 15:21:49,976] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 19.0, 26.60909327785895, 0.700888062623922, 0.0, 1.0, 65.0, 46.68213256102879], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3091800.0000, 
sim time next is 3092400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.67613866083714, 0.7040301192502533, 0.0, 1.0, 60.0, 38.18124129922651], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7230115550697617, 0.7346767064167511, 0.0, 1.0, 0.9, 0.3818124129922651], 
reward next is 0.6182, 
noisyNet noise sample is [array([1.6675545], dtype=float32), 0.8993155]. 
=============================================
[2019-04-09 15:21:50,515] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3439991e-06 2.6606582e-02 9.1092356e-02 9.8454359e-04 4.2312196e-05
 4.2973038e-06 7.0760870e-01 6.3803594e-04 2.0218000e-03 3.1822172e-03
 1.6781777e-01], sum to 1.0000
[2019-04-09 15:21:50,519] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7680
[2019-04-09 15:21:50,526] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 99.16666666666666, 75.33333333333334, 619.0, 22.5, 29.1964556445913, 1.373074114978176, 1.0, 1.0, 45.0, 2.151414929544586], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3167400.0000, 
sim time next is 3168000.0000, 
raw observation next is [6.6, 99.0, 71.0, 589.0, 22.5, 29.23170633568426, 1.379552048731818, 1.0, 1.0, 45.0, 2.075155080443642], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.99, 0.23666666666666666, 0.6508287292817679, 0.375, 0.9359755279736884, 0.959850682910606, 1.0, 1.0, 0.6, 0.02075155080443642], 
reward next is 0.9792, 
noisyNet noise sample is [array([0.9446844], dtype=float32), -0.37176526]. 
=============================================
[2019-04-09 15:21:50,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[19.370153]
 [19.495218]
 [19.381042]
 [19.590054]
 [19.542692]], R is [[20.49079514]
 [21.26437187]
 [22.0273304 ]
 [22.78199959]
 [23.52847099]].
[2019-04-09 15:21:50,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1546526e-05 3.0731602e-02 4.4596899e-02 7.7318992e-03 1.1272674e-03
 6.9483773e-05 5.3027397e-01 2.7283514e-03 1.2847269e-02 1.1829351e-02
 3.5802233e-01], sum to 1.0000
[2019-04-09 15:21:50,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1898
[2019-04-09 15:21:50,589] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 100.0, 0.0, 0.0, 19.0, 26.64557944765592, 0.6276788037337352, 0.0, 1.0, 65.0, 42.04337910093141], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3118800.0000, 
sim time next is 3119400.0000, 
raw observation next is [1.5, 100.0, 0.0, 0.0, 19.0, 26.69293301394775, 0.637651659444561, 0.0, 1.0, 45.0, 37.57513532064717], 
processed observation next is [1.0, 0.08695652173913043, 0.5041551246537397, 1.0, 0.0, 0.0, 0.08333333333333333, 0.724411084495646, 0.712550553148187, 0.0, 1.0, 0.6, 0.3757513532064717], 
reward next is 0.6242, 
noisyNet noise sample is [array([-0.02192984], dtype=float32), -0.14386775]. 
=============================================
[2019-04-09 15:21:50,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6641366e-05 9.4706513e-02 6.6239834e-02 1.1036445e-02 1.1892028e-03
 2.3084582e-04 2.9066980e-01 4.8726867e-03 6.2715948e-02 9.3139531e-03
 4.5897818e-01], sum to 1.0000
[2019-04-09 15:21:50,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1321
[2019-04-09 15:21:50,628] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 19.0, 27.01241078513558, 0.8155215827440357, 0.0, 1.0, 65.0, 33.74415113673852], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3079200.0000, 
sim time next is 3079800.0000, 
raw observation next is [0.5, 39.5, 84.0, 673.0, 19.0, 27.04312824348271, 0.8240884126772231, 0.0, 1.0, 25.0, 29.20532341783212], 
processed observation next is [0.0, 0.6521739130434783, 0.4764542936288089, 0.395, 0.28, 0.7436464088397791, 0.08333333333333333, 0.7535940202902257, 0.7746961375590744, 0.0, 1.0, 0.2, 0.2920532341783212], 
reward next is 0.7079, 
noisyNet noise sample is [array([-1.1200933], dtype=float32), 1.7927654]. 
=============================================
[2019-04-09 15:21:50,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.74803707e-05 8.90025198e-02 1.14231616e-01 1.31802019e-02
 2.11109896e-03 2.46980489e-04 3.13600957e-01 8.92968103e-03
 3.22697721e-02 2.38790065e-02 4.02490765e-01], sum to 1.0000
[2019-04-09 15:21:50,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-09 15:21:50,742] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.0, 70.5, 579.5, 19.0, 27.17222109606786, 0.8337111312133088, 0.0, 1.0, 65.0, 32.17708652905011], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3081600.0000, 
sim time next is 3082200.0000, 
raw observation next is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 19.0, 27.17670529510784, 0.8321709940636243, 0.0, 1.0, 65.0, 32.66204087098887], 
processed observation next is [0.0, 0.6956521739130435, 0.4856879039704525, 0.4533333333333334, 0.22, 0.6058931860036832, 0.08333333333333333, 0.7647254412589867, 0.7773903313545415, 0.0, 1.0, 1.0, 0.3266204087098887], 
reward next is 0.6734, 
noisyNet noise sample is [array([0.44263473], dtype=float32), -1.9642419]. 
=============================================
[2019-04-09 15:21:51,437] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-09 15:21:51,437] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:21:51,437] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:21:51,440] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:21:51,441] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:21:51,443] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:21:51,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:21:51,446] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run23
[2019-04-09 15:21:51,465] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run23
[2019-04-09 15:21:51,465] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run23
[2019-04-09 15:22:03,976] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.03762678], dtype=float32), 0.04232291]
[2019-04-09 15:22:03,977] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-6.0, 59.0, 158.0, 346.0, 22.5, 26.6535723900187, 0.6664029758860036, 1.0, 1.0, 65.0, 44.14377868889139]
[2019-04-09 15:22:03,977] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:22:03,978] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [3.0356534e-05 9.9384397e-02 1.5877894e-01 1.3872073e-02 8.9369126e-04
 9.6741802e-05 3.4846222e-01 5.8800112e-03 2.2943936e-02 1.4906621e-02
 3.3475095e-01], sampled 0.10783957132542565
[2019-04-09 15:23:31,342] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5645.7160 285033.7123 2926.5591
[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,371] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:31,498] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,464] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5284.7954 321123.7098 2235.4102
[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,484] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:37,603] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,339] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5388.6615 310746.4538 2611.9005
[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,359] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:38,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:23:39,362] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 220000, evaluation results [220000.0, 5388.661492550798, 310746.4537893914, 2611.9005109453096, 5645.7160114546905, 285033.71227994724, 2926.5590600385967, 5284.795374158934, 321123.70980369195, 2235.4102347135854]
[2019-04-09 15:23:39,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0313708e-05 4.4448927e-02 1.3324395e-01 5.5806874e-03 2.5356867e-04
 2.4852325e-05 6.5469486e-01 2.0841567e-03 1.0691784e-02 5.6655384e-03
 1.4329146e-01], sum to 1.0000
[2019-04-09 15:23:39,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7877
[2019-04-09 15:23:39,661] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 22.5, 26.91783097305619, 0.6816381129071641, 1.0, 1.0, 45.0, 39.93677873618623], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3138600.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 22.5, 26.89043610428988, 0.7132094858763408, 1.0, 1.0, 45.0, 41.69253056811918], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 0.375, 0.7408696753574899, 0.7377364952921136, 1.0, 1.0, 0.6, 0.4169253056811918], 
reward next is 0.5831, 
noisyNet noise sample is [array([0.13440733], dtype=float32), 0.16970706]. 
=============================================
[2019-04-09 15:23:40,373] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1605843e-05 1.0448677e-01 1.4145285e-01 6.2747481e-03 6.1586936e-04
 1.3312434e-04 3.0831465e-01 7.7267992e-03 1.2974023e-02 2.2882110e-02
 3.9510742e-01], sum to 1.0000
[2019-04-09 15:23:40,380] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0018
[2019-04-09 15:23:40,395] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 27.32810979536234, 0.9664379210405954, 0.0, 1.0, 45.0, 37.66862577497744], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3207000.0000, 
sim time next is 3207600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.24628585781893, 0.9654960069892238, 0.0, 1.0, 20.0, 35.57160582737723], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7705238214849107, 0.8218320023297413, 0.0, 1.0, 0.1, 0.35571605827377234], 
reward next is 0.6443, 
noisyNet noise sample is [array([-1.4761888], dtype=float32), 0.53361666]. 
=============================================
[2019-04-09 15:23:40,401] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6420734e-05 5.3843226e-02 1.3871245e-01 8.3277570e-03 3.2988435e-04
 4.1823103e-05 3.7007451e-01 5.7021012e-03 1.0289430e-02 9.3149114e-03
 4.0334743e-01], sum to 1.0000
[2019-04-09 15:23:40,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-09 15:23:40,415] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.62629017453633, 0.82104002747876, 0.0, 1.0, 65.0, 52.63426289074687], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3213600.0000, 
sim time next is 3214200.0000, 
raw observation next is [-1.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.52663089885821, 0.8144569864350472, 0.0, 1.0, 30.0, 40.03893310845199], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7105525749048507, 0.7714856621450158, 0.0, 1.0, 0.3, 0.4003893310845199], 
reward next is 0.5996, 
noisyNet noise sample is [array([-1.502629], dtype=float32), -1.1153208]. 
=============================================
[2019-04-09 15:23:40,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2665820e-06 1.1681857e-01 2.0955685e-01 2.7854550e-03 1.2139004e-04
 1.0674557e-05 2.9409218e-01 1.4924952e-03 6.4827297e-03 3.2908251e-03
 3.6534661e-01], sum to 1.0000
[2019-04-09 15:23:40,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7223
[2019-04-09 15:23:40,594] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 22.5, 28.65526176570524, 1.226737116302859, 1.0, 1.0, 65.0, 10.88026524479946], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3158400.0000, 
sim time next is 3159000.0000, 
raw observation next is [7.0, 100.0, 112.0, 806.0, 22.5, 28.72355830999963, 1.241074156079781, 1.0, 1.0, 65.0, 9.071966818629274], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.37333333333333335, 0.8906077348066298, 0.375, 0.8936298591666357, 0.9136913853599271, 1.0, 1.0, 1.0, 0.09071966818629275], 
reward next is 0.9093, 
noisyNet noise sample is [array([-0.7083863], dtype=float32), 0.12087668]. 
=============================================
[2019-04-09 15:23:40,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[19.42818 ]
 [19.151676]
 [19.162321]
 [19.27247 ]
 [19.291527]], R is [[20.22628021]
 [20.91521454]
 [21.61547279]
 [22.28796387]
 [22.9485836 ]].
[2019-04-09 15:23:40,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1513027e-07 4.3262281e-02 1.3005379e-01 3.8012324e-03 1.6951658e-04
 1.0452606e-05 3.0482623e-01 1.5199992e-03 9.9571729e-03 6.5021189e-03
 4.9989638e-01], sum to 1.0000
[2019-04-09 15:23:40,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1592
[2019-04-09 15:23:40,667] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.85319042678848, 1.058014507159167, 1.0, 1.0, 45.0, 21.55418494455423], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.84807676407334, 1.055200469057472, 1.0, 1.0, 25.0, 25.37179332087716], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.8206730636727784, 0.8517334896858241, 1.0, 1.0, 0.2, 0.2537179332087716], 
reward next is 0.7463, 
noisyNet noise sample is [array([0.49769998], dtype=float32), 1.557085]. 
=============================================
[2019-04-09 15:23:41,381] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2930673e-06 4.0648788e-02 2.7343553e-02 1.3864189e-03 7.1648581e-05
 2.5723098e-06 4.0794462e-01 8.4731221e-04 5.9951185e-03 2.7532282e-03
 5.1300544e-01], sum to 1.0000
[2019-04-09 15:23:41,383] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1743
[2019-04-09 15:23:41,397] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 88.0, 107.6666666666667, 735.5, 22.5, 27.96759922788161, 1.081844442431029, 1.0, 1.0, 25.0, 22.46938204302394], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3234000.0000, 
sim time next is 3234600.0000, 
raw observation next is [-2.7, 86.0, 109.0, 752.0, 22.5, 27.97724682045092, 1.088423769196087, 1.0, 1.0, 45.0, 23.86212545717746], 
processed observation next is [1.0, 0.43478260869565216, 0.38781163434903054, 0.86, 0.36333333333333334, 0.830939226519337, 0.375, 0.8314372350375766, 0.8628079230653624, 1.0, 1.0, 0.6, 0.2386212545717746], 
reward next is 0.7614, 
noisyNet noise sample is [array([-1.052605], dtype=float32), 2.4320245]. 
=============================================
[2019-04-09 15:23:41,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5871332e-06 2.5456566e-01 9.9006467e-02 3.8077335e-03 1.8216050e-04
 1.5098209e-05 2.9334855e-01 1.5737849e-03 8.8529969e-03 6.9926004e-03
 3.3165243e-01], sum to 1.0000
[2019-04-09 15:23:41,468] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9913
[2019-04-09 15:23:41,481] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.333333333333334, 97.66666666666667, 113.0, 795.1666666666666, 22.5, 28.46325285583725, 1.101734191376625, 1.0, 1.0, 20.0, 14.52904756526905], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3151200.0000, 
sim time next is 3151800.0000, 
raw observation next is [7.5, 96.5, 114.0, 805.0, 22.5, 28.43703423524961, 1.120045793246341, 1.0, 1.0, 65.0, 14.12137447847633], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.965, 0.38, 0.8895027624309392, 0.375, 0.8697528529374674, 0.8733485977487803, 1.0, 1.0, 1.0, 0.1412137447847633], 
reward next is 0.8588, 
noisyNet noise sample is [array([0.8544104], dtype=float32), 1.1103323]. 
=============================================
[2019-04-09 15:23:41,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5093448e-06 6.7858761e-03 1.3112067e-01 3.9574718e-03 8.1929662e-05
 5.7183111e-06 1.6793090e-01 5.5781362e-04 3.1904280e-03 5.5377758e-03
 6.8082982e-01], sum to 1.0000
[2019-04-09 15:23:41,677] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5052
[2019-04-09 15:23:41,690] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.066666666666666, 72.5, 113.6666666666667, 815.0, 22.5, 28.18059021335403, 1.131605411769155, 1.0, 1.0, 65.0, 21.5611416609524], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3239400.0000, 
sim time next is 3240000.0000, 
raw observation next is [-2.0, 71.0, 114.0, 817.0, 22.5, 28.17516867308859, 1.135268048246573, 1.0, 1.0, 65.0, 18.72439061038756], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.71, 0.38, 0.9027624309392265, 0.375, 0.8479307227573823, 0.8784226827488576, 1.0, 1.0, 1.0, 0.1872439061038756], 
reward next is 0.8128, 
noisyNet noise sample is [array([0.22208102], dtype=float32), -0.2869664]. 
=============================================
[2019-04-09 15:23:41,699] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[18.45919 ]
 [18.593178]
 [18.288792]
 [18.621943]
 [18.625118]], R is [[19.32486916]
 [19.9160099 ]
 [20.54241562]
 [21.14163208]
 [21.88749886]].
[2019-04-09 15:23:41,871] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4798321e-07 3.4883790e-02 5.8668002e-02 8.8164833e-04 2.4825038e-05
 4.6118980e-06 7.6806432e-01 5.9538230e-04 1.4965968e-03 2.1006120e-03
 1.3327935e-01], sum to 1.0000
[2019-04-09 15:23:41,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7339
[2019-04-09 15:23:41,884] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 22.5, 27.84801591089922, 1.198932839352066, 1.0, 1.0, 65.0, 25.73903571826048], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 27.85199098989784, 1.190015217143088, 0.0, 1.0, 45.0, 22.57703856725982], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8209992491581533, 0.896671739047696, 0.0, 1.0, 0.6, 0.2257703856725982], 
reward next is 0.7742, 
noisyNet noise sample is [array([-0.38032815], dtype=float32), -0.8064771]. 
=============================================
[2019-04-09 15:23:41,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[19.30619 ]
 [19.386868]
 [19.620605]
 [19.372482]
 [19.59508 ]], R is [[19.38323212]
 [19.93200874]
 [20.51074982]
 [21.09312248]
 [21.6726532 ]].
[2019-04-09 15:23:41,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6264897e-05 6.5578893e-02 1.2571530e-01 2.9521550e-03 4.3397903e-04
 3.2155975e-05 4.0731388e-01 3.4388665e-03 1.1001020e-02 8.7951357e-03
 3.7471232e-01], sum to 1.0000
[2019-04-09 15:23:41,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9065
[2019-04-09 15:23:41,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6146616e-07 3.5196122e-02 8.5285164e-02 1.3083338e-03 4.9153536e-05
 2.4922333e-06 1.0899133e-01 9.5963100e-04 3.3277050e-03 3.4617130e-03
 7.6141751e-01], sum to 1.0000
[2019-04-09 15:23:41,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5851
[2019-04-09 15:23:42,000] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 100.0, 0.0, 0.0, 19.0, 26.95271797872563, 0.9061812317383561, 0.0, 1.0, 65.0, 43.04140321434795], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [-2.166666666666667, 100.0, 0.0, 0.0, 19.0, 26.993005448925, 0.8967018078336406, 0.0, 1.0, 65.0, 39.37281367829042], 
processed observation next is [1.0, 0.21739130434782608, 0.4025854108956602, 1.0, 0.0, 0.0, 0.08333333333333333, 0.74941712074375, 0.7989006026112135, 0.0, 1.0, 1.0, 0.3937281367829042], 
reward next is 0.6063, 
noisyNet noise sample is [array([-0.50265235], dtype=float32), -0.8822746]. 
=============================================
[2019-04-09 15:23:42,005] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.5, 111.0, 812.0, 22.5, 28.15258990502758, 1.139869464308912, 1.0, 1.0, 65.0, 19.82192004984869], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3245400.0000, 
sim time next is 3246000.0000, 
raw observation next is [-3.333333333333333, 95.0, 109.3333333333333, 804.8333333333334, 22.5, 28.23872973047457, 1.149373670119465, 1.0, 1.0, 65.0, 19.98447434820348], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.95, 0.36444444444444435, 0.8893186003683242, 0.375, 0.8532274775395475, 0.8831245567064884, 1.0, 1.0, 1.0, 0.19984474348203482], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.4760809], dtype=float32), -0.4489313]. 
=============================================
[2019-04-09 15:23:42,009] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[19.683392]
 [19.251637]
 [19.451939]
 [19.410585]
 [19.441519]], R is [[20.11268997]
 [20.71334457]
 [21.2718544 ]
 [21.80857277]
 [22.37627983]].
[2019-04-09 15:23:42,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5513258e-06 5.6583758e-02 1.5245427e-01 2.6991689e-03 1.0741980e-04
 8.9233472e-06 4.0238622e-01 1.3129601e-03 7.8323754e-03 5.2114772e-03
 3.7140185e-01], sum to 1.0000
[2019-04-09 15:23:42,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-09 15:23:42,063] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 22.5, 28.05536527496619, 1.102986842619349, 1.0, 1.0, 45.0, 17.63549975715136], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3243000.0000, 
sim time next is 3243600.0000, 
raw observation next is [-2.0, 85.0, 113.0, 817.5, 22.5, 28.06760031954464, 1.130737019510579, 1.0, 1.0, 45.0, 22.71713021174686], 
processed observation next is [1.0, 0.5652173913043478, 0.40720221606648205, 0.85, 0.37666666666666665, 0.9033149171270718, 0.375, 0.8389666932953865, 0.8769123398368596, 1.0, 1.0, 0.6, 0.22717130211746858], 
reward next is 0.7728, 
noisyNet noise sample is [array([0.22498223], dtype=float32), -0.15528974]. 
=============================================
[2019-04-09 15:23:42,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2987382e-07 3.5013437e-02 4.9551848e-02 1.4892656e-03 8.5912281e-05
 5.3230692e-06 4.0116853e-01 1.2266662e-03 4.7114245e-03 7.5158253e-03
 4.9923110e-01], sum to 1.0000
[2019-04-09 15:23:42,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1065
[2019-04-09 15:23:42,722] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.7108856573689, 1.166925961158553, 0.0, 1.0, 45.0, 23.5993542072893], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3185400.0000, 
sim time next is 3186000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 27.69148284795327, 1.163649789532115, 0.0, 1.0, 45.0, 23.0485091227214], 
processed observation next is [1.0, 0.9130434782608695, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8076235706627726, 0.8878832631773715, 0.0, 1.0, 0.6, 0.23048509122721397], 
reward next is 0.7695, 
noisyNet noise sample is [array([1.4638852], dtype=float32), 1.0166041]. 
=============================================
[2019-04-09 15:23:42,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[18.778187]
 [18.910873]
 [19.245079]
 [19.312246]
 [19.53687 ]], R is [[19.27700233]
 [19.84823799]
 [20.38197708]
 [20.90969086]
 [21.43867493]].
[2019-04-09 15:23:42,818] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4013580e-06 6.2587619e-02 1.5378782e-01 3.0242661e-03 3.4067518e-05
 6.2687218e-06 5.1551759e-01 1.4068132e-04 3.4270745e-03 6.7062778e-03
 2.5476700e-01], sum to 1.0000
[2019-04-09 15:23:42,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1398
[2019-04-09 15:23:42,832] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.166666666666667, 72.0, 67.66666666666666, 569.3333333333333, 22.5, 27.13290353547247, 1.096898103152715, 1.0, 1.0, 25.0, 1.1712592610554], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3255000.0000, 
sim time next is 3255600.0000, 
raw observation next is [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 22.5, 28.38035280838769, 1.161094236053468, 1.0, 1.0, 25.0, 17.03672737562765], 
processed observation next is [1.0, 0.6956521739130435, 0.37026777469990774, 0.73, 0.21111111111111114, 0.5968692449355434, 0.375, 0.8650294006989743, 0.8870314120178225, 1.0, 1.0, 0.2, 0.1703672737562765], 
reward next is 0.8296, 
noisyNet noise sample is [array([-0.19095306], dtype=float32), -0.6772557]. 
=============================================
[2019-04-09 15:23:42,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3587246e-05 5.6195002e-02 2.2910129e-01 5.3836061e-03 3.4113761e-04
 8.4795654e-05 3.4557420e-01 3.7227934e-03 6.1116861e-03 9.7299926e-03
 3.4372193e-01], sum to 1.0000
[2019-04-09 15:23:42,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0921
[2019-04-09 15:23:42,863] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 27.31228086970729, 0.9604656107261752, 0.0, 1.0, 45.0, 38.64953175839704], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3207000.0000, 
sim time next is 3207600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.23034639185087, 0.9594765521642338, 0.0, 1.0, 25.0, 36.04172542336671], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7691955326542391, 0.819825517388078, 0.0, 1.0, 0.2, 0.36041725423366705], 
reward next is 0.6396, 
noisyNet noise sample is [array([-0.5796115], dtype=float32), -0.22041768]. 
=============================================
[2019-04-09 15:23:42,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.40988991e-05 9.54247043e-02 2.57785082e-01 4.46374528e-03
 8.38137174e-04 8.23978044e-05 2.83675164e-01 5.79810422e-03
 1.02066705e-02 2.11747475e-02 3.20517182e-01], sum to 1.0000
[2019-04-09 15:23:42,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7625
[2019-04-09 15:23:42,933] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 22.5, 26.54884928148748, 0.8060713010893704, 1.0, 1.0, 20.0, 31.56052631267138], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [-3.0, 92.0, 29.0, 178.0, 22.5, 26.55843336543757, 0.8264946593934619, 1.0, 1.0, 65.0, 59.32245335720258], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.09666666666666666, 0.19668508287292819, 0.375, 0.7132027804531308, 0.7754982197978206, 1.0, 1.0, 1.0, 0.5932245335720259], 
reward next is 0.4068, 
noisyNet noise sample is [array([0.30336592], dtype=float32), 0.6734111]. 
=============================================
[2019-04-09 15:23:42,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[14.041002 ]
 [13.775507 ]
 [13.9353695]
 [13.926426 ]
 [13.994949 ]], R is [[14.71078491]
 [15.24807167]
 [15.70998383]
 [15.9873457 ]
 [16.53567886]].
[2019-04-09 15:23:43,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7317441e-05 2.1303833e-02 1.2662044e-01 3.8326562e-03 5.8574410e-04
 8.1152299e-05 4.2565623e-01 3.3781296e-03 1.6589412e-02 1.2020008e-02
 3.8989511e-01], sum to 1.0000
[2019-04-09 15:23:43,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1028
[2019-04-09 15:23:43,066] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.21892632955328, 0.9548174348169315, 0.0, 1.0, 45.0, 36.31080405600536], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3207600.0000, 
sim time next is 3208200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.15568695811383, 0.9648578925783026, 0.0, 1.0, 25.0, 34.36531244216278], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7629739131761525, 0.8216192975261009, 0.0, 1.0, 0.2, 0.3436531244216278], 
reward next is 0.6563, 
noisyNet noise sample is [array([-0.3133382], dtype=float32), -0.76195294]. 
=============================================
[2019-04-09 15:23:43,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0845152e-06 6.4620756e-02 1.3950869e-01 1.9717664e-03 5.9177473e-05
 2.5300491e-05 3.1271333e-01 5.1654764e-03 7.5651696e-03 1.1683933e-02
 4.5668235e-01], sum to 1.0000
[2019-04-09 15:23:43,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3508
[2019-04-09 15:23:43,161] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 27.30586300308693, 1.046102586263457, 0.0, 1.0, 45.0, 30.88805006769895], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3194400.0000, 
sim time next is 3195000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 27.28689942794056, 1.042058895715082, 0.0, 1.0, 45.0, 31.0349364245579], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7739082856617134, 0.8473529652383607, 0.0, 1.0, 0.6, 0.31034936424557896], 
reward next is 0.6897, 
noisyNet noise sample is [array([1.5853692], dtype=float32), -0.16464563]. 
=============================================
[2019-04-09 15:23:43,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[16.736444]
 [16.932156]
 [16.917612]
 [17.13959 ]
 [17.28298 ]], R is [[17.34040451]
 [17.85811996]
 [18.37075996]
 [18.89044571]
 [19.38124466]].
[2019-04-09 15:23:43,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5353369e-05 5.1856603e-02 1.1734205e-01 5.2114753e-03 1.2429536e-04
 7.8642268e-05 3.9330271e-01 4.5949062e-03 9.5158527e-03 1.2150169e-02
 4.0580800e-01], sum to 1.0000
[2019-04-09 15:23:43,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4752
[2019-04-09 15:23:43,404] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 100.0, 0.0, 0.0, 19.0, 26.91080920778712, 0.8836565165526435, 0.0, 1.0, 25.0, 36.74644773636715], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [-2.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.90844848367526, 0.8761951912729341, 0.0, 1.0, 45.0, 36.2707619857411], 
processed observation next is [1.0, 0.21739130434782608, 0.38873499538319484, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7423707069729385, 0.7920650637576446, 0.0, 1.0, 0.6, 0.362707619857411], 
reward next is 0.6373, 
noisyNet noise sample is [array([-0.08187487], dtype=float32), 0.20633687]. 
=============================================
[2019-04-09 15:23:43,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8684892e-05 1.6961752e-02 9.9362336e-02 2.7838089e-03 5.3347507e-04
 2.0358653e-05 4.8854223e-01 2.2231063e-03 1.1624429e-02 9.4287386e-03
 3.6850104e-01], sum to 1.0000
[2019-04-09 15:23:43,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2381
[2019-04-09 15:23:43,758] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 100.0, 0.0, 0.0, 19.0, 27.18335920630737, 0.9761946082914936, 0.0, 1.0, 65.0, 40.26423320534533], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3204600.0000, 
sim time next is 3205200.0000, 
raw observation next is [-0.3333333333333333, 100.0, 0.0, 0.0, 19.0, 27.19733401073583, 0.9779850565956396, 0.0, 1.0, 45.0, 37.1105332865198], 
processed observation next is [1.0, 0.08695652173913043, 0.4533702677747, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7664445008946524, 0.8259950188652132, 0.0, 1.0, 0.6, 0.371105332865198], 
reward next is 0.6289, 
noisyNet noise sample is [array([0.5272772], dtype=float32), -0.031204246]. 
=============================================
[2019-04-09 15:23:43,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7636593e-05 6.2274825e-02 9.0147056e-02 7.1298331e-03 3.9246876e-04
 1.9270973e-04 2.6526555e-01 1.0057161e-02 7.3284437e-03 2.7053151e-02
 5.3012121e-01], sum to 1.0000
[2019-04-09 15:23:43,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-09 15:23:43,785] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.83333333333333, 76.0, 0.0, 0.0, 19.0, 26.11379615960235, 0.5930875276422962, 0.0, 1.0, 65.0, 53.22907501240621], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3304200.0000, 
sim time next is 3304800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 26.05600223748658, 0.5876536681503434, 0.0, 1.0, 45.0, 51.49472316130566], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6713335197905484, 0.6958845560501145, 0.0, 1.0, 0.6, 0.5149472316130566], 
reward next is 0.4851, 
noisyNet noise sample is [array([-0.9149022], dtype=float32), 1.1237867]. 
=============================================
[2019-04-09 15:23:43,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2437225e-05 2.0542335e-02 8.8971920e-02 2.9199321e-03 1.5729379e-04
 3.6157428e-05 6.8878174e-01 3.1147499e-03 5.4969327e-03 1.0080170e-02
 1.7988631e-01], sum to 1.0000
[2019-04-09 15:23:43,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5265
[2019-04-09 15:23:43,983] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 22.5, 26.93091789358578, 0.8579214706483526, 1.0, 1.0, 45.0, 38.25440509789305], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [-3.0, 92.0, 29.0, 178.0, 22.5, 26.89782507546393, 0.8702410227323488, 1.0, 1.0, 45.0, 37.97452209314138], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.09666666666666666, 0.19668508287292819, 0.375, 0.7414854229553276, 0.7900803409107829, 1.0, 1.0, 0.6, 0.37974522093141383], 
reward next is 0.6203, 
noisyNet noise sample is [array([-0.16297634], dtype=float32), -1.4925971]. 
=============================================
[2019-04-09 15:23:44,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[15.626164]
 [15.030638]
 [15.307074]
 [15.499224]
 [15.269923]], R is [[16.26375198]
 [16.71857071]
 [17.23125648]
 [17.68053436]
 [18.04566956]].
[2019-04-09 15:23:44,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2584809e-05 4.4095770e-02 4.6445675e-02 4.2870552e-03 3.4642124e-04
 4.4589375e-05 6.4352643e-01 2.6308885e-03 9.5786992e-03 6.4796098e-03
 2.4255221e-01], sum to 1.0000
[2019-04-09 15:23:44,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1374
[2019-04-09 15:23:44,208] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.98182510119808, 0.9039195048362748, 0.0, 1.0, 45.0, 41.2605286825567], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3213600.0000, 
sim time next is 3214200.0000, 
raw observation next is [-1.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.95995933303212, 0.8954064145336265, 0.0, 1.0, 45.0, 33.7479644985369], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7466632777526767, 0.7984688048445422, 0.0, 1.0, 0.6, 0.33747964498536903], 
reward next is 0.6625, 
noisyNet noise sample is [array([-0.36610612], dtype=float32), 1.1277187]. 
=============================================
[2019-04-09 15:23:44,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8693531e-06 9.7986303e-02 1.1651153e-01 4.9903863e-03 9.6551557e-05
 1.4987114e-05 4.4329914e-01 3.1245451e-03 5.5205305e-03 6.5737949e-03
 3.2188046e-01], sum to 1.0000
[2019-04-09 15:23:44,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3684
[2019-04-09 15:23:44,337] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 90.0, 106.3333333333333, 719.0, 22.5, 27.92885938622814, 1.081062008523722, 1.0, 1.0, 45.0, 20.97176092907134], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3233400.0000, 
sim time next is 3234000.0000, 
raw observation next is [-2.8, 88.0, 107.6666666666667, 735.5, 22.5, 27.99994473985286, 1.093647561573805, 1.0, 1.0, 45.0, 22.54684858798666], 
processed observation next is [1.0, 0.43478260869565216, 0.38504155124653744, 0.88, 0.358888888888889, 0.812707182320442, 0.375, 0.8333287283210717, 0.8645491871912684, 1.0, 1.0, 0.6, 0.2254684858798666], 
reward next is 0.7745, 
noisyNet noise sample is [array([1.0296413], dtype=float32), -0.09066862]. 
=============================================
[2019-04-09 15:23:44,357] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[18.270212]
 [17.792559]
 [17.735662]
 [17.69282 ]
 [17.379456]], R is [[19.03689957]
 [19.63681412]
 [20.20020294]
 [20.73968315]
 [21.31103897]].
[2019-04-09 15:23:44,598] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.5505693e-06 4.6259645e-02 1.0592200e-01 2.9630617e-03 2.0174697e-04
 1.9666788e-05 5.0878793e-01 1.5567833e-03 5.9324820e-03 1.8463776e-02
 3.0988735e-01], sum to 1.0000
[2019-04-09 15:23:44,598] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5925
[2019-04-09 15:23:44,619] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 86.66666666666667, 0.0, 0.0, 19.0, 26.6813989335944, 0.7976509280265284, 0.0, 1.0, 45.0, 47.12798218924802], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3282000.0000, 
sim time next is 3282600.0000, 
raw observation next is [-6.833333333333334, 85.33333333333333, 0.0, 0.0, 19.0, 26.66166373838054, 0.7890789612497041, 0.0, 1.0, 45.0, 37.65776971212156], 
processed observation next is [1.0, 1.0, 0.27331486611265005, 0.8533333333333333, 0.0, 0.0, 0.08333333333333333, 0.7218053115317117, 0.763026320416568, 0.0, 1.0, 0.6, 0.37657769712121564], 
reward next is 0.6234, 
noisyNet noise sample is [array([-0.37502155], dtype=float32), 0.41165188]. 
=============================================
[2019-04-09 15:23:44,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.3039824e-07 3.3915266e-02 1.4012656e-01 2.7189136e-03 6.7566987e-05
 2.6959581e-06 4.1013789e-01 5.5237524e-03 4.3100007e-03 8.7138023e-03
 3.9448258e-01], sum to 1.0000
[2019-04-09 15:23:44,960] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8533
[2019-04-09 15:23:44,997] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.5, 111.0, 812.0, 22.5, 28.15347403858267, 1.145560999833947, 1.0, 1.0, 65.0, 20.10096204536346], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3245400.0000, 
sim time next is 3246000.0000, 
raw observation next is [-3.333333333333333, 95.0, 109.3333333333333, 804.8333333333334, 22.5, 28.24634746778407, 1.156200090992131, 1.0, 1.0, 65.0, 19.94425640827073], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.95, 0.36444444444444435, 0.8893186003683242, 0.375, 0.8538622889820058, 0.8854000303307105, 1.0, 1.0, 1.0, 0.19944256408270727], 
reward next is 0.8006, 
noisyNet noise sample is [array([-0.513662], dtype=float32), -0.7078595]. 
=============================================
[2019-04-09 15:23:45,017] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[18.604582]
 [18.474741]
 [18.535542]
 [18.225801]
 [18.204832]], R is [[19.18896294]
 [19.79606438]
 [20.36001587]
 [20.90995407]
 [21.5122261 ]].
[2019-04-09 15:23:45,405] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.4564281e-06 5.7441149e-02 7.8886881e-02 2.6243993e-03 8.9473571e-05
 2.9504296e-05 6.3849068e-01 1.1385523e-03 8.9163557e-03 4.2239660e-03
 2.0815057e-01], sum to 1.0000
[2019-04-09 15:23:45,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8155
[2019-04-09 15:23:45,432] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6685956e-05 4.3498401e-02 1.2027664e-01 5.3836252e-03 7.6440745e-04
 1.1352668e-04 2.8259218e-01 9.5328707e-03 8.1407921e-03 2.1464650e-02
 5.0817621e-01], sum to 1.0000
[2019-04-09 15:23:45,432] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 22.5, 27.42093989317718, 0.9575866083811869, 1.0, 1.0, 45.0, 26.96901383101439], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3228600.0000, 
sim time next is 3229200.0000, 
raw observation next is [-3.0, 92.0, 93.0, 511.5, 22.5, 27.51314370404271, 0.9728228798071866, 1.0, 1.0, 65.0, 45.01399916235545], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.31, 0.5651933701657459, 0.375, 0.7927619753368926, 0.8242742932690622, 1.0, 1.0, 1.0, 0.45013999162355445], 
reward next is 0.5499, 
noisyNet noise sample is [array([0.43840832], dtype=float32), 0.05712662]. 
=============================================
[2019-04-09 15:23:45,437] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-09 15:23:45,458] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 19.0, 26.32663187860062, 0.6515343278466325, 0.0, 1.0, 45.0, 37.25147076740054], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3298800.0000, 
sim time next is 3299400.0000, 
raw observation next is [-9.45, 76.5, 0.0, 0.0, 19.0, 26.32862106028796, 0.6288812828265927, 0.0, 1.0, 45.0, 37.90273266899491], 
processed observation next is [1.0, 0.17391304347826086, 0.20083102493074795, 0.765, 0.0, 0.0, 0.08333333333333333, 0.6940517550239967, 0.7096270942755308, 0.0, 1.0, 0.6, 0.3790273266899491], 
reward next is 0.6210, 
noisyNet noise sample is [array([0.30455798], dtype=float32), 0.49142322]. 
=============================================
[2019-04-09 15:23:45,734] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1629200e-05 5.3824596e-02 1.0197933e-01 4.9267556e-03 2.3194554e-04
 2.3676534e-05 5.6016093e-01 6.0315537e-03 1.3541904e-02 1.2686204e-02
 2.4658142e-01], sum to 1.0000
[2019-04-09 15:23:45,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9216
[2019-04-09 15:23:45,751] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 19.0, 26.44288904934732, 0.723430079710227, 0.0, 1.0, 45.0, 49.16775671140811], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [-7.0, 77.0, 0.0, 0.0, 19.0, 26.45789766634232, 0.7291120234393954, 0.0, 1.0, 65.0, 52.68311527148194], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7048248055285266, 0.7430373411464651, 0.0, 1.0, 1.0, 0.5268311527148194], 
reward next is 0.4732, 
noisyNet noise sample is [array([1.0855031], dtype=float32), -0.7121914]. 
=============================================
[2019-04-09 15:23:45,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7079512e-05 4.8274033e-02 1.2764551e-01 4.7649834e-03 4.9920502e-04
 2.5292145e-05 5.1330757e-01 6.5977378e-03 1.6367545e-02 1.0545138e-02
 2.7195588e-01], sum to 1.0000
[2019-04-09 15:23:45,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3355
[2019-04-09 15:23:45,762] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[15.188221]
 [15.532803]
 [15.408725]
 [15.636214]
 [15.588186]], R is [[15.41083145]
 [15.76504612]
 [16.06527138]
 [16.41692734]
 [16.62561798]].
[2019-04-09 15:23:45,771] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 96.0, 0.0, 0.0, 19.0, 26.84102038430485, 0.8546161907485499, 0.0, 1.0, 45.0, 37.20025435505835], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 26.79616922496281, 0.8482731564461732, 0.0, 1.0, 45.0, 39.0670334211807], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9466666666666665, 0.0, 0.0, 0.08333333333333333, 0.7330141020802342, 0.782757718815391, 0.0, 1.0, 0.6, 0.390670334211807], 
reward next is 0.6093, 
noisyNet noise sample is [array([-0.11125214], dtype=float32), -0.57559466]. 
=============================================
[2019-04-09 15:23:45,906] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3905184e-05 5.1993072e-02 1.2645042e-01 4.7967844e-03 3.1485152e-04
 4.9211976e-05 4.6077424e-01 4.4670566e-03 1.1815807e-02 1.2947216e-02
 3.2635745e-01], sum to 1.0000
[2019-04-09 15:23:45,906] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7203
[2019-04-09 15:23:45,922] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 25.86961426853388, 0.5555354590114768, 0.0, 1.0, 65.0, 56.50387570495231], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 25.8572775782662, 0.5598719334507504, 0.0, 1.0, 65.0, 57.6763553251035], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6547731315221833, 0.6866239778169168, 0.0, 1.0, 1.0, 0.576763553251035], 
reward next is 0.4232, 
noisyNet noise sample is [array([0.25711194], dtype=float32), -0.89056665]. 
=============================================
[2019-04-09 15:23:46,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4318825e-06 2.6992057e-02 1.1351878e-01 1.4012051e-03 1.8043851e-04
 3.9680140e-06 6.0851765e-01 6.3402823e-04 7.4595902e-03 4.5272605e-03
 2.3676366e-01], sum to 1.0000
[2019-04-09 15:23:46,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3993
[2019-04-09 15:23:46,299] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 22.5, 27.42704042797712, 0.9874049590549733, 1.0, 1.0, 65.0, 37.47821882788185], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3267000.0000, 
sim time next is 3267600.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 22.5, 27.3780214046003, 0.9778065889298864, 1.0, 1.0, 45.0, 30.71163944606023], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.69, 0.0, 0.0, 0.375, 0.7815017837166917, 0.8259355296432954, 1.0, 1.0, 0.6, 0.3071163944606023], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.9024914], dtype=float32), -0.032367624]. 
=============================================
[2019-04-09 15:23:46,327] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8708204e-06 6.2383678e-02 1.7380737e-01 1.6354112e-03 3.0304052e-04
 4.0482273e-06 4.3469042e-01 2.7094744e-03 4.8620841e-03 8.1270766e-03
 3.1147549e-01], sum to 1.0000
[2019-04-09 15:23:46,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1539
[2019-04-09 15:23:46,342] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 22.5, 27.29201739959865, 0.9712480134486295, 0.0, 1.0, 25.0, 35.96586126529242], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [-4.166666666666667, 72.66666666666667, 0.0, 0.0, 19.0, 27.23557800142754, 0.9628167601624013, 0.0, 1.0, 25.0, 30.50087887066204], 
processed observation next is [1.0, 0.8695652173913043, 0.3471837488457987, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7696315001189618, 0.8209389200541337, 0.0, 1.0, 0.2, 0.3050087887066204], 
reward next is 0.6950, 
noisyNet noise sample is [array([0.5738052], dtype=float32), 1.451263]. 
=============================================
[2019-04-09 15:23:46,643] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7482220e-07 2.9117523e-02 3.2848215e-01 3.6772226e-03 1.4036524e-04
 4.9988234e-06 2.8160858e-01 2.2107491e-03 5.5958363e-03 9.7248899e-03
 3.3943704e-01], sum to 1.0000
[2019-04-09 15:23:46,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6738
[2019-04-09 15:23:46,672] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.80614796031659, 1.044519530829911, 1.0, 1.0, 65.0, 28.95575392732787], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.72122227879876, 1.036339131520493, 1.0, 1.0, 65.0, 32.0550443362885], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.8101018565665633, 0.8454463771734977, 1.0, 1.0, 1.0, 0.320550443362885], 
reward next is 0.6794, 
noisyNet noise sample is [array([-0.4498785], dtype=float32), 2.4958556]. 
=============================================
[2019-04-09 15:23:46,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4384207e-06 3.9484035e-02 1.2808190e-01 3.8263602e-03 2.2956867e-04
 1.8699353e-05 4.7836211e-01 1.7342377e-03 1.7980147e-02 9.7959572e-03
 3.2048357e-01], sum to 1.0000
[2019-04-09 15:23:46,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6252
[2019-04-09 15:23:46,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.90500598508504, 0.8108390892042988, 0.0, 1.0, 20.0, 38.59376991884021], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.89102056104352, 0.8069886656038912, 0.0, 1.0, 45.0, 32.29265799616017], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7409183800869599, 0.7689962218679637, 0.0, 1.0, 0.6, 0.32292657996160173], 
reward next is 0.6771, 
noisyNet noise sample is [array([0.40304095], dtype=float32), -1.5260236]. 
=============================================
[2019-04-09 15:23:47,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7484083e-06 4.0899567e-02 1.2683630e-01 4.1286419e-03 2.4376455e-04
 2.1654427e-05 4.6603835e-01 1.7472196e-03 1.9953191e-02 9.7034462e-03
 3.3042419e-01], sum to 1.0000
[2019-04-09 15:23:47,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-09 15:23:47,046] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.87998386612269, 0.8033793712533175, 0.0, 1.0, 65.0, 39.98525374388063], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3361200.0000, 
sim time next is 3361800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.86952594269389, 0.7999385280733277, 0.0, 1.0, 45.0, 38.47410796284279], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7391271618911576, 0.7666461760244426, 0.0, 1.0, 0.6, 0.38474107962842785], 
reward next is 0.6153, 
noisyNet noise sample is [array([0.40304095], dtype=float32), -1.5260236]. 
=============================================
[2019-04-09 15:23:47,056] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7458756e-05 3.6657277e-02 2.0036171e-01 1.0390768e-02 3.1770513e-04
 9.6786636e-05 3.4028518e-01 5.9222798e-03 1.1929281e-02 1.3143215e-02
 3.8086843e-01], sum to 1.0000
[2019-04-09 15:23:47,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9743
[2019-04-09 15:23:47,072] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 25.21986586257376, 0.4321614203432314, 0.0, 1.0, 65.0, 57.56039684160794], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 25.27067647573729, 0.4658646531091659, 0.0, 1.0, 65.0, 65.79845766534078], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6058897063114408, 0.6552882177030553, 0.0, 1.0, 1.0, 0.6579845766534078], 
reward next is 0.3420, 
noisyNet noise sample is [array([-0.09005489], dtype=float32), -1.0255588]. 
=============================================
[2019-04-09 15:23:47,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4413946e-07 8.3728738e-02 5.7928048e-02 3.3999933e-03 5.0184637e-05
 4.7327107e-06 2.5475791e-01 3.9980377e-04 2.6758886e-03 6.7698727e-03
 5.9028399e-01], sum to 1.0000
[2019-04-09 15:23:47,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5334
[2019-04-09 15:23:47,355] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 50.0, 106.0, 752.0, 22.5, 27.92973720308215, 0.986658060151441, 1.0, 1.0, 65.0, 21.66660659541677], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3335400.0000, 
sim time next is 3336000.0000, 
raw observation next is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 22.5, 27.97764162688692, 0.9893804843523489, 1.0, 1.0, 65.0, 21.98187929741421], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.5, 0.3427777777777777, 0.8165745856353591, 0.375, 0.8314701355739101, 0.8297934947841163, 1.0, 1.0, 1.0, 0.21981879297414209], 
reward next is 0.7802, 
noisyNet noise sample is [array([1.2849785], dtype=float32), -1.5404994]. 
=============================================
[2019-04-09 15:23:47,364] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[18.041704]
 [18.18895 ]
 [17.974234]
 [17.970385]
 [17.867838]], R is [[18.53932381]
 [19.13726616]
 [19.78821182]
 [20.3055172 ]
 [20.77982903]].
[2019-04-09 15:23:48,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4785079e-05 2.5411310e-02 2.2239214e-01 1.0981945e-02 3.3254933e-04
 6.5736669e-05 5.0398308e-01 5.1385337e-03 1.1937274e-02 1.2256454e-02
 2.0742616e-01], sum to 1.0000
[2019-04-09 15:23:48,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9593
[2019-04-09 15:23:48,062] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 62.5, 2.0, 107.0, 22.5, 26.14150269176832, 0.5476942697603101, 1.0, 1.0, 45.0, 35.78690906139732], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3396600.0000, 
sim time next is 3397200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 26.17235158495255, 0.5708815855205405, 1.0, 1.0, 65.0, 61.64572714696449], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.6166666666666667, 0.05388888888888887, 0.17624309392265194, 0.375, 0.6810292987460459, 0.6902938618401802, 1.0, 1.0, 1.0, 0.6164572714696449], 
reward next is 0.3835, 
noisyNet noise sample is [array([0.46906877], dtype=float32), 0.36454153]. 
=============================================
[2019-04-09 15:23:48,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.39624533e-07 2.71700285e-02 1.13280609e-01 4.73599369e-03
 1.05912404e-04 6.70568988e-06 3.83613318e-01 3.95259541e-03
 6.56577945e-03 2.60572089e-03 4.57962453e-01], sum to 1.0000
[2019-04-09 15:23:48,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-09 15:23:48,680] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 112.0, 784.0, 22.5, 27.40392477733789, 0.8788178357959073, 1.0, 1.0, 45.0, 35.57642727717428], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3409200.0000, 
sim time next is 3409800.0000, 
raw observation next is [3.0, 48.33333333333334, 113.0, 790.6666666666667, 22.5, 27.65661411948833, 0.9072814906691221, 1.0, 1.0, 65.0, 43.04719687380862], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.48333333333333345, 0.37666666666666665, 0.8736648250460406, 0.375, 0.8047178432906943, 0.802427163556374, 1.0, 1.0, 1.0, 0.43047196873808624], 
reward next is 0.5695, 
noisyNet noise sample is [array([-1.2189859], dtype=float32), -0.802778]. 
=============================================
[2019-04-09 15:23:48,793] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0380562e-06 3.8548008e-02 6.6154480e-02 2.1355618e-03 1.6448692e-04
 6.6603034e-06 2.7998629e-01 9.6439838e-04 1.5477205e-02 3.0169229e-03
 5.9354496e-01], sum to 1.0000
[2019-04-09 15:23:48,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3257
[2019-04-09 15:23:48,822] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 22.5, 27.02016095311616, 0.9468784726892115, 1.0, 1.0, 65.0, 30.85362625963128], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [-3.0, 50.0, 96.5, 713.0, 22.5, 26.29879000620777, 0.8435146923042023, 1.0, 1.0, 45.0, 6.854015413145849], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.5, 0.32166666666666666, 0.7878453038674034, 0.375, 0.6915658338506475, 0.7811715641014008, 1.0, 1.0, 0.6, 0.06854015413145849], 
reward next is 0.9315, 
noisyNet noise sample is [array([-0.56824386], dtype=float32), -0.2860892]. 
=============================================
[2019-04-09 15:23:49,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.06339976e-05 7.58612677e-02 6.62506595e-02 1.30822379e-02
 1.60412848e-04 3.81665268e-05 5.84801972e-01 3.86879337e-03
 1.35532394e-02 1.42654665e-02 2.28107169e-01], sum to 1.0000
[2019-04-09 15:23:49,089] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0158
[2019-04-09 15:23:49,107] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.333333333333334, 72.33333333333333, 105.1666666666667, 635.8333333333333, 22.5, 26.98237886173135, 0.7715764527575156, 1.0, 1.0, 45.0, 31.9073887939748], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3318000.0000, 
sim time next is 3318600.0000, 
raw observation next is [-8.166666666666666, 71.16666666666667, 106.3333333333333, 656.6666666666666, 22.5, 27.0976105220658, 0.7888333037202085, 1.0, 1.0, 20.0, 30.29906493394781], 
processed observation next is [1.0, 0.391304347826087, 0.23638042474607576, 0.7116666666666667, 0.35444444444444434, 0.725598526703499, 0.375, 0.75813421017215, 0.7629444345734028, 1.0, 1.0, 0.1, 0.3029906493394781], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.9854017], dtype=float32), -1.7906648]. 
=============================================
[2019-04-09 15:23:49,149] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1513042e-05 6.2605605e-02 1.2355162e-01 7.7967709e-03 4.0204229e-04
 8.6962173e-05 5.8115059e-01 8.0661690e-03 1.5034432e-02 1.0187718e-02
 1.9110654e-01], sum to 1.0000
[2019-04-09 15:23:49,149] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-09 15:23:49,166] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 19.0, 26.70016016665429, 0.7331043461133145, 0.0, 1.0, 45.0, 42.7004613342072], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.66185142519467, 0.7268387354207926, 0.0, 1.0, 65.0, 46.77893810763882], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7218209520995558, 0.7422795784735975, 0.0, 1.0, 1.0, 0.46778938107638823], 
reward next is 0.5322, 
noisyNet noise sample is [array([0.45143938], dtype=float32), -1.5539548]. 
=============================================
[2019-04-09 15:23:49,194] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[16.363966]
 [16.562616]
 [16.612621]
 [16.555836]
 [16.975311]], R is [[16.8319397 ]
 [17.23661613]
 [17.60691261]
 [18.04263687]
 [18.38618088]].
[2019-04-09 15:23:49,256] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7602991e-06 2.0928428e-02 5.2363873e-02 8.2366271e-03 3.5523891e-04
 6.7486981e-06 1.3548805e-01 2.7628264e-03 4.4153105e-03 1.9160275e-03
 7.7352417e-01], sum to 1.0000
[2019-04-09 15:23:49,258] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3112
[2019-04-09 15:23:49,285] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.333333333333333, 66.0, 111.8333333333333, 749.6666666666667, 22.5, 27.37073361123248, 0.8638435391719946, 1.0, 1.0, 65.0, 28.98569029720182], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3321600.0000, 
sim time next is 3322200.0000, 
raw observation next is [-7.166666666666667, 65.0, 112.6666666666667, 759.3333333333333, 22.5, 27.43212082057067, 0.5619535726129337, 1.0, 1.0, 65.0, 57.07974541661978], 
processed observation next is [1.0, 0.43478260869565216, 0.26408125577100644, 0.65, 0.37555555555555564, 0.8390423572744014, 0.375, 0.7860100683808892, 0.6873178575376446, 1.0, 1.0, 1.0, 0.5707974541661978], 
reward next is 0.4292, 
noisyNet noise sample is [array([0.6239932], dtype=float32), 1.0703849]. 
=============================================
[2019-04-09 15:23:49,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7075516e-06 2.2637735e-01 7.0626773e-02 3.7233401e-03 9.7955875e-05
 1.1742038e-05 1.6789190e-01 1.9816733e-03 2.1253146e-02 6.2127057e-03
 5.0182068e-01], sum to 1.0000
[2019-04-09 15:23:49,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-09 15:23:49,354] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 22.5, 27.45668178632969, 0.9260363647991294, 1.0, 1.0, 45.0, 35.8534055254821], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 22.5, 26.98571114097241, 0.8632224944317078, 1.0, 1.0, 20.0, 25.04441685144743], 
processed observation next is [1.0, 0.5217391304347826, 0.3056325023084026, 0.54, 0.3922222222222223, 0.8937384898710866, 0.375, 0.7488092617477008, 0.787740831477236, 1.0, 1.0, 0.1, 0.2504441685144743], 
reward next is 0.7496, 
noisyNet noise sample is [array([0.72457075], dtype=float32), 0.49161407]. 
=============================================
[2019-04-09 15:23:49,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4659158e-05 3.4515150e-02 1.3645706e-01 1.0208800e-02 3.2182218e-04
 8.1274622e-05 4.4758445e-01 5.9929984e-03 5.7216301e-03 9.3209799e-03
 3.4978127e-01], sum to 1.0000
[2019-04-09 15:23:49,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8386
[2019-04-09 15:23:49,491] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 70.0, 0.0, 0.0, 19.0, 26.59735929851751, 0.756229056225112, 0.0, 1.0, 45.0, 47.96779075732736], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3286800.0000, 
sim time next is 3287400.0000, 
raw observation next is [-7.166666666666667, 71.16666666666667, 0.0, 0.0, 19.0, 26.60118087258944, 0.7569461902536597, 0.0, 1.0, 65.0, 49.63040555608314], 
processed observation next is [1.0, 0.043478260869565216, 0.26408125577100644, 0.7116666666666667, 0.0, 0.0, 0.08333333333333333, 0.7167650727157865, 0.7523153967512198, 0.0, 1.0, 1.0, 0.4963040555608314], 
reward next is 0.5037, 
noisyNet noise sample is [array([-0.945221], dtype=float32), 0.37650275]. 
=============================================
[2019-04-09 15:23:49,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.7011663e-07 1.6801137e-01 2.7654633e-01 3.2472715e-03 5.0128750e-05
 3.8014966e-06 1.7158860e-01 7.2581979e-04 4.7391159e-03 2.3899209e-03
 3.7269691e-01], sum to 1.0000
[2019-04-09 15:23:49,902] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-09 15:23:49,914] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 22.5, 27.69572411995377, 0.9375716458699751, 1.0, 1.0, 45.0, 17.45383996945266], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3332400.0000, 
sim time next is 3333000.0000, 
raw observation next is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 22.5, 27.825438068985, 0.9645190660111643, 1.0, 1.0, 20.0, 22.19425905101628], 
processed observation next is [1.0, 0.5652173913043478, 0.3471837488457987, 0.5066666666666667, 0.371111111111111, 0.8662983425414365, 0.375, 0.8187865057487501, 0.8215063553370547, 1.0, 1.0, 0.1, 0.22194259051016282], 
reward next is 0.7781, 
noisyNet noise sample is [array([1.1944176], dtype=float32), -1.2626544]. 
=============================================
[2019-04-09 15:23:49,919] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[18.682886]
 [18.698633]
 [18.8152  ]
 [18.853092]
 [19.012413]], R is [[19.15638542]
 [19.7902832 ]
 [20.40917206]
 [21.11211777]
 [21.54566193]].
[2019-04-09 15:23:50,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.23573661e-05 9.38919336e-02 1.03666924e-01 4.73586097e-03
 3.56933073e-04 3.17169470e-05 3.72170538e-01 3.51573154e-03
 2.08093710e-02 1.03872465e-02 3.90411347e-01], sum to 1.0000
[2019-04-09 15:23:50,088] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2391
[2019-04-09 15:23:50,104] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 26.10509158080229, 0.5569387416209267, 1.0, 1.0, 65.0, 50.36915524957378], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 22.5, 26.17917543702691, 0.5818665896951051, 1.0, 1.0, 45.0, 47.26239968900525], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.375, 0.6815979530855758, 0.6939555298983683, 1.0, 1.0, 0.6, 0.4726239968900525], 
reward next is 0.5274, 
noisyNet noise sample is [array([-0.17218646], dtype=float32), -0.0721249]. 
=============================================
[2019-04-09 15:23:50,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3109975e-05 6.8922237e-02 8.2041539e-02 3.2601815e-03 2.0127205e-04
 5.0832699e-05 6.5027303e-01 4.8325523e-03 7.9597887e-03 8.8225808e-03
 1.7362283e-01], sum to 1.0000
[2019-04-09 15:23:50,217] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4512
[2019-04-09 15:23:50,250] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 19.0, 26.45106258942068, 0.5953823846610756, 0.0, 1.0, 45.0, 44.41960304657314], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 19.0, 26.35441916295242, 0.5834658828690041, 0.0, 1.0, 45.0, 38.9511870186702], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.6962015969127018, 0.6944886276230013, 0.0, 1.0, 0.6, 0.38951187018670197], 
reward next is 0.6105, 
noisyNet noise sample is [array([-0.18881294], dtype=float32), 0.42679623]. 
=============================================
[2019-04-09 15:23:50,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2144962e-06 9.9496834e-02 7.7537410e-02 3.6110533e-03 1.4777781e-04
 7.1172740e-06 5.4830587e-01 5.7544163e-03 5.4831654e-03 1.1602521e-02
 2.4805062e-01], sum to 1.0000
[2019-04-09 15:23:50,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-09 15:23:50,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 19.0, 26.76691002466553, 0.7391342450774224, 0.0, 1.0, 45.0, 37.19269638849935], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3364200.0000, 
sim time next is 3364800.0000, 
raw observation next is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 26.70885975966296, 0.7295660966921185, 0.0, 1.0, 20.0, 40.07734926243459], 
processed observation next is [1.0, 0.9565217391304348, 0.33333333333333337, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7257383133052467, 0.7431886988973728, 0.0, 1.0, 0.1, 0.40077349262434586], 
reward next is 0.5992, 
noisyNet noise sample is [array([-2.0605226], dtype=float32), -0.6572292]. 
=============================================
[2019-04-09 15:23:50,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6351985e-06 1.9115113e-02 1.4755492e-01 2.1627801e-03 9.1548660e-05
 9.9802655e-06 5.9947544e-01 1.6744881e-03 9.1591282e-03 2.6344627e-03
 2.1812052e-01], sum to 1.0000
[2019-04-09 15:23:50,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0197
[2019-04-09 15:23:50,376] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.26778921442227, 0.8534963886690948, 0.0, 1.0, 45.0, 32.32717021578285], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.18765722772081, 0.8473011443715971, 1.0, 1.0, 25.0, 31.86702910142381], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7656381023100675, 0.7824337147905324, 1.0, 1.0, 0.2, 0.3186702910142381], 
reward next is 0.6813, 
noisyNet noise sample is [array([1.3415958], dtype=float32), -0.83572096]. 
=============================================
[2019-04-09 15:23:50,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[18.896019]
 [19.13378 ]
 [19.058676]
 [19.385897]
 [19.135002]], R is [[19.25943756]
 [19.74357224]
 [20.24959373]
 [20.82055664]
 [21.29329491]].
[2019-04-09 15:23:50,534] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.46032493e-06 2.60813087e-02 1.80209652e-01 3.56017496e-03
 1.12905036e-04 8.38928190e-06 2.33260974e-01 4.70623933e-03
 8.93541705e-03 7.30586518e-03 5.35815597e-01], sum to 1.0000
[2019-04-09 15:23:50,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2693
[2019-04-09 15:23:50,553] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 50.0, 106.0, 752.0, 22.5, 27.90208622248892, 0.9785441408377329, 1.0, 1.0, 65.0, 22.13624490895038], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3335400.0000, 
sim time next is 3336000.0000, 
raw observation next is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 22.5, 27.94632767031852, 0.9812652568900292, 1.0, 1.0, 45.0, 19.54495829114083], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.5, 0.3427777777777777, 0.8165745856353591, 0.375, 0.8288606391932101, 0.8270884189633431, 1.0, 1.0, 0.6, 0.1954495829114083], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.8371579], dtype=float32), -0.90883994]. 
=============================================
[2019-04-09 15:23:50,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[19.06604 ]
 [19.07643 ]
 [19.013504]
 [18.83336 ]
 [18.90556 ]], R is [[19.41366386]
 [19.99816513]
 [20.57408714]
 [21.10683632]
 [21.70902824]].
[2019-04-09 15:23:50,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7349721e-07 7.7015117e-02 1.2389196e-01 4.5631733e-03 1.7128789e-04
 6.7869314e-06 4.9926460e-01 1.3457556e-03 6.3148355e-03 8.0183884e-03
 2.7940711e-01], sum to 1.0000
[2019-04-09 15:23:50,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1682
[2019-04-09 15:23:50,763] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 22.5, 27.85562237004982, 1.024082169098508, 1.0, 1.0, 25.0, 30.35836883668584], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3343800.0000, 
sim time next is 3344400.0000, 
raw observation next is [-2.0, 50.0, 35.5, 317.0, 22.5, 27.45005026680242, 0.9654469571443055, 1.0, 1.0, 25.0, 10.45182287128355], 
processed observation next is [1.0, 0.7391304347826086, 0.40720221606648205, 0.5, 0.11833333333333333, 0.35027624309392263, 0.375, 0.7875041889002018, 0.8218156523814352, 1.0, 1.0, 0.2, 0.1045182287128355], 
reward next is 0.8955, 
noisyNet noise sample is [array([-0.8527253], dtype=float32), -0.28198725]. 
=============================================
[2019-04-09 15:23:50,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.3759780e-07 4.1329227e-02 1.6208987e-01 1.4554834e-03 5.2649273e-05
 2.4809337e-06 4.8699406e-01 3.1853665e-04 3.3819643e-03 6.1052623e-03
 2.9826954e-01], sum to 1.0000
[2019-04-09 15:23:50,898] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1654
[2019-04-09 15:23:50,925] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 67.0, 61.0, 513.0, 22.5, 28.1439597218022, 1.123456864608607, 1.0, 1.0, 45.0, 10.40374694778256], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 22.5, 28.49419048584794, 1.131887665533255, 1.0, 1.0, 25.0, 18.87499780017234], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.17611111111111113, 0.4946593001841621, 0.375, 0.8745158738206618, 0.877295888511085, 1.0, 1.0, 0.2, 0.1887499780017234], 
reward next is 0.8113, 
noisyNet noise sample is [array([0.04686105], dtype=float32), -0.30128056]. 
=============================================
[2019-04-09 15:23:51,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2825006e-06 6.4599477e-02 2.0518681e-01 4.1884896e-03 8.5271386e-05
 1.0412629e-05 3.8606754e-01 1.0715685e-03 8.9816609e-03 3.6686487e-03
 3.2613787e-01], sum to 1.0000
[2019-04-09 15:23:51,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-09 15:23:51,165] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 50.0, 110.0, 776.0, 22.5, 27.78516187646713, 0.9415852674135329, 1.0, 1.0, 65.0, 26.09814039923675], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3333600.0000, 
sim time next is 3334200.0000, 
raw observation next is [-3.833333333333333, 50.0, 108.6666666666667, 768.0, 22.5, 27.79398763359659, 0.952847727925508, 1.0, 1.0, 65.0, 24.123243644321], 
processed observation next is [1.0, 0.6086956521739131, 0.3564173591874424, 0.5, 0.36222222222222233, 0.8486187845303867, 0.375, 0.8161656361330492, 0.8176159093085027, 1.0, 1.0, 1.0, 0.24123243644321002], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.90923727], dtype=float32), -0.8744051]. 
=============================================
[2019-04-09 15:23:51,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9368683e-06 2.4590630e-02 3.1809503e-01 1.7978848e-03 5.2795418e-05
 4.8461916e-06 2.2465934e-01 1.3509223e-03 6.4398698e-03 6.9399467e-03
 4.1606477e-01], sum to 1.0000
[2019-04-09 15:23:51,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0768
[2019-04-09 15:23:51,234] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.38283289179806, 0.8789505080378609, 0.0, 1.0, 45.0, 29.6951299372427], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3352800.0000, 
sim time next is 3353400.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.27247754943874, 0.8674641756408191, 0.0, 1.0, 25.0, 33.0111908407248], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7727064624532284, 0.7891547252136064, 0.0, 1.0, 0.2, 0.330111908407248], 
reward next is 0.6699, 
noisyNet noise sample is [array([1.9040295], dtype=float32), 0.056654993]. 
=============================================
[2019-04-09 15:23:51,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7827125e-06 4.8509374e-02 3.5590272e-02 4.0067970e-03 2.0324106e-04
 3.9439699e-05 3.6953607e-01 1.6302745e-03 4.8976704e-03 1.3051540e-02
 5.2253056e-01], sum to 1.0000
[2019-04-09 15:23:51,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0036
[2019-04-09 15:23:51,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2757544e-06 3.2026395e-02 9.3014643e-02 2.7425492e-03 9.2441674e-05
 1.1376318e-05 5.2411729e-01 1.1962688e-03 5.6854044e-03 2.4351086e-03
 3.3867532e-01], sum to 1.0000
[2019-04-09 15:23:51,733] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 25.87241394786123, 0.5494416063771376, 0.0, 1.0, 45.0, 28.71762283589156], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3367200.0000, 
sim time next is 3367800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 19.0, 25.73383195023896, 0.5252017331900893, 0.0, 1.0, 20.0, 24.88774615942531], 
processed observation next is [1.0, 1.0, 0.3102493074792244, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6444859958532468, 0.6750672443966964, 0.0, 1.0, 0.1, 0.24887746159425309], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.47052595], dtype=float32), 0.05705359]. 
=============================================
[2019-04-09 15:23:51,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6654
[2019-04-09 15:23:51,751] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 26.18315103062081, 0.6154187338762868, 0.0, 1.0, 45.0, 29.29816488122104], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3367200.0000, 
sim time next is 3367800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 19.0, 26.07193164581386, 0.6106393977921364, 0.0, 1.0, 65.0, 57.23597041778732], 
processed observation next is [1.0, 1.0, 0.3102493074792244, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6726609704844883, 0.7035464659307121, 0.0, 1.0, 1.0, 0.5723597041778732], 
reward next is 0.4276, 
noisyNet noise sample is [array([0.1712222], dtype=float32), -0.19805947]. 
=============================================
[2019-04-09 15:23:51,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2516602e-05 5.3348813e-02 1.2934871e-01 2.2811629e-03 3.4088301e-04
 7.8392204e-06 6.2275153e-01 3.0632752e-03 5.7508713e-03 5.1480685e-03
 1.7794636e-01], sum to 1.0000
[2019-04-09 15:23:51,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1013
[2019-04-09 15:23:51,839] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.44571264874058, 0.6667365271710896, 0.0, 1.0, 45.0, 33.51089748248568], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.39092114455959, 0.6448267342236316, 0.0, 1.0, 25.0, 31.7743999094604], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6992434287132993, 0.7149422447412105, 0.0, 1.0, 0.2, 0.317743999094604], 
reward next is 0.6823, 
noisyNet noise sample is [array([0.884427], dtype=float32), 0.32816204]. 
=============================================
[2019-04-09 15:23:52,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6186571e-05 9.0660989e-02 5.5023242e-02 7.8449287e-03 6.7472190e-04
 8.5183427e-05 3.1884652e-01 8.6761545e-03 1.5648669e-02 4.9938690e-02
 4.5255467e-01], sum to 1.0000
[2019-04-09 15:23:52,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6619
[2019-04-09 15:23:52,437] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.41512727803315, 0.6279016729049355, 0.0, 1.0, 45.0, 40.24436545506681], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3380400.0000, 
sim time next is 3381000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.41910416781156, 0.6262635011539478, 0.0, 1.0, 65.0, 49.25697517101251], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7015920139842967, 0.7087545003846493, 0.0, 1.0, 1.0, 0.4925697517101251], 
reward next is 0.5074, 
noisyNet noise sample is [array([-0.2297036], dtype=float32), -1.037822]. 
=============================================
[2019-04-09 15:23:52,446] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[14.491916]
 [14.554709]
 [14.618246]
 [14.974459]
 [14.761851]], R is [[14.52372837]
 [14.97604752]
 [15.41353607]
 [15.75089264]
 [16.1015892 ]].
[2019-04-09 15:23:52,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2917302e-05 3.3723399e-02 9.5911980e-02 7.2005047e-03 3.4373609e-04
 8.7096872e-05 4.5775047e-01 5.3794403e-03 9.2764534e-03 7.1792044e-03
 3.8311481e-01], sum to 1.0000
[2019-04-09 15:23:52,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0332
[2019-04-09 15:23:52,519] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 62.5, 0.0, 0.0, 19.0, 25.49042135590937, 0.4025773530112209, 0.0, 1.0, 25.0, 40.41691324318359], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3393000.0000, 
sim time next is 3393600.0000, 
raw observation next is [-3.0, 63.33333333333333, 0.0, 0.0, 19.0, 25.43410642148324, 0.3863219170035175, 0.0, 1.0, 20.0, 29.11358189727798], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.6195088684569366, 0.6287739723345058, 0.0, 1.0, 0.1, 0.2911358189727798], 
reward next is 0.7089, 
noisyNet noise sample is [array([1.7901719], dtype=float32), -0.37199068]. 
=============================================
[2019-04-09 15:23:52,535] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1705970e-06 2.4511252e-02 2.0735741e-01 3.1890881e-03 4.2994219e-05
 1.1408484e-06 3.4103036e-01 1.2815034e-03 1.2225602e-02 6.0831131e-03
 4.0427643e-01], sum to 1.0000
[2019-04-09 15:23:52,535] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0223
[2019-04-09 15:23:52,551] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 19.0, 27.33802002664583, 0.9608390117292852, 1.0, 1.0, 65.0, 36.31223078738321], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3442200.0000, 
sim time next is 3442800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 27.36276570685009, 0.9586304941138416, 0.0, 1.0, 20.0, 32.47367118421423], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7802304755708409, 0.8195434980379472, 0.0, 1.0, 0.1, 0.3247367118421423], 
reward next is 0.6753, 
noisyNet noise sample is [array([-1.8569206], dtype=float32), 0.6112168]. 
=============================================
[2019-04-09 15:23:52,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6213125e-06 4.1359097e-02 1.3010192e-01 3.3842586e-03 4.1238120e-04
 1.5009036e-05 6.4168864e-01 2.3364681e-03 3.4064422e-03 4.7972882e-03
 1.7249490e-01], sum to 1.0000
[2019-04-09 15:23:52,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-09 15:23:52,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.88711678752185, 0.7993264919392481, 0.0, 1.0, 45.0, 33.66229100390931], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.86791807149371, 0.7951817045355939, 0.0, 1.0, 45.0, 34.78457115158621], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7389931726244757, 0.7650605681785313, 0.0, 1.0, 0.6, 0.34784571151586213], 
reward next is 0.6522, 
noisyNet noise sample is [array([0.97276783], dtype=float32), -0.9800952]. 
=============================================
[2019-04-09 15:23:53,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1265301e-06 4.5113228e-02 9.6272781e-02 3.6688242e-03 3.3077929e-04
 6.0708979e-05 4.8003700e-01 3.8339908e-03 8.3853612e-03 5.4521961e-03
 3.5683599e-01], sum to 1.0000
[2019-04-09 15:23:53,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-09 15:23:53,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.29337695e-05 2.44919602e-02 2.03491628e-01 2.77779554e-03
 3.01141030e-04 3.57305289e-05 4.18712646e-01 2.69684545e-03
 5.97417029e-03 7.27621093e-03 3.34228992e-01], sum to 1.0000
[2019-04-09 15:23:53,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7794
[2019-04-09 15:23:53,257] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 62.5, 2.0, 107.0, 22.5, 26.19054101464909, 0.5667354260883579, 1.0, 1.0, 65.0, 62.72191584416464], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3396600.0000, 
sim time next is 3397200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 26.22945444059881, 0.5935566771373134, 1.0, 1.0, 65.0, 53.30344607066274], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.6166666666666667, 0.05388888888888887, 0.17624309392265194, 0.375, 0.6857878700499009, 0.6978522257124378, 1.0, 1.0, 1.0, 0.5330344607066274], 
reward next is 0.4670, 
noisyNet noise sample is [array([-2.56673], dtype=float32), -1.1201123]. 
=============================================
[2019-04-09 15:23:53,279] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.42877174073516, 0.6605748357500031, 0.0, 1.0, 45.0, 39.28843756845301], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3372600.0000, 
sim time next is 3373200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.48432107812823, 0.6618688102885476, 0.0, 1.0, 45.0, 36.42344288973545], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7070267565106857, 0.7206229367628492, 0.0, 1.0, 0.6, 0.36423442889735447], 
reward next is 0.6358, 
noisyNet noise sample is [array([-0.37500498], dtype=float32), 0.25480217]. 
=============================================
[2019-04-09 15:23:53,440] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6451109e-06 6.0817517e-02 2.4396357e-01 6.2052738e-03 2.1781749e-04
 1.0743502e-05 4.1225275e-01 5.5907895e-03 1.1801319e-02 5.5121807e-03
 2.5361934e-01], sum to 1.0000
[2019-04-09 15:23:53,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8012
[2019-04-09 15:23:53,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 54.0, 99.0, 658.0, 22.5, 27.35380762344747, 0.7911009905120218, 1.0, 1.0, 45.0, 26.48588796558481], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3403800.0000, 
sim time next is 3404400.0000, 
raw observation next is [0.9999999999999999, 52.0, 100.6666666666667, 675.6666666666666, 22.5, 27.46204006210879, 0.8109020966657389, 1.0, 1.0, 65.0, 26.89425094075451], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.52, 0.33555555555555566, 0.7465930018416206, 0.375, 0.788503338509066, 0.7703006988885797, 1.0, 1.0, 1.0, 0.2689425094075451], 
reward next is 0.7311, 
noisyNet noise sample is [array([-1.006838], dtype=float32), -0.2996932]. 
=============================================
[2019-04-09 15:23:53,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3967135e-05 1.1622158e-01 8.0620274e-02 3.7045819e-03 3.0738991e-04
 2.1618353e-05 5.0168020e-01 5.1254476e-03 1.4690950e-02 8.0461232e-03
 2.6954776e-01], sum to 1.0000
[2019-04-09 15:23:53,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-09 15:23:53,756] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.86392952864081, 0.748198013646343, 0.0, 1.0, 45.0, 35.91955803481032], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3477600.0000, 
sim time next is 3478200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.81573637416433, 0.7416351690713622, 0.0, 1.0, 45.0, 36.93295749310065], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7346446978470276, 0.7472117230237875, 0.0, 1.0, 0.6, 0.36932957493100654], 
reward next is 0.6307, 
noisyNet noise sample is [array([-0.95480525], dtype=float32), 1.3103305]. 
=============================================
[2019-04-09 15:23:53,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4440475e-06 2.7478863e-02 3.0352277e-01 3.7447973e-03 1.8307129e-04
 1.5817577e-05 2.9805148e-01 6.9132037e-03 5.8204019e-03 8.0218296e-03
 3.4624237e-01], sum to 1.0000
[2019-04-09 15:23:53,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8443
[2019-04-09 15:23:53,815] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 60.0, 93.0, 540.0, 22.5, 27.18669979009972, 0.7531821347281156, 1.0, 1.0, 45.0, 23.08291615291563], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3402000.0000, 
sim time next is 3402600.0000, 
raw observation next is [-0.5, 58.0, 95.0, 579.3333333333334, 22.5, 27.24984948856232, 0.7818430913883566, 1.0, 1.0, 65.0, 31.70687747895359], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.58, 0.31666666666666665, 0.6401473296500921, 0.375, 0.7708207907135266, 0.7606143637961189, 1.0, 1.0, 1.0, 0.3170687747895359], 
reward next is 0.6829, 
noisyNet noise sample is [array([1.293746], dtype=float32), 0.5304691]. 
=============================================
[2019-04-09 15:23:53,849] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5288759e-06 3.6256935e-02 1.1566687e-01 3.3823191e-03 1.5441945e-04
 3.4833378e-05 3.5646459e-01 6.2148282e-03 5.6516002e-03 7.1626366e-03
 4.6900445e-01], sum to 1.0000
[2019-04-09 15:23:53,854] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-09 15:23:53,903] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 73.16666666666666, 0.0, 0.0, 19.0, 26.85225739874613, 0.782581760675126, 0.0, 1.0, 20.0, 32.64064519203811], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3466200.0000, 
sim time next is 3466800.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 26.81552118198283, 0.7810227439948564, 0.0, 1.0, 45.0, 33.32737209710153], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.734626765165236, 0.7603409146649521, 0.0, 1.0, 0.6, 0.3332737209710153], 
reward next is 0.6667, 
noisyNet noise sample is [array([0.78459185], dtype=float32), -0.3854462]. 
=============================================
[2019-04-09 15:23:54,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5334025e-06 4.8039977e-02 3.9298814e-02 5.1962170e-03 9.9720273e-05
 1.0117561e-05 5.7399046e-01 1.6953788e-03 8.0380915e-03 6.3152257e-03
 3.1731352e-01], sum to 1.0000
[2019-04-09 15:23:54,095] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-09 15:23:54,114] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 22.5, 27.60456071160986, 0.8446403978865549, 1.0, 1.0, 65.0, 26.58528534477874], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3405000.0000, 
sim time next is 3405600.0000, 
raw observation next is [2.0, 48.0, 104.0, 711.0, 22.5, 27.62281882024015, 0.862049474548754, 1.0, 1.0, 45.0, 24.23064486954798], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.48, 0.3466666666666667, 0.7856353591160221, 0.375, 0.8019015683533457, 0.7873498248495846, 1.0, 1.0, 0.6, 0.2423064486954798], 
reward next is 0.7577, 
noisyNet noise sample is [array([-1.2020831], dtype=float32), -0.12796764]. 
=============================================
[2019-04-09 15:23:54,229] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4237205e-07 3.6084093e-02 6.9872841e-02 2.2723873e-03 6.2643434e-05
 6.9124085e-06 4.6834794e-01 2.0204131e-03 7.5678546e-03 4.5201792e-03
 4.0924412e-01], sum to 1.0000
[2019-04-09 15:23:54,229] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1332
[2019-04-09 15:23:54,256] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.66666666666666, 116.3333333333333, 812.6666666666667, 22.5, 28.01103679144937, 0.9807141895662735, 1.0, 1.0, 45.0, 30.78396833406133], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3413400.0000, 
sim time next is 3414000.0000, 
raw observation next is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 22.5, 28.01546848222957, 0.9866569107200007, 1.0, 1.0, 45.0, 27.45954791454884], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.46333333333333343, 0.388888888888889, 0.9003683241252303, 0.375, 0.8346223735191307, 0.828885636906667, 1.0, 1.0, 0.6, 0.2745954791454884], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.5551152], dtype=float32), 0.46512368]. 
=============================================
[2019-04-09 15:23:54,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[19.329437]
 [19.193804]
 [19.136187]
 [19.26747 ]
 [19.059776]], R is [[20.02064896]
 [20.51260185]
 [21.03948021]
 [21.50307274]
 [22.07036209]].
[2019-04-09 15:23:54,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9062819e-06 1.7656911e-02 1.0932164e-01 2.2667949e-03 5.4599517e-05
 1.0965079e-05 3.5492757e-01 1.3585849e-03 1.0730496e-02 4.0841359e-03
 4.9958342e-01], sum to 1.0000
[2019-04-09 15:23:54,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8644
[2019-04-09 15:23:54,448] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 27.06339882814383, 0.8418583093490496, 0.0, 1.0, 25.0, 39.89330214893688], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3457200.0000, 
sim time next is 3457800.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 19.0, 27.09067637532775, 0.8358610243237727, 0.0, 1.0, 65.0, 39.45013205444891], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.825, 0.0, 0.0, 0.08333333333333333, 0.7575563646106458, 0.7786203414412576, 0.0, 1.0, 1.0, 0.39450132054448905], 
reward next is 0.6055, 
noisyNet noise sample is [array([-0.7667537], dtype=float32), -0.886157]. 
=============================================
[2019-04-09 15:23:55,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7060605e-07 6.8028420e-02 1.7884976e-01 2.8624828e-03 5.0332801e-05
 6.1353517e-06 3.0358443e-01 1.8891639e-03 8.2283346e-03 2.0168028e-03
 4.3448389e-01], sum to 1.0000
[2019-04-09 15:23:55,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6998
[2019-04-09 15:23:55,226] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 22.5, 28.47785100437341, 1.119839243842673, 1.0, 1.0, 65.0, 22.84562242725109], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3424800.0000, 
sim time next is 3425400.0000, 
raw observation next is [2.5, 62.5, 84.0, 704.0, 22.5, 28.52497808151432, 1.130428958536005, 1.0, 1.0, 65.0, 21.15519023499689], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.625, 0.28, 0.7779005524861878, 0.375, 0.87708150679286, 0.876809652845335, 1.0, 1.0, 1.0, 0.2115519023499689], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.36967564], dtype=float32), 0.20184082]. 
=============================================
[2019-04-09 15:23:55,343] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5576416e-05 6.7224182e-02 1.0439790e-01 4.7898274e-03 2.5488049e-04
 8.3155181e-05 4.8453140e-01 5.9078606e-03 7.9178102e-03 1.4812251e-02
 3.1006518e-01], sum to 1.0000
[2019-04-09 15:23:55,344] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6686
[2019-04-09 15:23:55,366] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 26.44219024484036, 0.6025664430511267, 0.0, 1.0, 65.0, 51.21252995962702], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [-3.0, 60.83333333333333, 0.0, 0.0, 19.0, 26.39310997354011, 0.5987200732587068, 0.0, 1.0, 55.0, 46.02814068885144], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6083333333333333, 0.0, 0.0, 0.08333333333333333, 0.6994258311283424, 0.6995733577529023, 0.0, 1.0, 0.8, 0.4602814068885144], 
reward next is 0.5397, 
noisyNet noise sample is [array([-1.1197875], dtype=float32), 0.8684836]. 
=============================================
[2019-04-09 15:23:55,464] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5013461e-07 2.0700093e-02 4.7871660e-02 3.4948734e-03 7.4749303e-05
 2.1438132e-06 6.2467778e-01 6.4707256e-04 8.1332279e-03 1.3658367e-02
 2.8073931e-01], sum to 1.0000
[2019-04-09 15:23:55,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0719
[2019-04-09 15:23:55,496] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 67.0, 44.66666666666667, 382.3333333333334, 22.5, 28.11679459454055, 1.08144596786264, 1.0, 1.0, 45.0, 16.64553603918969], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3430200.0000, 
sim time next is 3430800.0000, 
raw observation next is [2.0, 67.0, 36.5, 317.0, 22.5, 28.32626952820499, 1.107248712400089, 1.0, 1.0, 30.0, 23.35948768795194], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.12166666666666667, 0.35027624309392263, 0.375, 0.860522460683749, 0.869082904133363, 1.0, 1.0, 0.3, 0.2335948768795194], 
reward next is 0.7664, 
noisyNet noise sample is [array([-0.11445095], dtype=float32), -0.0634514]. 
=============================================
[2019-04-09 15:23:55,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9745636e-06 3.1394087e-02 2.5582698e-01 3.2316574e-03 2.2684410e-04
 1.0194398e-05 3.8253525e-01 1.4992449e-03 1.3641196e-02 2.9323853e-03
 3.0869913e-01], sum to 1.0000
[2019-04-09 15:23:55,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2053
[2019-04-09 15:23:55,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4745606e-07 5.8230720e-02 6.7535542e-02 2.3130141e-03 5.3231372e-05
 3.7206396e-06 7.3953658e-01 5.1911781e-04 6.3738171e-03 9.1012623e-03
 1.1633267e-01], sum to 1.0000
[2019-04-09 15:23:55,524] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7092
[2019-04-09 15:23:55,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 27.32815623104988, 0.9307911212631255, 0.0, 1.0, 25.0, 36.01807653971441], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 27.32473397027593, 0.9287480197582632, 0.0, 1.0, 25.0, 27.89259295995082], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7770611641896608, 0.8095826732527543, 0.0, 1.0, 0.2, 0.27892592959950824], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.66100377], dtype=float32), 0.28093225]. 
=============================================
[2019-04-09 15:23:55,535] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.5, 27.71343715159315, 0.9879650369625307, 1.0, 1.0, 45.0, 30.10157860164831], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3438000.0000, 
sim time next is 3438600.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 22.5, 27.63943140965561, 0.9778778249793852, 1.0, 1.0, 45.0, 29.63873049769787], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.375, 0.8032859508046343, 0.8259592749931284, 1.0, 1.0, 0.6, 0.29638730497697874], 
reward next is 0.7036, 
noisyNet noise sample is [array([0.8232404], dtype=float32), 1.6286387]. 
=============================================
[2019-04-09 15:23:55,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[18.701828]
 [19.026443]
 [19.432262]
 [19.521324]
 [19.667683]], R is [[19.3573494 ]
 [19.8035965 ]
 [20.11470032]
 [20.60354042]
 [21.08729172]].
[2019-04-09 15:23:55,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1348630e-06 4.8284229e-02 1.5737270e-01 5.3137247e-03 1.9633680e-04
 3.9089919e-05 3.4083816e-01 5.1009008e-03 1.4997058e-02 9.1688447e-03
 4.1868472e-01], sum to 1.0000
[2019-04-09 15:23:55,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6507
[2019-04-09 15:23:55,787] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 71.5, 3.0, 107.0, 22.5, 26.49299302754181, 0.7037128136809169, 1.0, 1.0, 65.0, 82.37698339667405], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 22.5, 26.56271100314573, 0.7131393284683868, 0.0, 1.0, 65.0, 61.89632184110494], 
processed observation next is [1.0, 0.30434782608695654, 0.44413665743305636, 0.7133333333333333, 0.0572222222222222, 0.17200736648250467, 0.375, 0.7135592502621441, 0.7377131094894622, 0.0, 1.0, 1.0, 0.6189632184110494], 
reward next is 0.3810, 
noisyNet noise sample is [array([-0.7486263], dtype=float32), 0.46080682]. 
=============================================
[2019-04-09 15:23:55,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1651971e-06 5.4295350e-02 1.5763657e-01 6.5123420e-03 1.8707856e-04
 2.9145804e-05 3.3141768e-01 4.5069833e-03 1.6698478e-02 8.7079760e-03
 4.2000514e-01], sum to 1.0000
[2019-04-09 15:23:55,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7467
[2019-04-09 15:23:55,869] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 71.0, 45.5, 253.0, 22.5, 26.71419397069352, 0.7433596614954515, 1.0, 1.0, 25.0, 43.35982735549987], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3484800.0000, 
sim time next is 3485400.0000, 
raw observation next is [-1.0, 71.0, 59.66666666666667, 301.6666666666667, 22.5, 26.7765315350821, 0.7615570516131392, 1.0, 1.0, 65.0, 53.76677657682528], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.1988888888888889, 0.33333333333333337, 0.375, 0.7313776279235084, 0.7538523505377132, 1.0, 1.0, 1.0, 0.5376677657682528], 
reward next is 0.4623, 
noisyNet noise sample is [array([-0.7486263], dtype=float32), 0.46080682]. 
=============================================
[2019-04-09 15:23:55,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9257835e-06 5.5510081e-02 1.7302193e-01 6.9440193e-03 1.5428623e-04
 3.2072294e-05 3.3324891e-01 3.9735879e-03 1.5630757e-02 7.8004533e-03
 4.0368098e-01], sum to 1.0000
[2019-04-09 15:23:55,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6262
[2019-04-09 15:23:55,898] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 71.0, 59.66666666666667, 301.6666666666667, 22.5, 26.7765315350821, 0.7615570516131392, 1.0, 1.0, 65.0, 53.76677657682528], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3485400.0000, 
sim time next is 3486000.0000, 
raw observation next is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 22.5, 26.83380401984403, 0.7902565608166654, 1.0, 1.0, 65.0, 46.5525941003125], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.24611111111111114, 0.3871086556169429, 0.375, 0.7361503349870026, 0.7634188536055552, 1.0, 1.0, 1.0, 0.46552594100312494], 
reward next is 0.5345, 
noisyNet noise sample is [array([-0.7486263], dtype=float32), 0.46080682]. 
=============================================
[2019-04-09 15:23:55,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[16.199177]
 [15.793392]
 [16.018244]
 [15.699037]
 [15.51341 ]], R is [[16.47673225]
 [16.77429771]
 [17.17295647]
 [17.43623924]
 [17.64291382]].
[2019-04-09 15:23:56,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9991736e-06 1.8991163e-01 2.0957367e-01 8.0623617e-03 1.3914997e-04
 2.6097052e-05 1.4691149e-01 2.1755178e-03 9.8550087e-03 1.5962660e-02
 4.1737750e-01], sum to 1.0000
[2019-04-09 15:23:56,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2889
[2019-04-09 15:23:56,249] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.0, 60.0, 104.0, 720.0, 22.5, 27.97270384509793, 0.9886391574656196, 1.0, 1.0, 45.0, 23.93178785334815], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3492000.0000, 
sim time next is 3492600.0000, 
raw observation next is [0.1666666666666667, 60.16666666666666, 105.6666666666667, 736.6666666666666, 22.5, 28.03282042018763, 1.003711733583687, 1.0, 1.0, 25.0, 23.46943993232528], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.6016666666666666, 0.3522222222222223, 0.8139963167587476, 0.375, 0.8360683683489691, 0.834570577861229, 1.0, 1.0, 0.2, 0.23469439932325278], 
reward next is 0.7653, 
noisyNet noise sample is [array([3.2892895], dtype=float32), 0.41317424]. 
=============================================
[2019-04-09 15:23:56,343] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6718244e-07 1.0111585e-01 1.4642268e-01 3.8345370e-03 4.4241338e-04
 2.7708156e-06 3.6880568e-01 1.5725701e-03 1.7972576e-02 8.1380000e-03
 3.5169238e-01], sum to 1.0000
[2019-04-09 15:23:56,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-09 15:23:56,391] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 22.5, 28.58874673331698, 0.9021835412396234, 1.0, 1.0, 45.0, 55.34939335635404], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 22.5, 28.41732458814219, 1.163169199895614, 1.0, 1.0, 65.0, 19.08698662263441], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.2161111111111111, 0.602025782688766, 0.375, 0.8681103823451825, 0.8877230666318713, 1.0, 1.0, 1.0, 0.19086986622634408], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.0296977], dtype=float32), 1.276621]. 
=============================================
[2019-04-09 15:23:56,604] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.82189022e-07 7.06804693e-02 1.49582461e-01 5.10434806e-03
 1.10845045e-04 6.10899588e-06 3.58070284e-01 2.25852872e-03
 1.32050570e-02 6.12284616e-03 3.94858003e-01], sum to 1.0000
[2019-04-09 15:23:56,608] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1778
[2019-04-09 15:23:56,631] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 22.5, 27.76094874314253, 1.087436079901993, 1.0, 1.0, 45.0, 12.92148891318128], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 22.5, 28.10260277342583, 1.123864956246297, 1.0, 1.0, 25.0, 15.92023718555924], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.375, 0.8418835644521524, 0.874621652082099, 1.0, 1.0, 0.2, 0.15920237185559238], 
reward next is 0.8408, 
noisyNet noise sample is [array([1.1957016], dtype=float32), -0.18879475]. 
=============================================
[2019-04-09 15:23:56,734] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6336347e-06 5.8591485e-02 1.5797095e-01 2.2546512e-03 7.8775432e-05
 3.9176916e-06 5.1649046e-01 2.4727173e-03 3.7248412e-03 7.1950774e-03
 2.5121444e-01], sum to 1.0000
[2019-04-09 15:23:56,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5060
[2019-04-09 15:23:56,747] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.07719078537428, 0.8584628851660452, 0.0, 1.0, 65.0, 39.93443002853297], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3453600.0000, 
sim time next is 3454200.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.10554519561983, 0.8500162871111945, 0.0, 1.0, 45.0, 36.08582455823315], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7587954329683191, 0.7833387623703981, 0.0, 1.0, 0.6, 0.3608582455823315], 
reward next is 0.6391, 
noisyNet noise sample is [array([0.8477321], dtype=float32), 0.58901376]. 
=============================================
[2019-04-09 15:23:57,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3000261e-06 2.4431456e-02 9.2236280e-02 3.7221115e-03 7.4038202e-05
 2.2665121e-05 4.0756661e-01 4.0866379e-03 6.0115363e-03 2.9223880e-03
 4.5891997e-01], sum to 1.0000
[2019-04-09 15:23:57,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0668
[2019-04-09 15:23:57,098] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 26.98827902434992, 0.8086736275729948, 0.0, 1.0, 25.0, 28.77745934630547], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 19.0, 26.93051325573757, 0.7869187673472301, 0.0, 1.0, 25.0, 27.44691948112396], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.08333333333333333, 0.7442094379781308, 0.7623062557824101, 0.0, 1.0, 0.2, 0.2744691948112396], 
reward next is 0.7255, 
noisyNet noise sample is [array([-0.48924452], dtype=float32), -0.74204904]. 
=============================================
[2019-04-09 15:23:57,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4458719e-06 4.9538046e-02 5.0604470e-02 3.2049080e-03 2.3687680e-04
 2.6048547e-05 5.3049874e-01 6.6635557e-03 9.7320126e-03 7.0116781e-03
 3.4247425e-01], sum to 1.0000
[2019-04-09 15:23:57,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6790
[2019-04-09 15:23:57,691] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.84842631259522, 0.7538979748988797, 0.0, 1.0, 65.0, 43.00467487363328], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3476400.0000, 
sim time next is 3477000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.81941924317145, 0.7588878600333754, 0.0, 1.0, 65.0, 46.48848127190094], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7349516035976208, 0.7529626200111251, 0.0, 1.0, 1.0, 0.4648848127190094], 
reward next is 0.5351, 
noisyNet noise sample is [array([-1.7606719], dtype=float32), -2.0086477]. 
=============================================
[2019-04-09 15:23:57,701] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[15.706745 ]
 [15.7779045]
 [15.43991  ]
 [15.443203 ]
 [15.631775 ]], R is [[16.05273056]
 [16.4621563 ]
 [16.80580902]
 [17.18785667]
 [17.57637405]].
[2019-04-09 15:23:57,890] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3454035e-04 1.4837813e-01 2.2548203e-01 3.1033393e-02 3.1752002e-03
 5.6598929e-04 2.5028414e-01 1.5641520e-02 1.9799879e-02 2.0228582e-02
 2.8517658e-01], sum to 1.0000
[2019-04-09 15:23:57,896] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1978
[2019-04-09 15:23:57,923] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.166666666666667, 66.0, 0.0, 0.0, 19.0, 26.86721972407284, 0.7767561620681033, 0.0, 1.0, 20.0, 36.96484647122227], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3553800.0000, 
sim time next is 3554400.0000, 
raw observation next is [-3.333333333333333, 67.0, 0.0, 0.0, 19.0, 26.83849221598548, 0.7683517420117277, 0.0, 1.0, 25.0, 38.14751187880869], 
processed observation next is [0.0, 0.13043478260869565, 0.37026777469990774, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7365410179987899, 0.7561172473372425, 0.0, 1.0, 0.2, 0.38147511878808693], 
reward next is 0.6185, 
noisyNet noise sample is [array([0.5788088], dtype=float32), 0.02511919]. 
=============================================
[2019-04-09 15:23:58,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2387155e-06 6.2990926e-02 9.9885963e-02 2.7970690e-03 2.6705133e-05
 3.6664533e-06 2.5021517e-01 1.4018562e-03 8.9654382e-03 8.1382245e-03
 5.6557369e-01], sum to 1.0000
[2019-04-09 15:23:58,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2718
[2019-04-09 15:23:58,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 22.5, 27.59049372963764, 1.046947815382454, 1.0, 1.0, 25.0, 10.67292169613715], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [2.0, 57.0, 115.5, 816.5, 22.5, 28.02043641573641, 1.077104521247056, 1.0, 1.0, 45.0, 18.36732321144616], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.57, 0.385, 0.9022099447513812, 0.375, 0.835036367978034, 0.8590348404156853, 1.0, 1.0, 0.6, 0.1836732321144616], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.52355653], dtype=float32), -0.25283924]. 
=============================================
[2019-04-09 15:23:58,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4274852e-05 7.1171984e-02 6.9726400e-02 2.8764869e-03 1.7844017e-04
 1.3893243e-05 6.4266407e-01 2.7539756e-03 8.7875649e-03 1.2986587e-02
 1.8882643e-01], sum to 1.0000
[2019-04-09 15:23:58,214] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2173
[2019-04-09 15:23:58,233] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.76697345677545, 0.7369422317884036, 0.0, 1.0, 45.0, 42.07835917700763], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.78147280696274, 0.7372728087252036, 0.0, 1.0, 45.0, 36.52695958596117], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7317894005802282, 0.7457576029084012, 0.0, 1.0, 0.6, 0.3652695958596117], 
reward next is 0.6347, 
noisyNet noise sample is [array([0.2922994], dtype=float32), 1.0081309]. 
=============================================
[2019-04-09 15:23:58,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[16.322857]
 [16.253212]
 [16.21033 ]
 [16.136925]
 [16.488686]], R is [[16.9775219 ]
 [17.38696289]
 [17.75595665]
 [18.18551064]
 [18.60738373]].
[2019-04-09 15:23:58,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2431376e-05 8.5628599e-02 1.9748816e-01 1.7917475e-02 5.3847249e-04
 1.2779303e-04 3.1066552e-01 5.6537529e-03 2.5771568e-02 2.0422224e-02
 3.3573401e-01], sum to 1.0000
[2019-04-09 15:23:58,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4193
[2019-04-09 15:23:58,383] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 19.0, 26.70859944458398, 0.7966963845660691, 0.0, 1.0, 45.0, 38.9817107715639], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3580800.0000, 
sim time next is 3581400.0000, 
raw observation next is [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 19.0, 26.74010092069164, 0.8038124276366089, 0.0, 1.0, 45.0, 37.83173964770729], 
processed observation next is [0.0, 0.43478260869565216, 0.3471837488457987, 0.5583333333333332, 0.37333333333333335, 0.8589318600368323, 0.08333333333333333, 0.7283417433909699, 0.7679374758788696, 0.0, 1.0, 0.6, 0.37831739647707285], 
reward next is 0.6217, 
noisyNet noise sample is [array([-0.09926882], dtype=float32), 0.33653998]. 
=============================================
[2019-04-09 15:23:58,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5420541e-05 7.7901706e-02 1.7680812e-01 7.7303480e-03 4.1391177e-04
 3.1678763e-05 2.7090856e-01 6.2611210e-03 2.6466805e-02 1.1867421e-02
 4.2158490e-01], sum to 1.0000
[2019-04-09 15:23:58,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2742
[2019-04-09 15:23:58,524] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 22.5, 26.66067396762848, 0.7356889793865985, 1.0, 1.0, 25.0, 49.48789178686796], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3484200.0000, 
sim time next is 3484800.0000, 
raw observation next is [-1.0, 71.0, 45.5, 253.0, 22.5, 26.70747679875917, 0.7344390704385212, 1.0, 1.0, 45.0, 40.55317042767835], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.15166666666666667, 0.2795580110497238, 0.375, 0.7256230665632643, 0.7448130234795071, 1.0, 1.0, 0.6, 0.40553170427678353], 
reward next is 0.5945, 
noisyNet noise sample is [array([-0.05052152], dtype=float32), 0.9492469]. 
=============================================
[2019-04-09 15:23:58,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5077844e-06 2.9569885e-02 1.7666064e-01 7.6189509e-04 4.2575084e-05
 7.9131069e-06 3.6520481e-01 1.2266490e-03 7.3838290e-03 3.2780438e-03
 4.1585827e-01], sum to 1.0000
[2019-04-09 15:23:58,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0507
[2019-04-09 15:23:58,795] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.03325766295779, 0.854081597193463, 0.0, 1.0, 65.0, 41.87664558153637], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 27.04855588943461, 0.8518539131862886, 0.0, 1.0, 25.0, 39.44546147524134], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.08333333333333333, 0.7540463241195509, 0.7839513043954295, 0.0, 1.0, 0.2, 0.3944546147524134], 
reward next is 0.6055, 
noisyNet noise sample is [array([-0.54354674], dtype=float32), -1.7090544]. 
=============================================
[2019-04-09 15:23:59,441] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8763492e-05 4.0071119e-02 2.5054252e-01 5.2505117e-03 3.0180049e-04
 4.3231972e-05 3.8061017e-01 8.9572221e-03 2.0679275e-02 3.1138958e-02
 2.6238644e-01], sum to 1.0000
[2019-04-09 15:23:59,443] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5485
[2019-04-09 15:23:59,464] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 67.83333333333333, 0.0, 0.0, 19.0, 27.10257864361509, 0.8020150620454998, 0.0, 1.0, 25.0, 41.5315751649219], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3469800.0000, 
sim time next is 3470400.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 19.0, 27.05579289289111, 0.7931727276108701, 0.0, 1.0, 45.0, 34.17346733790581], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7546494077409257, 0.7643909092036233, 0.0, 1.0, 0.6, 0.3417346733790581], 
reward next is 0.6583, 
noisyNet noise sample is [array([-0.04130578], dtype=float32), 1.6856424]. 
=============================================
[2019-04-09 15:23:59,794] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-09 15:23:59,795] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:23:59,795] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:59,797] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:23:59,798] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:23:59,798] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:59,799] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:59,803] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run24
[2019-04-09 15:23:59,825] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run24
[2019-04-09 15:23:59,841] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/78/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run24
[2019-04-09 15:25:11,624] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.03959911], dtype=float32), 0.044274203]
[2019-04-09 15:25:11,624] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [1.257477558666667, 75.85648307333334, 0.0, 0.0, 19.0, 26.89895213177231, 0.7837967902008817, 0.0, 1.0, 45.0, 36.83362035741548]
[2019-04-09 15:25:11,624] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:25:11,625] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [4.3826283e-05 6.3068591e-02 1.2228834e-01 9.1127353e-03 7.1101647e-04
 9.3703246e-05 4.3864048e-01 5.3469865e-03 1.3443329e-02 1.2577340e-02
 3.3467355e-01], sampled 0.6714387916263822
[2019-04-09 15:25:40,570] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 5644.0333 285204.7209 2925.0493
[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,591] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:40,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:45,989] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 5296.4810 319951.8990 2179.1525
[2019-04-09 15:25:45,998] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 5398.1567 309784.3325 2512.8108
[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,009] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,129] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:46,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:47,020] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 230000, evaluation results [230000.0, 5398.156674844571, 309784.33251554106, 2512.8108432262875, 5644.033305289011, 285204.7208757964, 2925.049252563288, 5296.481010141777, 319951.8989858216, 2179.152498968677]
[2019-04-09 15:25:47,117] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9366129e-05 5.1636133e-02 4.7108024e-02 6.4310483e-03 2.0362159e-04
 1.5777126e-05 6.9033289e-01 6.8382239e-03 6.7781108e-03 1.1311598e-02
 1.7931522e-01], sum to 1.0000
[2019-04-09 15:25:47,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4476
[2019-04-09 15:25:47,134] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.813514402284, 0.7514005166914878, 0.0, 1.0, 45.0, 39.85072917352618], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.79732540409378, 0.7484202828032922, 0.0, 1.0, 45.0, 37.03450850585654], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7331104503411483, 0.7494734276010974, 0.0, 1.0, 0.6, 0.37034508505856545], 
reward next is 0.6297, 
noisyNet noise sample is [array([-1.1542937], dtype=float32), 0.43827686]. 
=============================================
[2019-04-09 15:25:47,175] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0259354e-07 4.5054000e-02 1.1183470e-01 4.8082052e-03 1.8112503e-04
 7.3669439e-06 3.1516087e-01 3.3119002e-03 6.9200662e-03 6.8004923e-03
 5.0592089e-01], sum to 1.0000
[2019-04-09 15:25:47,176] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8030
[2019-04-09 15:25:47,195] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 28.33647993885405, 1.123576802312576, 1.0, 1.0, 45.0, 20.84461656659803], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3522600.0000, 
sim time next is 3523200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 28.19457426621546, 1.111093388751933, 1.0, 1.0, 65.0, 26.88068271631927], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8495478555179551, 0.8703644629173111, 1.0, 1.0, 1.0, 0.2688068271631927], 
reward next is 0.7312, 
noisyNet noise sample is [array([1.5196244], dtype=float32), 0.7430236]. 
=============================================
[2019-04-09 15:25:47,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9051832e-06 3.4894042e-02 5.6100644e-02 4.3614269e-03 1.5181483e-04
 2.9347322e-05 4.9480632e-01 3.9226613e-03 9.4452007e-03 1.1443412e-02
 3.8484028e-01], sum to 1.0000
[2019-04-09 15:25:47,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9397
[2019-04-09 15:25:47,300] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.83474426343861, 0.7276482167587535, 0.0, 1.0, 65.0, 55.70934580393905], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3480600.0000, 
sim time next is 3481200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.78501617853575, 0.7207569746322635, 0.0, 1.0, 65.0, 61.03200263548192], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7320846815446457, 0.7402523248774212, 0.0, 1.0, 1.0, 0.6103200263548192], 
reward next is 0.3897, 
noisyNet noise sample is [array([-0.35579044], dtype=float32), -0.15874492]. 
=============================================
[2019-04-09 15:25:47,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.04209584e-06 3.49818170e-02 5.96394464e-02 4.15068306e-03
 1.54491543e-04 2.85614969e-05 4.84695137e-01 3.97593249e-03
 9.64031927e-03 1.24131795e-02 3.90315354e-01], sum to 1.0000
[2019-04-09 15:25:47,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7100
[2019-04-09 15:25:47,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0179479e-06 5.5097908e-02 1.1312637e-01 6.7353449e-03 7.5403579e-05
 7.3311958e-06 4.0644962e-01 2.9159759e-03 6.7531029e-03 5.6605539e-03
 4.0317741e-01], sum to 1.0000
[2019-04-09 15:25:47,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7298
[2019-04-09 15:25:47,370] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.1666666666666667, 71.83333333333333, 0.0, 0.0, 22.5, 26.72047304894156, 0.7200129983190774, 0.0, 1.0, 65.0, 63.28456328052413], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 22.5, 26.72230213388217, 0.7289823066944866, 0.0, 1.0, 65.0, 60.33743462271711], 
processed observation next is [1.0, 0.30434782608695654, 0.4533702677747, 0.7166666666666667, 0.0, 0.0, 0.375, 0.7268585111568475, 0.7429941022314955, 0.0, 1.0, 1.0, 0.6033743462271711], 
reward next is 0.3966, 
noisyNet noise sample is [array([-0.35579044], dtype=float32), -0.15874492]. 
=============================================
[2019-04-09 15:25:47,381] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 102.0, 781.0, 22.5, 28.6277573240386, 1.191388525522099, 1.0, 1.0, 65.0, 12.0376471847616], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3508200.0000, 
sim time next is 3508800.0000, 
raw observation next is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 22.5, 28.68026484684378, 1.197133821297359, 1.0, 1.0, 65.0, 14.08551940382515], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3322222222222222, 0.8456721915285451, 0.375, 0.890022070570315, 0.8990446070991197, 1.0, 1.0, 1.0, 0.1408551940382515], 
reward next is 0.8591, 
noisyNet noise sample is [array([0.89485955], dtype=float32), 0.26860672]. 
=============================================
[2019-04-09 15:25:47,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5940124e-05 1.4677730e-01 1.3794135e-01 8.6744120e-03 5.7136035e-04
 1.0587789e-04 2.4114272e-01 4.2095394e-03 9.4237877e-03 1.8012844e-02
 4.3312484e-01], sum to 1.0000
[2019-04-09 15:25:47,620] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6472
[2019-04-09 15:25:47,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3561886e-07 3.6467925e-02 5.5460960e-02 2.0378744e-03 9.7841017e-05
 7.0967673e-07 5.2965474e-01 3.4520339e-04 2.3782649e-03 9.5461623e-04
 3.7260142e-01], sum to 1.0000
[2019-04-09 15:25:47,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9825
[2019-04-09 15:25:47,638] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 22.5, 28.74541459863818, 1.213248383873262, 1.0, 1.0, 45.0, 5.165558314859353], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [3.0, 49.0, 79.33333333333333, 633.0, 22.5, 28.88063785057746, 1.234145813470829, 1.0, 1.0, 65.0, 14.14410859010318], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2644444444444444, 0.6994475138121546, 0.375, 0.9067198208814551, 0.9113819378236098, 1.0, 1.0, 1.0, 0.1414410859010318], 
reward next is 0.8586, 
noisyNet noise sample is [array([0.75332606], dtype=float32), -0.9061984]. 
=============================================
[2019-04-09 15:25:47,646] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 39.66666666666666, 46.66666666666667, 390.6666666666667, 19.0, 27.4763095526823, 0.947233001973713, 0.0, 1.0, 65.0, 29.62197210780865], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3603000.0000, 
sim time next is 3603600.0000, 
raw observation next is [0.0, 39.0, 38.5, 328.5, 19.0, 27.46817625931168, 0.9399104208728883, 0.0, 1.0, 65.0, 30.01280554513475], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.39, 0.12833333333333333, 0.3629834254143646, 0.08333333333333333, 0.7890146882759733, 0.8133034736242961, 0.0, 1.0, 1.0, 0.3001280554513475], 
reward next is 0.6999, 
noisyNet noise sample is [array([0.46202603], dtype=float32), -1.5961015]. 
=============================================
[2019-04-09 15:25:47,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[20.704502]
 [20.777308]
 [20.85945 ]
 [20.598326]
 [20.946316]], R is [[21.66307259]
 [22.39478683]
 [23.12356949]
 [23.87680054]
 [24.46850777]].
[2019-04-09 15:25:47,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1927003e-07 4.9193639e-02 5.9685491e-02 1.9270248e-03 3.3485569e-05
 1.9905713e-06 1.9303331e-01 7.9930981e-04 6.3218554e-03 8.8592768e-03
 6.8014401e-01], sum to 1.0000
[2019-04-09 15:25:47,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6722
[2019-04-09 15:25:47,807] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 62.0, 525.0, 22.5, 28.82951497945378, 1.275475809265213, 1.0, 1.0, 65.0, 14.44858098849291], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3515400.0000, 
sim time next is 3516000.0000, 
raw observation next is [3.0, 49.0, 53.83333333333334, 462.6666666666667, 22.5, 28.49273111394069, 1.210771626827613, 1.0, 1.0, 65.0, 8.647526777001083], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.1794444444444445, 0.5112338858195212, 0.375, 0.8743942594950574, 0.9035905422758711, 1.0, 1.0, 1.0, 0.08647526777001083], 
reward next is 0.9135, 
noisyNet noise sample is [array([-1.6792741], dtype=float32), 1.355988]. 
=============================================
[2019-04-09 15:25:47,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[19.841324]
 [19.628979]
 [19.709974]
 [19.586159]
 [19.614906]], R is [[20.45436859]
 [21.10533905]
 [21.35162163]
 [22.01659203]
 [22.67126846]].
[2019-04-09 15:25:47,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6616208e-06 3.7508931e-02 1.6504624e-01 1.0720780e-03 9.5697906e-05
 6.0283342e-06 5.8478212e-01 1.8735069e-03 2.9377136e-03 4.4233957e-03
 2.0225263e-01], sum to 1.0000
[2019-04-09 15:25:47,912] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4912
[2019-04-09 15:25:47,923] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 77.0, 0.0, 0.0, 19.0, 27.42521080340251, 0.9841660706804166, 0.0, 1.0, 25.0, 29.38478049659568], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3534600.0000, 
sim time next is 3535200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 19.0, 27.41073891483199, 0.9788736449531267, 0.0, 1.0, 45.0, 29.92283509598014], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.08333333333333333, 0.784228242902666, 0.8262912149843755, 0.0, 1.0, 0.6, 0.2992283509598014], 
reward next is 0.7008, 
noisyNet noise sample is [array([1.2382127], dtype=float32), 0.4281058]. 
=============================================
[2019-04-09 15:25:48,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7139686e-07 2.1200273e-02 8.2469448e-02 2.4044798e-03 6.7635541e-05
 1.0120630e-06 3.5981449e-01 1.7971131e-03 7.9336595e-03 3.8233839e-03
 5.2048826e-01], sum to 1.0000
[2019-04-09 15:25:48,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1431
[2019-04-09 15:25:48,359] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 22.5, 27.97097149793215, 0.8300410638414316, 1.0, 1.0, 45.0, 47.79166940571461], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3498000.0000, 
sim time next is 3498600.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 22.5, 27.95816143283858, 1.112971100410811, 1.0, 1.0, 45.0, 20.88409007584985], 
processed observation next is [1.0, 0.4782608695652174, 0.5133887349953832, 0.5766666666666665, 0.3844444444444443, 0.899815837937385, 0.375, 0.8298467860698816, 0.8709903668036038, 1.0, 1.0, 0.6, 0.2088409007584985], 
reward next is 0.7912, 
noisyNet noise sample is [array([2.4773753], dtype=float32), -1.3908545]. 
=============================================
[2019-04-09 15:25:48,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3685562e-05 1.2259272e-01 9.4773442e-02 1.6953938e-02 9.7134011e-04
 2.8291639e-04 4.0952438e-01 7.9076216e-03 1.8476930e-02 1.9207712e-02
 3.0925539e-01], sum to 1.0000
[2019-04-09 15:25:48,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5840
[2019-04-09 15:25:48,406] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 19.0, 26.70565670726227, 0.7943212800757223, 0.0, 1.0, 45.0, 38.43958332471686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3580800.0000, 
sim time next is 3581400.0000, 
raw observation next is [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 19.0, 26.73265296304525, 0.7999389792460422, 0.0, 1.0, 45.0, 36.53755013479721], 
processed observation next is [0.0, 0.43478260869565216, 0.3471837488457987, 0.5583333333333332, 0.37333333333333335, 0.8589318600368323, 0.08333333333333333, 0.7277210802537709, 0.7666463264153474, 0.0, 1.0, 0.6, 0.3653755013479721], 
reward next is 0.6346, 
noisyNet noise sample is [array([-1.4159027], dtype=float32), -0.037989236]. 
=============================================
[2019-04-09 15:25:48,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.55962060e-06 1.34646585e-02 5.01879193e-02 4.32504155e-03
 1.44986567e-04 4.03970844e-05 2.67806292e-01 2.24040984e-03
 5.28447377e-03 2.38347356e-03 6.54116809e-01], sum to 1.0000
[2019-04-09 15:25:48,416] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-09 15:25:48,441] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 19.0, 26.8906883940835, 0.8184999665403466, 0.0, 1.0, 45.0, 38.27829904253134], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 19.0, 26.87919988325648, 0.8139271665923274, 0.0, 1.0, 45.0, 37.19429968350795], 
processed observation next is [0.0, 0.043478260869565216, 0.3841181902123731, 0.6916666666666668, 0.0, 0.0, 0.08333333333333333, 0.7399333236047066, 0.7713090555307759, 0.0, 1.0, 0.6, 0.3719429968350795], 
reward next is 0.6281, 
noisyNet noise sample is [array([1.0324156], dtype=float32), 0.5902872]. 
=============================================
[2019-04-09 15:25:48,459] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[15.283158]
 [15.715615]
 [15.967502]
 [15.496321]
 [16.149479]], R is [[15.32784653]
 [15.79178524]
 [16.19312096]
 [16.58704376]
 [16.83010864]].
[2019-04-09 15:25:48,476] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.8608956e-05 3.9709590e-02 1.3852781e-01 1.2405192e-02 5.9720461e-04
 3.8827464e-05 4.5164016e-01 8.0149276e-03 2.2324225e-02 1.2642581e-02
 3.1405085e-01], sum to 1.0000
[2019-04-09 15:25:48,482] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7332
[2019-04-09 15:25:48,491] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 19.0, 26.96395811524672, 0.8521787264570438, 0.0, 1.0, 65.0, 47.31385193520311], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3586200.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 19.0, 26.96927998874682, 0.8574314704588305, 0.0, 1.0, 65.0, 35.42777341246049], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 0.08333333333333333, 0.747439999062235, 0.7858104901529436, 0.0, 1.0, 1.0, 0.3542777341246049], 
reward next is 0.6457, 
noisyNet noise sample is [array([-0.91352713], dtype=float32), 0.6227385]. 
=============================================
[2019-04-09 15:25:48,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4556516e-07 2.3988599e-02 1.1249956e-01 2.6251832e-03 2.8035310e-04
 9.7690508e-06 4.6136340e-01 1.8947339e-03 6.7326948e-03 4.7920202e-03
 3.8581291e-01], sum to 1.0000
[2019-04-09 15:25:48,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7017
[2019-04-09 15:25:48,881] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 19.0, 27.42032978871842, 0.9875458878137428, 0.0, 1.0, 65.0, 35.33591218359044], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 19.0, 27.41899718244069, 0.978240555887257, 0.0, 1.0, 65.0, 33.5087465423638], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7849164318700576, 0.8260801852957523, 0.0, 1.0, 1.0, 0.33508746542363804], 
reward next is 0.6649, 
noisyNet noise sample is [array([0.38479736], dtype=float32), -1.271427]. 
=============================================
[2019-04-09 15:25:49,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5466049e-07 5.1155414e-02 3.6953472e-02 3.2562255e-03 9.3653711e-05
 1.0049100e-06 3.6430264e-01 7.3831069e-04 7.9609714e-03 1.3243167e-03
 5.3421348e-01], sum to 1.0000
[2019-04-09 15:25:49,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1334
[2019-04-09 15:25:49,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.78005779e-05 7.99678192e-02 1.31681532e-01 1.23722395e-02
 1.28277461e-03 1.83162076e-04 3.43999624e-01 1.62398480e-02
 1.06894886e-02 1.72245465e-02 3.86281133e-01], sum to 1.0000
[2019-04-09 15:25:49,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4166
[2019-04-09 15:25:49,064] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.96039720719498, 1.089775418488957, 1.0, 1.0, 25.0, 24.2708573286815], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [1.666666666666667, 55.33333333333334, 0.0, 0.0, 22.5, 27.93193698554733, 1.07706909541157, 1.0, 1.0, 45.0, 23.26065812301124], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.5533333333333335, 0.0, 0.0, 0.375, 0.8276614154622776, 0.8590230318038566, 1.0, 1.0, 0.6, 0.2326065812301124], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.4067532], dtype=float32), 0.117987275]. 
=============================================
[2019-04-09 15:25:49,065] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666666, 70.0, 0.0, 0.0, 19.0, 26.39114594422909, 0.668000632620918, 0.0, 1.0, 65.0, 49.92555352047557], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3568200.0000, 
sim time next is 3568800.0000, 
raw observation next is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 26.3789466416108, 0.6635746510283151, 0.0, 1.0, 45.0, 44.09939921655667], 
processed observation next is [0.0, 0.30434782608695654, 0.28716528162511545, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6982455534675666, 0.7211915503427718, 0.0, 1.0, 0.6, 0.4409939921655667], 
reward next is 0.5590, 
noisyNet noise sample is [array([-2.420044], dtype=float32), 0.7426362]. 
=============================================
[2019-04-09 15:25:49,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[19.934664]
 [20.017414]
 [20.071615]
 [19.939001]
 [19.989191]], R is [[20.43482971]
 [20.9877739 ]
 [21.52623558]
 [22.0402813 ]
 [22.60150528]].
[2019-04-09 15:25:49,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7475438e-05 5.1549528e-02 1.5336141e-01 9.4643384e-03 6.4042828e-04
 1.1495559e-04 4.7013971e-01 7.2436547e-03 1.1286089e-02 1.5419102e-02
 2.8074330e-01], sum to 1.0000
[2019-04-09 15:25:49,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7144
[2019-04-09 15:25:49,233] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 19.0, 26.83015949059916, 0.7818989260353497, 0.0, 1.0, 65.0, 44.58661941172405], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3552000.0000, 
sim time next is 3552600.0000, 
raw observation next is [-3.0, 66.0, 0.0, 0.0, 19.0, 26.80630721096545, 0.7838492937176628, 0.0, 1.0, 60.0, 40.72363116777425], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7338589342471208, 0.7612830979058876, 0.0, 1.0, 0.9, 0.4072363116777425], 
reward next is 0.5928, 
noisyNet noise sample is [array([1.1041366], dtype=float32), -0.15136853]. 
=============================================
[2019-04-09 15:25:49,265] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7530914e-05 5.5945762e-02 2.1794134e-01 4.4741887e-03 1.1037973e-03
 3.8666480e-05 4.0270177e-01 3.9888523e-03 8.6677559e-03 1.5650183e-02
 2.8945017e-01], sum to 1.0000
[2019-04-09 15:25:49,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7096590e-05 4.1583471e-02 1.6135608e-01 9.4059883e-03 1.2584742e-03
 2.9741912e-04 3.4585345e-01 7.1032364e-03 1.5964938e-02 1.2742702e-02
 4.0438703e-01], sum to 1.0000
[2019-04-09 15:25:49,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9473
[2019-04-09 15:25:49,271] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2862
[2019-04-09 15:25:49,292] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 27.17192343226946, 0.7600405413408685, 0.0, 1.0, 65.0, 34.67351156681585], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3635400.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 27.16835380620191, 0.7589665487691427, 0.0, 1.0, 65.0, 34.66164125593642], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7640294838501592, 0.7529888495897142, 0.0, 1.0, 1.0, 0.34661641255936426], 
reward next is 0.6534, 
noisyNet noise sample is [array([-2.1530232], dtype=float32), -1.00122]. 
=============================================
[2019-04-09 15:25:49,293] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 19.0, 26.47325719520563, 0.7090631224210807, 0.0, 1.0, 65.0, 62.95729219472306], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 19.0, 26.46838233095849, 0.706717113059959, 0.0, 1.0, 45.0, 45.12971268915737], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.08333333333333333, 0.7056985275798743, 0.7355723710199863, 0.0, 1.0, 0.6, 0.45129712689157375], 
reward next is 0.5487, 
noisyNet noise sample is [array([0.0924615], dtype=float32), 0.4087741]. 
=============================================
[2019-04-09 15:25:49,317] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[13.162166]
 [13.244213]
 [13.381704]
 [13.160201]
 [13.453079]], R is [[13.5601511 ]
 [14.07781506]
 [14.59016228]
 [15.13923168]
 [15.64118004]].
[2019-04-09 15:25:49,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7911723e-05 9.0328604e-02 1.1157866e-01 5.2198698e-03 1.1406268e-03
 8.1398837e-05 3.3710018e-01 2.6268037e-03 1.0747714e-02 1.0348231e-02
 4.3080997e-01], sum to 1.0000
[2019-04-09 15:25:49,377] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-09 15:25:49,389] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 19.0, 27.4629350450472, 0.9643561346521113, 0.0, 1.0, 45.0, 27.03621417520569], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3599400.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 19.0, 27.45349933074572, 0.9654085174142163, 0.0, 1.0, 45.0, 24.96227101464865], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 0.08333333333333333, 0.7877916108954768, 0.8218028391380722, 0.0, 1.0, 0.6, 0.2496227101464865], 
reward next is 0.7504, 
noisyNet noise sample is [array([-0.47650245], dtype=float32), -0.89965713]. 
=============================================
[2019-04-09 15:25:49,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[15.380607]
 [15.719493]
 [15.599349]
 [15.630661]
 [15.574021]], R is [[15.9801836 ]
 [16.55002022]
 [17.11639404]
 [17.7064476 ]
 [18.23872566]].
[2019-04-09 15:25:49,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.18760235e-07 4.77688536e-02 1.78528875e-01 9.59400320e-04
 1.00605816e-04 6.45874707e-07 4.37654704e-01 9.02680040e-04
 1.63251953e-03 3.00312834e-03 3.29448283e-01], sum to 1.0000
[2019-04-09 15:25:49,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7645
[2019-04-09 15:25:49,680] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 28.09287708374083, 1.100534635349616, 1.0, 1.0, 65.0, 27.1261108725172], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3523800.0000, 
sim time next is 3524400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.98540258579421, 1.097717366542587, 1.0, 1.0, 65.0, 28.53833453298994], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8321168821495174, 0.8659057888475289, 1.0, 1.0, 1.0, 0.2853833453298994], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.25782093], dtype=float32), -0.28708228]. 
=============================================
