Using TensorFlow backend.
[2019-04-03 21:51:49,373] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-03 21:51:49,373] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-03 21:51:49.405208: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-03 21:52:05,120] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-03 21:52:05,120] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-03 21:52:05,143] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,168] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,191] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-03 21:52:05,192] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:05,192] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-03 21:52:05,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-03 21:52:06,193] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:06,195] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-03 21:52:06,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:06,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-03 21:52:07,196] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:07,197] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-03 21:52:07,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:07,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-03 21:52:08,198] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:08,199] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-03 21:52:08,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:08,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-03 21:52:09,199] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:09,200] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-03 21:52:09,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-03 21:52:09,362] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 21:52:09,362] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,363] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 21:52:09,363] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,364] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,367] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-03 21:52:09,376] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:09,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:10,201] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:10,202] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-03 21:52:10,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:10,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-03 21:52:11,203] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:11,203] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-03 21:52:11,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:11,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-03 21:52:12,204] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:12,211] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-03 21:52:12,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:12,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-03 21:52:13,211] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:13,212] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-03 21:52:13,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:13,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-03 21:52:14,213] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:14,214] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-03 21:52:14,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:14,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-03 21:52:15,215] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:15,216] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-03 21:52:15,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:15,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-03 21:52:16,216] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:16,246] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-03 21:52:16,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:16,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-03 21:52:17,247] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:17,253] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-03 21:52:17,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:17,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-03 21:52:18,253] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:18,254] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-03 21:52:19,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-03 21:52:19,269] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:19,270] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-03 21:52:19,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-03 21:52:20,285] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:20,293] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-03 21:52:20,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:20,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-03 21:53:12,040] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:12,041] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.633333333333334, 69.66666666666667, 226.6666666666667, 199.3333333333333, 19.5, 21.02514176778423, -0.5013038979696127, 1.0, 1.0, 0.0]
[2019-04-03 21:53:12,041] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:12,042] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.29535767 0.05710881 0.14783882 0.06952234 0.23516577 0.15613434
 0.03887217], sampled 0.7370836540922121
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [18.33333333333334, 27.66666666666667, 113.3333333333333, 807.5, 19.0, 25.47998487573564, 0.5130360313132977, 0.0, 0.0, 0.0]
[2019-04-03 21:53:51,425] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:51,426] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.24825431 0.15387644 0.07108331 0.03398669 0.26426855 0.1600439
 0.06848678], sampled 0.48126357533880193
[2019-04-03 21:54:00,273] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7487.7648 180804475.9848 157.5327
[2019-04-03 21:54:16,992] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7238.7653 205192080.8023 -462.1879
[2019-04-03 21:54:18,848] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7029.0849 219185844.7234 -571.2267
[2019-04-03 21:54:19,872] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7238.765252242774, 205192080.8023241, -462.1879363835367, 7487.76483304243, 180804475.98477572, 157.5327408785089, 7029.084874054795, 219185844.72339004, -571.2267313975015]
[2019-04-03 21:54:22,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05218945 0.10030949 0.2340054  0.15904444 0.25305936 0.13520248
 0.06618941], sum to 1.0000
[2019-04-03 21:54:22,608] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6494
[2019-04-03 21:54:22,700] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 23.0, 20.59993950745187, -0.6797576976732419, 0.0, 1.0, 63207.95701647342], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 23.5, 20.68806440247398, -0.6628033567031629, 0.0, 1.0, 48080.04928047442], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.4583333333333333, 0.22400536687283168, 0.2790655477656124, 0.0, 1.0, 0.22895261562130675], 
reward next is 0.7710, 
noisyNet noise sample is [array([0.99304634], dtype=float32), -0.3451928]. 
=============================================
[2019-04-03 21:54:24,661] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00331325 0.09501556 0.3812245  0.14290395 0.15536426 0.20748755
 0.01469096], sum to 1.0000
[2019-04-03 21:54:24,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5656
[2019-04-03 21:54:24,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 82.0, 0.0, 22.0, 22.71075411741278, -0.2570272608354004, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 42600.0000, 
sim time next is 43200.0000, 
raw observation next is [7.7, 93.0, 85.5, 0.0, 22.0, 22.64199669795142, -0.2677039851741841, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6759002770083103, 0.93, 0.285, 0.0, 0.3333333333333333, 0.3868330581626183, 0.410765338275272, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3307431], dtype=float32), 1.3370479]. 
=============================================
[2019-04-03 21:54:27,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7720619e-07 2.4961217e-03 8.4030437e-01 9.9043354e-02 5.0874963e-02
 6.9374945e-03 3.4333099e-04], sum to 1.0000
[2019-04-03 21:54:27,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6390
[2019-04-03 21:54:27,574] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 19.0, 19.33621231722455, -1.004916573329498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 19.232438681472, -1.029638788483528, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.10270322345600007, 0.15678707050549065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70230705], dtype=float32), 0.89377916]. 
=============================================
[2019-04-03 21:54:27,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7218360e-08 8.5537927e-04 9.1076672e-01 7.2108164e-02 1.3164518e-02
 3.0514137e-03 5.3729033e-05], sum to 1.0000
[2019-04-03 21:54:27,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9649
[2019-04-03 21:54:27,922] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 69.16666666666667, 0.0, 0.0, 19.0, 18.84353651059873, -1.072882648077402, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 111000.0000, 
sim time next is 111600.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 19.0, 18.86413336498549, -1.087634815353061, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.08333333333333333, 0.07201111374879095, 0.13745506154897966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42249033], dtype=float32), -1.3635674]. 
=============================================
[2019-04-03 21:54:32,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5116667e-09 1.1921683e-03 8.1775254e-01 1.6922353e-01 9.5153833e-03
 2.3050890e-03 1.1239213e-05], sum to 1.0000
[2019-04-03 21:54:32,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0086
[2019-04-03 21:54:32,224] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 18.60049842110701, -1.192838908478353, 0.0, 1.0, 31213.29031416882], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 18.56338789566691, -1.198792198660052, 0.0, 1.0, 55948.85257508064], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.04694899130557584, 0.10040260044664932, 0.0, 1.0, 0.266423107500384], 
reward next is 0.7336, 
noisyNet noise sample is [array([0.7703871], dtype=float32), 0.058684394]. 
=============================================
[2019-04-03 21:54:34,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3073475e-05 1.5752574e-02 7.4386108e-01 1.9941944e-01 1.7058205e-02
 2.2265976e-02 1.6296246e-03], sum to 1.0000
[2019-04-03 21:54:34,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-03 21:54:34,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 19.19547341570266, -1.214270970483244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 234600.0000, 
sim time next is 235200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 19.17664315847131, -1.243893967551409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.09805359653927592, 0.0853686774828637, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45725167], dtype=float32), 0.3428187]. 
=============================================
[2019-04-03 21:54:39,159] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7695: loss 2.7383
[2019-04-03 21:54:39,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7695: learning rate 0.0005
[2019-04-03 21:54:39,258] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7721: loss -3.8198
[2019-04-03 21:54:39,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7721: learning rate 0.0005
[2019-04-03 21:54:39,429] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7781: loss -2.0301
[2019-04-03 21:54:39,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7781: learning rate 0.0005
[2019-04-03 21:54:39,501] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7801: loss 2.7259
[2019-04-03 21:54:39,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7802: learning rate 0.0005
[2019-04-03 21:54:39,707] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7880: loss 2.4261
[2019-04-03 21:54:39,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7881: learning rate 0.0005
[2019-04-03 21:54:39,887] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7952: loss 2.4591
[2019-04-03 21:54:39,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7952: learning rate 0.0005
[2019-04-03 21:54:39,904] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7958: loss -1.2203
[2019-04-03 21:54:39,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7958: learning rate 0.0005
[2019-04-03 21:54:39,944] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7978: loss 2.5477
[2019-04-03 21:54:39,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7978: learning rate 0.0005
[2019-04-03 21:54:40,145] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8051: loss -2.6873
[2019-04-03 21:54:40,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8051: learning rate 0.0005
[2019-04-03 21:54:40,448] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8147: loss -4.4444
[2019-04-03 21:54:40,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8147: learning rate 0.0005
[2019-04-03 21:54:40,563] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8166: loss -3.4737
[2019-04-03 21:54:40,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8166: learning rate 0.0005
[2019-04-03 21:54:40,718] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8194: loss -0.1526
[2019-04-03 21:54:40,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8194: learning rate 0.0005
[2019-04-03 21:54:40,907] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8228: loss -4.8200
[2019-04-03 21:54:40,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8228: learning rate 0.0005
[2019-04-03 21:54:40,966] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8238: loss -2.8366
[2019-04-03 21:54:40,966] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8238: learning rate 0.0005
[2019-04-03 21:54:41,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8284: loss 2.3607
[2019-04-03 21:54:41,278] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8284: learning rate 0.0005
[2019-04-03 21:54:41,937] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8396: loss -5.0837
[2019-04-03 21:54:41,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8396: learning rate 0.0005
[2019-04-03 21:54:42,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2356952e-06 8.1025576e-03 4.8982650e-01 5.0221648e-02 5.9171814e-02
 3.9220253e-01 4.7176442e-04], sum to 1.0000
[2019-04-03 21:54:42,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6620
[2019-04-03 21:54:42,221] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 93.0, 652.1666666666666, 25.5, 23.83663131679204, -0.08831889038324951, 1.0, 1.0, 84680.4561043097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 307200.0000, 
sim time next is 307800.0000, 
raw observation next is [-9.5, 44.0, 95.0, 631.0, 26.0, 24.22910840677069, -0.1490813370999395, 1.0, 1.0, 83159.1465372545], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31666666666666665, 0.6972375690607735, 0.6666666666666666, 0.5190923672308907, 0.4503062209666868, 1.0, 1.0, 0.39599593589168813], 
reward next is 0.6040, 
noisyNet noise sample is [array([1.0638667], dtype=float32), -0.44152853]. 
=============================================
[2019-04-03 21:54:44,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1775933e-16 2.3203254e-06 9.9717391e-01 3.0496615e-04 4.1352917e-04
 2.1052177e-03 2.3362579e-09], sum to 1.0000
[2019-04-03 21:54:44,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5863
[2019-04-03 21:54:44,868] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 19.0, 19.56069961200904, -1.000949568726879, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 19.0, 19.43289207290363, -1.029003323046685, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.11940767274196921, 0.15699889231777164, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05505029], dtype=float32), 1.3918334]. 
=============================================
[2019-04-03 21:54:44,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.749504]
 [67.60857 ]
 [66.22649 ]
 [66.06203 ]
 [65.86723 ]], R is [[67.54194641]
 [67.86653137]
 [68.18786621]
 [68.50598907]
 [68.82093048]].
[2019-04-03 21:54:58,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8444718e-16 8.9500209e-05 2.5202323e-02 7.6089107e-04 6.5231508e-01
 3.2163221e-01 1.1703200e-08], sum to 1.0000
[2019-04-03 21:54:58,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7506
[2019-04-03 21:54:58,581] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.1, 52.0, 0.0, 0.0, 26.0, 21.93353702713187, -0.4992256650301629, 0.0, 1.0, 47190.31800176897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 26.0, 21.85814974008795, -0.5065307884382473, 0.0, 1.0, 47292.97456245552], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.6666666666666666, 0.32151247834066243, 0.3311564038539176, 0.0, 1.0, 0.22520464077359773], 
reward next is 0.7748, 
noisyNet noise sample is [array([2.3387072], dtype=float32), 0.8756776]. 
=============================================
[2019-04-03 21:55:12,581] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15178: loss 0.3405
[2019-04-03 21:55:12,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15178: learning rate 0.0005
[2019-04-03 21:55:13,645] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15420: loss -1.8555
[2019-04-03 21:55:13,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15420: learning rate 0.0005
[2019-04-03 21:55:14,062] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15529: loss -1.7098
[2019-04-03 21:55:14,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15529: learning rate 0.0005
[2019-04-03 21:55:14,545] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15642: loss -1.4616
[2019-04-03 21:55:14,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15642: learning rate 0.0005
[2019-04-03 21:55:15,221] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15816: loss -0.0574
[2019-04-03 21:55:15,222] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15816: learning rate 0.0005
[2019-04-03 21:55:15,604] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 15924: loss -0.2240
[2019-04-03 21:55:15,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 15924: learning rate 0.0005
[2019-04-03 21:55:15,839] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15973: loss -0.0214
[2019-04-03 21:55:15,842] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15973: learning rate 0.0005
[2019-04-03 21:55:15,999] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16017: loss -0.1738
[2019-04-03 21:55:16,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16017: learning rate 0.0005
[2019-04-03 21:55:16,764] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16214: loss -0.0811
[2019-04-03 21:55:16,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16214: learning rate 0.0005
[2019-04-03 21:55:16,815] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16231: loss -0.2142
[2019-04-03 21:55:16,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16231: learning rate 0.0005
[2019-04-03 21:55:16,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16232: loss -0.0017
[2019-04-03 21:55:16,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16232: learning rate 0.0005
[2019-04-03 21:55:17,402] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16387: loss -0.6230
[2019-04-03 21:55:17,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16387: learning rate 0.0005
[2019-04-03 21:55:17,512] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16423: loss -0.5630
[2019-04-03 21:55:17,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16423: learning rate 0.0005
[2019-04-03 21:55:17,649] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16453: loss -0.2076
[2019-04-03 21:55:17,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16453: learning rate 0.0005
[2019-04-03 21:55:17,769] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16478: loss -0.0006
[2019-04-03 21:55:17,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16478: learning rate 0.0005
[2019-04-03 21:55:18,590] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16645: loss -1.4431
[2019-04-03 21:55:18,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16645: learning rate 0.0005
[2019-04-03 21:55:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8930017e-19 4.0539046e-05 9.8234916e-04 2.3665933e-05 8.0735862e-01
 1.9159484e-01 9.1761543e-11], sum to 1.0000
[2019-04-03 21:55:26,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-03 21:55:26,548] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.03143698967301, -0.01745961037973359, 0.0, 1.0, 41862.8588172094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 712200.0000, 
sim time next is 712800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.06282913246363, -0.01297802031459702, 0.0, 1.0, 41895.58742310471], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5052357610386359, 0.49567399322846767, 0.0, 1.0, 0.1995027972528796], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.9479938], dtype=float32), 2.0474946]. 
=============================================
[2019-04-03 21:55:34,289] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1955319e-21 7.2804169e-06 6.3914587e-05 2.1344792e-07 4.4341391e-01
 5.5651474e-01 2.2226226e-13], sum to 1.0000
[2019-04-03 21:55:34,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1056
[2019-04-03 21:55:34,505] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.04682075577638, 0.3064695124447858, 1.0, 1.0, 28458.79941226137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 760800.0000, 
sim time next is 761400.0000, 
raw observation next is [-4.45, 55.5, 0.0, 0.0, 26.0, 25.05760531118658, 0.2970658248499397, 1.0, 1.0, 30448.97023440519], 
processed observation next is [1.0, 0.8260869565217391, 0.3393351800554017, 0.555, 0.0, 0.0, 0.6666666666666666, 0.588133775932215, 0.5990219416166466, 1.0, 1.0, 0.14499509635431043], 
reward next is 0.8550, 
noisyNet noise sample is [array([-0.22677079], dtype=float32), -0.87427735]. 
=============================================
[2019-04-03 21:55:49,173] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.24930635e-17 1.14188697e-05 5.73471698e-05 6.65195671e-07
 7.92694807e-01 2.07235739e-01 5.72066439e-12], sum to 1.0000
[2019-04-03 21:55:49,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-03 21:55:49,212] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88027426770575, 0.2625381709491256, 0.0, 1.0, 42555.07319005419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.85566671565665, 0.2565097974399529, 0.0, 1.0, 42308.78922784868], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5713055596380542, 0.5855032658133176, 0.0, 1.0, 0.20147042489451755], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.40640062], dtype=float32), -1.2390872]. 
=============================================
[2019-04-03 21:55:50,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 22954: loss 4.7164
[2019-04-03 21:55:50,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 22954: learning rate 0.0005
[2019-04-03 21:55:50,518] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 22994: loss 4.5903
[2019-04-03 21:55:50,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 22994: learning rate 0.0005
[2019-04-03 21:55:51,121] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23169: loss 9.9971
[2019-04-03 21:55:51,124] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23169: learning rate 0.0005
[2019-04-03 21:55:53,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23669: loss 3.8820
[2019-04-03 21:55:53,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23669: learning rate 0.0005
[2019-04-03 21:55:53,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23801: loss 6.8059
[2019-04-03 21:55:53,654] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23801: learning rate 0.0005
[2019-04-03 21:55:53,926] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23866: loss 10.0501
[2019-04-03 21:55:53,934] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23866: learning rate 0.0005
[2019-04-03 21:55:54,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23964: loss 3.0662
[2019-04-03 21:55:54,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23964: learning rate 0.0005
[2019-04-03 21:55:54,657] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24025: loss 6.0603
[2019-04-03 21:55:54,659] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24025: learning rate 0.0005
[2019-04-03 21:55:54,920] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24083: loss 5.0944
[2019-04-03 21:55:54,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24083: learning rate 0.0005
[2019-04-03 21:55:55,580] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24263: loss 7.8492
[2019-04-03 21:55:55,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24263: learning rate 0.0005
[2019-04-03 21:55:55,917] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24358: loss 9.4083
[2019-04-03 21:55:55,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24358: learning rate 0.0005
[2019-04-03 21:55:56,701] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24535: loss 7.6114
[2019-04-03 21:55:56,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24543: learning rate 0.0005
[2019-04-03 21:55:56,881] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24572: loss 3.8669
[2019-04-03 21:55:56,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24572: learning rate 0.0005
[2019-04-03 21:55:57,009] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24608: loss 8.1436
[2019-04-03 21:55:57,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24608: learning rate 0.0005
[2019-04-03 21:55:57,581] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24736: loss 3.6839
[2019-04-03 21:55:57,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24736: learning rate 0.0005
[2019-04-03 21:55:58,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24930: loss 3.1396
[2019-04-03 21:55:58,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24930: learning rate 0.0005
[2019-04-03 21:56:00,031] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2251097e-18 3.0252854e-06 7.2642310e-05 6.9722313e-07 3.0281276e-01
 6.9711089e-01 3.2385376e-11], sum to 1.0000
[2019-04-03 21:56:00,031] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7886
[2019-04-03 21:56:00,177] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 26.0, 25.2349940183401, 0.4058565743650567, 0.0, 1.0, 38085.93055891249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.25876206675157, 0.410687643100103, 0.0, 1.0, 38058.27010464832], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 0.6666666666666666, 0.604896838895964, 0.6368958810333677, 0.0, 1.0, 0.18122985764118246], 
reward next is 0.8188, 
noisyNet noise sample is [array([1.6095223], dtype=float32), 0.22733755]. 
=============================================
[2019-04-03 21:56:01,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6311515e-19 1.3189878e-06 1.3516865e-04 1.4556162e-05 6.8105765e-02
 9.3174320e-01 4.0980330e-11], sum to 1.0000
[2019-04-03 21:56:01,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5152
[2019-04-03 21:56:01,466] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45628984959813, 0.4643143628584527, 0.0, 1.0, 40171.03299944349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.56846046993575, 0.4759921567979018, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6307050391613126, 0.6586640522659672, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51849633], dtype=float32), -2.172516]. 
=============================================
[2019-04-03 21:56:01,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.18127 ]
 [84.214935]
 [84.14237 ]
 [84.22881 ]
 [84.28426 ]], R is [[84.17595673]
 [84.14290619]
 [84.04769897]
 [84.11793518]
 [84.27675629]].
[2019-04-03 21:56:05,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1004504e-21 1.5623508e-09 6.2154828e-07 1.4041652e-09 2.6282910e-06
 9.9999678e-01 4.6914119e-14], sum to 1.0000
[2019-04-03 21:56:05,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5837
[2019-04-03 21:56:05,283] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.88694223208595, 0.6276386118926739, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 26.0, 25.94285774835221, 0.6289108342290571, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6619048123626842, 0.7096369447430191, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.24892], dtype=float32), -0.57858694]. 
=============================================
[2019-04-03 21:56:07,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0565752e-24 5.2870305e-08 3.0180078e-07 3.6780047e-08 1.2724071e-03
 9.9872726e-01 1.0606773e-14], sum to 1.0000
[2019-04-03 21:56:07,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7156
[2019-04-03 21:56:07,312] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.7293761269718, 0.6069222718372939, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.8405609423404, 0.6107631339609298, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6533800785283667, 0.70358771132031, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13368121], dtype=float32), 0.87120336]. 
=============================================
[2019-04-03 21:56:11,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2868948e-23 2.1373585e-07 1.5458285e-05 1.2973233e-07 9.4435003e-05
 9.9988973e-01 3.1990095e-13], sum to 1.0000
[2019-04-03 21:56:11,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7551
[2019-04-03 21:56:11,590] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.84077930929694, 0.6108564138892248, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 25.88046920676895, 0.6074323790349297, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8333333333333334, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6567057672307458, 0.70247745967831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9758724], dtype=float32), 1.2294745]. 
=============================================
[2019-04-03 21:56:13,664] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 30186: loss 0.9056
[2019-04-03 21:56:13,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 30188: learning rate 0.0005
[2019-04-03 21:56:13,917] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 30311: loss 1.0124
[2019-04-03 21:56:13,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 30311: learning rate 0.0005
[2019-04-03 21:56:14,056] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 30389: loss 3.4251
[2019-04-03 21:56:14,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 30389: learning rate 0.0005
[2019-04-03 21:56:15,923] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7607441e-18 2.1983790e-06 6.3931680e-06 6.3433308e-06 1.4034408e-02
 9.8595065e-01 5.3942167e-11], sum to 1.0000
[2019-04-03 21:56:15,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4899
[2019-04-03 21:56:15,933] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 171.0, 0.0, 26.0, 25.10197675501551, 0.5069373756860273, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1168200.0000, 
sim time next is 1168800.0000, 
raw observation next is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 26.0, 25.11459217225383, 0.5066780451774603, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9741458910433982, 0.6433333333333333, 0.5633333333333334, 0.0, 0.6666666666666666, 0.5928826810211524, 0.6688926817258202, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2379901], dtype=float32), 0.24448161]. 
=============================================
[2019-04-03 21:56:16,052] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31405: loss 0.4849
[2019-04-03 21:56:16,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31405: learning rate 0.0005
[2019-04-03 21:56:16,864] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31724: loss 0.3420
[2019-04-03 21:56:16,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31725: learning rate 0.0005
[2019-04-03 21:56:17,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31897: loss 0.2110
[2019-04-03 21:56:17,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31897: learning rate 0.0005
[2019-04-03 21:56:17,374] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31925: loss 0.2657
[2019-04-03 21:56:17,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31925: learning rate 0.0005
[2019-04-03 21:56:17,871] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32112: loss 0.2046
[2019-04-03 21:56:17,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32112: learning rate 0.0005
[2019-04-03 21:56:17,899] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32127: loss 0.2462
[2019-04-03 21:56:17,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32127: learning rate 0.0005
[2019-04-03 21:56:18,493] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32375: loss 0.1879
[2019-04-03 21:56:18,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32375: learning rate 0.0005
[2019-04-03 21:56:18,920] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32528: loss 0.1866
[2019-04-03 21:56:18,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32528: learning rate 0.0005
[2019-04-03 21:56:18,960] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32546: loss 0.2274
[2019-04-03 21:56:18,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32546: learning rate 0.0005
[2019-04-03 21:56:19,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.31697165e-23 4.13439144e-11 4.70017596e-07 1.01670681e-08
 2.82006404e-05 9.99971271e-01 4.44651470e-15], sum to 1.0000
[2019-04-03 21:56:19,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4095
[2019-04-03 21:56:19,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.47961741114141, 0.586298205088504, 0.0, 1.0, 36960.12672906319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1292400.0000, 
sim time next is 1293000.0000, 
raw observation next is [5.316666666666667, 99.33333333333334, 0.0, 0.0, 26.0, 25.45752934350334, 0.588322216630032, 0.0, 1.0, 46872.99723456703], 
processed observation next is [0.0, 1.0, 0.6098799630655587, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6214607786252783, 0.6961074055433439, 0.0, 1.0, 0.2232047487360335], 
reward next is 0.7768, 
noisyNet noise sample is [array([-2.5129247], dtype=float32), -0.3925216]. 
=============================================
[2019-04-03 21:56:19,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32789: loss 0.1012
[2019-04-03 21:56:19,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[106.510086]
 [106.35057 ]
 [106.27151 ]
 [106.24533 ]
 [106.274086]], R is [[106.42341614]
 [106.18318176]
 [105.95742035]
 [105.80856323]
 [105.66117859]].
[2019-04-03 21:56:19,636] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32789: learning rate 0.0005
[2019-04-03 21:56:20,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32914: loss 0.0219
[2019-04-03 21:56:20,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32914: learning rate 0.0005
[2019-04-03 21:56:20,806] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 33105: loss 0.0060
[2019-04-03 21:56:20,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 33105: learning rate 0.0005
[2019-04-03 21:56:21,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33270: loss 0.0281
[2019-04-03 21:56:21,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33270: learning rate 0.0005
[2019-04-03 21:56:36,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8927056e-19 1.1603905e-09 5.0980844e-07 7.4582713e-09 3.1024034e-03
 9.9689710e-01 3.0017913e-13], sum to 1.0000
[2019-04-03 21:56:36,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7735
[2019-04-03 21:56:36,027] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28023041050545, 0.5113809897651479, 0.0, 1.0, 43183.50296995221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1378800.0000, 
sim time next is 1379400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30731211376429, 0.5094672343771376, 0.0, 1.0, 42095.64461868651], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089426761470241, 0.6698224114590459, 0.0, 1.0, 0.20045545056517383], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.05579865], dtype=float32), 0.30761316]. 
=============================================
[2019-04-03 21:56:41,872] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 38419: loss 3.8335
[2019-04-03 21:56:41,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 38419: learning rate 0.0005
[2019-04-03 21:56:42,265] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 38522: loss 3.8511
[2019-04-03 21:56:42,267] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 38522: learning rate 0.0005
[2019-04-03 21:56:43,055] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 38741: loss 3.3702
[2019-04-03 21:56:43,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 38741: learning rate 0.0005
[2019-04-03 21:56:44,791] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4389605e-24 9.3831117e-13 5.5633307e-09 1.1835782e-12 2.5888650e-07
 9.9999976e-01 2.5293987e-16], sum to 1.0000
[2019-04-03 21:56:44,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-03 21:56:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 26.17735463620768, 0.7069521436153606, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1546800.0000, 
sim time next is 1547400.0000, 
raw observation next is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 26.16000857858801, 0.6981999176658906, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6505078485687905, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.680000714882334, 0.7327333058886302, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53029364], dtype=float32), 0.6288674]. 
=============================================
[2019-04-03 21:56:45,498] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39592: loss 2.5987
[2019-04-03 21:56:45,500] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39593: learning rate 0.0005
[2019-04-03 21:56:45,682] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39660: loss 2.8688
[2019-04-03 21:56:45,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39660: learning rate 0.0005
[2019-04-03 21:56:46,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40002: loss 3.2121
[2019-04-03 21:56:46,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40002: learning rate 0.0005
[2019-04-03 21:56:46,645] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40031: loss 2.8636
[2019-04-03 21:56:46,646] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40031: learning rate 0.0005
[2019-04-03 21:56:46,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6474290e-20 2.8822089e-10 5.7286974e-07 1.0240390e-08 2.4494257e-06
 9.9999690e-01 2.7238555e-13], sum to 1.0000
[2019-04-03 21:56:46,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5146
[2019-04-03 21:56:46,789] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.48548859529517, 0.4891777182183658, 0.0, 1.0, 55018.53451209464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1462200.0000, 
sim time next is 1462800.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.44535404002222, 0.4829982295905602, 0.0, 1.0, 63926.20109956854], 
processed observation next is [1.0, 0.9565217391304348, 0.4976915974145891, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6204461700018516, 0.66099940986352, 0.0, 1.0, 0.3044104814265169], 
reward next is 0.6956, 
noisyNet noise sample is [array([-0.03405539], dtype=float32), 1.5687543]. 
=============================================
[2019-04-03 21:56:46,806] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 40056: loss 3.1413
[2019-04-03 21:56:46,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 40058: learning rate 0.0005
[2019-04-03 21:56:47,166] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40217: loss 2.7958
[2019-04-03 21:56:47,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40217: learning rate 0.0005
[2019-04-03 21:56:47,252] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40260: loss 2.5832
[2019-04-03 21:56:47,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40273: learning rate 0.0005
[2019-04-03 21:56:48,421] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40813: loss 2.7363
[2019-04-03 21:56:48,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40815: learning rate 0.0005
[2019-04-03 21:56:48,686] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40932: loss 2.7997
[2019-04-03 21:56:48,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40933: learning rate 0.0005
[2019-04-03 21:56:48,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40952: loss 2.9652
[2019-04-03 21:56:48,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40952: learning rate 0.0005
[2019-04-03 21:56:48,882] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 41025: loss 2.6248
[2019-04-03 21:56:48,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 41025: learning rate 0.0005
[2019-04-03 21:56:49,057] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 41114: loss 2.7561
[2019-04-03 21:56:49,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 41114: learning rate 0.0005
[2019-04-03 21:56:49,511] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41345: loss 3.1113
[2019-04-03 21:56:49,511] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41345: learning rate 0.0005
[2019-04-03 21:56:51,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3337507e-19 8.5089439e-08 1.7210397e-04 3.8160854e-07 3.9961070e-02
 9.5986634e-01 4.1669813e-11], sum to 1.0000
[2019-04-03 21:56:51,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7326
[2019-04-03 21:56:51,196] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.8, 79.0, 37.0, 35.0, 26.0, 25.82473932130492, 0.5597598565371857, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1585800.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05797675397686, 0.5804240930148894, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.6666666666666666, 0.671498062831405, 0.6934746976716298, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8160363], dtype=float32), -1.2426319]. 
=============================================
[2019-04-03 21:56:58,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9884628e-23 5.2565050e-11 8.4651322e-07 8.4337970e-10 5.8278232e-07
 9.9999857e-01 1.7881704e-14], sum to 1.0000
[2019-04-03 21:56:58,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-03 21:56:58,717] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 26.0, 25.61125527404931, 0.514790247793765, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72068154524737, 0.5391253285103353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6433901287706142, 0.6797084428367784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6804553], dtype=float32), -0.3316243]. 
=============================================
[2019-04-03 21:57:04,835] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 46786: loss 0.1972
[2019-04-03 21:57:04,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 46786: learning rate 0.0005
[2019-04-03 21:57:04,973] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 46818: loss 0.1279
[2019-04-03 21:57:04,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 46820: learning rate 0.0005
[2019-04-03 21:57:05,381] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 46922: loss 0.0401
[2019-04-03 21:57:05,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 46922: learning rate 0.0005
[2019-04-03 21:57:07,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47549: loss 0.1498
[2019-04-03 21:57:07,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47549: learning rate 0.0005
[2019-04-03 21:57:09,735] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47867: loss 0.0104
[2019-04-03 21:57:09,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47867: learning rate 0.0005
[2019-04-03 21:57:10,078] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47928: loss 0.0162
[2019-04-03 21:57:10,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47928: learning rate 0.0005
[2019-04-03 21:57:10,665] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48034: loss 0.0042
[2019-04-03 21:57:10,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48034: learning rate 0.0005
[2019-04-03 21:57:10,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7519339e-20 7.9586232e-10 1.7563737e-07 2.5255384e-09 1.1930944e-06
 9.9999857e-01 3.5720222e-13], sum to 1.0000
[2019-04-03 21:57:10,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-03 21:57:11,002] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.02952870423989, 0.3158426263707014, 0.0, 1.0, 46021.95165223091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1803600.0000, 
sim time next is 1804200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.05256506632329, 0.3093053554774092, 0.0, 1.0, 45981.37811129295], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5877137555269408, 0.6031017851591364, 0.0, 1.0, 0.21895894338710928], 
reward next is 0.7810, 
noisyNet noise sample is [array([0.05378729], dtype=float32), -0.28019023]. 
=============================================
[2019-04-03 21:57:11,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48107: loss 0.0028
[2019-04-03 21:57:11,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48107: learning rate 0.0005
[2019-04-03 21:57:11,518] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48203: loss 0.0139
[2019-04-03 21:57:11,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48203: learning rate 0.0005
[2019-04-03 21:57:11,741] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48263: loss 0.0312
[2019-04-03 21:57:11,741] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48263: learning rate 0.0005
[2019-04-03 21:57:13,033] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48574: loss 0.0169
[2019-04-03 21:57:13,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48574: learning rate 0.0005
[2019-04-03 21:57:14,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48791: loss 0.0493
[2019-04-03 21:57:14,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48791: learning rate 0.0005
[2019-04-03 21:57:14,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48874: loss 0.0511
[2019-04-03 21:57:14,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48874: learning rate 0.0005
[2019-04-03 21:57:14,654] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48918: loss 0.0218
[2019-04-03 21:57:14,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48918: learning rate 0.0005
[2019-04-03 21:57:14,743] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48930: loss 0.0329
[2019-04-03 21:57:14,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48930: learning rate 0.0005
[2019-04-03 21:57:15,273] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 49057: loss 0.0082
[2019-04-03 21:57:15,278] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 49057: learning rate 0.0005
[2019-04-03 21:57:18,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.7862517e-20 3.6541822e-10 6.1582845e-08 2.4996110e-09 1.8277998e-06
 9.9999809e-01 2.1337892e-13], sum to 1.0000
[2019-04-03 21:57:18,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1095
[2019-04-03 21:57:18,730] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.02770247555555, 0.2848621361801081, 0.0, 1.0, 41963.60892076751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02297283625713, 0.288091452417594, 0.0, 1.0, 45361.88558615628], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 0.6666666666666666, 0.5852477363547607, 0.596030484139198, 0.0, 1.0, 0.21600897898169655], 
reward next is 0.7840, 
noisyNet noise sample is [array([0.52442026], dtype=float32), -0.27761677]. 
=============================================
[2019-04-03 21:57:19,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3103565e-20 4.9614972e-11 4.4990571e-08 3.5422887e-10 2.9692858e-08
 1.0000000e+00 5.3056396e-14], sum to 1.0000
[2019-04-03 21:57:19,419] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4953
[2019-04-03 21:57:19,506] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07953803100467, 0.2778965265293961, 0.0, 1.0, 21248.72017079855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869000.0000, 
sim time next is 1869600.0000, 
raw observation next is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06347685767314, 0.2680606223869741, 0.0, 1.0, 38962.26795501552], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8033333333333335, 0.30333333333333334, 0.015469613259668509, 0.6666666666666666, 0.5886230714727617, 0.589353540795658, 0.0, 1.0, 0.18553460930959773], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.10124384], dtype=float32), 0.68732774]. 
=============================================
[2019-04-03 21:57:25,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0608634e-19 2.8442665e-10 3.8933152e-07 1.9274566e-08 3.2649245e-07
 9.9999928e-01 2.2783615e-12], sum to 1.0000
[2019-04-03 21:57:25,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2461
[2019-04-03 21:57:25,183] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45068155726197, 0.1163091935857354, 0.0, 1.0, 44905.18684656543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896600.0000, 
sim time next is 1897200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.4143822534751, 0.1078136825152412, 0.0, 1.0, 44906.43099290472], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5345318544562584, 0.5359378941717471, 0.0, 1.0, 0.2138401475852606], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.8761958], dtype=float32), -0.78770614]. 
=============================================
[2019-04-03 21:57:37,383] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 54529: loss 0.5048
[2019-04-03 21:57:37,383] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 54529: learning rate 0.0005
[2019-04-03 21:57:37,394] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 54533: loss 0.5233
[2019-04-03 21:57:37,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 54533: learning rate 0.0005
[2019-04-03 21:57:38,280] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 54738: loss 0.5845
[2019-04-03 21:57:38,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 54738: learning rate 0.0005
[2019-04-03 21:57:38,391] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 54759: loss 0.5683
[2019-04-03 21:57:38,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 54759: learning rate 0.0005
[2019-04-03 21:57:40,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8203791e-18 2.2640703e-09 4.8615956e-08 1.1775810e-09 2.3306939e-06
 9.9999762e-01 1.4946625e-12], sum to 1.0000
[2019-04-03 21:57:40,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 21:57:40,996] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.3273155443786, 0.4321740232455049, 0.0, 1.0, 46960.85105736881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2062200.0000, 
sim time next is 2062800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.37493476057239, 0.4347458125985291, 0.0, 1.0, 42443.2345292986], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6145778967143659, 0.6449152708661764, 0.0, 1.0, 0.2021106406157076], 
reward next is 0.7979, 
noisyNet noise sample is [array([-1.1152136], dtype=float32), -0.57238173]. 
=============================================
[2019-04-03 21:57:41,099] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55329: loss 0.7468
[2019-04-03 21:57:41,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55329: learning rate 0.0005
[2019-04-03 21:57:41,961] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55523: loss 0.6869
[2019-04-03 21:57:41,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55523: learning rate 0.0005
[2019-04-03 21:57:44,012] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55967: loss 0.9497
[2019-04-03 21:57:44,014] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55967: learning rate 0.0005
[2019-04-03 21:57:44,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3715253e-20 4.9149701e-10 5.3443699e-07 5.7687771e-08 7.1873746e-06
 9.9999225e-01 1.3005092e-12], sum to 1.0000
[2019-04-03 21:57:44,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2962
[2019-04-03 21:57:44,120] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 26.0, 24.46761407463516, 0.1703216865033971, 0.0, 1.0, 43302.02121605805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 26.0, 24.45800821621918, 0.1579698437008022, 0.0, 1.0, 43376.19038947848], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5381673513515984, 0.552656614566934, 0.0, 1.0, 0.20655328756894514], 
reward next is 0.7934, 
noisyNet noise sample is [array([-1.1088495], dtype=float32), 0.59227264]. 
=============================================
[2019-04-03 21:57:44,301] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56041: loss 1.0368
[2019-04-03 21:57:44,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56041: learning rate 0.0005
[2019-04-03 21:57:44,541] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56114: loss 1.0229
[2019-04-03 21:57:44,557] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56114: learning rate 0.0005
[2019-04-03 21:57:44,945] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56223: loss 1.2643
[2019-04-03 21:57:44,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56226: learning rate 0.0005
[2019-04-03 21:57:45,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56293: loss 1.2782
[2019-04-03 21:57:45,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56293: learning rate 0.0005
[2019-04-03 21:57:46,416] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56613: loss 0.9838
[2019-04-03 21:57:46,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56613: learning rate 0.0005
[2019-04-03 21:57:46,985] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56738: loss 1.1136
[2019-04-03 21:57:46,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56738: learning rate 0.0005
[2019-04-03 21:57:47,289] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56801: loss 1.1972
[2019-04-03 21:57:47,290] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56801: learning rate 0.0005
[2019-04-03 21:57:47,793] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56886: loss 1.1606
[2019-04-03 21:57:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56886: learning rate 0.0005
[2019-04-03 21:57:48,469] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56988: loss 1.2173
[2019-04-03 21:57:48,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56988: learning rate 0.0005
[2019-04-03 21:57:55,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8720508e-20 1.6188451e-10 5.0966509e-08 8.5234159e-10 5.0904042e-08
 1.0000000e+00 1.6507902e-13], sum to 1.0000
[2019-04-03 21:57:55,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3730
[2019-04-03 21:57:55,312] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.30472019758839, 0.1492212143056924, 0.0, 1.0, 42594.85055250865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2166000.0000, 
sim time next is 2166600.0000, 
raw observation next is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.27622089527996, 0.1490203256972489, 0.0, 1.0, 42612.04233808369], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5230184079399965, 0.549673441899083, 0.0, 1.0, 0.20291448732420803], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.6093931], dtype=float32), 1.5226696]. 
=============================================
[2019-04-03 21:58:05,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1445805e-19 1.7385138e-10 1.8762446e-08 5.9314789e-11 1.6548107e-08
 1.0000000e+00 1.9068662e-13], sum to 1.0000
[2019-04-03 21:58:05,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6578
[2019-04-03 21:58:05,172] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.24302040476955, 0.4156225489422677, 0.0, 1.0, 47026.5790330975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.28257068139613, 0.4137176416608617, 0.0, 1.0, 45955.16680279129], 
processed observation next is [1.0, 0.9130434782608695, 0.32132963988919666, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6068808901163442, 0.6379058805536205, 0.0, 1.0, 0.21883412763233948], 
reward next is 0.7812, 
noisyNet noise sample is [array([1.3190421], dtype=float32), -0.04745185]. 
=============================================
[2019-04-03 21:58:17,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1898624e-20 1.2251912e-10 8.7270111e-09 1.0939606e-09 7.0833039e-08
 9.9999988e-01 8.8484382e-14], sum to 1.0000
[2019-04-03 21:58:17,209] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1580
[2019-04-03 21:58:17,282] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999999, 46.66666666666667, 81.33333333333333, 152.3333333333333, 26.0, 24.9896586896549, 0.2918495943636563, 0.0, 1.0, 36720.25447046908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2391000.0000, 
sim time next is 2391600.0000, 
raw observation next is [-0.2, 46.33333333333334, 80.16666666666667, 105.1666666666667, 26.0, 24.96868696804399, 0.2869227872639658, 0.0, 1.0, 47496.06029924707], 
processed observation next is [0.0, 0.6956521739130435, 0.4570637119113574, 0.46333333333333343, 0.26722222222222225, 0.11620626151012894, 0.6666666666666666, 0.5807239140036659, 0.5956409290879886, 0.0, 1.0, 0.22617171571070033], 
reward next is 0.7738, 
noisyNet noise sample is [array([1.5386945], dtype=float32), 0.8195619]. 
=============================================
[2019-04-03 21:58:18,697] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 62501: loss 0.2174
[2019-04-03 21:58:18,698] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 62502: learning rate 0.0005
[2019-04-03 21:58:19,982] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 62770: loss 0.1616
[2019-04-03 21:58:19,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 62770: learning rate 0.0005
[2019-04-03 21:58:20,657] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 62905: loss 0.1741
[2019-04-03 21:58:20,747] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 62922: learning rate 0.0005
[2019-04-03 21:58:20,975] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 62970: loss 0.2073
[2019-04-03 21:58:20,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 62970: learning rate 0.0005
[2019-04-03 21:58:22,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1144214e-23 3.0739885e-12 6.8362804e-10 1.5706455e-11 1.3380346e-09
 1.0000000e+00 1.1963047e-15], sum to 1.0000
[2019-04-03 21:58:22,101] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5764
[2019-04-03 21:58:22,324] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40240332550685, 0.4230556299071875, 0.0, 1.0, 51787.16157976544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.3721759175675, 0.4236471988311393, 0.0, 1.0, 60818.98671828004], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6143479931306249, 0.6412157329437131, 0.0, 1.0, 0.28961422246800017], 
reward next is 0.7104, 
noisyNet noise sample is [array([-1.7938111], dtype=float32), -0.38066885]. 
=============================================
[2019-04-03 21:58:22,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.82168]
 [80.75235]
 [80.71194]
 [81.06478]
 [81.28295]], R is [[81.06459045]
 [81.00733948]
 [81.04356384]
 [81.14379883]
 [81.24301147]].
[2019-04-03 21:58:23,213] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63461: loss 0.1663
[2019-04-03 21:58:23,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63461: learning rate 0.0005
[2019-04-03 21:58:23,804] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63577: loss 0.1320
[2019-04-03 21:58:23,804] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63577: learning rate 0.0005
[2019-04-03 21:58:26,272] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63987: loss 0.1006
[2019-04-03 21:58:26,273] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63987: learning rate 0.0005
[2019-04-03 21:58:27,690] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64219: loss 0.0549
[2019-04-03 21:58:27,693] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64219: learning rate 0.0005
[2019-04-03 21:58:28,827] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64424: loss 0.0020
[2019-04-03 21:58:28,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64424: learning rate 0.0005
[2019-04-03 21:58:29,344] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64506: loss 0.0084
[2019-04-03 21:58:29,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64506: learning rate 0.0005
[2019-04-03 21:58:29,729] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64573: loss 0.0067
[2019-04-03 21:58:29,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64573: learning rate 0.0005
[2019-04-03 21:58:30,180] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64655: loss 0.0004
[2019-04-03 21:58:30,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64655: learning rate 0.0005
[2019-04-03 21:58:31,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64959: loss 0.0131
[2019-04-03 21:58:31,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64959: learning rate 0.0005
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64969: loss 0.0058
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64969: loss 0.0052
[2019-04-03 21:58:31,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64969: learning rate 0.0005
[2019-04-03 21:58:31,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64969: learning rate 0.0005
[2019-04-03 21:58:32,999] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 65189: loss 0.0075
[2019-04-03 21:58:33,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 65195: learning rate 0.0005
[2019-04-03 21:58:37,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3065224e-20 6.8406586e-10 3.2680461e-07 4.1216071e-09 2.5363420e-07
 9.9999940e-01 1.2606595e-12], sum to 1.0000
[2019-04-03 21:58:37,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-03 21:58:38,289] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 26.0, 25.21421835214917, 0.2521129642014204, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2452200.0000, 
sim time next is 2452800.0000, 
raw observation next is [-6.733333333333333, 47.66666666666667, 57.5, 623.5, 26.0, 25.29693728395682, 0.2570157779971627, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2760849492151431, 0.47666666666666674, 0.19166666666666668, 0.6889502762430939, 0.6666666666666666, 0.6080781069964015, 0.5856719259990543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08229618], dtype=float32), 0.009040223]. 
=============================================
[2019-04-03 21:58:43,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.860985e-21 6.469950e-11 3.888488e-08 6.808046e-09 5.972162e-08
 9.999999e-01 9.294492e-14], sum to 1.0000
[2019-04-03 21:58:43,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-03 21:58:43,383] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 26.0, 24.97821870063637, 0.1891552842617701, 0.0, 1.0, 38901.04798505783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2512800.0000, 
sim time next is 2513400.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.95760769203838, 0.1971406201867428, 0.0, 1.0, 38844.48013329619], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5798006410031983, 0.5657135400622476, 0.0, 1.0, 0.18497371492045803], 
reward next is 0.8150, 
noisyNet noise sample is [array([-0.40226266], dtype=float32), 1.3388016]. 
=============================================
[2019-04-03 21:58:58,999] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 70426: loss 0.7972
[2019-04-03 21:58:59,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 70426: learning rate 0.0005
[2019-04-03 21:58:59,563] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 70538: loss 0.4579
[2019-04-03 21:58:59,577] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 70538: learning rate 0.0005
[2019-04-03 21:59:00,352] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 70658: loss 0.4322
[2019-04-03 21:59:00,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 70658: learning rate 0.0005
[2019-04-03 21:59:01,995] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 70940: loss 0.3301
[2019-04-03 21:59:01,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 70940: learning rate 0.0005
[2019-04-03 21:59:03,076] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71125: loss 0.5006
[2019-04-03 21:59:03,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71125: learning rate 0.0005
[2019-04-03 21:59:03,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3107066e-22 5.5139081e-11 3.4648036e-09 1.8495858e-11 8.4370635e-09
 1.0000000e+00 2.1219399e-15], sum to 1.0000
[2019-04-03 21:59:03,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-03 21:59:03,450] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.833333333333334, 64.0, 113.6666666666667, 745.3333333333334, 26.0, 26.07071143929954, 0.4774139342445959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2718600.0000, 
sim time next is 2719200.0000, 
raw observation next is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 26.0, 26.04552730193993, 0.4791306487238304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.22253000923361033, 0.64, 0.376111111111111, 0.8432780847145489, 0.6666666666666666, 0.6704606084949942, 0.6597102162412768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3890837], dtype=float32), -0.17461655]. 
=============================================
[2019-04-03 21:59:05,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71714: loss 0.3781
[2019-04-03 21:59:05,873] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71714: learning rate 0.0005
[2019-04-03 21:59:07,222] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72009: loss 0.1144
[2019-04-03 21:59:07,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72009: learning rate 0.0005
[2019-04-03 21:59:07,465] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72055: loss 0.0533
[2019-04-03 21:59:07,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72055: learning rate 0.0005
[2019-04-03 21:59:07,621] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 72078: loss 0.0624
[2019-04-03 21:59:07,624] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 72078: learning rate 0.0005
[2019-04-03 21:59:08,715] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72290: loss 0.0069
[2019-04-03 21:59:08,749] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72290: learning rate 0.0005
[2019-04-03 21:59:09,333] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72425: loss 0.0348
[2019-04-03 21:59:09,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72425: learning rate 0.0005
[2019-04-03 21:59:10,459] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72647: loss 0.0124
[2019-04-03 21:59:10,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72647: learning rate 0.0005
[2019-04-03 21:59:10,551] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72675: loss 0.0526
[2019-04-03 21:59:10,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72675: learning rate 0.0005
[2019-04-03 21:59:11,102] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72779: loss 0.0698
[2019-04-03 21:59:11,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72779: learning rate 0.0005
[2019-04-03 21:59:12,479] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 73018: loss 0.1600
[2019-04-03 21:59:12,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 73018: learning rate 0.0005
[2019-04-03 21:59:12,563] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 73027: loss 0.2195
[2019-04-03 21:59:12,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 73027: learning rate 0.0005
[2019-04-03 21:59:17,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3332730e-18 7.2916131e-09 1.9913937e-06 1.5990533e-09 2.5837424e-05
 9.9997211e-01 7.3495723e-13], sum to 1.0000
[2019-04-03 21:59:17,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0840
[2019-04-03 21:59:17,348] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 61.5, 0.0, 0.0, 26.0, 25.20103169256716, 0.441489642560388, 0.0, 1.0, 112343.5338559846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2752200.0000, 
sim time next is 2752800.0000, 
raw observation next is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.26604262793023, 0.454393174125205, 0.0, 1.0, 74458.3736824409], 
processed observation next is [1.0, 0.8695652173913043, 0.30563250230840255, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.6055035523275191, 0.6514643913750683, 0.0, 1.0, 0.3545636842020995], 
reward next is 0.6454, 
noisyNet noise sample is [array([-0.5559447], dtype=float32), 2.2298746]. 
=============================================
[2019-04-03 21:59:19,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.12262729e-18 3.28671246e-09 3.77461911e-05 7.84255718e-08
 4.93669359e-05 9.99912858e-01 1.05081195e-11], sum to 1.0000
[2019-04-03 21:59:19,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1928
[2019-04-03 21:59:19,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1434050e-17 4.6850936e-09 2.1669462e-06 4.7281752e-09 2.7970227e-06
 9.9999511e-01 4.4500133e-12], sum to 1.0000
[2019-04-03 21:59:19,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5907
[2019-04-03 21:59:19,104] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.32543854232649, 0.1143977375789343, 0.0, 1.0, 41105.57512124209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2783400.0000, 
sim time next is 2784000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.31845036407891, 0.111353418757714, 0.0, 1.0, 41157.98634539922], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5265375303399091, 0.5371178062525713, 0.0, 1.0, 0.19599041116856772], 
reward next is 0.8040, 
noisyNet noise sample is [array([-0.17025498], dtype=float32), -1.7094152]. 
=============================================
[2019-04-03 21:59:19,161] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.00000000000001, 0.0, 0.0, 26.0, 25.02078361069814, 0.3171562581787732, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2837400.0000, 
sim time next is 2838000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.94933142044419, 0.3173390701900599, 0.0, 1.0, 198420.1055810666], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5791109517036824, 0.6057796900633533, 0.0, 1.0, 0.9448576456241267], 
reward next is 0.0551, 
noisyNet noise sample is [array([-0.40411085], dtype=float32), -1.5498414]. 
=============================================
[2019-04-03 21:59:19,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[69.593994]
 [69.70538 ]
 [69.81824 ]
 [69.90932 ]
 [69.983894]], R is [[69.5994873 ]
 [69.70775604]
 [69.81534576]
 [69.9223175 ]
 [70.02858734]].
[2019-04-03 21:59:19,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.318245]
 [66.03692 ]
 [66.82341 ]
 [68.40486 ]
 [69.73078 ]], R is [[65.38555145]
 [65.73169708]
 [66.07437897]
 [66.41363525]
 [66.74949646]].
[2019-04-03 21:59:25,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2038281e-19 2.1482165e-09 7.6690770e-07 7.8983570e-10 7.0223659e-06
 9.9999225e-01 6.4139978e-14], sum to 1.0000
[2019-04-03 21:59:25,229] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4895
[2019-04-03 21:59:25,245] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.866666666666667, 24.33333333333333, 98.83333333333333, 0.0, 26.0, 25.46238541603329, 0.3684546280589625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2820000.0000, 
sim time next is 2820600.0000, 
raw observation next is [6.8, 24.5, 95.0, 0.0, 26.0, 25.66029375194089, 0.3819403165734907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6509695290858727, 0.245, 0.31666666666666665, 0.0, 0.6666666666666666, 0.638357812661741, 0.6273134388578302, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6286335], dtype=float32), 0.4093768]. 
=============================================
[2019-04-03 21:59:26,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4480696e-21 7.4380502e-11 8.0385362e-08 9.8941182e-11 2.2401444e-07
 9.9999964e-01 5.9223642e-15], sum to 1.0000
[2019-04-03 21:59:26,775] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6918
[2019-04-03 21:59:26,796] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.8075375082347, 0.3045047193670034, 1.0, 1.0, 177996.0085079917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09708216911681, 0.3468933364565815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 0.6666666666666666, 0.5914235140930675, 0.6156311121521938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.491964], dtype=float32), -0.10566163]. 
=============================================
[2019-04-03 21:59:28,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.79592599e-21 2.08051160e-10 1.14003626e-06 4.39833142e-10
 1.06938344e-07 9.99998689e-01 6.72556154e-14], sum to 1.0000
[2019-04-03 21:59:28,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-03 21:59:28,474] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.94182164581336, 0.3770510129656441, 0.0, 1.0, 113291.3175873807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2839200.0000, 
sim time next is 2839800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.09097901432423, 0.3975566317161953, 0.0, 1.0, 71025.6547611692], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5909149178603524, 0.6325188772387318, 0.0, 1.0, 0.3382174036246152], 
reward next is 0.6618, 
noisyNet noise sample is [array([-1.3159634], dtype=float32), -0.48610723]. 
=============================================
[2019-04-03 21:59:34,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1543833e-19 4.8676996e-10 1.1349547e-04 4.8387722e-10 2.6193868e-06
 9.9988389e-01 1.6830028e-13], sum to 1.0000
[2019-04-03 21:59:34,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-03 21:59:35,002] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.5, 78.0, 0.0, 26.0, 25.42653144950308, 0.3102101797759229, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889000.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.42665634380329, 0.313184858875852, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.6666666666666666, 0.6188880286502743, 0.6043949529586173, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.62397987], dtype=float32), -1.7549498]. 
=============================================
[2019-04-03 21:59:37,040] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 78689: loss 0.5647
[2019-04-03 21:59:37,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 78689: learning rate 0.0005
[2019-04-03 21:59:37,547] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 78779: loss 0.6371
[2019-04-03 21:59:37,549] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 78780: learning rate 0.0005
[2019-04-03 21:59:37,883] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 78843: loss 0.5307
[2019-04-03 21:59:37,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 78843: learning rate 0.0005
[2019-04-03 21:59:39,087] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79080: loss 0.5820
[2019-04-03 21:59:39,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79080: learning rate 0.0005
[2019-04-03 21:59:40,819] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79427: loss 0.8343
[2019-04-03 21:59:40,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79427: learning rate 0.0005
[2019-04-03 21:59:42,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79859: loss 0.7041
[2019-04-03 21:59:42,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79859: learning rate 0.0005
[2019-04-03 21:59:43,701] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80045: loss 0.8095
[2019-04-03 21:59:43,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80045: learning rate 0.0005
[2019-04-03 21:59:44,007] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80070: loss 1.0196
[2019-04-03 21:59:44,012] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80070: learning rate 0.0005
[2019-04-03 21:59:44,525] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 80128: loss 0.9652
[2019-04-03 21:59:44,525] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 80128: learning rate 0.0005
[2019-04-03 21:59:45,716] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80332: loss 1.1091
[2019-04-03 21:59:45,716] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80332: learning rate 0.0005
[2019-04-03 21:59:46,840] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80519: loss 1.7574
[2019-04-03 21:59:46,843] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80519: learning rate 0.0005
[2019-04-03 21:59:48,568] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80890: loss 1.2951
[2019-04-03 21:59:48,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80890: learning rate 0.0005
[2019-04-03 21:59:49,006] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80948: loss 1.2778
[2019-04-03 21:59:49,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80948: learning rate 0.0005
[2019-04-03 21:59:49,392] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80994: loss 1.2983
[2019-04-03 21:59:49,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80994: learning rate 0.0005
[2019-04-03 21:59:49,585] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 81007: loss 1.2568
[2019-04-03 21:59:49,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 81007: learning rate 0.0005
[2019-04-03 21:59:50,017] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 81064: loss 0.8593
[2019-04-03 21:59:50,017] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 81064: learning rate 0.0005
[2019-04-03 21:59:53,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8761320e-20 7.1098474e-09 1.3801590e-05 1.2554606e-09 9.0305798e-07
 9.9998522e-01 6.1221541e-13], sum to 1.0000
[2019-04-03 21:59:53,765] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1128
[2019-04-03 21:59:53,825] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 40.0, 96.5, 758.0, 26.0, 25.11646513623975, 0.3630527174207967, 0.0, 1.0, 18703.9667681089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3076800.0000, 
sim time next is 3077400.0000, 
raw observation next is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.11740121887014, 0.3638207082200542, 0.0, 1.0, 18702.78602316953], 
processed observation next is [0.0, 0.6086956521739131, 0.4579870729455217, 0.395, 0.31333333333333335, 0.8187845303867404, 0.6666666666666666, 0.5931167682391782, 0.6212735694066848, 0.0, 1.0, 0.08906088582461681], 
reward next is 0.9109, 
noisyNet noise sample is [array([-2.4083679], dtype=float32), 0.27679676]. 
=============================================
[2019-04-03 22:00:09,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7847590e-18 1.3680005e-07 1.6754062e-05 1.0077200e-08 2.0953426e-05
 9.9996209e-01 1.4714683e-12], sum to 1.0000
[2019-04-03 22:00:09,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4825
[2019-04-03 22:00:09,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 26.59048569934307, 0.7463084053744781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.53444377149907, 0.621362246945949, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 0.6666666666666666, 0.7112036476249225, 0.707120748981983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07524352], dtype=float32), 0.89579415]. 
=============================================
[2019-04-03 22:00:09,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.2088  ]
 [73.30702 ]
 [73.4276  ]
 [73.56279 ]
 [73.719574]], R is [[73.32977295]
 [73.59647369]
 [73.86051178]
 [74.1219101 ]
 [74.38069153]].
[2019-04-03 22:00:10,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 86355: loss 0.0091
[2019-04-03 22:00:10,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 86355: learning rate 0.0005
[2019-04-03 22:00:10,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5884167e-19 4.5636619e-09 3.0187119e-04 7.9093709e-09 7.8880316e-07
 9.9969733e-01 1.3294451e-12], sum to 1.0000
[2019-04-03 22:00:10,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7705
[2019-04-03 22:00:10,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.54491094203361, 0.5940747115436943, 0.0, 1.0, 41280.66613665034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3196800.0000, 
sim time next is 3197400.0000, 
raw observation next is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 26.0, 25.50400213273945, 0.5894911390414177, 0.0, 1.0, 58319.58770387038], 
processed observation next is [1.0, 0.0, 0.5133887349953832, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6253335110616209, 0.6964970463471393, 0.0, 1.0, 0.27771232239938276], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.27666453], dtype=float32), 0.28490734]. 
=============================================
[2019-04-03 22:00:11,136] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 86555: loss -0.1472
[2019-04-03 22:00:11,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 86555: learning rate 0.0005
[2019-04-03 22:00:12,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 86844: loss -0.0020
[2019-04-03 22:00:12,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 86845: learning rate 0.0005
[2019-04-03 22:00:12,640] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 86896: loss 0.0365
[2019-04-03 22:00:12,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 86896: learning rate 0.0005
[2019-04-03 22:00:14,763] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87361: loss 0.0262
[2019-04-03 22:00:14,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87361: learning rate 0.0005
[2019-04-03 22:00:15,086] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87460: loss -0.0561
[2019-04-03 22:00:15,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87460: learning rate 0.0005
[2019-04-03 22:00:15,938] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 87716: loss -0.0007
[2019-04-03 22:00:15,955] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 87716: learning rate 0.0005
[2019-04-03 22:00:16,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87828: loss 0.0054
[2019-04-03 22:00:16,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87828: learning rate 0.0005
[2019-04-03 22:00:17,634] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88151: loss 0.2312
[2019-04-03 22:00:17,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88154: learning rate 0.0005
[2019-04-03 22:00:18,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88445: loss -0.8786
[2019-04-03 22:00:18,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88445: learning rate 0.0005
[2019-04-03 22:00:19,052] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88502: loss -0.0943
[2019-04-03 22:00:19,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88502: learning rate 0.0005
[2019-04-03 22:00:20,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88899: loss 0.0177
[2019-04-03 22:00:20,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88899: learning rate 0.0005
[2019-04-03 22:00:21,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 89018: loss 0.0065
[2019-04-03 22:00:21,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 89018: learning rate 0.0005
[2019-04-03 22:00:21,050] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 89019: loss 0.0255
[2019-04-03 22:00:21,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 89019: learning rate 0.0005
[2019-04-03 22:00:21,104] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 89046: loss 0.0841
[2019-04-03 22:00:21,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 89046: learning rate 0.0005
[2019-04-03 22:00:21,185] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 89079: loss 0.0425
[2019-04-03 22:00:21,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 89099: learning rate 0.0005
[2019-04-03 22:00:27,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8815663e-20 6.6182984e-09 9.5694895e-06 1.1970865e-09 1.1599441e-07
 9.9999034e-01 3.9787246e-13], sum to 1.0000
[2019-04-03 22:00:27,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9010
[2019-04-03 22:00:27,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5099978071045, 0.5363975356470367, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5937996787222, 0.5329860277394621, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6328166398935166, 0.6776620092464873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2852211], dtype=float32), -0.047753602]. 
=============================================
[2019-04-03 22:00:39,033] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 94269: loss 0.0316
[2019-04-03 22:00:39,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 94269: learning rate 0.0005
[2019-04-03 22:00:39,883] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 94561: loss 0.0162
[2019-04-03 22:00:39,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 94561: learning rate 0.0005
[2019-04-03 22:00:40,628] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 94806: loss 0.0162
[2019-04-03 22:00:40,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 94806: learning rate 0.0005
[2019-04-03 22:00:40,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2931717e-17 1.4990064e-08 5.1713143e-02 2.0439644e-05 2.2940552e-05
 9.4824344e-01 1.5126388e-10], sum to 1.0000
[2019-04-03 22:00:40,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-03 22:00:41,106] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.95228934502558, 0.3382701830010058, 0.0, 1.0, 41003.08071104908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.94728333469578, 0.3311399763821865, 0.0, 1.0, 40948.34480770417], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.578940277891315, 0.6103799921273955, 0.0, 1.0, 0.1949921181319246], 
reward next is 0.8050, 
noisyNet noise sample is [array([1.2590172], dtype=float32), -0.007872718]. 
=============================================
[2019-04-03 22:00:41,780] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95166: loss -0.0114
[2019-04-03 22:00:41,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95167: learning rate 0.0005
[2019-04-03 22:00:42,063] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95256: loss -0.5059
[2019-04-03 22:00:42,064] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95256: learning rate 0.0005
[2019-04-03 22:00:43,027] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95556: loss 0.0124
[2019-04-03 22:00:43,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95559: learning rate 0.0005
[2019-04-03 22:00:43,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3674336e-17 1.0425049e-06 5.7745539e-03 7.6517435e-06 3.2941455e-06
 9.9421340e-01 5.7912408e-11], sum to 1.0000
[2019-04-03 22:00:43,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-03 22:00:43,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.24407800834071, 0.4343784704363613, 0.0, 1.0, 18709.55539026789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.1866242110592, 0.4319047297631146, 0.0, 1.0, 33688.4761138659], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.6666666666666666, 0.5988853509216, 0.6439682432543715, 0.0, 1.0, 0.16042131482793287], 
reward next is 0.8396, 
noisyNet noise sample is [array([-0.8127595], dtype=float32), 0.48175284]. 
=============================================
[2019-04-03 22:00:43,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5586366e-17 1.2882740e-06 2.2553323e-02 2.6749264e-04 9.7763550e-05
 9.7708011e-01 5.1113702e-10], sum to 1.0000
[2019-04-03 22:00:43,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2577
[2019-04-03 22:00:43,519] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.833333333333332, 27.33333333333333, 0.0, 0.0, 26.0, 25.47495353517648, 0.3644652545696934, 0.0, 1.0, 22431.68340246523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3646200.0000, 
sim time next is 3646800.0000, 
raw observation next is [9.0, 27.0, 0.0, 0.0, 26.0, 25.51528887004728, 0.3639663083015345, 0.0, 1.0, 18751.02241748978], 
processed observation next is [0.0, 0.21739130434782608, 0.7119113573407203, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6262740725039398, 0.6213221027671781, 0.0, 1.0, 0.08929058294042753], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.2678631], dtype=float32), -0.5141407]. 
=============================================
[2019-04-03 22:00:44,242] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 95958: loss 0.1125
[2019-04-03 22:00:44,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 95960: learning rate 0.0005
[2019-04-03 22:00:44,701] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96103: loss 0.0403
[2019-04-03 22:00:44,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96103: learning rate 0.0005
[2019-04-03 22:00:44,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96153: loss 0.0109
[2019-04-03 22:00:44,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96154: learning rate 0.0005
[2019-04-03 22:00:45,545] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96389: loss 0.0012
[2019-04-03 22:00:45,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96389: learning rate 0.0005
[2019-04-03 22:00:45,963] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 96506: loss 0.0024
[2019-04-03 22:00:45,965] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 96506: learning rate 0.0005
[2019-04-03 22:00:47,684] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96999: loss 0.0035
[2019-04-03 22:00:47,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 97000: learning rate 0.0005
[2019-04-03 22:00:48,265] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97201: loss 0.0056
[2019-04-03 22:00:48,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97202: learning rate 0.0005
[2019-04-03 22:00:48,597] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97324: loss -0.0005
[2019-04-03 22:00:48,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97324: learning rate 0.0005
[2019-04-03 22:00:48,714] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 97362: loss 0.0034
[2019-04-03 22:00:48,716] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 97362: learning rate 0.0005
[2019-04-03 22:00:49,299] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 97539: loss 0.0074
[2019-04-03 22:00:49,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 97539: learning rate 0.0005
[2019-04-03 22:00:51,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8509976e-19 3.9554942e-07 9.1921883e-03 1.1370680e-05 7.4393024e-06
 9.9078864e-01 7.1337880e-11], sum to 1.0000
[2019-04-03 22:00:51,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4652
[2019-04-03 22:00:51,418] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.49825325382211, 0.3709744615595101, 0.0, 1.0, 27958.96205084966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3635400.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50871244719762, 0.3698028217957272, 0.0, 1.0, 23400.98856363521], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6257260372664684, 0.6232676072652424, 0.0, 1.0, 0.11143327887445337], 
reward next is 0.8886, 
noisyNet noise sample is [array([-0.37977505], dtype=float32), -1.5174121]. 
=============================================
[2019-04-03 22:00:51,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.462036]
 [89.46572 ]
 [89.33744 ]
 [88.95572 ]
 [88.47038 ]], R is [[89.39442444]
 [89.36734009]
 [89.29366302]
 [89.16086578]
 [88.98986816]].
[2019-04-03 22:00:56,780] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-03 22:00:56,781] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:00:56,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,781] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:00:56,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:00:56,784] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:00:56,802] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,807] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-03 22:00:56,808] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:00:56,841] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.133605770666668, 66.05329349333334, 0.0, 0.0, 26.0, 22.78152247362507, -0.2344497773705189, 0.0, 1.0, 47366.67109174636]
[2019-04-03 22:01:17,701] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:01:17,702] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.2313886e-16 2.2168706e-07 7.4986013e-04 6.0207600e-05 6.1458736e-06
 9.9918348e-01 8.2894741e-10], sampled 0.20616447260962367
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.9, 88.33333333333334, 0.0, 0.0, 26.0, 25.44363675698062, 0.5058181522224473, 0.0, 1.0, 52058.56017621368]
[2019-04-03 22:01:58,651] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:01:58,652] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.12268305e-20 1.34928446e-09 2.11054685e-05 4.22177123e-07
 5.46203651e-08 9.99978423e-01 3.75501361e-13], sampled 0.04769601166774706
[2019-04-03 22:02:55,798] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5638 239911596.9837 1605.3029
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.737735662, 65.76812495666667, 0.0, 0.0, 26.0, 25.11012478139116, 0.3269459359285679, 0.0, 1.0, 39754.27929915791]
[2019-04-03 22:02:57,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:02:57,311] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8514263e-18 1.6635429e-08 1.4463524e-04 8.6777327e-06 6.0772129e-07
 9.9984610e-01 1.4989987e-11], sampled 0.8826365902296978
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23437133], dtype=float32), 0.16893505]
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 40.0, 0.0, 0.0, 26.0, 25.55733216885408, 0.4395876615075293, 0.0, 1.0, 37456.04408780335]
[2019-04-03 22:03:17,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:03:17,762] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.4793665e-18 2.2403832e-08 6.9604350e-05 3.1213372e-06 8.4121564e-07
 9.9992645e-01 1.1649215e-11], sampled 0.843924806155416
[2019-04-03 22:03:19,103] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8201 263377775.6296 1552.0399
[2019-04-03 22:03:20,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6735 275798561.6764 1233.0993
[2019-04-03 22:03:21,372] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 100000, evaluation results [100000.0, 7241.820116049566, 263377775.62958866, 1552.0399168798824, 7353.563823886941, 239911596.98373976, 1605.3028739965662, 7182.673515826508, 275798561.67643094, 1233.0993326628943]
[2019-04-03 22:03:21,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2540924e-19 1.3769066e-09 1.6308206e-05 2.0607149e-07 1.8139940e-06
 9.9998164e-01 2.0834087e-12], sum to 1.0000
[2019-04-03 22:03:21,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2285
[2019-04-03 22:03:21,859] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45771792892574, 0.4581543926351443, 0.0, 1.0, 40031.39358479137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.39941856820069, 0.4507390249385974, 0.0, 1.0, 68850.0281906832], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6166182140167242, 0.6502463416461991, 0.0, 1.0, 0.3278572770984914], 
reward next is 0.6721, 
noisyNet noise sample is [array([0.447345], dtype=float32), 0.883619]. 
=============================================
[2019-04-03 22:03:25,850] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102282: loss 1.0072
[2019-04-03 22:03:25,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102282: learning rate 0.0005
[2019-04-03 22:03:25,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 102328: loss 1.0131
[2019-04-03 22:03:25,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 102328: learning rate 0.0005
[2019-04-03 22:03:26,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 102525: loss 0.7514
[2019-04-03 22:03:26,348] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 102525: learning rate 0.0005
[2019-04-03 22:03:27,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 102866: loss 0.4378
[2019-04-03 22:03:27,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 102867: learning rate 0.0005
[2019-04-03 22:03:27,598] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103097: loss 0.2209
[2019-04-03 22:03:27,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103097: learning rate 0.0005
[2019-04-03 22:03:27,840] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103217: loss 0.3008
[2019-04-03 22:03:27,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103220: learning rate 0.0005
[2019-04-03 22:03:29,177] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 103822: loss 0.2964
[2019-04-03 22:03:29,178] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 103822: learning rate 0.0005
[2019-04-03 22:03:29,452] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103961: loss 0.1444
[2019-04-03 22:03:29,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103961: learning rate 0.0005
[2019-04-03 22:03:29,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104138: loss 0.1026
[2019-04-03 22:03:29,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104138: learning rate 0.0005
[2019-04-03 22:03:30,222] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 104337: loss 0.1000
[2019-04-03 22:03:30,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 104338: learning rate 0.0005
[2019-04-03 22:03:30,447] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104448: loss 0.0786
[2019-04-03 22:03:30,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104449: learning rate 0.0005
[2019-04-03 22:03:30,980] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104691: loss 0.0291
[2019-04-03 22:03:30,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104691: learning rate 0.0005
[2019-04-03 22:03:31,422] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104893: loss 0.0379
[2019-04-03 22:03:31,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104893: learning rate 0.0005
[2019-04-03 22:03:31,445] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104904: loss 0.0263
[2019-04-03 22:03:31,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104906: learning rate 0.0005
[2019-04-03 22:03:31,674] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 105004: loss 0.0616
[2019-04-03 22:03:31,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 105004: learning rate 0.0005
[2019-04-03 22:03:32,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 105321: loss 0.0633
[2019-04-03 22:03:32,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 105321: learning rate 0.0005
[2019-04-03 22:03:32,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1881791e-19 9.7764907e-10 5.1184156e-06 3.1433765e-07 4.5929863e-08
 9.9999452e-01 3.8159035e-13], sum to 1.0000
[2019-04-03 22:03:32,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1152
[2019-04-03 22:03:33,007] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.41705053019068, 0.5311809055695283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [-10.0, 48.5, 101.0, 698.0, 26.0, 26.47915302306455, 0.5431928216890557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.18559556786703602, 0.485, 0.33666666666666667, 0.7712707182320442, 0.6666666666666666, 0.7065960852553793, 0.6810642738963519, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3020251], dtype=float32), 0.2796537]. 
=============================================
[2019-04-03 22:03:40,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0131891e-21 4.6758103e-10 4.8534639e-07 3.4890757e-08 4.5916643e-10
 9.9999952e-01 8.9614807e-15], sum to 1.0000
[2019-04-03 22:03:40,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3451
[2019-04-03 22:03:40,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.64135761758542, 0.6757873138430988, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.70846346888795, 0.6851850542329982, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.41181902123730385, 0.2033333333333334, 0.31333333333333335, 0.8169429097605894, 0.6666666666666666, 0.7257052890739958, 0.728395018077666, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8355649], dtype=float32), -0.65646803]. 
=============================================
[2019-04-03 22:03:40,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.3758 ]
 [84.52496]
 [84.6224 ]
 [84.78403]
 [84.92206]], R is [[84.34172058]
 [84.49830627]
 [84.65332031]
 [84.80678558]
 [84.95871735]].
[2019-04-03 22:03:44,600] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110216: loss 4.7388
[2019-04-03 22:03:44,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110217: learning rate 0.0005
[2019-04-03 22:03:44,710] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 110268: loss 5.0367
[2019-04-03 22:03:44,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 110268: learning rate 0.0005
[2019-04-03 22:03:45,232] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 110447: loss 5.1873
[2019-04-03 22:03:45,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 110449: learning rate 0.0005
[2019-04-03 22:03:46,906] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111045: loss 7.4308
[2019-04-03 22:03:46,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111048: learning rate 0.0005
[2019-04-03 22:03:47,756] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111326: loss 7.3951
[2019-04-03 22:03:47,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111327: learning rate 0.0005
[2019-04-03 22:03:48,629] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111649: loss 6.1655
[2019-04-03 22:03:48,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111649: learning rate 0.0005
[2019-04-03 22:03:49,916] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 112038: loss 6.4707
[2019-04-03 22:03:49,921] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 112039: learning rate 0.0005
[2019-04-03 22:03:50,165] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112104: loss 8.3464
[2019-04-03 22:03:50,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112104: learning rate 0.0005
[2019-04-03 22:03:50,488] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112202: loss 7.1430
[2019-04-03 22:03:50,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112202: learning rate 0.0005
[2019-04-03 22:03:51,248] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112478: loss 6.3414
[2019-04-03 22:03:51,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112478: learning rate 0.0005
[2019-04-03 22:03:51,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112598: loss 7.3814
[2019-04-03 22:03:51,646] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112598: learning rate 0.0005
[2019-04-03 22:03:52,593] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112918: loss 5.9029
[2019-04-03 22:03:52,594] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112918: learning rate 0.0005
[2019-04-03 22:03:52,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7069705e-19 6.4359579e-10 2.3688777e-05 1.4766194e-03 3.1559498e-08
 9.9849975e-01 3.1502361e-12], sum to 1.0000
[2019-04-03 22:03:52,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6389
[2019-04-03 22:03:52,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.21623109008554, 0.3866894477922274, 0.0, 1.0, 60250.94501150013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4218600.0000, 
sim time next is 4219200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.33347756506318, 0.407018178689407, 0.0, 1.0, 46677.25638396776], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6111231304219317, 0.6356727262298023, 0.0, 1.0, 0.22227264944746553], 
reward next is 0.7777, 
noisyNet noise sample is [array([-1.3301735], dtype=float32), -1.5834994]. 
=============================================
[2019-04-03 22:03:53,350] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113186: loss 7.5858
[2019-04-03 22:03:53,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113186: learning rate 0.0005
[2019-04-03 22:03:53,479] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113235: loss 7.2382
[2019-04-03 22:03:53,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113235: learning rate 0.0005
[2019-04-03 22:03:53,921] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 113376: loss 7.1628
[2019-04-03 22:03:53,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 113376: learning rate 0.0005
[2019-04-03 22:03:55,027] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 113709: loss 5.2627
[2019-04-03 22:03:55,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 113709: learning rate 0.0005
[2019-04-03 22:03:57,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2990108e-21 9.7748543e-10 1.6467036e-05 2.1558462e-04 1.3442565e-09
 9.9976796e-01 1.5011459e-13], sum to 1.0000
[2019-04-03 22:03:57,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-03 22:03:57,678] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 26.0, 25.5057640817145, 0.4144368531666691, 0.0, 1.0, 62519.91348185801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 26.0, 25.49983686827767, 0.4176000178538983, 0.0, 1.0, 48105.00621079143], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6249864056898057, 0.6392000059512994, 0.0, 1.0, 0.22907145814662586], 
reward next is 0.7709, 
noisyNet noise sample is [array([-0.34627175], dtype=float32), 0.40890238]. 
=============================================
[2019-04-03 22:04:04,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3891019e-22 2.6916449e-09 4.3992245e-07 4.4642920e-06 1.9303248e-09
 9.9999511e-01 5.7164826e-15], sum to 1.0000
[2019-04-03 22:04:04,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-03 22:04:04,645] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.50199900973207, 0.6428191096252512, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.47499823988807, 0.6339084623803816, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7062498533240058, 0.7113028207934605, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2957335], dtype=float32), -0.520198]. 
=============================================
[2019-04-03 22:04:04,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.38771863e-21 2.15758567e-09 4.07548043e-07 4.50529427e-07
 1.37414515e-08 9.99999166e-01 3.14952227e-14], sum to 1.0000
[2019-04-03 22:04:04,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-03 22:04:04,952] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.5270171482848, 0.6507608175596741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.49766434310361, 0.6415855174691892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7081386952586343, 0.7138618391563964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5225362], dtype=float32), -0.13326964]. 
=============================================
[2019-04-03 22:04:08,374] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 117999: loss 0.1592
[2019-04-03 22:04:08,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 118000: learning rate 0.0005
[2019-04-03 22:04:09,452] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118342: loss 0.2667
[2019-04-03 22:04:09,454] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118342: learning rate 0.0005
[2019-04-03 22:04:09,686] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 118436: loss 0.3600
[2019-04-03 22:04:09,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 118439: learning rate 0.0005
[2019-04-03 22:04:11,877] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119115: loss 0.3316
[2019-04-03 22:04:11,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119115: learning rate 0.0005
[2019-04-03 22:04:11,964] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119144: loss 0.3354
[2019-04-03 22:04:11,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119144: learning rate 0.0005
[2019-04-03 22:04:14,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119810: loss 0.0052
[2019-04-03 22:04:14,060] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119812: learning rate 0.0005
[2019-04-03 22:04:14,714] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120050: loss 0.0180
[2019-04-03 22:04:14,715] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120050: learning rate 0.0005
[2019-04-03 22:04:15,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120155: loss 0.0111
[2019-04-03 22:04:15,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120155: learning rate 0.0005
[2019-04-03 22:04:15,109] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 120185: loss 0.2859
[2019-04-03 22:04:15,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 120185: learning rate 0.0005
[2019-04-03 22:04:15,480] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120307: loss 0.0033
[2019-04-03 22:04:15,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120307: learning rate 0.0005
[2019-04-03 22:04:15,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120309: loss 0.0081
[2019-04-03 22:04:15,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120311: learning rate 0.0005
[2019-04-03 22:04:15,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7406250e-21 4.9468246e-10 2.3107084e-05 3.6563090e-06 4.5585349e-09
 9.9997318e-01 7.9569787e-14], sum to 1.0000
[2019-04-03 22:04:15,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1111
[2019-04-03 22:04:15,903] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 26.0, 26.33292630173455, 0.6198442537408139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4545600.0000, 
sim time next is 4546200.0000, 
raw observation next is [3.0, 45.66666666666666, 227.0, 79.33333333333334, 26.0, 26.41954360645968, 0.5198971660566319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.45666666666666655, 0.7566666666666667, 0.08766114180478822, 0.6666666666666666, 0.70162863387164, 0.6732990553522106, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4361941], dtype=float32), 0.72280043]. 
=============================================
[2019-04-03 22:04:17,237] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120826: loss 0.0078
[2019-04-03 22:04:17,238] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120826: learning rate 0.0005
[2019-04-03 22:04:17,713] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120946: loss -0.1141
[2019-04-03 22:04:17,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120946: learning rate 0.0005
[2019-04-03 22:04:19,023] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121337: loss 0.0581
[2019-04-03 22:04:19,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121337: learning rate 0.0005
[2019-04-03 22:04:19,144] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 121387: loss 0.0658
[2019-04-03 22:04:19,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 121389: learning rate 0.0005
[2019-04-03 22:04:19,584] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 121539: loss 0.0158
[2019-04-03 22:04:19,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 121540: learning rate 0.0005
[2019-04-03 22:04:21,495] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7665301e-21 4.6129270e-07 1.7326140e-04 6.6253692e-03 5.8579595e-08
 9.9320096e-01 2.1567198e-13], sum to 1.0000
[2019-04-03 22:04:21,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9587
[2019-04-03 22:04:21,543] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24522543855575, 0.5852027362027761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4614000.0000, 
sim time next is 4614600.0000, 
raw observation next is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 26.0, 26.36582308364526, 0.6098804684347624, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6183333333333333, 0.5077777777777777, 0.6574585635359116, 0.6666666666666666, 0.697151923637105, 0.7032934894782542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9408237], dtype=float32), -1.513208]. 
=============================================
[2019-04-03 22:04:33,542] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 126394: loss 0.1254
[2019-04-03 22:04:33,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 126394: learning rate 0.0005
[2019-04-03 22:04:34,057] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126553: loss 0.1236
[2019-04-03 22:04:34,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126553: learning rate 0.0005
[2019-04-03 22:04:34,354] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 126658: loss -1.0918
[2019-04-03 22:04:34,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 126658: learning rate 0.0005
[2019-04-03 22:04:35,672] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127159: loss 0.0756
[2019-04-03 22:04:35,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127159: learning rate 0.0005
[2019-04-03 22:04:35,941] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127244: loss 0.1082
[2019-04-03 22:04:35,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127244: learning rate 0.0005
[2019-04-03 22:04:37,746] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127840: loss 0.0267
[2019-04-03 22:04:37,747] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127840: learning rate 0.0005
[2019-04-03 22:04:38,563] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128120: loss 0.0167
[2019-04-03 22:04:38,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128120: learning rate 0.0005
[2019-04-03 22:04:38,619] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128142: loss -0.0664
[2019-04-03 22:04:38,620] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128142: learning rate 0.0005
[2019-04-03 22:04:38,643] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128151: loss 0.0570
[2019-04-03 22:04:38,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128153: learning rate 0.0005
[2019-04-03 22:04:38,756] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 128198: loss 0.0374
[2019-04-03 22:04:38,756] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 128198: learning rate 0.0005
[2019-04-03 22:04:40,071] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128616: loss 0.0147
[2019-04-03 22:04:40,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128616: learning rate 0.0005
[2019-04-03 22:04:41,683] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129076: loss 0.0668
[2019-04-03 22:04:41,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129076: learning rate 0.0005
[2019-04-03 22:04:42,063] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 129189: loss 0.0952
[2019-04-03 22:04:42,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 129189: learning rate 0.0005
[2019-04-03 22:04:42,464] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129321: loss 0.0187
[2019-04-03 22:04:42,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129321: learning rate 0.0005
[2019-04-03 22:04:43,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 129583: loss 0.0172
[2019-04-03 22:04:43,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 129583: learning rate 0.0005
[2019-04-03 22:04:43,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 129611: loss 0.0573
[2019-04-03 22:04:43,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 129611: learning rate 0.0005
[2019-04-03 22:04:46,436] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2817198e-21 4.4723718e-09 1.1960034e-07 3.1723728e-05 1.9806587e-09
 9.9996817e-01 5.5509518e-14], sum to 1.0000
[2019-04-03 22:04:46,437] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5607
[2019-04-03 22:04:46,458] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09399801558412, 0.3656426637357439, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08128743586594, 0.365187305304503, 0.0, 1.0, 18689.71272695318], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5901072863221616, 0.6217291017681676, 0.0, 1.0, 0.08899863203311038], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.01116925], dtype=float32), -1.5936224]. 
=============================================
[2019-04-03 22:04:46,486] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.66283 ]
 [87.77362 ]
 [87.90889 ]
 [88.01283 ]
 [88.079895]], R is [[87.72002411]
 [87.84282684]
 [87.96440125]
 [88.08475494]
 [88.20391083]].
[2019-04-03 22:04:53,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.00837222e-24 1.03935636e-11 3.54724357e-08 1.27594685e-05
 1.51099244e-10 9.99987245e-01 3.32635053e-16], sum to 1.0000
[2019-04-03 22:04:53,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5390
[2019-04-03 22:04:53,711] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.5, 25.5, 72.0, 641.0, 26.0, 28.014928885422, 0.8217866186817794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4984200.0000, 
sim time next is 4984800.0000, 
raw observation next is [8.333333333333334, 25.66666666666666, 65.66666666666667, 584.8333333333334, 26.0, 27.42774709324719, 0.8676958094481827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6934441366574331, 0.2566666666666666, 0.2188888888888889, 0.6462246777163905, 0.6666666666666666, 0.7856455911039326, 0.7892319364827275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4594525], dtype=float32), -1.2565961]. 
=============================================
[2019-04-03 22:04:57,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-03 22:04:57,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-03 22:04:57,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:57,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:57,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-03 22:04:58,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3805837e-22 1.2967388e-08 6.5887880e-06 5.3814685e-01 8.9277101e-09
 4.6184656e-01 3.0907113e-13], sum to 1.0000
[2019-04-03 22:04:58,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3471
[2019-04-03 22:04:58,630] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 19.0, 101.0, 769.6666666666667, 26.0, 28.64061787051327, 1.126175329473696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5065800.0000, 
sim time next is 5066400.0000, 
raw observation next is [12.0, 19.0, 98.5, 757.3333333333334, 26.0, 28.70039509672981, 1.020146829304957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.3283333333333333, 0.8368324125230203, 0.6666666666666666, 0.8916995913941509, 0.8400489431016522, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2677875], dtype=float32), 0.10129533]. 
=============================================
[2019-04-03 22:04:59,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:04:59,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:04:59,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-03 22:05:00,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:00,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:00,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-03 22:05:02,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:02,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:02,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-03 22:05:02,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:02,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:02,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-03 22:05:03,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,263] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-03 22:05:03,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-03 22:05:03,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:03,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:03,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-03 22:05:05,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:05,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:05,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-03 22:05:06,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:06,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:06,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-03 22:05:06,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:06,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:06,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-03 22:05:07,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:07,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:07,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-03 22:05:08,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:08,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:08,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-03 22:05:08,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:08,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:08,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-03 22:05:19,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0482324e-19 4.5353063e-10 1.9751492e-07 4.5779845e-04 9.2020905e-11
 9.9954200e-01 1.3017777e-11], sum to 1.0000
[2019-04-03 22:05:19,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4830
[2019-04-03 22:05:19,792] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5217261893856, 0.1771732158551255, 0.0, 1.0, 50744.26970951291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50386184605057, 0.1859069886179371, 0.0, 1.0, 59015.47869182152], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.6666666666666666, 0.5419884871708808, 0.5619689962059791, 0.0, 1.0, 0.2810260890086739], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.16963407], dtype=float32), 0.34855208]. 
=============================================
[2019-04-03 22:05:41,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2583983e-18 1.2466339e-08 1.1253108e-05 2.6351583e-04 5.5906471e-09
 9.9972516e-01 3.4788710e-11], sum to 1.0000
[2019-04-03 22:05:41,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-03 22:05:42,125] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 26.0, 25.18582318401195, 0.1458756657854181, 1.0, 1.0, 18721.76290850715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 220200.0000, 
sim time next is 220800.0000, 
raw observation next is [-4.133333333333334, 64.0, 145.6666666666667, 0.0, 26.0, 24.72218954334707, 0.2360052485151558, 1.0, 1.0, 199538.1360595971], 
processed observation next is [1.0, 0.5652173913043478, 0.34810710987996313, 0.64, 0.48555555555555574, 0.0, 0.6666666666666666, 0.5601824619455892, 0.5786684161717186, 1.0, 1.0, 0.9501816002837957], 
reward next is 0.0498, 
noisyNet noise sample is [array([-1.0756253], dtype=float32), -0.5200883]. 
=============================================
[2019-04-03 22:05:42,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0051987e-20 9.6138708e-10 1.7358157e-07 2.0823977e-08 1.5290998e-10
 9.9999976e-01 1.3036165e-11], sum to 1.0000
[2019-04-03 22:05:42,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4135
[2019-04-03 22:05:42,599] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 76.16666666666667, 0.0, 0.0, 26.0, 24.49206839131318, 0.1664085478418742, 0.0, 1.0, 44191.58824757619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46075229543123, 0.1591776734807486, 0.0, 1.0, 44179.26249534143], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5383960246192693, 0.5530592244935829, 0.0, 1.0, 0.2103774404540068], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.9996735], dtype=float32), -1.1405826]. 
=============================================
[2019-04-03 22:05:48,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0237232e-17 1.2047220e-08 4.4196091e-05 1.7572993e-02 5.1642601e-08
 9.8238277e-01 1.0056248e-09], sum to 1.0000
[2019-04-03 22:05:48,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8040
[2019-04-03 22:05:48,453] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 68.0, 0.0, 0.0, 26.0, 22.54314000165216, -0.2726453480366316, 0.0, 1.0, 47882.97179625639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283200.0000, 
sim time next is 283800.0000, 
raw observation next is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49414729299266, -0.2715521864936798, 0.0, 1.0, 47923.56784467668], 
processed observation next is [1.0, 0.2608695652173913, 0.12465373961218838, 0.675, 0.0, 0.0, 0.6666666666666666, 0.3745122744160551, 0.4094826045021067, 0.0, 1.0, 0.22820746592703184], 
reward next is 0.7718, 
noisyNet noise sample is [array([0.5388698], dtype=float32), 1.2071283]. 
=============================================
[2019-04-03 22:06:19,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8274726e-21 1.1303834e-10 5.4758038e-09 1.6787566e-02 2.4845356e-10
 9.8321241e-01 1.5357678e-12], sum to 1.0000
[2019-04-03 22:06:19,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1929
[2019-04-03 22:06:19,078] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.08810795553453, 0.2949925818132944, 1.0, 1.0, 87610.09038443935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415800.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 26.0, 25.08472063433556, 0.2978821066783279, 1.0, 1.0, 73203.56090163538], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5903933861946301, 0.5992940355594426, 1.0, 1.0, 0.34858838524588276], 
reward next is 0.6514, 
noisyNet noise sample is [array([0.14180383], dtype=float32), -0.781098]. 
=============================================
[2019-04-03 22:06:34,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2995859e-19 6.2365211e-09 3.5891027e-08 1.6102443e-02 4.1812279e-11
 9.8389751e-01 1.5508582e-11], sum to 1.0000
[2019-04-03 22:06:34,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3725
[2019-04-03 22:06:34,169] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.22112828104014, 0.1032846593110688, 0.0, 1.0, 42430.91032704792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.21714705622947, 0.09725527744671521, 0.0, 1.0, 42541.77143152586], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5180955880191226, 0.5324184258155717, 0.0, 1.0, 0.20257986395964697], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.8372937], dtype=float32), -1.9515916]. 
=============================================
[2019-04-03 22:06:34,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3984434e-20 4.3187592e-10 9.4861239e-09 9.9998343e-01 1.7318452e-11
 1.6598864e-05 4.2146577e-12], sum to 1.0000
[2019-04-03 22:06:34,640] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-03 22:06:34,699] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.73668359418617, -0.01137927217754517, 0.0, 1.0, 44067.00476043121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625200.0000, 
sim time next is 625800.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.7123152832534, -0.01238591243125258, 0.0, 1.0, 43985.4052556622], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.47602627360445, 0.4958713625229158, 0.0, 1.0, 0.20945431074124857], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.4255928], dtype=float32), 0.5859358]. 
=============================================
[2019-04-03 22:06:34,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.2754967e-18 5.1569721e-08 9.0651639e-07 7.5657469e-01 1.1000454e-08
 2.4342436e-01 3.6480036e-10], sum to 1.0000
[2019-04-03 22:06:34,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-03 22:06:34,934] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.9834378117394, 0.2928523028375311, 0.0, 1.0, 32506.05895351528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 580200.0000, 
sim time next is 580800.0000, 
raw observation next is [-1.9, 87.0, 0.0, 0.0, 26.0, 24.9636030592504, 0.2877425200533035, 0.0, 1.0, 47948.17820900137], 
processed observation next is [0.0, 0.7391304347826086, 0.4099722991689751, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5803002549375332, 0.5959141733511012, 0.0, 1.0, 0.22832465813810177], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.40531102], dtype=float32), -0.8448017]. 
=============================================
[2019-04-03 22:06:47,507] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2214620e-20 1.0911148e-09 4.6871658e-09 9.5414704e-01 8.1716350e-10
 4.5852900e-02 1.1253650e-11], sum to 1.0000
[2019-04-03 22:06:47,507] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3102
[2019-04-03 22:06:47,600] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 25.02410300615295, 0.2221303497555626, 0.0, 1.0, 43265.86210195989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02531936966371, 0.2196593467886331, 0.0, 1.0, 42740.31806773723], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.585443280805309, 0.5732197822628777, 0.0, 1.0, 0.20352532413208205], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.63596004], dtype=float32), 0.22017439]. 
=============================================
[2019-04-03 22:06:49,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2086220e-21 3.8917316e-12 8.4036195e-11 9.9998116e-01 5.2311168e-12
 1.8799410e-05 2.2759087e-13], sum to 1.0000
[2019-04-03 22:06:49,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-03 22:06:49,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 69.83333333333334, 0.0, 0.0, 26.0, 23.57621379442949, -0.05482083146737604, 0.0, 1.0, 43850.81213819271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 630600.0000, 
sim time next is 631200.0000, 
raw observation next is [-4.5, 71.66666666666667, 0.0, 0.0, 26.0, 23.54574507010143, -0.05888595018967408, 0.0, 1.0, 43892.65273252814], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.4621454225084524, 0.48037134993677527, 0.0, 1.0, 0.2090126320596578], 
reward next is 0.7910, 
noisyNet noise sample is [array([-1.0367107], dtype=float32), 0.001881684]. 
=============================================
[2019-04-03 22:06:55,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1220148e-20 2.4784205e-08 1.2784105e-07 3.7938345e-02 1.6128281e-08
 9.6206152e-01 4.1217561e-11], sum to 1.0000
[2019-04-03 22:06:55,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7585
[2019-04-03 22:06:55,245] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742281717996, 0.3958715926577653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750600.0000, 
sim time next is 751200.0000, 
raw observation next is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 26.0, 25.67409943608536, 0.4271760999592983, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40535549399815335, 0.51, 0.1888888888888889, 0.0031307550644567215, 0.6666666666666666, 0.6395082863404467, 0.6423920333197661, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8944669], dtype=float32), 0.9218042]. 
=============================================
[2019-04-03 22:07:07,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9440540e-22 1.2037872e-10 7.6414436e-10 9.9999940e-01 7.4129192e-12
 6.0981358e-07 8.8654798e-14], sum to 1.0000
[2019-04-03 22:07:07,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4252
[2019-04-03 22:07:07,957] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.0814771004057, 0.3103114309494024, 0.0, 1.0, 66294.35121485805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 851400.0000, 
sim time next is 852000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.06402759586089, 0.3060660049250876, 0.0, 1.0, 50999.98890726877], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5886689663217407, 0.6020220016416958, 0.0, 1.0, 0.2428570900346132], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.8964162], dtype=float32), 0.21807194]. 
=============================================
[2019-04-03 22:07:07,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.88705 ]
 [81.55181 ]
 [82.22501 ]
 [85.13517 ]
 [87.664085]], R is [[80.87262726]
 [80.74821472]
 [80.51134491]
 [80.70623016]
 [80.73989868]].
[2019-04-03 22:07:22,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1891126e-27 6.7928404e-14 6.7314669e-12 9.9999988e-01 2.0202722e-14
 6.2732092e-08 2.6577636e-16], sum to 1.0000
[2019-04-03 22:07:22,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-03 22:07:22,858] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978459567886, 0.456711086641393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.56930436978565, 0.4482434457319038, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6307753641488043, 0.6494144819106346, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9893894], dtype=float32), -0.7406092]. 
=============================================
[2019-04-03 22:07:30,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0294784e-24 3.0461272e-11 1.1231728e-08 9.9445772e-01 7.7719255e-12
 5.5422429e-03 4.0139878e-13], sum to 1.0000
[2019-04-03 22:07:30,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-03 22:07:30,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486237009361, 0.5870049208842719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987546805733, 0.5776842513325583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6666562890047775, 0.6925614171108528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5684999], dtype=float32), 0.7941704]. 
=============================================
[2019-04-03 22:07:42,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1779992e-30 3.0609468e-16 1.3849731e-13 1.0000000e+00 8.2184636e-17
 2.9362430e-12 7.2904029e-19], sum to 1.0000
[2019-04-03 22:07:42,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-03 22:07:42,368] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.61666666666667, 86.0, 125.0, 0.0, 26.0, 25.6469106409182, 0.5560839064335747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 996600.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 26.0, 25.95070354710873, 0.5811338451229963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.6666666666666666, 0.6625586289257276, 0.6937112817076655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.247134], dtype=float32), -1.1168836]. 
=============================================
[2019-04-03 22:07:52,640] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2253734e-27 3.1445549e-14 3.7893351e-13 1.0000000e+00 1.0677236e-15
 6.3082634e-10 1.5561495e-16], sum to 1.0000
[2019-04-03 22:07:52,687] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-03 22:07:52,728] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.59668832600839, 0.3842976190811451, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1195200.0000, 
sim time next is 1195800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.59686958586542, 0.3810769865533581, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5497391321554517, 0.6270256621844527, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3796501], dtype=float32), 1.00237]. 
=============================================
[2019-04-03 22:07:57,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3245427e-29 1.0944416e-15 2.0656193e-14 1.0000000e+00 1.3615493e-17
 2.5705124e-11 4.3300969e-18], sum to 1.0000
[2019-04-03 22:07:57,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6779
[2019-04-03 22:07:57,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.5020058270154, 0.5909868028155305, 0.0, 1.0, 18749.97769297432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1291200.0000, 
sim time next is 1291800.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48288061093533, 0.5892334736185498, 0.0, 1.0, 35418.31171603059], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6235733842446107, 0.6964111578728499, 0.0, 1.0, 0.1686586272191933], 
reward next is 0.8313, 
noisyNet noise sample is [array([0.14292102], dtype=float32), -0.673809]. 
=============================================
[2019-04-03 22:08:02,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2382467e-21 8.6152568e-10 9.2155936e-09 9.9998868e-01 2.5726085e-10
 1.1287355e-05 1.5659337e-12], sum to 1.0000
[2019-04-03 22:08:02,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2434
[2019-04-03 22:08:02,692] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.366666666666667, 92.0, 0.0, 0.0, 26.0, 25.44896967154099, 0.5545390752306684, 0.0, 1.0, 41243.8790289588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1309200.0000, 
sim time next is 1309800.0000, 
raw observation next is [2.283333333333333, 92.0, 0.0, 0.0, 26.0, 25.43766344701285, 0.5481153366633863, 0.0, 1.0, 42224.56229516613], 
processed observation next is [1.0, 0.13043478260869565, 0.5258541089566021, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6198052872510708, 0.6827051122211287, 0.0, 1.0, 0.20106934426269585], 
reward next is 0.7989, 
noisyNet noise sample is [array([1.3634658], dtype=float32), 0.45315248]. 
=============================================
[2019-04-03 22:08:14,320] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3142694e-25 4.0788675e-14 3.7062974e-12 1.0000000e+00 9.5962794e-15
 4.7944443e-10 1.7848431e-15], sum to 1.0000
[2019-04-03 22:08:14,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-03 22:08:14,351] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.28962217131632, 0.4597507503624739, 0.0, 1.0, 38995.64302750208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1398600.0000, 
sim time next is 1399200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.29583365187685, 0.461015291734918, 0.0, 1.0, 38789.67651029267], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6079861376564043, 0.6536717639116393, 0.0, 1.0, 0.18471274528710796], 
reward next is 0.8153, 
noisyNet noise sample is [array([-0.3980503], dtype=float32), -0.10083048]. 
=============================================
[2019-04-03 22:08:17,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6851850e-26 5.4614608e-14 4.8178773e-13 1.0000000e+00 2.2569157e-15
 8.6826393e-11 3.8386578e-16], sum to 1.0000
[2019-04-03 22:08:17,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7611
[2019-04-03 22:08:17,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.7016986789647, 0.5338645116602434, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1565400.0000, 
sim time next is 1566000.0000, 
raw observation next is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706412787, 0.5257040620735105, 0.0, 1.0, 18736.71446041186], 
processed observation next is [1.0, 0.13043478260869565, 0.5844875346260389, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6334639220106558, 0.6752346873578369, 0.0, 1.0, 0.08922244981148504], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.37337062], dtype=float32), 0.6509109]. 
=============================================
[2019-04-03 22:08:17,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.86988]
 [78.88402]
 [78.8966 ]
 [78.99444]
 [79.07888]], R is [[78.85839081]
 [79.06980896]
 [79.27911377]
 [79.44168091]
 [79.54399109]].
[2019-04-03 22:08:26,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3251382e-23 1.1824949e-13 1.4508882e-10 1.0000000e+00 2.9652824e-13
 4.1254655e-08 3.8910563e-15], sum to 1.0000
[2019-04-03 22:08:26,097] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0565
[2019-04-03 22:08:26,118] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.30940493897585, 0.6307638643150252, 0.0, 1.0, 173753.9425738378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.46048683418699, 0.6887371586476867, 0.0, 1.0, 42066.26284043533], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6217072361822492, 0.7295790528825622, 0.0, 1.0, 0.20031553733540633], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.8707387], dtype=float32), -0.112503566]. 
=============================================
[2019-04-03 22:08:27,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4694201e-24 4.5248977e-13 1.0802664e-11 1.0000000e+00 1.6510202e-13
 5.8982828e-09 4.0664523e-15], sum to 1.0000
[2019-04-03 22:08:27,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4200
[2019-04-03 22:08:27,040] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.416666666666667, 79.50000000000001, 0.0, 0.0, 26.0, 25.45817698002547, 0.4907562761383306, 0.0, 1.0, 39311.98276358261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1581000.0000, 
sim time next is 1581600.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.44032852103544, 0.4990318731000995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6103416435826409, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6200273767529533, 0.6663439577000332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79332644], dtype=float32), -0.1853996]. 
=============================================
[2019-04-03 22:08:30,356] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4469462e-23 5.0064297e-12 6.8826733e-10 9.9999988e-01 2.7387329e-12
 9.1074668e-08 1.9454355e-14], sum to 1.0000
[2019-04-03 22:08:30,357] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-03 22:08:30,380] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.26561614666427, 0.4539233782520944, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1707600.0000, 
sim time next is 1708200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20181073256224, 0.3301090328970632, 1.0, 1.0, 22333.3191628539], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6001508943801867, 0.6100363442990211, 1.0, 1.0, 0.10634913887073286], 
reward next is 0.8937, 
noisyNet noise sample is [array([0.9542977], dtype=float32), -1.2662407]. 
=============================================
[2019-04-03 22:08:35,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9666290e-23 5.9079343e-12 2.3069421e-10 1.0000000e+00 6.8093838e-13
 3.5467746e-08 2.7111326e-14], sum to 1.0000
[2019-04-03 22:08:35,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8702
[2019-04-03 22:08:35,812] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.18859009468912, 0.3570922796135538, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.12097042545417, 0.3442666230397556, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.41274238227146814, 0.8366666666666667, 0.17333333333333334, 0.0, 0.6666666666666666, 0.5934142021211809, 0.6147555410132518, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46261817], dtype=float32), -1.2053255]. 
=============================================
[2019-04-03 22:08:35,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.78351]
 [73.91905]
 [74.05206]
 [74.14143]
 [74.07455]], R is [[73.94097137]
 [74.20156097]
 [74.45954895]
 [74.71495056]
 [74.96780396]].
[2019-04-03 22:08:46,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8484197e-21 1.2433790e-11 2.1459798e-10 1.0000000e+00 2.1129572e-12
 1.1999562e-08 1.9862641e-13], sum to 1.0000
[2019-04-03 22:08:46,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3419
[2019-04-03 22:08:46,295] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 25.093145237173, 0.2335842265182289, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849800.0000, 
sim time next is 1850400.0000, 
raw observation next is [-5.6, 78.0, 134.0, 72.5, 26.0, 25.01960917117569, 0.2180462253221129, 0.0, 1.0, 18757.33677820371], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.78, 0.44666666666666666, 0.08011049723756906, 0.6666666666666666, 0.5849674309313077, 0.572682075107371, 0.0, 1.0, 0.08932065132477957], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.45641094], dtype=float32), 0.1466791]. 
=============================================
[2019-04-03 22:08:49,678] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0392321e-20 2.9007982e-10 2.1525604e-09 1.0000000e+00 2.7588147e-11
 7.6717699e-09 2.2816161e-12], sum to 1.0000
[2019-04-03 22:08:49,679] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4342
[2019-04-03 22:08:49,704] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.100000000000001, 85.0, 0.0, 0.0, 26.0, 23.25883988553956, -0.1686694708306329, 0.0, 1.0, 44687.21542290307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1923600.0000, 
sim time next is 1924200.0000, 
raw observation next is [-9.2, 86.5, 0.0, 0.0, 26.0, 23.19586501485474, -0.1795779042534386, 0.0, 1.0, 44627.96568197844], 
processed observation next is [1.0, 0.2608695652173913, 0.20775623268698065, 0.865, 0.0, 0.0, 0.6666666666666666, 0.43298875123789493, 0.44014069858218713, 0.0, 1.0, 0.21251412229513544], 
reward next is 0.7875, 
noisyNet noise sample is [array([-0.14663932], dtype=float32), 0.70116514]. 
=============================================
[2019-04-03 22:09:07,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.98755900e-26 1.40506105e-14 2.28748687e-11 1.00000000e+00
 6.57476189e-14 5.47131914e-12 4.96893773e-16], sum to 1.0000
[2019-04-03 22:09:07,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6094
[2019-04-03 22:09:07,343] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 79.0, 140.6666666666667, 0.0, 26.0, 25.81266566273765, 0.4526215261815746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2036400.0000, 
sim time next is 2037000.0000, 
raw observation next is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848470301, 0.4760679568449861, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.4444444444444443, 0.0, 0.6666666666666666, 0.677542373725251, 0.6586893189483287, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38119915], dtype=float32), -0.8181434]. 
=============================================
[2019-04-03 22:09:07,387] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.14248]
 [86.35628]
 [86.34464]
 [85.72248]
 [85.31757]], R is [[86.09253693]
 [86.23161316]
 [86.36930084]
 [85.55178833]
 [84.75310516]].
[2019-04-03 22:09:44,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9935191e-25 1.1193415e-13 4.5958739e-13 1.0000000e+00 3.1560797e-14
 5.6300489e-09 8.3212781e-16], sum to 1.0000
[2019-04-03 22:09:44,725] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-03 22:09:44,773] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 52.0, 0.0, 0.0, 26.0, 25.50686074055875, 0.4156808413630275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311200.0000, 
sim time next is 2311800.0000, 
raw observation next is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466084579, 0.418846898110051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6328245550704826, 0.6396156327033503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8466387], dtype=float32), 0.5380558]. 
=============================================
[2019-04-03 22:09:57,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5548745e-21 2.0556589e-11 5.4754795e-10 1.0000000e+00 2.6308719e-12
 2.5086475e-09 5.5626201e-13], sum to 1.0000
[2019-04-03 22:09:57,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2701
[2019-04-03 22:09:57,996] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 26.0, 24.96083074995219, 0.2726831361285817, 0.0, 1.0, 40076.67721661839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395200.0000, 
sim time next is 2395800.0000, 
raw observation next is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.95557783111534, 0.2654938209473628, 0.0, 1.0, 44726.35813851634], 
processed observation next is [0.0, 0.7391304347826086, 0.4307479224376732, 0.445, 0.0, 0.0, 0.6666666666666666, 0.5796314859262782, 0.5884979403157876, 0.0, 1.0, 0.21298265780245876], 
reward next is 0.7870, 
noisyNet noise sample is [array([0.97243285], dtype=float32), 0.3673849]. 
=============================================
[2019-04-03 22:09:59,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6527284e-19 4.7517362e-10 5.0253406e-09 9.9999964e-01 5.6612905e-11
 3.7213050e-07 1.4104274e-11], sum to 1.0000
[2019-04-03 22:09:59,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2867
[2019-04-03 22:09:59,623] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.3, 42.0, 0.0, 0.0, 26.0, 24.65143696616505, 0.1709530897751696, 0.0, 1.0, 43107.33324174085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [-5.4, 42.33333333333333, 0.0, 0.0, 26.0, 24.65102450313191, 0.1622502105850496, 0.0, 1.0, 43105.29747559249], 
processed observation next is [0.0, 1.0, 0.31301939058171746, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5542520419276592, 0.5540834035283498, 0.0, 1.0, 0.20526332131234518], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.5230466], dtype=float32), -0.5509812]. 
=============================================
[2019-04-03 22:09:59,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.3189  ]
 [69.489075]
 [69.6729  ]
 [69.87319 ]
 [70.087555]], R is [[69.2562561 ]
 [69.35842133]
 [69.45957184]
 [69.55979156]
 [69.65913391]].
[2019-04-03 22:10:01,153] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-03 22:10:01,206] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:01,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:01,285] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:01,285] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,287] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:01,339] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:10:01,339] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:01,341] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-03 22:10:43,313] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.4411699], dtype=float32), 0.33004472]
[2019-04-03 22:10:43,314] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [11.88571411666667, 87.24346477666667, 84.631277725, 0.0, 20.0, 21.11275124841271, -0.6877300651133843, 1.0, 1.0, 0.0]
[2019-04-03 22:10:43,314] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:10:43,315] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.9329364e-19 1.7081232e-10 3.1591527e-09 9.9999988e-01 3.5881048e-11
 1.1306648e-07 4.4157746e-12], sampled 0.3266407123791709
[2019-04-03 22:11:30,873] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5789.3442 154058886.3106 -1773.4456
[2019-04-03 22:11:37,693] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5043.9723 174442790.2437 -2782.7422
[2019-04-03 22:11:50,163] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5275.4799 196153257.0920 -2270.6413
[2019-04-03 22:11:51,186] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 200000, evaluation results [200000.0, 5043.972297248031, 174442790.24373633, -2782.7422032994377, 5789.344163732218, 154058886.31058824, -1773.4456022536383, 5275.479945300525, 196153257.0919658, -2270.6413487550612]
[2019-04-03 22:12:01,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3840776e-31 3.8248116e-18 3.3778869e-16 1.0000000e+00 1.4949352e-18
 2.1470582e-16 3.9004687e-20], sum to 1.0000
[2019-04-03 22:12:01,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5983
[2019-04-03 22:12:01,926] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.8365895151169, 0.3531739161155074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 26.0, 25.82779172399819, 0.3593648272531154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.6666666666666666, 0.6523159769998491, 0.6197882757510385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91660273], dtype=float32), -1.4623982]. 
=============================================
[2019-04-03 22:12:05,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5068660e-26 2.8871457e-15 9.5161997e-13 1.0000000e+00 2.6410085e-15
 1.1647723e-10 2.0333134e-16], sum to 1.0000
[2019-04-03 22:12:05,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0732
[2019-04-03 22:12:05,890] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30308667457226, 0.314852574383751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41834293453547, 0.3428142724455684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181952445446225, 0.6142714241485229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.069424], dtype=float32), 1.0356063]. 
=============================================
[2019-04-03 22:12:06,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6002250e-28 4.8731433e-17 2.0951248e-14 1.0000000e+00 7.2912480e-16
 5.3799767e-11 3.2071687e-18], sum to 1.0000
[2019-04-03 22:12:06,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3530
[2019-04-03 22:12:06,293] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.73362999716765, 0.3870396064142303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2633400.0000, 
sim time next is 2634000.0000, 
raw observation next is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 26.0, 25.71648444177423, 0.3868027801010965, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.5666666666666668, 0.7583333333333333, 0.18453038674033148, 0.6666666666666666, 0.6430403701478525, 0.6289342600336988, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19621745], dtype=float32), 1.3771554]. 
=============================================
[2019-04-03 22:12:06,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.743195]
 [93.774925]
 [93.44449 ]
 [92.837456]
 [92.850426]], R is [[93.68241119]
 [93.74559021]
 [93.57091522]
 [93.2488327 ]
 [93.31634521]].
[2019-04-03 22:12:12,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5672329e-20 8.5197613e-12 3.2645243e-08 9.9999559e-01 3.1913874e-10
 4.4512894e-06 8.5385189e-12], sum to 1.0000
[2019-04-03 22:12:12,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7633
[2019-04-03 22:12:12,882] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401701, 0.4455401773113885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216947739, 0.4813745730640873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474564491, 0.6604581910213624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69397724], dtype=float32), -0.899698]. 
=============================================
[2019-04-03 22:12:12,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.34145 ]
 [71.97003 ]
 [72.228424]
 [71.74907 ]
 [71.54306 ]], R is [[71.04421997]
 [71.33377838]
 [71.30812073]
 [70.65331268]
 [70.31493378]].
[2019-04-03 22:12:14,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1827137e-22 5.6670638e-14 2.5831845e-10 1.0000000e+00 5.9616998e-13
 7.2180312e-10 5.7584925e-14], sum to 1.0000
[2019-04-03 22:12:14,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7346
[2019-04-03 22:12:14,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08824129612724, -0.1239193550621719, 0.0, 1.0, 43343.74044588086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2700000.0000, 
sim time next is 2700600.0000, 
raw observation next is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.00802433545124, -0.1373052174927247, 0.0, 1.0, 43285.8750069052], 
processed observation next is [1.0, 0.2608695652173913, 0.02400738688827338, 0.83, 0.0, 0.0, 0.6666666666666666, 0.41733536128760323, 0.45423159416909176, 0.0, 1.0, 0.20612321431859618], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.9625291], dtype=float32), 1.6316282]. 
=============================================
[2019-04-03 22:12:16,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7645061e-23 1.0915491e-13 2.1490865e-12 1.0000000e+00 2.4660719e-14
 1.5856061e-10 3.0789129e-15], sum to 1.0000
[2019-04-03 22:12:16,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-03 22:12:16,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 60.5, 0.0, 0.0, 26.0, 25.19194566223795, 0.3902333788554735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.19958971886931, 0.3729138759017004, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6100000000000001, 0.0, 0.0, 0.6666666666666666, 0.5999658099057757, 0.6243046253005667, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17238948], dtype=float32), -0.76615626]. 
=============================================
[2019-04-03 22:12:22,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.646614e-25 2.628034e-15 6.284883e-13 1.000000e+00 9.003322e-15
 9.496083e-11 1.537615e-16], sum to 1.0000
[2019-04-03 22:12:22,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7324
[2019-04-03 22:12:22,061] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.44306858244278, 0.1547975692137588, 0.0, 1.0, 40765.97050748167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2779800.0000, 
sim time next is 2780400.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.47681007647444, 0.1529900940123659, 0.0, 1.0, 40769.99870622924], 
processed observation next is [1.0, 0.17391304347826086, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5397341730395366, 0.550996698004122, 0.0, 1.0, 0.194142850982044], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.5461247], dtype=float32), -0.08636715]. 
=============================================
[2019-04-03 22:12:24,574] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.08044787e-25 1.14247275e-13 1.05349245e-11 1.00000000e+00
 6.63931741e-15 4.62871625e-12 1.47342686e-14], sum to 1.0000
[2019-04-03 22:12:24,574] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-03 22:12:24,590] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.11922476812327, 0.2957754292357351, 0.0, 1.0, 56223.8232665979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.10678755959061, 0.2943149028761669, 0.0, 1.0, 56190.29491634563], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.6666666666666666, 0.592232296632551, 0.598104967625389, 0.0, 1.0, 0.2675728329349792], 
reward next is 0.7324, 
noisyNet noise sample is [array([1.1938579], dtype=float32), 0.7002565]. 
=============================================
[2019-04-03 22:12:28,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8331151e-24 8.5236467e-13 6.8508442e-11 1.0000000e+00 7.0911809e-13
 3.7904342e-09 2.5074547e-14], sum to 1.0000
[2019-04-03 22:12:28,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-03 22:12:28,542] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.80962299489654, 0.2404127293227629, 0.0, 1.0, 55777.89861489003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2869200.0000, 
sim time next is 2869800.0000, 
raw observation next is [1.0, 94.16666666666666, 0.0, 0.0, 26.0, 24.83654257675716, 0.238330936419893, 0.0, 1.0, 55764.72135766777], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.56971188139643, 0.5794436454732977, 0.0, 1.0, 0.2655462921793703], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.9816289], dtype=float32), -0.5544955]. 
=============================================
[2019-04-03 22:12:29,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2655283e-21 1.7411413e-11 1.1556072e-09 1.0000000e+00 2.6848459e-11
 1.5665670e-08 1.1628593e-13], sum to 1.0000
[2019-04-03 22:12:29,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5838
[2019-04-03 22:12:29,444] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00047585508761, 0.4418587343767859, 0.0, 1.0, 101749.5652524668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09548248273574, 0.4687732792137925, 0.0, 1.0, 64644.21166589476], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.591290206894645, 0.6562577597379309, 0.0, 1.0, 0.30782957936140365], 
reward next is 0.6922, 
noisyNet noise sample is [array([0.43607858], dtype=float32), 1.853726]. 
=============================================
[2019-04-03 22:12:35,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4786087e-21 2.5259478e-12 4.8995552e-10 1.0000000e+00 7.8035137e-13
 7.5759105e-10 2.8662766e-13], sum to 1.0000
[2019-04-03 22:12:35,151] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-03 22:12:35,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73310335852472, -0.0154404130890531, 0.0, 1.0, 40257.14743058592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70673305169077, -0.02006278648256676, 0.0, 1.0, 40309.20008162671], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4755610876408974, 0.4933124045058111, 0.0, 1.0, 0.19194857181727004], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.4823942], dtype=float32), -1.5007519]. 
=============================================
[2019-04-03 22:12:36,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5694237e-21 3.9941093e-12 7.7868691e-11 1.0000000e+00 1.5791216e-12
 1.2229570e-10 1.9854071e-14], sum to 1.0000
[2019-04-03 22:12:36,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8219
[2019-04-03 22:12:36,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00604255882617, 0.3209373738062801, 0.0, 1.0, 48801.29409907426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.0039578251085, 0.3207204469599723, 0.0, 1.0, 40921.83565136779], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5836631520923751, 0.606906815653324, 0.0, 1.0, 0.19486588405413235], 
reward next is 0.8051, 
noisyNet noise sample is [array([-1.6336944], dtype=float32), 0.75405043]. 
=============================================
[2019-04-03 22:12:38,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5435194e-21 1.6027842e-12 9.7107683e-11 1.0000000e+00 2.5202413e-11
 3.7987213e-09 2.3253516e-13], sum to 1.0000
[2019-04-03 22:12:38,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-03 22:12:38,145] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.01216597481954, 0.3165066491434173, 0.0, 1.0, 31285.3274087641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01217155059833, 0.3131507246121719, 0.0, 1.0, 36561.31839217097], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5843476292165276, 0.6043835748707239, 0.0, 1.0, 0.1741015161531951], 
reward next is 0.8259, 
noisyNet noise sample is [array([1.4285723], dtype=float32), -0.36809105]. 
=============================================
[2019-04-03 22:12:45,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3721922e-31 1.1511636e-16 2.3629458e-15 1.0000000e+00 6.3204533e-17
 1.2309523e-12 3.0793630e-20], sum to 1.0000
[2019-04-03 22:12:45,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0769
[2019-04-03 22:12:45,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07159067414852, 0.5026342534109856, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21135069374869, 0.5173583551914502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6842792244790576, 0.6724527850638168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9915074], dtype=float32), -0.2795945]. 
=============================================
[2019-04-03 22:12:49,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1319060e-26 1.6488470e-15 3.2567693e-13 1.0000000e+00 1.9011236e-14
 4.0769409e-13 1.6800332e-16], sum to 1.0000
[2019-04-03 22:12:49,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0059
[2019-04-03 22:12:49,990] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.19729730994136, 0.29251287761979, 0.0, 1.0, 53979.67027896643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3126000.0000, 
sim time next is 3126600.0000, 
raw observation next is [2.8, 100.0, 0.0, 0.0, 26.0, 25.24632109332966, 0.2965888649438422, 0.0, 1.0, 53901.9814110275], 
processed observation next is [1.0, 0.17391304347826086, 0.5401662049861496, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6038600911108049, 0.5988629549812807, 0.0, 1.0, 0.2566761019572738], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.95081425], dtype=float32), 1.3231145]. 
=============================================
[2019-04-03 22:12:50,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8641156e-24 2.4209754e-15 7.8284804e-14 1.0000000e+00 6.5461444e-14
 4.3458058e-12 5.9597774e-17], sum to 1.0000
[2019-04-03 22:12:50,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-03 22:12:50,854] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41776429113253, 0.6985187319810303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41044701211077, 0.7061939709825796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008705843425641, 0.7353979903275265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70396686], dtype=float32), 0.3440396]. 
=============================================
[2019-04-03 22:12:52,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8582501e-25 2.0621678e-13 2.7882206e-12 1.0000000e+00 2.3500125e-13
 1.2395389e-10 1.2015682e-16], sum to 1.0000
[2019-04-03 22:12:52,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8968
[2019-04-03 22:12:52,729] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.4210052140895, 0.631533918731311, 0.0, 1.0, 43686.15855874274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3193200.0000, 
sim time next is 3193800.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.51459271548904, 0.6373342712827814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6262160596240868, 0.7124447570942604, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5548072], dtype=float32), 1.4533598]. 
=============================================
[2019-04-03 22:12:53,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0112520e-22 1.2017037e-12 1.1493236e-11 1.0000000e+00 1.9456528e-12
 1.8147919e-10 8.5095217e-15], sum to 1.0000
[2019-04-03 22:12:53,510] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3541
[2019-04-03 22:12:53,526] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.166666666666667, 71.16666666666667, 0.0, 0.0, 26.0, 25.04233650639868, 0.3863407445507412, 0.0, 1.0, 43749.63653430805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3287400.0000, 
sim time next is 3288000.0000, 
raw observation next is [-7.333333333333334, 72.33333333333334, 0.0, 0.0, 26.0, 25.04907506087838, 0.3805829478910958, 0.0, 1.0, 43761.26229218441], 
processed observation next is [1.0, 0.043478260869565216, 0.2594644506001847, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.587422921739865, 0.6268609826303653, 0.0, 1.0, 0.20838696329611622], 
reward next is 0.7916, 
noisyNet noise sample is [array([1.5019102], dtype=float32), 0.3542031]. 
=============================================
[2019-04-03 22:12:53,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.71344]
 [67.0926 ]
 [67.6082 ]
 [68.07981]
 [68.57674]], R is [[66.56488037]
 [66.69090271]
 [66.8157959 ]
 [66.93959808]
 [67.06224823]].
[2019-04-03 22:12:54,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.85559033e-25 1.36099314e-14 2.53499240e-12 1.00000000e+00
 8.62526806e-14 1.08690487e-11 7.79763920e-16], sum to 1.0000
[2019-04-03 22:12:54,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1908
[2019-04-03 22:12:54,910] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.59432046310412, 0.6257093286045742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.60528811520976, 0.6151634927576999, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6337740096008133, 0.7050544975859, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1872635], dtype=float32), 0.5158603]. 
=============================================
[2019-04-03 22:13:07,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.08358636e-23 1.00449195e-12 1.31112189e-12 1.00000000e+00
 3.09455478e-12 1.42747436e-09 7.76605440e-16], sum to 1.0000
[2019-04-03 22:13:07,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9816
[2019-04-03 22:13:07,627] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 26.0, 26.70104312391956, 0.719860404447319, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3508800.0000, 
sim time next is 3509400.0000, 
raw observation next is [3.0, 49.0, 97.33333333333334, 749.6666666666667, 26.0, 26.74344988119462, 0.7223304576243632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3244444444444445, 0.8283609576427257, 0.6666666666666666, 0.7286208234328851, 0.7407768192081211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8416052], dtype=float32), -0.33868337]. 
=============================================
[2019-04-03 22:13:12,194] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.05258466e-26 3.14924711e-15 1.51512483e-13 1.00000000e+00
 2.39997485e-14 6.64924713e-13 1.09389789e-17], sum to 1.0000
[2019-04-03 22:13:12,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5791
[2019-04-03 22:13:12,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.85387625279866, 0.5189949064705648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490800.0000, 
sim time next is 3491400.0000, 
raw observation next is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4579870729455217, 0.6183333333333333, 0.341111111111111, 0.7771639042357275, 0.6666666666666666, 0.6678772918322308, 0.6807303812886881, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4542322], dtype=float32), -0.4002936]. 
=============================================
[2019-04-03 22:13:15,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6453743e-26 2.6329123e-14 1.9503928e-13 1.0000000e+00 2.6535018e-14
 3.3444945e-11 2.3229053e-16], sum to 1.0000
[2019-04-03 22:13:15,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2885
[2019-04-03 22:13:15,315] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.68766442348429, 0.4991903413894219, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.69809642866755, 0.4997874376128952, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.6666666666666666, 0.6415080357222959, 0.6665958125376318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09115051], dtype=float32), 0.3123463]. 
=============================================
[2019-04-03 22:13:15,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[93.169754]
 [93.4117  ]
 [93.68625 ]
 [93.930115]
 [94.13797 ]], R is [[92.99434662]
 [93.06440735]
 [93.13376617]
 [93.20243073]
 [93.27040863]].
[2019-04-03 22:13:16,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8278196e-25 4.6853667e-13 5.3437273e-13 1.0000000e+00 1.6943884e-13
 3.1047959e-11 2.5189325e-16], sum to 1.0000
[2019-04-03 22:13:16,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9063
[2019-04-03 22:13:16,854] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.76153513195192, 0.507373816237126, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3578400.0000, 
sim time next is 3579000.0000, 
raw observation next is [-4.833333333333334, 63.16666666666667, 107.3333333333333, 730.6666666666666, 26.0, 25.70160110788565, 0.4986732650702106, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32871652816251157, 0.6316666666666667, 0.3577777777777777, 0.807366482504604, 0.6666666666666666, 0.6418000923238042, 0.6662244216900702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7643], dtype=float32), 0.5605714]. 
=============================================
[2019-04-03 22:13:16,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.70369 ]
 [89.25915 ]
 [89.67025 ]
 [90.064674]
 [90.25451 ]], R is [[88.27754974]
 [88.39477539]
 [88.51082611]
 [88.62571716]
 [88.73946381]].
[2019-04-03 22:13:22,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9801696e-21 1.2824836e-11 8.4766916e-10 1.0000000e+00 6.2725871e-12
 1.4091253e-09 3.5709920e-13], sum to 1.0000
[2019-04-03 22:13:22,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3708
[2019-04-03 22:13:22,429] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.70954607277753, 0.2303202758449619, 0.0, 1.0, 42838.82894860673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3826800.0000, 
sim time next is 3827400.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.67823031375888, 0.2233559700656119, 0.0, 1.0, 42782.35399864827], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5565191928132401, 0.5744519900218706, 0.0, 1.0, 0.20372549523165845], 
reward next is 0.7963, 
noisyNet noise sample is [array([-1.4737177], dtype=float32), 0.45710155]. 
=============================================
[2019-04-03 22:13:23,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2868968e-29 3.0468258e-17 5.0880952e-15 1.0000000e+00 2.5751861e-17
 2.5589482e-11 6.7797894e-19], sum to 1.0000
[2019-04-03 22:13:23,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8099
[2019-04-03 22:13:23,634] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 60.0, 111.1666666666667, 782.8333333333334, 26.0, 26.55612262888638, 0.6272319979571849, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840000.0000, 
sim time next is 3840600.0000, 
raw observation next is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.57259700018373, 0.6328546667760958, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43028624192059095, 0.6, 0.37444444444444436, 0.8736648250460406, 0.6666666666666666, 0.7143830833486442, 0.7109515555920319, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0305244], dtype=float32), 0.061332397]. 
=============================================
[2019-04-03 22:13:33,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2167184e-21 2.3513153e-11 1.7661070e-10 1.0000000e+00 1.2992901e-11
 4.5080912e-09 5.5136185e-13], sum to 1.0000
[2019-04-03 22:13:33,065] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5875
[2019-04-03 22:13:33,120] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.38801183790561, 0.1936584808212278, 0.0, 1.0, 43777.23037785777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.38413364002111, 0.1806322529518629, 0.0, 1.0, 43765.62954486269], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5320111366684257, 0.5602107509839543, 0.0, 1.0, 0.20840775973744138], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.0381131], dtype=float32), 0.18817963]. 
=============================================
[2019-04-03 22:13:46,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9960721e-26 3.0434460e-13 1.2099280e-12 1.0000000e+00 1.6357949e-14
 3.1922947e-11 7.7115288e-16], sum to 1.0000
[2019-04-03 22:13:46,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-03 22:13:46,886] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.3391475820235, 0.411579838436313, 0.0, 1.0, 46773.3717495204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34193089866002, 0.4081655314764128, 0.0, 1.0, 41711.45392747389], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6118275748883351, 0.6360551771588042, 0.0, 1.0, 0.198625971083209], 
reward next is 0.8014, 
noisyNet noise sample is [array([1.1846365], dtype=float32), -0.66937554]. 
=============================================
[2019-04-03 22:14:27,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8824673e-23 5.2627976e-13 8.0025611e-11 1.0000000e+00 7.6062464e-13
 1.3774520e-09 7.4353467e-14], sum to 1.0000
[2019-04-03 22:14:27,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6402
[2019-04-03 22:14:27,152] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.34747231279461, 0.4472415114791808, 0.0, 1.0, 42507.43949570083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4503000.0000, 
sim time next is 4503600.0000, 
raw observation next is [-1.0, 73.0, 0.0, 0.0, 26.0, 25.37820716584761, 0.4536743103513864, 0.0, 1.0, 38845.08339354803], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6148505971539674, 0.6512247701171288, 0.0, 1.0, 0.18497658758832397], 
reward next is 0.8150, 
noisyNet noise sample is [array([1.3138825], dtype=float32), -1.5913926]. 
=============================================
[2019-04-03 22:14:27,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1832447e-23 2.2259035e-12 6.0982198e-11 1.0000000e+00 4.3334376e-13
 1.0885864e-09 3.4637640e-14], sum to 1.0000
[2019-04-03 22:14:27,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-03 22:14:27,564] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9166666666666666, 73.0, 0.0, 0.0, 26.0, 25.37348421044976, 0.4342007444057552, 0.0, 1.0, 50378.31392951412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4506600.0000, 
sim time next is 4507200.0000, 
raw observation next is [-0.9, 73.0, 0.0, 0.0, 26.0, 25.37416586175607, 0.4260779547760612, 0.0, 1.0, 45315.33373559439], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6145138218130057, 0.6420259849253537, 0.0, 1.0, 0.21578730350283043], 
reward next is 0.7842, 
noisyNet noise sample is [array([-1.8708025], dtype=float32), 0.5021942]. 
=============================================
[2019-04-03 22:14:28,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3222573e-26 3.9059578e-15 1.2542512e-13 1.0000000e+00 4.0200734e-15
 5.4367972e-14 5.3456534e-17], sum to 1.0000
[2019-04-03 22:14:28,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9380
[2019-04-03 22:14:28,400] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 69.0, 0.0, 0.0, 26.0, 25.41576766497441, 0.4035105577577499, 0.0, 1.0, 51858.28422197361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4593600.0000, 
sim time next is 4594200.0000, 
raw observation next is [-1.583333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 25.34856445953804, 0.3996179270284876, 0.0, 1.0, 67012.6934579657], 
processed observation next is [1.0, 0.17391304347826086, 0.4187442289935365, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6123803716281699, 0.6332059756761625, 0.0, 1.0, 0.3191080640855509], 
reward next is 0.6809, 
noisyNet noise sample is [array([0.22177179], dtype=float32), -0.35976982]. 
=============================================
[2019-04-03 22:14:28,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9006522e-25 7.1001771e-14 1.4989263e-11 1.0000000e+00 1.1422505e-13
 5.7753524e-11 3.5801131e-15], sum to 1.0000
[2019-04-03 22:14:28,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9037
[2019-04-03 22:14:29,010] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.61494848365923, 0.5138844977000021, 0.0, 1.0, 18735.6487910902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58359069485099, 0.5047793785115325, 0.0, 1.0, 30181.95782463135], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6319658912375825, 0.6682597928371775, 0.0, 1.0, 0.14372360868872072], 
reward next is 0.8563, 
noisyNet noise sample is [array([1.1061444], dtype=float32), -0.124763586]. 
=============================================
[2019-04-03 22:14:36,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1381373e-27 2.3930727e-14 4.1651453e-13 1.0000000e+00 4.4075095e-13
 1.2168253e-11 9.2632072e-17], sum to 1.0000
[2019-04-03 22:14:36,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2735
[2019-04-03 22:14:36,148] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.2949911806831, 0.8653183832388179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.39803003979415, 0.8812215734781818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7831691699828459, 0.7937405244927273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14100182], dtype=float32), 0.4352073]. 
=============================================
[2019-04-03 22:14:49,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6461845e-24 1.9024767e-14 2.2068296e-12 1.0000000e+00 4.6896108e-14
 1.2951680e-11 9.2485939e-16], sum to 1.0000
[2019-04-03 22:14:49,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1734
[2019-04-03 22:14:49,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.21446660026619, 0.4183232410269039, 0.0, 1.0, 56619.91032928967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4823400.0000, 
sim time next is 4824000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.33007870057573, 0.431761796121312, 0.0, 1.0, 45613.18788537791], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6108398917146441, 0.643920598707104, 0.0, 1.0, 0.21720565659703767], 
reward next is 0.7828, 
noisyNet noise sample is [array([0.07333731], dtype=float32), -1.0486108]. 
=============================================
[2019-04-03 22:14:49,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.20432]
 [76.78217]
 [76.0422 ]
 [75.02413]
 [73.92376]], R is [[77.45253754]
 [77.40840149]
 [77.20786285]
 [76.65290833]
 [75.93959045]].
[2019-04-03 22:14:57,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3498928e-24 8.3535008e-15 2.2408939e-12 1.0000000e+00 4.2288611e-15
 9.5521424e-11 2.3005516e-15], sum to 1.0000
[2019-04-03 22:14:57,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1641
[2019-04-03 22:14:57,456] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78060795889647, 0.2227844876729617, 0.0, 1.0, 39435.71227414166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.7790003560034, 0.217262363353719, 0.0, 1.0, 39495.63758729546], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649166963336167, 0.572420787784573, 0.0, 1.0, 0.18807446470140696], 
reward next is 0.8119, 
noisyNet noise sample is [array([1.8941076], dtype=float32), -1.9866402]. 
=============================================
[2019-04-03 22:15:02,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8851243e-27 5.6104362e-15 2.4949140e-13 1.0000000e+00 8.3685188e-16
 1.2755091e-11 7.2388375e-17], sum to 1.0000
[2019-04-03 22:15:02,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8782
[2019-04-03 22:15:02,958] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.22759297626612, 0.2751161859718199, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953600.0000, 
sim time next is 4954200.0000, 
raw observation next is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18668349071388, 0.2710698352613869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.41181902123730385, 0.4483333333333334, 0.2066666666666667, 0.412523020257827, 0.6666666666666666, 0.5988902908928232, 0.5903566117537956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5893333], dtype=float32), -0.6015037]. 
=============================================
[2019-04-03 22:15:11,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:11,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:12,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-03 22:15:14,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:14,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:14,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-03 22:15:15,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3823685e-23 1.3660679e-13 1.0723469e-10 1.0000000e+00 2.1629684e-13
 1.8910618e-11 1.0732164e-14], sum to 1.0000
[2019-04-03 22:15:15,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9758
[2019-04-03 22:15:15,214] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03037806962126, 0.2197346755060496, 0.0, 1.0, 38671.46466260744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948800.0000, 
sim time next is 4949400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 24.97386009954149, 0.2106167006281967, 0.0, 1.0, 38699.36080466043], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5811550082951241, 0.5702055668760656, 0.0, 1.0, 0.18428267049838298], 
reward next is 0.8157, 
noisyNet noise sample is [array([-0.19874902], dtype=float32), 0.050735425]. 
=============================================
[2019-04-03 22:15:15,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3591912e-29 2.4959001e-16 1.7750256e-14 1.0000000e+00 6.8642724e-16
 8.4905455e-13 1.4945132e-18], sum to 1.0000
[2019-04-03 22:15:15,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-03 22:15:15,439] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.5, 25.5, 124.0, 865.0, 26.0, 27.44755140689433, 0.8705603298880525, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5056200.0000, 
sim time next is 5056800.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 123.0, 864.1666666666667, 26.0, 27.2090781293771, 0.8491683161207071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.7026777469990768, 0.2533333333333333, 0.41, 0.9548802946593002, 0.6666666666666666, 0.7674231774480917, 0.7830561053735691, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8966634], dtype=float32), 0.26424402]. 
=============================================
[2019-04-03 22:15:15,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:15,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:15,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-03 22:15:17,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:17,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:17,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-03 22:15:17,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:17,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:17,767] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-03 22:15:18,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:18,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:18,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-03 22:15:18,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:18,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:18,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-03 22:15:19,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:19,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:19,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-03 22:15:21,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:21,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:21,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-03 22:15:21,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:21,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:21,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-03 22:15:23,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:23,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:23,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-03 22:15:23,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:23,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:23,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-03 22:15:24,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4170259e-30 1.7321915e-18 3.2412531e-16 1.0000000e+00 1.5655442e-17
 3.5497451e-12 3.8768676e-20], sum to 1.0000
[2019-04-03 22:15:24,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4666
[2019-04-03 22:15:24,935] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10828845635121, 1.027266345899885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 26.0, 28.29822797726312, 1.057243774114189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.6666666666666666, 0.8581856647719267, 0.8524145913713963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47693363], dtype=float32), -0.39884573]. 
=============================================
[2019-04-03 22:15:25,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:25,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:25,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-03 22:15:25,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:25,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:26,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-03 22:15:27,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:27,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:27,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-03 22:15:28,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:15:28,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:15:28,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-03 22:15:30,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4361108e-26 1.4911442e-15 7.9157042e-13 1.0000000e+00 7.2443346e-17
 7.1613063e-12 9.1028414e-17], sum to 1.0000
[2019-04-03 22:15:30,687] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-03 22:15:30,746] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 20.0, 19.05927140476307, -1.007600876341694, 0.0, 1.0, 24371.50048321574], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 20.0, 19.06104134726149, -1.005430618297602, 0.0, 1.0, 27207.00570289432], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.16666666666666666, 0.08842011227179085, 0.16485646056746597, 0.0, 1.0, 0.12955717001378247], 
reward next is 0.8704, 
noisyNet noise sample is [array([0.53853977], dtype=float32), -0.5122813]. 
=============================================
[2019-04-03 22:15:30,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.42512 ]
 [84.409546]
 [84.40371 ]
 [84.38676 ]
 [84.3735  ]], R is [[84.48339844]
 [84.52250671]
 [84.56503296]
 [84.6091156 ]
 [84.65559387]].
[2019-04-03 22:15:33,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9059023e-14 9.3583337e-09 1.2670124e-06 9.9998951e-01 2.6851188e-08
 9.1932934e-06 1.3933023e-09], sum to 1.0000
[2019-04-03 22:15:33,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-03 22:15:33,197] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 80.0, 190.0, 36.0, 19.0, 19.43254462424228, -1.056666354230989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 124200.0000, 
sim time next is 124800.0000, 
raw observation next is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 19.0, 19.45672691733969, -1.081814955335701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188372, 0.82, 0.63, 0.03554327808471454, 0.08333333333333333, 0.12139390977830751, 0.13939501488809966, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24623089], dtype=float32), 1.5012543]. 
=============================================
[2019-04-03 22:15:38,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5362236e-20 9.7341059e-13 6.0846522e-10 1.0000000e+00 6.1917743e-14
 6.2222220e-11 6.8268843e-14], sum to 1.0000
[2019-04-03 22:15:38,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-03 22:15:38,097] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 18.65668316138253, -1.134381419650754, 0.0, 1.0, 102875.3299029185], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 19.0, 18.68027390928736, -1.127987612531068, 0.0, 1.0, 58024.80217010051], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.05668949244061346, 0.12400412915631069, 0.0, 1.0, 0.2763085817623834], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.44014844], dtype=float32), -0.22053841]. 
=============================================
[2019-04-03 22:15:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2933840e-23 1.4355621e-13 3.0134573e-09 9.9999845e-01 9.8494990e-15
 1.6073343e-06 2.8029178e-15], sum to 1.0000
[2019-04-03 22:15:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1971
[2019-04-03 22:15:47,424] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 23.0, 20.41049604780098, -0.8268500482111346, 0.0, 1.0, 46587.65766679077], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 196800.0000, 
sim time next is 197400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 23.0, 20.40964781201227, -0.8251587866057556, 0.0, 1.0, 46584.8980447763], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.4166666666666667, 0.20080398433435587, 0.2249470711314148, 0.0, 1.0, 0.22183284783226812], 
reward next is 0.7782, 
noisyNet noise sample is [array([-1.6129504], dtype=float32), 0.5817387]. 
=============================================
[2019-04-03 22:16:02,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9403431e-20 3.7509839e-13 1.2392080e-09 9.9955326e-01 9.1837614e-12
 4.4665986e-04 2.0717892e-14], sum to 1.0000
[2019-04-03 22:16:02,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-03 22:16:02,423] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.5, 46.0, 51.5, 859.5, 26.0, 25.91010263821143, 0.2660620358370725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 396000.0000, 
sim time next is 396600.0000, 
raw observation next is [-10.33333333333333, 45.0, 50.33333333333333, 850.3333333333334, 26.0, 25.38019348392116, 0.3107684025922671, 1.0, 1.0, 127983.7299234879], 
processed observation next is [1.0, 0.6086956521739131, 0.17636195752539252, 0.45, 0.16777777777777778, 0.9395948434622469, 0.6666666666666666, 0.6150161236600967, 0.6035894675307557, 1.0, 1.0, 0.60944633296899], 
reward next is 0.3906, 
noisyNet noise sample is [array([-0.8266647], dtype=float32), -0.43361422]. 
=============================================
[2019-04-03 22:16:09,449] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4240603e-21 2.3094578e-11 6.0545178e-09 9.9996889e-01 2.5018091e-12
 3.1124793e-05 1.4185890e-14], sum to 1.0000
[2019-04-03 22:16:09,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6038
[2019-04-03 22:16:09,496] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.99426684350549, 0.2538370231186862, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.93168332221417, 0.3416757941852078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 0.6666666666666666, 0.6609736101845142, 0.6138919313950693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18346977], dtype=float32), -0.67582136]. 
=============================================
[2019-04-03 22:16:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3680145e-21 1.0381709e-11 7.9566703e-08 9.9973875e-01 7.4799325e-12
 2.6116017e-04 7.6625514e-14], sum to 1.0000
[2019-04-03 22:16:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3859
[2019-04-03 22:16:11,623] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.58592601688154, 0.275071620490227, 1.0, 1.0, 49182.82244568649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 383400.0000, 
sim time next is 384000.0000, 
raw observation next is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.66429106449905, 0.2929477894255819, 1.0, 1.0, 48562.38125566705], 
processed observation next is [1.0, 0.43478260869565216, 0.08125577100646345, 0.62, 0.22944444444444448, 0.8119705340699815, 0.6666666666666666, 0.6386909220415875, 0.5976492631418606, 1.0, 1.0, 0.2312494345507955], 
reward next is 0.7688, 
noisyNet noise sample is [array([0.69266105], dtype=float32), -1.5776407]. 
=============================================
[2019-04-03 22:16:11,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.531784]
 [80.76628 ]
 [80.928635]
 [81.28952 ]
 [81.75765 ]], R is [[80.04890442]
 [80.01420593]
 [79.97950745]
 [79.95323944]
 [79.95785522]].
[2019-04-03 22:16:11,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9452146e-21 2.0000406e-11 5.8299062e-08 9.9977559e-01 6.1425425e-13
 2.2440075e-04 5.0516642e-14], sum to 1.0000
[2019-04-03 22:16:11,971] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-03 22:16:11,996] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.18301105731646, 0.05797876128959101, 0.0, 1.0, 44953.85630018304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.12602029763864, 0.04486284260288812, 0.0, 1.0, 44742.92017621888], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5105016914698867, 0.5149542808676294, 0.0, 1.0, 0.21306152464866135], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.00520158], dtype=float32), 0.099479735]. 
=============================================
[2019-04-03 22:16:14,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9192955e-20 1.1450057e-11 1.5821210e-07 9.9973792e-01 1.8956213e-11
 2.6191145e-04 2.3056391e-13], sum to 1.0000
[2019-04-03 22:16:14,119] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6153
[2019-04-03 22:16:14,219] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 26.0, 25.04542249145425, 0.2361770351010515, 1.0, 1.0, 122380.7200873134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415200.0000, 
sim time next is 415800.0000, 
raw observation next is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.00067968177372, 0.2505547425914198, 1.0, 1.0, 119126.4449882472], 
processed observation next is [1.0, 0.8260869565217391, 0.19252077562326872, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5833899734811432, 0.5835182475304733, 1.0, 1.0, 0.56726878565832], 
reward next is 0.4327, 
noisyNet noise sample is [array([0.9042378], dtype=float32), -1.1020639]. 
=============================================
[2019-04-03 22:16:17,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6840602e-22 4.5699603e-12 9.7708906e-08 9.9999845e-01 9.0103093e-13
 1.4730316e-06 5.0701776e-14], sum to 1.0000
[2019-04-03 22:16:17,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0608
[2019-04-03 22:16:17,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.76937678750893, -0.03343747451878076, 0.0, 1.0, 44860.31850849077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 429000.0000, 
sim time next is 429600.0000, 
raw observation next is [-11.7, 54.00000000000001, 0.0, 0.0, 26.0, 23.70752265049268, -0.04653859858804465, 0.0, 1.0, 44921.52853886635], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.47562688754105675, 0.4844871338039851, 0.0, 1.0, 0.21391204066126832], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.7261264], dtype=float32), 0.84283566]. 
=============================================
[2019-04-03 22:16:22,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.76583581e-27 6.94648147e-17 3.86595297e-13 1.00000000e+00
 1.13383044e-17 1.08016451e-09 8.37157486e-18], sum to 1.0000
[2019-04-03 22:16:22,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-03 22:16:22,248] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.68612328502263, -0.03489743001663027, 0.0, 1.0, 45028.88711352285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.67303296167939, -0.04589443114744567, 0.0, 1.0, 45086.27141969712], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.6666666666666666, 0.4727527468066158, 0.4847018562841848, 0.0, 1.0, 0.21469653056998628], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.04274149], dtype=float32), -0.034139834]. 
=============================================
[2019-04-03 22:16:24,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1245836e-38 2.6596193e-24 6.3945601e-19 1.0000000e+00 5.3520361e-26
 5.6413341e-17 3.0253512e-26], sum to 1.0000
[2019-04-03 22:16:24,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-03 22:16:24,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 26.0, 24.68974726963645, 0.1975613004210561, 0.0, 1.0, 39887.96828596338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.67057545210193, 0.1942317676544762, 0.0, 1.0, 39936.0801322546], 
processed observation next is [0.0, 0.13043478260869565, 0.5627885503231764, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5558812876751608, 0.5647439225514921, 0.0, 1.0, 0.19017181015359336], 
reward next is 0.8098, 
noisyNet noise sample is [array([0.60630476], dtype=float32), -3.1274295]. 
=============================================
[2019-04-03 22:16:26,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4624314e-31 8.8775993e-18 1.0234186e-13 1.0000000e+00 1.3299208e-20
 2.8392724e-09 1.0867797e-20], sum to 1.0000
[2019-04-03 22:16:26,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6271
[2019-04-03 22:16:26,389] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 87.0, 0.0, 0.0, 26.0, 24.94290251263538, 0.264474894879541, 0.0, 1.0, 51720.93042635026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 585600.0000, 
sim time next is 586200.0000, 
raw observation next is [-2.716666666666667, 87.0, 0.0, 0.0, 26.0, 24.92443697512989, 0.2646405982040636, 0.0, 1.0, 59047.6467075807], 
processed observation next is [0.0, 0.782608695652174, 0.3873499538319483, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5770364145941574, 0.5882135327346879, 0.0, 1.0, 0.2811792700360986], 
reward next is 0.7188, 
noisyNet noise sample is [array([0.9505388], dtype=float32), -0.12761484]. 
=============================================
[2019-04-03 22:16:30,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.16877624e-26 2.15309582e-13 3.17686033e-09 9.99991298e-01
 2.57502886e-15 8.67301969e-06 3.37739353e-16], sum to 1.0000
[2019-04-03 22:16:30,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2497
[2019-04-03 22:16:30,901] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.81700295900406, 0.2224968480667339, 0.0, 1.0, 40371.80208759144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.81828103885892, 0.222276513231635, 0.0, 1.0, 40231.70906538489], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5681900865715767, 0.5740921710772117, 0.0, 1.0, 0.1915795669780233], 
reward next is 0.8084, 
noisyNet noise sample is [array([-0.40705037], dtype=float32), -0.52178377]. 
=============================================
[2019-04-03 22:16:38,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2799732e-25 1.8177203e-14 3.2719532e-12 9.9999893e-01 1.3584579e-14
 1.0859575e-06 5.0091411e-18], sum to 1.0000
[2019-04-03 22:16:38,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0695
[2019-04-03 22:16:38,967] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 45.83333333333333, 86.0, 753.3333333333333, 26.0, 25.61056745034281, 0.3953140876461953, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 741000.0000, 
sim time next is 741600.0000, 
raw observation next is [0.5, 45.0, 84.5, 743.5, 26.0, 25.65844049976323, 0.4028530785094862, 1.0, 1.0, 18680.53207983417], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.2816666666666667, 0.8215469613259668, 0.6666666666666666, 0.6382033749802692, 0.6342843595031621, 1.0, 1.0, 0.088954914665877], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.2359022], dtype=float32), -0.3229327]. 
=============================================
[2019-04-03 22:16:40,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2809313e-18 1.8455982e-09 1.3637282e-06 9.9423343e-01 5.1544286e-11
 5.7651508e-03 3.6856157e-11], sum to 1.0000
[2019-04-03 22:16:40,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7484
[2019-04-03 22:16:40,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.48517100765816, 0.1564467438511813, 0.0, 1.0, 42245.87461447751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [-3.816666666666666, 86.16666666666667, 0.0, 0.0, 26.0, 24.4570267670837, 0.1487692150708797, 0.0, 1.0, 42214.6955861192], 
processed observation next is [0.0, 0.0, 0.3568790397045245, 0.8616666666666667, 0.0, 0.0, 0.6666666666666666, 0.5380855639236417, 0.5495897383569599, 0.0, 1.0, 0.20102235993390094], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.7960378], dtype=float32), 0.035600465]. 
=============================================
[2019-04-03 22:16:44,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2898448e-25 4.7104268e-14 2.6802272e-10 1.0000000e+00 6.4760952e-16
 7.4691089e-09 1.6315461e-16], sum to 1.0000
[2019-04-03 22:16:44,800] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4181
[2019-04-03 22:16:44,894] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.87857647065, 0.2197897303132198, 0.0, 1.0, 38456.47899494076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 649200.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 26.0, 24.87843191401977, 0.2221090729233982, 0.0, 1.0, 40993.5420309503], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.6666666666666666, 0.5732026595016476, 0.5740363576411327, 0.0, 1.0, 0.19520734300452525], 
reward next is 0.8048, 
noisyNet noise sample is [array([-1.6885734], dtype=float32), 0.43034628]. 
=============================================
[2019-04-03 22:16:49,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0287986e-22 6.3714762e-13 1.7022181e-10 9.9999869e-01 4.6823096e-15
 1.2620507e-06 2.5210551e-15], sum to 1.0000
[2019-04-03 22:16:49,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-03 22:16:49,762] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.34695011375173, 0.08876294628961734, 0.0, 1.0, 40965.24069361052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693600.0000, 
sim time next is 694200.0000, 
raw observation next is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 26.0, 24.38514630873762, 0.08613502556598851, 0.0, 1.0, 40952.07469223085], 
processed observation next is [1.0, 0.0, 0.3661126500461681, 0.7183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5320955257281351, 0.5287116751886628, 0.0, 1.0, 0.1950098794868136], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.06884918], dtype=float32), 1.30004]. 
=============================================
[2019-04-03 22:16:51,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9904814e-24 1.5597042e-13 2.0737743e-09 9.9999523e-01 2.3297429e-14
 4.8090992e-06 1.8949880e-15], sum to 1.0000
[2019-04-03 22:16:51,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4018
[2019-04-03 22:16:51,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.0, 46.5, 0.0, 26.0, 25.75443385416356, 0.2973692307839985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [-6.2, 75.0, 51.33333333333334, 0.0, 26.0, 25.68545752777084, 0.2943336030284032, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.17111111111111113, 0.0, 0.6666666666666666, 0.6404547939809033, 0.5981112010094677, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4974309], dtype=float32), -0.8151575]. 
=============================================
[2019-04-03 22:17:04,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7996550e-28 2.3096492e-18 2.1260085e-14 1.0000000e+00 1.7969538e-17
 4.9546225e-11 2.2483009e-19], sum to 1.0000
[2019-04-03 22:17:04,059] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3166
[2019-04-03 22:17:04,131] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 26.0, 25.71326417793728, 0.3012067277155048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 812400.0000, 
sim time next is 813000.0000, 
raw observation next is [-6.2, 75.0, 69.66666666666666, 0.0, 26.0, 25.72017678865073, 0.3000307164153063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.2322222222222222, 0.0, 0.6666666666666666, 0.6433480657208941, 0.6000102388051021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7832957], dtype=float32), -0.039762788]. 
=============================================
[2019-04-03 22:17:04,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.12052 ]
 [86.92408 ]
 [86.67802 ]
 [86.480774]
 [86.406075]], R is [[87.36864471]
 [87.49495697]
 [87.62001038]
 [87.74381256]
 [87.86637878]].
[2019-04-03 22:17:04,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1506474e-25 3.4404367e-13 6.6079381e-10 9.9999976e-01 1.7050393e-15
 2.3922956e-07 6.9621349e-16], sum to 1.0000
[2019-04-03 22:17:04,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4571
[2019-04-03 22:17:04,683] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.2, 79.83333333333334, 0.0, 0.0, 26.0, 24.74225773459167, 0.2112167281581119, 0.0, 1.0, 39517.026670036], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868200.0000, 
sim time next is 868800.0000, 
raw observation next is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.73596133760008, 0.2051175810114657, 0.0, 1.0, 39486.99546173165], 
processed observation next is [1.0, 0.043478260869565216, 0.404432132963989, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5613301114666734, 0.5683725270038219, 0.0, 1.0, 0.18803331172253165], 
reward next is 0.8120, 
noisyNet noise sample is [array([-1.2237152], dtype=float32), 0.76394653]. 
=============================================
[2019-04-03 22:17:08,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3405858e-24 8.5893629e-13 4.0996520e-10 9.9997461e-01 2.8991258e-14
 2.5347992e-05 2.2603652e-15], sum to 1.0000
[2019-04-03 22:17:08,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5161
[2019-04-03 22:17:08,792] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.816666666666666, 85.5, 0.0, 0.0, 26.0, 25.30559909798084, 0.3643785075882966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 846600.0000, 
sim time next is 847200.0000, 
raw observation next is [-3.733333333333333, 85.0, 0.0, 0.0, 26.0, 25.43659273337963, 0.3549650302180143, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.35918744228993543, 0.85, 0.0, 0.0, 0.6666666666666666, 0.6197160611149691, 0.6183216767393381, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1606513], dtype=float32), -0.40842864]. 
=============================================
[2019-04-03 22:17:12,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.63850676e-31 2.58300378e-20 1.15467879e-14 1.00000000e+00
 1.29020869e-20 1.20353533e-10 1.08407585e-20], sum to 1.0000
[2019-04-03 22:17:12,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7782
[2019-04-03 22:17:12,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.7274029831801, 0.6525049118480025, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.67769361429676, 0.6479754679928201, 0.0, 1.0, 67970.44993549299], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6398078011913967, 0.7159918226642734, 0.0, 1.0, 0.32366880921663327], 
reward next is 0.6763, 
noisyNet noise sample is [array([2.1426666], dtype=float32), 0.41401905]. 
=============================================
[2019-04-03 22:17:17,632] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:17:17,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:17:17,637] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:17:17,638] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,638] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:17,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:17:17,675] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:17:17,694] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-03 22:17:52,380] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:17:52,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.16666666666667, 53.33333333333333, 172.8333333333333, 379.0, 23.0, 22.22642249275395, -0.4510259707987962, 0.0, 1.0, 38227.44632065635]
[2019-04-03 22:17:52,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:17:52,382] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.1669586e-15 1.1159206e-08 2.1298765e-06 9.9900085e-01 6.5590076e-09
 9.9705008e-04 4.9926929e-10], sampled 0.61271754442895
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.4, 76.33333333333334, 0.0, 0.0, 25.0, 25.04586668148599, 0.4167187513553286, 0.0, 1.0, 0.0]
[2019-04-03 22:18:18,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:18:18,849] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9597152e-28 2.4198569e-16 2.1880028e-12 1.0000000e+00 2.6800168e-18
 2.4735742e-08 1.5823488e-18], sampled 0.11482232297714012
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.366666666666667, 76.66666666666667, 0.0, 0.0, 24.0, 23.1932538557667, 0.01725746420882547, 0.0, 1.0, 23875.77563522308]
[2019-04-03 22:18:30,022] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:18:30,027] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8155863e-21 2.9536751e-12 2.3022944e-09 9.9998748e-01 1.8961034e-13
 1.2536199e-05 2.0868668e-14], sampled 0.6683177126599659
[2019-04-03 22:19:54,242] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7424.8829 218147676.0016 677.4138
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.018722428000001, 77.88229066666668, 0.0, 0.0, 26.0, 25.19804215822753, 0.3370629751400994, 0.0, 1.0, 40812.88606177432]
[2019-04-03 22:19:59,777] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:19:59,778] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2618236e-25 1.4089550e-14 2.6387481e-11 1.0000000e+00 4.1880660e-16
 3.8664641e-08 7.7771084e-17], sampled 0.6337944894959395
[2019-04-03 22:20:06,886] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.54348725], dtype=float32), 0.41790733]
[2019-04-03 22:20:06,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.585523663, 21.65124907, 92.70500256, 733.91434355, 26.0, 27.05068343441062, 0.4538748360923122, 1.0, 1.0, 0.0]
[2019-04-03 22:20:06,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:20:06,888] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.87398132e-17 5.47338730e-10 1.15189394e-07 9.95192885e-01
 1.00535524e-09 4.80697490e-03 5.97654435e-12], sampled 0.5809684586131342
[2019-04-03 22:20:18,088] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7259.8962 251731141.4290 953.2711
[2019-04-03 22:20:26,363] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7160.8238 273251237.2589 1158.7003
[2019-04-03 22:20:27,384] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 300000, evaluation results [300000.0, 7259.896238201633, 251731141.42904967, 953.271083148114, 7424.882935288908, 218147676.0015941, 677.4137627110454, 7160.8238393580905, 273251237.2588752, 1158.7003355112215]
[2019-04-03 22:20:28,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4435694e-30 7.0599038e-19 1.0268045e-14 1.0000000e+00 5.1584188e-20
 7.0970063e-11 7.3158584e-21], sum to 1.0000
[2019-04-03 22:20:28,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9371
[2019-04-03 22:20:28,415] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.63991022405189, 0.6312112614816175, 0.0, 1.0, 26274.72273056874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1133400.0000, 
sim time next is 1134000.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64434941561353, 0.6308576074107844, 0.0, 1.0, 22598.60353281325], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6370291179677942, 0.7102858691369281, 0.0, 1.0, 0.10761239777530118], 
reward next is 0.8924, 
noisyNet noise sample is [array([0.03446808], dtype=float32), -0.27545753]. 
=============================================
[2019-04-03 22:20:28,435] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[95.69215]
 [95.88763]
 [96.11046]
 [96.36439]
 [96.74011]], R is [[95.40644073]
 [95.32726288]
 [95.22982025]
 [95.12133026]
 [95.0210495 ]].
[2019-04-03 22:20:30,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2445791e-32 3.3442641e-19 7.7373749e-15 1.0000000e+00 2.8549518e-19
 1.7786353e-10 4.6355068e-21], sum to 1.0000
[2019-04-03 22:20:30,441] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4957
[2019-04-03 22:20:30,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40220278208122, 0.4226676693418523, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33119803865171, 0.4847949739308919, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6109331698876425, 0.6615983246436307, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0631709], dtype=float32), -1.464973]. 
=============================================
[2019-04-03 22:20:30,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2092222e-29 2.5227028e-15 5.2235393e-11 9.9999976e-01 5.5289917e-17
 2.9618118e-07 2.1073286e-17], sum to 1.0000
[2019-04-03 22:20:30,652] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-03 22:20:30,658] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.06666666666667, 72.33333333333334, 74.33333333333334, 0.0, 26.0, 25.71542981289831, 0.6167583295330389, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1156800.0000, 
sim time next is 1157400.0000, 
raw observation next is [16.35, 71.0, 83.0, 0.0, 26.0, 25.72409331151405, 0.596403231240188, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.9155124653739612, 0.71, 0.27666666666666667, 0.0, 0.6666666666666666, 0.6436744426261708, 0.6988010770800627, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02947584], dtype=float32), -1.8765999]. 
=============================================
[2019-04-03 22:20:35,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1194400e-30 5.2369423e-17 1.6968141e-13 1.0000000e+00 1.2473312e-19
 1.1200993e-10 1.3463683e-19], sum to 1.0000
[2019-04-03 22:20:35,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8526
[2019-04-03 22:20:35,571] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 59.5, 0.0, 0.0, 26.0, 25.8394410856923, 0.6691128330902664, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1108200.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 26.0, 25.78541751704458, 0.6558936188490198, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6487847930870482, 0.7186312062830066, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30600098], dtype=float32), 0.6322775]. 
=============================================
[2019-04-03 22:20:37,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9414912e-30 1.0539434e-17 7.1673387e-14 1.0000000e+00 2.4749231e-20
 6.3294966e-11 8.0608447e-20], sum to 1.0000
[2019-04-03 22:20:37,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0574
[2019-04-03 22:20:37,980] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 82.0, 0.0, 0.0, 26.0, 25.66460151724338, 0.6086740949482573, 0.0, 1.0, 38387.56249333372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1144200.0000, 
sim time next is 1144800.0000, 
raw observation next is [11.6, 83.0, 0.0, 0.0, 26.0, 25.64111268022508, 0.6089412737128503, 0.0, 1.0, 42518.35617403167], 
processed observation next is [0.0, 0.2608695652173913, 0.7839335180055402, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6367593900187568, 0.7029804245709501, 0.0, 1.0, 0.20246836273348415], 
reward next is 0.7975, 
noisyNet noise sample is [array([-1.0524641], dtype=float32), 1.0800871]. 
=============================================
[2019-04-03 22:20:42,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2415902e-28 7.6779196e-16 2.5069374e-11 1.0000000e+00 2.7543832e-18
 1.8672604e-10 4.0669117e-18], sum to 1.0000
[2019-04-03 22:20:42,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5756
[2019-04-03 22:20:42,497] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.38417861753091, 0.5551270432705414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62404610312917, 0.5844389785400343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6353371752607643, 0.6948129928466781, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84835106], dtype=float32), -1.3891525]. 
=============================================
[2019-04-03 22:20:42,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.34056 ]
 [80.69607 ]
 [80.743484]
 [80.89929 ]
 [81.02605 ]], R is [[85.34168243]
 [85.48826599]
 [85.36221313]
 [85.373703  ]
 [85.4200058 ]].
[2019-04-03 22:20:46,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3609217e-29 1.1147245e-17 1.4984809e-14 1.0000000e+00 2.8239196e-19
 3.4709497e-12 2.0058228e-19], sum to 1.0000
[2019-04-03 22:20:46,594] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5286
[2019-04-03 22:20:46,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38777654639577, 0.4532130883135812, 0.0, 1.0, 63617.75973189619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 26.0, 25.32698521203524, 0.4494903281876955, 0.0, 1.0, 57514.96604075958], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.6666666666666666, 0.6105821010029366, 0.6498301093958986, 0.0, 1.0, 0.2738807906702837], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.5511986], dtype=float32), 0.66954476]. 
=============================================
[2019-04-03 22:20:47,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2937180e-27 1.6669901e-17 9.9775860e-14 1.0000000e+00 4.8995909e-18
 1.0003345e-10 2.9662125e-19], sum to 1.0000
[2019-04-03 22:20:47,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1518
[2019-04-03 22:20:47,752] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.08333333333333333, 94.5, 94.0, 0.0, 26.0, 25.68167910411987, 0.4800532192026494, 1.0, 1.0, 18681.88250266992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1426200.0000, 
sim time next is 1426800.0000, 
raw observation next is [0.1666666666666667, 94.0, 95.0, 0.0, 26.0, 25.70004459795155, 0.4718279441764028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.94, 0.31666666666666665, 0.0, 0.6666666666666666, 0.6416703831626291, 0.6572759813921343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0455797], dtype=float32), 0.76717645]. 
=============================================
[2019-04-03 22:20:51,061] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0134703e-31 8.0763919e-19 2.0167841e-14 1.0000000e+00 5.6661119e-18
 9.1463788e-11 1.2649491e-21], sum to 1.0000
[2019-04-03 22:20:51,062] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7963
[2019-04-03 22:20:51,069] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.5, 58.0, 44.5, 17.0, 26.0, 26.46707234463368, 0.7102408105034711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1530000.0000, 
sim time next is 1530600.0000, 
raw observation next is [10.41666666666667, 58.33333333333334, 30.33333333333333, 13.33333333333333, 26.0, 26.75724291778535, 0.7293998778251948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7511542012927056, 0.5833333333333335, 0.1011111111111111, 0.0147329650092081, 0.6666666666666666, 0.7297702431487793, 0.7431332926083982, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20428471], dtype=float32), 0.10775268]. 
=============================================
[2019-04-03 22:20:57,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9990975e-32 3.2929026e-21 6.7217001e-17 1.0000000e+00 4.3603740e-21
 6.9280465e-14 3.1251097e-22], sum to 1.0000
[2019-04-03 22:20:57,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3801
[2019-04-03 22:20:57,906] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.083333333333334, 81.5, 13.0, 15.0, 26.0, 25.47043129174267, 0.4726860147856939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [5.0, 82.0, 19.0, 20.0, 26.0, 25.40772047358766, 0.458382580079626, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6011080332409973, 0.82, 0.06333333333333334, 0.022099447513812154, 0.6666666666666666, 0.6173100394656382, 0.6527941933598753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8574866], dtype=float32), -0.25961852]. 
=============================================
[2019-04-03 22:20:57,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.450836]
 [90.974525]
 [89.861885]
 [88.07895 ]
 [85.240585]], R is [[91.54641724]
 [91.63095093]
 [91.71464539]
 [91.79750061]
 [91.87952423]].
[2019-04-03 22:20:58,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9300183e-27 7.6158413e-17 2.2792705e-13 1.0000000e+00 2.5099533e-17
 2.8320990e-10 1.6299271e-18], sum to 1.0000
[2019-04-03 22:20:58,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7249
[2019-04-03 22:20:58,561] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83539158987032, 0.7473501309004229, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 26.0, 26.82755223753324, 0.7600501140400552, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.6666666666666666, 0.73562935312777, 0.7533500380133518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.11342], dtype=float32), -0.0152848065]. 
=============================================
[2019-04-03 22:20:59,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4717524e-26 3.1336667e-16 1.5818128e-13 1.0000000e+00 2.7420558e-17
 2.6841560e-10 1.1988748e-18], sum to 1.0000
[2019-04-03 22:20:59,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-03 22:20:59,377] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 25.41226560175919, 0.4574972546341039, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 26.0, 25.38365776171205, 0.4591118778428754, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.6666666666666666, 0.6153048134760043, 0.6530372926142918, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.4967275], dtype=float32), 0.47590578]. 
=============================================
[2019-04-03 22:21:02,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1504861e-22 6.8264323e-13 6.7995887e-12 1.0000000e+00 2.4693289e-14
 1.7557774e-08 3.0174615e-15], sum to 1.0000
[2019-04-03 22:21:02,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-03 22:21:02,469] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 87.0, 53.33333333333334, 0.0, 26.0, 25.02576579451155, 0.332379098737996, 0.0, 1.0, 44761.61356151287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1785000.0000, 
sim time next is 1785600.0000, 
raw observation next is [-3.4, 87.0, 47.0, 0.0, 26.0, 24.99753993508586, 0.3295337962093232, 0.0, 1.0, 61522.18678487505], 
processed observation next is [0.0, 0.6956521739130435, 0.368421052631579, 0.87, 0.15666666666666668, 0.0, 0.6666666666666666, 0.5831283279238217, 0.609844598736441, 0.0, 1.0, 0.2929627942136907], 
reward next is 0.7070, 
noisyNet noise sample is [array([-1.9567055], dtype=float32), -0.6184525]. 
=============================================
[2019-04-03 22:21:02,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.65731456e-24 1.00047745e-14 3.53897966e-10 1.00000000e+00
 5.55451469e-15 1.66596141e-08 2.00276745e-16], sum to 1.0000
[2019-04-03 22:21:02,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9114
[2019-04-03 22:21:02,807] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.72573492507746, 0.5037387735408272, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.73444344026926, 0.4980592411199457, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.6445369533557717, 0.6660197470399819, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20470361], dtype=float32), 0.93366563]. 
=============================================
[2019-04-03 22:21:11,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1654102e-23 3.7289985e-14 7.5866979e-10 1.0000000e+00 9.0733830e-15
 1.1922263e-09 3.9530858e-15], sum to 1.0000
[2019-04-03 22:21:11,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7430
[2019-04-03 22:21:11,839] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.63951670093263, -0.02195029784625067, 0.0, 1.0, 47079.56348664278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1835400.0000, 
sim time next is 1836000.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.60217305157551, -0.02958515412175255, 0.0, 1.0, 47076.67131546295], 
processed observation next is [0.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.46684775429795905, 0.49013828195941583, 0.0, 1.0, 0.22417462531172833], 
reward next is 0.7758, 
noisyNet noise sample is [array([0.04495563], dtype=float32), -1.0966325]. 
=============================================
[2019-04-03 22:21:11,908] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.11649]
 [68.18373]
 [68.24748]
 [68.31494]
 [68.39797]], R is [[68.14784241]
 [68.24217224]
 [68.33558655]
 [68.4280014 ]
 [68.51939392]].
[2019-04-03 22:21:27,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8796005e-22 2.2517067e-13 8.0726828e-11 1.0000000e+00 2.4549681e-14
 7.1471664e-09 1.0041782e-15], sum to 1.0000
[2019-04-03 22:21:27,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7890
[2019-04-03 22:21:27,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.64587652648017, 0.159077284919021, 0.0, 1.0, 44815.24171330706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1893600.0000, 
sim time next is 1894200.0000, 
raw observation next is [-6.383333333333334, 75.66666666666667, 0.0, 0.0, 26.0, 24.60780213357963, 0.1509926407095598, 0.0, 1.0, 44842.53067229597], 
processed observation next is [0.0, 0.9565217391304348, 0.28578024007386893, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5506501777983024, 0.55033088023652, 0.0, 1.0, 0.21353586034426653], 
reward next is 0.7865, 
noisyNet noise sample is [array([0.24620545], dtype=float32), 0.09420632]. 
=============================================
[2019-04-03 22:21:56,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3295354e-27 6.6265836e-17 1.0448770e-12 1.0000000e+00 1.4581515e-17
 1.8892990e-11 1.0552141e-18], sum to 1.0000
[2019-04-03 22:21:56,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8356
[2019-04-03 22:21:56,206] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 61.0, 0.0, 0.0, 26.0, 25.02454089909659, 0.3142103091947624, 0.0, 1.0, 38590.85011670386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.96794114898626, 0.3067616322479531, 0.0, 1.0, 38543.06911340538], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5806617624155216, 0.6022538774159844, 0.0, 1.0, 0.18353842434954942], 
reward next is 0.8165, 
noisyNet noise sample is [array([1.3051946], dtype=float32), 0.5427359]. 
=============================================
[2019-04-03 22:21:56,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.561035]
 [82.55725 ]
 [82.240616]
 [82.377144]
 [82.501526]], R is [[82.82115173]
 [82.80917358]
 [82.79708862]
 [82.78516388]
 [82.7730484 ]].
[2019-04-03 22:21:57,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2216242e-26 1.8392034e-16 2.7871624e-12 1.0000000e+00 6.4147473e-17
 6.0411194e-12 1.8071069e-17], sum to 1.0000
[2019-04-03 22:21:57,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1658
[2019-04-03 22:21:57,322] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58402140984873, -0.03032781081132568, 0.0, 1.0, 43230.6737921388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.59982724784562, -0.03527179495094892, 0.0, 1.0, 43205.47408206232], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4666522706538017, 0.48824273501635035, 0.0, 1.0, 0.20574035277172534], 
reward next is 0.7943, 
noisyNet noise sample is [array([0.72634935], dtype=float32), 2.04019]. 
=============================================
[2019-04-03 22:21:57,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.862335]
 [76.92553 ]
 [76.98861 ]
 [77.053474]
 [77.12313 ]], R is [[76.82196045]
 [76.8478775 ]
 [76.87348175]
 [76.89868927]
 [76.92340851]].
[2019-04-03 22:21:57,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4468468e-24 3.7142198e-14 1.5942206e-11 1.0000000e+00 4.7295975e-16
 1.7581214e-10 5.1003935e-16], sum to 1.0000
[2019-04-03 22:21:57,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9173
[2019-04-03 22:21:57,875] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 23.97068898303435, 0.05384602451171985, 0.0, 1.0, 41698.10390135674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2360400.0000, 
sim time next is 2361000.0000, 
raw observation next is [-3.4, 69.0, 13.66666666666666, 0.0, 26.0, 23.93886963541086, 0.04869084017571013, 0.0, 1.0, 41788.31698511787], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.04555555555555554, 0.0, 0.6666666666666666, 0.494905802950905, 0.5162302800585701, 0.0, 1.0, 0.1989919856434184], 
reward next is 0.8010, 
noisyNet noise sample is [array([-1.1953832], dtype=float32), 0.48229215]. 
=============================================
[2019-04-03 22:21:57,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.828636]
 [69.73816 ]
 [69.77195 ]
 [69.79737 ]
 [69.82664 ]], R is [[70.03886414]
 [70.13991547]
 [70.24037933]
 [70.34026337]
 [70.43952942]].
[2019-04-03 22:21:58,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9429519e-29 2.5307877e-17 5.3981725e-14 1.0000000e+00 1.0131567e-19
 2.2894704e-13 1.2569367e-19], sum to 1.0000
[2019-04-03 22:21:58,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2658
[2019-04-03 22:21:58,205] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 24.0, 18.0, 26.0, 25.05128418999058, 0.2442015924792174, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2275200.0000, 
sim time next is 2275800.0000, 
raw observation next is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 26.0, 25.13682705899164, 0.2465606549482225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20452446906740537, 0.9033333333333334, 0.10333333333333333, 0.01915285451197053, 0.6666666666666666, 0.5947355882493032, 0.5821868849827408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77536905], dtype=float32), -1.5651101]. 
=============================================
[2019-04-03 22:22:00,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8087964e-24 3.5754048e-15 2.9955548e-12 1.0000000e+00 7.1337125e-16
 9.0906593e-10 1.7001288e-16], sum to 1.0000
[2019-04-03 22:22:00,449] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-03 22:22:00,489] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.9, 42.5, 0.0, 0.0, 26.0, 24.96447788899392, 0.2773627122773961, 0.0, 1.0, 86666.31466195334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2403000.0000, 
sim time next is 2403600.0000, 
raw observation next is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 26.0, 25.02037178341049, 0.2837608017957658, 0.0, 1.0, 58112.46398412666], 
processed observation next is [0.0, 0.8260869565217391, 0.3776546629732226, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5850309819508741, 0.594586933931922, 0.0, 1.0, 0.2767260189720317], 
reward next is 0.7233, 
noisyNet noise sample is [array([1.0304434], dtype=float32), -0.43263054]. 
=============================================
[2019-04-03 22:22:06,115] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7325923e-24 2.5187910e-14 1.9907720e-11 1.0000000e+00 1.1069751e-15
 2.0920314e-09 3.8659470e-16], sum to 1.0000
[2019-04-03 22:22:06,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0869
[2019-04-03 22:22:06,139] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.10634198163038, 0.2785274025836397, 0.0, 1.0, 43114.55555594756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2408400.0000, 
sim time next is 2409000.0000, 
raw observation next is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.15338423156319, 0.2738766417485896, 0.0, 1.0, 43056.80064673977], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5961153526302659, 0.5912922139161966, 0.0, 1.0, 0.20503238403209414], 
reward next is 0.7950, 
noisyNet noise sample is [array([-1.6776102], dtype=float32), 1.002671]. 
=============================================
[2019-04-03 22:22:06,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.967224]
 [76.194   ]
 [76.332214]
 [76.4483  ]
 [76.59618 ]], R is [[75.81467438]
 [75.85121918]
 [75.88720703]
 [75.92282104]
 [75.95829773]].
[2019-04-03 22:22:15,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6345135e-25 3.9175247e-14 3.6457690e-12 1.0000000e+00 1.9236490e-16
 1.3916510e-10 5.9443608e-17], sum to 1.0000
[2019-04-03 22:22:15,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0626
[2019-04-03 22:22:15,441] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.28140541650853, 0.2746904531245379, 0.0, 1.0, 40010.71337667303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.26141562246207, 0.2773260448066114, 0.0, 1.0, 40052.73218226815], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6051179685385059, 0.592442014935537, 0.0, 1.0, 0.1907272961060388], 
reward next is 0.8093, 
noisyNet noise sample is [array([-2.582822], dtype=float32), 0.016694287]. 
=============================================
[2019-04-03 22:22:20,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6662877e-28 7.0343952e-18 5.0473081e-14 1.0000000e+00 4.1271130e-18
 2.4455361e-11 2.6438559e-19], sum to 1.0000
[2019-04-03 22:22:20,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-03 22:22:20,128] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80632016257758, 0.2561584659181442, 0.0, 1.0, 41898.11738892963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592000.0000, 
sim time next is 2592600.0000, 
raw observation next is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76994392610926, 0.2475436148764931, 0.0, 1.0, 41906.38832614882], 
processed observation next is [1.0, 0.0, 0.3356417359187443, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5641619938424384, 0.5825145382921644, 0.0, 1.0, 0.1995542301245182], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.692053], dtype=float32), 0.3164852]. 
=============================================
[2019-04-03 22:22:30,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7176366e-24 1.6802006e-14 9.5865837e-10 9.9999940e-01 1.5833987e-14
 5.7285206e-07 2.2834138e-15], sum to 1.0000
[2019-04-03 22:22:30,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4578
[2019-04-03 22:22:30,113] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.75964523307044, 0.3971659804958405, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82046626200415, 0.3767225540135963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.6666666666666666, 0.6517055218336791, 0.6255741846711987, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.43434], dtype=float32), -0.48729134]. 
=============================================
[2019-04-03 22:22:31,047] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6075720e-23 4.6030676e-13 6.0067940e-10 1.0000000e+00 3.2541037e-14
 3.8002503e-09 4.2287480e-15], sum to 1.0000
[2019-04-03 22:22:31,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4052
[2019-04-03 22:22:31,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.47655265074362, 0.149315234138898, 0.0, 1.0, 40796.05793726338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48117584916071, 0.1409724573615548, 0.0, 1.0, 40847.37888138447], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5400979874300592, 0.5469908191205183, 0.0, 1.0, 0.19451132800659274], 
reward next is 0.8055, 
noisyNet noise sample is [array([0.7599484], dtype=float32), -1.0661882]. 
=============================================
[2019-04-03 22:22:39,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8092054e-24 1.5994965e-14 6.8941533e-11 1.0000000e+00 3.1773614e-16
 1.2198622e-08 1.8053991e-16], sum to 1.0000
[2019-04-03 22:22:39,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4245
[2019-04-03 22:22:39,091] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 24.93948616950559, 0.3063349771050075, 0.0, 1.0, 55413.06276508685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2852400.0000, 
sim time next is 2853000.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 24.96307443809051, 0.3107566846702643, 0.0, 1.0, 54863.71436822393], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.580256203174209, 0.6035855615567548, 0.0, 1.0, 0.2612557827058283], 
reward next is 0.7387, 
noisyNet noise sample is [array([-1.4005916], dtype=float32), -0.4736322]. 
=============================================
[2019-04-03 22:22:39,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.30037 ]
 [75.878105]
 [75.35632 ]
 [74.561455]
 [74.563705]], R is [[76.68943024]
 [76.65866852]
 [76.63072968]
 [76.61586761]
 [76.62800598]].
[2019-04-03 22:23:04,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2086226e-24 2.5029080e-16 6.5101136e-13 1.0000000e+00 1.5073788e-14
 3.7802974e-09 1.5616255e-17], sum to 1.0000
[2019-04-03 22:23:04,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-03 22:23:04,955] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66893931695246, 0.7655215966764467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.6957184173755, 0.7607620507813907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.6666666666666666, 0.7246432014479582, 0.7535873502604636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59046274], dtype=float32), 0.23597004]. 
=============================================
[2019-04-03 22:23:11,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4593379e-23 9.4266276e-14 2.7053873e-11 1.0000000e+00 2.7530455e-13
 1.8401302e-08 3.6732693e-16], sum to 1.0000
[2019-04-03 22:23:11,179] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9208
[2019-04-03 22:23:11,200] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.51386480720796, 0.4922352817145849, 1.0, 1.0, 123347.7630408472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346800.0000, 
sim time next is 3347400.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.42584275866513, 0.5135991016301592, 1.0, 1.0, 74180.45497006456], 
processed observation next is [1.0, 0.7391304347826086, 0.3841181902123731, 0.5416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6188202298887608, 0.6711997005433864, 1.0, 1.0, 0.3532402617622122], 
reward next is 0.6468, 
noisyNet noise sample is [array([1.0882928], dtype=float32), 1.5337119]. 
=============================================
[2019-04-03 22:23:14,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2577139e-24 3.5173711e-14 1.3066875e-09 1.0000000e+00 6.0778167e-15
 1.7337001e-09 4.3682265e-16], sum to 1.0000
[2019-04-03 22:23:14,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9680
[2019-04-03 22:23:14,778] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.05023902424551, 0.4001330880597533, 0.0, 1.0, 43632.3611321646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3285000.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 26.0, 25.0485995889534, 0.4026679784151452, 0.0, 1.0, 43670.74878321346], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5873832990794501, 0.6342226594717151, 0.0, 1.0, 0.20795594658673075], 
reward next is 0.7920, 
noisyNet noise sample is [array([-1.5459788], dtype=float32), -1.5793445]. 
=============================================
[2019-04-03 22:23:15,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4160151e-23 5.0796715e-14 1.3845269e-09 1.0000000e+00 3.5246356e-15
 2.6868108e-09 9.7392536e-16], sum to 1.0000
[2019-04-03 22:23:15,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-03 22:23:15,292] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.60621044045154, 0.5234755889091764, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.5647155376764, 0.5089443941607962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6303929614730333, 0.669648131386932, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.152766], dtype=float32), -0.08104691]. 
=============================================
[2019-04-03 22:23:18,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1884543e-30 3.0833377e-19 4.3456395e-16 1.0000000e+00 1.6889038e-20
 3.4136034e-14 1.3695583e-21], sum to 1.0000
[2019-04-03 22:23:18,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-03 22:23:18,504] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 67.0, 0.0, 0.0, 26.0, 25.44244056177675, 0.4496200503034353, 0.0, 1.0, 34380.183109109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3363600.0000, 
sim time next is 3364200.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.40864610147992, 0.4361427112349051, 0.0, 1.0, 53575.49267001326], 
processed observation next is [1.0, 0.9565217391304348, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6173871751233267, 0.6453809037449684, 0.0, 1.0, 0.2551213936667298], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.60840386], dtype=float32), -1.4664947]. 
=============================================
[2019-04-03 22:23:20,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3723638e-25 2.8124614e-14 1.6120993e-11 1.0000000e+00 2.6647077e-16
 1.8455282e-09 2.6981234e-17], sum to 1.0000
[2019-04-03 22:23:20,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3914
[2019-04-03 22:23:20,484] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 26.0, 25.17119637731125, 0.5090473396755306, 0.0, 1.0, 87138.34461717264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3357600.0000, 
sim time next is 3358200.0000, 
raw observation next is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.33053906300839, 0.5262858806604943, 0.0, 1.0, 56898.24793763848], 
processed observation next is [1.0, 0.8695652173913043, 0.3564173591874424, 0.6333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6108782552506993, 0.6754286268868315, 0.0, 1.0, 0.27094403779827847], 
reward next is 0.7291, 
noisyNet noise sample is [array([-1.6661594], dtype=float32), -2.5129507]. 
=============================================
[2019-04-03 22:23:21,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3835646e-29 4.5442598e-18 2.0552979e-15 1.0000000e+00 7.7218276e-17
 5.8660549e-11 4.5045251e-20], sum to 1.0000
[2019-04-03 22:23:21,045] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1470
[2019-04-03 22:23:21,064] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 26.0, 25.91291518814006, 0.5296354332092695, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3414000.0000, 
sim time next is 3414600.0000, 
raw observation next is [3.0, 47.0, 117.0, 817.0, 26.0, 25.10513408543662, 0.46239303777929, 1.0, 1.0, 90562.32981011267], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.47, 0.39, 0.9027624309392265, 0.6666666666666666, 0.5920945071197184, 0.6541310125930967, 1.0, 1.0, 0.4312491895719651], 
reward next is 0.5688, 
noisyNet noise sample is [array([1.4997256], dtype=float32), -0.28297594]. 
=============================================
[2019-04-03 22:23:27,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5440905e-26 2.4693346e-17 1.8312558e-14 1.0000000e+00 8.8874350e-16
 8.2295941e-11 5.3211117e-19], sum to 1.0000
[2019-04-03 22:23:27,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-03 22:23:27,999] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.5, 103.0, 775.0, 26.0, 25.74927616152893, 0.570733956393311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 26.0, 26.01085954151717, 0.5940060838659225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.55, 0.3327777777777778, 0.8432780847145489, 0.6666666666666666, 0.6675716284597643, 0.6980020279553075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37818804], dtype=float32), 0.079408035]. 
=============================================
[2019-04-03 22:23:28,973] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2500130e-30 3.2695477e-20 3.3691377e-15 1.0000000e+00 1.4213270e-19
 1.3559749e-13 2.6122142e-21], sum to 1.0000
[2019-04-03 22:23:28,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4890
[2019-04-03 22:23:29,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72908050099876, 0.4989751044678837, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490200.0000, 
sim time next is 3490800.0000, 
raw observation next is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.8537690686422, 0.5189891438583886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6366666666666667, 0.33555555555555566, 0.7587476979742174, 0.6666666666666666, 0.6544807557201834, 0.6729963812861296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1385005], dtype=float32), 0.03193012]. 
=============================================
[2019-04-03 22:23:32,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6375691e-24 6.5225540e-17 1.3436435e-12 1.0000000e+00 1.3488521e-14
 3.0060951e-10 1.5931116e-18], sum to 1.0000
[2019-04-03 22:23:32,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1123
[2019-04-03 22:23:32,479] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 26.0, 26.31150183787612, 0.6666827170926121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47723743744925, 0.6855678367280172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 0.6666666666666666, 0.7064364531207709, 0.7285226122426725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5897498], dtype=float32), -1.1537302]. 
=============================================
[2019-04-03 22:23:32,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.7315  ]
 [78.80624 ]
 [78.952225]
 [79.02507 ]
 [79.148926]], R is [[78.83018494]
 [79.04188538]
 [79.25146484]
 [79.37000275]
 [79.57630157]].
[2019-04-03 22:23:34,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8413357e-26 1.2545005e-16 1.4413067e-13 1.0000000e+00 5.3135714e-17
 7.0822126e-11 1.5996450e-19], sum to 1.0000
[2019-04-03 22:23:34,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3886
[2019-04-03 22:23:34,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 53.5, 103.0, 775.0, 26.0, 25.7493404132532, 0.570739505297469, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421800.0000, 
sim time next is 3422400.0000, 
raw observation next is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 26.0, 26.01091902988952, 0.5939010912568217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.55, 0.3327777777777778, 0.8432780847145489, 0.6666666666666666, 0.6675765858241268, 0.6979670304189405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7759206], dtype=float32), -0.9418285]. 
=============================================
[2019-04-03 22:23:35,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1618350e-32 1.1513144e-21 5.5923107e-17 1.0000000e+00 8.5000463e-23
 6.1954154e-17 1.6211434e-22], sum to 1.0000
[2019-04-03 22:23:35,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4679
[2019-04-03 22:23:35,165] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 26.0, 25.2370960995254, 0.3929386226952383, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3486000.0000, 
sim time next is 3486600.0000, 
raw observation next is [-1.0, 71.0, 88.0, 399.0, 26.0, 25.3076095543194, 0.4171800040036407, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.29333333333333333, 0.4408839779005525, 0.6666666666666666, 0.6089674628599498, 0.6390600013345469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.736072], dtype=float32), -1.3199667]. 
=============================================
[2019-04-03 22:23:35,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.02953257e-25 1.54453427e-16 1.62828125e-13 1.00000000e+00
 1.48673669e-15 1.78608274e-11 3.83786327e-18], sum to 1.0000
[2019-04-03 22:23:35,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0682
[2019-04-03 22:23:35,985] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.18603551615465, 0.4333428904984992, 0.0, 1.0, 34002.25222819425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583800.0000, 
sim time next is 3584400.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 26.0, 25.14818615435576, 0.4418685641870687, 0.0, 1.0, 45346.34529588869], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.38222222222222235, 0.9029465930018415, 0.6666666666666666, 0.5956821795296466, 0.6472895213956896, 0.0, 1.0, 0.21593497759946997], 
reward next is 0.7841, 
noisyNet noise sample is [array([0.07106902], dtype=float32), -0.58130133]. 
=============================================
[2019-04-03 22:23:36,422] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3089309e-27 4.0081235e-18 6.3208607e-14 1.0000000e+00 1.8696789e-15
 6.6280197e-11 7.0986019e-19], sum to 1.0000
[2019-04-03 22:23:36,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4976
[2019-04-03 22:23:36,511] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19184308558067, 0.6377387187600917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24514487756601, 0.6549033506676769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.6666666666666666, 0.687095406463834, 0.7183011168892257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44169578], dtype=float32), -0.027731245]. 
=============================================
[2019-04-03 22:23:46,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0558971e-31 1.1388360e-19 6.8319352e-15 1.0000000e+00 3.4486144e-22
 1.5207865e-15 5.3763771e-22], sum to 1.0000
[2019-04-03 22:23:46,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7704
[2019-04-03 22:23:46,885] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 24.94223773968323, 0.325713435541265, 0.0, 1.0, 197695.6176672984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3698400.0000, 
sim time next is 3699000.0000, 
raw observation next is [3.5, 61.0, 0.0, 0.0, 26.0, 24.94296760785531, 0.3535212930814133, 0.0, 1.0, 199161.4493692914], 
processed observation next is [0.0, 0.8260869565217391, 0.5595567867036012, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5785806339879423, 0.6178404310271378, 0.0, 1.0, 0.9483878541394828], 
reward next is 0.0516, 
noisyNet noise sample is [array([-1.3597137], dtype=float32), 0.22846404]. 
=============================================
[2019-04-03 22:23:46,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.537544]
 [85.387344]
 [84.90781 ]
 [84.513725]
 [84.07433 ]], R is [[88.49073792]
 [87.66442871]
 [87.62088776]
 [87.61937714]
 [87.62026978]].
[2019-04-03 22:24:17,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1916962e-30 2.6653137e-19 5.0600457e-16 1.0000000e+00 2.2199967e-20
 3.9539451e-15 3.8511296e-21], sum to 1.0000
[2019-04-03 22:24:17,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-03 22:24:17,779] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 58.0, 48.5, 314.5, 26.0, 25.7076781674931, 0.4398746924156852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3916800.0000, 
sim time next is 3917400.0000, 
raw observation next is [-8.0, 57.16666666666667, 62.66666666666667, 365.0, 26.0, 25.70754652071324, 0.4375751035120896, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5716666666666668, 0.2088888888888889, 0.40331491712707185, 0.6666666666666666, 0.6422955433927701, 0.6458583678373632, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58896446], dtype=float32), -1.8561255]. 
=============================================
[2019-04-03 22:24:19,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1689564e-25 3.2577487e-18 1.0128121e-13 1.0000000e+00 8.2768301e-16
 5.0876564e-12 5.5074947e-19], sum to 1.0000
[2019-04-03 22:24:19,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-03 22:24:19,304] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.9806856553853, 0.7798254943983539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3940800.0000, 
sim time next is 3941400.0000, 
raw observation next is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 27.0237053493827, 0.6664172585015516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3471837488457987, 0.38, 0.33111111111111113, 0.8471454880294659, 0.6666666666666666, 0.7519754457818916, 0.7221390861671839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08902755], dtype=float32), 0.30045754]. 
=============================================
[2019-04-03 22:24:36,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.51710877e-25 3.11653745e-15 1.28245175e-11 1.00000000e+00
 2.06731189e-16 9.08271201e-12 4.48902481e-17], sum to 1.0000
[2019-04-03 22:24:36,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9750
[2019-04-03 22:24:36,167] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.12638579136174, 0.3561274278764796, 0.0, 1.0, 40694.27736043111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4060800.0000, 
sim time next is 4061400.0000, 
raw observation next is [-6.0, 37.66666666666667, 0.0, 0.0, 26.0, 25.10205897275176, 0.348879054926468, 0.0, 1.0, 40675.99590506363], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5918382477293133, 0.6162930183088227, 0.0, 1.0, 0.1936952185955411], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.21300977], dtype=float32), -1.1800363]. 
=============================================
[2019-04-03 22:24:38,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1650201e-28 2.8070804e-18 7.8071383e-15 1.0000000e+00 6.5178715e-19
 4.6001099e-13 6.3728626e-19], sum to 1.0000
[2019-04-03 22:24:38,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8051
[2019-04-03 22:24:38,260] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 52.66666666666667, 0.0, 0.0, 26.0, 24.87979331111254, 0.2795376068223611, 0.0, 1.0, 39449.83630951471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4166400.0000, 
sim time next is 4167000.0000, 
raw observation next is [-4.0, 52.0, 0.0, 0.0, 26.0, 24.84597849581558, 0.2707737679421314, 0.0, 1.0, 39472.92782680141], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5704982079846316, 0.5902579226473771, 0.0, 1.0, 0.18796632298476862], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.64124894], dtype=float32), -0.8312708]. 
=============================================
[2019-04-03 22:24:38,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.57078]
 [79.67304]
 [79.77791]
 [79.8799 ]
 [79.95898]], R is [[79.47662354]
 [79.49399567]
 [79.51122284]
 [79.52825165]
 [79.5450592 ]].
[2019-04-03 22:24:55,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2192205e-25 3.8163480e-16 2.7728353e-12 1.0000000e+00 5.9133140e-15
 9.4219146e-11 6.4760182e-17], sum to 1.0000
[2019-04-03 22:24:55,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-03 22:24:55,678] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 39.0, 205.0, 475.6666666666667, 26.0, 25.1542343301244, 0.3957627008014177, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4197000.0000, 
sim time next is 4197600.0000, 
raw observation next is [2.0, 40.0, 200.5, 379.0, 26.0, 25.14204096190622, 0.3956149336930013, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4, 0.6683333333333333, 0.41878453038674035, 0.6666666666666666, 0.5951700801588515, 0.6318716445643338, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22535658], dtype=float32), -0.7091451]. 
=============================================
[2019-04-03 22:25:05,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1354992e-26 7.5650803e-14 2.7837264e-11 1.0000000e+00 3.8879792e-18
 9.9636230e-11 1.3779201e-16], sum to 1.0000
[2019-04-03 22:25:05,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9514
[2019-04-03 22:25:05,415] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38175734819262, 0.3409059507161259, 0.0, 1.0, 51986.05808991961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4244400.0000, 
sim time next is 4245000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39925348783603, 0.3431008303098439, 0.0, 1.0, 36476.18453081213], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6166044573196693, 0.6143669434366147, 0.0, 1.0, 0.1736961168133911], 
reward next is 0.8263, 
noisyNet noise sample is [array([0.54420274], dtype=float32), 1.0166105]. 
=============================================
[2019-04-03 22:25:05,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.01037 ]
 [82.9     ]
 [82.916   ]
 [82.92794 ]
 [82.953186]], R is [[83.02872467]
 [82.95088959]
 [82.93514252]
 [82.92488861]
 [82.91999817]].
[2019-04-03 22:25:22,080] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.60930048e-30 6.81117835e-20 1.33580755e-14 1.00000000e+00
 1.25222204e-20 3.18535007e-14 7.81518950e-21], sum to 1.0000
[2019-04-03 22:25:22,080] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2841
[2019-04-03 22:25:22,155] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.62215852705079, 0.4289362006725356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [-0.8333333333333334, 72.66666666666667, 36.99999999999999, 22.0, 26.0, 25.53268853766414, 0.4240989308236863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7266666666666667, 0.12333333333333331, 0.02430939226519337, 0.6666666666666666, 0.6277240448053449, 0.6413663102745621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1199983], dtype=float32), 0.33493567]. 
=============================================
[2019-04-03 22:25:22,182] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.80967 ]
 [84.782394]
 [81.99932 ]
 [78.447495]
 [78.47149 ]], R is [[88.90126801]
 [89.01225281]
 [89.12213135]
 [89.23091125]
 [89.14604187]].
[2019-04-03 22:25:43,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3394997e-23 1.5225497e-13 4.7977351e-11 1.0000000e+00 2.8408823e-12
 1.8587438e-09 6.0043704e-16], sum to 1.0000
[2019-04-03 22:25:43,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-03 22:25:43,741] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.1983843241401, 0.4340979641895544, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.19917244079073, 0.4331050301870354, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.6666666666666666, 0.5999310367325608, 0.6443683433956785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16969447], dtype=float32), -0.55672264]. 
=============================================
[2019-04-03 22:25:51,327] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-03 22:25:51,328] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:25:51,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:25:51,343] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:25:51,345] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,348] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:25:51,377] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:25:51,377] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:25:51,379] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.39035508], dtype=float32), 0.44681135]
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.533333333333333, 80.0, 0.0, 0.0, 20.0, 19.67284561818493, -0.8923305062064376, 0.0, 1.0, 91285.99712942846]
[2019-04-03 22:26:45,316] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:26:45,317] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.7193699e-15 6.6082682e-09 1.0024543e-06 9.9999762e-01 1.6933854e-10
 1.4837173e-06 5.0913190e-10], sampled 0.7409389515350201
[2019-04-03 22:27:46,181] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5795.4745 154867945.9689 -1751.8056
[2019-04-03 22:27:55,305] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5310.2295 184149682.1556 -2309.9646
[2019-04-03 22:28:14,058] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6845.3892 222958959.3179 -1140.6564
[2019-04-03 22:28:15,083] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 400000, evaluation results [400000.0, 5310.229506067209, 184149682.15557247, -2309.9646318309824, 5795.474499695362, 154867945.96888202, -1751.80563718481, 6845.389207628345, 222958959.31789902, -1140.656370484203]
[2019-04-03 22:28:16,886] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.53123131e-28 1.32110943e-17 2.18349373e-14 1.00000000e+00
 3.30744627e-19 1.27516717e-13 1.05783734e-19], sum to 1.0000
[2019-04-03 22:28:16,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8864
[2019-04-03 22:28:16,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40866496456461, 0.4061598700275006, 0.0, 1.0, 46261.04657581358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40624482902701, 0.4036904433851708, 0.0, 1.0, 41882.57046507694], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171870690855842, 0.6345634811283903, 0.0, 1.0, 0.19944081173846162], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.44694757], dtype=float32), 0.060925603]. 
=============================================
[2019-04-03 22:28:16,913] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.76278 ]
 [82.82402 ]
 [82.7512  ]
 [82.656555]
 [82.681595]], R is [[82.55651093]
 [82.51065826]
 [82.39040375]
 [82.2689743 ]
 [82.20160675]].
[2019-04-03 22:28:20,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2180284e-24 1.9006069e-16 1.9771973e-12 1.0000000e+00 9.9178707e-15
 1.3133915e-09 4.6740836e-17], sum to 1.0000
[2019-04-03 22:28:20,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5255
[2019-04-03 22:28:20,375] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09597704164484, 0.3659527310015996, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08327575056166, 0.3655929162925274, 0.0, 1.0, 18689.71169411487], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5902729792134718, 0.6218643054308425, 0.0, 1.0, 0.08899862711483272], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.09190313], dtype=float32), -0.7221807]. 
=============================================
[2019-04-03 22:28:20,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.776535]
 [83.94701 ]
 [84.1472  ]
 [84.317825]
 [84.48388 ]], R is [[83.80570221]
 [83.96764374]
 [84.12796783]
 [84.28668976]
 [84.44382477]].
[2019-04-03 22:28:22,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0778273e-28 1.1895815e-18 2.9032498e-14 1.0000000e+00 9.0554874e-19
 2.0624934e-14 5.6627357e-20], sum to 1.0000
[2019-04-03 22:28:22,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1096
[2019-04-03 22:28:22,043] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.38557119612973, 0.3938228770780937, 0.0, 1.0, 45958.91665859851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5029200.0000, 
sim time next is 5029800.0000, 
raw observation next is [-1.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.44044932560495, 0.4044385959465776, 0.0, 1.0, 18762.28679629367], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6200374438004124, 0.6348128653155259, 0.0, 1.0, 0.08934422283949366], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.6546336], dtype=float32), -0.28268373]. 
=============================================
[2019-04-03 22:28:26,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9677655e-25 1.9841388e-14 9.9872680e-12 1.0000000e+00 2.0214652e-16
 1.5049229e-10 9.3302697e-17], sum to 1.0000
[2019-04-03 22:28:26,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-03 22:28:26,256] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 26.0, 25.75267808635432, 0.527574423667359, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.6889120945305, 0.527327680027165, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6407426745442084, 0.6757758933423883, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.01901794], dtype=float32), -0.7610117]. 
=============================================
[2019-04-03 22:28:26,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:26,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:26,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-03 22:28:26,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:26,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:26,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-03 22:28:27,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:27,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:27,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-03 22:28:30,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:30,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:30,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-03 22:28:30,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:30,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:30,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-03 22:28:31,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:31,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:31,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-03 22:28:31,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:31,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:31,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-03 22:28:32,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:32,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:32,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-03 22:28:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-03 22:28:33,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-03 22:28:33,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:33,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:33,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-03 22:28:34,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:34,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:34,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-03 22:28:34,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:34,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:34,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-03 22:28:36,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:36,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:36,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-03 22:28:37,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.04035208e-21 3.26978407e-13 1.05900245e-11 1.00000000e+00
 1.98970011e-16 3.29919356e-11 5.21991465e-15], sum to 1.0000
[2019-04-03 22:28:37,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0023
[2019-04-03 22:28:37,049] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 18.12393092355549, -1.214291001420683, 0.0, 1.0, 22972.33916739317], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 18.12486053898423, -1.212126460703748, 0.0, 1.0, 25808.24657558489], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.010405044915352471, 0.09595784643208398, 0.0, 1.0, 0.12289641226468995], 
reward next is 0.8771, 
noisyNet noise sample is [array([-0.89855987], dtype=float32), -0.4316919]. 
=============================================
[2019-04-03 22:28:37,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[57.4596  ]
 [57.457443]
 [57.4747  ]
 [57.504147]
 [57.545547]], R is [[57.78136826]
 [58.0941658 ]
 [58.41316223]
 [58.73855591]
 [59.06221771]].
[2019-04-03 22:28:38,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:38,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:38,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-03 22:28:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:28:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:28:41,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-03 22:28:47,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8581859e-22 3.5040859e-14 3.2379848e-12 1.0000000e+00 3.3643437e-17
 7.0291164e-09 3.5324408e-16], sum to 1.0000
[2019-04-03 22:28:47,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6010
[2019-04-03 22:28:47,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 19.0, 18.98676887906039, -1.074153081835211, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 78000.0000, 
sim time next is 78600.0000, 
raw observation next is [0.6833333333333333, 94.16666666666666, 0.0, 0.0, 19.0, 18.91939404249011, -1.084178795726999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4815327793167129, 0.9416666666666665, 0.0, 0.0, 0.08333333333333333, 0.07661617020750928, 0.13860706809100032, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43265143], dtype=float32), -1.1522077]. 
=============================================
[2019-04-03 22:28:50,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1070419e-27 1.5926142e-17 3.6180629e-14 1.0000000e+00 9.2943765e-20
 2.1605968e-10 1.6256239e-18], sum to 1.0000
[2019-04-03 22:28:50,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-03 22:28:50,639] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.200000000000001, 86.0, 90.0, 0.0, 19.0, 18.18461116060688, -1.200961154891405, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 47400.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 18.16280310554908, -1.205606516617211, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.08333333333333333, 0.013566925462423404, 0.09813116112759634, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70013225], dtype=float32), 0.39867616]. 
=============================================
[2019-04-03 22:28:50,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.31885 ]
 [84.60121 ]
 [84.89179 ]
 [85.181885]
 [85.41047 ]], R is [[84.21690369]
 [84.37473297]
 [84.53098297]
 [84.68567657]
 [84.83882141]].
[2019-04-03 22:28:55,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7778259e-21 5.0921141e-14 3.5344577e-10 9.9986780e-01 7.3266046e-14
 1.3214764e-04 3.8571157e-15], sum to 1.0000
[2019-04-03 22:28:55,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6422
[2019-04-03 22:28:55,310] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 62.5, 30.66666666666666, 0.0, 23.0, 23.10618207209236, -0.321537456251576, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [-3.4, 63.0, 24.33333333333333, 0.0, 23.0, 23.08960388061696, -0.3331904963862678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.63, 0.08111111111111109, 0.0, 0.4166666666666667, 0.42413365671807995, 0.3889365012045774, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.6684456], dtype=float32), 0.5401706]. 
=============================================
[2019-04-03 22:28:56,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2388456e-26 3.3558946e-15 2.7254392e-13 9.9999452e-01 2.4667426e-16
 5.4874581e-06 2.9036157e-18], sum to 1.0000
[2019-04-03 22:28:56,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5058
[2019-04-03 22:28:56,216] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 72.0, 138.5, 0.0, 26.0, 25.11406145158627, 0.1529614264404865, 1.0, 1.0, 30827.39434494522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 212400.0000, 
sim time next is 213000.0000, 
raw observation next is [-6.0, 70.83333333333333, 143.3333333333333, 0.0, 26.0, 25.15356411518865, 0.1577944808357121, 1.0, 1.0, 18739.45656092354], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.7083333333333333, 0.47777777777777763, 0.0, 0.6666666666666666, 0.5961303429323875, 0.5525981602785707, 1.0, 1.0, 0.08923550743296924], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.8935812], dtype=float32), 0.2681245]. 
=============================================
[2019-04-03 22:28:56,225] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[95.23894 ]
 [95.038475]
 [94.64849 ]
 [94.30491 ]
 [94.0902  ]], R is [[95.44830322]
 [95.34702301]
 [95.10689545]
 [94.82894897]
 [94.74761963]].
[2019-04-03 22:29:12,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1721981e-17 2.7444713e-10 9.5705090e-08 2.7168435e-01 3.4482459e-10
 7.2831559e-01 1.2396324e-11], sum to 1.0000
[2019-04-03 22:29:12,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-03 22:29:12,203] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 26.0, 22.83771486320975, -0.2682948777040744, 0.0, 1.0, 46224.64971022945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [-10.7, 49.5, 0.0, 0.0, 26.0, 22.7869602785884, -0.2672976221706915, 0.0, 1.0, 46249.60993094477], 
processed observation next is [1.0, 0.13043478260869565, 0.1662049861495845, 0.495, 0.0, 0.0, 0.6666666666666666, 0.39891335654903326, 0.41090079260976947, 0.0, 1.0, 0.22023623776640366], 
reward next is 0.7798, 
noisyNet noise sample is [array([-2.0855246], dtype=float32), 0.59472585]. 
=============================================
[2019-04-03 22:29:26,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7195389e-36 4.1396671e-24 8.0751289e-19 1.0000000e+00 4.6135498e-26
 4.4567135e-11 5.4900861e-26], sum to 1.0000
[2019-04-03 22:29:26,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4300
[2019-04-03 22:29:26,633] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.56207025186659, 0.1739911786926399, 0.0, 1.0, 40137.23372205723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.5407346215195, 0.1704966594157321, 0.0, 1.0, 40193.09349028937], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5450612184599585, 0.5568322198052441, 0.0, 1.0, 0.19139568328709225], 
reward next is 0.8086, 
noisyNet noise sample is [array([-0.5147678], dtype=float32), -0.19665802]. 
=============================================
[2019-04-03 22:29:29,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2873297e-32 3.9036669e-20 4.0250701e-17 9.9999952e-01 2.8113170e-22
 4.9542143e-07 4.2233928e-23], sum to 1.0000
[2019-04-03 22:29:29,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7290
[2019-04-03 22:29:29,103] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 26.0, 24.83659968624704, 0.2285418245747937, 0.0, 1.0, 41104.8447968999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 511200.0000, 
sim time next is 511800.0000, 
raw observation next is [2.8, 92.66666666666667, 0.0, 0.0, 26.0, 24.83073820618, 0.2277855912903854, 0.0, 1.0, 41010.26811306912], 
processed observation next is [1.0, 0.9565217391304348, 0.5401662049861496, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5692281838483334, 0.5759285304301285, 0.0, 1.0, 0.19528699101461486], 
reward next is 0.8047, 
noisyNet noise sample is [array([1.3282341], dtype=float32), 0.7544789]. 
=============================================
[2019-04-03 22:29:31,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2919588e-26 4.0166481e-17 1.1012220e-13 9.9999535e-01 1.7666450e-17
 4.6013729e-06 6.4851564e-18], sum to 1.0000
[2019-04-03 22:29:31,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1984
[2019-04-03 22:29:31,639] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.93980681265154, 0.3151259953827499, 0.0, 1.0, 36788.83979588761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 26.0, 24.92342467017881, 0.319988354335182, 0.0, 1.0, 45226.86652978959], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45111111111111096, 0.5841620626151013, 0.6666666666666666, 0.5769520558482343, 0.606662784778394, 0.0, 1.0, 0.21536603109423616], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.92881113], dtype=float32), -0.38821134]. 
=============================================
[2019-04-03 22:29:32,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2754764e-23 1.5701841e-14 1.5896342e-12 9.9989653e-01 5.0675216e-15
 1.0342016e-04 9.0886463e-17], sum to 1.0000
[2019-04-03 22:29:32,708] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-03 22:29:32,790] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 26.0, 24.98477379240468, 0.3143910640620838, 0.0, 1.0, 22625.88456096275], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 574800.0000, 
sim time next is 575400.0000, 
raw observation next is [-1.2, 83.0, 80.33333333333334, 63.66666666666667, 26.0, 24.98236118159492, 0.3073885763134878, 0.0, 1.0, 28107.59455801492], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.26777777777777784, 0.0703499079189687, 0.6666666666666666, 0.5818634317995768, 0.6024628587711626, 0.0, 1.0, 0.13384568837149963], 
reward next is 0.8662, 
noisyNet noise sample is [array([-0.4132936], dtype=float32), -0.6279057]. 
=============================================
[2019-04-03 22:29:34,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5023830e-19 2.7966052e-10 2.2696605e-07 8.6950582e-01 5.2584820e-12
 1.3049400e-01 8.7640371e-13], sum to 1.0000
[2019-04-03 22:29:34,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9439
[2019-04-03 22:29:34,844] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.98887693532407, 0.2103445832099669, 0.0, 1.0, 43229.9591361296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.98746140330588, 0.2075246625001305, 0.0, 1.0, 42713.18789571621], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5822884502754899, 0.5691748875000435, 0.0, 1.0, 0.20339613283674388], 
reward next is 0.7966, 
noisyNet noise sample is [array([0.45396233], dtype=float32), 0.39045265]. 
=============================================
[2019-04-03 22:29:38,787] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0011276e-23 8.7333197e-15 2.7272778e-12 9.9995124e-01 1.8578145e-15
 4.8773152e-05 3.7765252e-16], sum to 1.0000
[2019-04-03 22:29:38,788] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0603
[2019-04-03 22:29:38,829] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 55.0, 81.66666666666667, 50.0, 26.0, 24.86794872226801, 0.2196914535584718, 0.0, 1.0, 40612.78649519024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658200.0000, 
sim time next is 658800.0000, 
raw observation next is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.87168584245802, 0.2227802971812309, 0.0, 1.0, 35540.29504358783], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2733333333333333, 0.051933701657458566, 0.6666666666666666, 0.5726404868715017, 0.5742600990604103, 0.0, 1.0, 0.1692395002075611], 
reward next is 0.8308, 
noisyNet noise sample is [array([-0.92863446], dtype=float32), 0.086311646]. 
=============================================
[2019-04-03 22:29:48,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2308920e-25 1.2661782e-16 4.1057405e-14 9.9999964e-01 1.2348712e-17
 3.7618074e-07 4.2734629e-19], sum to 1.0000
[2019-04-03 22:29:48,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7126
[2019-04-03 22:29:48,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.45, 65.5, 0.0, 0.0, 26.0, 24.62340605518425, 0.2198831239251062, 0.0, 1.0, 43080.74319212529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772200.0000, 
sim time next is 772800.0000, 
raw observation next is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.59054814711147, 0.2125927464113868, 0.0, 1.0, 42944.27926626257], 
processed observation next is [1.0, 0.9565217391304348, 0.2816251154201293, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5492123455926224, 0.5708642488037956, 0.0, 1.0, 0.20449656793458368], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.9936708], dtype=float32), 1.3309615]. 
=============================================
[2019-04-03 22:29:51,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8851359e-28 6.8833773e-19 1.3532606e-15 1.0000000e+00 1.6627087e-18
 3.2176117e-09 5.0881766e-21], sum to 1.0000
[2019-04-03 22:29:51,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4089
[2019-04-03 22:29:51,955] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.57946267480467, 0.2953929992877982, 1.0, 1.0, 18715.5676860229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816600.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.60057484447367, 0.2971204353587585, 1.0, 1.0, 18714.20428912349], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.6666666666666666, 0.6333812370394725, 0.5990401451195861, 1.0, 1.0, 0.08911525851963566], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.66096115], dtype=float32), 0.81682074]. 
=============================================
[2019-04-03 22:29:54,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4486646e-26 5.1817906e-17 3.6278863e-13 9.9999869e-01 1.3207291e-18
 1.2982401e-06 4.1181257e-19], sum to 1.0000
[2019-04-03 22:29:54,338] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-03 22:29:54,386] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.98883048636159, 0.439404624537714, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826800.0000, 
sim time next is 827400.0000, 
raw observation next is [-4.0, 79.0, 80.33333333333333, 0.0, 26.0, 26.17778740169233, 0.4578085843004602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.2677777777777778, 0.0, 0.6666666666666666, 0.6814822834743609, 0.6526028614334868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33041674], dtype=float32), -1.3001467]. 
=============================================
[2019-04-03 22:30:04,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0869892e-32 4.3833888e-21 6.8826231e-17 1.0000000e+00 1.7566292e-22
 1.1740117e-12 9.3507287e-23], sum to 1.0000
[2019-04-03 22:30:04,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-03 22:30:04,703] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 81.0, 0.0, 0.0, 26.0, 25.39292103864041, 0.4455296939743209, 0.0, 1.0, 32733.64881514197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 962400.0000, 
sim time next is 963000.0000, 
raw observation next is [7.7, 81.5, 0.0, 0.0, 26.0, 25.47874902162784, 0.4433266401313378, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6232290851356534, 0.6477755467104459, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8634207], dtype=float32), 0.7237519]. 
=============================================
[2019-04-03 22:30:04,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.24605 ]
 [92.15973 ]
 [92.027565]
 [91.83147 ]
 [91.67821 ]], R is [[92.25544739]
 [92.17701721]
 [92.05458069]
 [91.83678436]
 [91.62509155]].
[2019-04-03 22:30:08,743] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5553408e-32 2.5321539e-21 9.8079938e-18 1.0000000e+00 3.1088675e-22
 1.0229064e-11 9.2668191e-23], sum to 1.0000
[2019-04-03 22:30:08,748] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5249
[2019-04-03 22:30:08,765] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.57103484556265, 0.4777880969151747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 972000.0000, 
sim time next is 972600.0000, 
raw observation next is [9.0, 83.0, 0.0, 0.0, 26.0, 25.69084169001898, 0.4776686597490849, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7119113573407203, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6409034741682484, 0.6592228865830283, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7866774], dtype=float32), -0.39465535]. 
=============================================
[2019-04-03 22:30:10,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5368044e-36 1.1290227e-24 4.0876248e-20 1.0000000e+00 1.9439696e-26
 1.1251632e-14 2.4362573e-26], sum to 1.0000
[2019-04-03 22:30:10,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0816
[2019-04-03 22:30:10,661] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.3, 96.66666666666666, 99.0, 0.0, 26.0, 25.07439048837099, 0.4861729428038233, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.05668777786639, 0.4837660689816374, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8559556786703602, 0.9733333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5880573148221991, 0.6612553563272124, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5286756], dtype=float32), -0.24525362]. 
=============================================
[2019-04-03 22:30:10,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[105.497604]
 [105.808754]
 [105.92503 ]
 [106.18693 ]
 [106.33752 ]], R is [[105.19297791]
 [105.14105225]
 [105.08964539]
 [105.03874969]
 [104.98836517]].
[2019-04-03 22:30:16,254] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8601273e-35 1.0059871e-24 8.0533101e-21 1.0000000e+00 2.2439403e-26
 6.2292592e-16 3.6031243e-26], sum to 1.0000
[2019-04-03 22:30:16,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5755
[2019-04-03 22:30:16,274] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.06039465577191, 0.484722894330939, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254000.0000, 
sim time next is 1254600.0000, 
raw observation next is [14.1, 98.0, 101.0, 0.0, 26.0, 25.03081697377322, 0.4813410102064959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8531855955678671, 0.98, 0.33666666666666667, 0.0, 0.6666666666666666, 0.5859014144811017, 0.6604470034021653, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4017652], dtype=float32), -0.6872821]. 
=============================================
[2019-04-03 22:30:20,887] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8778876e-27 3.8491255e-17 1.1255152e-14 1.0000000e+00 6.3999533e-19
 3.1513476e-09 6.2202948e-20], sum to 1.0000
[2019-04-03 22:30:20,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-03 22:30:20,899] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5465243e-24 2.7085304e-15 1.5176819e-12 9.9998820e-01 2.6932188e-16
 1.1789825e-05 4.9103321e-17], sum to 1.0000
[2019-04-03 22:30:20,902] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.47071303379795, 0.5881729647628896, 0.0, 1.0, 34838.73956509513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 26.0, 25.45711836731868, 0.5855540021614762, 0.0, 1.0, 40976.07881020577], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6214265306098902, 0.6951846673871588, 0.0, 1.0, 0.19512418481050367], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.90060115], dtype=float32), -0.06113463]. 
=============================================
[2019-04-03 22:30:20,904] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4637
[2019-04-03 22:30:20,919] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.7000000000000001, 92.0, 92.5, 0.0, 26.0, 26.10108044943661, 0.5906751919332902, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1333200.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 26.0, 26.10362989854378, 0.5900971540596633, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.6666666666666666, 0.675302491545315, 0.6966990513532211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0133985], dtype=float32), 0.99656904]. 
=============================================
[2019-04-03 22:30:38,003] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9416635e-30 2.0253359e-20 4.0615031e-17 1.0000000e+00 5.9177866e-21
 1.0241455e-12 5.7774577e-22], sum to 1.0000
[2019-04-03 22:30:38,005] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-03 22:30:38,027] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.5, 85.5, 0.0, 0.0, 26.0, 25.54555888129156, 0.5077558380549925, 0.0, 1.0, 49638.32485043934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1567800.0000, 
sim time next is 1568400.0000, 
raw observation next is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48343453758703, 0.5143981216712735, 0.0, 1.0, 72494.15544986009], 
processed observation next is [1.0, 0.13043478260869565, 0.5881809787626964, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6236195447989191, 0.6714660405570911, 0.0, 1.0, 0.3452102640469528], 
reward next is 0.6548, 
noisyNet noise sample is [array([-1.1970588], dtype=float32), 0.028004995]. 
=============================================
[2019-04-03 22:30:39,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0095020e-29 1.9708787e-20 1.0608137e-15 1.0000000e+00 1.7578643e-20
 9.9518805e-12 5.4093153e-21], sum to 1.0000
[2019-04-03 22:30:39,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-03 22:30:39,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.36681314474454, 0.447521448571136, 0.0, 1.0, 57872.017503806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31759645452876, 0.45426112824074, 0.0, 1.0, 47955.27094420572], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6097997045440634, 0.6514203760802467, 0.0, 1.0, 0.22835843306764628], 
reward next is 0.7716, 
noisyNet noise sample is [array([0.7512652], dtype=float32), -0.19794019]. 
=============================================
[2019-04-03 22:30:42,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2631363e-28 1.4703834e-18 1.0148032e-14 1.0000000e+00 3.1345809e-20
 1.1500786e-10 2.7209515e-20], sum to 1.0000
[2019-04-03 22:30:42,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4985
[2019-04-03 22:30:42,389] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.1, 87.16666666666667, 0.0, 0.0, 26.0, 25.62142952373982, 0.5828437039600413, 0.0, 1.0, 38167.10953910777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1642200.0000, 
sim time next is 1642800.0000, 
raw observation next is [7.0, 88.33333333333334, 0.0, 0.0, 26.0, 25.57693931377625, 0.591247395804213, 0.0, 1.0, 56596.53492632153], 
processed observation next is [1.0, 0.0, 0.6565096952908588, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6314116094813542, 0.697082465268071, 0.0, 1.0, 0.2695073091729597], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.28624108], dtype=float32), -0.44893008]. 
=============================================
[2019-04-03 22:30:43,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6206482e-30 2.4216897e-20 3.6186095e-17 1.0000000e+00 2.6464565e-21
 9.0136069e-13 1.3045604e-21], sum to 1.0000
[2019-04-03 22:30:43,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-03 22:30:43,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.5837766651116, 0.5350303444755304, 0.0, 1.0, 30685.46959574369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.57269605113063, 0.5275693418723318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6310580042608859, 0.6758564472907773, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5906485], dtype=float32), 1.0981245]. 
=============================================
[2019-04-03 22:30:43,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.99994]
 [82.18237]
 [82.39488]
 [82.62107]
 [82.82843]], R is [[84.61148071]
 [84.61924744]
 [84.63998413]
 [84.70437622]
 [84.76810455]].
[2019-04-03 22:30:43,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9889455e-31 2.9987953e-20 1.5633727e-16 1.0000000e+00 7.8818267e-23
 8.1754541e-13 2.6479685e-23], sum to 1.0000
[2019-04-03 22:30:43,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-03 22:30:43,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 26.0, 26.77670627186249, 0.752904965319309, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1618800.0000, 
sim time next is 1619400.0000, 
raw observation next is [10.78333333333333, 59.83333333333334, 0.0, 0.0, 26.0, 26.6390926676611, 0.745921382537278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7613111726685133, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.7199243889717583, 0.7486404608457593, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3825027], dtype=float32), -0.10702468]. 
=============================================
[2019-04-03 22:30:53,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0118625e-29 2.8873506e-18 7.5465246e-16 1.0000000e+00 4.1856573e-22
 1.4405728e-13 2.8180519e-21], sum to 1.0000
[2019-04-03 22:30:53,128] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9759
[2019-04-03 22:30:53,141] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 26.0, 25.28781885436402, 0.4401674144668438, 0.0, 1.0, 42923.43741880801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1735800.0000, 
sim time next is 1736400.0000, 
raw observation next is [0.1333333333333334, 91.0, 0.0, 0.0, 26.0, 25.27343632816496, 0.4382061886542994, 0.0, 1.0, 42936.07068210789], 
processed observation next is [0.0, 0.08695652173913043, 0.46629732225300097, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6061196940137465, 0.6460687295514331, 0.0, 1.0, 0.204457479438609], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.14277382], dtype=float32), 0.56379265]. 
=============================================
[2019-04-03 22:31:01,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3354051e-29 8.8807310e-22 9.3289835e-18 1.0000000e+00 1.1237320e-19
 3.1270336e-12 4.2713044e-23], sum to 1.0000
[2019-04-03 22:31:01,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2206
[2019-04-03 22:31:01,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.80767532803569, 0.3218809059230635, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.7468756547589, 0.3170652345587175, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2996306555863343, 0.7566666666666666, 0.6377777777777777, 0.1775322283609577, 0.6666666666666666, 0.6455729712299082, 0.6056884115195725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62909365], dtype=float32), 0.8680393]. 
=============================================
[2019-04-03 22:31:06,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9419739e-28 7.8167517e-18 2.4143891e-16 1.0000000e+00 4.2701565e-19
 2.0411144e-10 7.4268760e-20], sum to 1.0000
[2019-04-03 22:31:06,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6361
[2019-04-03 22:31:06,818] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 25.52610492924003, 0.3802881758082912, 0.0, 1.0, 9355.897741457735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2057400.0000, 
sim time next is 2058000.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 25.24162614444192, 0.3444135820326544, 0.0, 1.0, 18709.84912982773], 
processed observation next is [1.0, 0.8260869565217391, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.60346884537016, 0.6148045273442181, 0.0, 1.0, 0.08909451966584633], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.0828574], dtype=float32), -1.0612997]. 
=============================================
[2019-04-03 22:31:06,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.980316]
 [86.92338 ]
 [87.01022 ]
 [87.04581 ]
 [87.08727 ]], R is [[82.50930023]
 [82.63965607]
 [82.81326294]
 [82.98513031]
 [83.15528107]].
[2019-04-03 22:31:18,824] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2998959e-28 3.9941276e-18 3.2901735e-16 1.0000000e+00 6.0476202e-20
 2.6369757e-12 1.5977659e-20], sum to 1.0000
[2019-04-03 22:31:18,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8708
[2019-04-03 22:31:18,931] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.50784610324806, 0.2841818679353589, 1.0, 1.0, 18724.46380219951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2021400.0000, 
sim time next is 2022000.0000, 
raw observation next is [-5.733333333333333, 84.0, 74.5, 0.0, 26.0, 25.53167680850036, 0.2910859133443859, 1.0, 1.0, 18721.93189313847], 
processed observation next is [1.0, 0.391304347826087, 0.30378578024007385, 0.84, 0.24833333333333332, 0.0, 0.6666666666666666, 0.6276397340416967, 0.597028637781462, 1.0, 1.0, 0.08915205663399271], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.11029071], dtype=float32), 0.6429042]. 
=============================================
[2019-04-03 22:31:18,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.77821]
 [80.04651]
 [80.36167]
 [80.6557 ]
 [81.0729 ]], R is [[79.79393768]
 [79.90683746]
 [80.10777283]
 [80.30669403]
 [80.50363159]].
[2019-04-03 22:31:28,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5571025e-25 3.2541980e-17 2.5051375e-13 1.0000000e+00 6.5670406e-18
 1.8403152e-09 1.0985684e-18], sum to 1.0000
[2019-04-03 22:31:28,374] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0714
[2019-04-03 22:31:28,439] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83405164096588, 0.4531578940153474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98552293966285, 0.4584943154444107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.6666666666666666, 0.6654602449719041, 0.6528314384814703, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1110376], dtype=float32), -0.25968692]. 
=============================================
[2019-04-03 22:31:28,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5751802e-26 3.4550472e-18 7.0112095e-14 1.0000000e+00 1.6864397e-18
 6.8947092e-10 2.0870867e-19], sum to 1.0000
[2019-04-03 22:31:28,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.58643 ]
 [73.940994]
 [74.24183 ]
 [73.72351 ]
 [73.71253 ]], R is [[73.67917633]
 [73.94238281]
 [74.20295715]
 [73.51599121]
 [73.420578  ]].
[2019-04-03 22:31:28,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-03 22:31:28,502] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 86.0, 87.5, 0.0, 26.0, 26.24713443844342, 0.4537235158962847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2041200.0000, 
sim time next is 2041800.0000, 
raw observation next is [-4.4, 85.33333333333334, 82.0, 0.0, 26.0, 26.19597098016751, 0.4446634559273726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3407202216066482, 0.8533333333333334, 0.2733333333333333, 0.0, 0.6666666666666666, 0.6829975816806257, 0.6482211519757909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1134672], dtype=float32), -1.8302103]. 
=============================================
[2019-04-03 22:31:29,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3126132e-27 1.4639256e-17 1.4979058e-15 1.0000000e+00 7.5529355e-19
 1.1638006e-12 1.7537326e-19], sum to 1.0000
[2019-04-03 22:31:29,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-03 22:31:29,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.66098857306477, 0.233944312941347, 0.0, 1.0, 42695.72949621734], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2077800.0000, 
sim time next is 2078400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 26.0, 24.67087845879976, 0.2254727660941713, 0.0, 1.0, 42696.79055778226], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5559065382333133, 0.5751575886980571, 0.0, 1.0, 0.20331805027515362], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.49312466], dtype=float32), -0.83198494]. 
=============================================
[2019-04-03 22:31:51,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.84227058e-29 7.28431080e-20 7.49093969e-16 1.00000000e+00
 4.36728454e-20 1.36330942e-12 1.00212834e-20], sum to 1.0000
[2019-04-03 22:31:51,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5248
[2019-04-03 22:31:51,218] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.583333333333334, 87.66666666666666, 63.66666666666666, 23.66666666666666, 26.0, 25.62655415564215, 0.3039962482337284, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2278200.0000, 
sim time next is 2278800.0000, 
raw observation next is [-8.4, 87.0, 73.0, 27.5, 26.0, 25.62537242936445, 0.3043227667640716, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.87, 0.24333333333333335, 0.03038674033149171, 0.6666666666666666, 0.6354477024470375, 0.6014409222546905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21383928], dtype=float32), -0.7230983]. 
=============================================
[2019-04-03 22:32:00,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0256837e-24 1.2855539e-16 9.4197436e-14 1.0000000e+00 1.2439066e-17
 4.6115238e-12 2.0011923e-18], sum to 1.0000
[2019-04-03 22:32:00,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-03 22:32:00,226] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.43285882666422, -0.1008303509853151, 0.0, 1.0, 44416.0790300315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39689345731361, -0.1087012121410302, 0.0, 1.0, 44403.88808588979], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4497411214428008, 0.4637662626196566, 0.0, 1.0, 0.21144708612328472], 
reward next is 0.7886, 
noisyNet noise sample is [array([-1.3405062], dtype=float32), -0.054966718]. 
=============================================
[2019-04-03 22:32:01,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6711341e-28 3.9013338e-19 3.3917671e-16 1.0000000e+00 6.8451998e-19
 8.8402793e-13 1.3688277e-20], sum to 1.0000
[2019-04-03 22:32:01,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3622
[2019-04-03 22:32:01,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 26.0, 23.87036965869764, 0.01830697170904154, 0.0, 1.0, 43468.44719606221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2263800.0000, 
sim time next is 2264400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.81049915970418, 0.008865452582490102, 0.0, 1.0, 43421.80087154032], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.48420826330868155, 0.50295515086083, 0.0, 1.0, 0.2067704803406682], 
reward next is 0.7932, 
noisyNet noise sample is [array([3.0625925], dtype=float32), -0.29344496]. 
=============================================
[2019-04-03 22:32:01,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9721497e-28 4.4327291e-19 6.6118153e-16 1.0000000e+00 1.6112698e-18
 2.2261880e-12 7.2787798e-21], sum to 1.0000
[2019-04-03 22:32:01,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0296
[2019-04-03 22:32:01,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.76265836962004, -0.003636141143075084, 0.0, 1.0, 43366.43729055744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69374840195481, -0.008760625414634432, 0.0, 1.0, 43320.40252292853], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47447903349623416, 0.4970797915284552, 0.0, 1.0, 0.20628763106156445], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.28337705], dtype=float32), -0.8793433]. 
=============================================
[2019-04-03 22:32:12,871] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4221935e-25 9.0812685e-17 3.6022187e-14 1.0000000e+00 3.5000977e-18
 1.7824006e-11 3.8394010e-18], sum to 1.0000
[2019-04-03 22:32:12,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0587
[2019-04-03 22:32:12,921] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15743960709527, 0.2868123765157258, 0.0, 1.0, 43326.75035819096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15860901874729, 0.2810689106720466, 0.0, 1.0, 43113.824395203], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.596550751562274, 0.5936896368906822, 0.0, 1.0, 0.20530392569144287], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.06327444], dtype=float32), 1.1942933]. 
=============================================
[2019-04-03 22:32:18,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2154138e-28 3.6101538e-19 9.1441184e-17 1.0000000e+00 2.1711696e-20
 4.4681823e-13 1.8295849e-20], sum to 1.0000
[2019-04-03 22:32:18,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4449
[2019-04-03 22:32:18,231] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.00566339439197, 0.05943498292303753, 0.0, 1.0, 41607.94252565796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2359800.0000, 
sim time next is 2360400.0000, 
raw observation next is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 23.97057340625974, 0.05382990174113351, 0.0, 1.0, 41698.12880672357], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.026111111111111106, 0.0, 0.6666666666666666, 0.4975477838549782, 0.5179433005803779, 0.0, 1.0, 0.19856251812725512], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.786593], dtype=float32), -0.20239143]. 
=============================================
[2019-04-03 22:32:23,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.26865235e-26 5.22772092e-17 6.57463653e-14 1.00000000e+00
 5.47231757e-17 5.15877951e-10 3.11955219e-19], sum to 1.0000
[2019-04-03 22:32:23,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3064
[2019-04-03 22:32:23,038] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.35, 57.5, 0.0, 0.0, 26.0, 25.33661159781706, 0.3449537641752138, 0.0, 1.0, 53831.95408900568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2586600.0000, 
sim time next is 2587200.0000, 
raw observation next is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.21961152602672, 0.3322293445587113, 0.0, 1.0, 48843.12742881214], 
processed observation next is [1.0, 0.9565217391304348, 0.36472760849492153, 0.58, 0.0, 0.0, 0.6666666666666666, 0.6016342938355601, 0.6107431148529038, 0.0, 1.0, 0.23258632108958163], 
reward next is 0.7674, 
noisyNet noise sample is [array([-0.21899551], dtype=float32), -0.21231778]. 
=============================================
[2019-04-03 22:32:47,077] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7835227e-30 3.0605405e-22 1.1383110e-18 1.0000000e+00 6.4129939e-22
 1.4388304e-15 6.6991785e-23], sum to 1.0000
[2019-04-03 22:32:47,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2027
[2019-04-03 22:32:47,130] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 23.83789298488022, 0.01046738961455313, 0.0, 1.0, 44623.55142461543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2616000.0000, 
sim time next is 2616600.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 23.78980662573852, 0.01311993383323187, 0.0, 1.0, 44741.33137886503], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.7883333333333334, 0.0, 0.0, 0.6666666666666666, 0.4824838854782101, 0.504373311277744, 0.0, 1.0, 0.21305395894697632], 
reward next is 0.7869, 
noisyNet noise sample is [array([-2.4864063], dtype=float32), 0.8426616]. 
=============================================
[2019-04-03 22:32:53,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6031226e-27 9.9898859e-18 1.9518134e-14 1.0000000e+00 3.5023948e-18
 5.6282611e-11 1.0640741e-19], sum to 1.0000
[2019-04-03 22:32:53,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3268
[2019-04-03 22:32:53,908] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 26.0, 24.36647836804698, 0.130942942934135, 0.0, 1.0, 42645.56232276826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2607600.0000, 
sim time next is 2608200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 26.0, 24.36927679623818, 0.132622972426569, 0.0, 1.0, 42725.50414343813], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5307730663531816, 0.544207657475523, 0.0, 1.0, 0.20345478163541966], 
reward next is 0.7965, 
noisyNet noise sample is [array([2.137209], dtype=float32), 0.21983807]. 
=============================================
[2019-04-03 22:33:13,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5115696e-27 7.8849699e-19 9.8922098e-16 1.0000000e+00 3.1690633e-20
 6.7097115e-13 6.2692006e-20], sum to 1.0000
[2019-04-03 22:33:13,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-03 22:33:13,673] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 24.30756462897351, 0.1971941858782554, 0.0, 1.0, 42805.47883383481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.28846898285393, 0.1902497868380746, 0.0, 1.0, 42742.97934097329], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5240390819044943, 0.5634165956126915, 0.0, 1.0, 0.20353799686177756], 
reward next is 0.7965, 
noisyNet noise sample is [array([-1.2740859], dtype=float32), 0.5026602]. 
=============================================
[2019-04-03 22:33:19,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1667747e-23 1.2228495e-14 6.0912966e-13 1.0000000e+00 2.6859073e-14
 1.7410906e-08 3.3443806e-18], sum to 1.0000
[2019-04-03 22:33:19,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8240
[2019-04-03 22:33:19,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.0286942750993, 0.4558908694970039, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2803200.0000, 
sim time next is 2803800.0000, 
raw observation next is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 26.0, 26.02323793888207, 0.4633068999606353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.5083333333333334, 0.5255555555555557, 0.6552486187845303, 0.6666666666666666, 0.6686031615735057, 0.6544356333202118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5839209], dtype=float32), 1.1252546]. 
=============================================
[2019-04-03 22:33:24,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.04429376e-26 2.29071396e-18 8.15874671e-15 1.00000000e+00
 1.22580748e-19 7.97783335e-13 2.45486811e-19], sum to 1.0000
[2019-04-03 22:33:24,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4153
[2019-04-03 22:33:24,284] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.79758930385015, 0.3081028442276084, 0.0, 1.0, 43140.4072917513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2944800.0000, 
sim time next is 2945400.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 24.76082735178473, 0.3048323513806284, 0.0, 1.0, 43100.42036472868], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5634022793153942, 0.6016107837935428, 0.0, 1.0, 0.20524009697489848], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.33677346], dtype=float32), 1.5817033]. 
=============================================
[2019-04-03 22:33:41,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8242191e-27 2.3809130e-17 7.8924794e-15 1.0000000e+00 6.1499167e-19
 8.4172523e-11 4.0790732e-20], sum to 1.0000
[2019-04-03 22:33:41,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8288
[2019-04-03 22:33:41,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 26.0, 25.03082763993773, 0.2856719377429218, 0.0, 1.0, 48696.21634903156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00510342157401, 0.283941093206694, 0.0, 1.0, 53608.2186960215], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5837586184645008, 0.5946470310688979, 0.0, 1.0, 0.25527723188581664], 
reward next is 0.7447, 
noisyNet noise sample is [array([2.0056362], dtype=float32), 0.9637852]. 
=============================================
[2019-04-03 22:33:48,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1794589e-27 3.6809402e-17 2.4238771e-15 1.0000000e+00 1.8174839e-17
 1.7424652e-09 5.9338848e-20], sum to 1.0000
[2019-04-03 22:33:48,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8699
[2019-04-03 22:33:48,336] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 27.23751715913389, 0.9315683675329908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3172200.0000, 
sim time next is 3172800.0000, 
raw observation next is [6.0, 100.0, 16.33333333333333, 179.8333333333333, 26.0, 27.44162832480716, 0.686143755516969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.05444444444444443, 0.19871086556169423, 0.6666666666666666, 0.7868023604005966, 0.728714585172323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0078111], dtype=float32), 0.21715908]. 
=============================================
[2019-04-03 22:33:52,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7860806e-26 9.6279495e-17 2.1363629e-13 1.0000000e+00 6.0740554e-17
 9.9962878e-11 2.6098065e-19], sum to 1.0000
[2019-04-03 22:33:52,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-03 22:33:52,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29469152192078, 0.4759387271112726, 0.0, 1.0, 50863.16512205518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31607494448173, 0.4712733330345962, 0.0, 1.0, 45716.19565910571], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096729120401442, 0.6570911110115321, 0.0, 1.0, 0.2176961698052653], 
reward next is 0.7823, 
noisyNet noise sample is [array([-1.3077669], dtype=float32), 0.6705577]. 
=============================================
[2019-04-03 22:33:55,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3803356e-22 3.8950078e-14 9.6563816e-13 9.9999976e-01 2.2903741e-13
 2.7644137e-07 2.7488208e-17], sum to 1.0000
[2019-04-03 22:33:55,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6518
[2019-04-03 22:33:55,697] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08307794843409, 0.5240276842594996, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3255600.0000, 
sim time next is 3256200.0000, 
raw observation next is [-3.5, 74.0, 59.0, 511.0, 26.0, 26.92044941831657, 0.7846024476287273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.36565096952908593, 0.74, 0.19666666666666666, 0.5646408839779006, 0.6666666666666666, 0.7433707848597141, 0.7615341492095759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1990135], dtype=float32), -0.041916005]. 
=============================================
[2019-04-03 22:34:03,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2301419e-30 9.6206656e-20 4.8721957e-17 1.0000000e+00 3.0066056e-21
 1.4996369e-13 4.1794189e-22], sum to 1.0000
[2019-04-03 22:34:03,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7487
[2019-04-03 22:34:03,276] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22227620931559, 0.3830099670167806, 0.0, 1.0, 41941.61244485113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3474000.0000, 
sim time next is 3474600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.19712566773391, 0.3818579412783036, 0.0, 1.0, 41909.55255039756], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5997604723111593, 0.6272859804261012, 0.0, 1.0, 0.199569297859036], 
reward next is 0.8004, 
noisyNet noise sample is [array([2.4748955], dtype=float32), 0.030311871]. 
=============================================
[2019-04-03 22:34:07,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3596822e-28 4.6040899e-19 4.1578844e-16 1.0000000e+00 8.4081804e-20
 1.6616984e-13 1.9142094e-20], sum to 1.0000
[2019-04-03 22:34:07,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1958
[2019-04-03 22:34:07,517] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.80925681766695, 0.2319095762458566, 0.0, 1.0, 42730.63027412078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.7213697716537, 0.2231204989818958, 0.0, 1.0, 42664.45301560876], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5601141476378082, 0.5743734996606319, 0.0, 1.0, 0.20316406197908932], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.9676731], dtype=float32), 0.63041013]. 
=============================================
[2019-04-03 22:34:09,971] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-03 22:34:09,971] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:34:09,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:34:09,994] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:34:09,995] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,996] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:34:09,997] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:34:09,999] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:34:10,022] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-03 22:36:40,753] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7098.8260 178876465.7181 -817.5205
[2019-04-03 22:36:56,594] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6569.6489 208183432.0125 -1203.7559
[2019-04-03 22:37:02,386] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6935.8559 223694268.1349 -1082.0515
[2019-04-03 22:37:03,421] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 500000, evaluation results [500000.0, 6569.648938209325, 208183432.01248318, -1203.7558623349244, 7098.826049830529, 178876465.7180862, -817.5204724140117, 6935.855901593388, 223694268.13492185, -1082.0514637920874]
[2019-04-03 22:37:09,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4355611e-25 1.1745089e-16 1.3334865e-13 1.0000000e+00 1.1120710e-15
 2.8565350e-09 2.5819229e-18], sum to 1.0000
[2019-04-03 22:37:09,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-03 22:37:09,145] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30735822507274, 0.4475595360129611, 0.0, 1.0, 9342.17152307084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [0.0, 41.0, 63.0, 515.0, 26.0, 25.28263011593106, 0.4403150708072341, 0.0, 1.0, 18684.24474005441], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41, 0.21, 0.569060773480663, 0.6666666666666666, 0.606885842994255, 0.646771690269078, 0.0, 1.0, 0.0889725940002591], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.98830783], dtype=float32), -1.9676818]. 
=============================================
[2019-04-03 22:37:15,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3746066e-32 1.1735423e-22 2.0993366e-18 1.0000000e+00 3.3936578e-23
 4.3849409e-15 6.7838078e-24], sum to 1.0000
[2019-04-03 22:37:15,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0356
[2019-04-03 22:37:15,564] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.69011580173639, 0.499441110201415, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.6998467129139, 0.4999729014682073, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.6666666666666666, 0.6416538927428249, 0.6666576338227358, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55640006], dtype=float32), 1.4139748]. 
=============================================
[2019-04-03 22:37:15,585] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[99.24862 ]
 [99.259514]
 [99.2417  ]
 [99.11554 ]
 [98.90261 ]], R is [[99.0916748 ]
 [99.10076141]
 [99.10975647]
 [99.11865997]
 [99.12747192]].
[2019-04-03 22:37:17,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2033045e-27 4.7625048e-19 1.9014200e-15 1.0000000e+00 1.4768078e-19
 5.3831429e-13 2.6149174e-20], sum to 1.0000
[2019-04-03 22:37:17,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9395
[2019-04-03 22:37:17,998] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06086851525421, 0.3240253606944634, 0.0, 1.0, 43615.46490208627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3814800.0000, 
sim time next is 3815400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.01709305017878, 0.3187075242061106, 0.0, 1.0, 43675.38840927117], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.584757754181565, 0.6062358414020369, 0.0, 1.0, 0.2079780400441484], 
reward next is 0.7920, 
noisyNet noise sample is [array([0.05698205], dtype=float32), -0.6322422]. 
=============================================
[2019-04-03 22:37:21,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1477879e-26 3.3945745e-17 4.8533931e-14 1.0000000e+00 7.1499151e-18
 8.3114765e-12 1.0877928e-18], sum to 1.0000
[2019-04-03 22:37:21,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-03 22:37:21,027] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 73.0, 0.0, 0.0, 26.0, 25.09291090813137, 0.2654621140308234, 0.0, 1.0, 42293.90543760363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [-3.833333333333333, 75.0, 0.0, 0.0, 26.0, 25.05436497412045, 0.2548464068040417, 0.0, 1.0, 42197.95534893483], 
processed observation next is [1.0, 0.2608695652173913, 0.3564173591874424, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5878637478433708, 0.5849488022680139, 0.0, 1.0, 0.2009426445187373], 
reward next is 0.7991, 
noisyNet noise sample is [array([-0.08722121], dtype=float32), 1.0308955]. 
=============================================
[2019-04-03 22:37:21,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6931144e-26 3.1928149e-17 3.0454843e-13 1.0000000e+00 3.5822016e-16
 2.9900564e-09 1.6623907e-19], sum to 1.0000
[2019-04-03 22:37:21,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3514
[2019-04-03 22:37:21,368] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 26.0, 26.87165023588729, 0.7120412876341571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3772800.0000, 
sim time next is 3773400.0000, 
raw observation next is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 26.0, 26.9077387714812, 0.7153324793165563, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.23888888888888885, 0.6589318600368324, 0.6666666666666666, 0.7423115642900999, 0.7384441597721855, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5317768], dtype=float32), 0.037251417]. 
=============================================
[2019-04-03 22:37:41,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6527740e-31 5.1917367e-22 1.0134670e-17 1.0000000e+00 7.0995626e-23
 4.4577650e-16 3.8827967e-23], sum to 1.0000
[2019-04-03 22:37:41,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2099
[2019-04-03 22:37:41,243] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.76299860689167, 0.2120322687508842, 0.0, 1.0, 40423.77094260397], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.70766150910635, 0.215251483835028, 0.0, 1.0, 40401.16543666989], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5589717924255293, 0.5717504946116759, 0.0, 1.0, 0.19238650207938043], 
reward next is 0.8076, 
noisyNet noise sample is [array([0.83125865], dtype=float32), -1.0835823]. 
=============================================
[2019-04-03 22:37:49,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4997747e-32 3.4170141e-23 2.2791386e-18 1.0000000e+00 1.5315989e-21
 6.2189965e-14 7.9847115e-25], sum to 1.0000
[2019-04-03 22:37:49,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-03 22:37:49,233] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 28.0, 119.0, 840.5, 26.0, 28.17332425980817, 1.016928573208307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4363200.0000, 
sim time next is 4363800.0000, 
raw observation next is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 26.0, 28.25284425835605, 1.036944396855141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8762696214219761, 0.28166666666666673, 0.39555555555555566, 0.9333333333333335, 0.6666666666666666, 0.8544036881963374, 0.845648132285047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2946268], dtype=float32), 0.1268159]. 
=============================================
[2019-04-03 22:37:53,916] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.06193925e-30 1.47759047e-20 9.99011966e-17 1.00000000e+00
 1.04342485e-20 1.74500266e-13 9.60283157e-23], sum to 1.0000
[2019-04-03 22:37:53,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-03 22:37:53,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 26.0, 25.62360857760529, 0.560972513787685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 26.0, 25.88798166413678, 0.5788749133425245, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 0.6666666666666666, 0.6573318053447318, 0.6929583044475082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4201021], dtype=float32), -1.4145024]. 
=============================================
[2019-04-03 22:37:57,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1117280e-29 1.0000409e-17 1.6598152e-15 1.0000000e+00 2.1113653e-20
 5.5039258e-11 1.3358785e-22], sum to 1.0000
[2019-04-03 22:37:57,204] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-03 22:37:57,218] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.3, 34.0, 92.33333333333334, 0.0, 26.0, 28.80972130494817, 1.136568116867391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.44827313836435, 1.11098993546738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.826869806094183, 0.345, 0.27222222222222225, 0.0, 0.6666666666666666, 0.8706894281970291, 0.8703299784891266, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.1076226], dtype=float32), 0.8975513]. 
=============================================
[2019-04-03 22:37:57,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.61393 ]
 [90.44236 ]
 [91.255936]
 [92.17009 ]
 [93.44037 ]], R is [[91.14767456]
 [91.23619843]
 [91.32383728]
 [91.41059875]
 [91.49649048]].
[2019-04-03 22:37:59,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8637610e-33 1.0040722e-20 2.8542237e-18 1.0000000e+00 7.2386700e-25
 1.6224987e-14 6.7027279e-24], sum to 1.0000
[2019-04-03 22:37:59,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0507
[2019-04-03 22:37:59,279] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44652993431989, 0.9951267682328072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383600.0000, 
sim time next is 4384200.0000, 
raw observation next is [12.5, 43.0, 6.000000000000001, 0.0, 26.0, 27.54888427844538, 1.006463445694857, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.808864265927978, 0.43, 0.020000000000000004, 0.0, 0.6666666666666666, 0.7957403565371152, 0.8354878152316191, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20691845], dtype=float32), -0.39522567]. 
=============================================
[2019-04-03 22:38:01,126] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1655322e-25 2.1012628e-15 7.1548448e-13 1.0000000e+00 2.5569303e-17
 7.4859635e-10 5.5833124e-18], sum to 1.0000
[2019-04-03 22:38:01,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0030
[2019-04-03 22:38:01,153] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.16437137599721, 0.5461793801326474, 0.0, 1.0, 88739.53701545706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4480800.0000, 
sim time next is 4481400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.39614245142772, 0.5701762527816734, 0.0, 1.0, 45124.80553104326], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6163452042856434, 0.6900587509272245, 0.0, 1.0, 0.21488002633830125], 
reward next is 0.7851, 
noisyNet noise sample is [array([0.17235279], dtype=float32), 0.22909407]. 
=============================================
[2019-04-03 22:38:03,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2173190e-31 1.8975378e-21 5.3490190e-17 1.0000000e+00 3.9593478e-23
 1.1668073e-13 8.6459234e-25], sum to 1.0000
[2019-04-03 22:38:03,526] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-03 22:38:03,576] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.82538354377208, 0.468205885853114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4539000.0000, 
sim time next is 4539600.0000, 
raw observation next is [2.0, 52.0, 187.0, 24.0, 26.0, 25.7410262196968, 0.3668182378091009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.6233333333333333, 0.026519337016574586, 0.6666666666666666, 0.6450855183080666, 0.6222727459363669, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7954625], dtype=float32), 0.18767627]. 
=============================================
[2019-04-03 22:38:04,353] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2793415e-30 1.0572887e-19 7.9814202e-18 1.0000000e+00 1.8392258e-22
 1.9652776e-13 4.2101560e-23], sum to 1.0000
[2019-04-03 22:38:04,354] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0586
[2019-04-03 22:38:04,364] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 49.33333333333333, 125.5, 0.0, 26.0, 26.22298779505073, 0.5089233451664487, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4537200.0000, 
sim time next is 4537800.0000, 
raw observation next is [2.0, 50.0, 127.0, 0.0, 26.0, 26.08129888971366, 0.4946196116921664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5, 0.42333333333333334, 0.0, 0.6666666666666666, 0.6734415741428051, 0.6648732038973888, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1599367], dtype=float32), 0.84571856]. 
=============================================
[2019-04-03 22:38:16,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.35477105e-29 1.88942812e-19 2.09793078e-15 1.00000000e+00
 1.34306925e-20 2.03141913e-11 1.05042527e-21], sum to 1.0000
[2019-04-03 22:38:16,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-03 22:38:16,163] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.47809227176143, 0.597559943903479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 26.0, 26.47504338842134, 0.592220573802546, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.6666666666666666, 0.7062536157017784, 0.697406857934182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9889201], dtype=float32), 1.4975439]. 
=============================================
[2019-04-03 22:38:16,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1594237e-29 8.0204182e-20 9.5524048e-17 1.0000000e+00 1.5952985e-21
 1.8991366e-15 1.3424693e-21], sum to 1.0000
[2019-04-03 22:38:16,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-03 22:38:16,968] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.033333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 24.13420438404821, 0.1495293375841316, 0.0, 1.0, 41588.25712345779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.09165591819002, 0.1410376505286061, 0.0, 1.0, 41648.2063664497], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5076379931825018, 0.5470125501762021, 0.0, 1.0, 0.19832479222118907], 
reward next is 0.8017, 
noisyNet noise sample is [array([0.29520214], dtype=float32), 0.39022964]. 
=============================================
[2019-04-03 22:38:21,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0751486e-25 5.7737240e-18 2.5140873e-14 1.0000000e+00 1.7297425e-17
 1.6897243e-10 6.4595547e-19], sum to 1.0000
[2019-04-03 22:38:21,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6247
[2019-04-03 22:38:21,782] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09706233311238, 0.3658624461021128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08358657550237, 0.3652380375677605, 0.0, 1.0, 18689.59051107672], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5902988812918641, 0.6217460125225869, 0.0, 1.0, 0.08899805005274629], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.54041606], dtype=float32), 1.1504625]. 
=============================================
[2019-04-03 22:38:21,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.04872 ]
 [80.206406]
 [80.429016]
 [80.65709 ]
 [80.89655 ]], R is [[80.11327362]
 [80.31214142]
 [80.50901794]
 [80.70392609]
 [80.89688873]].
[2019-04-03 22:38:23,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7302072e-26 6.0419538e-18 2.2239744e-14 1.0000000e+00 4.3938397e-19
 1.2139317e-12 6.0245053e-19], sum to 1.0000
[2019-04-03 22:38:23,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0807
[2019-04-03 22:38:23,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.84263967778729, 0.2422687500188045, 0.0, 1.0, 39191.33812314594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 24.81479232726386, 0.2357222423419324, 0.0, 1.0, 39200.44592258374], 
processed observation next is [0.0, 0.17391304347826086, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5678993606053216, 0.5785740807806441, 0.0, 1.0, 0.18666879010754162], 
reward next is 0.8133, 
noisyNet noise sample is [array([1.0565486], dtype=float32), 0.44571403]. 
=============================================
[2019-04-03 22:38:23,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.383766]
 [77.41931 ]
 [77.47543 ]
 [77.5289  ]
 [77.582924]], R is [[77.39366913]
 [77.4331131 ]
 [77.47213745]
 [77.51074219]
 [77.54892731]].
[2019-04-03 22:38:26,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8094676e-30 1.5374778e-19 1.7263495e-15 1.0000000e+00 5.5949451e-21
 4.2382805e-13 3.5566484e-21], sum to 1.0000
[2019-04-03 22:38:26,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-03 22:38:26,472] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 25.48764462230172, 0.4747296274114809, 0.0, 1.0, 138133.5886816417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5006400.0000, 
sim time next is 5007000.0000, 
raw observation next is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45188560470893, 0.4861284426841192, 0.0, 1.0, 98192.50172383156], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.345, 0.0, 0.0, 0.6666666666666666, 0.6209904670590776, 0.6620428142280398, 0.0, 1.0, 0.46758334154205505], 
reward next is 0.5324, 
noisyNet noise sample is [array([1.0498997], dtype=float32), -1.5760239]. 
=============================================
[2019-04-03 22:38:26,490] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.72601 ]
 [83.24056 ]
 [83.030235]
 [83.30806 ]
 [83.354706]], R is [[83.64674377]
 [83.15250397]
 [82.75954437]
 [82.93195343]
 [83.10263824]].
[2019-04-03 22:38:26,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5200020e-29 2.2144747e-19 6.1105227e-16 1.0000000e+00 4.0081174e-21
 1.6701506e-13 6.2200068e-21], sum to 1.0000
[2019-04-03 22:38:26,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7897
[2019-04-03 22:38:27,013] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.166666666666667, 46.5, 0.0, 0.0, 26.0, 24.96852274129503, 0.2926737340903257, 0.0, 1.0, 35701.47584930951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906200.0000, 
sim time next is 4906800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97074361792596, 0.2911618238960345, 0.0, 1.0, 33056.75133544044], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.5808953014938302, 0.5970539412986782, 0.0, 1.0, 0.1574131015973354], 
reward next is 0.8426, 
noisyNet noise sample is [array([0.50533026], dtype=float32), -1.0137049]. 
=============================================
[2019-04-03 22:38:31,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:31,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:31,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-03 22:38:31,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:31,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:31,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-03 22:38:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:32,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:32,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-03 22:38:33,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:33,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:33,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-03 22:38:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-03 22:38:36,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-03 22:38:36,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:36,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:36,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-03 22:38:38,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:38,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:38,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-03 22:38:39,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:39,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:39,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-03 22:38:39,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:39,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:39,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-03 22:38:40,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-03 22:38:40,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-03 22:38:40,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:40,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:40,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-03 22:38:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:41,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:41,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-03 22:38:43,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:43,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:43,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-03 22:38:43,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0519635e-24 3.3820128e-16 1.3340564e-13 1.0000000e+00 1.4464163e-19
 7.1203471e-13 3.0252828e-18], sum to 1.0000
[2019-04-03 22:38:43,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0825
[2019-04-03 22:38:43,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 18.16280310554908, -1.205606516617211, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 19.0, 18.13969536199948, -1.209009933556422, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 0.08333333333333333, 0.011641280166623247, 0.09699668881452601, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.9457743], dtype=float32), 0.53449154]. 
=============================================
[2019-04-03 22:38:45,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:38:45,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:38:45,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-03 22:38:48,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2029991e-11 5.6234364e-09 4.7167514e-08 9.9999976e-01 1.6394375e-09
 2.1481345e-07 3.4316812e-09], sum to 1.0000
[2019-04-03 22:38:48,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-03 22:38:48,491] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.4, 95.33333333333334, 0.0, 0.0, 19.0, 18.69959041274062, -1.106104896984767, 0.0, 1.0, 90374.5423478948], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1200.0000, 
sim time next is 1800.0000, 
raw observation next is [3.6, 95.5, 0.0, 0.0, 19.0, 18.69357862774905, -1.093296861457242, 0.0, 1.0, 64739.41657741964], 
processed observation next is [0.0, 0.0, 0.5623268698060943, 0.955, 0.0, 0.0, 0.08333333333333333, 0.05779821897908762, 0.13556771284758598, 0.0, 1.0, 0.30828293608295065], 
reward next is 0.6917, 
noisyNet noise sample is [array([-0.40651512], dtype=float32), 0.33687654]. 
=============================================
[2019-04-03 22:38:49,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5604352e-28 6.8341890e-19 1.4692139e-15 1.0000000e+00 3.6921391e-22
 3.5222317e-14 2.2010909e-20], sum to 1.0000
[2019-04-03 22:38:49,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-03 22:38:49,234] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 19.0, 18.10743107192922, -1.215390845914321, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 19.0, 18.09589111889358, -1.212823360565247, 0.0, 1.0, 24505.09229659983], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.08333333333333333, 0.007990926574464948, 0.095725546478251, 0.0, 1.0, 0.11669091569809442], 
reward next is 0.8833, 
noisyNet noise sample is [array([0.26545215], dtype=float32), -0.19908729]. 
=============================================
[2019-04-03 22:38:51,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1918167e-19 3.5488410e-14 4.1694814e-11 1.0000000e+00 2.0904744e-14
 7.0329387e-09 1.8415150e-14], sum to 1.0000
[2019-04-03 22:38:51,305] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9377
[2019-04-03 22:38:51,318] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.466666666666667, 75.66666666666666, 0.0, 0.0, 19.0, 18.70553870019928, -1.157581621611581, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 103200.0000, 
sim time next is 103800.0000, 
raw observation next is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 19.0, 18.61253706030474, -1.163457496398047, 0.0, 1.0, 105218.0500970827], 
processed observation next is [1.0, 0.17391304347826086, 0.3314866112650046, 0.7483333333333334, 0.0, 0.0, 0.08333333333333333, 0.051044755025394885, 0.11218083453398431, 0.0, 1.0, 0.5010383337956319], 
reward next is 0.4990, 
noisyNet noise sample is [array([0.6440058], dtype=float32), 1.2369432]. 
=============================================
[2019-04-03 22:38:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5286529e-19 9.4170062e-12 3.4040923e-10 2.9055548e-01 1.7766478e-11
 7.0944452e-01 7.3647055e-14], sum to 1.0000
[2019-04-03 22:38:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-03 22:38:56,112] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 24.53107968086193, 0.06075003949760924, 1.0, 1.0, 83310.5665070343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 26.0, 24.78676601621358, 0.0911991689912332, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.6666666666666666, 0.565563834684465, 0.5303997229970777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4815004], dtype=float32), 0.2688832]. 
=============================================
[2019-04-03 22:38:56,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5988989e-22 8.9957633e-12 2.9198075e-11 9.9396402e-01 4.4779621e-15
 6.0359621e-03 2.0611377e-16], sum to 1.0000
[2019-04-03 22:38:56,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6560
[2019-04-03 22:38:56,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 63.5, 49.66666666666667, 31.0, 25.0, 24.00169828362197, -0.02964809047561351, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 143400.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 25.0, 24.33907164804834, -0.0006436305895128706, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.5833333333333334, 0.528255970670695, 0.4997854564701624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.290521], dtype=float32), -1.9576154]. 
=============================================
[2019-04-03 22:38:56,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.188835]
 [78.76201 ]
 [76.14372 ]
 [73.912224]
 [71.58141 ]], R is [[81.64266205]
 [81.82623291]
 [81.03973389]
 [80.26817322]
 [79.51072693]].
[2019-04-03 22:38:58,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5321910e-27 6.1274839e-18 3.7078018e-16 9.9999964e-01 8.0899562e-19
 3.5984738e-07 6.3173082e-20], sum to 1.0000
[2019-04-03 22:38:58,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7898
[2019-04-03 22:38:58,488] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.3946273436661, -0.3717219154498438, 0.0, 1.0, 44791.59532069331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 185400.0000, 
sim time next is 186000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.32130141683499, -0.3855446037210558, 0.0, 1.0, 44839.13826061658], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.360108451402916, 0.37148513209298145, 0.0, 1.0, 0.2135197060029361], 
reward next is 0.7865, 
noisyNet noise sample is [array([-0.03085002], dtype=float32), -0.3601681]. 
=============================================
[2019-04-03 22:38:58,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[90.88775 ]
 [90.82867 ]
 [90.743065]
 [90.421295]
 [90.12301 ]], R is [[90.80846405]
 [90.68708801]
 [90.56708527]
 [90.44850159]
 [90.33139038]].
[2019-04-03 22:39:04,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0568748e-21 2.7494191e-14 3.5920834e-12 9.5579362e-01 1.1142270e-14
 4.4206362e-02 1.0171382e-15], sum to 1.0000
[2019-04-03 22:39:04,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0827
[2019-04-03 22:39:04,278] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.116666666666666, 74.5, 109.0, 0.0, 26.0, 25.23195931975694, 0.1384664807783725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 209400.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 26.0, 25.21473017345436, 0.1282427953108679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 0.6666666666666666, 0.6012275144545299, 0.542747598436956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3776963], dtype=float32), 0.9631029]. 
=============================================
[2019-04-03 22:39:04,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.4594 ]
 [79.60277]
 [79.86413]
 [80.05373]
 [80.47092]], R is [[79.54271698]
 [79.74729156]
 [79.94982147]
 [80.15032196]
 [80.34881592]].
[2019-04-03 22:39:08,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4520392e-23 9.4367141e-14 2.5762167e-13 9.9990594e-01 3.6994299e-16
 9.4065763e-05 1.8145014e-16], sum to 1.0000
[2019-04-03 22:39:08,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0681
[2019-04-03 22:39:08,220] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.67823501694714, 0.2480833469084845, 1.0, 1.0, 174162.1214051498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.06294208885637, 0.3098030514399574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5885785074046975, 0.6032676838133192, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36852694], dtype=float32), 1.1770533]. 
=============================================
[2019-04-03 22:39:11,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6186241e-20 1.6166488e-13 8.9820047e-11 9.9998868e-01 6.7960393e-15
 1.1306737e-05 2.3110102e-15], sum to 1.0000
[2019-04-03 22:39:11,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1727
[2019-04-03 22:39:11,623] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 23.81504663752546, -0.02389029999115069, 0.0, 1.0, 45373.36173240308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 26.0, 23.74867681074454, -0.0291598069048134, 0.0, 1.0, 45516.06909140416], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.6666666666666666, 0.47905640089537843, 0.4902800643650622, 0.0, 1.0, 0.21674318614954363], 
reward next is 0.7833, 
noisyNet noise sample is [array([-0.10045591], dtype=float32), -0.5069225]. 
=============================================
[2019-04-03 22:39:17,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6013212e-22 1.4777223e-14 5.3224430e-12 9.9999475e-01 2.5592810e-15
 5.2465221e-06 3.9753075e-16], sum to 1.0000
[2019-04-03 22:39:17,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5144
[2019-04-03 22:39:17,775] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.22099284053636, -0.1310931956878553, 0.0, 1.0, 47649.00693669276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 348000.0000, 
sim time next is 348600.0000, 
raw observation next is [-14.4, 68.5, 0.0, 0.0, 26.0, 23.19825233626228, -0.1401318659724934, 0.0, 1.0, 47699.44528340721], 
processed observation next is [1.0, 0.0, 0.0637119113573407, 0.685, 0.0, 0.0, 0.6666666666666666, 0.4331876946885232, 0.45328937800916885, 0.0, 1.0, 0.22714021563527242], 
reward next is 0.7729, 
noisyNet noise sample is [array([0.73290205], dtype=float32), 0.10590577]. 
=============================================
[2019-04-03 22:39:34,279] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4582779e-33 1.0199302e-21 7.1776494e-19 1.0000000e+00 4.4404497e-25
 1.2125364e-12 3.5446696e-24], sum to 1.0000
[2019-04-03 22:39:34,286] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-03 22:39:34,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.783333333333333, 95.33333333333333, 0.0, 0.0, 26.0, 24.83877007456969, 0.2242059232675625, 0.0, 1.0, 42150.74192791605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 508200.0000, 
sim time next is 508800.0000, 
raw observation next is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 26.0, 24.8305119814289, 0.2225151841040347, 0.0, 1.0, 41591.94716732633], 
processed observation next is [1.0, 0.9130434782608695, 0.5170821791320407, 0.9466666666666665, 0.0, 0.0, 0.6666666666666666, 0.5692093317857415, 0.5741717280346782, 0.0, 1.0, 0.19805689127298254], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.28406724], dtype=float32), 0.87665284]. 
=============================================
[2019-04-03 22:39:43,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7758487e-28 4.7265985e-18 1.1332176e-15 1.0000000e+00 3.5487493e-20
 5.2923367e-11 1.8134297e-20], sum to 1.0000
[2019-04-03 22:39:43,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5633
[2019-04-03 22:39:43,664] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93692411629354, 0.279626652908787, 0.0, 1.0, 46124.75625581931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.9646483493171, 0.2870491075064719, 0.0, 1.0, 124012.5975553091], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5803873624430915, 0.5956830358354906, 0.0, 1.0, 0.5905361788348052], 
reward next is 0.4095, 
noisyNet noise sample is [array([-1.1283033], dtype=float32), 1.1364727]. 
=============================================
[2019-04-03 22:39:43,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.30784 ]
 [78.3611  ]
 [78.415794]
 [78.45112 ]
 [78.55515 ]], R is [[78.09964752]
 [78.09900665]
 [78.05374908]
 [77.99729156]
 [77.97625732]].
[2019-04-03 22:39:43,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0490798e-28 1.6446663e-19 1.5842531e-16 1.0000000e+00 5.1911650e-21
 3.2442211e-09 1.2143198e-21], sum to 1.0000
[2019-04-03 22:39:43,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2523
[2019-04-03 22:39:44,025] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.91267634812948, 0.3235465780802169, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727800.0000, 
sim time next is 728400.0000, 
raw observation next is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 26.0, 25.85759925325688, 0.3228313743149845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6733333333333335, 0.4422222222222224, 0.07163904235727442, 0.6666666666666666, 0.6547999377714065, 0.6076104581049948, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0111196], dtype=float32), -0.1359199]. 
=============================================
[2019-04-03 22:39:44,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5803775e-24 1.8569618e-16 5.3704108e-15 1.0000000e+00 3.5319234e-19
 4.4290247e-09 1.9194542e-18], sum to 1.0000
[2019-04-03 22:39:44,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8463
[2019-04-03 22:39:44,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.67206221725558, 0.1495234284465078, 0.0, 1.0, 41795.74568487392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.67863897517698, 0.1443381193880831, 0.0, 1.0, 41715.57567237774], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.556553247931415, 0.5481127064626944, 0.0, 1.0, 0.198645598439894], 
reward next is 0.8014, 
noisyNet noise sample is [array([-1.4946232], dtype=float32), 1.2297708]. 
=============================================
[2019-04-03 22:39:57,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1139065e-27 7.8959609e-18 3.0629457e-15 1.0000000e+00 3.2560433e-20
 2.6905095e-08 4.8517309e-20], sum to 1.0000
[2019-04-03 22:39:57,221] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5440
[2019-04-03 22:39:57,316] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.7005611851027, 0.2898529093313969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.64976262520116, 0.2879480753041654, 1.0, 1.0, 44107.12059276935], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6374802187667633, 0.5959826917680552, 1.0, 1.0, 0.21003390758461596], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.62461996], dtype=float32), -1.3960743]. 
=============================================
[2019-04-03 22:40:02,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0285771e-26 4.5257665e-18 3.6508221e-15 1.0000000e+00 5.1009504e-19
 3.8718664e-08 1.9409533e-19], sum to 1.0000
[2019-04-03 22:40:02,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0487
[2019-04-03 22:40:02,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.50687355590981, 0.2900225707319784, 1.0, 1.0, 27790.7972225566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.428720771584, 0.2867311260333665, 1.0, 1.0, 27881.13490143905], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6190600642986667, 0.5955770420111222, 1.0, 1.0, 0.13276730905447168], 
reward next is 0.8672, 
noisyNet noise sample is [array([1.353993], dtype=float32), -1.2341284]. 
=============================================
[2019-04-03 22:40:03,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1922424e-32 1.8244275e-20 1.2856850e-18 1.0000000e+00 1.1842448e-23
 1.9440745e-10 3.7359849e-24], sum to 1.0000
[2019-04-03 22:40:03,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6318
[2019-04-03 22:40:03,470] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 26.0, 26.73732321615047, 0.5714372229190067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 25.77773553062474, 0.5784693669379505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 0.6666666666666666, 0.6481446275520616, 0.6928231223126501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6066203], dtype=float32), -1.5133151]. 
=============================================
[2019-04-03 22:40:14,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7452840e-26 1.8073483e-17 1.8208652e-15 1.0000000e+00 1.7408476e-20
 1.1704139e-09 3.1014452e-19], sum to 1.0000
[2019-04-03 22:40:14,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-03 22:40:14,182] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 26.48958882970687, 0.7672880619246326, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [15.0, 57.0, 0.0, 0.0, 26.0, 26.39774794761123, 0.7642225186729116, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8781163434903049, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6998123289676025, 0.7547408395576372, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19066842], dtype=float32), -2.174967]. 
=============================================
[2019-04-03 22:40:17,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8109912e-33 4.3106855e-24 9.9116801e-20 1.0000000e+00 3.7824860e-26
 1.1013400e-14 4.4012241e-25], sum to 1.0000
[2019-04-03 22:40:17,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2422
[2019-04-03 22:40:17,257] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 26.0, 25.66046962917044, 0.6096030320953489, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 26.0, 25.65482285862309, 0.6262480164015342, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.6666666666666666, 0.6379019048852573, 0.7087493388005114, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0283725], dtype=float32), 0.57396036]. 
=============================================
[2019-04-03 22:40:23,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7418597e-31 2.3842891e-21 7.8125146e-19 1.0000000e+00 8.6147792e-25
 2.4803264e-11 9.7795331e-24], sum to 1.0000
[2019-04-03 22:40:23,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0272
[2019-04-03 22:40:23,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.0, 98.66666666666666, 100.0, 0.0, 26.0, 25.00157805506298, 0.4785204946282536, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1255200.0000, 
sim time next is 1255800.0000, 
raw observation next is [13.9, 99.33333333333334, 99.0, 0.0, 26.0, 24.96958191606826, 0.4740753900667249, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.847645429362881, 0.9933333333333334, 0.33, 0.0, 0.6666666666666666, 0.5807984930056884, 0.6580251300222416, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5978577], dtype=float32), 1.3314947]. 
=============================================
[2019-04-03 22:40:26,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4686396e-26 4.4985381e-17 4.5172695e-15 9.9999988e-01 2.5639193e-20
 7.5753107e-08 5.3272419e-20], sum to 1.0000
[2019-04-03 22:40:26,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6523
[2019-04-03 22:40:26,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 26.0, 25.99214656936732, 0.5070150491534328, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 26.0, 25.94461186296463, 0.5007392618595353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.6666666666666666, 0.6620509885803859, 0.6669130872865118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40135777], dtype=float32), 0.64321893]. 
=============================================
[2019-04-03 22:40:39,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1623704e-26 5.5960821e-19 9.7882323e-16 1.0000000e+00 8.1906623e-19
 3.5420233e-11 6.1740813e-19], sum to 1.0000
[2019-04-03 22:40:39,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2955
[2019-04-03 22:40:39,549] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 26.0, 25.28076843055315, 0.4677264320497178, 0.0, 1.0, 44039.32004191025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 26.0, 25.39706354767142, 0.4675321647058615, 0.0, 1.0, 25372.60414044195], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6164219623059516, 0.6558440549019539, 0.0, 1.0, 0.120821924478295], 
reward next is 0.8792, 
noisyNet noise sample is [array([-0.35090217], dtype=float32), -0.03752158]. 
=============================================
[2019-04-03 22:40:54,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9089982e-27 1.2579591e-18 8.2799879e-16 1.0000000e+00 2.8969681e-20
 1.0760191e-09 1.2096053e-19], sum to 1.0000
[2019-04-03 22:40:54,899] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0988
[2019-04-03 22:40:54,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.01811397073084, 0.4381441953792087, 0.0, 1.0, 26699.47565576638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713600.0000, 
sim time next is 1714200.0000, 
raw observation next is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 24.94740115376582, 0.4343009308013339, 0.0, 1.0, 69324.8448212488], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5789500961471518, 0.6447669769337779, 0.0, 1.0, 0.3301183086726133], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.10680385], dtype=float32), 0.36165965]. 
=============================================
[2019-04-03 22:41:01,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6699827e-23 7.1609809e-15 2.0834201e-11 1.0000000e+00 1.1732991e-17
 1.9832784e-08 2.3276586e-17], sum to 1.0000
[2019-04-03 22:41:01,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2851
[2019-04-03 22:41:01,194] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.08335039608588, 0.4773127051318155, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1711200.0000, 
sim time next is 1711800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.13621639966397, 0.47223781713526, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5946846999719974, 0.6574126057117533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00750216], dtype=float32), 0.45779625]. 
=============================================
[2019-04-03 22:41:12,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9864233e-24 5.3990807e-17 8.1226658e-14 1.0000000e+00 2.8014736e-19
 4.9111193e-10 1.8120278e-18], sum to 1.0000
[2019-04-03 22:41:12,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0469
[2019-04-03 22:41:12,083] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.100000000000001, 78.83333333333334, 0.0, 0.0, 26.0, 24.50234516792595, 0.1857213187043273, 0.0, 1.0, 45590.39129024601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1815000.0000, 
sim time next is 1815600.0000, 
raw observation next is [-5.2, 78.66666666666667, 0.0, 0.0, 26.0, 24.47233176743466, 0.1788063224992393, 0.0, 1.0, 45647.24732417997], 
processed observation next is [0.0, 0.0, 0.31855955678670367, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5393609806195551, 0.5596021074997465, 0.0, 1.0, 0.217367844400857], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.768207], dtype=float32), 1.419728]. 
=============================================
[2019-04-03 22:41:19,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1937383e-22 2.0685434e-15 3.7117567e-13 9.9999988e-01 2.3730073e-16
 1.7193013e-07 6.0280741e-17], sum to 1.0000
[2019-04-03 22:41:19,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8301
[2019-04-03 22:41:19,524] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02390830763958, 0.2848511118981354, 0.0, 1.0, 45305.79505239567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02573709589707, 0.2847454880482947, 0.0, 1.0, 43143.58934885474], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.6666666666666666, 0.5854780913247559, 0.5949151626827649, 0.0, 1.0, 0.20544566356597493], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.54003066], dtype=float32), -0.2472174]. 
=============================================
[2019-04-03 22:41:25,803] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3138450e-27 1.2577672e-18 5.5544905e-16 1.0000000e+00 6.2727618e-21
 2.6306000e-09 1.6764894e-20], sum to 1.0000
[2019-04-03 22:41:25,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3349
[2019-04-03 22:41:25,906] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.96404485164885, 0.3667516443947216, 0.0, 1.0, 55084.00507041119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 25.16429748862451, 0.381251409777945, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5970247907187091, 0.627083803259315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2542285], dtype=float32), -1.2103591]. 
=============================================
[2019-04-03 22:41:42,635] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-03 22:41:42,636] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:41:42,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,636] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:41:42,637] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,645] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:41:42,646] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:41:42,647] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:41:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:41:42,742] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/2/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-03 22:44:17,534] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7267.6968 188917510.1569 -443.5460
[2019-04-03 22:44:27,599] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.73323405], dtype=float32), 0.25206852]
[2019-04-03 22:44:27,599] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.79469208, 71.93855661, 0.0, 0.0, 25.0, 24.13833095524847, 0.1879957358938009, 0.0, 1.0, 0.0]
[2019-04-03 22:44:27,600] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:44:27,600] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9445825e-24 4.4659198e-15 1.7273986e-13 1.0000000e+00 4.5355466e-19
 6.9893171e-09 4.8811411e-18], sampled 0.5303608550016032
[2019-04-03 22:44:44,610] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7407.7463 223362470.3656 -399.9828
[2019-04-03 22:45:17,797] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7227.1198 261048080.5577 579.7060
[2019-04-03 22:45:18,846] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 600000, evaluation results [600000.0, 7407.746345491542, 223362470.3655589, -399.9828152270937, 7267.696820125497, 188917510.1569302, -443.5459585406526, 7227.119769459546, 261048080.55773956, 579.706023077094]
[2019-04-03 22:45:20,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8853568e-30 1.3261256e-21 9.9133543e-18 1.0000000e+00 2.0127487e-22
 1.8290087e-13 4.7262623e-23], sum to 1.0000
[2019-04-03 22:45:20,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-03 22:45:20,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.22017748895677, 0.3099482685266184, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2188200.0000, 
sim time next is 2188800.0000, 
raw observation next is [-5.6, 75.0, 21.5, 131.0, 26.0, 25.49588567166465, 0.3471356930734757, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.07166666666666667, 0.14475138121546963, 0.6666666666666666, 0.6246571393053874, 0.6157118976911585, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6288556], dtype=float32), -0.37251762]. 
=============================================
[2019-04-03 22:45:23,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1808866e-27 2.9075316e-19 2.3832405e-16 1.0000000e+00 2.5646631e-20
 3.3116829e-11 2.9838243e-20], sum to 1.0000
[2019-04-03 22:45:23,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0220
[2019-04-03 22:45:23,613] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 82.0, 123.0, 77.5, 26.0, 25.48138615200819, 0.3384670860147466, 1.0, 1.0, 18741.53088383785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2106000.0000, 
sim time next is 2106600.0000, 
raw observation next is [-7.8, 82.00000000000001, 140.0, 91.0, 26.0, 25.56952479364689, 0.3455196559705467, 1.0, 1.0, 18737.94707287288], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.8200000000000002, 0.4666666666666667, 0.1005524861878453, 0.6666666666666666, 0.6307937328039076, 0.6151732186568489, 1.0, 1.0, 0.08922831939463277], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.85034204], dtype=float32), -0.6720641]. 
=============================================
[2019-04-03 22:45:28,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8146343e-27 2.2585797e-21 2.2207550e-16 1.0000000e+00 3.0147153e-20
 2.8278659e-11 7.1168423e-21], sum to 1.0000
[2019-04-03 22:45:28,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3530
[2019-04-03 22:45:28,744] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 26.0, 23.87998084915323, 0.02471747262777029, 0.0, 1.0, 41905.91843185117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2180400.0000, 
sim time next is 2181000.0000, 
raw observation next is [-6.2, 78.33333333333334, 0.0, 0.0, 26.0, 23.83145146150664, 0.01227527021832714, 0.0, 1.0, 41896.12780058199], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.48595428845888655, 0.5040917567394424, 0.0, 1.0, 0.19950537047896186], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.74393755], dtype=float32), -0.8833738]. 
=============================================
[2019-04-03 22:45:28,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.49252 ]
 [82.438965]
 [82.40408 ]
 [82.398926]
 [82.39921 ]], R is [[82.50507355]
 [82.48046875]
 [82.45594788]
 [82.43160248]
 [82.4074707 ]].
[2019-04-03 22:45:29,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8728137e-27 2.1548006e-19 2.7077449e-16 1.0000000e+00 5.0600572e-19
 4.1591085e-11 1.7896518e-20], sum to 1.0000
[2019-04-03 22:45:29,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2284
[2019-04-03 22:45:29,254] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.99377385663495, 0.3981764884641075, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197200.0000, 
sim time next is 2197800.0000, 
raw observation next is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.96452967655491, 0.3853483414297085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3310249307479225, 0.71, 0.39, 0.0, 0.6666666666666666, 0.6637108063795759, 0.6284494471432361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52713597], dtype=float32), 0.094755985]. 
=============================================
[2019-04-03 22:45:39,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7620702e-24 6.5920704e-16 1.0958907e-12 9.9999988e-01 2.3178850e-17
 7.3999701e-08 1.7753953e-17], sum to 1.0000
[2019-04-03 22:45:39,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-03 22:45:39,954] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.88776368137875, 0.3947350912910308, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305800.0000, 
sim time next is 2306400.0000, 
raw observation next is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 26.0, 25.81877906675232, 0.3865886356230021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.45152354570637127, 0.4733333333333333, 0.1166666666666667, 0.0, 0.6666666666666666, 0.65156492222936, 0.6288628785410008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1876587], dtype=float32), 0.9357427]. 
=============================================
[2019-04-03 22:45:46,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6421429e-26 7.0073721e-18 4.7836537e-15 1.0000000e+00 6.1874939e-20
 3.6246837e-12 1.8592555e-19], sum to 1.0000
[2019-04-03 22:45:46,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-03 22:45:46,715] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15855606131137, 0.2810488123752466, 0.0, 1.0, 43113.64613146947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.13559495472015, 0.2756497768725827, 0.0, 1.0, 43156.72813957219], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.594632912893346, 0.5918832589575276, 0.0, 1.0, 0.20550822923605805], 
reward next is 0.7945, 
noisyNet noise sample is [array([0.44962212], dtype=float32), -0.5034757]. 
=============================================
[2019-04-03 22:45:58,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9342215e-31 3.8832429e-22 2.3450175e-19 1.0000000e+00 2.2310395e-22
 2.0627697e-13 6.7168862e-23], sum to 1.0000
[2019-04-03 22:45:58,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9391
[2019-04-03 22:45:58,689] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 59.0, 0.0, 0.0, 26.0, 24.99521964086517, 0.3759343569657551, 1.0, 1.0, 91820.73423881375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [-1.2, 60.0, 0.0, 0.0, 26.0, 25.06536919033127, 0.3934296300442569, 1.0, 1.0, 20917.24031758263], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5887807658609393, 0.6311432100147524, 1.0, 1.0, 0.099605906274203], 
reward next is 0.9004, 
noisyNet noise sample is [array([0.48277196], dtype=float32), 0.96200264]. 
=============================================
