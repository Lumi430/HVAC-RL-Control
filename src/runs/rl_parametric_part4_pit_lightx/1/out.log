Using TensorFlow backend.
[2019-04-10 14:12:31,601] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v3', activation='linear', agent_num=5, check_args_only=False, clip_norm=1.0, debug_log_prob=0.001, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-Repeat-v2', eval_act_func='part4_v4', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=50000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part4_v2', model_dir='None', model_param=[13, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-Repeat-v2-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_v7', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=15.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=11, test_env=['Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'], test_mode='Multiple', train_act_func='part4_v4', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=5.0, weight_initer='glorot_uniform', window_len=10)
[2019-04-10 14:12:31,601] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-10 14:12:31.632420: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-10 14:12:46,103] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-10 14:12:46,103] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-Repeat-v2', 'Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'] ...
[2019-04-10 14:12:46,119] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation worker starts!
[2019-04-10 14:12:46,129] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation worker starts!
[2019-04-10 14:12:46,137] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation worker starts!
[2019-04-10 14:12:46,137] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:46,137] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-10 14:12:46,191] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:46,192] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run1
[2019-04-10 14:12:47,138] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:47,140] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-10 14:12:47,207] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:47,208] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run1
[2019-04-10 14:12:48,141] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:48,142] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-10 14:12:48,209] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:48,210] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run1
[2019-04-10 14:12:49,143] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:49,144] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-10 14:12:49,212] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:49,213] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run1
[2019-04-10 14:12:50,145] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:50,146] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-10 14:12:50,219] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:50,221] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run1
[2019-04-10 14:12:51,147] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:51,148] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-10 14:12:51,232] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:51,233] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run1
[2019-04-10 14:12:51,348] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 14:12:51,349] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:12:51,349] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:12:51,349] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:51,349] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:12:51,350] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:51,350] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:51,354] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run1
[2019-04-10 14:12:51,360] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run1
[2019-04-10 14:12:51,368] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run1
[2019-04-10 14:12:52,148] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:52,149] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-10 14:12:52,240] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:52,242] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run1
[2019-04-10 14:12:53,150] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:53,151] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-10 14:12:53,245] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:53,247] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run1
[2019-04-10 14:12:54,152] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:54,153] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-10 14:12:54,248] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:54,250] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run1
[2019-04-10 14:12:55,154] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:55,155] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-10 14:12:55,282] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:55,283] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run1
[2019-04-10 14:12:56,155] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:56,157] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-10 14:12:56,244] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:56,246] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run1
[2019-04-10 14:12:57,158] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:57,159] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-10 14:12:57,262] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:57,264] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run1
[2019-04-10 14:12:58,160] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:58,162] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-10 14:12:58,258] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:58,259] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run1
[2019-04-10 14:12:59,162] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:12:59,163] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-10 14:12:59,261] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:12:59,263] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run1
[2019-04-10 14:13:00,164] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:13:00,165] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-10 14:13:00,261] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:13:00,262] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run1
[2019-04-10 14:13:01,165] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:13:01,166] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-10 14:13:01,266] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:13:01,267] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run1
[2019-04-10 14:13:16,188] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 14:13:16,189] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [0.268916293, 90.73385788, 0.0, 0.0, 19.0, 21.6788510404751, -0.4975795642550881, 0.0, 1.0, 35.0, 27.076938940759195]
[2019-04-10 14:13:16,189] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-10 14:13:16,191] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.10221428 0.02158702 0.02737065 0.01631059 0.59713656 0.02246433
 0.01596531 0.11177315 0.03078558 0.01451464 0.03987782], sampled 0.5651542663456821
[2019-04-10 14:13:17,514] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 14:13:17,514] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-1.439841635, 79.87267297, 0.0, 0.0, 19.0, 18.36413141883173, -1.198282272623056, 0.0, 1.0, 60.0, 55.05645701209532]
[2019-04-10 14:13:17,514] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-10 14:13:17,514] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.08220019 0.03491341 0.06318591 0.02552911 0.47666115 0.0271439
 0.03278269 0.10463079 0.06769302 0.02776711 0.05749273], sampled 0.6190841415811364
[2019-04-10 14:14:08,117] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3024.5403 119997.6341 796.5331
[2019-04-10 14:14:08,137] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:14:08,242] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:14:08,415] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 14:14:08,415] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-4.0, 35.33333333333334, 0.0, 0.0, 19.0, 22.76892658354274, -0.2133079208431396, 0.0, 1.0, 35.0, 20.47034149954601]
[2019-04-10 14:14:08,415] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-10 14:14:08,416] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.10444424 0.03030525 0.07672848 0.02767492 0.5043166  0.02702565
 0.02511356 0.10161433 0.04232347 0.02024092 0.04021258], sampled 0.7106149587142074
[2019-04-10 14:14:20,023] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2965.7334 128392.7721 492.9824
[2019-04-10 14:14:20,043] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:14:20,149] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:14:21,945] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2944.9096 129512.0603 261.8235
[2019-04-10 14:14:21,964] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:14:22,077] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:14:22,967] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 2965.7334104497763, 128392.77208460968, 492.9823539736225, 3024.540304738464, 119997.63407582858, 796.53305021464, 2944.909586193057, 129512.06033024938, 261.823474438473]
[2019-04-10 14:14:24,811] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.10904054 0.03284059 0.06245739 0.03079404 0.38075233 0.065255
 0.04888906 0.08855119 0.09553861 0.04354928 0.04233195], sum to 1.0000
[2019-04-10 14:14:24,818] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5536
[2019-04-10 14:14:24,936] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 20.89986148177638, -0.6245569096896054, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 6000.0000, 
sim time next is 7200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 20.77951278034835, -0.6021375084060328, 0.0, 1.0, 35.0, 28.579731879021153], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.2316260650290293, 0.2992874971979891, 0.0, 1.0, 0.4, 0.28579731879021153], 
reward next is 0.7142, 
noisyNet noise sample is [array([-0.33308378], dtype=float32), -1.2661653]. 
=============================================
[2019-04-10 14:14:26,386] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.10819122 0.03139585 0.05476689 0.02639084 0.46704513 0.01933204
 0.01715591 0.17157277 0.02688837 0.01528609 0.06197484], sum to 1.0000
[2019-04-10 14:14:26,390] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0993
[2019-04-10 14:14:26,482] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 19.0, 23.0701086950751, -0.1732998779921293, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 58800.0000, 
sim time next is 60000.0000, 
raw observation next is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 19.0, 22.53370324977123, -0.2754487320758809, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6251154201292707, 0.8466666666666666, 0.05, 0.0, 0.08333333333333333, 0.37780860414760237, 0.40818375597470635, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2397904], dtype=float32), -2.1785]. 
=============================================
[2019-04-10 14:14:26,492] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.4109603]
 [1.3832505]
 [1.8118382]
 [1.5223327]
 [1.4478323]], R is [[2.6313014 ]
 [3.60498834]
 [4.19634438]
 [4.86417007]
 [5.44379616]].
[2019-04-10 14:14:28,745] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04100867 0.02322093 0.01387005 0.01289628 0.62317103 0.00722229
 0.00952702 0.17044191 0.02146709 0.00394331 0.07323145], sum to 1.0000
[2019-04-10 14:14:28,746] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-10 14:14:28,987] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 22.5, 21.42312183815036, -0.4173283626235129, 1.0, 1.0, 50.0, 66.22944726548286], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 120000.0000, 
sim time next is 121200.0000, 
raw observation next is [-7.799999999999999, 69.66666666666666, 69.16666666666666, 5.999999999999998, 22.5, 22.67669559167578, -0.2982033253347976, 1.0, 1.0, 35.0, 40.38007082979952], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188372, 0.6966666666666665, 0.2305555555555555, 0.006629834254143645, 0.375, 0.3897246326396484, 0.4005988915550675, 1.0, 1.0, 0.4, 0.4038007082979952], 
reward next is 0.5962, 
noisyNet noise sample is [array([-1.1135559], dtype=float32), 0.6609471]. 
=============================================
[2019-04-10 14:14:34,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3572222e-03 5.2274391e-03 9.9064540e-03 6.5798163e-03 4.6663558e-01
 4.6367248e-04 4.8699186e-04 4.6814218e-01 2.1174317e-03 1.4299541e-04
 3.4940176e-02], sum to 1.0000
[2019-04-10 14:14:34,035] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6393
[2019-04-10 14:14:34,243] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 22.5, 22.81927266929776, -0.341208567019282, 1.0, 1.0, 35.0, 39.9333520743952], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 211200.0000, 
sim time next is 212400.0000, 
raw observation next is [-6.2, 72.0, 138.5, 0.0, 22.5, 23.0195350365841, -0.3919130432541688, 1.0, 1.0, 50.0, 55.09804188761768], 
processed observation next is [1.0, 0.4782608695652174, 0.2908587257617729, 0.72, 0.46166666666666667, 0.0, 0.375, 0.4182945863820082, 0.36936231891527704, 1.0, 1.0, 0.7, 0.5509804188761769], 
reward next is 0.4490, 
noisyNet noise sample is [array([-1.048939], dtype=float32), -0.56581163]. 
=============================================
[2019-04-10 14:14:40,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8563551e-04 5.7635512e-05 2.7806824e-04 9.0944799e-05 9.9030262e-01
 2.0961784e-06 1.4199111e-06 7.8553455e-03 9.6911244e-06 6.3501926e-07
 1.2160108e-03], sum to 1.0000
[2019-04-10 14:14:40,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6990
[2019-04-10 14:14:40,386] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.33333333333333, 69.0, 0.0, 0.0, 19.0, 19.40699071084465, -0.9680893977477859, 0.0, 1.0, 35.0, 29.694883956689203], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 279600.0000, 
sim time next is 280800.0000, 
raw observation next is [-11.7, 70.0, 0.0, 0.0, 19.0, 19.26104799602276, -0.9973382715627719, 0.0, 1.0, 35.0, 29.770500170918957], 
processed observation next is [1.0, 0.2608695652173913, 0.13850415512465375, 0.7, 0.0, 0.0, 0.08333333333333333, 0.10508733300189672, 0.16755390947907603, 0.0, 1.0, 0.4, 0.29770500170918957], 
reward next is 0.7023, 
noisyNet noise sample is [array([0.97453994], dtype=float32), -0.88067013]. 
=============================================
[2019-04-10 14:14:45,080] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8560509e-05 1.1998645e-05 5.9172606e-05 2.8020871e-05 9.9604934e-01
 2.4084466e-07 2.1643395e-07 3.5472088e-03 1.9597956e-06 3.9084803e-08
 2.5328016e-04], sum to 1.0000
[2019-04-10 14:14:45,081] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4762
[2019-04-10 14:14:45,193] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 19.0, 17.97294094198976, -1.329650732842551, 0.0, 1.0, 35.0, 31.396414372126134], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 351600.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 17.96537562365366, -1.349459726959003, 0.0, 1.0, 35.0, 31.69903768977347], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, -0.0028853646955283216, 0.05018009101366568, 0.0, 1.0, 0.4, 0.3169903768977347], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.4585452], dtype=float32), 0.22723064]. 
=============================================
[2019-04-10 14:14:49,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6589109e-05 5.6651320e-06 5.9972175e-05 6.1564519e-05 2.0302871e-02
 1.3353585e-08 6.6550352e-09 9.7903436e-01 7.2480049e-08 1.3063987e-09
 5.1884248e-04], sum to 1.0000
[2019-04-10 14:14:49,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9216
[2019-04-10 14:14:49,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8194554e-04 1.9364183e-04 5.7362963e-04 2.5729107e-04 5.6833223e-02
 1.4147560e-06 1.4886859e-06 9.3806398e-01 8.5372149e-06 4.5485001e-07
 3.7843545e-03], sum to 1.0000
[2019-04-10 14:14:49,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5316
[2019-04-10 14:14:50,095] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 22.5, 23.82950924152228, -0.0840193291038495, 1.0, 1.0, 50.0, 50.07931312793629], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 402000.0000, 
sim time next is 403200.0000, 
raw observation next is [-8.9, 38.0, 29.0, 555.0, 22.5, 24.43408920805492, -0.017821883767679, 1.0, 1.0, 50.0, 49.45998550565288], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.38, 0.09666666666666666, 0.6132596685082873, 0.375, 0.5361741006712434, 0.4940593720774404, 1.0, 1.0, 0.7, 0.4945998550565288], 
reward next is 0.5054, 
noisyNet noise sample is [array([-2.0773137], dtype=float32), 2.8538122]. 
=============================================
[2019-04-10 14:14:50,159] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 19.0, 20.64024871498395, -0.8149093991225715, 0.0, 1.0, 50.0, 45.21893117037488], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 438000.0000, 
sim time next is 439200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 19.0, 20.55220405007047, -0.8287670486154025, 0.0, 1.0, 50.0, 45.30584472256068], 
processed observation next is [1.0, 0.08695652173913043, 0.15235457063711913, 0.55, 0.0, 0.0, 0.08333333333333333, 0.2126836708392057, 0.22374431712819917, 0.0, 1.0, 0.7, 0.4530584472256068], 
reward next is 0.5469, 
noisyNet noise sample is [array([-0.48531604], dtype=float32), 0.1795394]. 
=============================================
[2019-04-10 14:14:59,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.19777804e-08 2.40570985e-09 1.20818368e-06 1.94430726e-07
 1.63805584e-04 6.04658684e-11 1.95177815e-12 9.99834061e-01
 1.07885034e-10 1.62192701e-13 5.45775151e-07], sum to 1.0000
[2019-04-10 14:14:59,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4508
[2019-04-10 14:14:59,250] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 19.0, 24.92594322751744, 0.2057563227199147, 0.0, 1.0, 50.0, 38.146957716866964], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 508800.0000, 
sim time next is 510000.0000, 
raw observation next is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 24.85013136367964, 0.1974284640353216, 0.0, 1.0, 50.0, 37.814611475744286], 
processed observation next is [1.0, 0.9130434782608695, 0.5272391505078486, 0.9333333333333332, 0.0, 0.0, 0.08333333333333333, 0.5708442803066367, 0.5658094880117739, 0.0, 1.0, 0.7, 0.37814611475744286], 
reward next is 0.6219, 
noisyNet noise sample is [array([0.07151031], dtype=float32), 1.1640396]. 
=============================================
[2019-04-10 14:14:59,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.24439 ]
 [78.3907  ]
 [79.15928 ]
 [80.27494 ]
 [80.360016]], R is [[76.80129242]
 [76.65180969]
 [76.50287628]
 [76.35444641]
 [76.16755676]].
[2019-04-10 14:14:59,628] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7124: loss -0.0142
[2019-04-10 14:14:59,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7124: learning rate 0.0010
[2019-04-10 14:14:59,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4772142e-05 3.5157811e-07 7.3104027e-05 9.4692232e-06 3.3275343e-02
 3.6659639e-08 2.1851121e-09 9.6659529e-01 8.4477939e-08 5.7038713e-10
 2.1554926e-05], sum to 1.0000
[2019-04-10 14:14:59,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5322
[2019-04-10 14:14:59,837] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 85.66666666666667, 0.0, 0.0, 19.0, 22.73773706571721, -0.233774048292813, 0.0, 1.0, 50.0, 40.90153513382232], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 603600.0000, 
sim time next is 604800.0000, 
raw observation next is [-3.4, 87.0, 0.0, 0.0, 19.0, 22.69776664495384, -0.2452500809357967, 0.0, 1.0, 50.0, 40.87723847487038], 
processed observation next is [0.0, 0.0, 0.368421052631579, 0.87, 0.0, 0.0, 0.08333333333333333, 0.3914805537461534, 0.4182499730214011, 0.0, 1.0, 0.7, 0.4087723847487038], 
reward next is 0.5912, 
noisyNet noise sample is [array([-0.20726337], dtype=float32), -0.5739166]. 
=============================================
[2019-04-10 14:14:59,888] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7215: loss 0.0110
[2019-04-10 14:14:59,889] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7216: learning rate 0.0010
[2019-04-10 14:15:00,661] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7569: loss 0.0165
[2019-04-10 14:15:00,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7570: learning rate 0.0010
[2019-04-10 14:15:00,679] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7576: loss 0.0361
[2019-04-10 14:15:00,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7578: learning rate 0.0010
[2019-04-10 14:15:00,933] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3960773e-06 3.2145053e-07 7.6803582e-05 4.7012327e-06 7.8961793e-03
 2.8392064e-08 1.2502424e-09 9.9200892e-01 3.3710364e-08 1.6324711e-10
 5.4887132e-06], sum to 1.0000
[2019-04-10 14:15:00,935] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3806
[2019-04-10 14:15:01,035] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 24.10501081951415, 0.05764906136047343, 0.0, 1.0, 50.0, 38.94316549143702], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 544800.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.99465290444461, 0.04209620365593401, 0.0, 1.0, 50.0, 39.025318484237815], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.49955440870371753, 0.5140320678853113, 0.0, 1.0, 0.7, 0.39025318484237814], 
reward next is 0.6097, 
noisyNet noise sample is [array([2.3075283], dtype=float32), -0.76808465]. 
=============================================
[2019-04-10 14:15:01,044] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[56.407837]
 [56.28843 ]
 [56.16976 ]
 [56.112083]
 [56.098797]], R is [[56.55023193]
 [56.59529877]
 [56.64145279]
 [56.68967056]
 [56.73300171]].
[2019-04-10 14:15:01,112] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7741: loss 0.0615
[2019-04-10 14:15:01,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7741: learning rate 0.0010
[2019-04-10 14:15:01,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8006: loss 0.0982
[2019-04-10 14:15:01,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8007: learning rate 0.0010
[2019-04-10 14:15:01,785] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8031: loss 0.0601
[2019-04-10 14:15:01,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8031: learning rate 0.0010
[2019-04-10 14:15:01,841] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8053: loss 0.0783
[2019-04-10 14:15:01,841] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8053: learning rate 0.0010
[2019-04-10 14:15:02,028] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8131: loss 0.0179
[2019-04-10 14:15:02,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8131: learning rate 0.0010
[2019-04-10 14:15:02,144] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8185: loss 0.0112
[2019-04-10 14:15:02,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8192: learning rate 0.0010
[2019-04-10 14:15:02,377] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8285: loss 0.0016
[2019-04-10 14:15:02,378] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8285: learning rate 0.0010
[2019-04-10 14:15:02,530] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8350: loss 0.0684
[2019-04-10 14:15:02,532] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8350: learning rate 0.0010
[2019-04-10 14:15:02,539] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8355: loss 0.0434
[2019-04-10 14:15:02,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8355: learning rate 0.0010
[2019-04-10 14:15:02,649] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 8402: loss 0.0698
[2019-04-10 14:15:02,650] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 8402: learning rate 0.0010
[2019-04-10 14:15:02,734] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 8441: loss -4.1104
[2019-04-10 14:15:02,735] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 8441: learning rate 0.0010
[2019-04-10 14:15:02,753] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8448: loss 0.0504
[2019-04-10 14:15:02,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8448: learning rate 0.0010
[2019-04-10 14:15:03,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6990154e-06 6.1059177e-07 5.9234852e-05 6.7361229e-06 3.3166748e-04
 1.0131878e-07 1.8182442e-08 9.9958211e-01 1.2075309e-07 1.8247992e-09
 1.1730039e-05], sum to 1.0000
[2019-04-10 14:15:03,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-10 14:15:03,077] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 19.0, 22.11812150494403, -0.3869626982368579, 0.0, 1.0, 50.0, 42.26907732471252], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 628800.0000, 
sim time next is 630000.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 19.0, 22.06699715867461, -0.3991993556102051, 0.0, 1.0, 50.0, 42.33902139448445], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.68, 0.0, 0.0, 0.08333333333333333, 0.33891642988955084, 0.3669335481299316, 0.0, 1.0, 0.7, 0.4233902139448445], 
reward next is 0.5766, 
noisyNet noise sample is [array([1.1397032], dtype=float32), -1.649696]. 
=============================================
[2019-04-10 14:15:03,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[47.360237]
 [47.495045]
 [47.642494]
 [47.84321 ]
 [48.10232 ]], R is [[47.34473038]
 [47.44859314]
 [47.55101395]
 [47.65135574]
 [47.74905396]].
[2019-04-10 14:15:05,726] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.5563272e-08 9.6449670e-10 7.1327963e-06 9.6913368e-08 1.5549298e-05
 1.9061747e-10 2.3648611e-12 9.9997711e-01 2.7866390e-10 7.7298709e-14
 4.9173480e-08], sum to 1.0000
[2019-04-10 14:15:05,726] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1633
[2019-04-10 14:15:05,766] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 72.0, 0.0, 0.0, 19.0, 21.82196689041085, -0.4791804285810015, 0.0, 1.0, 50.0, 40.079105489618115], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 694800.0000, 
sim time next is 696000.0000, 
raw observation next is [-3.4, 73.0, 0.0, 0.0, 19.0, 21.88792786045488, -0.4781688827401305, 0.0, 1.0, 50.0, 40.137789667985004], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.73, 0.0, 0.0, 0.08333333333333333, 0.32399398837124, 0.3406103724199565, 0.0, 1.0, 0.7, 0.40137789667985], 
reward next is 0.5986, 
noisyNet noise sample is [array([-0.59337634], dtype=float32), 0.49338457]. 
=============================================
[2019-04-10 14:15:05,774] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[81.981514]
 [80.53893 ]
 [79.0914  ]
 [77.71295 ]
 [76.59409 ]], R is [[83.56656647]
 [83.33011627]
 [83.09615326]
 [82.86436462]
 [82.63365173]].
[2019-04-10 14:15:13,903] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0619543e-08 2.2418523e-09 1.3338122e-07 5.0109435e-08 2.7845720e-06
 3.7557173e-11 1.0885815e-12 9.9999690e-01 2.9806657e-11 1.8924144e-13
 6.8662565e-08], sum to 1.0000
[2019-04-10 14:15:13,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9393
[2019-04-10 14:15:13,936] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 80.33333333333334, 0.0, 0.0, 19.0, 24.93221576342866, 0.282608517562226, 0.0, 1.0, 50.0, 38.140096380145025], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 859200.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 19.0, 24.91411905404691, 0.2691354303391053, 0.0, 1.0, 50.0, 38.0048988160533], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.08333333333333333, 0.5761765878372426, 0.5897118101130351, 0.0, 1.0, 0.7, 0.380048988160533], 
reward next is 0.6200, 
noisyNet noise sample is [array([0.09465021], dtype=float32), -0.34898317]. 
=============================================
[2019-04-10 14:15:17,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3767766e-09 3.4611550e-10 8.6686471e-09 1.7585612e-09 1.4888040e-07
 2.9108872e-12 3.4978831e-13 9.9999976e-01 9.6709056e-13 3.0487365e-14
 1.3532969e-07], sum to 1.0000
[2019-04-10 14:15:17,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1253
[2019-04-10 14:15:17,316] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 22.5, 24.96795186805078, 0.4392359649962058, 1.0, 1.0, 50.0, 34.80814415861422], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 927600.0000, 
sim time next is 928800.0000, 
raw observation next is [4.4, 96.0, 0.0, 0.0, 22.5, 25.53565618549342, 0.5067646511661484, 1.0, 1.0, 50.0, 34.270458243346646], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.96, 0.0, 0.0, 0.375, 0.6279713487911183, 0.6689215503887161, 1.0, 1.0, 0.7, 0.34270458243346646], 
reward next is 0.6573, 
noisyNet noise sample is [array([0.08774751], dtype=float32), 0.83809763]. 
=============================================
[2019-04-10 14:15:18,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2175348e-08 1.6548417e-09 2.9713382e-07 3.4788442e-08 3.3291078e-06
 8.7567335e-11 2.9333493e-12 9.9999630e-01 3.6781238e-11 3.2245726e-13
 3.9226212e-08], sum to 1.0000
[2019-04-10 14:15:18,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-10 14:15:18,347] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.066666666666666, 83.0, 0.0, 0.0, 19.0, 25.12923632384942, 0.3891030572607529, 0.0, 1.0, 50.0, 34.99085410778748], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 966000.0000, 
sim time next is 967200.0000, 
raw observation next is [8.433333333333334, 83.0, 0.0, 0.0, 19.0, 25.1090655169865, 0.3999454384186846, 0.0, 1.0, 50.0, 34.91889380816833], 
processed observation next is [1.0, 0.17391304347826086, 0.6962142197599263, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5924221264155417, 0.6333151461395615, 0.0, 1.0, 0.7, 0.3491889380816833], 
reward next is 0.6508, 
noisyNet noise sample is [array([-0.5103969], dtype=float32), 2.2324]. 
=============================================
[2019-04-10 14:15:19,916] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0113872e-09 1.1910911e-10 8.6582030e-09 1.4071077e-09 3.1244471e-07
 2.0028703e-12 1.8879203e-13 9.9999964e-01 1.2101400e-12 8.9148380e-15
 2.8174714e-09], sum to 1.0000
[2019-04-10 14:15:19,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4853
[2019-04-10 14:15:20,029] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 19.0, 27.20261163590698, 0.9579630204930334, 0.0, 1.0, 50.0, 30.906238469237945], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1027200.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 19.0, 27.19536457701776, 0.9580730252533168, 0.0, 1.0, 50.0, 30.717251597457434], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7662803814181466, 0.819357675084439, 0.0, 1.0, 0.7, 0.30717251597457434], 
reward next is 0.6928, 
noisyNet noise sample is [array([1.5861541], dtype=float32), -2.6720366]. 
=============================================
[2019-04-10 14:15:20,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4300050e-07 6.6160517e-08 3.4553116e-07 2.8524991e-07 2.1102335e-06
 1.4686907e-09 1.7579829e-10 9.9999368e-01 3.3348310e-10 2.6974046e-11
 3.1836180e-06], sum to 1.0000
[2019-04-10 14:15:20,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3779
[2019-04-10 14:15:20,986] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.8, 54.0, 155.5, 0.0, 22.5, 29.35169066979717, 1.405911016937097, 1.0, 0.0, 50.0, 1.6814339930072275], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1087200.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 22.5, 29.44536633399941, 1.422725907683817, 1.0, 0.0, 50.0, 1.703289797247633], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.375, 0.9537805278332842, 0.9742419692279389, 1.0, 0.0, 0.7, 0.01703289797247633], 
reward next is 0.9830, 
noisyNet noise sample is [array([0.7122459], dtype=float32), -0.5543266]. 
=============================================
[2019-04-10 14:15:22,103] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 15130: loss 1.9356
[2019-04-10 14:15:22,104] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 15130: learning rate 0.0010
[2019-04-10 14:15:22,296] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15231: loss 1.7047
[2019-04-10 14:15:22,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15235: learning rate 0.0010
[2019-04-10 14:15:22,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5377881e-12 4.1560670e-13 4.2436743e-10 1.8381383e-11 3.6896786e-09
 2.8223868e-14 7.0325249e-16 1.0000000e+00 1.1372109e-14 2.4442555e-17
 3.9075809e-12], sum to 1.0000
[2019-04-10 14:15:22,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-10 14:15:22,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.2, 66.0, 0.0, 0.0, 19.0, 27.86774001103981, 1.177688661348209, 0.0, 1.0, 50.0, 25.77706403759663], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1119600.0000, 
sim time next is 1120800.0000, 
raw observation next is [12.0, 67.66666666666667, 0.0, 0.0, 19.0, 27.84957466482583, 1.171737091381662, 0.0, 1.0, 50.0, 26.05986688746598], 
processed observation next is [1.0, 1.0, 0.7950138504155125, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8207978887354859, 0.890579030460554, 0.0, 1.0, 0.7, 0.26059866887465977], 
reward next is 0.7394, 
noisyNet noise sample is [array([-1.2962874], dtype=float32), -0.3989731]. 
=============================================
[2019-04-10 14:15:22,451] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15287: loss 1.8342
[2019-04-10 14:15:22,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15288: learning rate 0.0010
[2019-04-10 14:15:22,728] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15428: loss 0.9696
[2019-04-10 14:15:22,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15428: learning rate 0.0010
[2019-04-10 14:15:22,930] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15532: loss 0.7092
[2019-04-10 14:15:22,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15534: learning rate 0.0010
[2019-04-10 14:15:23,735] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15966: loss 0.3153
[2019-04-10 14:15:23,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15966: learning rate 0.0010
[2019-04-10 14:15:23,852] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16037: loss 0.3097
[2019-04-10 14:15:23,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16037: learning rate 0.0010
[2019-04-10 14:15:23,855] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16037: loss 0.3211
[2019-04-10 14:15:23,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16037: learning rate 0.0010
[2019-04-10 14:15:23,866] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16042: loss 0.3525
[2019-04-10 14:15:23,867] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16042: learning rate 0.0010
[2019-04-10 14:15:24,291] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16274: loss 0.0843
[2019-04-10 14:15:24,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16274: learning rate 0.0010
[2019-04-10 14:15:24,435] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16354: loss 0.1690
[2019-04-10 14:15:24,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16354: learning rate 0.0010
[2019-04-10 14:15:24,500] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16394: loss 0.2312
[2019-04-10 14:15:24,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16394: learning rate 0.0010
[2019-04-10 14:15:24,521] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 16407: loss 0.2312
[2019-04-10 14:15:24,523] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 16409: learning rate 0.0010
[2019-04-10 14:15:24,620] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 16460: loss 0.1512
[2019-04-10 14:15:24,621] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 16461: learning rate 0.0010
[2019-04-10 14:15:24,740] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.87795307e-09 8.35666605e-12 1.05797056e-07 3.15317439e-09
 2.50601403e-07 1.00151155e-11 1.13576737e-13 9.99999642e-01
 1.93055606e-11 9.96036419e-16 6.46497786e-11], sum to 1.0000
[2019-04-10 14:15:24,741] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3373
[2019-04-10 14:15:24,762] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [16.1, 80.0, 0.0, 0.0, 19.0, 28.19239927194533, 1.225281389534081, 0.0, 0.0, 50.0, 20.948003395089152], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1213200.0000, 
sim time next is 1214400.0000, 
raw observation next is [16.1, 81.0, 0.0, 0.0, 19.0, 28.15565088955447, 1.223857106954115, 0.0, 0.0, 50.0, 22.17896571428582], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.81, 0.0, 0.0, 0.08333333333333333, 0.8463042407962057, 0.907952368984705, 0.0, 0.0, 0.7, 0.2217896571428582], 
reward next is 0.7782, 
noisyNet noise sample is [array([1.355747], dtype=float32), 1.1089514]. 
=============================================
[2019-04-10 14:15:24,874] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16591: loss 0.0683
[2019-04-10 14:15:24,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16591: learning rate 0.0010
[2019-04-10 14:15:24,946] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16631: loss 0.0883
[2019-04-10 14:15:24,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16631: learning rate 0.0010
[2019-04-10 14:15:25,247] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8776254e-09 1.1262360e-11 1.3213376e-07 2.8319811e-09 5.0973792e-08
 5.0992478e-11 2.5911050e-13 9.9999976e-01 2.9276317e-11 2.5294964e-15
 4.3158054e-11], sum to 1.0000
[2019-04-10 14:15:25,247] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9714
[2019-04-10 14:15:25,268] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.11502636338697, 1.22105461443045, 0.0, 0.0, 50.0, 22.25243841133311], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1224000.0000, 
sim time next is 1225200.0000, 
raw observation next is [15.33333333333333, 94.0, 0.0, 0.0, 19.0, 28.09538683182087, 1.221337445511467, 0.0, 0.0, 50.0, 22.888242721347133], 
processed observation next is [0.0, 0.17391304347826086, 0.8873499538319484, 0.94, 0.0, 0.0, 0.08333333333333333, 0.8412822359850726, 0.9071124818371556, 0.0, 0.0, 0.7, 0.22888242721347132], 
reward next is 0.7711, 
noisyNet noise sample is [array([0.541114], dtype=float32), 0.57491124]. 
=============================================
[2019-04-10 14:15:26,065] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4135790e-11 3.5445185e-14 1.4320550e-10 1.4805529e-12 7.7279849e-10
 1.7584363e-14 1.7248536e-17 1.0000000e+00 6.5576130e-15 1.8639008e-18
 1.3965067e-13], sum to 1.0000
[2019-04-10 14:15:26,065] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4709
[2019-04-10 14:15:26,084] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [13.8, 100.0, 80.0, 0.0, 19.0, 28.12624674158735, 1.276588676636015, 0.0, 1.0, 50.0, 20.49672480474803], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1261200.0000, 
sim time next is 1262400.0000, 
raw observation next is [13.8, 100.0, 72.66666666666667, 0.0, 19.0, 28.15897780379693, 1.279436781805579, 0.0, 1.0, 50.0, 19.71639054629224], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.24222222222222223, 0.0, 0.08333333333333333, 0.8465814836497442, 0.9264789272685263, 0.0, 1.0, 0.7, 0.1971639054629224], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.21006213], dtype=float32), 0.5523181]. 
=============================================
[2019-04-10 14:15:29,998] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.0158203e-10 6.4405731e-11 7.4522886e-09 2.9002611e-10 5.2828941e-08
 3.2764464e-12 1.5724722e-13 9.9999988e-01 3.3383168e-12 3.8878534e-14
 5.4333660e-10], sum to 1.0000
[2019-04-10 14:15:30,002] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1873
[2019-04-10 14:15:30,035] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.11902860324721, 0.6999746287283747, 0.0, 1.0, 50.0, 35.56435863305762], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1399200.0000, 
sim time next is 1400400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.0457860478596, 0.6731905230114568, 0.0, 1.0, 50.0, 35.4976560581896], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6704821706549667, 0.724396841003819, 0.0, 1.0, 0.7, 0.354976560581896], 
reward next is 0.6450, 
noisyNet noise sample is [array([0.04359194], dtype=float32), 0.388243]. 
=============================================
[2019-04-10 14:15:30,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9117857e-09 2.2105005e-10 2.1576826e-08 3.2685925e-09 1.7456900e-07
 2.0568861e-11 9.2734772e-13 9.9999976e-01 1.8052174e-11 1.8686359e-13
 2.8126503e-09], sum to 1.0000
[2019-04-10 14:15:30,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-10 14:15:30,167] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.18230908275556, 0.7176552850319351, 0.0, 1.0, 50.0, 35.898436737600036], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1396800.0000, 
sim time next is 1398000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.15673408015638, 0.7084109905002419, 0.0, 1.0, 50.0, 35.73628270472426], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6797278400130317, 0.736136996833414, 0.0, 1.0, 0.7, 0.3573628270472426], 
reward next is 0.6426, 
noisyNet noise sample is [array([-0.6075871], dtype=float32), 1.291557]. 
=============================================
[2019-04-10 14:15:30,171] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[63.510174]
 [63.4946  ]
 [63.49741 ]
 [63.495132]
 [63.23543 ]], R is [[63.50873184]
 [63.51465988]
 [63.51927567]
 [63.52249146]
 [63.5250206 ]].
[2019-04-10 14:15:31,022] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3147179e-10 1.2431331e-10 4.7110382e-10 6.0145100e-10 6.6875319e-09
 1.0264062e-12 1.3522969e-13 1.0000000e+00 8.4915820e-13 2.6575791e-14
 1.2493788e-08], sum to 1.0000
[2019-04-10 14:15:31,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-10 14:15:31,054] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 90.66666666666666, 0.0, 0.0, 22.5, 26.81608375643177, 0.8560878928288548, 1.0, 1.0, 50.0, 31.65980078772407], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1449600.0000, 
sim time next is 1450800.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 26.81619958235311, 0.8467864130627448, 1.0, 1.0, 50.0, 31.62233505627802], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7346832985294259, 0.7822621376875816, 1.0, 1.0, 0.7, 0.31622335056278017], 
reward next is 0.6838, 
noisyNet noise sample is [array([-1.0467025], dtype=float32), 0.25227076]. 
=============================================
[2019-04-10 14:15:36,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0573185e-11 1.0517739e-11 3.1736525e-10 6.1800440e-11 9.3602885e-09
 9.0082225e-14 1.1292007e-14 1.0000000e+00 7.7786037e-14 6.7616681e-16
 1.4441542e-10], sum to 1.0000
[2019-04-10 14:15:36,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-10 14:15:36,463] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.6, 86.0, 0.0, 0.0, 19.0, 27.6920472283885, 1.112202875366273, 0.0, 1.0, 50.0, 28.27506157873047], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1634400.0000, 
sim time next is 1635600.0000, 
raw observation next is [6.800000000000001, 84.66666666666667, 0.0, 0.0, 19.0, 27.65173046782188, 1.067670874726144, 0.0, 1.0, 50.0, 35.135744883178674], 
processed observation next is [1.0, 0.9565217391304348, 0.6509695290858727, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.8043108723184901, 0.8558902915753813, 0.0, 1.0, 0.7, 0.35135744883178677], 
reward next is 0.6486, 
noisyNet noise sample is [array([0.5754122], dtype=float32), 0.67715585]. 
=============================================
[2019-04-10 14:15:38,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.30032441e-07 3.06642605e-08 2.96268269e-07 2.47242241e-07
 1.75250057e-06 4.32804859e-09 3.19307303e-10 9.99997258e-01
 3.40951622e-09 1.09427654e-10 2.30717902e-07], sum to 1.0000
[2019-04-10 14:15:38,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-10 14:15:38,453] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.2, 87.0, 0.0, 0.0, 19.0, 25.3998160799388, 0.511032361068371, 0.0, 1.0, 50.0, 40.85961832669632], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1749600.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 19.0, 25.33476293380021, 0.4968264204515704, 0.0, 1.0, 50.0, 40.96893783382942], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6112302444833508, 0.6656088068171901, 0.0, 1.0, 0.7, 0.4096893783382942], 
reward next is 0.5903, 
noisyNet noise sample is [array([-0.51688224], dtype=float32), -0.081214346]. 
=============================================
[2019-04-10 14:15:39,666] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23215: loss 0.0028
[2019-04-10 14:15:39,667] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23215: learning rate 0.0010
[2019-04-10 14:15:39,957] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23331: loss 0.0041
[2019-04-10 14:15:39,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23331: learning rate 0.0010
[2019-04-10 14:15:40,250] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9649765e-07 7.6396113e-08 2.5813733e-07 1.5648968e-07 2.9925598e-06
 2.2633952e-09 2.8287597e-10 9.9999511e-01 2.2821396e-09 1.9424454e-10
 1.0010637e-06], sum to 1.0000
[2019-04-10 14:15:40,251] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23445: loss 0.0144
[2019-04-10 14:15:40,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9269
[2019-04-10 14:15:40,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23447: learning rate 0.0010
[2019-04-10 14:15:40,290] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 19.0, 23.7297573910936, 0.06687599476688601, 0.0, 1.0, 50.0, 43.27213434155811], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1810800.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 23.67637250775815, 0.05464298421495575, 0.0, 1.0, 50.0, 43.22171936629601], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.4730310423131791, 0.5182143280716519, 0.0, 1.0, 0.7, 0.4322171936629601], 
reward next is 0.5678, 
noisyNet noise sample is [array([0.80404997], dtype=float32), 2.1970499]. 
=============================================
[2019-04-10 14:15:40,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[43.180412]
 [43.27231 ]
 [43.3523  ]
 [43.43125 ]
 [43.46318 ]], R is [[43.19363022]
 [43.32897568]
 [43.46223068]
 [43.59374237]
 [43.72402573]].
[2019-04-10 14:15:40,675] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23624: loss 0.0023
[2019-04-10 14:15:40,676] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23624: learning rate 0.0010
[2019-04-10 14:15:40,918] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23726: loss 0.0032
[2019-04-10 14:15:40,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23726: learning rate 0.0010
[2019-04-10 14:15:41,309] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23894: loss 0.0230
[2019-04-10 14:15:41,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23894: learning rate 0.0010
[2019-04-10 14:15:41,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0486596e-07 2.1396113e-08 3.2509686e-07 4.4656566e-08 8.5332584e-07
 8.8286534e-10 9.8120942e-11 9.9999857e-01 8.0175411e-10 1.2222340e-11
 1.0240646e-07], sum to 1.0000
[2019-04-10 14:15:41,352] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8843
[2019-04-10 14:15:41,391] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 19.0, 24.88738512291621, 0.3812268775692783, 0.0, 1.0, 50.0, 41.310777030523084], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1768800.0000, 
sim time next is 1770000.0000, 
raw observation next is [-2.3, 84.33333333333333, 120.1666666666667, 0.0, 19.0, 24.8616638896936, 0.3769630900393572, 0.0, 1.0, 50.0, 41.2954362360051], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8433333333333333, 0.40055555555555566, 0.0, 0.08333333333333333, 0.5718053241411333, 0.6256543633464524, 0.0, 1.0, 0.7, 0.412954362360051], 
reward next is 0.5870, 
noisyNet noise sample is [array([-0.08274127], dtype=float32), 1.354027]. 
=============================================
[2019-04-10 14:15:41,394] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[51.76686 ]
 [51.758842]
 [51.71006 ]
 [51.620293]
 [51.517258]], R is [[51.82145691]
 [51.8901329 ]
 [51.95825958]
 [52.02602768]
 [52.09325027]].
[2019-04-10 14:15:41,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8613997e-08 8.2866976e-09 9.7480246e-08 4.2674433e-08 1.8400550e-07
 4.8861615e-10 4.6419611e-11 9.9999964e-01 3.4930012e-10 1.5612693e-11
 3.8993900e-08], sum to 1.0000
[2019-04-10 14:15:41,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4055
[2019-04-10 14:15:41,472] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 19.0, 24.84319255445727, 0.3731373499231193, 0.0, 1.0, 50.0, 41.2622401163278], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1771200.0000, 
sim time next is 1772400.0000, 
raw observation next is [-2.466666666666667, 83.0, 124.8333333333333, 0.0, 19.0, 24.82822335984827, 0.3690495288555281, 0.0, 1.0, 50.0, 41.23586093314243], 
processed observation next is [0.0, 0.5217391304347826, 0.39427516158818104, 0.83, 0.416111111111111, 0.0, 0.08333333333333333, 0.5690186133206891, 0.6230165096185094, 0.0, 1.0, 0.7, 0.4123586093314243], 
reward next is 0.5876, 
noisyNet noise sample is [array([0.5959298], dtype=float32), 0.41183352]. 
=============================================
[2019-04-10 14:15:41,612] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24022: loss 0.0031
[2019-04-10 14:15:41,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24022: learning rate 0.0010
[2019-04-10 14:15:41,658] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24041: loss 0.0035
[2019-04-10 14:15:41,658] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24041: learning rate 0.0010
[2019-04-10 14:15:41,750] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24079: loss 0.0028
[2019-04-10 14:15:41,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24079: learning rate 0.0010
[2019-04-10 14:15:42,067] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24215: loss 0.0048
[2019-04-10 14:15:42,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24215: learning rate 0.0010
[2019-04-10 14:15:42,132] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24243: loss 0.0079
[2019-04-10 14:15:42,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24245: learning rate 0.0010
[2019-04-10 14:15:42,311] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 24316: loss 0.0585
[2019-04-10 14:15:42,316] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 24319: learning rate 0.0010
[2019-04-10 14:15:42,382] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24350: loss 0.0436
[2019-04-10 14:15:42,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24351: learning rate 0.0010
[2019-04-10 14:15:42,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9515971e-08 1.6779254e-08 6.9749603e-08 5.4185694e-08 4.4241128e-07
 4.8194532e-10 3.0995994e-11 9.9999940e-01 5.0855825e-10 1.0416613e-11
 3.0896462e-08], sum to 1.0000
[2019-04-10 14:15:42,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9868
[2019-04-10 14:15:42,442] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.3, 82.66666666666666, 0.0, 0.0, 19.0, 24.25521259772645, 0.2091812751239515, 0.0, 1.0, 50.0, 42.62114811415188], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1795200.0000, 
sim time next is 1796400.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 19.0, 24.20449536536633, 0.1962925644367056, 0.0, 1.0, 50.0, 42.67274273209326], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5170412804471942, 0.5654308548122352, 0.0, 1.0, 0.7, 0.4267274273209326], 
reward next is 0.5733, 
noisyNet noise sample is [array([1.5877442], dtype=float32), 1.5034882]. 
=============================================
[2019-04-10 14:15:42,555] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24421: loss 0.0004
[2019-04-10 14:15:42,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24421: learning rate 0.0010
[2019-04-10 14:15:42,594] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 24438: loss 0.0004
[2019-04-10 14:15:42,597] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 24439: learning rate 0.0010
[2019-04-10 14:15:42,760] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24508: loss 0.0032
[2019-04-10 14:15:42,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24508: learning rate 0.0010
[2019-04-10 14:15:45,915] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0237955e-09 2.6446295e-10 8.4341032e-09 1.5844460e-09 3.3459926e-08
 8.8823688e-12 1.0137789e-12 1.0000000e+00 1.6695534e-11 1.3513507e-13
 6.0553180e-09], sum to 1.0000
[2019-04-10 14:15:45,916] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2338
[2019-04-10 14:15:45,956] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 19.0, 21.9168007630993, -0.4542276203196655, 0.0, 1.0, 50.0, 43.84771053780304], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1917600.0000, 
sim time next is 1918800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 19.0, 21.94867401653985, -0.4552835239503055, 0.0, 1.0, 50.0, 43.77279534275474], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.08333333333333333, 0.32905616804498755, 0.3482388253498982, 0.0, 1.0, 0.7, 0.4377279534275474], 
reward next is 0.5623, 
noisyNet noise sample is [array([0.13536525], dtype=float32), 0.37474102]. 
=============================================
[2019-04-10 14:15:50,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9747471e-10 1.2307234e-10 7.1252662e-09 9.0340335e-10 3.5222222e-08
 8.6399143e-12 2.0841969e-13 1.0000000e+00 9.9353043e-12 2.8872961e-14
 1.4868320e-09], sum to 1.0000
[2019-04-10 14:15:50,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3796
[2019-04-10 14:15:50,895] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 24.53494311460988, 0.1644701945058863, 0.0, 1.0, 50.0, 38.725200579698424], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2001600.0000, 
sim time next is 2002800.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 24.35482097174189, 0.1601573214762994, 0.0, 1.0, 50.0, 39.00731268211031], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.5295684143118242, 0.5533857738254332, 0.0, 1.0, 0.7, 0.3900731268211031], 
reward next is 0.6099, 
noisyNet noise sample is [array([0.19866957], dtype=float32), -0.27568698]. 
=============================================
[2019-04-10 14:15:51,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6897035e-12 2.2569537e-12 2.7602358e-11 1.0581464e-11 1.8667974e-10
 1.6973797e-14 2.2647641e-15 1.0000000e+00 1.8869336e-14 2.8300170e-16
 5.6823445e-11], sum to 1.0000
[2019-04-10 14:15:51,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7966
[2019-04-10 14:15:51,222] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 26.56136518938765, 0.6523984629202535, 0.0, 1.0, 50.0, 45.90427732090911], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1972800.0000, 
sim time next is 1974000.0000, 
raw observation next is [-5.6, 81.33333333333334, 0.0, 0.0, 19.0, 26.47023964897006, 0.6363422962643802, 0.0, 1.0, 50.0, 42.47131190904737], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.8133333333333335, 0.0, 0.0, 0.08333333333333333, 0.7058533040808385, 0.7121140987547934, 0.0, 1.0, 0.7, 0.4247131190904737], 
reward next is 0.5753, 
noisyNet noise sample is [array([2.6808455], dtype=float32), -0.65003633]. 
=============================================
[2019-04-10 14:15:51,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.68983 ]
 [72.21136 ]
 [71.986855]
 [71.664734]
 [71.4218  ]], R is [[70.46921539]
 [70.30548096]
 [70.14469147]
 [69.98712158]
 [69.83217621]].
[2019-04-10 14:15:52,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9886167e-10 2.2027107e-10 3.1137468e-10 2.8377767e-10 6.5786296e-09
 2.0928901e-12 3.9804103e-13 1.0000000e+00 2.1887249e-12 5.7833133e-14
 7.4091044e-09], sum to 1.0000
[2019-04-10 14:15:52,514] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0295
[2019-04-10 14:15:52,624] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 22.5, 24.87917848491772, 0.3118110497627573, 1.0, 1.0, 50.0, 39.803709040718275], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2055600.0000, 
sim time next is 2056800.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 22.5, 24.85179262487302, 0.3037361325138486, 1.0, 1.0, 50.0, 39.720480683873205], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.375, 0.5709827187394184, 0.6012453775046162, 1.0, 1.0, 0.7, 0.39720480683873205], 
reward next is 0.6028, 
noisyNet noise sample is [array([0.732193], dtype=float32), -0.2228647]. 
=============================================
[2019-04-10 14:15:55,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0814955e-09 1.2873448e-09 4.4435531e-09 3.1296699e-09 2.4334792e-08
 1.6202560e-11 4.1205026e-12 1.0000000e+00 1.1603280e-11 2.9554069e-13
 1.0851065e-08], sum to 1.0000
[2019-04-10 14:15:55,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0443
[2019-04-10 14:15:55,517] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 23.0356678872249, -0.09271463359218864, 0.0, 1.0, 50.0, 40.130846084269166], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2158800.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 22.94130639645734, -0.1130743910977388, 0.0, 1.0, 50.0, 40.2986651778304], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.4117755330381116, 0.46230853630075375, 0.0, 1.0, 0.7, 0.402986651778304], 
reward next is 0.5970, 
noisyNet noise sample is [array([-0.2702861], dtype=float32), 0.486679]. 
=============================================
[2019-04-10 14:15:55,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[54.744007]
 [54.92377 ]
 [55.12837 ]
 [55.114437]
 [55.44355 ]], R is [[54.63690186]
 [54.68922424]
 [54.74289322]
 [54.79812241]
 [54.85597229]].
[2019-04-10 14:15:56,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6713208e-09 8.3390184e-10 3.1024324e-08 4.1032937e-09 9.6015484e-08
 4.8138490e-11 4.7750913e-12 9.9999988e-01 7.6799893e-11 7.4085090e-13
 6.0274661e-09], sum to 1.0000
[2019-04-10 14:15:56,120] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9286
[2019-04-10 14:15:56,157] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 23.0635609496053, -0.1665653179007673, 0.0, 1.0, 50.0, 40.87868305152548], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2172000.0000, 
sim time next is 2173200.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 22.83693635922379, -0.1876097592204001, 0.0, 1.0, 50.0, 40.44804704408779], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.403078029935316, 0.4374634135932, 0.0, 1.0, 0.7, 0.40448047044087787], 
reward next is 0.5955, 
noisyNet noise sample is [array([-1.1736461], dtype=float32), -0.10624198]. 
=============================================
[2019-04-10 14:15:59,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1432369e-10 8.3799323e-10 9.2476343e-10 6.1014921e-10 6.5123000e-09
 3.6996443e-12 5.2355749e-13 1.0000000e+00 2.4636391e-12 1.6429155e-13
 5.4791491e-09], sum to 1.0000
[2019-04-10 14:15:59,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5153
[2019-04-10 14:15:59,679] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.899999999999999, 69.0, 126.1666666666667, 47.49999999999999, 22.5, 24.67162945035034, 0.1828993443249327, 1.0, 1.0, 50.0, 39.80186961328607], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2212800.0000, 
sim time next is 2214000.0000, 
raw observation next is [-3.9, 68.0, 138.5, 142.5, 22.5, 24.78932012543661, 0.2157683865695675, 1.0, 1.0, 50.0, 39.906121165430406], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.46166666666666667, 0.1574585635359116, 0.375, 0.5657766771197176, 0.5719227955231891, 1.0, 1.0, 0.7, 0.3990612116543041], 
reward next is 0.6009, 
noisyNet noise sample is [array([0.8264638], dtype=float32), 0.36834565]. 
=============================================
[2019-04-10 14:15:59,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.840645]
 [57.881073]
 [57.88749 ]
 [57.848988]
 [57.78521 ]], R is [[57.91651154]
 [57.93932724]
 [57.95853424]
 [57.9736824 ]
 [57.99616241]].
[2019-04-10 14:16:07,272] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31266: loss 0.0046
[2019-04-10 14:16:07,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31266: learning rate 0.0010
[2019-04-10 14:16:07,460] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31328: loss 0.0039
[2019-04-10 14:16:07,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31328: learning rate 0.0010
[2019-04-10 14:16:07,726] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31424: loss 0.0080
[2019-04-10 14:16:07,726] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31424: learning rate 0.0010
[2019-04-10 14:16:08,236] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31628: loss 0.0174
[2019-04-10 14:16:08,243] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31628: learning rate 0.0010
[2019-04-10 14:16:08,597] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31775: loss 0.0149
[2019-04-10 14:16:08,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31777: learning rate 0.0010
[2019-04-10 14:16:08,875] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31901: loss 0.0084
[2019-04-10 14:16:08,876] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31901: loss 0.0018
[2019-04-10 14:16:08,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31901: learning rate 0.0010
[2019-04-10 14:16:08,880] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31901: learning rate 0.0010
[2019-04-10 14:16:08,890] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31905: loss 0.0214
[2019-04-10 14:16:08,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31905: learning rate 0.0010
[2019-04-10 14:16:09,194] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32037: loss 0.0034
[2019-04-10 14:16:09,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32037: learning rate 0.0010
[2019-04-10 14:16:09,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32085: loss 0.0077
[2019-04-10 14:16:09,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32085: learning rate 0.0010
[2019-04-10 14:16:09,408] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3839478e-07 5.7031155e-08 2.1124483e-06 4.0689605e-07 6.0739289e-06
 5.9505232e-09 6.5538175e-10 9.9999070e-01 8.1940659e-09 2.3950281e-10
 2.1551040e-07], sum to 1.0000
[2019-04-10 14:16:09,408] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2593
[2019-04-10 14:16:09,435] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 19.0, 23.03256101197147, -0.1773364308683322, 0.0, 1.0, 50.0, 41.657733203917246], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2427600.0000, 
sim time next is 2428800.0000, 
raw observation next is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 22.96181633406453, -0.1965746502294995, 0.0, 1.0, 50.0, 41.76734414244471], 
processed observation next is [0.0, 0.08695652173913043, 0.2511542012927055, 0.5433333333333333, 0.0, 0.0, 0.08333333333333333, 0.41348469450537745, 0.4344751165901668, 0.0, 1.0, 0.7, 0.4176734414244471], 
reward next is 0.5823, 
noisyNet noise sample is [array([1.1139265], dtype=float32), 0.92876744]. 
=============================================
[2019-04-10 14:16:09,556] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32191: loss 0.0027
[2019-04-10 14:16:09,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32192: learning rate 0.0010
[2019-04-10 14:16:09,780] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32288: loss 0.0055
[2019-04-10 14:16:09,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32290: learning rate 0.0010
[2019-04-10 14:16:10,122] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 32438: loss 0.0883
[2019-04-10 14:16:10,124] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 32438: learning rate 0.0010
[2019-04-10 14:16:10,336] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 32532: loss 0.0564
[2019-04-10 14:16:10,337] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 32532: learning rate 0.0010
[2019-04-10 14:16:10,443] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32578: loss 0.0859
[2019-04-10 14:16:10,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32578: learning rate 0.0010
[2019-04-10 14:16:10,723] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32695: loss 0.0987
[2019-04-10 14:16:10,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32695: learning rate 0.0010
[2019-04-10 14:16:12,835] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1030616e-11 7.2826836e-13 4.2506249e-10 2.0928793e-11 2.1648208e-10
 1.7294822e-13 3.1523000e-15 1.0000000e+00 2.2069632e-13 3.7091753e-16
 1.1326710e-11], sum to 1.0000
[2019-04-10 14:16:12,836] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0777
[2019-04-10 14:16:12,886] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.633333333333333, 55.0, 0.0, 0.0, 19.0, 23.4411215922679, -0.1183175528985792, 0.0, 1.0, 50.0, 37.536417845184445], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2529600.0000, 
sim time next is 2530800.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 19.0, 23.64447779896884, -0.1215308505624624, 0.0, 1.0, 50.0, 37.10217251847604], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.08333333333333333, 0.47037314991407, 0.4594897164791792, 0.0, 1.0, 0.7, 0.3710217251847604], 
reward next is 0.6290, 
noisyNet noise sample is [array([-0.776112], dtype=float32), 0.56425834]. 
=============================================
[2019-04-10 14:16:15,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3149701e-10 1.1514674e-11 1.9070627e-09 1.1412788e-10 1.5580973e-09
 7.0495774e-13 2.1574791e-14 1.0000000e+00 9.7313769e-13 1.6850662e-15
 9.2368939e-11], sum to 1.0000
[2019-04-10 14:16:15,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-10 14:16:15,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.4, 76.66666666666667, 0.0, 0.0, 19.0, 24.07876973074548, 0.08076077951113247, 0.0, 1.0, 50.0, 39.90193748923974], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2605200.0000, 
sim time next is 2606400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 19.0, 24.01615643603214, 0.06438692043781746, 0.0, 1.0, 50.0, 40.11379167301814], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.78, 0.0, 0.0, 0.08333333333333333, 0.5013463696693451, 0.5214623068126057, 0.0, 1.0, 0.7, 0.4011379167301814], 
reward next is 0.5989, 
noisyNet noise sample is [array([0.76625335], dtype=float32), 0.36917117]. 
=============================================
[2019-04-10 14:16:18,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4930093e-10 1.5646505e-11 2.2530419e-10 1.6599777e-10 4.9891078e-09
 3.8394033e-13 5.8734349e-14 1.0000000e+00 3.2260362e-13 1.3197817e-14
 4.3918488e-10], sum to 1.0000
[2019-04-10 14:16:18,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1345
[2019-04-10 14:16:18,219] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.666666666666667, 70.0, 0.0, 0.0, 19.0, 24.40058720921233, 0.2206148594956628, 0.0, 1.0, 50.0, 41.18616738510198], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2676000.0000, 
sim time next is 2677200.0000, 
raw observation next is [-6.333333333333333, 71.0, 0.0, 0.0, 19.0, 24.31731137015797, 0.2031310057356806, 0.0, 1.0, 50.0, 41.31845037754741], 
processed observation next is [1.0, 1.0, 0.28716528162511545, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5264426141798308, 0.5677103352452268, 0.0, 1.0, 0.7, 0.41318450377547405], 
reward next is 0.5868, 
noisyNet noise sample is [array([0.09749778], dtype=float32), -0.72137004]. 
=============================================
[2019-04-10 14:16:18,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2780015e-10 4.7846055e-10 1.2157270e-09 4.8068605e-10 1.4016820e-08
 2.8030709e-12 3.2976738e-13 1.0000000e+00 8.0225314e-12 2.7190105e-13
 1.0270741e-08], sum to 1.0000
[2019-04-10 14:16:18,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-10 14:16:18,455] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 22.5, 23.618563509076, -0.005359905641594315, 1.0, 1.0, 50.0, 38.437535088550604], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2713200.0000, 
sim time next is 2714400.0000, 
raw observation next is [-12.0, 76.0, 107.0, 643.0, 22.5, 23.96055930789499, 0.06208024020501399, 1.0, 1.0, 50.0, 37.601234872687016], 
processed observation next is [1.0, 0.43478260869565216, 0.13019390581717452, 0.76, 0.3566666666666667, 0.7104972375690608, 0.375, 0.4967132756579158, 0.5206934134016713, 1.0, 1.0, 0.7, 0.37601234872687017], 
reward next is 0.6240, 
noisyNet noise sample is [array([1.1819679], dtype=float32), -1.3399168]. 
=============================================
[2019-04-10 14:16:18,771] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1896259e-08 2.7084619e-09 3.1468982e-08 1.2313373e-08 8.8037702e-08
 9.8102165e-11 7.1717350e-12 9.9999988e-01 8.2923043e-11 5.6006181e-12
 4.0119222e-08], sum to 1.0000
[2019-04-10 14:16:18,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2264
[2019-04-10 14:16:18,812] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 19.0, 22.65777096997791, -0.1862334160493099, 0.0, 1.0, 50.0, 41.53751330241001], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2697600.0000, 
sim time next is 2698800.0000, 
raw observation next is [-15.66666666666667, 83.0, 0.0, 0.0, 19.0, 22.60179747636159, -0.2166383200962102, 0.0, 1.0, 50.0, 41.36000598060858], 
processed observation next is [1.0, 0.21739130434782608, 0.02862419205909501, 0.83, 0.0, 0.0, 0.08333333333333333, 0.3834831230301325, 0.4277872266345966, 0.0, 1.0, 0.7, 0.4136000598060858], 
reward next is 0.5864, 
noisyNet noise sample is [array([0.56175387], dtype=float32), 0.19551282]. 
=============================================
[2019-04-10 14:16:24,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4392534e-11 2.1205474e-12 1.2013679e-10 9.2209462e-11 5.9093491e-10
 5.0505051e-14 4.3986116e-15 1.0000000e+00 9.6487563e-14 2.5135573e-16
 5.7320263e-11], sum to 1.0000
[2019-04-10 14:16:24,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3420
[2019-04-10 14:16:24,577] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 24.198364242664, 0.1304914993502188, 0.0, 1.0, 50.0, 52.8264074776456], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2869200.0000, 
sim time next is 2870400.0000, 
raw observation next is [1.0, 95.33333333333334, 0.0, 0.0, 19.0, 24.20799353003218, 0.1478511712242576, 0.0, 1.0, 50.0, 53.174166368058465], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.5173327941693483, 0.5492837237414192, 0.0, 1.0, 0.7, 0.5317416636805846], 
reward next is 0.4683, 
noisyNet noise sample is [array([-0.740921], dtype=float32), -1.8684176]. 
=============================================
[2019-04-10 14:16:25,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5139205e-11 2.0024250e-11 1.9425354e-11 3.5451732e-11 4.3027565e-10
 1.2547081e-13 6.2089311e-15 1.0000000e+00 1.7694795e-13 2.1541886e-15
 1.9398974e-09], sum to 1.0000
[2019-04-10 14:16:25,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8738
[2019-04-10 14:16:25,259] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 97.66666666666667, 73.33333333333334, 45.0, 22.5, 26.63269527704503, 0.6849143765097195, 1.0, 1.0, 50.0, 35.349838488788194], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2910000.0000, 
sim time next is 2911200.0000, 
raw observation next is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 22.5, 26.72465007723362, 0.7040656904969228, 1.0, 1.0, 50.0, 35.71352040823308], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9533333333333335, 0.20055555555555557, 0.057274401473296495, 0.375, 0.7270541731028016, 0.7346885634989743, 1.0, 1.0, 0.7, 0.3571352040823308], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.1043378], dtype=float32), 0.93690217]. 
=============================================
[2019-04-10 14:16:26,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.16101749e-08 1.19094921e-08 1.12415854e-07 5.52713928e-08
 2.37199245e-07 3.47836149e-10 8.81356585e-11 9.99999642e-01
 1.32916445e-09 2.78004841e-11 5.00016881e-08], sum to 1.0000
[2019-04-10 14:16:26,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6293
[2019-04-10 14:16:26,289] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 19.0, 23.38898768401787, 0.03065870904611536, 0.0, 1.0, 50.0, 43.75953700776091], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2976000.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 19.0, 23.41625623605038, 0.05888326875369828, 0.0, 1.0, 50.0, 43.207817663049624], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.08333333333333333, 0.4513546863375317, 0.5196277562512327, 0.0, 1.0, 0.7, 0.43207817663049625], 
reward next is 0.5679, 
noisyNet noise sample is [array([0.28934324], dtype=float32), -1.0174081]. 
=============================================
[2019-04-10 14:16:26,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39199: loss 0.7947
[2019-04-10 14:16:26,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39200: learning rate 0.0010
[2019-04-10 14:16:27,016] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39223: loss 1.1824
[2019-04-10 14:16:27,017] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39223: learning rate 0.0010
[2019-04-10 14:16:27,182] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39296: loss 1.4110
[2019-04-10 14:16:27,184] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39296: learning rate 0.0010
[2019-04-10 14:16:28,252] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39749: loss 0.9176
[2019-04-10 14:16:28,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39749: learning rate 0.0010
[2019-04-10 14:16:28,542] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39867: loss 0.9269
[2019-04-10 14:16:28,551] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39868: learning rate 0.0010
[2019-04-10 14:16:28,634] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39907: loss 1.0501
[2019-04-10 14:16:28,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39907: learning rate 0.0010
[2019-04-10 14:16:28,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8419439e-08 2.1094604e-09 1.2221587e-07 5.9185009e-08 1.7325955e-07
 3.6514708e-10 2.1609329e-11 9.9999964e-01 3.7843992e-10 2.1817980e-11
 3.2733730e-08], sum to 1.0000
[2019-04-10 14:16:28,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2716
[2019-04-10 14:16:28,895] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40015: loss 1.0341
[2019-04-10 14:16:28,896] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40015: learning rate 0.0010
[2019-04-10 14:16:28,906] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 23.63838078106303, 0.05076219028000278, 0.0, 1.0, 50.0, 52.396034522726495], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2960400.0000, 
sim time next is 2961600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 23.49415512116322, 0.02310227968734897, 0.0, 1.0, 50.0, 57.02225247877608], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.4578462600969351, 0.507700759895783, 0.0, 1.0, 0.7, 0.5702225247877608], 
reward next is 0.4298, 
noisyNet noise sample is [array([-1.2418127], dtype=float32), 0.72306466]. 
=============================================
[2019-04-10 14:16:28,997] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40061: loss 1.0268
[2019-04-10 14:16:28,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40062: learning rate 0.0010
[2019-04-10 14:16:29,124] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40114: loss 1.2227
[2019-04-10 14:16:29,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40114: learning rate 0.0010
[2019-04-10 14:16:29,195] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40146: loss 1.0715
[2019-04-10 14:16:29,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40146: learning rate 0.0010
[2019-04-10 14:16:29,333] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40210: loss 1.3270
[2019-04-10 14:16:29,334] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40210: learning rate 0.0010
[2019-04-10 14:16:29,441] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40261: loss 1.6935
[2019-04-10 14:16:29,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40261: learning rate 0.0010
[2019-04-10 14:16:29,580] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 40323: loss 2.1289
[2019-04-10 14:16:29,582] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 40323: learning rate 0.0010
[2019-04-10 14:16:30,014] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 40516: loss 2.5605
[2019-04-10 14:16:30,015] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 40516: learning rate 0.0010
[2019-04-10 14:16:30,123] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40563: loss 2.5656
[2019-04-10 14:16:30,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40563: learning rate 0.0010
[2019-04-10 14:16:30,207] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40602: loss 2.5474
[2019-04-10 14:16:30,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40602: learning rate 0.0010
[2019-04-10 14:16:30,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3739859e-09 4.3924020e-10 3.1844245e-08 1.0547346e-08 1.8654539e-08
 5.8225924e-11 2.1171365e-12 1.0000000e+00 4.1873258e-11 9.2412905e-13
 1.0899993e-09], sum to 1.0000
[2019-04-10 14:16:30,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-10 14:16:30,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 54.0, 108.5, 782.0, 19.0, 23.48054953383138, 0.008925547496342182, 0.0, 1.0, 50.0, 32.527613843162015], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3063600.0000, 
sim time next is 3064800.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 19.0, 23.60016369777757, 0.0282567701360985, 0.0, 1.0, 50.0, 32.72041849189144], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.36722222222222234, 0.8810313075506446, 0.08333333333333333, 0.46668030814813094, 0.5094189233786995, 0.0, 1.0, 0.7, 0.32720418491891434], 
reward next is 0.6728, 
noisyNet noise sample is [array([-1.4112619], dtype=float32), 0.662339]. 
=============================================
[2019-04-10 14:16:38,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0550165e-09 2.9307914e-09 8.1884997e-09 8.3431582e-09 1.7395992e-07
 3.5177247e-11 5.8974626e-12 9.9999976e-01 8.7018032e-11 3.7226767e-12
 2.7283313e-08], sum to 1.0000
[2019-04-10 14:16:38,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8405
[2019-04-10 14:16:38,604] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-10.33333333333333, 81.66666666666667, 72.0, 345.6666666666667, 22.5, 23.83091473335699, 0.1791613037846389, 1.0, 1.0, 50.0, 40.66429932469241], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3313200.0000, 
sim time next is 3314400.0000, 
raw observation next is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 22.5, 24.39198855015012, 0.245195781710715, 1.0, 1.0, 50.0, 39.55307999995202], 
processed observation next is [1.0, 0.34782608695652173, 0.19482917820867957, 0.7933333333333333, 0.2966666666666667, 0.47790055248618785, 0.375, 0.53266571251251, 0.581731927236905, 1.0, 1.0, 0.7, 0.3955307999995202], 
reward next is 0.6045, 
noisyNet noise sample is [array([0.1104247], dtype=float32), -1.665931]. 
=============================================
[2019-04-10 14:16:43,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3310117e-09 2.2498041e-10 7.2658374e-10 9.9330788e-10 4.0134052e-09
 3.5017174e-12 3.2468144e-13 1.0000000e+00 1.6016565e-12 3.2194283e-13
 4.5562230e-09], sum to 1.0000
[2019-04-10 14:16:43,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-10 14:16:43,762] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 26.81555324458479, 0.8281092065657981, 0.0, 1.0, 50.0, 35.599684166716216], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3445200.0000, 
sim time next is 3446400.0000, 
raw observation next is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 26.78248234142824, 0.8205195044716592, 0.0, 1.0, 50.0, 35.457738618528325], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8133333333333335, 0.0, 0.0, 0.08333333333333333, 0.7318735284523532, 0.773506501490553, 0.0, 1.0, 0.7, 0.3545773861852832], 
reward next is 0.6454, 
noisyNet noise sample is [array([-0.35161203], dtype=float32), -1.4985763]. 
=============================================
[2019-04-10 14:16:43,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1831206e-10 1.1154005e-10 1.6813022e-09 8.7193042e-10 1.9179187e-09
 1.6532300e-12 3.3963040e-13 1.0000000e+00 5.1480404e-12 8.8119765e-14
 1.6458953e-09], sum to 1.0000
[2019-04-10 14:16:43,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-10 14:16:43,850] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 76.66666666666667, 0.0, 0.0, 19.0, 26.16404992545285, 0.6297470992186426, 0.0, 1.0, 50.0, 37.371451517009824], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3464400.0000, 
sim time next is 3465600.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 19.0, 26.11851550723135, 0.606112688978044, 0.0, 1.0, 50.0, 37.47446879290402], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.08333333333333333, 0.6765429589359458, 0.7020375629926813, 0.0, 1.0, 0.7, 0.3747446879290402], 
reward next is 0.6253, 
noisyNet noise sample is [array([-0.67471033], dtype=float32), -0.620783]. 
=============================================
[2019-04-10 14:16:46,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47096: loss 0.0779
[2019-04-10 14:16:46,427] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47096: learning rate 0.0010
[2019-04-10 14:16:46,504] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47129: loss 0.0913
[2019-04-10 14:16:46,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47130: learning rate 0.0010
[2019-04-10 14:16:46,720] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47220: loss 0.0398
[2019-04-10 14:16:46,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47221: learning rate 0.0010
[2019-04-10 14:16:48,023] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47794: loss 0.1357
[2019-04-10 14:16:48,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47794: learning rate 0.0010
[2019-04-10 14:16:48,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47841: loss 0.0828
[2019-04-10 14:16:48,131] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47841: learning rate 0.0010
[2019-04-10 14:16:48,202] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47873: loss 0.1715
[2019-04-10 14:16:48,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47873: learning rate 0.0010
[2019-04-10 14:16:48,634] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48064: loss 0.2006
[2019-04-10 14:16:48,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48064: learning rate 0.0010
[2019-04-10 14:16:48,753] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48117: loss 0.1946
[2019-04-10 14:16:48,766] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48117: learning rate 0.0010
[2019-04-10 14:16:48,936] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48201: loss 0.1936
[2019-04-10 14:16:48,938] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48201: learning rate 0.0010
[2019-04-10 14:16:49,013] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48234: loss 0.1598
[2019-04-10 14:16:49,016] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48234: learning rate 0.0010
[2019-04-10 14:16:49,023] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48240: loss 0.1360
[2019-04-10 14:16:49,025] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48241: learning rate 0.0010
[2019-04-10 14:16:49,130] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48285: loss 0.0171
[2019-04-10 14:16:49,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48287: learning rate 0.0010
[2019-04-10 14:16:49,400] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 48407: loss 0.0029
[2019-04-10 14:16:49,401] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 48407: learning rate 0.0010
[2019-04-10 14:16:49,645] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48528: loss 0.0020
[2019-04-10 14:16:49,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48529: learning rate 0.0010
[2019-04-10 14:16:49,684] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48546: loss 0.0064
[2019-04-10 14:16:49,688] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48546: learning rate 0.0010
[2019-04-10 14:16:49,692] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48551: loss 0.0049
[2019-04-10 14:16:49,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48551: learning rate 0.0010
[2019-04-10 14:16:50,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1514543e-10 1.3472497e-11 7.7834770e-09 1.0752763e-09 5.0629794e-09
 6.8845836e-12 1.5476624e-13 1.0000000e+00 7.9996270e-12 1.4630122e-14
 8.5278180e-11], sum to 1.0000
[2019-04-10 14:16:50,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1357
[2019-04-10 14:16:50,170] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 26.14874812800469, 0.5362441129343464, 0.0, 1.0, 50.0, 33.28275440299898], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3634800.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 26.15541451731067, 0.5355262808652884, 0.0, 1.0, 50.0, 33.157186177071154], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.6796178764425559, 0.6785087602884294, 0.0, 1.0, 0.7, 0.3315718617707115], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.89345205], dtype=float32), 0.44539323]. 
=============================================
[2019-04-10 14:16:50,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.94011 ]
 [70.80624 ]
 [70.796   ]
 [70.66421 ]
 [70.715614]], R is [[70.92094421]
 [70.87891388]
 [70.83603668]
 [70.7922821 ]
 [70.74732971]].
[2019-04-10 14:16:50,213] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.9319982e-11 4.5764486e-12 1.2842742e-09 1.3064501e-10 1.7204446e-09
 4.1799802e-13 5.1414215e-14 1.0000000e+00 4.4193536e-13 2.5900751e-15
 1.3143106e-10], sum to 1.0000
[2019-04-10 14:16:50,213] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-10 14:16:50,248] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.17473320755344, 0.5716326128686587, 0.0, 1.0, 50.0, 37.73298460712378], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3727200.0000, 
sim time next is 3728400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.18551241660847, 0.5704675232956055, 0.0, 1.0, 50.0, 37.797297108644095], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6821260347173727, 0.6901558410985351, 0.0, 1.0, 0.7, 0.37797297108644096], 
reward next is 0.6220, 
noisyNet noise sample is [array([-0.49632818], dtype=float32), -0.64196944]. 
=============================================
[2019-04-10 14:16:52,911] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 14:16:52,912] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:16:52,913] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:16:52,914] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run2
[2019-04-10 14:16:52,928] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:16:52,931] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:16:52,932] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:16:52,933] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:16:52,937] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run2
[2019-04-10 14:16:52,949] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run2
[2019-04-10 14:17:25,637] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([-0.0502126], dtype=float32), -0.13266434]
[2019-04-10 14:17:25,637] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [6.7, 80.0, 146.5, 134.0, 22.5, 27.86293349679025, 1.118911182380522, 1.0, 1.0, 50.0, 39.326763597098115]
[2019-04-10 14:17:25,637] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-10 14:17:25,638] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [9.2651737e-12 6.3467048e-13 3.0497948e-11 5.4592685e-12 1.3522784e-10
 3.1067342e-14 7.4829998e-16 1.0000000e+00 3.6587710e-14 3.4539158e-17
 9.4216223e-12], sampled 0.013718703860376658
[2019-04-10 14:17:51,752] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([-0.0502126], dtype=float32), -0.13266434]
[2019-04-10 14:17:51,752] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.0, 67.0, 0.0, 0.0, 19.0, 26.09683763737482, 0.5837308841467467, 0.0, 1.0, 50.0, 37.80350479640444]
[2019-04-10 14:17:51,753] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-10 14:17:51,753] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [8.7716479e-10 5.1608041e-11 5.0081090e-09 6.9797751e-10 8.4126226e-09
 7.5365859e-12 2.8015193e-13 1.0000000e+00 9.0017906e-12 2.9373605e-14
 3.1481201e-10], sampled 0.39283327402597
[2019-04-10 14:18:01,775] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2824.8074 141968.0060 1220.6114
[2019-04-10 14:18:01,794] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:01,794] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:01,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:01,903] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:12,278] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2741.8289 150712.3196 941.3419
[2019-04-10 14:18:12,297] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:12,297] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:12,410] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:12,410] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:16,116] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2704.0682 154400.1257 786.4212
[2019-04-10 14:18:16,135] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:16,135] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:16,248] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:16,248] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:17,138] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 50000, evaluation results [50000.0, 2741.828868918773, 150712.31962660837, 941.3419215034918, 2824.8073743801824, 141968.005968651, 1220.6113995757278, 2704.068242241476, 154400.12574442284, 786.4212023905551]
[2019-04-10 14:18:21,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.59646806e-10 1.75226944e-10 9.34121780e-10 6.63285427e-10
 1.93414582e-08 2.12335618e-12 4.26749890e-13 1.00000000e+00
 7.26472138e-12 1.04752246e-13 5.67089531e-09], sum to 1.0000
[2019-04-10 14:18:21,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5701
[2019-04-10 14:18:21,683] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.17938028670861, 0.651598446026215, 0.0, 1.0, 50.0, 35.5222124591249], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3890400.0000, 
sim time next is 3891600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.12853798529303, 0.6409745215676578, 0.0, 1.0, 50.0, 35.61136387146457], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6773781654410858, 0.7136581738558859, 0.0, 1.0, 0.7, 0.3561136387146457], 
reward next is 0.6439, 
noisyNet noise sample is [array([-0.6634944], dtype=float32), 1.9120891]. 
=============================================
[2019-04-10 14:18:26,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7057243e-10 6.3851771e-11 3.9241582e-10 4.7381538e-10 6.8120216e-09
 1.0637817e-12 4.8156081e-13 1.0000000e+00 3.9764056e-12 1.1966248e-14
 7.0828921e-10], sum to 1.0000
[2019-04-10 14:18:26,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6873
[2019-04-10 14:18:26,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 22.5, 24.12678531581914, 0.1462298366678505, 1.0, 1.0, 50.0, 38.569992094228354], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4005600.0000, 
sim time next is 4006800.0000, 
raw observation next is [-11.0, 53.0, 97.0, 571.0, 22.5, 24.64058975796097, 0.2297049193014382, 1.0, 1.0, 50.0, 37.42210315198956], 
processed observation next is [1.0, 0.391304347826087, 0.15789473684210528, 0.53, 0.3233333333333333, 0.630939226519337, 0.375, 0.5533824798300809, 0.5765683064338127, 1.0, 1.0, 0.7, 0.3742210315198956], 
reward next is 0.6258, 
noisyNet noise sample is [array([0.8261957], dtype=float32), 0.7406196]. 
=============================================
[2019-04-10 14:18:27,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7153910e-10 3.7502529e-11 4.7396181e-10 2.6358521e-10 8.8635126e-09
 7.1108874e-13 5.4589610e-14 1.0000000e+00 7.0335551e-13 1.4418723e-14
 4.3262427e-10], sum to 1.0000
[2019-04-10 14:18:27,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3544
[2019-04-10 14:18:27,445] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 19.0, 25.04966132864029, 0.3459380690464518, 0.0, 1.0, 50.0, 36.46517871143285], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4060800.0000, 
sim time next is 4062000.0000, 
raw observation next is [-6.0, 38.33333333333334, 0.0, 0.0, 19.0, 24.95306910165881, 0.3241005962178299, 0.0, 1.0, 50.0, 36.71728726978536], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3833333333333334, 0.0, 0.0, 0.08333333333333333, 0.579422425138234, 0.6080335320726099, 0.0, 1.0, 0.7, 0.3671728726978536], 
reward next is 0.6328, 
noisyNet noise sample is [array([0.13172267], dtype=float32), -0.39032736]. 
=============================================
[2019-04-10 14:18:27,456] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.57262 ]
 [69.8164  ]
 [69.946945]
 [70.101616]
 [70.36097 ]], R is [[69.38894653]
 [69.33040619]
 [69.27508545]
 [69.22340393]
 [69.17575073]].
[2019-04-10 14:18:29,571] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 54916: loss 2.0734
[2019-04-10 14:18:29,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 54916: learning rate 0.0010
[2019-04-10 14:18:29,820] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55016: loss 2.4568
[2019-04-10 14:18:29,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55016: learning rate 0.0010
[2019-04-10 14:18:30,122] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55137: loss 2.4246
[2019-04-10 14:18:30,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55137: learning rate 0.0010
[2019-04-10 14:18:30,163] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5628080e-12 5.9537483e-12 2.7406159e-11 1.5920237e-11 8.0731137e-11
 4.2319779e-14 2.6428127e-15 1.0000000e+00 8.0429620e-14 7.0420011e-16
 3.9166115e-11], sum to 1.0000
[2019-04-10 14:18:30,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4977
[2019-04-10 14:18:30,183] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.666666666666667, 41.0, 0.0, 0.0, 22.5, 27.47079642086602, 0.926868794238836, 0.0, 1.0, 50.0, 28.567653076448607], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4131600.0000, 
sim time next is 4132800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 22.5, 27.38549497468966, 0.9121263479655163, 0.0, 1.0, 50.0, 28.03082579521501], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.375, 0.7821245812241383, 0.8040421159885054, 0.0, 1.0, 0.7, 0.2803082579521501], 
reward next is 0.7197, 
noisyNet noise sample is [array([-1.4135342], dtype=float32), 1.1637642]. 
=============================================
[2019-04-10 14:18:31,326] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55647: loss 3.5166
[2019-04-10 14:18:31,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55648: learning rate 0.0010
[2019-04-10 14:18:31,659] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55800: loss 2.1374
[2019-04-10 14:18:31,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55800: learning rate 0.0010
[2019-04-10 14:18:31,805] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.63637764e-10 6.98312727e-11 9.88004523e-09 1.05698361e-09
 7.00906178e-09 1.16123135e-11 2.50921164e-13 1.00000000e+00
 4.14046054e-11 2.00758853e-14 9.43471828e-11], sum to 1.0000
[2019-04-10 14:18:31,805] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-10 14:18:31,841] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.0, 25.61793611236516, 0.4082831798238899, 0.0, 1.0, 50.0, 36.454349479313294], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4260000.0000, 
sim time next is 4261200.0000, 
raw observation next is [3.0, 49.0, 18.33333333333333, 8.833333333333332, 19.0, 25.60433552311713, 0.4079012164604101, 0.0, 1.0, 50.0, 36.4448573208269], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.061111111111111095, 0.009760589318600367, 0.08333333333333333, 0.6336946269264274, 0.63596707215347, 0.0, 1.0, 0.7, 0.36444857320826896], 
reward next is 0.6356, 
noisyNet noise sample is [array([1.2078623], dtype=float32), -1.1610097]. 
=============================================
[2019-04-10 14:18:32,140] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56014: loss 2.6518
[2019-04-10 14:18:32,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56014: learning rate 0.0010
[2019-04-10 14:18:32,350] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56104: loss 2.2529
[2019-04-10 14:18:32,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56106: learning rate 0.0010
[2019-04-10 14:18:32,356] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56107: loss 2.3109
[2019-04-10 14:18:32,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56107: learning rate 0.0010
[2019-04-10 14:18:32,429] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56141: loss 2.1981
[2019-04-10 14:18:32,432] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56141: learning rate 0.0010
[2019-04-10 14:18:32,765] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56293: loss 1.5383
[2019-04-10 14:18:32,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56297: learning rate 0.0010
[2019-04-10 14:18:32,893] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56349: loss 1.3780
[2019-04-10 14:18:32,894] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56349: learning rate 0.0010
[2019-04-10 14:18:33,035] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56412: loss 1.6700
[2019-04-10 14:18:33,038] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56412: learning rate 0.0010
[2019-04-10 14:18:33,156] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 56468: loss 1.3147
[2019-04-10 14:18:33,157] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 56468: learning rate 0.0010
[2019-04-10 14:18:33,354] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56556: loss 1.5342
[2019-04-10 14:18:33,355] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56556: learning rate 0.0010
[2019-04-10 14:18:33,504] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 56624: loss 1.4836
[2019-04-10 14:18:33,505] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 56624: learning rate 0.0010
[2019-04-10 14:18:33,517] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56631: loss 1.4080
[2019-04-10 14:18:33,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56632: learning rate 0.0010
[2019-04-10 14:18:35,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.12070769e-11 6.96762038e-13 6.96126490e-11 1.23780179e-11
 1.05815905e-10 3.95827102e-14 3.52922180e-16 1.00000000e+00
 3.97157350e-14 3.63784802e-17 8.18071479e-12], sum to 1.0000
[2019-04-10 14:18:35,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-10 14:18:35,220] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.4, 75.66666666666667, 0.0, 0.0, 19.0, 27.10969597061781, 0.7858351414480902, 0.0, 1.0, 50.0, 35.0613994394785], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4321200.0000, 
sim time next is 4322400.0000, 
raw observation next is [4.3, 75.33333333333334, 0.0, 0.0, 19.0, 27.04775935077342, 0.7764913457725608, 0.0, 1.0, 50.0, 35.25736705155816], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7539799458977848, 0.7588304485908536, 0.0, 1.0, 0.7, 0.35257367051558164], 
reward next is 0.6474, 
noisyNet noise sample is [array([0.36690834], dtype=float32), -0.12602137]. 
=============================================
[2019-04-10 14:18:40,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6407478e-10 9.4327934e-11 4.0888931e-10 7.9904550e-10 1.2635197e-09
 2.7064802e-12 2.1387724e-13 1.0000000e+00 1.4457678e-12 5.7515990e-14
 3.4536646e-10], sum to 1.0000
[2019-04-10 14:18:40,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5762
[2019-04-10 14:18:40,171] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.666666666666667, 72.0, 0.0, 0.0, 19.0, 27.42399125807485, 0.9505445975616565, 0.0, 1.0, 50.0, 35.098340117249364], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4429200.0000, 
sim time next is 4430400.0000, 
raw observation next is [2.333333333333333, 76.0, 0.0, 0.0, 19.0, 27.39616509326015, 0.954403859688011, 0.0, 1.0, 50.0, 35.68429357207704], 
processed observation next is [1.0, 0.2608695652173913, 0.5272391505078486, 0.76, 0.0, 0.0, 0.08333333333333333, 0.7830137577716793, 0.8181346198960037, 0.0, 1.0, 0.7, 0.35684293572077036], 
reward next is 0.6432, 
noisyNet noise sample is [array([0.9547979], dtype=float32), -0.06186484]. 
=============================================
[2019-04-10 14:18:42,696] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7508858e-11 6.9291808e-11 3.8732784e-11 2.6970834e-10 1.6715179e-09
 5.8942306e-14 1.6292505e-13 1.0000000e+00 6.0082402e-13 9.2783879e-15
 9.8231234e-10], sum to 1.0000
[2019-04-10 14:18:42,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9402
[2019-04-10 14:18:42,730] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 22.5, 27.17603035726673, 1.041055646834884, 1.0, 1.0, 50.0, 12.243088614870839], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4628400.0000, 
sim time next is 4629600.0000, 
raw observation next is [4.7, 49.0, 171.0, 706.0, 22.5, 28.49203569342212, 1.143443073670048, 1.0, 1.0, 50.0, 6.226486424721562], 
processed observation next is [1.0, 0.6086956521739131, 0.592797783933518, 0.49, 0.57, 0.7801104972375691, 0.375, 0.8743363077851768, 0.8811476912233495, 1.0, 1.0, 0.7, 0.062264864247215626], 
reward next is 0.9377, 
noisyNet noise sample is [array([-0.15837984], dtype=float32), -1.654984]. 
=============================================
[2019-04-10 14:18:44,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7058490e-10 1.4173783e-10 8.0918949e-10 2.3987636e-09 7.1688881e-09
 2.0658891e-12 7.3784869e-13 1.0000000e+00 6.7144771e-12 5.0052410e-14
 1.5457068e-09], sum to 1.0000
[2019-04-10 14:18:44,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-10 14:18:44,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 19.0, 25.69780749926871, 0.5003878341487579, 0.0, 1.0, 50.0, 33.25015622491692], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4604400.0000, 
sim time next is 4605600.0000, 
raw observation next is [-2.666666666666667, 75.0, 0.0, 0.0, 22.5, 25.59129148236219, 0.4792937510203407, 0.0, 1.0, 50.0, 33.353797130613174], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.75, 0.0, 0.0, 0.375, 0.6326076235301826, 0.6597645836734469, 0.0, 1.0, 0.7, 0.3335379713061317], 
reward next is 0.6665, 
noisyNet noise sample is [array([-0.47743177], dtype=float32), -1.9499544]. 
=============================================
[2019-04-10 14:18:44,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9583580e-12 5.9987020e-13 5.3728779e-12 6.1600933e-12 3.4017143e-11
 6.9122831e-15 1.4015647e-15 1.0000000e+00 1.6649398e-14 4.7222219e-17
 2.3085343e-11], sum to 1.0000
[2019-04-10 14:18:44,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8118
[2019-04-10 14:18:44,763] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 22.5, 27.22115376511082, 0.8243036005825991, 1.0, 1.0, 50.0, 25.43604530897995], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4614000.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 22.5, 27.47426237106754, 0.8755884171307914, 1.0, 1.0, 50.0, 24.26954782650818], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.375, 0.7895218642556282, 0.7918628057102638, 1.0, 1.0, 0.7, 0.2426954782650818], 
reward next is 0.7573, 
noisyNet noise sample is [array([0.8764703], dtype=float32), 0.70395315]. 
=============================================
[2019-04-10 14:18:47,451] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 62820: loss 0.4391
[2019-04-10 14:18:47,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 62820: learning rate 0.0010
[2019-04-10 14:18:47,926] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63019: loss 0.4578
[2019-04-10 14:18:47,926] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63019: learning rate 0.0010
[2019-04-10 14:18:48,071] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63077: loss 0.4743
[2019-04-10 14:18:48,072] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63077: learning rate 0.0010
[2019-04-10 14:18:48,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5557275e-09 2.6647146e-11 7.8357454e-09 7.2847373e-10 3.9581378e-09
 1.1533862e-11 4.0197788e-13 1.0000000e+00 2.8106049e-11 4.2855723e-14
 1.6402441e-10], sum to 1.0000
[2019-04-10 14:18:48,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1727
[2019-04-10 14:18:48,812] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.48283001290601, 0.4444957147630209, 0.0, 1.0, 50.0, 35.99664913000488], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4844400.0000, 
sim time next is 4845600.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.46086598421288, 0.4308764205970286, 0.0, 1.0, 50.0, 36.02759072587621], 
processed observation next is [0.0, 0.08695652173913043, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.62173883201774, 0.6436254735323429, 0.0, 1.0, 0.7, 0.3602759072587621], 
reward next is 0.6397, 
noisyNet noise sample is [array([0.22198194], dtype=float32), 0.92058563]. 
=============================================
[2019-04-10 14:18:49,331] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63609: loss 0.1557
[2019-04-10 14:18:49,332] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63609: learning rate 0.0010
[2019-04-10 14:18:50,061] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63928: loss 0.1504
[2019-04-10 14:18:50,061] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63928: learning rate 0.0010
[2019-04-10 14:18:50,127] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2059745e-09 2.6360431e-10 4.4270738e-09 3.2649980e-09 1.2304806e-08
 5.7350088e-12 8.9278677e-13 1.0000000e+00 2.4431910e-11 4.4559330e-14
 4.5481546e-10], sum to 1.0000
[2019-04-10 14:18:50,129] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-10 14:18:50,163] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 46.0, 137.5, 784.5, 19.0, 25.13442261485976, 0.4704810425832435, 0.0, 1.0, 50.0, 33.78908319828593], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4791600.0000, 
sim time next is 4792800.0000, 
raw observation next is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 19.0, 25.24658496275633, 0.4939825558720481, 0.0, 1.0, 50.0, 33.23377556632459], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.45, 0.423888888888889, 0.9069981583793739, 0.08333333333333333, 0.6038820802296941, 0.6646608519573494, 0.0, 1.0, 0.7, 0.33233775566324586], 
reward next is 0.6677, 
noisyNet noise sample is [array([0.14106531], dtype=float32), 1.7900819]. 
=============================================
[2019-04-10 14:18:50,176] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63979: loss 0.1160
[2019-04-10 14:18:50,177] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63979: learning rate 0.0010
[2019-04-10 14:18:50,368] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64064: loss 0.0573
[2019-04-10 14:18:50,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64065: learning rate 0.0010
[2019-04-10 14:18:50,398] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64076: loss 0.0313
[2019-04-10 14:18:50,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64076: learning rate 0.0010
[2019-04-10 14:18:50,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9735163e-09 3.0699782e-10 9.0858450e-09 9.7467030e-09 3.1803090e-08
 3.2537178e-11 2.2370335e-12 1.0000000e+00 8.4645860e-11 3.6789964e-13
 2.4106337e-09], sum to 1.0000
[2019-04-10 14:18:50,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9718
[2019-04-10 14:18:50,452] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.2, 93.0, 0.0, 0.0, 19.0, 24.33972708075122, 0.234130795696191, 0.0, 1.0, 50.0, 39.09727868325642], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4777200.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 19.0, 24.27039640031064, 0.2182151255590654, 0.0, 1.0, 50.0, 39.17035283161016], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.5225330333592201, 0.5727383751863552, 0.0, 1.0, 0.7, 0.3917035283161016], 
reward next is 0.6083, 
noisyNet noise sample is [array([-0.58710194], dtype=float32), -0.40515533]. 
=============================================
[2019-04-10 14:18:50,467] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64106: loss 0.0247
[2019-04-10 14:18:50,468] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64106: learning rate 0.0010
[2019-04-10 14:18:50,859] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64287: loss 0.0062
[2019-04-10 14:18:50,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64287: learning rate 0.0010
[2019-04-10 14:18:51,113] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64393: loss 0.0067
[2019-04-10 14:18:51,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64394: learning rate 0.0010
[2019-04-10 14:18:51,156] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 64419: loss 0.0074
[2019-04-10 14:18:51,159] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 64420: learning rate 0.0010
[2019-04-10 14:18:51,166] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4649063e-09 3.3640990e-10 4.6947868e-09 3.4325331e-09 7.6532229e-09
 1.4593941e-11 9.0808020e-13 1.0000000e+00 1.6806336e-11 1.2838161e-13
 4.9592291e-10], sum to 1.0000
[2019-04-10 14:18:51,167] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8455
[2019-04-10 14:18:51,178] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64426: loss 0.0275
[2019-04-10 14:18:51,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64426: learning rate 0.0010
[2019-04-10 14:18:51,200] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 19.0, 26.03604557401352, 0.6548515536913967, 0.0, 1.0, 50.0, 30.28739339077986], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4800000.0000, 
sim time next is 4801200.0000, 
raw observation next is [2.666666666666667, 38.0, 208.0, 597.0, 19.0, 26.08746438573916, 0.6735211145037661, 0.0, 1.0, 50.0, 30.16143706936638], 
processed observation next is [0.0, 0.5652173913043478, 0.5364727608494922, 0.38, 0.6933333333333334, 0.6596685082872928, 0.08333333333333333, 0.6739553654782634, 0.7245070381679221, 0.0, 1.0, 0.7, 0.3016143706936638], 
reward next is 0.6984, 
noisyNet noise sample is [array([0.5087212], dtype=float32), 1.1384307]. 
=============================================
[2019-04-10 14:18:51,335] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64495: loss 0.0160
[2019-04-10 14:18:51,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64495: learning rate 0.0010
[2019-04-10 14:18:51,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9339323e-09 4.4896312e-10 1.5444579e-08 7.3952675e-09 6.1451779e-08
 3.8119268e-11 1.3755725e-12 9.9999988e-01 1.0239121e-10 1.9370830e-13
 4.0817394e-10], sum to 1.0000
[2019-04-10 14:18:51,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0887
[2019-04-10 14:18:51,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.74149435537524, 0.4898806629319308, 0.0, 1.0, 50.0, 35.806945990248295], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4840800.0000, 
sim time next is 4842000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.64032966119287, 0.471456908794763, 0.0, 1.0, 50.0, 35.879214035445244], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.636694138432739, 0.6571523029315877, 0.0, 1.0, 0.7, 0.35879214035445245], 
reward next is 0.6412, 
noisyNet noise sample is [array([-0.57545775], dtype=float32), -0.19008608]. 
=============================================
[2019-04-10 14:18:51,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.10344 ]
 [62.225876]
 [62.318993]
 [62.448772]
 [62.49773 ]], R is [[62.06362152]
 [62.08491516]
 [62.10690689]
 [62.12952423]
 [62.15202332]].
[2019-04-10 14:18:51,728] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64671: loss 0.0266
[2019-04-10 14:18:51,729] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64671: learning rate 0.0010
[2019-04-10 14:18:51,849] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 64729: loss 0.0264
[2019-04-10 14:18:51,850] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 64729: learning rate 0.0010
[2019-04-10 14:18:54,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4290572e-10 2.9988106e-10 4.2283093e-09 8.2207191e-10 8.5513472e-09
 9.4012480e-12 1.1645134e-12 1.0000000e+00 1.2750105e-11 5.8037010e-14
 1.7388928e-09], sum to 1.0000
[2019-04-10 14:18:54,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0165
[2019-04-10 14:18:54,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 25.15758761597247, 0.2876789531940869, 0.0, 1.0, 50.0, 36.03974442057566], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4948800.0000, 
sim time next is 4950000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 25.06761330176459, 0.2727574668261668, 0.0, 1.0, 50.0, 36.15161645339283], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5889677751470493, 0.5909191556087222, 0.0, 1.0, 0.7, 0.3615161645339283], 
reward next is 0.6385, 
noisyNet noise sample is [array([0.3451979], dtype=float32), -0.4015219]. 
=============================================
[2019-04-10 14:18:54,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[59.618393]
 [59.711433]
 [59.830757]
 [59.963867]
 [60.107872]], R is [[59.56772232]
 [59.61164856]
 [59.65587234]
 [59.69982529]
 [59.74440765]].
[2019-04-10 14:18:55,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6935152e-14 1.1317839e-14 3.3999919e-13 1.8696131e-13 6.8067866e-13
 1.3003107e-16 6.7664863e-18 1.0000000e+00 7.3106289e-17 1.0115965e-19
 7.9254333e-13], sum to 1.0000
[2019-04-10 14:18:55,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-10 14:18:55,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [9.666666666666668, 23.33333333333334, 119.0, 860.8333333333334, 22.5, 28.83975633542157, 1.213007422540914, 1.0, 1.0, 50.0, 3.590759102568286], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 5059200.0000, 
sim time next is 5060400.0000, 
raw observation next is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 28.52855517416674, 1.241477157397366, 1.0, 1.0, 50.0, 21.1740908181565], 
processed observation next is [1.0, 0.5652173913043478, 0.7488457987072946, 0.21666666666666662, 0.3894444444444443, 0.9427255985267036, 0.375, 0.8773795978472284, 0.9138257191324554, 1.0, 1.0, 0.7, 0.211740908181565], 
reward next is 0.7883, 
noisyNet noise sample is [array([1.3265648], dtype=float32), 0.24794242]. 
=============================================
[2019-04-10 14:18:56,057] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2065381e-12 2.3510078e-13 5.3715049e-12 1.3203294e-12 1.0092860e-11
 2.9156788e-15 2.0807620e-16 1.0000000e+00 3.8203261e-15 1.7495982e-17
 4.9370829e-12], sum to 1.0000
[2019-04-10 14:18:56,058] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-10 14:18:56,078] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 27.95897787813497, 1.057705148024018, 1.0, 1.0, 50.0, 22.733732787477962], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4995600.0000, 
sim time next is 4996800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 27.90890917475134, 1.047811480321715, 0.0, 1.0, 50.0, 21.848397047391543], 
processed observation next is [1.0, 0.8695652173913043, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.8257424312292784, 0.8492704934405717, 0.0, 1.0, 0.7, 0.21848397047391543], 
reward next is 0.7815, 
noisyNet noise sample is [array([-0.9624015], dtype=float32), 0.53468287]. 
=============================================
[2019-04-10 14:18:56,334] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:56,482] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:56,604] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:56,757] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:56,805] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:56,968] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:57,335] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:18:57,335] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:18:57,337] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run2
[2019-04-10 14:18:57,607] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:18:57,607] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:18:57,609] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run2
[2019-04-10 14:18:57,806] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:18:57,806] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:18:57,808] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run2
[2019-04-10 14:18:57,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4096694e-13 4.0561928e-13 4.4949652e-12 3.5860919e-12 4.0160889e-11
 2.6808389e-15 1.5030680e-16 1.0000000e+00 5.2489276e-15 1.4904205e-17
 3.6530150e-12], sum to 1.0000
[2019-04-10 14:18:57,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0510
[2019-04-10 14:18:58,006] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 28.5285545728438, 1.241476980355009, 1.0, 1.0, 50.0, 21.174092043648837], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 5060400.0000, 
sim time next is 5061600.0000, 
raw observation next is [11.0, 20.0, 114.5, 839.5, 22.5, 28.81103223038322, 1.252395230229266, 1.0, 1.0, 50.0, 0.7301228207237063], 
processed observation next is [1.0, 0.6086956521739131, 0.7673130193905818, 0.2, 0.38166666666666665, 0.9276243093922651, 0.375, 0.9009193525319349, 0.9174650767430886, 1.0, 1.0, 0.7, 0.0073012282072370625], 
reward next is 0.9927, 
noisyNet noise sample is [array([-0.19893524], dtype=float32), 1.9015472]. 
=============================================
[2019-04-10 14:18:58,028] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:58,189] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:58,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.07856059e-13 1.80804834e-13 1.40400170e-12 8.35134208e-13
 1.09779366e-10 1.01673183e-16 8.39985765e-17 1.00000000e+00
 4.06063953e-15 2.56444064e-18 2.82142083e-12], sum to 1.0000
[2019-04-10 14:18:58,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-10 14:18:58,820] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [9.0, 25.0, 121.0, 862.5, 22.5, 28.72235859873317, 1.179279546396517, 1.0, 1.0, 50.0, 3.798646346436956], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 5058000.0000, 
sim time next is 5059200.0000, 
raw observation next is [9.666666666666668, 23.33333333333334, 119.0, 860.8333333333334, 22.5, 28.83975622378617, 1.213007390440177, 1.0, 1.0, 50.0, 3.590759102655129], 
processed observation next is [1.0, 0.5652173913043478, 0.7303785780240075, 0.2333333333333334, 0.39666666666666667, 0.9511970534069982, 0.375, 0.9033130186488476, 0.9043357968133924, 1.0, 1.0, 0.7, 0.03590759102655129], 
reward next is 0.9641, 
noisyNet noise sample is [array([0.17369545], dtype=float32), 0.84129167]. 
=============================================
[2019-04-10 14:18:58,864] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,029] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:18:59,029] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:18:59,031] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run2
[2019-04-10 14:18:59,043] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,045] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,047] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,080] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,142] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,190] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,227] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,247] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,328] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,399] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,554] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,623] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,676] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,711] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,781] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,817] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,823] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,865] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:18:59,865] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:18:59,867] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run2
[2019-04-10 14:18:59,883] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:18:59,884] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:18:59,936] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:19:00,026] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:19:00,028] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,028] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,029] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run2
[2019-04-10 14:19:00,048] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,049] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,049] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:19:00,051] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run2
[2019-04-10 14:19:00,081] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,081] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,083] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run2
[2019-04-10 14:19:00,139] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,139] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,141] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run2
[2019-04-10 14:19:00,401] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,401] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,403] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run2
[2019-04-10 14:19:00,624] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,624] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,626] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run2
[2019-04-10 14:19:00,677] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,677] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,679] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run2
[2019-04-10 14:19:00,715] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,715] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,717] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run2
[2019-04-10 14:19:00,825] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,825] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,827] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run2
[2019-04-10 14:19:00,899] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,900] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,905] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:19:00,905] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:19:00,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run2
[2019-04-10 14:19:00,991] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run2
[2019-04-10 14:19:07,460] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0102996e-10 1.5678320e-11 9.2673798e-09 1.2658009e-10 4.2214920e-09
 1.3621249e-11 5.4374739e-13 1.0000000e+00 2.3217197e-11 2.6827636e-14
 4.8052541e-11], sum to 1.0000
[2019-04-10 14:19:07,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-10 14:19:07,484] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 22.26457387884437, -0.298949451428009, 0.0, 1.0, 50.0, 38.73272354928501], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 8400.0000, 
sim time next is 9600.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 22.34613112610671, -0.2877170068743466, 0.0, 1.0, 50.0, 38.386446726356816], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.36217759384222575, 0.40409433104188447, 0.0, 1.0, 0.7, 0.38386446726356815], 
reward next is 0.6161, 
noisyNet noise sample is [array([0.7594992], dtype=float32), 1.2610499]. 
=============================================
[2019-04-10 14:19:14,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0617612e-11 5.2676884e-13 4.7727566e-10 1.5937623e-10 1.6406392e-09
 4.5698058e-14 6.5479391e-15 1.0000000e+00 8.8310564e-14 7.6619736e-16
 8.4904367e-12], sum to 1.0000
[2019-04-10 14:19:14,355] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8911
[2019-04-10 14:19:14,392] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.0, 84.66666666666667, 54.83333333333334, 0.0, 19.0, 23.79134383179046, 0.08079268960537682, 0.0, 1.0, 50.0, 36.06516107709194], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 55200.0000, 
sim time next is 56400.0000, 
raw observation next is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 19.0, 23.80717071819758, 0.08460655786346487, 0.0, 1.0, 50.0, 36.2248034613626], 
processed observation next is [0.0, 0.6521739130434783, 0.6509695290858727, 0.8333333333333335, 0.1488888888888889, 0.0, 0.08333333333333333, 0.4839308931831316, 0.5282021859544883, 0.0, 1.0, 0.7, 0.362248034613626], 
reward next is 0.6378, 
noisyNet noise sample is [array([-0.78362757], dtype=float32), -0.20949814]. 
=============================================
[2019-04-10 14:19:16,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3311745e-12 5.3409204e-11 2.4613389e-11 8.9919114e-11 1.4735908e-09
 2.5385945e-14 2.2267675e-14 1.0000000e+00 2.6984364e-14 8.3591428e-15
 3.3464298e-10], sum to 1.0000
[2019-04-10 14:19:16,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1197
[2019-04-10 14:19:16,490] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 22.5, 24.13141779357282, 0.1264140980970954, 1.0, 1.0, 50.0, 50.16578133583782], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 218400.0000, 
sim time next is 219600.0000, 
raw observation next is [-4.5, 65.0, 139.0, 0.0, 22.5, 25.00230322209013, 0.2019517669957513, 1.0, 1.0, 50.0, 49.413922003811365], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.65, 0.4633333333333333, 0.0, 0.375, 0.5835252685075109, 0.5673172556652505, 1.0, 1.0, 0.7, 0.49413922003811367], 
reward next is 0.5059, 
noisyNet noise sample is [array([0.19440076], dtype=float32), 0.095590726]. 
=============================================
[2019-04-10 14:19:18,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7324757e-13 3.8706159e-13 1.2491267e-11 6.2766355e-11 3.4011710e-10
 4.1874450e-15 7.7414746e-16 1.0000000e+00 4.3091764e-15 3.7250547e-17
 7.9331654e-12], sum to 1.0000
[2019-04-10 14:19:18,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8197
[2019-04-10 14:19:18,433] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.4, 78.0, 56.5, 148.0, 22.5, 23.03575572997618, -0.2192817189337498, 1.0, 1.0, 50.0, 51.82492853924619], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 205200.0000, 
sim time next is 206400.0000, 
raw observation next is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 22.5, 23.31776038259672, -0.1642410079362181, 1.0, 1.0, 50.0, 51.859993701585225], 
processed observation next is [1.0, 0.391304347826087, 0.24007386888273316, 0.77, 0.23833333333333337, 0.05451197053406997, 0.375, 0.4431466985497267, 0.445252997354594, 1.0, 1.0, 0.7, 0.5185999370158523], 
reward next is 0.4814, 
noisyNet noise sample is [array([-0.14219019], dtype=float32), -0.3648985]. 
=============================================
[2019-04-10 14:19:26,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0441786e-12 2.0219023e-11 2.0061916e-10 2.5340668e-10 2.7263019e-09
 6.2831420e-14 1.7384308e-14 1.0000000e+00 2.8029518e-13 7.5874315e-15
 4.8286203e-10], sum to 1.0000
[2019-04-10 14:19:26,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3240
[2019-04-10 14:19:26,577] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.9, 64.33333333333334, 93.66666666666667, 404.5, 22.5, 23.68258539757493, -0.05762937743612557, 1.0, 1.0, 50.0, 42.76110345291626], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 294000.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 22.5, 23.83798671008101, -0.02855898116107484, 1.0, 1.0, 50.0, 42.523132342510074], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.375, 0.486498892506751, 0.49048033961297505, 1.0, 1.0, 0.7, 0.4252313234251007], 
reward next is 0.5748, 
noisyNet noise sample is [array([0.3320645], dtype=float32), 0.6951982]. 
=============================================
[2019-04-10 14:19:40,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6822767e-12 1.2656548e-12 6.1743094e-10 8.5524843e-10 1.3948625e-08
 1.0831290e-13 2.0148234e-14 1.0000000e+00 2.8033102e-13 1.7916373e-15
 1.4193877e-11], sum to 1.0000
[2019-04-10 14:19:40,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5711
[2019-04-10 14:19:40,393] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 87.0, 0.0, 0.0, 19.0, 23.45093970714249, -0.05495796682191054, 0.0, 1.0, 50.0, 40.34435515933649], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 604800.0000, 
sim time next is 606000.0000, 
raw observation next is [-3.566666666666667, 86.66666666666667, 0.0, 0.0, 19.0, 23.39197536678349, -0.06767874148536492, 0.0, 1.0, 50.0, 40.34895723104839], 
processed observation next is [0.0, 0.0, 0.3638042474607572, 0.8666666666666667, 0.0, 0.0, 0.08333333333333333, 0.4493312805652909, 0.4774404195048783, 0.0, 1.0, 0.7, 0.4034895723104839], 
reward next is 0.5965, 
noisyNet noise sample is [array([0.8486161], dtype=float32), -0.66225255]. 
=============================================
[2019-04-10 14:19:40,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[55.026672]
 [55.01907 ]
 [55.03117 ]
 [55.054325]
 [55.11039 ]], R is [[55.05150223]
 [55.09754562]
 [55.14290619]
 [55.18740463]
 [55.23059464]].
[2019-04-10 14:19:42,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7899610e-12 1.6806584e-12 4.9470517e-10 5.4466065e-10 1.3648309e-09
 8.8259376e-14 7.0384287e-16 1.0000000e+00 1.2241850e-13 7.1500862e-16
 4.1308662e-12], sum to 1.0000
[2019-04-10 14:19:42,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3721
[2019-04-10 14:19:42,715] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 19.0, 24.37964140349718, 0.2234399375341677, 0.0, 1.0, 50.0, 37.74709682931839], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 564000.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 19.0, 24.46012313430592, 0.232599549177807, 0.0, 1.0, 50.0, 37.61105592193355], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 0.08333333333333333, 0.5383435945254934, 0.577533183059269, 0.0, 1.0, 0.7, 0.3761105592193355], 
reward next is 0.6239, 
noisyNet noise sample is [array([0.20684752], dtype=float32), 0.87157637]. 
=============================================
[2019-04-10 14:19:43,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8716952e-11 2.5815220e-12 1.1953138e-09 3.3766345e-10 2.4946111e-08
 1.8865747e-13 6.4725176e-15 1.0000000e+00 3.8434769e-13 1.7595662e-15
 2.9767435e-11], sum to 1.0000
[2019-04-10 14:19:43,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8321
[2019-04-10 14:19:43,452] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 23.97989302922189, 0.07994860250304558, 0.0, 1.0, 50.0, 39.8410400162516], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 590400.0000, 
sim time next is 591600.0000, 
raw observation next is [-2.8, 85.66666666666667, 0.0, 0.0, 19.0, 23.9263689560264, 0.06524817013488941, 0.0, 1.0, 50.0, 39.93249858949319], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.49386407966886675, 0.5217493900449631, 0.0, 1.0, 0.7, 0.3993249858949319], 
reward next is 0.6007, 
noisyNet noise sample is [array([0.9167074], dtype=float32), -1.1975507]. 
=============================================
[2019-04-10 14:19:48,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9998770e-13 1.9837993e-13 5.1253037e-11 3.2805130e-11 2.6587860e-10
 1.7182552e-15 6.7870971e-16 1.0000000e+00 1.3375514e-14 3.0518279e-17
 8.0917434e-13], sum to 1.0000
[2019-04-10 14:19:48,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2980
[2019-04-10 14:19:48,910] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.38042231056374, -0.38063616534869, 0.0, 1.0, 50.0, 40.73209154770058], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 715200.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.31894139278934, -0.400878724561996, 0.0, 1.0, 50.0, 40.799808114252755], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.3599117827324451, 0.3663737584793347, 0.0, 1.0, 0.7, 0.40799808114252756], 
reward next is 0.5920, 
noisyNet noise sample is [array([1.2592838], dtype=float32), 1.0970415]. 
=============================================
[2019-04-10 14:19:50,111] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.36617544e-15 3.43116919e-14 6.27261968e-13 1.39916301e-12
 1.23170675e-11 8.49725189e-17 9.25313681e-18 1.00000000e+00
 2.79088456e-17 8.33991928e-19 4.59036346e-13], sum to 1.0000
[2019-04-10 14:19:50,111] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8645
[2019-04-10 14:19:50,159] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 22.5, 24.64557415260371, 0.2307984781897147, 1.0, 1.0, 50.0, 35.726751777533856], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 913200.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 22.5, 24.81579273127642, 0.2507644303563071, 1.0, 1.0, 50.0, 34.91757475234993], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 0.375, 0.5679827276063684, 0.5835881434521023, 1.0, 1.0, 0.7, 0.3491757475234993], 
reward next is 0.6508, 
noisyNet noise sample is [array([-0.7973349], dtype=float32), -0.5438392]. 
=============================================
[2019-04-10 14:19:51,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.59211223e-14 1.94630401e-14 2.48788368e-11 7.87964572e-12
 4.24640295e-10 5.08506542e-16 1.20934815e-17 1.00000000e+00
 1.04124066e-15 1.57083170e-18 1.92696431e-13], sum to 1.0000
[2019-04-10 14:19:51,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2412
[2019-04-10 14:19:51,079] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 19.0, 22.70705089671491, -0.2586224694821553, 0.0, 1.0, 50.0, 37.88928821077444], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 879600.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 19.0, 22.61771991481682, -0.2704562596317562, 0.0, 1.0, 50.0, 37.93052187088466], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.3848099929014017, 0.4098479134560813, 0.0, 1.0, 0.7, 0.3793052187088466], 
reward next is 0.6207, 
noisyNet noise sample is [array([-0.7952951], dtype=float32), -2.0049112]. 
=============================================
[2019-04-10 14:19:51,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3384840e-15 1.6123306e-14 2.7531835e-12 1.4960236e-12 1.1890231e-11
 4.2116950e-17 1.1207920e-17 1.0000000e+00 9.5319840e-17 5.1569459e-19
 9.9107322e-13], sum to 1.0000
[2019-04-10 14:19:51,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7988
[2019-04-10 14:19:51,440] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 22.5, 23.97164095419225, 0.02062981883098612, 1.0, 1.0, 50.0, 38.00795767928521], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 830400.0000, 
sim time next is 831600.0000, 
raw observation next is [-3.9, 86.0, 54.0, 0.0, 22.5, 24.00197750064709, 0.03466491317702115, 1.0, 1.0, 50.0, 37.60832021538762], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.86, 0.18, 0.0, 0.375, 0.5001647917205908, 0.511554971059007, 1.0, 1.0, 0.7, 0.3760832021538762], 
reward next is 0.6239, 
noisyNet noise sample is [array([-0.21895127], dtype=float32), 0.089705944]. 
=============================================
[2019-04-10 14:19:51,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9303513e-14 7.3230843e-15 1.2093380e-11 7.7713010e-12 6.9458700e-10
 1.2786383e-16 4.3271102e-17 1.0000000e+00 2.1189057e-16 9.5435084e-19
 9.1693166e-14], sum to 1.0000
[2019-04-10 14:19:51,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0828
[2019-04-10 14:19:51,952] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 19.0, 22.58310064709183, -0.2392073294243717, 0.0, 1.0, 50.0, 38.29505612878101], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 866400.0000, 
sim time next is 867600.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 22.63989681950227, -0.2407966178626514, 0.0, 1.0, 50.0, 38.19842223763341], 
processed observation next is [1.0, 0.043478260869565216, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.3866580682918557, 0.4197344607124495, 0.0, 1.0, 0.7, 0.3819842223763341], 
reward next is 0.6180, 
noisyNet noise sample is [array([0.17005727], dtype=float32), -0.66897476]. 
=============================================
[2019-04-10 14:19:52,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6864544e-14 3.4926089e-14 4.2626935e-12 2.7947323e-12 3.1762183e-11
 7.3096561e-16 3.6667993e-17 1.0000000e+00 1.4973973e-15 1.9608704e-17
 1.4046231e-12], sum to 1.0000
[2019-04-10 14:19:52,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1794
[2019-04-10 14:19:52,301] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 22.71984155646805, -0.2010337677146791, 0.0, 1.0, 50.0, 39.484310884934146], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 853200.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 22.64622354089929, -0.214603132602691, 0.0, 1.0, 50.0, 39.51857510420608], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.38718529507494076, 0.4284656224657697, 0.0, 1.0, 0.7, 0.3951857510420608], 
reward next is 0.6048, 
noisyNet noise sample is [array([-0.614926], dtype=float32), -0.24135962]. 
=============================================
[2019-04-10 14:19:52,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2508836e-15 9.9875719e-16 1.1258034e-13 8.4192947e-14 3.8939802e-12
 2.6560079e-18 1.1251866e-18 1.0000000e+00 1.0178295e-17 3.2859382e-21
 6.9383577e-15], sum to 1.0000
[2019-04-10 14:19:52,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7125
[2019-04-10 14:19:52,992] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.4, 81.0, 75.5, 0.0, 22.5, 25.49341178218163, 0.7456093646587044, 1.0, 1.0, 50.0, 29.58641333064671], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1004400.0000, 
sim time next is 1005600.0000, 
raw observation next is [14.76666666666667, 79.0, 63.16666666666666, 0.0, 22.5, 27.33632088371615, 0.9014260185686059, 1.0, 1.0, 50.0, 24.582972330508575], 
processed observation next is [1.0, 0.6521739130434783, 0.8716528162511544, 0.79, 0.21055555555555552, 0.0, 0.375, 0.7780267403096793, 0.8004753395228686, 1.0, 1.0, 0.7, 0.24582972330508576], 
reward next is 0.7542, 
noisyNet noise sample is [array([-0.68197906], dtype=float32), -0.4800262]. 
=============================================
[2019-04-10 14:19:57,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3879498e-17 2.1795137e-16 3.8519514e-14 1.1291302e-13 1.5197246e-12
 6.0876192e-19 4.2710036e-19 1.0000000e+00 1.7286072e-18 6.8444615e-21
 7.4606289e-15], sum to 1.0000
[2019-04-10 14:19:57,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5911
[2019-04-10 14:19:57,358] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [15.13333333333333, 79.0, 0.0, 0.0, 22.5, 27.88769219427738, 0.9527702376568771, 1.0, 1.0, 50.0, 25.948855549380234], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1012800.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 22.5, 27.65732225683207, 0.962188194202814, 1.0, 1.0, 50.0, 28.40715549985676], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 0.375, 0.8047768547360059, 0.8207293980676047, 1.0, 1.0, 0.7, 0.2840715549985676], 
reward next is 0.7159, 
noisyNet noise sample is [array([-1.0008979], dtype=float32), -0.4182963]. 
=============================================
[2019-04-10 14:19:57,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.46928]
 [72.32979]
 [72.3353 ]
 [72.47168]
 [72.45964]], R is [[72.63973999]
 [72.65385437]
 [72.71076965]
 [72.7411499 ]
 [72.69108582]].
[2019-04-10 14:19:57,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8739899e-17 1.6506965e-16 3.2973641e-14 5.4941810e-15 2.0660587e-12
 1.8488870e-19 3.2607292e-20 1.0000000e+00 2.1278043e-19 3.8980123e-21
 7.8786719e-15], sum to 1.0000
[2019-04-10 14:19:57,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2275
[2019-04-10 14:19:57,773] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.53333333333333, 86.0, 126.5, 0.0, 22.5, 27.16271982827965, 0.8394414061331581, 1.0, 1.0, 50.0, 30.952712945086255], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 996000.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 22.5, 27.17900478583696, 0.6086808543244402, 1.0, 1.0, 50.0, 32.323765597698454], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.375, 0.7649170654864133, 0.7028936181081468, 1.0, 1.0, 0.7, 0.32323765597698456], 
reward next is 0.6768, 
noisyNet noise sample is [array([0.96531093], dtype=float32), 1.0107526]. 
=============================================
[2019-04-10 14:19:57,806] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4739317e-16 4.1420377e-16 9.4875106e-13 3.9441351e-13 3.1015763e-11
 3.2019975e-18 2.2224794e-20 1.0000000e+00 1.1104717e-17 1.3682952e-20
 2.9122551e-15], sum to 1.0000
[2019-04-10 14:19:57,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7692
[2019-04-10 14:19:57,834] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [13.1, 62.66666666666667, 0.0, 0.0, 19.0, 27.90390368142024, 1.201513919063127, 0.0, 1.0, 50.0, 25.17313501317943], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1113600.0000, 
sim time next is 1114800.0000, 
raw observation next is [12.9, 63.33333333333333, 0.0, 0.0, 19.0, 27.89145996573117, 1.196314926552173, 0.0, 1.0, 50.0, 25.179573860780103], 
processed observation next is [1.0, 0.9130434782608695, 0.8199445983379503, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.8242883304775974, 0.8987716421840576, 0.0, 1.0, 0.7, 0.251795738607801], 
reward next is 0.7482, 
noisyNet noise sample is [array([-0.30565506], dtype=float32), 1.7046958]. 
=============================================
[2019-04-10 14:20:00,744] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6562499e-14 5.5657984e-15 4.4684170e-11 5.9879137e-12 1.1541159e-09
 5.4418235e-16 4.8357715e-18 1.0000000e+00 5.7921939e-16 3.8690393e-19
 3.0211141e-14], sum to 1.0000
[2019-04-10 14:20:00,745] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1878
[2019-04-10 14:20:00,781] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 19.0, 27.63968412209282, 1.094681351097226, 0.0, 1.0, 50.0, 28.69608800941254], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1142400.0000, 
sim time next is 1143600.0000, 
raw observation next is [11.6, 81.0, 0.0, 0.0, 19.0, 27.62230878378733, 1.091347229502343, 0.0, 1.0, 50.0, 29.75742865795823], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.81, 0.0, 0.0, 0.08333333333333333, 0.8018590653156107, 0.8637824098341144, 0.0, 1.0, 0.7, 0.2975742865795823], 
reward next is 0.7024, 
noisyNet noise sample is [array([-0.2870579], dtype=float32), 0.7628425]. 
=============================================
[2019-04-10 14:20:08,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5077046e-13 8.0669290e-14 2.8385006e-11 3.7004770e-12 5.4570670e-10
 1.2164702e-15 4.2435157e-16 1.0000000e+00 2.7901314e-15 2.4248087e-17
 1.4933275e-11], sum to 1.0000
[2019-04-10 14:20:08,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5773
[2019-04-10 14:20:08,110] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 25.93329423711443, 0.6558946076284173, 0.0, 1.0, 50.0, 35.43559306884289], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1402800.0000, 
sim time next is 1404000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 25.85563262867596, 0.6471361841582391, 0.0, 1.0, 50.0, 35.49068644753006], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6546360523896633, 0.7157120613860797, 0.0, 1.0, 0.7, 0.3549068644753006], 
reward next is 0.6451, 
noisyNet noise sample is [array([0.34081995], dtype=float32), 1.4954665]. 
=============================================
[2019-04-10 14:20:08,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.922096]
 [63.02886 ]
 [63.115234]
 [63.207996]
 [63.27307 ]], R is [[62.82045364]
 [62.83789444]
 [62.85454178]
 [62.87093735]
 [62.8864975 ]].
[2019-04-10 14:20:09,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2914885e-15 4.0457190e-15 3.2081312e-13 2.1351164e-13 6.2849582e-12
 2.2454405e-17 2.8635481e-18 1.0000000e+00 1.6484555e-17 4.0696565e-19
 5.4311720e-13], sum to 1.0000
[2019-04-10 14:20:09,839] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5626
[2019-04-10 14:20:09,880] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.3, 96.0, 80.5, 354.0, 22.5, 27.27955736991029, 0.8792840817949746, 1.0, 1.0, 50.0, 26.776026763112988], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1508400.0000, 
sim time next is 1509600.0000, 
raw observation next is [3.666666666666667, 95.0, 85.5, 590.0, 22.5, 27.53760393565311, 0.9390339233582234, 1.0, 1.0, 50.0, 23.788048112087402], 
processed observation next is [1.0, 0.4782608695652174, 0.564173591874423, 0.95, 0.285, 0.6519337016574586, 0.375, 0.7948003279710925, 0.8130113077860744, 1.0, 1.0, 0.7, 0.237880481120874], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.13024822], dtype=float32), -0.8520491]. 
=============================================
[2019-04-10 14:20:13,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0276224e-14 6.6741317e-15 2.7451450e-12 2.5441231e-12 5.1943016e-11
 4.3356526e-17 4.6509417e-18 1.0000000e+00 1.0742721e-16 1.0793358e-18
 4.0576630e-13], sum to 1.0000
[2019-04-10 14:20:13,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3811
[2019-04-10 14:20:13,182] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.166666666666666, 93.66666666666666, 0.0, 0.0, 19.0, 27.3548685252358, 0.9766874457494739, 0.0, 1.0, 50.0, 33.55317217019169], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1665600.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 19.0, 27.36227735488462, 0.9730597812403774, 0.0, 1.0, 50.0, 32.64416901682994], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7801897795737182, 0.8243532604134591, 0.0, 1.0, 0.7, 0.32644169016829944], 
reward next is 0.6736, 
noisyNet noise sample is [array([-0.4265639], dtype=float32), 0.56104165]. 
=============================================
[2019-04-10 14:20:15,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1118057e-11 3.8497684e-11 3.9311345e-09 1.4610108e-09 6.9042962e-08
 1.0635583e-12 2.1195139e-14 9.9999988e-01 5.8537539e-13 2.9155132e-14
 3.7039389e-11], sum to 1.0000
[2019-04-10 14:20:15,211] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0335
[2019-04-10 14:20:15,248] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.95845798071793, 0.1167861628333422, 0.0, 1.0, 50.0, 43.383715641204276], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1806000.0000, 
sim time next is 1807200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.92599983192939, 0.09941720686528023, 0.0, 1.0, 50.0, 43.383574404642715], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.49383331932744906, 0.5331390689550934, 0.0, 1.0, 0.7, 0.43383574404642716], 
reward next is 0.5662, 
noisyNet noise sample is [array([-0.6175679], dtype=float32), 1.43621]. 
=============================================
[2019-04-10 14:20:17,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3533490e-10 3.0038594e-10 6.3106519e-08 3.0310272e-08 1.0248749e-07
 1.8887366e-11 3.0076113e-12 9.9999988e-01 9.0418844e-12 6.8265353e-13
 3.2041867e-09], sum to 1.0000
[2019-04-10 14:20:17,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1588
[2019-04-10 14:20:17,061] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.700000000000001, 78.0, 22.66666666666667, 0.0, 19.0, 22.51338329081348, -0.2400067873514206, 0.0, 1.0, 50.0, 44.7569907893992], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1844400.0000, 
sim time next is 1845600.0000, 
raw observation next is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 19.0, 22.4821822027432, -0.2430907270730808, 0.0, 1.0, 50.0, 44.709740263059174], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.15722222222222218, 0.017311233885819514, 0.08333333333333333, 0.3735151835619333, 0.4189697576423064, 0.0, 1.0, 0.7, 0.4470974026305917], 
reward next is 0.5529, 
noisyNet noise sample is [array([0.67240274], dtype=float32), -1.1297536]. 
=============================================
[2019-04-10 14:20:17,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3948353e-11 2.5203615e-11 3.6082586e-09 1.0709004e-09 1.1318564e-08
 1.3117073e-12 4.4602526e-14 1.0000000e+00 3.9017921e-13 9.5142346e-14
 3.9057368e-10], sum to 1.0000
[2019-04-10 14:20:17,561] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9463
[2019-04-10 14:20:17,609] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.833333333333334, 71.0, 130.6666666666667, 13.5, 19.0, 22.73048792489372, -0.1882770803266942, 0.0, 1.0, 50.0, 42.841531484573835], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1858800.0000, 
sim time next is 1860000.0000, 
raw observation next is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 19.0, 22.73895478819593, -0.1888583633898806, 0.0, 1.0, 50.0, 42.83414085250562], 
processed observation next is [0.0, 0.5217391304347826, 0.3333333333333333, 0.71, 0.42777777777777765, 0.00736648250460405, 0.08333333333333333, 0.3949128990163275, 0.4370472122033731, 0.0, 1.0, 0.7, 0.4283414085250562], 
reward next is 0.5717, 
noisyNet noise sample is [array([-0.6777574], dtype=float32), -0.06671911]. 
=============================================
[2019-04-10 14:20:17,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[45.41945 ]
 [45.610237]
 [45.90492 ]
 [45.9842  ]
 [45.76016 ]], R is [[45.61371613]
 [45.72916412]
 [45.84263611]
 [45.95341492]
 [46.06097412]].
[2019-04-10 14:20:20,893] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7959749e-11 7.2814792e-12 2.6275611e-09 2.3350897e-09 5.4367412e-08
 1.2176397e-12 6.7378835e-14 1.0000000e+00 5.9754746e-13 4.0780899e-14
 2.3596236e-10], sum to 1.0000
[2019-04-10 14:20:20,893] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3062
[2019-04-10 14:20:20,932] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.733333333333333, 83.66666666666667, 23.5, 0.0, 19.0, 24.55608973936884, 0.2765963335912696, 0.0, 1.0, 50.0, 42.17139132731121], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1788000.0000, 
sim time next is 1789200.0000, 
raw observation next is [-3.9, 82.0, 14.5, 0.0, 19.0, 24.4960287748996, 0.2623333050200095, 0.0, 1.0, 50.0, 42.267459104097696], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.04833333333333333, 0.0, 0.08333333333333333, 0.5413357312416333, 0.5874444350066699, 0.0, 1.0, 0.7, 0.422674591040977], 
reward next is 0.5773, 
noisyNet noise sample is [array([1.0746742], dtype=float32), -0.6381805]. 
=============================================
[2019-04-10 14:20:21,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1580507e-11 1.3396848e-12 5.1966426e-10 3.4910708e-10 5.3795821e-09
 1.0300849e-13 1.3847797e-14 1.0000000e+00 9.4560888e-14 3.0335039e-15
 1.3495960e-10], sum to 1.0000
[2019-04-10 14:20:21,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2823
[2019-04-10 14:20:21,761] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 22.02768195363405, -0.4421983538395954, 0.0, 1.0, 50.0, 43.62139407274826], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1915200.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 21.94041302574865, -0.4510667929322359, 0.0, 1.0, 50.0, 43.813431885026205], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.3283677521457209, 0.34964440235592137, 0.0, 1.0, 0.7, 0.43813431885026205], 
reward next is 0.5619, 
noisyNet noise sample is [array([-0.74088335], dtype=float32), -0.9892536]. 
=============================================
[2019-04-10 14:20:25,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0931106e-15 1.3137784e-15 7.0805579e-13 3.8309251e-13 3.3071176e-12
 1.2860781e-17 3.2335421e-19 1.0000000e+00 2.0755763e-17 2.0124754e-19
 8.0686412e-13], sum to 1.0000
[2019-04-10 14:20:25,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6569
[2019-04-10 14:20:25,399] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.199999999999999, 68.33333333333333, 231.1666666666667, 9.0, 22.5, 25.53317853983176, 0.2933643727600194, 1.0, 1.0, 50.0, 46.65397235614816], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1942800.0000, 
sim time next is 1944000.0000, 
raw observation next is [-5.0, 65.0, 229.5, 7.0, 22.5, 25.74348809462411, 0.3412046197088536, 1.0, 1.0, 50.0, 46.53648858227339], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.765, 0.0077348066298342545, 0.375, 0.6452906745520091, 0.6137348732362845, 1.0, 1.0, 0.7, 0.46536488582273394], 
reward next is 0.5346, 
noisyNet noise sample is [array([-0.8065861], dtype=float32), -0.443059]. 
=============================================
[2019-04-10 14:20:25,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.12673 ]
 [68.55033 ]
 [67.77515 ]
 [66.946365]
 [66.15983 ]], R is [[69.27145386]
 [69.11220551]
 [68.95355225]
 [68.79396057]
 [68.63324738]].
[2019-04-10 14:20:30,472] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7788540e-12 7.4878656e-13 6.3493384e-11 4.1936642e-11 6.9813327e-10
 3.0106892e-15 6.6672293e-17 1.0000000e+00 2.3642081e-15 8.1405518e-17
 5.3993312e-12], sum to 1.0000
[2019-04-10 14:20:30,473] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4467
[2019-04-10 14:20:30,503] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 24.23743227840521, 0.07339661625496074, 1.0, 1.0, 50.0, 38.566680572327414], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2013600.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 24.14654623826669, 0.05483831370150628, 1.0, 1.0, 50.0, 38.73332425072094], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.5122121865222242, 0.5182794379005021, 1.0, 1.0, 0.7, 0.3873332425072094], 
reward next is 0.6127, 
noisyNet noise sample is [array([0.81208336], dtype=float32), -2.2844412]. 
=============================================
[2019-04-10 14:20:30,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5715520e-14 1.1297528e-13 8.5898805e-12 1.0982958e-11 2.4008498e-10
 4.1090516e-16 2.8480910e-17 1.0000000e+00 2.3816772e-16 2.1302023e-17
 6.3307007e-12], sum to 1.0000
[2019-04-10 14:20:30,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2894
[2019-04-10 14:20:30,566] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.6, 83.0, 85.5, 0.0, 22.5, 25.1468233696284, 0.2327690547231707, 1.0, 1.0, 50.0, 39.65525420220092], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2023200.0000, 
sim time next is 2024400.0000, 
raw observation next is [-5.6, 83.0, 96.5, 0.0, 22.5, 25.20471534330286, 0.2489964780692848, 1.0, 1.0, 50.0, 39.735242799121906], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.32166666666666666, 0.0, 0.375, 0.6003929452752382, 0.5829988260230949, 1.0, 1.0, 0.7, 0.39735242799121906], 
reward next is 0.6026, 
noisyNet noise sample is [array([1.0731957], dtype=float32), -0.53663903]. 
=============================================
[2019-04-10 14:20:30,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1413654e-12 1.4272435e-12 7.4776255e-11 3.8045764e-11 5.0884990e-09
 6.1943583e-15 1.0593094e-15 1.0000000e+00 7.9601183e-15 1.5909026e-16
 2.1732926e-11], sum to 1.0000
[2019-04-10 14:20:30,717] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2817
[2019-04-10 14:20:30,758] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.0, 23.45748649425495, -0.004664261062449844, 0.0, 1.0, 50.0, 40.655609818998855], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2077200.0000, 
sim time next is 2078400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 23.46003482221564, -0.01218102658502694, 0.0, 1.0, 50.0, 40.69200015489011], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.08333333333333333, 0.4550029018513033, 0.49593965780499105, 0.0, 1.0, 0.7, 0.4069200015489011], 
reward next is 0.5931, 
noisyNet noise sample is [array([-0.5841346], dtype=float32), -0.7264955]. 
=============================================
[2019-04-10 14:20:34,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.1069445e-13 6.2220933e-14 5.6441067e-11 5.8894299e-11 1.4503936e-09
 1.5897871e-15 5.5327938e-16 1.0000000e+00 9.9936314e-16 9.0136574e-17
 3.1646064e-12], sum to 1.0000
[2019-04-10 14:20:34,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-10 14:20:34,532] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.9, 77.33333333333334, 0.0, 0.0, 19.0, 22.62574658861215, -0.2109751711839722, 0.0, 1.0, 50.0, 42.39717868144064], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2251200.0000, 
sim time next is 2252400.0000, 
raw observation next is [-7.1, 79.66666666666667, 0.0, 0.0, 19.0, 22.61031467978242, -0.2235479949232443, 0.0, 1.0, 50.0, 42.338437868630464], 
processed observation next is [1.0, 0.043478260869565216, 0.2659279778393352, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.3841928899818683, 0.4254840016922519, 0.0, 1.0, 0.7, 0.42338437868630463], 
reward next is 0.5766, 
noisyNet noise sample is [array([0.6278167], dtype=float32), 1.9850444]. 
=============================================
[2019-04-10 14:20:35,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1925115e-11 1.2036449e-11 1.6213264e-10 3.7446510e-10 3.3477656e-09
 6.3154743e-14 4.0043550e-15 1.0000000e+00 4.9394987e-14 8.5160820e-15
 2.8759287e-11], sum to 1.0000
[2019-04-10 14:20:35,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6326
[2019-04-10 14:20:35,163] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 23.03414046974054, -0.09313655651423634, 0.0, 1.0, 50.0, 40.13191878363668], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2158800.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 22.93979147929769, -0.1134935437434827, 0.0, 1.0, 50.0, 40.299732263681165], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.4116492899414741, 0.4621688187521724, 0.0, 1.0, 0.7, 0.40299732263681165], 
reward next is 0.5970, 
noisyNet noise sample is [array([-0.8454825], dtype=float32), -0.08312673]. 
=============================================
[2019-04-10 14:20:35,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.267067]
 [53.41734 ]
 [53.511063]
 [53.563248]
 [53.8847  ]], R is [[53.29774475]
 [53.3634491 ]
 [53.4303627 ]
 [53.49870682]
 [53.56953812]].
[2019-04-10 14:20:37,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0489890e-11 8.7250346e-12 9.5953512e-10 1.1022318e-09 1.2344345e-08
 6.0842573e-14 9.7909631e-15 1.0000000e+00 1.7077950e-13 3.8430406e-15
 5.0061941e-11], sum to 1.0000
[2019-04-10 14:20:37,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0881
[2019-04-10 14:20:37,547] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 19.0, 22.1110453648197, -0.3644380196192228, 0.0, 1.0, 50.0, 41.74574061296454], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2266800.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 22.0821669909631, -0.3683170554945649, 0.0, 1.0, 50.0, 41.71709123644698], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.34018058258025824, 0.37722764816847837, 0.0, 1.0, 0.7, 0.41717091236446985], 
reward next is 0.5828, 
noisyNet noise sample is [array([-1.3255149], dtype=float32), 0.28918466]. 
=============================================
[2019-04-10 14:20:37,567] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[54.457066]
 [54.57272 ]
 [54.672626]
 [54.755604]
 [54.835957]], R is [[54.39265442]
 [54.4312706 ]
 [54.46876526]
 [54.50492096]
 [54.54003525]].
[2019-04-10 14:20:45,011] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7986863e-11 8.6211767e-13 1.8590594e-09 4.4818030e-10 7.1641724e-09
 2.0415473e-13 8.2533764e-15 1.0000000e+00 5.3355158e-14 7.6218905e-16
 1.8466636e-11], sum to 1.0000
[2019-04-10 14:20:45,013] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7652
[2019-04-10 14:20:45,046] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 24.53200587894817, 0.207234566680646, 0.0, 1.0, 50.0, 38.374115281952626], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2352000.0000, 
sim time next is 2353200.0000, 
raw observation next is [-3.0, 66.33333333333333, 0.0, 0.0, 19.0, 24.49310735979887, 0.1943970141112416, 0.0, 1.0, 50.0, 38.51202332960186], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.5410922799832392, 0.5647990047037472, 0.0, 1.0, 0.7, 0.3851202332960186], 
reward next is 0.6149, 
noisyNet noise sample is [array([-1.1127391], dtype=float32), 0.03627911]. 
=============================================
[2019-04-10 14:20:45,979] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-10 14:20:45,980] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:20:45,981] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:20:45,982] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:20:45,984] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:20:45,986] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:20:45,987] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:20:45,990] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run3
[2019-04-10 14:20:46,006] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run3
[2019-04-10 14:20:46,023] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run3
[2019-04-10 14:21:54,434] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2824.7400 141975.2428 1220.7053
[2019-04-10 14:21:54,454] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:21:54,454] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:21:54,454] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:21:54,565] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:21:54,565] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:21:54,565] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:05,551] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2741.8289 150712.3196 941.3419
[2019-04-10 14:22:05,571] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:05,571] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:05,571] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:05,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:05,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:05,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:09,310] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2704.0892 154398.2722 786.3333
[2019-04-10 14:22:09,329] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:09,329] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:09,329] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:22:09,440] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:09,440] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:09,440] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:22:10,331] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 100000, evaluation results [100000.0, 2741.828868918773, 150712.31962660837, 941.3419215034918, 2824.7399698979193, 141975.2427887979, 1220.7052505585646, 2704.0891666007647, 154398.27216610115, 786.3333053506527]
[2019-04-10 14:22:12,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.1715763e-12 8.8653078e-12 1.5455535e-09 8.1959040e-10 2.1158691e-08
 4.6748597e-13 4.1659855e-14 1.0000000e+00 6.0604820e-13 3.7210534e-15
 3.6983482e-11], sum to 1.0000
[2019-04-10 14:22:12,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5778
[2019-04-10 14:22:12,274] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 19.0, 22.4830831096254, -0.3000032775864316, 0.0, 1.0, 50.0, 42.49902665894374], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2436000.0000, 
sim time next is 2437200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 19.0, 22.42093721131849, -0.3110544109996881, 0.0, 1.0, 50.0, 42.46228407501792], 
processed observation next is [0.0, 0.21739130434782608, 0.2299168975069252, 0.61, 0.0, 0.0, 0.08333333333333333, 0.3684114342765407, 0.3963151963334373, 0.0, 1.0, 0.7, 0.4246228407501792], 
reward next is 0.5754, 
noisyNet noise sample is [array([-1.5594296], dtype=float32), -0.1738519]. 
=============================================
[2019-04-10 14:22:13,026] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.71028280e-14 7.13268234e-15 1.28321026e-11 1.14460047e-11
 3.21867810e-10 2.77450809e-15 3.11789149e-16 1.00000000e+00
 6.50973090e-16 1.29921925e-17 7.35944237e-13], sum to 1.0000
[2019-04-10 14:22:13,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3773
[2019-04-10 14:22:13,058] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.7, 42.0, 0.0, 0.0, 19.0, 23.69723973488482, -0.08143113068780276, 0.0, 1.0, 50.0, 37.04274182606486], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2515200.0000, 
sim time next is 2516400.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 19.0, 23.66511066924019, -0.089091566695863, 0.0, 1.0, 50.0, 36.96527341419993], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.44, 0.0, 0.0, 0.08333333333333333, 0.47209255577001574, 0.470302811101379, 0.0, 1.0, 0.7, 0.3696527341419993], 
reward next is 0.6303, 
noisyNet noise sample is [array([0.1509414], dtype=float32), -0.43977487]. 
=============================================
[2019-04-10 14:22:14,751] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8432163e-16 2.8054360e-15 2.8996486e-13 6.6694520e-14 6.5648689e-12
 4.0219697e-18 2.3137573e-19 1.0000000e+00 1.1840722e-17 5.2902236e-20
 8.0562578e-14], sum to 1.0000
[2019-04-10 14:22:14,752] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1486
[2019-04-10 14:22:14,785] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.03333333333333333, 44.33333333333334, 215.5, 66.66666666666667, 22.5, 25.39667866194407, 0.2752749928042927, 1.0, 1.0, 50.0, 35.11703020698086], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2546400.0000, 
sim time next is 2547600.0000, 
raw observation next is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 22.5, 25.55549778220935, 0.3161581332047289, 1.0, 1.0, 50.0, 34.80945091536391], 
processed observation next is [1.0, 0.4782608695652174, 0.4773776546629733, 0.41666666666666674, 0.7655555555555558, 0.06942909760589319, 0.375, 0.6296248151841125, 0.6053860444015763, 1.0, 1.0, 0.7, 0.3480945091536391], 
reward next is 0.6519, 
noisyNet noise sample is [array([-0.12500404], dtype=float32), -0.75025785]. 
=============================================
[2019-04-10 14:22:15,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9465727e-15 2.3251748e-16 4.0507055e-12 6.1381109e-13 3.2164318e-10
 3.3430941e-17 5.9426315e-19 1.0000000e+00 2.5688577e-17 1.1061075e-19
 2.5953826e-14], sum to 1.0000
[2019-04-10 14:22:15,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5540
[2019-04-10 14:22:15,203] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.8, 54.0, 13.5, 2.999999999999999, 22.5, 23.57853116425398, -0.1309396264595579, 1.0, 1.0, 50.0, 37.27942816901381], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2533200.0000, 
sim time next is 2534400.0000, 
raw observation next is [-2.8, 54.0, 28.5, 9.0, 22.5, 23.53343128994126, -0.1378640226637454, 1.0, 1.0, 50.0, 37.27111244182498], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.54, 0.095, 0.009944751381215469, 0.375, 0.46111927416177156, 0.45404532577875156, 1.0, 1.0, 0.7, 0.37271112441824983], 
reward next is 0.6273, 
noisyNet noise sample is [array([0.15876132], dtype=float32), -0.643486]. 
=============================================
[2019-04-10 14:22:15,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8736845e-15 3.8928065e-16 5.6974161e-12 3.6765994e-12 3.3605896e-11
 3.7981189e-17 1.2818700e-18 1.0000000e+00 4.0346644e-17 1.1434023e-19
 3.2377347e-14], sum to 1.0000
[2019-04-10 14:22:15,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2185
[2019-04-10 14:22:15,870] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.2, 75.33333333333334, 0.0, 0.0, 19.0, 24.22572279404088, 0.1061645454799231, 0.0, 1.0, 50.0, 39.64881001443742], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2604000.0000, 
sim time next is 2605200.0000, 
raw observation next is [-5.4, 76.66666666666667, 0.0, 0.0, 19.0, 24.07838807024128, 0.08066488709642651, 0.0, 1.0, 50.0, 39.90219933036643], 
processed observation next is [1.0, 0.13043478260869565, 0.31301939058171746, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.5065323391867734, 0.5268882956988089, 0.0, 1.0, 0.7, 0.3990219933036643], 
reward next is 0.6010, 
noisyNet noise sample is [array([-0.32413748], dtype=float32), 1.1752272]. 
=============================================
[2019-04-10 14:22:26,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2194831e-14 3.8607726e-13 2.8836610e-12 5.2223625e-12 2.6715150e-10
 5.0558973e-16 8.2331483e-17 1.0000000e+00 2.2666903e-16 1.1963536e-17
 2.2259344e-11], sum to 1.0000
[2019-04-10 14:22:26,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2593
[2019-04-10 14:22:26,500] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.0, 28.0, 38.0, 61.0, 22.5, 26.61609433137639, 0.6142602751050394, 1.0, 1.0, 50.0, 42.16129650346106], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2826000.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 22.5, 26.72812691359758, 0.6119575406673916, 1.0, 1.0, 50.0, 40.74114901921842], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.375, 0.7273439094664651, 0.7039858468891306, 1.0, 1.0, 0.7, 0.4074114901921842], 
reward next is 0.5926, 
noisyNet noise sample is [array([1.5664243], dtype=float32), -0.064967886]. 
=============================================
[2019-04-10 14:22:27,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5469812e-11 3.6993006e-11 2.9584810e-09 1.1773322e-09 2.6119974e-08
 8.2147033e-13 4.1339047e-14 1.0000000e+00 4.4466470e-13 2.8289728e-14
 6.5398431e-10], sum to 1.0000
[2019-04-10 14:22:27,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6972
[2019-04-10 14:22:27,710] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 23.80598733191149, 0.04202163994758795, 0.0, 1.0, 50.0, 35.77423153038569], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3022800.0000, 
sim time next is 3024000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 23.74365864257922, 0.02718377476828522, 0.0, 1.0, 50.0, 35.77753956927956], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.478638220214935, 0.509061258256095, 0.0, 1.0, 0.7, 0.3577753956927956], 
reward next is 0.6422, 
noisyNet noise sample is [array([1.0120666], dtype=float32), 0.9212174]. 
=============================================
[2019-04-10 14:22:27,713] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.342926]
 [47.39356 ]
 [47.45943 ]
 [47.544945]
 [47.610325]], R is [[48.05400467]
 [48.21572113]
 [48.37543488]
 [48.53326035]
 [48.68925858]].
[2019-04-10 14:22:27,985] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5811589e-11 8.1843516e-12 4.7434456e-09 3.4536646e-10 3.1661340e-08
 6.2890079e-13 4.6386367e-14 1.0000000e+00 4.2712692e-13 8.9555726e-15
 5.2414978e-10], sum to 1.0000
[2019-04-10 14:22:27,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-10 14:22:28,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 19.0, 23.84054275224326, 0.1163099448160693, 0.0, 1.0, 50.0, 40.47337311344042], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2956800.0000, 
sim time next is 2958000.0000, 
raw observation next is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 23.80425795934032, 0.1057243270987391, 0.0, 1.0, 50.0, 40.40182806785693], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.4836881632783599, 0.5352414423662464, 0.0, 1.0, 0.7, 0.4040182806785693], 
reward next is 0.5960, 
noisyNet noise sample is [array([-1.1259036], dtype=float32), 0.87892085]. 
=============================================
[2019-04-10 14:22:28,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[50.548046]
 [50.677483]
 [50.74723 ]
 [50.7973  ]
 [50.86197 ]], R is [[50.4745369 ]
 [50.56505585]
 [50.65346146]
 [50.7402916 ]
 [50.82662964]].
[2019-04-10 14:22:29,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5651281e-11 9.7453313e-12 2.2419533e-09 1.8744756e-10 3.2765286e-08
 2.5724225e-13 5.8017060e-15 1.0000000e+00 9.7398394e-14 8.2295146e-15
 3.5284786e-10], sum to 1.0000
[2019-04-10 14:22:29,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4137
[2019-04-10 14:22:29,387] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 19.0, 24.82860785733602, 0.3105880678541773, 0.0, 1.0, 50.0, 34.971069410762865], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3000000.0000, 
sim time next is 3001200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 24.76992661298245, 0.2939970152068292, 0.0, 1.0, 50.0, 35.222658347294086], 
processed observation next is [0.0, 0.7391304347826086, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.5641605510818707, 0.5979990050689431, 0.0, 1.0, 0.7, 0.35222658347294084], 
reward next is 0.6478, 
noisyNet noise sample is [array([-1.3860253], dtype=float32), -1.9205668]. 
=============================================
[2019-04-10 14:22:35,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.14804535e-14 1.20696266e-14 2.54733873e-11 3.70848968e-12
 2.64299205e-09 7.00751310e-16 3.14230761e-17 1.00000000e+00
 9.70660979e-17 1.53782461e-18 1.68009026e-13], sum to 1.0000
[2019-04-10 14:22:35,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7237
[2019-04-10 14:22:35,708] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6666666666666667, 100.0, 0.0, 0.0, 19.0, 24.33629884185969, 0.1283431202320873, 0.0, 1.0, 50.0, 37.92875480369402], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3104400.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 24.3546866625609, 0.1347678281405122, 0.0, 1.0, 50.0, 37.858798852225064], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5295572218800751, 0.5449226093801708, 0.0, 1.0, 0.7, 0.3785879885222506], 
reward next is 0.6214, 
noisyNet noise sample is [array([-0.48132613], dtype=float32), -0.017028512]. 
=============================================
[2019-04-10 14:22:36,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1082212e-12 2.6140491e-13 6.0447189e-11 4.6761310e-11 6.6357169e-09
 8.3182989e-15 7.7728709e-16 1.0000000e+00 2.8520948e-15 2.5845860e-16
 9.6269506e-11], sum to 1.0000
[2019-04-10 14:22:36,164] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5745
[2019-04-10 14:22:36,206] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 25.54119574420795, 0.5704207961290341, 0.0, 1.0, 50.0, 37.792016469650406], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3220800.0000, 
sim time next is 3222000.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 19.0, 25.4463411186325, 0.5523716444713662, 0.0, 1.0, 50.0, 38.023311557539614], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6205284265527083, 0.6841238814904553, 0.0, 1.0, 0.7, 0.38023311557539613], 
reward next is 0.6198, 
noisyNet noise sample is [array([-0.29854232], dtype=float32), 3.2258105]. 
=============================================
[2019-04-10 14:22:36,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[57.463173]
 [57.641315]
 [57.791348]
 [57.916023]
 [58.04082 ]], R is [[57.32614517]
 [57.37496185]
 [57.42592621]
 [57.47904968]
 [57.53383255]].
[2019-04-10 14:22:36,982] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.22163685e-11 1.34653044e-11 4.20687679e-10 5.36741103e-11
 3.30362360e-09 5.39722516e-14 1.19031922e-14 1.00000000e+00
 2.49428469e-14 2.02916123e-15 6.35548503e-10], sum to 1.0000
[2019-04-10 14:22:36,982] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5496
[2019-04-10 14:22:37,013] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 24.15739701694093, 0.1617618046731707, 0.0, 1.0, 50.0, 41.26990037989647], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3306000.0000, 
sim time next is 3307200.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 23.97654852111988, 0.1318033218197783, 0.0, 1.0, 50.0, 41.41104308466268], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4980457100933234, 0.5439344406065928, 0.0, 1.0, 0.7, 0.4141104308466268], 
reward next is 0.5859, 
noisyNet noise sample is [array([-0.29535273], dtype=float32), -0.52590793]. 
=============================================
[2019-04-10 14:22:38,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3338260e-13 4.0775941e-13 2.9246577e-11 6.9915198e-12 5.8687438e-10
 3.1360241e-15 4.5602756e-16 1.0000000e+00 2.3959763e-15 2.3854235e-16
 3.4705343e-11], sum to 1.0000
[2019-04-10 14:22:38,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-10 14:22:38,654] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 19.0, 24.66373992209202, 0.2784145243081744, 0.0, 1.0, 50.0, 41.0847395442168], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3300000.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 19.0, 24.57984603686957, 0.2683490832983393, 0.0, 1.0, 50.0, 41.04074762570942], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.548320503072464, 0.5894496944327797, 0.0, 1.0, 0.7, 0.41040747625709423], 
reward next is 0.5896, 
noisyNet noise sample is [array([-1.2040035], dtype=float32), 0.14976905]. 
=============================================
[2019-04-10 14:22:38,890] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.97570735e-13 8.68862762e-13 1.04116125e-11 3.74095130e-11
 4.86929552e-10 2.36063932e-15 3.86039130e-16 1.00000000e+00
 2.16146503e-15 1.91507519e-16 1.17399441e-11], sum to 1.0000
[2019-04-10 14:22:38,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7066
[2019-04-10 14:22:38,933] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 19.0, 25.42797552266632, 0.5082445301576035, 0.0, 1.0, 50.0, 39.81237741219844], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3284400.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 19.0, 25.3540887084709, 0.4999223998789236, 0.0, 1.0, 50.0, 40.090817892883955], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6128407257059084, 0.6666407999596412, 0.0, 1.0, 0.7, 0.40090817892883956], 
reward next is 0.5991, 
noisyNet noise sample is [array([1.3894327], dtype=float32), -0.27604267]. 
=============================================
[2019-04-10 14:22:39,985] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3301180e-13 1.1918602e-13 1.2995528e-11 4.0673276e-12 2.8653862e-09
 7.6321607e-16 6.2847264e-17 1.0000000e+00 5.0912572e-16 7.8539343e-18
 7.3255126e-12], sum to 1.0000
[2019-04-10 14:22:39,985] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6157
[2019-04-10 14:22:40,018] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.04878721640233, 0.687582867535642, 0.0, 1.0, 50.0, 36.68030195573317], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3213600.0000, 
sim time next is 3214800.0000, 
raw observation next is [-2.0, 100.0, 0.0, 0.0, 19.0, 25.9125482753279, 0.6709521861457343, 0.0, 1.0, 50.0, 36.80903803713031], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6593790229439916, 0.7236507287152447, 0.0, 1.0, 0.7, 0.3680903803713031], 
reward next is 0.6319, 
noisyNet noise sample is [array([-0.5665913], dtype=float32), 0.29581288]. 
=============================================
[2019-04-10 14:22:42,675] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5061219e-13 9.2948349e-13 3.8317620e-11 2.1963370e-11 5.0569449e-10
 2.9195074e-15 3.7885706e-16 1.0000000e+00 6.2521132e-16 2.8468744e-17
 2.8679694e-11], sum to 1.0000
[2019-04-10 14:22:42,678] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1487
[2019-04-10 14:22:42,718] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 25.45080553832911, 0.459144343959679, 0.0, 1.0, 50.0, 37.12434024477868], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3366000.0000, 
sim time next is 3367200.0000, 
raw observation next is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 25.30075012225585, 0.4306348981266619, 0.0, 1.0, 50.0, 37.357375353618686], 
processed observation next is [1.0, 1.0, 0.31486611265004616, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6083958435213207, 0.6435449660422207, 0.0, 1.0, 0.7, 0.3735737535361869], 
reward next is 0.6264, 
noisyNet noise sample is [array([-0.8849583], dtype=float32), 0.8536373]. 
=============================================
[2019-04-10 14:22:44,813] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5913216e-14 2.0202636e-15 9.1548566e-12 1.5674067e-12 6.6811376e-11
 8.6148148e-17 2.0805148e-17 1.0000000e+00 2.2341209e-16 1.0961237e-18
 1.6884146e-12], sum to 1.0000
[2019-04-10 14:22:44,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9378
[2019-04-10 14:22:44,847] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 70.33333333333334, 0.0, 0.0, 19.0, 26.02744878773452, 0.5915176125245537, 0.0, 1.0, 50.0, 37.59747427048238], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3468000.0000, 
sim time next is 3469200.0000, 
raw observation next is [1.0, 68.66666666666667, 0.0, 0.0, 19.0, 26.10864390936342, 0.6060778625166693, 0.0, 1.0, 50.0, 37.7648945411277], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.6866666666666668, 0.0, 0.0, 0.08333333333333333, 0.675720325780285, 0.702025954172223, 0.0, 1.0, 0.7, 0.377648945411277], 
reward next is 0.6224, 
noisyNet noise sample is [array([-0.6969963], dtype=float32), 1.123141]. 
=============================================
[2019-04-10 14:22:45,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5506349e-13 4.6498536e-13 2.9155055e-12 4.9489622e-12 5.9696398e-10
 4.3234002e-16 1.4454250e-17 1.0000000e+00 6.7909265e-17 7.6722028e-18
 6.2496583e-12], sum to 1.0000
[2019-04-10 14:22:45,916] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3446
[2019-04-10 14:22:45,944] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 26.75716579546672, 0.8103423765936743, 0.0, 1.0, 50.0, 34.559814720918126], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3448800.0000, 
sim time next is 3450000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 26.71828062908716, 0.7528939608450225, 0.0, 1.0, 50.0, 39.46608302149167], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7265233857572634, 0.7509646536150075, 0.0, 1.0, 0.7, 0.3946608302149167], 
reward next is 0.6053, 
noisyNet noise sample is [array([1.9752992], dtype=float32), -2.6878388]. 
=============================================
[2019-04-10 14:22:45,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.955734]
 [65.04053 ]
 [65.13188 ]
 [65.19856 ]
 [65.27256 ]], R is [[64.97427368]
 [64.97893524]
 [64.97410583]
 [64.9697876 ]
 [64.96409607]].
[2019-04-10 14:22:49,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.47337221e-12 1.65226136e-13 5.53541768e-10 1.11467953e-10
 7.30406180e-10 2.81447906e-14 3.97031405e-15 1.00000000e+00
 1.17526745e-14 2.11250560e-16 9.34201067e-12], sum to 1.0000
[2019-04-10 14:22:49,287] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8190
[2019-04-10 14:22:49,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 26.57747928763592, 0.700935089268783, 0.0, 1.0, 50.0, 31.674633641881314], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3607200.0000, 
sim time next is 3608400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 26.52744857910691, 0.6817390239427321, 0.0, 1.0, 50.0, 31.9608404335738], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7106207149255758, 0.727246341314244, 0.0, 1.0, 0.7, 0.319608404335738], 
reward next is 0.6804, 
noisyNet noise sample is [array([-0.44604293], dtype=float32), -1.5037061]. 
=============================================
[2019-04-10 14:22:50,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3997890e-14 1.0462659e-14 1.1487823e-11 3.2516190e-12 4.4144972e-11
 3.6537041e-16 2.6017463e-18 1.0000000e+00 2.8074793e-16 1.4730052e-18
 1.0786306e-13], sum to 1.0000
[2019-04-10 14:22:50,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2621
[2019-04-10 14:22:50,319] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 19.0, 26.30529916130821, 0.7332631216431436, 0.0, 1.0, 50.0, 31.106794743206684], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3597600.0000, 
sim time next is 3598800.0000, 
raw observation next is [-0.3333333333333334, 42.66666666666666, 82.16666666666667, 668.3333333333333, 19.0, 26.49889432499099, 0.7583788714194698, 0.0, 1.0, 50.0, 31.090725767874115], 
processed observation next is [0.0, 0.6521739130434783, 0.4533702677747, 0.4266666666666666, 0.2738888888888889, 0.7384898710865561, 0.08333333333333333, 0.7082411937492491, 0.7527929571398233, 0.0, 1.0, 0.7, 0.31090725767874117], 
reward next is 0.6891, 
noisyNet noise sample is [array([-1.1519132], dtype=float32), -2.0205662]. 
=============================================
[2019-04-10 14:22:52,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8077768e-14 1.6749396e-16 5.4677304e-12 6.0204803e-13 1.5911442e-10
 9.5674661e-17 6.3417963e-19 1.0000000e+00 5.5577094e-17 3.4980919e-20
 1.7694415e-14], sum to 1.0000
[2019-04-10 14:22:52,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6888
[2019-04-10 14:22:52,919] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 19.0, 27.34244040792837, 0.8676319606016051, 0.0, 1.0, 50.0, 34.732603077873534], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3704400.0000, 
sim time next is 3705600.0000, 
raw observation next is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 19.0, 27.33489911880074, 0.8499597848779145, 0.0, 1.0, 50.0, 33.99577525304754], 
processed observation next is [0.0, 0.9130434782608695, 0.4995383194829178, 0.6533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7779082599000618, 0.7833199282926381, 0.0, 1.0, 0.7, 0.3399577525304754], 
reward next is 0.6600, 
noisyNet noise sample is [array([-0.8407043], dtype=float32), 0.6106233]. 
=============================================
[2019-04-10 14:22:53,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1172592e-13 1.8712221e-13 4.7331743e-11 2.5280986e-11 7.5653694e-10
 7.2067461e-15 1.7402535e-16 1.0000000e+00 1.5183461e-15 1.2216028e-17
 1.8359966e-12], sum to 1.0000
[2019-04-10 14:22:53,305] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8370
[2019-04-10 14:22:53,339] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.5510988708984, 0.6681707458859946, 0.0, 1.0, 50.0, 36.329131673209346], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3718800.0000, 
sim time next is 3720000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 26.55028853183128, 0.6654380481118943, 0.0, 1.0, 50.0, 36.625808461972326], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7125240443192734, 0.7218126827039648, 0.0, 1.0, 0.7, 0.36625808461972326], 
reward next is 0.6337, 
noisyNet noise sample is [array([0.31162438], dtype=float32), -0.4552881]. 
=============================================
[2019-04-10 14:22:53,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.39464]
 [65.43302]
 [66.40326]
 [66.94176]
 [66.93293]], R is [[63.34527969]
 [63.34853363]
 [63.34824753]
 [63.35111618]
 [63.35422134]].
[2019-04-10 14:22:54,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.2721514e-17 5.6809401e-17 4.5742706e-14 2.1143176e-14 5.0902190e-12
 1.8016478e-18 2.9905474e-20 1.0000000e+00 2.5942426e-19 1.2570848e-20
 2.2691669e-14], sum to 1.0000
[2019-04-10 14:22:54,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7988
[2019-04-10 14:22:54,882] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 22.5, 27.23129233450351, 0.9268821424021491, 1.0, 1.0, 50.0, 35.0935454421387], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3787200.0000, 
sim time next is 3788400.0000, 
raw observation next is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 27.15405823372657, 0.9112392539795672, 0.0, 1.0, 50.0, 35.88588846351886], 
processed observation next is [1.0, 0.8695652173913043, 0.3979686057248385, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7628381861438808, 0.8037464179931891, 0.0, 1.0, 0.7, 0.3588588846351886], 
reward next is 0.6411, 
noisyNet noise sample is [array([1.1475519], dtype=float32), 0.14345394]. 
=============================================
[2019-04-10 14:22:55,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3861713e-14 6.9597668e-14 1.2045635e-11 1.3633466e-11 1.6446267e-10
 2.7951050e-16 1.0841768e-17 1.0000000e+00 2.7354021e-16 2.1887098e-18
 2.8803503e-13], sum to 1.0000
[2019-04-10 14:22:55,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3565
[2019-04-10 14:22:55,122] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 25.93950809805723, 0.5003094836726368, 0.0, 1.0, 50.0, 38.313091571028195], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3734400.0000, 
sim time next is 3735600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 25.89285883732161, 0.4895422495859622, 0.0, 1.0, 50.0, 38.47241872160235], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6577382364434676, 0.6631807498619874, 0.0, 1.0, 0.7, 0.3847241872160235], 
reward next is 0.6153, 
noisyNet noise sample is [array([-0.15508422], dtype=float32), 0.9267013]. 
=============================================
[2019-04-10 14:23:01,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7636346e-13 1.5505167e-13 9.2209591e-12 7.6639042e-12 1.0308019e-10
 1.4955649e-15 2.4612951e-16 1.0000000e+00 4.3760655e-16 9.3569308e-17
 1.5526105e-11], sum to 1.0000
[2019-04-10 14:23:01,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1034
[2019-04-10 14:23:01,308] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 30.33333333333334, 0.0, 0.0, 19.0, 26.22324418694087, 0.6342810057796197, 0.0, 1.0, 50.0, 34.334233834335976], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4047600.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 19.0, 26.16741942535105, 0.6207860463741112, 0.0, 1.0, 50.0, 34.54435932926346], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.08333333333333333, 0.6806182854459207, 0.7069286821247037, 0.0, 1.0, 0.7, 0.3454435932926346], 
reward next is 0.6546, 
noisyNet noise sample is [array([-1.0040112], dtype=float32), 1.0268328]. 
=============================================
[2019-04-10 14:23:06,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0397837e-14 7.4948304e-15 4.3679604e-12 4.2133450e-12 4.5263290e-10
 3.1037818e-16 9.8249577e-18 1.0000000e+00 5.8907482e-17 3.3794042e-18
 1.2187008e-13], sum to 1.0000
[2019-04-10 14:23:06,902] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0323
[2019-04-10 14:23:06,937] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 19.0, 27.08596925905742, 0.8589934049376433, 0.0, 1.0, 50.0, 29.159016591209202], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4138800.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.06639975277309, 0.8477281333563397, 0.0, 1.0, 50.0, 29.100649974946833], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7555333127310909, 0.7825760444521133, 0.0, 1.0, 0.7, 0.29100649974946835], 
reward next is 0.7090, 
noisyNet noise sample is [array([-0.46461913], dtype=float32), -2.8163903]. 
=============================================
[2019-04-10 14:23:06,946] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.05754 ]
 [68.09251 ]
 [68.02679 ]
 [67.994545]
 [68.21173 ]], R is [[68.00476837]
 [68.03313446]
 [68.0637207 ]
 [68.09499359]
 [68.12866211]].
[2019-04-10 14:23:11,638] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0209181e-15 4.5766071e-15 4.1397850e-13 4.4989685e-13 2.6427209e-12
 6.3481658e-18 4.2653214e-19 1.0000000e+00 1.9310802e-18 1.6739213e-19
 9.1754227e-14], sum to 1.0000
[2019-04-10 14:23:11,638] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6064
[2019-04-10 14:23:11,690] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 22.5, 29.73878507321327, 1.438538395932976, 1.0, 1.0, 50.0, 0.4166584321664457], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4372800.0000, 
sim time next is 4374000.0000, 
raw observation next is [13.9, 32.0, 149.0, 314.5, 22.5, 29.21004892050724, 1.402229577317797, 1.0, 1.0, 50.0, 22.633947428469934], 
processed observation next is [1.0, 0.6521739130434783, 0.847645429362881, 0.32, 0.49666666666666665, 0.3475138121546961, 0.375, 0.9341707433756034, 0.9674098591059325, 1.0, 1.0, 0.7, 0.22633947428469933], 
reward next is 0.7737, 
noisyNet noise sample is [array([0.37631753], dtype=float32), 0.4995352]. 
=============================================
[2019-04-10 14:23:11,698] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[71.10257]
 [71.57665]
 [71.76962]
 [72.02965]
 [72.19362]], R is [[70.84716034]
 [71.13452148]
 [71.40049744]
 [71.68476105]
 [71.95641327]].
[2019-04-10 14:23:15,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4437478e-13 2.8242461e-13 3.1627673e-11 4.4630848e-11 1.2978648e-09
 1.6451968e-15 8.8805846e-17 1.0000000e+00 3.8721312e-16 8.2250499e-17
 2.8341604e-12], sum to 1.0000
[2019-04-10 14:23:15,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2985
[2019-04-10 14:23:15,642] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [9.100000000000001, 61.33333333333334, 0.0, 0.0, 19.0, 28.15650676743522, 1.154498652740568, 0.0, 1.0, 50.0, 27.050170941805447], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4400400.0000, 
sim time next is 4401600.0000, 
raw observation next is [8.8, 61.66666666666666, 0.0, 0.0, 19.0, 28.0034979771534, 1.140293129516403, 0.0, 1.0, 50.0, 27.30194912779325], 
processed observation next is [1.0, 0.9565217391304348, 0.7063711911357342, 0.6166666666666666, 0.0, 0.0, 0.08333333333333333, 0.8336248314294501, 0.8800977098388009, 0.0, 1.0, 0.7, 0.2730194912779325], 
reward next is 0.7270, 
noisyNet noise sample is [array([0.29035676], dtype=float32), -0.51451266]. 
=============================================
[2019-04-10 14:23:18,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2138765e-14 1.1262738e-13 8.3659777e-13 1.5691094e-11 7.4219355e-12
 1.4413402e-16 1.6564800e-17 1.0000000e+00 2.9669526e-16 9.9447533e-18
 9.1425500e-13], sum to 1.0000
[2019-04-10 14:23:18,809] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2534
[2019-04-10 14:23:18,847] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 49.33333333333333, 116.3333333333333, 58.66666666666667, 22.5, 27.70647539304876, 0.9584148099133238, 1.0, 1.0, 50.0, 20.673151927132544], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4551600.0000, 
sim time next is 4552800.0000, 
raw observation next is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 22.5, 28.00804356306083, 0.9981532614001907, 1.0, 1.0, 50.0, 22.1598887829215], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5066666666666666, 0.31833333333333336, 0.06777163904235728, 0.375, 0.8340036302550692, 0.8327177538000635, 1.0, 1.0, 0.7, 0.221598887829215], 
reward next is 0.7784, 
noisyNet noise sample is [array([-1.1844972], dtype=float32), 1.1177812]. 
=============================================
[2019-04-10 14:23:22,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7312342e-14 3.9987548e-14 1.4289676e-12 1.4672789e-11 2.4958589e-11
 2.2411031e-16 9.7999211e-17 1.0000000e+00 7.2727752e-16 6.0958720e-18
 1.1383210e-12], sum to 1.0000
[2019-04-10 14:23:22,152] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9584
[2019-04-10 14:23:22,211] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 22.5, 27.22115424364394, 0.824303710858644, 1.0, 1.0, 50.0, 25.43604157922161], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4614000.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 22.5, 27.47426280797249, 0.8755885206436401, 1.0, 1.0, 50.0, 24.26954443389095], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.375, 0.7895219006643742, 0.7918628402145468, 1.0, 1.0, 0.7, 0.2426954443389095], 
reward next is 0.7573, 
noisyNet noise sample is [array([-0.5526023], dtype=float32), 1.1072954]. 
=============================================
[2019-04-10 14:23:24,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6736314e-13 2.6991982e-14 1.2699652e-10 3.3471448e-10 6.2949940e-10
 5.9725970e-15 5.6145625e-17 1.0000000e+00 7.3219671e-15 2.0398539e-17
 4.2127882e-13], sum to 1.0000
[2019-04-10 14:23:24,110] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5503
[2019-04-10 14:23:24,135] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 43.0, 146.0, 813.0, 19.0, 25.50436017247686, 0.5502173691045487, 0.0, 1.0, 50.0, 32.23271153548048], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4795200.0000, 
sim time next is 4796400.0000, 
raw observation next is [1.333333333333333, 42.0, 162.0, 795.6666666666667, 19.0, 25.65310733580937, 0.5796098995355676, 0.0, 1.0, 50.0, 31.538097137721408], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.42, 0.54, 0.8791896869244936, 0.08333333333333333, 0.6377589446507809, 0.6932032998451891, 0.0, 1.0, 0.7, 0.3153809713772141], 
reward next is 0.6846, 
noisyNet noise sample is [array([-0.8548746], dtype=float32), -0.62877876]. 
=============================================
[2019-04-10 14:23:24,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1689173e-12 8.7000990e-14 2.0963002e-10 1.0562192e-10 1.4503604e-09
 2.2784032e-15 1.2117870e-16 1.0000000e+00 2.3068174e-15 1.6388818e-16
 1.9799824e-12], sum to 1.0000
[2019-04-10 14:23:24,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7919
[2019-04-10 14:23:25,014] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 25.57822885770937, 0.5435200331261985, 0.0, 1.0, 50.0, 37.619530095019286], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4755600.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 25.52936775484388, 0.5372721925640377, 0.0, 1.0, 50.0, 37.551444614311336], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6274473129036565, 0.6790907308546793, 0.0, 1.0, 0.7, 0.37551444614311336], 
reward next is 0.6245, 
noisyNet noise sample is [array([-1.8884987], dtype=float32), 1.6747414]. 
=============================================
[2019-04-10 14:23:28,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4424986e-13 1.0219776e-12 2.8733585e-10 2.2248446e-10 5.4290467e-10
 5.7382277e-15 1.0144190e-16 1.0000000e+00 3.3749647e-15 7.6779454e-17
 8.6244006e-13], sum to 1.0000
[2019-04-10 14:23:28,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5554
[2019-04-10 14:23:28,206] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.0, 37.0, 89.5, 638.0, 19.0, 26.77722859206221, 0.8254329312526462, 0.0, 1.0, 50.0, 29.616983614334202], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4809600.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 19.0, 26.86904491238145, 0.8384674584091373, 0.0, 1.0, 50.0, 29.313309035091535], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.08333333333333333, 0.7390870760317876, 0.7794891528030458, 0.0, 1.0, 0.7, 0.29313309035091534], 
reward next is 0.7069, 
noisyNet noise sample is [array([-1.2195193], dtype=float32), -0.08606411]. 
=============================================
[2019-04-10 14:23:29,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0814101e-11 2.8038891e-12 1.6060304e-09 3.3891345e-09 2.7308973e-09
 1.1759609e-13 2.9848450e-15 1.0000000e+00 2.2541475e-13 1.2679095e-15
 3.7215560e-12], sum to 1.0000
[2019-04-10 14:23:29,943] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9256
[2019-04-10 14:23:29,966] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 26.39420497263453, 0.6203087871777406, 0.0, 1.0, 50.0, 32.91787672000345], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4910400.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 19.0, 26.33265372879556, 0.6046836903335662, 0.0, 1.0, 50.0, 33.24498409446595], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 0.08333333333333333, 0.6943878107329633, 0.7015612301111886, 0.0, 1.0, 0.7, 0.33244984094465946], 
reward next is 0.6676, 
noisyNet noise sample is [array([0.48402324], dtype=float32), 0.31931078]. 
=============================================
[2019-04-10 14:23:31,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4886657e-13 1.0515361e-13 2.7339084e-10 3.1535949e-10 3.4318195e-09
 1.1756306e-14 2.2560453e-16 1.0000000e+00 8.4385988e-15 1.0625060e-16
 7.2832392e-13], sum to 1.0000
[2019-04-10 14:23:31,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6176
[2019-04-10 14:23:31,320] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.4, 52.0, 291.5, 236.0, 19.0, 25.13415700702858, 0.4049269049299504, 0.0, 1.0, 50.0, 33.55190306685681], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4878000.0000, 
sim time next is 4879200.0000, 
raw observation next is [0.0666666666666666, 50.33333333333334, 285.8333333333333, 284.0, 19.0, 25.27797497894798, 0.4341825153475279, 0.0, 1.0, 50.0, 33.08328277052988], 
processed observation next is [0.0, 0.4782608695652174, 0.46445060018467227, 0.5033333333333334, 0.9527777777777777, 0.3138121546961326, 0.08333333333333333, 0.6064979149123317, 0.6447275051158426, 0.0, 1.0, 0.7, 0.33083282770529876], 
reward next is 0.6692, 
noisyNet noise sample is [array([0.30704826], dtype=float32), 1.3352537]. 
=============================================
[2019-04-10 14:23:32,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8523403e-13 1.4526996e-13 7.1024769e-11 9.3131586e-11 4.9681947e-10
 9.7278964e-15 1.3678073e-16 1.0000000e+00 6.7637355e-15 3.5831429e-17
 5.9727624e-13], sum to 1.0000
[2019-04-10 14:23:32,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6464
[2019-04-10 14:23:32,095] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 19.0, 25.86878045018401, 0.4930757279480467, 0.0, 1.0, 50.0, 34.93929539069703], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4927200.0000, 
sim time next is 4928400.0000, 
raw observation next is [0.0, 43.0, 0.0, 0.0, 19.0, 26.03243689914518, 0.4969556939762487, 0.0, 1.0, 50.0, 34.67978870252441], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.43, 0.0, 0.0, 0.08333333333333333, 0.6693697415954315, 0.6656518979920829, 0.0, 1.0, 0.7, 0.3467978870252441], 
reward next is 0.6532, 
noisyNet noise sample is [array([0.39558727], dtype=float32), 0.3736582]. 
=============================================
[2019-04-10 14:23:32,850] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:32,974] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:33,019] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:33,096] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.09671471e-13 9.56401950e-13 4.20648169e-11 1.98104963e-10
 1.11606654e-10 2.84112723e-15 4.03529752e-16 1.00000000e+00
 1.58950222e-15 2.70954273e-16 9.65791683e-12], sum to 1.0000
[2019-04-10 14:23:33,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-10 14:23:33,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 28.13843568521737, 1.074881185594432, 0.0, 1.0, 50.0, 19.945368992599285], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4994400.0000, 
sim time next is 4995600.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 27.9589628970627, 1.05770074930557, 1.0, 1.0, 50.0, 22.733801169251006], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.8299135747552251, 0.85256691643519, 1.0, 1.0, 0.7, 0.22733801169251006], 
reward next is 0.7727, 
noisyNet noise sample is [array([-1.5368416], dtype=float32), 0.10155159]. 
=============================================
[2019-04-10 14:23:33,150] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:33,358] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:33,406] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5232343e-16 7.9357982e-16 8.5199897e-13 3.4098765e-13 7.5848345e-13
 2.3232662e-18 9.9260588e-20 1.0000000e+00 6.9948430e-19 4.6687483e-20
 3.0101511e-14], sum to 1.0000
[2019-04-10 14:23:33,406] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0331
[2019-04-10 14:23:33,448] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.666666666666666, 24.33333333333334, 122.0, 864.1666666666667, 22.5, 27.41800817589228, 0.9406759791144116, 1.0, 1.0, 50.0, 34.28416327400812], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4970400.0000, 
sim time next is 4971600.0000, 
raw observation next is [7.0, 24.0, 120.0, 862.5, 22.5, 27.4857871064148, 0.9562808766819436, 1.0, 1.0, 50.0, 10.517019225556766], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 0.24, 0.4, 0.9530386740331491, 0.375, 0.7904822588679, 0.8187602922273145, 1.0, 1.0, 0.7, 0.10517019225556766], 
reward next is 0.8948, 
noisyNet noise sample is [array([1.1452024], dtype=float32), -0.9738615]. 
=============================================
[2019-04-10 14:23:33,524] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:33,852] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:33,852] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:33,862] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run3
[2019-04-10 14:23:33,970] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:33,970] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:33,977] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run3
[2019-04-10 14:23:34,355] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:34,356] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:34,362] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run3
[2019-04-10 14:23:34,677] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:34,842] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:35,105] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:35,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3109783e-13 8.2361296e-14 1.1696456e-10 3.6131102e-11 1.8195923e-10
 2.2363643e-15 3.1803567e-16 1.0000000e+00 3.6464794e-15 5.2265844e-17
 1.2945005e-12], sum to 1.0000
[2019-04-10 14:23:35,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-10 14:23:35,167] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.17505402015657, 0.5816491402194438, 0.0, 1.0, 50.0, 33.484595799765756], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 5036400.0000, 
sim time next is 5037600.0000, 
raw observation next is [-2.666666666666667, 65.0, 49.16666666666667, 84.16666666666667, 22.5, 26.02798310170872, 0.5796707937721658, 1.0, 1.0, 50.0, 33.46947720872424], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.65, 0.16388888888888892, 0.09300184162062615, 0.375, 0.6689985918090601, 0.6932235979240553, 1.0, 1.0, 0.7, 0.3346947720872424], 
reward next is 0.6653, 
noisyNet noise sample is [array([-0.9860199], dtype=float32), 2.5837486]. 
=============================================
[2019-04-10 14:23:35,280] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:35,390] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:35,547] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:35,555] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:35,613] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:35,679] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:35,680] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:35,686] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run3
[2019-04-10 14:23:35,713] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:35,780] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:35,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:36,070] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:36,105] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:36,105] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:36,108] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run3
[2019-04-10 14:23:36,393] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:36,393] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:36,393] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:36,395] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run3
[2019-04-10 14:23:36,455] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:36,549] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:36,549] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:36,551] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:36,552] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run3
[2019-04-10 14:23:36,613] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:36,613] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:36,615] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run3
[2019-04-10 14:23:36,632] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:36,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:36,657] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:36,798] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:36,830] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:36,909] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:36,909] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:36,911] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run3
[2019-04-10 14:23:37,241] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:37,391] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:37,391] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:37,393] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run3
[2019-04-10 14:23:37,410] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:37,457] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:37,457] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:37,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run3
[2019-04-10 14:23:37,635] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:37,635] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:37,637] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run3
[2019-04-10 14:23:37,657] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:37,657] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:37,659] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run3
[2019-04-10 14:23:38,242] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:38,242] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:38,244] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run3
[2019-04-10 14:23:38,263] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:38,607] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:38,806] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:23:38,971] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-10 14:23:39,264] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:39,264] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:39,266] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run3
[2019-04-10 14:23:39,807] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:23:39,807] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:23:39,809] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run3
[2019-04-10 14:23:48,061] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7118306e-15 1.0902865e-16 1.6157301e-13 1.2382445e-13 2.1339235e-13
 9.0806399e-18 1.0336683e-18 1.0000000e+00 2.9727839e-18 2.7221987e-19
 3.8942025e-16], sum to 1.0000
[2019-04-10 14:23:48,061] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3972
[2019-04-10 14:23:48,118] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.7, 93.0, 17.5, 0.0, 19.0, 23.04107817315113, -0.1173445789427737, 0.0, 1.0, 50.0, 36.741119363472684], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 30000.0000, 
sim time next is 31200.0000, 
raw observation next is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 19.0, 23.06558248126021, -0.1116707986570416, 0.0, 1.0, 50.0, 36.781753532401495], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07944444444444443, 0.0, 0.08333333333333333, 0.42213187343835096, 0.46277640044765284, 0.0, 1.0, 0.7, 0.36781753532401495], 
reward next is 0.6322, 
noisyNet noise sample is [array([-0.1649031], dtype=float32), 1.3945351]. 
=============================================
[2019-04-10 14:23:50,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3554052e-15 7.7156265e-14 3.3867364e-13 3.6275521e-13 2.5795926e-12
 6.9744137e-17 2.9166881e-17 1.0000000e+00 7.1921283e-17 5.7300819e-18
 7.5756208e-14], sum to 1.0000
[2019-04-10 14:23:50,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3132
[2019-04-10 14:23:50,595] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.466666666666667, 65.66666666666667, 30.83333333333334, 7.5, 22.5, 22.36641540730525, -0.22734162190334, 1.0, 1.0, 50.0, 44.68384355595641], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 116400.0000, 
sim time next is 117600.0000, 
raw observation next is [-7.633333333333333, 63.33333333333333, 38.33333333333334, 7.5, 22.5, 22.87042932694152, -0.1614398548641344, 1.0, 1.0, 50.0, 44.34749613037916], 
processed observation next is [1.0, 0.34782608695652173, 0.2511542012927055, 0.6333333333333333, 0.1277777777777778, 0.008287292817679558, 0.375, 0.4058691105784599, 0.4461867150452885, 1.0, 1.0, 0.7, 0.4434749613037916], 
reward next is 0.5565, 
noisyNet noise sample is [array([-0.03044579], dtype=float32), -1.4477993]. 
=============================================
[2019-04-10 14:23:51,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4750149e-15 3.2238783e-16 1.0438486e-13 2.9097430e-13 5.9844968e-13
 4.6341550e-17 8.6539727e-18 1.0000000e+00 3.1129341e-17 5.6262017e-18
 1.0860909e-14], sum to 1.0000
[2019-04-10 14:23:51,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1071
[2019-04-10 14:23:51,877] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 23.61786219108372, 0.04104651523391286, 0.0, 1.0, 50.0, 35.76460275053329], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 48000.0000, 
sim time next is 49200.0000, 
raw observation next is [7.9, 86.0, 85.66666666666666, 0.0, 19.0, 23.65533150250233, 0.05008519407479761, 0.0, 1.0, 50.0, 35.69662967804784], 
processed observation next is [0.0, 0.5652173913043478, 0.6814404432132966, 0.86, 0.2855555555555555, 0.0, 0.08333333333333333, 0.4712776252085276, 0.5166950646915992, 0.0, 1.0, 0.7, 0.3569662967804784], 
reward next is 0.6430, 
noisyNet noise sample is [array([0.7152664], dtype=float32), 0.49660137]. 
=============================================
[2019-04-10 14:23:56,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.15347942e-15 6.68542014e-15 2.25951527e-13 3.87674837e-13
 1.05940855e-11 1.06716516e-16 1.09733396e-16 1.00000000e+00
 1.65182396e-16 7.18642340e-17 3.99802478e-13], sum to 1.0000
[2019-04-10 14:23:56,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6703
[2019-04-10 14:23:56,288] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 22.5, 21.10486527372472, -0.5475906348615659, 1.0, 1.0, 50.0, 58.13181803761218], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 199200.0000, 
sim time next is 200400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 22.5, 21.56752380567779, -0.4889986335800627, 1.0, 1.0, 50.0, 56.586696322075156], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.375, 0.2972936504731492, 0.3370004554733124, 1.0, 1.0, 0.7, 0.5658669632207516], 
reward next is 0.4341, 
noisyNet noise sample is [array([1.4513732], dtype=float32), -0.979525]. 
=============================================
[2019-04-10 14:23:59,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0820116e-16 2.5914255e-14 2.7141179e-14 8.2218778e-14 5.4300016e-13
 1.1167845e-17 2.8126771e-17 1.0000000e+00 4.8294299e-18 6.2716946e-18
 1.4976788e-13], sum to 1.0000
[2019-04-10 14:23:59,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4224
[2019-04-10 14:23:59,394] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 22.5, 26.60492162445426, 0.4231561400114452, 1.0, 1.0, 50.0, 48.55728989159742], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 232800.0000, 
sim time next is 234000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 25.5915185569803, 0.4479787172080753, 1.0, 1.0, 50.0, 48.749792243073976], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.632626546415025, 0.6493262390693585, 1.0, 1.0, 0.7, 0.48749792243073975], 
reward next is 0.5125, 
noisyNet noise sample is [array([1.018249], dtype=float32), -1.5060933]. 
=============================================
[2019-04-10 14:23:59,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[56.463696]
 [56.209724]
 [56.00766 ]
 [55.80453 ]
 [55.49891 ]], R is [[56.65014648]
 [56.59807205]
 [56.55157471]
 [56.50493622]
 [56.45795059]].
[2019-04-10 14:24:00,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3172291e-14 1.1071557e-13 2.5320918e-13 1.7777957e-12 3.6609665e-12
 1.1705637e-16 4.2047462e-16 1.0000000e+00 2.8584760e-16 1.2248699e-16
 5.5129875e-13], sum to 1.0000
[2019-04-10 14:24:00,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4614
[2019-04-10 14:24:00,552] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.1, 81.0, 0.0, 0.0, 19.0, 24.50841994927132, 0.1687445560783475, 0.0, 1.0, 50.0, 41.44806323455425], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 256800.0000, 
sim time next is 258000.0000, 
raw observation next is [-4.3, 80.0, 0.0, 0.0, 19.0, 24.41227736398175, 0.1495227579232652, 0.0, 1.0, 50.0, 41.5508651602866], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.8, 0.0, 0.0, 0.08333333333333333, 0.5343564469984793, 0.549840919307755, 0.0, 1.0, 0.7, 0.415508651602866], 
reward next is 0.5845, 
noisyNet noise sample is [array([-0.5482261], dtype=float32), 1.5941843]. 
=============================================
[2019-04-10 14:24:00,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[51.526028]
 [51.571323]
 [51.65372 ]
 [51.70228 ]
 [51.863075]], R is [[51.50942612]
 [51.57984924]
 [51.65108109]
 [51.72272491]
 [51.79474258]].
[2019-04-10 14:24:03,582] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3925349e-14 7.7540140e-14 2.1966993e-13 1.5522476e-13 5.9723099e-12
 4.8895857e-17 1.1452461e-16 1.0000000e+00 2.4469061e-16 2.9868010e-17
 7.9032389e-14], sum to 1.0000
[2019-04-10 14:24:03,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8738
[2019-04-10 14:24:03,683] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 19.95452110377813, -0.8791750014264998, 0.0, 1.0, 50.0, 47.82066401407943], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 370800.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 22.5, 19.82972122317742, -0.8417952743382875, 1.0, 1.0, 50.0, 63.32197141337755], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 0.375, 0.15247676859811823, 0.21940157522057083, 1.0, 1.0, 0.7, 0.6332197141337754], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.8301933], dtype=float32), 0.14680111]. 
=============================================
[2019-04-10 14:24:03,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[50.528316]
 [50.530598]
 [50.527645]
 [50.54086 ]
 [50.55776 ]], R is [[52.80821609]
 [52.80192947]
 [52.7978096 ]
 [52.79546356]
 [52.79486465]].
[2019-04-10 14:24:13,781] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8457453e-14 3.6956787e-13 3.4620484e-13 3.1901429e-13 5.1279701e-12
 1.4879104e-16 2.5662441e-16 1.0000000e+00 1.2567805e-16 4.2392421e-17
 3.5395412e-13], sum to 1.0000
[2019-04-10 14:24:13,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9226
[2019-04-10 14:24:13,887] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 22.5, 25.30775774879189, 0.1602714000528145, 1.0, 1.0, 50.0, 48.3484234522123], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 472800.0000, 
sim time next is 474000.0000, 
raw observation next is [-1.9, 26.0, 123.1666666666667, 0.0, 22.5, 25.41405846116827, 0.2002294706403067, 1.0, 1.0, 50.0, 47.90859385667729], 
processed observation next is [1.0, 0.4782608695652174, 0.4099722991689751, 0.26, 0.4105555555555557, 0.0, 0.375, 0.6178382050973559, 0.5667431568801022, 1.0, 1.0, 0.7, 0.47908593856677295], 
reward next is 0.5209, 
noisyNet noise sample is [array([-0.6458472], dtype=float32), -0.9980907]. 
=============================================
[2019-04-10 14:24:13,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[49.923035]
 [50.060867]
 [50.21899 ]
 [50.355587]
 [50.582333]], R is [[49.76226807]
 [49.78116226]
 [49.79491043]
 [49.80313873]
 [49.8062706 ]].
[2019-04-10 14:24:14,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0175161e-15 9.1743502e-15 3.5437430e-13 1.7018444e-13 2.3944722e-12
 4.8437308e-17 4.1415299e-17 1.0000000e+00 1.2166460e-16 3.5681389e-17
 1.4257292e-14], sum to 1.0000
[2019-04-10 14:24:14,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-10 14:24:14,907] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.866666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 21.96617722888294, -0.4650067781620286, 0.0, 1.0, 50.0, 44.63833818329569], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 452400.0000, 
sim time next is 453600.0000, 
raw observation next is [-9.5, 44.0, 0.0, 0.0, 19.0, 21.87384852398982, -0.4757704662703335, 0.0, 1.0, 50.0, 44.565184755539846], 
processed observation next is [1.0, 0.2608695652173913, 0.1994459833795014, 0.44, 0.0, 0.0, 0.08333333333333333, 0.3228207103324851, 0.3414098445765555, 0.0, 1.0, 0.7, 0.44565184755539844], 
reward next is 0.5543, 
noisyNet noise sample is [array([-1.2340571], dtype=float32), -0.28031018]. 
=============================================
[2019-04-10 14:24:20,161] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8335720e-15 2.3044076e-15 5.1851302e-13 1.4900341e-13 4.6959057e-13
 7.4716154e-17 1.7763235e-17 1.0000000e+00 2.4554138e-17 2.1338123e-18
 2.9174144e-15], sum to 1.0000
[2019-04-10 14:24:20,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2314
[2019-04-10 14:24:20,187] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.899999999999999, 67.0, 129.1666666666667, 42.5, 19.0, 22.40803458427012, -0.2843192294291982, 0.0, 1.0, 50.0, 42.535382020847194], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 639600.0000, 
sim time next is 640800.0000, 
raw observation next is [-3.9, 65.0, 117.5, 25.5, 19.0, 22.45369154965559, -0.2812451834656632, 0.0, 1.0, 50.0, 42.61781367287031], 
processed observation next is [0.0, 0.43478260869565216, 0.3545706371191136, 0.65, 0.39166666666666666, 0.0281767955801105, 0.08333333333333333, 0.3711409624712993, 0.40625160551144557, 0.0, 1.0, 0.7, 0.42617813672870314], 
reward next is 0.5738, 
noisyNet noise sample is [array([0.27643022], dtype=float32), 0.4922553]. 
=============================================
[2019-04-10 14:24:21,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0430556e-14 2.4307303e-14 4.3941212e-13 1.2577686e-12 1.5281306e-12
 4.1141804e-16 1.2148279e-16 1.0000000e+00 6.6314167e-16 1.5843860e-16
 1.7256566e-13], sum to 1.0000
[2019-04-10 14:24:21,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-10 14:24:21,699] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 19.0, 22.18828658207381, -0.381509443159602, 0.0, 1.0, 50.0, 39.93258346135169], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 691200.0000, 
sim time next is 692400.0000, 
raw observation next is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 19.0, 22.19550908872876, -0.3873034139143465, 0.0, 1.0, 50.0, 39.80949073936783], 
processed observation next is [1.0, 0.0, 0.35918744228993543, 0.7133333333333334, 0.0, 0.0, 0.08333333333333333, 0.34962575739406326, 0.37089886202855116, 0.0, 1.0, 0.7, 0.3980949073936783], 
reward next is 0.6019, 
noisyNet noise sample is [array([-1.5839134], dtype=float32), -0.80218905]. 
=============================================
[2019-04-10 14:24:24,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00240566e-16 9.05266462e-17 1.64468218e-15 3.69201164e-15
 6.17343768e-14 6.86004767e-19 1.98445928e-19 1.00000000e+00
 1.89859677e-19 2.23781927e-19 1.84666430e-14], sum to 1.0000
[2019-04-10 14:24:24,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4722
[2019-04-10 14:24:24,221] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 22.5, 25.00992458567536, 0.280840746318764, 1.0, 1.0, 50.0, 37.729462638126805], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 756000.0000, 
sim time next is 757200.0000, 
raw observation next is [-3.9, 55.0, 0.0, 0.0, 22.5, 24.24658882322158, 0.1249457324112979, 1.0, 1.0, 50.0, 38.99660431033708], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.55, 0.0, 0.0, 0.375, 0.5205490686017983, 0.5416485774704326, 1.0, 1.0, 0.7, 0.38996604310337074], 
reward next is 0.6100, 
noisyNet noise sample is [array([-0.17634894], dtype=float32), 0.16381131]. 
=============================================
[2019-04-10 14:24:27,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1639169e-18 4.2045376e-16 3.9939485e-16 9.9270631e-16 1.4974431e-15
 3.9850376e-20 4.5963661e-20 1.0000000e+00 2.8077521e-20 2.9178302e-20
 6.2729211e-16], sum to 1.0000
[2019-04-10 14:24:27,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0620
[2019-04-10 14:24:27,093] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.733333333333333, 85.0, 0.0, 0.0, 22.5, 23.1646952362255, -0.1197541495592601, 0.0, 1.0, 50.0, 39.22125488972931], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 847200.0000, 
sim time next is 848400.0000, 
raw observation next is [-3.566666666666666, 84.0, 0.0, 0.0, 22.5, 23.01371488912283, -0.1467333868252417, 1.0, 1.0, 50.0, 39.33405970323793], 
processed observation next is [1.0, 0.8260869565217391, 0.3638042474607572, 0.84, 0.0, 0.0, 0.375, 0.4178095740935692, 0.4510888710582528, 1.0, 1.0, 0.7, 0.39334059703237934], 
reward next is 0.6067, 
noisyNet noise sample is [array([1.0902822], dtype=float32), 0.33346853]. 
=============================================
[2019-04-10 14:24:28,945] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9510963e-18 3.6115073e-17 6.1154662e-16 9.1818731e-16 2.8651984e-14
 1.1870333e-19 3.9637364e-20 1.0000000e+00 1.1833891e-19 3.4511445e-20
 9.0781211e-16], sum to 1.0000
[2019-04-10 14:24:28,946] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-10 14:24:28,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9896679e-19 1.8756912e-18 5.4615800e-16 1.7385625e-15 3.2040552e-15
 8.9727163e-20 3.5214416e-20 1.0000000e+00 7.1139420e-20 3.2292753e-20
 1.4214052e-16], sum to 1.0000
[2019-04-10 14:24:28,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-10 14:24:28,991] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 22.5, 23.89628988089115, -0.03782432607100076, 1.0, 1.0, 50.0, 36.75685676359929], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 898800.0000, 
sim time next is 900000.0000, 
raw observation next is [1.1, 84.0, 62.5, 0.0, 22.5, 24.00206159041448, -0.0003217099085864733, 1.0, 1.0, 50.0, 36.605695539604646], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.20833333333333334, 0.0, 0.375, 0.5001717992012068, 0.49989276336380456, 1.0, 1.0, 0.7, 0.36605695539604644], 
reward next is 0.6339, 
noisyNet noise sample is [array([0.5961986], dtype=float32), -0.30000404]. 
=============================================
[2019-04-10 14:24:28,996] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[65.67852 ]
 [65.58911 ]
 [65.44265 ]
 [65.327324]
 [65.15378 ]], R is [[65.7137146 ]
 [65.68901062]
 [65.66358185]
 [65.63782501]
 [65.61128235]].
[2019-04-10 14:24:29,004] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 19.0, 22.57373415183393, -0.2348116494452288, 0.0, 1.0, 50.0, 39.52941891056329], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 858000.0000, 
sim time next is 859200.0000, 
raw observation next is [-3.0, 80.33333333333334, 0.0, 0.0, 19.0, 22.5426999576795, -0.2358198721680544, 0.0, 1.0, 50.0, 39.467933158550494], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.3785583298066249, 0.42139337594398185, 0.0, 1.0, 0.7, 0.39467933158550494], 
reward next is 0.6053, 
noisyNet noise sample is [array([0.06119338], dtype=float32), 0.45570073]. 
=============================================
[2019-04-10 14:24:29,068] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1379729e-17 6.9983199e-17 8.1090974e-16 8.1450598e-16 1.0886585e-14
 1.0310528e-19 8.1382727e-20 1.0000000e+00 1.3985710e-19 6.8250931e-20
 1.0285917e-15], sum to 1.0000
[2019-04-10 14:24:29,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3109
[2019-04-10 14:24:29,098] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 19.0, 22.4884135998931, -0.2586846207590202, 0.0, 1.0, 50.0, 38.97138808966258], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 862800.0000, 
sim time next is 864000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 22.5041230130675, -0.2563658829656796, 0.0, 1.0, 50.0, 38.717125495836505], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.3753435844222916, 0.4145447056781068, 0.0, 1.0, 0.7, 0.3871712549583651], 
reward next is 0.6128, 
noisyNet noise sample is [array([-2.440531], dtype=float32), 0.7355442]. 
=============================================
[2019-04-10 14:24:29,105] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.94791 ]
 [63.93291 ]
 [63.888866]
 [63.868614]
 [64.00672 ]], R is [[64.26137543]
 [64.22904968]
 [64.19508362]
 [64.16003418]
 [64.12375641]].
[2019-04-10 14:24:33,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6102322e-13 2.2109110e-13 5.2529422e-12 2.9006748e-11 2.0138275e-11
 6.5641454e-15 2.9043229e-15 1.0000000e+00 1.4768792e-14 1.2740719e-15
 3.1637114e-13], sum to 1.0000
[2019-04-10 14:24:33,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2555
[2019-04-10 14:24:33,535] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [17.9, 65.66666666666667, 0.0, 0.0, 19.0, 28.2509058653178, 1.23890345018507, 0.0, 0.0, 50.0, 19.12256111281914], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1190400.0000, 
sim time next is 1191600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.22805573881573, 1.237071149584047, 0.0, 0.0, 50.0, 20.575936676712857], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8523379782346442, 0.9123570498613489, 0.0, 0.0, 0.7, 0.20575936676712858], 
reward next is 0.7942, 
noisyNet noise sample is [array([-0.2388238], dtype=float32), -0.59068555]. 
=============================================
[2019-04-10 14:24:33,734] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-10 14:24:33,735] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:24:33,735] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:24:33,738] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run4
[2019-04-10 14:24:33,749] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:24:33,750] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:24:33,750] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:24:33,751] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:24:33,752] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run4
[2019-04-10 14:24:33,753] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run4
[2019-04-10 14:25:37,483] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([-0.14845853], dtype=float32), -0.40766805]
[2019-04-10 14:25:37,483] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-4.666666666666666, 75.0, 0.0, 0.0, 19.0, 25.40972536646326, 0.4389214211797488, 0.0, 1.0, 50.0, 40.68139052880727]
[2019-04-10 14:25:37,483] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-10 14:25:37,484] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [7.3792499e-16 6.6185535e-16 3.1902385e-14 5.0324562e-14 2.1570153e-13
 9.3699628e-18 2.6060673e-18 1.0000000e+00 9.4560690e-18 1.1332764e-18
 5.3574584e-15], sampled 0.49884036155274636
[2019-04-10 14:25:43,140] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2824.7400 141975.2428 1220.7053
[2019-04-10 14:25:43,161] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:43,161] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:43,161] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:43,161] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:43,273] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:43,273] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:43,273] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:43,273] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:53,256] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2741.8289 150712.3196 941.3419
[2019-04-10 14:25:53,277] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:53,277] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:53,277] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:53,277] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:53,386] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:53,386] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:53,386] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:53,386] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:55,775] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2704.0892 154398.2722 786.3333
[2019-04-10 14:25:55,794] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:55,794] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:55,794] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:55,794] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:25:55,905] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:55,905] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:55,905] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:55,905] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:25:56,797] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 150000, evaluation results [150000.0, 2741.828868918773, 150712.31962660837, 941.3419215034918, 2824.7399698979193, 141975.2427887979, 1220.7052505585646, 2704.0891666007647, 154398.27216610115, 786.3333053506527]
[2019-04-10 14:25:59,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5081194e-18 1.0923461e-18 1.5953382e-17 3.8128555e-16 1.1638099e-15
 1.3105565e-20 7.0116093e-22 1.0000000e+00 1.4751623e-21 1.5680214e-21
 1.3235256e-17], sum to 1.0000
[2019-04-10 14:25:59,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2326
[2019-04-10 14:25:59,803] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.566666666666666, 96.0, 0.0, 0.0, 19.0, 27.77640447802714, 1.2020312378158, 0.0, 1.0, 50.0, 28.91986685411267], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1276800.0000, 
sim time next is 1278000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 27.72935350719463, 1.193105355521618, 0.0, 1.0, 50.0, 29.767313386859684], 
processed observation next is [0.0, 0.8260869565217391, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8107794589328859, 0.8977017851738727, 0.0, 1.0, 0.7, 0.29767313386859684], 
reward next is 0.7023, 
noisyNet noise sample is [array([0.71454716], dtype=float32), 0.8795375]. 
=============================================
[2019-04-10 14:25:59,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.99595 ]
 [77.312904]
 [77.717766]
 [78.228775]
 [78.66173 ]], R is [[76.70470428]
 [76.64846039]
 [76.59990692]
 [76.58279419]
 [76.56312561]].
[2019-04-10 14:26:00,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0272837e-17 4.8103286e-17 5.1165707e-15 2.5479666e-14 7.6120445e-15
 1.1629739e-18 8.7777117e-20 1.0000000e+00 4.5622521e-19 1.8816887e-19
 2.1419850e-17], sum to 1.0000
[2019-04-10 14:26:00,270] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1155
[2019-04-10 14:26:00,285] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [16.96666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 28.25067969751478, 1.23501446730648, 0.0, 0.0, 50.0, 19.839434164874064], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1201200.0000, 
sim time next is 1202400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 19.0, 28.21346972994932, 1.232152132364433, 0.0, 0.0, 50.0, 21.67606743040958], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8511224774957767, 0.9107173774548111, 0.0, 0.0, 0.7, 0.21676067430409582], 
reward next is 0.7832, 
noisyNet noise sample is [array([0.1551092], dtype=float32), 0.63029206]. 
=============================================
[2019-04-10 14:26:01,198] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9252695e-18 6.4139088e-19 8.3871144e-17 2.7671185e-16 1.2005374e-16
 5.6110609e-21 3.3893254e-22 1.0000000e+00 1.6055306e-21 2.2337133e-22
 1.5868877e-19], sum to 1.0000
[2019-04-10 14:26:01,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1834
[2019-04-10 14:26:01,222] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 19.0, 27.85969344128114, 1.149703161464663, 0.0, 0.0, 50.0, 23.983597691701], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1160400.0000, 
sim time next is 1161600.0000, 
raw observation next is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 19.0, 27.89111020952286, 1.165721193824462, 0.0, 0.0, 50.0, 23.34547772933816], 
processed observation next is [0.0, 0.43478260869565216, 0.9593721144967682, 0.6566666666666666, 0.45, 0.0, 0.08333333333333333, 0.824259184126905, 0.8885737312748206, 0.0, 0.0, 0.7, 0.2334547772933816], 
reward next is 0.7665, 
noisyNet noise sample is [array([-0.39672002], dtype=float32), 0.68991727]. 
=============================================
[2019-04-10 14:26:01,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4376744e-21 1.7498292e-20 6.5763162e-18 7.0235668e-17 1.2207550e-17
 5.4091478e-22 2.5072757e-23 1.0000000e+00 1.0427792e-22 5.2355881e-23
 7.3386424e-20], sum to 1.0000
[2019-04-10 14:26:01,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7696
[2019-04-10 14:26:01,623] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 22.5, 28.9895981163608, 1.434873222240998, 1.0, 0.0, 50.0, 24.641833115024838], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1096800.0000, 
sim time next is 1098000.0000, 
raw observation next is [17.7, 50.0, 18.0, 1.5, 22.5, 29.0467895558887, 1.382554376776893, 1.0, 0.0, 50.0, 0.7275139201010817], 
processed observation next is [1.0, 0.7391304347826086, 0.9529085872576178, 0.5, 0.06, 0.0016574585635359116, 0.375, 0.9205657963240584, 0.9608514589256311, 1.0, 0.0, 0.7, 0.007275139201010817], 
reward next is 0.9927, 
noisyNet noise sample is [array([1.4525405], dtype=float32), -0.8016535]. 
=============================================
[2019-04-10 14:26:01,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.20092 ]
 [81.860214]
 [82.25618 ]
 [82.65491 ]
 [83.07934 ]], R is [[81.93428802]
 [81.86852264]
 [82.03151703]
 [82.19335175]
 [82.31065369]].
[2019-04-10 14:26:01,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1778887e-20 2.8412174e-20 4.5490293e-18 2.7042028e-17 8.2507871e-17
 8.0470988e-23 4.8353349e-23 1.0000000e+00 1.1886446e-22 1.5258126e-23
 1.1410331e-17], sum to 1.0000
[2019-04-10 14:26:01,683] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7590
[2019-04-10 14:26:01,716] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.9000000000000001, 92.0, 15.0, 0.0, 22.5, 26.91997196300557, 0.9564988069774979, 1.0, 1.0, 50.0, 34.31838583592316], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1326000.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 22.5, 27.50011706101133, 0.9943968902060546, 1.0, 1.0, 50.0, 29.90418282835725], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.375, 0.7916764217509442, 0.8314656300686849, 1.0, 1.0, 0.7, 0.2990418282835725], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.5390238], dtype=float32), -0.44125956]. 
=============================================
[2019-04-10 14:26:04,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8099005e-17 2.8083778e-17 7.5739801e-16 1.7754929e-14 7.2387902e-15
 3.2002132e-19 1.0253928e-19 1.0000000e+00 2.5332096e-19 1.4235029e-19
 1.1303718e-16], sum to 1.0000
[2019-04-10 14:26:04,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2646
[2019-04-10 14:26:04,642] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.466666666666667, 96.0, 0.0, 0.0, 19.0, 27.65595876398261, 1.176305921078463, 0.0, 1.0, 50.0, 31.01789967560058], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1280400.0000, 
sim time next is 1281600.0000, 
raw observation next is [6.1, 96.0, 0.0, 0.0, 19.0, 27.62408667318325, 1.168096742811215, 0.0, 1.0, 50.0, 31.596631174325445], 
processed observation next is [0.0, 0.8695652173913043, 0.6315789473684211, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8020072227652708, 0.8893655809370716, 0.0, 1.0, 0.7, 0.31596631174325446], 
reward next is 0.6840, 
noisyNet noise sample is [array([-3.0733924], dtype=float32), 0.05932623]. 
=============================================
[2019-04-10 14:26:05,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1134912e-17 1.3558029e-15 5.7359705e-16 7.2973908e-14 2.4338153e-14
 6.4102398e-19 1.1523531e-18 1.0000000e+00 4.9268025e-19 1.4055300e-18
 5.0352500e-14], sum to 1.0000
[2019-04-10 14:26:05,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4377
[2019-04-10 14:26:05,502] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.1666666666666667, 95.33333333333333, 0.0, 0.0, 19.0, 26.54073340732561, 0.8391891398830672, 0.0, 1.0, 50.0, 37.79929574444927], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1377600.0000, 
sim time next is 1378800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.42964321074786, 0.8251688329365517, 0.0, 1.0, 50.0, 37.54318919799846], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7024702675623216, 0.7750562776455173, 0.0, 1.0, 0.7, 0.3754318919799846], 
reward next is 0.6246, 
noisyNet noise sample is [array([0.93187815], dtype=float32), 0.39846697]. 
=============================================
[2019-04-10 14:26:05,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.04873374e-17 1.43077841e-16 2.53967689e-16 1.32548552e-14
 5.71785525e-14 1.23638337e-19 2.16688466e-19 1.00000000e+00
 1.54984967e-19 7.73230988e-20 3.44791622e-15], sum to 1.0000
[2019-04-10 14:26:05,737] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0136
[2019-04-10 14:26:05,746] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1607451e-17 2.1767552e-16 8.1209800e-17 5.0637171e-15 2.1922227e-15
 2.2158509e-20 3.7379436e-21 1.0000000e+00 5.9022079e-21 3.5308036e-20
 1.2491861e-15], sum to 1.0000
[2019-04-10 14:26:05,746] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-10 14:26:05,771] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 100.0, 15.0, 0.0, 22.5, 25.81777486056496, 0.6771707568011026, 1.0, 1.0, 50.0, 35.804893012391396], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1412400.0000, 
sim time next is 1413600.0000, 
raw observation next is [-0.6, 100.0, 22.66666666666666, 0.0, 22.5, 26.3753170470681, 0.7246328065233804, 1.0, 1.0, 50.0, 35.11429906437617], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.07555555555555554, 0.0, 0.375, 0.6979430872556751, 0.7415442688411268, 1.0, 1.0, 0.7, 0.3511429906437617], 
reward next is 0.6489, 
noisyNet noise sample is [array([-0.86066115], dtype=float32), -0.5898266]. 
=============================================
[2019-04-10 14:26:05,773] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 107.1666666666667, 0.0, 22.5, 27.92941269809284, 1.10714782193883, 1.0, 1.0, 50.0, 28.559002027780295], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1344000.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 22.5, 27.96984535309126, 1.112050709712929, 1.0, 1.0, 50.0, 27.543001088155965], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 0.375, 0.8308204460909382, 0.8706835699043097, 1.0, 1.0, 0.7, 0.27543001088155966], 
reward next is 0.7246, 
noisyNet noise sample is [array([-1.2952238], dtype=float32), 1.8105363]. 
=============================================
[2019-04-10 14:26:07,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6847404e-17 1.9036481e-15 3.5375246e-16 1.3110280e-14 9.6333972e-15
 6.4523143e-19 1.0614516e-18 1.0000000e+00 2.7333015e-19 1.8641416e-19
 1.5844873e-14], sum to 1.0000
[2019-04-10 14:26:07,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-10 14:26:07,619] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 22.5, 27.14768516411144, 0.8405092717982411, 1.0, 1.0, 50.0, 32.24722757264249], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1418400.0000, 
sim time next is 1419600.0000, 
raw observation next is [0.0, 95.0, 67.66666666666667, 0.0, 22.5, 27.21182902144154, 0.8487743042074651, 1.0, 1.0, 50.0, 31.913920112452296], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.22555555555555556, 0.0, 0.375, 0.7676524184534618, 0.782924768069155, 1.0, 1.0, 0.7, 0.31913920112452293], 
reward next is 0.6809, 
noisyNet noise sample is [array([-1.169559], dtype=float32), 1.932516]. 
=============================================
[2019-04-10 14:26:13,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5794196e-20 5.3125727e-19 4.9053510e-18 2.2698819e-17 4.0927021e-17
 2.8637827e-22 3.1251558e-23 1.0000000e+00 1.4021212e-22 1.5840790e-22
 4.0425810e-18], sum to 1.0000
[2019-04-10 14:26:13,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6574
[2019-04-10 14:26:13,746] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 22.5, 28.06515794443664, 1.082257244408106, 1.0, 1.0, 50.0, 28.841389031376742], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1684800.0000, 
sim time next is 1686000.0000, 
raw observation next is [1.1, 85.33333333333334, 103.0, 0.0, 22.5, 28.08465253604449, 1.077014313496827, 1.0, 1.0, 50.0, 28.46266224704776], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8533333333333334, 0.3433333333333333, 0.0, 0.375, 0.8403877113370409, 0.8590047711656089, 1.0, 1.0, 0.7, 0.2846266224704776], 
reward next is 0.7154, 
noisyNet noise sample is [array([1.187113], dtype=float32), -0.639458]. 
=============================================
[2019-04-10 14:26:13,751] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.311005]
 [74.43964 ]
 [74.58268 ]
 [74.716934]
 [74.78755 ]], R is [[74.2372818 ]
 [74.20649719]
 [74.17095184]
 [74.12833405]
 [74.08634186]].
[2019-04-10 14:26:14,633] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8074379e-16 9.1809626e-16 4.1986759e-14 8.2540894e-14 3.1666561e-14
 3.8271761e-18 3.9219404e-19 1.0000000e+00 4.3317819e-18 1.4800571e-18
 3.4239437e-15], sum to 1.0000
[2019-04-10 14:26:14,634] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2551
[2019-04-10 14:26:14,664] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 19.0, 24.96831911239158, 0.3932572475330445, 0.0, 1.0, 50.0, 41.266506690081314], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1766400.0000, 
sim time next is 1767600.0000, 
raw observation next is [-2.3, 87.0, 108.0, 0.0, 19.0, 24.92167035975854, 0.3861159279451256, 0.0, 1.0, 50.0, 41.298761708745864], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.87, 0.36, 0.0, 0.08333333333333333, 0.5768058633132117, 0.6287053093150419, 0.0, 1.0, 0.7, 0.4129876170874586], 
reward next is 0.5870, 
noisyNet noise sample is [array([-0.36616862], dtype=float32), -1.188837]. 
=============================================
[2019-04-10 14:26:17,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6676723e-14 8.6039899e-14 3.5237655e-13 1.4182450e-12 2.8955161e-12
 7.3414855e-16 1.2581812e-16 1.0000000e+00 2.7139494e-16 2.0799525e-16
 6.0277879e-13], sum to 1.0000
[2019-04-10 14:26:17,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9248
[2019-04-10 14:26:17,583] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 87.0, 81.0, 0.0, 19.0, 25.01704905243814, 0.4022550333666123, 0.0, 1.0, 50.0, 41.238457475981036], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1764000.0000, 
sim time next is 1765200.0000, 
raw observation next is [-2.3, 87.0, 91.66666666666667, 0.0, 19.0, 24.99988965035622, 0.402483071176252, 0.0, 1.0, 50.0, 41.253171369061405], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3055555555555556, 0.0, 0.08333333333333333, 0.583324137529685, 0.6341610237254173, 0.0, 1.0, 0.7, 0.41253171369061403], 
reward next is 0.5875, 
noisyNet noise sample is [array([-1.1526049], dtype=float32), 0.2766309]. 
=============================================
[2019-04-10 14:26:21,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2967264e-13 1.5533909e-13 2.7121647e-12 1.0348713e-11 7.5408421e-12
 1.7034599e-15 4.5594407e-16 1.0000000e+00 1.3048978e-15 7.0695355e-16
 6.3977513e-13], sum to 1.0000
[2019-04-10 14:26:21,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0312
[2019-04-10 14:26:21,622] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.333333333333334, 78.0, 127.8333333333333, 78.33333333333334, 19.0, 22.46212661607435, -0.2279300372122583, 0.0, 1.0, 50.0, 44.437716498940944], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1848000.0000, 
sim time next is 1849200.0000, 
raw observation next is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 19.0, 22.49111818289952, -0.218541679510867, 0.0, 1.0, 50.0, 44.15318613072522], 
processed observation next is [0.0, 0.391304347826087, 0.2973222530009234, 0.78, 0.47777777777777763, 0.09594843462246778, 0.08333333333333333, 0.37425984857496, 0.42715277349637765, 0.0, 1.0, 0.7, 0.4415318613072522], 
reward next is 0.5585, 
noisyNet noise sample is [array([-0.85628605], dtype=float32), 1.1641269]. 
=============================================
[2019-04-10 14:26:26,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8268169e-17 4.4837515e-18 8.6378163e-17 2.7000929e-15 2.1508209e-16
 7.5810754e-20 6.5954283e-20 1.0000000e+00 3.2616623e-20 9.1774422e-20
 5.9958556e-16], sum to 1.0000
[2019-04-10 14:26:26,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-10 14:26:26,589] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.4, 71.66666666666667, 221.8333333333333, 47.66666666666666, 22.5, 25.36607571367736, 0.2598677372350788, 1.0, 1.0, 50.0, 46.7530734993402], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1941600.0000, 
sim time next is 1942800.0000, 
raw observation next is [-5.199999999999999, 68.33333333333333, 231.1666666666667, 9.0, 22.5, 25.53318096737564, 0.2933649838634183, 1.0, 1.0, 50.0, 46.65396989617588], 
processed observation next is [1.0, 0.4782608695652174, 0.31855955678670367, 0.6833333333333332, 0.7705555555555557, 0.009944751381215469, 0.375, 0.6277650806146365, 0.5977883279544728, 1.0, 1.0, 0.7, 0.4665396989617588], 
reward next is 0.5335, 
noisyNet noise sample is [array([-1.822313], dtype=float32), 0.06990695]. 
=============================================
[2019-04-10 14:26:34,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1795665e-17 4.9952794e-16 5.4324098e-15 5.4519449e-15 1.7267785e-14
 9.6024109e-19 9.0150238e-19 1.0000000e+00 4.6939661e-19 1.6465557e-19
 2.4350969e-14], sum to 1.0000
[2019-04-10 14:26:34,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6877
[2019-04-10 14:26:34,165] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.6, 75.0, 34.50000000000001, 218.3333333333333, 22.5, 22.66154022344322, -0.2062445178976132, 1.0, 1.0, 50.0, 39.60803989152873], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2190000.0000, 
sim time next is 2191200.0000, 
raw observation next is [-5.6, 75.0, 51.16666666666667, 293.5, 22.5, 23.16855611070304, -0.1269724624431547, 1.0, 1.0, 50.0, 38.96762530842324], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.17055555555555557, 0.3243093922651934, 0.375, 0.43071300922525335, 0.45767584585228177, 1.0, 1.0, 0.7, 0.38967625308423237], 
reward next is 0.6103, 
noisyNet noise sample is [array([-0.00925551], dtype=float32), 1.0761487]. 
=============================================
[2019-04-10 14:26:36,708] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0447711e-16 3.5976216e-16 1.0745514e-15 1.4086299e-14 2.3524556e-14
 2.4399225e-18 2.4994325e-18 1.0000000e+00 1.7410088e-18 2.1017164e-18
 3.2817160e-14], sum to 1.0000
[2019-04-10 14:26:36,708] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2826
[2019-04-10 14:26:36,732] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.199999999999999, 72.33333333333333, 104.5, 375.8333333333334, 22.5, 23.83008610885945, 0.002833472900672674, 1.0, 1.0, 50.0, 38.34932798208399], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2194800.0000, 
sim time next is 2196000.0000, 
raw observation next is [-5.0, 71.0, 109.5, 225.5, 22.5, 23.99401045825681, 0.02558239966889674, 1.0, 1.0, 50.0, 38.42251590940792], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.71, 0.365, 0.24917127071823206, 0.375, 0.4995008715214008, 0.5085274665562989, 1.0, 1.0, 0.7, 0.3842251590940792], 
reward next is 0.6158, 
noisyNet noise sample is [array([-1.0232973], dtype=float32), 1.4417815]. 
=============================================
[2019-04-10 14:26:36,736] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[56.895596]
 [57.053226]
 [57.047604]
 [57.179516]
 [57.283733]], R is [[56.57596207]
 [56.6267128 ]
 [56.67484283]
 [56.72106934]
 [56.76418304]].
[2019-04-10 14:26:37,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6458273e-16 5.3053129e-15 6.2181515e-15 1.2465835e-13 5.3240866e-15
 1.0930714e-18 3.8176089e-19 1.0000000e+00 2.4551116e-19 2.1452303e-18
 7.1595604e-15], sum to 1.0000
[2019-04-10 14:26:37,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7751
[2019-04-10 14:26:37,065] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 68.0, 135.5, 0.0, 22.5, 24.38612103246383, 0.09576820689295633, 1.0, 1.0, 50.0, 39.94642852629298], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2203200.0000, 
sim time next is 2204400.0000, 
raw observation next is [-3.733333333333333, 67.0, 130.5, 0.0, 22.5, 24.44474774925212, 0.106558323897402, 1.0, 1.0, 50.0, 39.97015793448083], 
processed observation next is [1.0, 0.5217391304347826, 0.35918744228993543, 0.67, 0.435, 0.0, 0.375, 0.5370623124376767, 0.535519441299134, 1.0, 1.0, 0.7, 0.3997015793448083], 
reward next is 0.6003, 
noisyNet noise sample is [array([1.2036145], dtype=float32), -0.0105386395]. 
=============================================
[2019-04-10 14:26:37,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6486126e-17 4.7460442e-16 1.5552897e-15 3.1947691e-14 5.4661891e-15
 3.7086462e-19 7.0168342e-20 1.0000000e+00 1.8451738e-19 3.5230827e-19
 6.1363844e-15], sum to 1.0000
[2019-04-10 14:26:37,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1166
[2019-04-10 14:26:37,551] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.4, 64.66666666666667, 163.8333333333333, 100.0, 22.5, 25.46367604690446, 0.2995336573347236, 1.0, 1.0, 50.0, 45.95315919557598], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2287200.0000, 
sim time next is 2288400.0000, 
raw observation next is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 22.5, 25.64485659548074, 0.342614918799664, 1.0, 1.0, 50.0, 45.389886128260116], 
processed observation next is [1.0, 0.4782608695652174, 0.3573407202216067, 0.6133333333333333, 0.5927777777777776, 0.11491712707182321, 0.375, 0.6370713829567283, 0.6142049729332214, 1.0, 1.0, 0.7, 0.45389886128260115], 
reward next is 0.5461, 
noisyNet noise sample is [array([-2.833851], dtype=float32), 0.6288834]. 
=============================================
[2019-04-10 14:26:40,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0031898e-16 5.2644084e-15 8.3940642e-16 1.3508456e-13 2.0629457e-14
 1.0819947e-18 6.6489858e-19 1.0000000e+00 2.6336991e-19 1.5919817e-18
 1.6191599e-14], sum to 1.0000
[2019-04-10 14:26:40,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8933
[2019-04-10 14:26:41,037] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.2, 52.66666666666667, 0.0, 0.0, 22.5, 27.38130799909187, 0.7873508414407923, 1.0, 1.0, 50.0, 39.83074718229106], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2312400.0000, 
sim time next is 2313600.0000, 
raw observation next is [-1.2, 53.33333333333334, 0.0, 0.0, 22.5, 27.32506941945509, 0.796768019338033, 1.0, 1.0, 50.0, 41.349667658017765], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5333333333333334, 0.0, 0.0, 0.375, 0.7770891182879242, 0.7655893397793444, 1.0, 1.0, 0.7, 0.41349667658017764], 
reward next is 0.5865, 
noisyNet noise sample is [array([-0.63850135], dtype=float32), -1.35552]. 
=============================================
[2019-04-10 14:26:46,901] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.04049468e-14 3.06112371e-15 2.02068573e-13 2.31760710e-13
 1.23564754e-12 1.35141817e-16 1.36845386e-17 1.00000000e+00
 5.35428617e-17 1.27067300e-17 1.61104565e-14], sum to 1.0000
[2019-04-10 14:26:46,901] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5882
[2019-04-10 14:26:46,941] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.2, 59.0, 0.0, 0.0, 19.0, 22.71459613197525, -0.2503772913266159, 0.0, 1.0, 50.0, 42.28048935870501], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2432400.0000, 
sim time next is 2433600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 19.0, 22.63046164736124, -0.2683017488185891, 0.0, 1.0, 50.0, 42.40236446267657], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.08333333333333333, 0.38587180394677006, 0.410566083727137, 0.0, 1.0, 0.7, 0.42402364462676573], 
reward next is 0.5760, 
noisyNet noise sample is [array([2.094584], dtype=float32), -0.0056718905]. 
=============================================
[2019-04-10 14:26:48,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6784695e-17 6.6372125e-16 4.3244271e-14 3.3004974e-14 9.3241638e-14
 1.7901930e-18 7.1153296e-19 1.0000000e+00 7.5772918e-19 2.3100091e-19
 5.8943511e-15], sum to 1.0000
[2019-04-10 14:26:48,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2538
[2019-04-10 14:26:48,924] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 19.0, 23.66511201478465, -0.08909127505130514, 0.0, 1.0, 50.0, 36.965272626373675], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2516400.0000, 
sim time next is 2517600.0000, 
raw observation next is [-1.7, 45.66666666666667, 0.0, 0.0, 19.0, 23.71548768634986, -0.077472212163394, 0.0, 1.0, 50.0, 36.88208727489574], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4566666666666667, 0.0, 0.0, 0.08333333333333333, 0.476290640529155, 0.4741759292788687, 0.0, 1.0, 0.7, 0.36882087274895736], 
reward next is 0.6312, 
noisyNet noise sample is [array([0.05931794], dtype=float32), -1.3691003]. 
=============================================
[2019-04-10 14:26:50,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8079590e-15 8.1917372e-16 2.3210044e-13 2.8038127e-13 2.6535233e-13
 4.7917031e-17 6.7533093e-18 1.0000000e+00 2.1688634e-17 7.2440822e-18
 5.5563371e-15], sum to 1.0000
[2019-04-10 14:26:50,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7469
[2019-04-10 14:26:50,373] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 19.0, 23.92858965413942, 0.001925523936931309, 0.0, 1.0, 50.0, 36.8049463755948], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2496000.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 19.0, 23.9829449100595, 0.004397558286212087, 0.0, 1.0, 50.0, 37.11140358229996], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.08333333333333333, 0.4985787425049584, 0.5014658527620707, 0.0, 1.0, 0.7, 0.37111403582299957], 
reward next is 0.6289, 
noisyNet noise sample is [array([-0.6090909], dtype=float32), -0.44206494]. 
=============================================
[2019-04-10 14:26:54,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5025864e-15 7.4426075e-15 2.4451873e-14 3.7027556e-13 6.2740164e-14
 1.2012881e-17 2.1857488e-17 1.0000000e+00 5.4583043e-18 2.6566881e-17
 4.4646470e-13], sum to 1.0000
[2019-04-10 14:26:54,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4421
[2019-04-10 14:26:54,083] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 24.97013770736762, 0.3399860400325704, 0.0, 1.0, 50.0, 57.6288335120656], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2748000.0000, 
sim time next is 2749200.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 24.81522341951089, 0.3100997215173767, 0.0, 1.0, 50.0, 57.715652537144365], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.5679352849592408, 0.6033665738391255, 0.0, 1.0, 0.7, 0.5771565253714437], 
reward next is 0.4228, 
noisyNet noise sample is [array([1.0583147], dtype=float32), 0.47459206]. 
=============================================
[2019-04-10 14:26:55,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4702210e-14 1.7451325e-13 1.3371753e-12 2.1928366e-12 1.6654348e-11
 4.0105290e-16 2.1522817e-16 1.0000000e+00 1.2333845e-16 1.2770881e-16
 1.6261468e-12], sum to 1.0000
[2019-04-10 14:26:55,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5738
[2019-04-10 14:26:55,544] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.126254606103, -0.1570028665708794, 0.0, 1.0, 50.0, 45.842532755543246], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2787600.0000, 
sim time next is 2788800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 22.9234697451305, -0.2081907279271874, 0.0, 1.0, 50.0, 58.685485105473674], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.4102891454275417, 0.4306030906909375, 0.0, 1.0, 0.7, 0.5868548510547368], 
reward next is 0.4131, 
noisyNet noise sample is [array([1.192501], dtype=float32), -0.7152834]. 
=============================================
[2019-04-10 14:26:57,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2355968e-15 1.3570117e-14 6.9320880e-15 4.2373592e-13 3.8921571e-14
 2.2414867e-17 2.0070648e-17 1.0000000e+00 1.5470218e-17 5.8742105e-17
 1.0290107e-13], sum to 1.0000
[2019-04-10 14:26:57,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8094
[2019-04-10 14:26:57,846] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.09179782886534, 0.3681308188957935, 1.0, 1.0, 50.0, 57.41978474809924], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2746800.0000, 
sim time next is 2748000.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 24.97013770736762, 0.3399860400325704, 0.0, 1.0, 50.0, 57.6288335120656], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.5808448089473016, 0.6133286800108567, 0.0, 1.0, 0.7, 0.576288335120656], 
reward next is 0.4237, 
noisyNet noise sample is [array([0.49281564], dtype=float32), 0.19778088]. 
=============================================
[2019-04-10 14:26:57,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[55.94756 ]
 [55.710957]
 [55.54768 ]
 [55.47172 ]
 [55.52049 ]], R is [[55.9293251 ]
 [55.79583359]
 [55.66776276]
 [55.56476974]
 [55.45230103]].
[2019-04-10 14:26:58,367] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3184030e-15 1.5222441e-14 4.3439949e-14 2.4061467e-12 4.5408728e-14
 1.7844395e-17 2.3001984e-17 1.0000000e+00 1.1391934e-17 4.4432976e-17
 1.0659950e-13], sum to 1.0000
[2019-04-10 14:26:58,373] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7369
[2019-04-10 14:26:58,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.333333333333334, 55.66666666666667, 0.0, 0.0, 22.5, 25.63372158003195, 0.423486929879392, 1.0, 1.0, 50.0, 54.63138500814959], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2744400.0000, 
sim time next is 2745600.0000, 
raw observation next is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 22.5, 25.19367833943858, 0.3808995331113698, 1.0, 1.0, 50.0, 57.0115802640606], 
processed observation next is [1.0, 0.782608695652174, 0.33333333333333337, 0.5733333333333333, 0.0, 0.0, 0.375, 0.599473194953215, 0.6269665110371233, 1.0, 1.0, 0.7, 0.570115802640606], 
reward next is 0.4299, 
noisyNet noise sample is [array([-1.3244572], dtype=float32), -1.7051364]. 
=============================================
[2019-04-10 14:27:01,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7296127e-14 1.0673363e-12 5.4867515e-13 1.6326586e-11 7.0850969e-13
 3.2744473e-16 9.4033045e-17 1.0000000e+00 5.3321912e-17 5.6582368e-15
 2.2847516e-12], sum to 1.0000
[2019-04-10 14:27:01,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0367
[2019-04-10 14:27:01,263] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 100.0, 102.3333333333333, 0.0, 22.5, 26.3516200259976, 0.6142052817016879, 1.0, 1.0, 50.0, 35.240775056768804], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2902800.0000, 
sim time next is 2904000.0000, 
raw observation next is [2.0, 100.0, 89.16666666666666, 0.0, 22.5, 26.44490149192303, 0.6262849337923094, 1.0, 1.0, 50.0, 35.32331255918116], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29722222222222217, 0.0, 0.375, 0.7037417909935858, 0.7087616445974364, 1.0, 1.0, 0.7, 0.35323312559181164], 
reward next is 0.6468, 
noisyNet noise sample is [array([0.17054957], dtype=float32), 0.3296949]. 
=============================================
[2019-04-10 14:27:01,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[52.827682]
 [53.073418]
 [53.463654]
 [53.75729 ]
 [53.977894]], R is [[52.78767776]
 [52.90739059]
 [53.02915573]
 [53.13080215]
 [53.23643875]].
[2019-04-10 14:27:06,792] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2374268e-18 1.5778411e-17 2.1854058e-18 1.9908770e-16 6.1668843e-17
 5.8086638e-21 2.8768124e-22 1.0000000e+00 1.4515665e-21 8.4619072e-22
 3.3740361e-16], sum to 1.0000
[2019-04-10 14:27:06,792] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5991
[2019-04-10 14:27:06,821] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.333333333333334, 100.0, 0.0, 0.0, 19.0, 24.52616343577806, 0.1378357139557444, 0.0, 1.0, 50.0, 49.21461191426465], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3134400.0000, 
sim time next is 3135600.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 19.0, 24.52946852238341, 0.1504326681333701, 0.0, 1.0, 50.0, 47.34679649094016], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5441223768652842, 0.5501442227111234, 0.0, 1.0, 0.7, 0.4734679649094016], 
reward next is 0.5265, 
noisyNet noise sample is [array([2.2503169], dtype=float32), -0.6484111]. 
=============================================
[2019-04-10 14:27:07,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8905464e-17 3.1367404e-16 1.5832200e-16 8.9262754e-15 7.3777332e-15
 2.2365731e-19 2.4680783e-20 1.0000000e+00 1.0725051e-20 8.4465325e-20
 4.1127232e-15], sum to 1.0000
[2019-04-10 14:27:07,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-10 14:27:07,701] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.333333333333333, 100.0, 0.0, 0.0, 19.0, 24.36014115058551, 0.1229404161274489, 0.0, 1.0, 50.0, 36.130598279382966], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3118800.0000, 
sim time next is 3120000.0000, 
raw observation next is [1.666666666666667, 100.0, 0.0, 0.0, 19.0, 24.49765416799925, 0.1345293996467732, 0.0, 1.0, 50.0, 35.691151511531714], 
processed observation next is [1.0, 0.08695652173913043, 0.5087719298245615, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5414711806666043, 0.5448431332155911, 0.0, 1.0, 0.7, 0.35691151511531716], 
reward next is 0.6431, 
noisyNet noise sample is [array([0.7948542], dtype=float32), 0.2890065]. 
=============================================
[2019-04-10 14:27:07,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[69.31851 ]
 [69.807755]
 [70.346306]
 [70.98941 ]
 [71.51804 ]], R is [[68.83594513]
 [68.78627777]
 [68.73439789]
 [68.68002319]
 [68.62530518]].
[2019-04-10 14:27:13,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3880175e-16 8.2548800e-17 7.0729038e-17 4.9720621e-14 1.3650038e-15
 1.8332047e-19 7.2215058e-20 1.0000000e+00 1.0628930e-20 9.5435084e-19
 4.7101522e-16], sum to 1.0000
[2019-04-10 14:27:13,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8603
[2019-04-10 14:27:13,719] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 22.5, 27.57873172771179, 0.999455181241751, 1.0, 1.0, 50.0, 23.63537160049366], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3241200.0000, 
sim time next is 3242400.0000, 
raw observation next is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 22.5, 27.73587418170997, 0.8795807355977345, 1.0, 1.0, 50.0, 33.684955109486495], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8033333333333332, 0.381111111111111, 0.9073664825046042, 0.375, 0.8113228484758309, 0.7931935785325782, 1.0, 1.0, 0.7, 0.336849551094865], 
reward next is 0.6632, 
noisyNet noise sample is [array([1.6633415], dtype=float32), 0.26866075]. 
=============================================
[2019-04-10 14:27:13,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2837773e-14 5.3463754e-14 2.4652066e-14 2.7544861e-12 1.4800213e-13
 4.3675246e-17 3.4871321e-17 1.0000000e+00 2.9855479e-17 2.4807694e-16
 2.1340252e-13], sum to 1.0000
[2019-04-10 14:27:13,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8669
[2019-04-10 14:27:13,911] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.81334858753826, 0.7565042410915351, 1.0, 1.0, 50.0, 34.850865815298995], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3350400.0000, 
sim time next is 3351600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.66633774681806, 0.7518542481081938, 1.0, 1.0, 50.0, 33.010313156501404], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7221948122348382, 0.7506180827027312, 1.0, 1.0, 0.7, 0.33010313156501403], 
reward next is 0.6699, 
noisyNet noise sample is [array([1.6698551], dtype=float32), -0.11904184]. 
=============================================
[2019-04-10 14:27:15,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7226613e-16 6.3322760e-14 2.5392131e-15 5.0509027e-13 1.1106731e-13
 6.0908745e-18 1.9986298e-17 1.0000000e+00 2.1160210e-17 1.5976403e-17
 1.0555435e-12], sum to 1.0000
[2019-04-10 14:27:15,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0560
[2019-04-10 14:27:15,426] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 25.99784149045665, 0.6382282004963923, 0.0, 1.0, 50.0, 38.00727842858694], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3278400.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 25.8883836745042, 0.6144814905168124, 0.0, 1.0, 50.0, 38.417346847395414], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6573653062086834, 0.7048271635056041, 0.0, 1.0, 0.7, 0.38417346847395417], 
reward next is 0.6158, 
noisyNet noise sample is [array([-0.04474219], dtype=float32), -1.3642492]. 
=============================================
[2019-04-10 14:27:17,607] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5846057e-17 3.3434783e-16 5.2227406e-16 3.4309648e-15 1.2656969e-14
 2.5038592e-19 1.7518738e-19 1.0000000e+00 1.5124755e-19 8.3595357e-20
 1.3599416e-15], sum to 1.0000
[2019-04-10 14:27:17,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4834
[2019-04-10 14:27:17,663] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.333333333333333, 60.0, 89.0, 461.3333333333333, 22.5, 25.20973416020198, 0.3626302434086066, 1.0, 1.0, 50.0, 37.81236249325935], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3400800.0000, 
sim time next is 3402000.0000, 
raw observation next is [-1.0, 60.0, 93.0, 540.0, 22.5, 25.6122552685025, 0.4371545650546505, 1.0, 1.0, 50.0, 37.014113325081354], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.6, 0.31, 0.5966850828729282, 0.375, 0.6343546057085417, 0.6457181883515501, 1.0, 1.0, 0.7, 0.37014113325081355], 
reward next is 0.6299, 
noisyNet noise sample is [array([1.6261935], dtype=float32), -0.14315577]. 
=============================================
[2019-04-10 14:27:17,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.123695]
 [65.57713 ]
 [64.97421 ]
 [64.30333 ]
 [63.590252]], R is [[66.48153687]
 [66.43859863]
 [66.38414001]
 [66.32479095]
 [66.26229858]].
[2019-04-10 14:27:25,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2257860e-16 8.5532858e-17 1.2875816e-15 5.8129377e-15 2.0102170e-14
 2.0615206e-19 1.4964577e-19 1.0000000e+00 7.7423195e-19 9.2797806e-20
 4.1715335e-15], sum to 1.0000
[2019-04-10 14:27:25,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4917
[2019-04-10 14:27:25,590] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 19.0, 26.46060803032573, 0.6362307545093965, 0.0, 1.0, 50.0, 36.78482801905138], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3721200.0000, 
sim time next is 3722400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.38993499478779, 0.6318687882278445, 0.0, 1.0, 50.0, 37.00130957269222], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6991612495656492, 0.7106229294092815, 0.0, 1.0, 0.7, 0.3700130957269222], 
reward next is 0.6300, 
noisyNet noise sample is [array([-0.24792136], dtype=float32), -0.80817163]. 
=============================================
[2019-04-10 14:27:25,705] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0507606e-16 2.1208534e-17 1.0103105e-15 5.6960519e-14 1.0202797e-15
 4.1207869e-19 1.6064573e-19 1.0000000e+00 2.2345604e-19 1.2071807e-19
 2.0896152e-16], sum to 1.0000
[2019-04-10 14:27:25,705] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8928
[2019-04-10 14:27:25,734] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 19.0, 26.72812599109467, 0.7491275947504453, 0.0, 1.0, 50.0, 31.102935085631167], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3604800.0000, 
sim time next is 3606000.0000, 
raw observation next is [-0.6666666666666666, 41.0, 11.66666666666667, 118.3333333333333, 19.0, 26.66977366320888, 0.7251987407992422, 0.0, 1.0, 50.0, 31.303838205745464], 
processed observation next is [0.0, 0.7391304347826086, 0.44413665743305636, 0.41, 0.038888888888888896, 0.1307550644567219, 0.08333333333333333, 0.7224811386007399, 0.7417329135997474, 0.0, 1.0, 0.7, 0.31303838205745466], 
reward next is 0.6870, 
noisyNet noise sample is [array([-0.5172346], dtype=float32), 1.2556771]. 
=============================================
[2019-04-10 14:27:25,738] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[72.86939 ]
 [73.136086]
 [73.42207 ]
 [73.60051 ]
 [73.63968 ]], R is [[72.69078827]
 [72.65285492]
 [72.61398315]
 [72.57550049]
 [72.5356369 ]].
[2019-04-10 14:27:26,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1569916e-16 1.0290195e-15 9.1194295e-15 4.4552788e-14 4.4390689e-14
 2.1692925e-18 5.8830171e-19 1.0000000e+00 1.1639369e-18 3.6652014e-19
 2.3326832e-15], sum to 1.0000
[2019-04-10 14:27:26,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9897
[2019-04-10 14:27:26,593] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 19.0, 25.31641247029162, 0.4475749779479222, 0.0, 1.0, 50.0, 38.01982610759612], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3561600.0000, 
sim time next is 3562800.0000, 
raw observation next is [-5.666666666666667, 68.33333333333333, 0.0, 0.0, 19.0, 25.24349069008037, 0.4326956053450504, 0.0, 1.0, 50.0, 37.9892722345068], 
processed observation next is [0.0, 0.21739130434782608, 0.30563250230840255, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.6036242241733643, 0.6442318684483501, 0.0, 1.0, 0.7, 0.379892722345068], 
reward next is 0.6201, 
noisyNet noise sample is [array([0.18229008], dtype=float32), 0.5438183]. 
=============================================
[2019-04-10 14:27:31,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2005285e-17 4.9999878e-16 7.8982540e-17 3.8023871e-14 8.4892818e-17
 8.1725871e-20 5.1233544e-20 1.0000000e+00 3.1948446e-20 1.2716519e-18
 3.4415806e-16], sum to 1.0000
[2019-04-10 14:27:31,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9691
[2019-04-10 14:27:31,408] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 22.5, 28.47329513845619, 1.138249696334631, 1.0, 1.0, 50.0, 16.05972817552179], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3772800.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 22.5, 28.62982650127348, 0.8821821091531588, 1.0, 1.0, 50.0, 25.038758443527485], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.375, 0.8858188751061234, 0.7940607030510529, 1.0, 1.0, 0.7, 0.25038758443527487], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.1349996], dtype=float32), -0.39763018]. 
=============================================
[2019-04-10 14:27:31,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.9235  ]
 [70.363884]
 [70.67116 ]
 [70.332695]
 [70.4991  ]], R is [[69.77801514]
 [69.91963959]
 [70.10422516]
 [70.00746918]
 [70.12038422]].
[2019-04-10 14:27:36,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2794922e-16 6.3398985e-15 1.6158834e-14 1.0091893e-12 7.6906537e-13
 1.1170486e-17 9.0680793e-17 1.0000000e+00 5.4593484e-17 4.8697615e-17
 3.8391009e-14], sum to 1.0000
[2019-04-10 14:27:36,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1778
[2019-04-10 14:27:36,250] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 19.0, 26.07359751167693, 0.6477406939331324, 0.0, 1.0, 50.0, 38.17835706363125], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3967200.0000, 
sim time next is 3968400.0000, 
raw observation next is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 19.0, 25.9853752140824, 0.5666176696059074, 0.0, 1.0, 50.0, 39.07888294057595], 
processed observation next is [1.0, 0.9565217391304348, 0.23176361957525393, 0.5033333333333334, 0.0, 0.0, 0.08333333333333333, 0.6654479345068666, 0.6888725565353025, 0.0, 1.0, 0.7, 0.39078882940575954], 
reward next is 0.6092, 
noisyNet noise sample is [array([-0.4530313], dtype=float32), 0.42658338]. 
=============================================
[2019-04-10 14:27:44,545] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8507392e-18 2.4447136e-16 2.4438465e-16 1.2938228e-14 3.3548506e-15
 2.8799666e-20 6.6343421e-19 1.0000000e+00 2.3186082e-19 5.1579267e-20
 8.0046281e-17], sum to 1.0000
[2019-04-10 14:27:44,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5754
[2019-04-10 14:27:44,574] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 38.33333333333334, 0.0, 0.0, 19.0, 27.24030166035396, 0.8873554377149553, 0.0, 1.0, 50.0, 28.538297699816262], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4135200.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 19.0, 27.18011602427982, 0.8763332894208498, 0.0, 1.0, 50.0, 28.809678182950442], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.08333333333333333, 0.7650096686899849, 0.7921110964736165, 0.0, 1.0, 0.7, 0.28809678182950443], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.22088423], dtype=float32), -0.37168476]. 
=============================================
[2019-04-10 14:28:00,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1933290e-17 7.2175275e-16 9.7919296e-16 1.0269299e-14 1.1955343e-14
 2.1118899e-19 2.6962686e-19 1.0000000e+00 8.2777289e-20 7.3166161e-20
 2.6306120e-15], sum to 1.0000
[2019-04-10 14:28:00,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7825
[2019-04-10 14:28:00,871] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.0, 71.0, 61.5, 85.5, 22.5, 25.67310405757487, 0.5212662789739607, 1.0, 1.0, 50.0, 33.1040371604352], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4608000.0000, 
sim time next is 4609200.0000, 
raw observation next is [-2.0, 71.0, 102.5, 142.5, 22.5, 25.7625720038902, 0.5912276936135902, 1.0, 1.0, 50.0, 33.00469667269108], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.3416666666666667, 0.1574585635359116, 0.375, 0.6468810003241833, 0.6970758978711967, 1.0, 1.0, 0.7, 0.3300469667269108], 
reward next is 0.6700, 
noisyNet noise sample is [array([-0.68213475], dtype=float32), 1.5137018]. 
=============================================
[2019-04-10 14:28:01,274] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5195158e-18 4.6402074e-16 2.4743616e-16 6.3421965e-15 4.4784250e-15
 6.6628965e-20 1.6472532e-19 1.0000000e+00 1.8301861e-19 1.4062372e-19
 3.8492176e-15], sum to 1.0000
[2019-04-10 14:28:01,274] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5541
[2019-04-10 14:28:01,302] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 81.66666666666667, 0.0, 0.0, 19.0, 26.3940201518125, 0.7106043193663342, 0.0, 1.0, 50.0, 38.31098120240416], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4746000.0000, 
sim time next is 4747200.0000, 
raw observation next is [-3.0, 79.33333333333333, 0.0, 0.0, 19.0, 26.14705141807814, 0.6772170900138247, 0.0, 1.0, 50.0, 37.42732245126439], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.6789209515065115, 0.7257390300046082, 0.0, 1.0, 0.7, 0.3742732245126439], 
reward next is 0.6257, 
noisyNet noise sample is [array([-1.0885161], dtype=float32), -0.4236398]. 
=============================================
[2019-04-10 14:28:02,124] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1912861e-15 1.5307319e-13 1.4586878e-14 2.3244700e-12 8.0879747e-14
 1.8714538e-17 2.4451788e-17 1.0000000e+00 2.5093885e-17 3.2596168e-16
 2.0792745e-12], sum to 1.0000
[2019-04-10 14:28:02,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9609
[2019-04-10 14:28:02,215] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 72.0, 123.5, 5.5, 22.5, 27.9918477179524, 1.006354636106312, 1.0, 1.0, 50.0, 30.126370520144935], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4723200.0000, 
sim time next is 4724400.0000, 
raw observation next is [1.0, 72.0, 107.8333333333333, 9.166666666666668, 22.5, 27.23645890256906, 0.9315983996207322, 1.0, 1.0, 50.0, 47.15655998440003], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.35944444444444434, 0.010128913443830573, 0.375, 0.7697049085474216, 0.8105327998735774, 1.0, 1.0, 0.7, 0.4715655998440003], 
reward next is 0.5284, 
noisyNet noise sample is [array([-0.8631912], dtype=float32), -0.24150826]. 
=============================================
[2019-04-10 14:28:02,772] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.02633692e-17 8.16826379e-18 5.15469206e-16 4.04959464e-15
 1.22396955e-14 1.54095373e-19 3.30410284e-20 1.00000000e+00
 9.29632683e-20 1.30180691e-20 4.63967591e-16], sum to 1.0000
[2019-04-10 14:28:02,778] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8348
[2019-04-10 14:28:02,803] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 19.0, 24.2086392329101, 0.2129973914815149, 0.0, 1.0, 50.0, 39.17383253168943], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4779600.0000, 
sim time next is 4780800.0000, 
raw observation next is [-6.0, 92.0, 62.0, 209.5, 19.0, 24.17389996604187, 0.2314202255773413, 0.0, 1.0, 50.0, 38.89801776286082], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.92, 0.20666666666666667, 0.23149171270718233, 0.08333333333333333, 0.5144916638368224, 0.577140075192447, 0.0, 1.0, 0.7, 0.38898017762860815], 
reward next is 0.6110, 
noisyNet noise sample is [array([-0.1171733], dtype=float32), -0.6024157]. 
=============================================
[2019-04-10 14:28:02,965] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 14:28:02,968] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3053154e-16 2.9379863e-15 1.8036446e-15 7.4187442e-14 1.1975655e-14
 1.0031544e-18 5.0612928e-19 1.0000000e+00 2.2790594e-19 3.8947463e-19
 2.8283023e-15], sum to 1.0000
[2019-04-10 14:28:02,968] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2794
[2019-04-10 14:28:02,974] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:28:02,974] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:28:02,975] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:28:02,975] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:28:02,977] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:28:02,977] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:28:02,979] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run5
[2019-04-10 14:28:02,992] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 19.0, 25.99298165666428, 0.5964986729505061, 0.0, 1.0, 50.0, 31.403086197061867], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4887600.0000, 
sim time next is 4888800.0000, 
raw observation next is [2.0, 44.0, 254.0, 381.0, 19.0, 26.07824407871673, 0.6145476141829168, 0.0, 1.0, 50.0, 31.321724498066573], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.44, 0.8466666666666667, 0.42099447513812155, 0.08333333333333333, 0.6731870065597274, 0.704849204727639, 0.0, 1.0, 0.7, 0.3132172449806657], 
reward next is 0.6868, 
noisyNet noise sample is [array([-2.1020813], dtype=float32), -0.6149136]. 
=============================================
[2019-04-10 14:28:02,993] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run5
[2019-04-10 14:28:03,004] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run5
[2019-04-10 14:29:11,594] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2824.4115 142006.9416 1219.4616
[2019-04-10 14:29:11,614] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:11,614] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:11,614] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:11,614] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:11,614] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:11,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:11,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:11,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:11,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:11,725] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:22,506] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2741.8289 150712.3196 941.3419
[2019-04-10 14:29:22,525] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:22,525] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:22,525] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:22,525] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:22,525] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:26,425] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2704.0892 154398.2722 786.3333
[2019-04-10 14:29:26,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:26,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:26,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:26,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:26,443] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:26,555] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:26,555] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:26,555] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:26,555] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:26,555] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:29:27,446] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 200000, evaluation results [200000.0, 2741.828868918773, 150712.31962660837, 941.3419215034918, 2824.411495936729, 142006.94161186027, 1219.461581683659, 2704.0891666007647, 154398.27216610115, 786.3333053506527]
[2019-04-10 14:29:33,072] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:33,221] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:33,689] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:33,860] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:34,073] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:34,074] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:34,078] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run4
[2019-04-10 14:29:34,113] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:34,270] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:34,284] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:34,447] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:34,672] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:34,682] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:34,683] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:34,687] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run4
[2019-04-10 14:29:34,841] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:34,894] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:35,049] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:35,108] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:35,108] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:35,111] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run4
[2019-04-10 14:29:35,111] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:35,260] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:35,286] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:35,286] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:35,288] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run4
[2019-04-10 14:29:35,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:35,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:35,678] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run4
[2019-04-10 14:29:35,896] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:35,896] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:35,899] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run4
[2019-04-10 14:29:35,917] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:35,961] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:36,095] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:36,113] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:36,113] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:36,115] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run4
[2019-04-10 14:29:36,132] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:36,349] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:36,545] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:36,672] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:36,772] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:36,803] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:36,837] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:36,917] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:36,917] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:36,919] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run4
[2019-04-10 14:29:36,953] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:36,960] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:36,960] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:36,962] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run4
[2019-04-10 14:29:36,976] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:37,350] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:37,351] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:37,353] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run4
[2019-04-10 14:29:37,671] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:37,671] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:37,673] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run4
[2019-04-10 14:29:37,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:37,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:37,775] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run4
[2019-04-10 14:29:37,805] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:37,805] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:37,808] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run4
[2019-04-10 14:29:37,863] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:38,166] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:38,180] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:38,481] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:38,861] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:38,861] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:38,863] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run4
[2019-04-10 14:29:39,181] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:39,181] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:39,183] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run4
[2019-04-10 14:29:39,987] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_2 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:29:40,275] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_2 ERROR:Aborted (core dumped)

[2019-04-10 14:29:40,988] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:29:40,988] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:29:40,990] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run4
[2019-04-10 14:29:52,813] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8939510e-15 3.5342928e-13 1.7356071e-15 1.3649382e-12 9.0739664e-16
 1.7847050e-17 1.2127582e-16 1.0000000e+00 5.9609825e-17 5.2951809e-16
 8.6875460e-14], sum to 1.0000
[2019-04-10 14:29:52,813] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8684
[2019-04-10 14:29:52,838] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.9, 66.33333333333334, 32.66666666666666, 9.999999999999998, 22.5, 24.58472208221005, 0.1981023785742309, 1.0, 1.0, 50.0, 42.15222498880151], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 145200.0000, 
sim time next is 146400.0000, 
raw observation next is [-7.1, 68.66666666666666, 22.5, 2.5, 22.5, 24.7428844220401, 0.2136316337480261, 1.0, 1.0, 50.0, 42.25428162170164], 
processed observation next is [1.0, 0.6956521739130435, 0.2659279778393352, 0.6866666666666665, 0.075, 0.0027624309392265192, 0.375, 0.5619070351700083, 0.5712105445826754, 1.0, 1.0, 0.7, 0.4225428162170164], 
reward next is 0.5775, 
noisyNet noise sample is [array([-0.11147764], dtype=float32), -0.5001172]. 
=============================================
[2019-04-10 14:29:54,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5526556e-18 3.1876322e-16 3.6607338e-17 2.7761069e-15 1.1076202e-17
 8.4360028e-20 8.6102341e-20 1.0000000e+00 4.2575325e-20 1.1153832e-18
 2.1471483e-16], sum to 1.0000
[2019-04-10 14:29:54,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0192
[2019-04-10 14:29:54,799] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.8, 86.0, 187.0, 24.5, 22.5, 23.86674668679184, 0.01738636299708483, 1.0, 1.0, 50.0, 43.08826085640882], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 126000.0000, 
sim time next is 127200.0000, 
raw observation next is [-8.0, 77.66666666666667, 185.0, 16.83333333333333, 22.5, 23.93433322781647, 0.0322963507614927, 1.0, 1.0, 50.0, 42.84017111830265], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.7766666666666667, 0.6166666666666667, 0.018600368324125226, 0.375, 0.49452776898470585, 0.5107654502538309, 1.0, 1.0, 0.7, 0.4284017111830265], 
reward next is 0.5716, 
noisyNet noise sample is [array([2.4774692], dtype=float32), 0.08412742]. 
=============================================
[2019-04-10 14:29:56,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0872012e-15 1.6205192e-14 4.4055971e-15 3.3264597e-14 2.6337712e-14
 1.3077319e-17 1.0023707e-17 1.0000000e+00 1.1933044e-17 1.7114893e-17
 3.9270853e-14], sum to 1.0000
[2019-04-10 14:29:56,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3269
[2019-04-10 14:29:56,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1526790e-17 3.7602924e-16 1.8952420e-16 5.6076546e-15 7.8956372e-16
 2.3025312e-19 1.9006919e-18 1.0000000e+00 7.4610693e-19 1.8464493e-18
 4.4732174e-15], sum to 1.0000
[2019-04-10 14:29:56,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3418
[2019-04-10 14:29:56,567] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 21.49860347081826, -0.4961612393967077, 0.0, 1.0, 50.0, 42.782252773164814], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 188400.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 21.39388512171407, -0.5136059191261849, 0.0, 1.0, 50.0, 42.94877182021619], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.2828237601428392, 0.32879802695793836, 0.0, 1.0, 0.7, 0.42948771820216186], 
reward next is 0.5705, 
noisyNet noise sample is [array([1.5539925], dtype=float32), -1.1407582]. 
=============================================
[2019-04-10 14:29:56,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 22.07830172362638, -0.3118775060897918, 0.0, 1.0, 50.0, 43.322138379651975], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 169200.0000, 
sim time next is 170400.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 22.01090346025447, -0.3270409127934025, 0.0, 1.0, 50.0, 43.24207345390187], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.33424195502120596, 0.3909863624021992, 0.0, 1.0, 0.7, 0.4324207345390187], 
reward next is 0.5676, 
noisyNet noise sample is [array([-1.3670019], dtype=float32), -0.9223138]. 
=============================================
[2019-04-10 14:30:03,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.41998522e-16 3.01530981e-15 7.10763610e-16 1.60401729e-15
 4.64667494e-15 1.01307612e-18 1.90109080e-18 1.00000000e+00
 1.86392929e-18 4.40410966e-19 1.12316476e-14], sum to 1.0000
[2019-04-10 14:30:03,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9410
[2019-04-10 14:30:03,630] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 21.02945137821917, -0.6258109715655104, 0.0, 1.0, 50.0, 47.416395973328946], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 354000.0000, 
sim time next is 355200.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 20.93510639081449, -0.6489167531128265, 0.0, 1.0, 50.0, 47.649894142275656], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.24459219923454079, 0.28369441562905784, 0.0, 1.0, 0.7, 0.47649894142275656], 
reward next is 0.5235, 
noisyNet noise sample is [array([1.0068243], dtype=float32), 0.23714706]. 
=============================================
[2019-04-10 14:30:14,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.62909582e-16 9.47482415e-15 5.16987459e-16 1.17970980e-14
 1.59117045e-15 8.62917865e-19 2.12018096e-18 1.00000000e+00
 1.05197873e-18 5.35497786e-18 1.39065835e-14], sum to 1.0000
[2019-04-10 14:30:14,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0719
[2019-04-10 14:30:14,543] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 19.0, 25.79495906104773, 0.4317914983583758, 0.0, 1.0, 50.0, 37.52143088052233], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 508800.0000, 
sim time next is 510000.0000, 
raw observation next is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 25.71490764449308, 0.4221775924855433, 0.0, 1.0, 50.0, 37.19645863634578], 
processed observation next is [1.0, 0.9130434782608695, 0.5272391505078486, 0.9333333333333332, 0.0, 0.0, 0.08333333333333333, 0.6429089703744234, 0.6407258641618477, 0.0, 1.0, 0.7, 0.3719645863634578], 
reward next is 0.6280, 
noisyNet noise sample is [array([0.9179018], dtype=float32), -0.2097516]. 
=============================================
[2019-04-10 14:30:14,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.756035]
 [54.747986]
 [54.924644]
 [55.18873 ]
 [56.24618 ]], R is [[54.73960114]
 [54.81699371]
 [54.89270401]
 [54.96677399]
 [55.00126648]].
[2019-04-10 14:30:17,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.39310841e-18 2.83534287e-17 1.24897099e-16 7.44731802e-16
 8.60154811e-17 8.17399073e-20 1.01929725e-19 1.00000000e+00
 3.27957855e-20 4.41696353e-19 4.65900838e-16], sum to 1.0000
[2019-04-10 14:30:17,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0105
[2019-04-10 14:30:17,529] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 22.5, 25.58173393154448, 0.2416031332918036, 1.0, 1.0, 50.0, 47.42328609517114], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 475200.0000, 
sim time next is 476400.0000, 
raw observation next is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 22.5, 25.77863007891787, 0.2727483329611519, 1.0, 1.0, 50.0, 46.936387384394806], 
processed observation next is [1.0, 0.5217391304347826, 0.42012927054478305, 0.26, 0.426111111111111, 0.0, 0.375, 0.6482191732431559, 0.5909161109870507, 1.0, 1.0, 0.7, 0.46936387384394807], 
reward next is 0.5306, 
noisyNet noise sample is [array([2.24908], dtype=float32), -1.9844126]. 
=============================================
[2019-04-10 14:30:19,788] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8673534e-17 2.3388912e-17 2.4185927e-16 1.2554730e-15 1.8073147e-16
 4.9363590e-19 4.2711154e-20 1.0000000e+00 6.0279275e-20 8.7895734e-20
 6.3122160e-16], sum to 1.0000
[2019-04-10 14:30:19,789] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6690
[2019-04-10 14:30:19,837] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 24.60241891848501, 0.1877348596015949, 0.0, 1.0, 50.0, 38.58528007814017], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 544800.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 24.49381124145967, 0.17159820157026, 0.0, 1.0, 50.0, 38.66780323155582], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.541150936788306, 0.55719940052342, 0.0, 1.0, 0.7, 0.38667803231555825], 
reward next is 0.6133, 
noisyNet noise sample is [array([1.1718997], dtype=float32), -0.22434105]. 
=============================================
[2019-04-10 14:30:19,859] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[58.73723 ]
 [58.787376]
 [58.814053]
 [58.828854]
 [58.80915 ]], R is [[58.71217346]
 [58.73920059]
 [58.7674675 ]
 [58.79825211]
 [58.82381439]].
[2019-04-10 14:30:21,388] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0902202e-17 2.1237367e-16 3.6754560e-16 2.1240781e-15 6.1759046e-16
 7.2608999e-19 2.8895420e-19 1.0000000e+00 6.3778970e-19 1.7208099e-19
 4.5365571e-16], sum to 1.0000
[2019-04-10 14:30:21,388] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-10 14:30:21,421] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.566666666666667, 58.66666666666667, 0.0, 0.0, 19.0, 22.70205656339707, -0.2655876275177503, 0.0, 1.0, 50.0, 41.41923336465572], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 670800.0000, 
sim time next is 672000.0000, 
raw observation next is [-1.933333333333333, 60.33333333333333, 0.0, 0.0, 19.0, 22.6665827533602, -0.2723762568253373, 0.0, 1.0, 50.0, 41.324664731809364], 
processed observation next is [0.0, 0.782608695652174, 0.40904893813481075, 0.6033333333333333, 0.0, 0.0, 0.08333333333333333, 0.38888189611334995, 0.4092079143915542, 0.0, 1.0, 0.7, 0.41324664731809363], 
reward next is 0.5868, 
noisyNet noise sample is [array([-0.4516733], dtype=float32), 0.7523977]. 
=============================================
[2019-04-10 14:30:21,449] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[55.992878]
 [56.169888]
 [56.272835]
 [56.298428]
 [56.332447]], R is [[55.82912064]
 [55.85663605]
 [55.88260269]
 [55.90748596]
 [55.93235397]].
[2019-04-10 14:30:22,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5196540e-17 1.4185497e-17 2.6751985e-16 2.5383595e-16 9.9661452e-16
 4.8604026e-19 5.5333310e-20 1.0000000e+00 1.3595756e-19 1.1907704e-19
 1.7702566e-16], sum to 1.0000
[2019-04-10 14:30:22,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-10 14:30:22,104] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 19.0, 22.76711579339153, -0.2165175489103112, 0.0, 1.0, 50.0, 42.51014600671766], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 622800.0000, 
sim time next is 624000.0000, 
raw observation next is [-4.5, 67.0, 0.0, 0.0, 19.0, 22.74689340736898, -0.2219726271416354, 0.0, 1.0, 50.0, 42.348110517676304], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.67, 0.0, 0.0, 0.08333333333333333, 0.39557445061408164, 0.42600912428612153, 0.0, 1.0, 0.7, 0.423481105176763], 
reward next is 0.5765, 
noisyNet noise sample is [array([-0.09782769], dtype=float32), -0.8846009]. 
=============================================
[2019-04-10 14:30:22,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.953934]
 [57.985477]
 [58.019047]
 [58.0702  ]
 [58.103893]], R is [[57.94960785]
 [57.94501114]
 [57.93992233]
 [57.93612671]
 [57.93560028]].
[2019-04-10 14:30:26,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2579026e-18 7.3268739e-18 6.5572811e-17 7.8879864e-17 5.0309633e-16
 3.8829209e-20 3.2445743e-20 1.0000000e+00 7.8871284e-21 2.5802266e-20
 1.2135820e-16], sum to 1.0000
[2019-04-10 14:30:26,427] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-10 14:30:26,453] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.2, 72.0, 0.0, 0.0, 19.0, 22.68562560691975, -0.2643186956894022, 0.0, 1.0, 50.0, 37.77868126525753], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 884400.0000, 
sim time next is 885600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 22.57988803885552, -0.2705508707537702, 0.0, 1.0, 50.0, 37.70750779425991], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.3816573365712932, 0.4098163764154099, 0.0, 1.0, 0.7, 0.3770750779425991], 
reward next is 0.6229, 
noisyNet noise sample is [array([0.5664217], dtype=float32), 0.9145654]. 
=============================================
[2019-04-10 14:30:27,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.29705953e-18 1.75867863e-17 4.02049879e-17 4.87632335e-17
 1.88347967e-16 1.21549296e-20 5.26379008e-21 1.00000000e+00
 2.94804787e-21 7.65812616e-21 8.32749788e-17], sum to 1.0000
[2019-04-10 14:30:27,072] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-10 14:30:27,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.29910759902897, -0.393730813841177, 0.0, 1.0, 50.0, 40.41844296936043], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 710400.0000, 
sim time next is 711600.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 22.26917784168851, -0.3882560372509589, 0.0, 1.0, 50.0, 40.48933903023737], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.35576482014070915, 0.37058132091634705, 0.0, 1.0, 0.7, 0.4048933903023737], 
reward next is 0.5951, 
noisyNet noise sample is [array([2.1093974], dtype=float32), 0.16895325]. 
=============================================
[2019-04-10 14:30:29,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3530520e-18 3.0340083e-18 1.1473839e-17 5.2909959e-18 4.4478087e-17
 4.0829631e-21 2.4255807e-21 1.0000000e+00 2.7807193e-21 4.8138067e-21
 3.5944116e-16], sum to 1.0000
[2019-04-10 14:30:29,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-10 14:30:29,261] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 80.33333333333334, 0.0, 0.0, 19.0, 22.5426999576795, -0.2358198721680544, 0.0, 1.0, 50.0, 39.467933158550494], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 859200.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 19.0, 22.57695822180007, -0.2403931560952559, 0.0, 1.0, 50.0, 39.30991363780325], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.08333333333333333, 0.3814131851500058, 0.41986894796824803, 0.0, 1.0, 0.7, 0.3930991363780325], 
reward next is 0.6069, 
noisyNet noise sample is [array([-1.9928461], dtype=float32), 1.0610083]. 
=============================================
[2019-04-10 14:30:29,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1190398e-19 1.0218283e-18 1.5459178e-18 5.3503428e-18 3.1978611e-17
 6.4902714e-22 3.3651567e-22 1.0000000e+00 4.4074807e-22 6.1310275e-22
 6.5332121e-17], sum to 1.0000
[2019-04-10 14:30:29,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0014
[2019-04-10 14:30:29,739] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 24.09548518264637, 0.1771918631652351, 0.0, 1.0, 50.0, 35.792434703422586], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 950400.0000, 
sim time next is 951600.0000, 
raw observation next is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 19.0, 24.07746891531743, 0.1853695157003639, 0.0, 1.0, 50.0, 35.83184031442214], 
processed observation next is [1.0, 0.0, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.08333333333333333, 0.506455742943119, 0.561789838566788, 0.0, 1.0, 0.7, 0.35831840314422136], 
reward next is 0.6417, 
noisyNet noise sample is [array([-0.46754533], dtype=float32), -1.3689622]. 
=============================================
[2019-04-10 14:30:30,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4413772e-18 1.3688976e-17 1.8597540e-17 7.8376567e-17 3.5732194e-17
 3.8459491e-20 8.6755430e-21 1.0000000e+00 2.1694144e-20 4.5149502e-20
 1.6990720e-16], sum to 1.0000
[2019-04-10 14:30:30,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5483
[2019-04-10 14:30:30,095] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 22.5, 23.44134746615337, -0.1544674306792809, 1.0, 1.0, 50.0, 39.41549822321699], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 812400.0000, 
sim time next is 813600.0000, 
raw observation next is [-6.2, 75.0, 74.0, 0.0, 22.5, 23.59999508890496, -0.1257690180146608, 1.0, 1.0, 50.0, 39.20765027773132], 
processed observation next is [1.0, 0.43478260869565216, 0.2908587257617729, 0.75, 0.24666666666666667, 0.0, 0.375, 0.4666662574087468, 0.45807699399511304, 1.0, 1.0, 0.7, 0.3920765027773132], 
reward next is 0.6079, 
noisyNet noise sample is [array([1.2433324], dtype=float32), 0.42374134]. 
=============================================
[2019-04-10 14:30:33,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8382176e-20 1.6512797e-19 3.9153761e-20 6.1067146e-19 1.7401913e-19
 2.7193184e-23 1.4526285e-23 1.0000000e+00 2.6973420e-24 2.1821515e-23
 1.7854735e-18], sum to 1.0000
[2019-04-10 14:30:33,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2973
[2019-04-10 14:30:33,240] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 24.11536015188825, 0.1872179194344166, 0.0, 1.0, 50.0, 35.915860994475636], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 943200.0000, 
sim time next is 944400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 24.09975070042194, 0.1838168094922047, 0.0, 1.0, 50.0, 35.888605647340334], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.508312558368495, 0.5612722698307349, 0.0, 1.0, 0.7, 0.3588860564734033], 
reward next is 0.6411, 
noisyNet noise sample is [array([0.29885307], dtype=float32), 0.65391815]. 
=============================================
[2019-04-10 14:30:35,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6578863e-21 3.1130141e-19 1.4777490e-19 2.7502344e-19 1.6124442e-18
 3.8252105e-23 2.1421745e-23 1.0000000e+00 8.2791331e-24 1.9376500e-23
 3.7226643e-19], sum to 1.0000
[2019-04-10 14:30:35,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2638
[2019-04-10 14:30:35,712] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 19.0, 27.3426549987584, 0.9431776788983953, 0.0, 1.0, 50.0, 29.16628685242036], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1053600.0000, 
sim time next is 1054800.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 19.0, 27.30517026719538, 0.9428905094195187, 0.0, 1.0, 50.0, 30.299494178573806], 
processed observation next is [1.0, 0.21739130434782608, 0.844875346260388, 0.78, 0.0, 0.0, 0.08333333333333333, 0.7754308555996149, 0.814296836473173, 0.0, 1.0, 0.7, 0.30299494178573805], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.37056068], dtype=float32), -0.63494676]. 
=============================================
[2019-04-10 14:30:35,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3146778e-22 6.3698213e-20 3.4313338e-21 6.0472738e-20 7.3509144e-20
 2.2725511e-24 7.1517173e-25 1.0000000e+00 5.2524666e-25 5.1953571e-24
 1.6374982e-19], sum to 1.0000
[2019-04-10 14:30:35,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9437
[2019-04-10 14:30:35,986] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [15.13333333333333, 79.0, 0.0, 0.0, 22.5, 27.88769219427738, 0.9527702376568771, 1.0, 1.0, 50.0, 25.948855549380234], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1012800.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 22.5, 27.65732225683207, 0.962188194202814, 1.0, 1.0, 50.0, 28.40715549985676], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 0.375, 0.8047768547360059, 0.8207293980676047, 1.0, 1.0, 0.7, 0.2840715549985676], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.61246425], dtype=float32), -0.65012604]. 
=============================================
[2019-04-10 14:30:35,991] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.341835]
 [75.372604]
 [75.46097 ]
 [75.62398 ]
 [75.63973 ]], R is [[75.3458252 ]
 [75.33287811]
 [75.36300659]
 [75.36685944]
 [75.29053497]].
[2019-04-10 14:30:36,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3678809e-19 9.7435071e-20 2.4497816e-17 1.4618727e-16 3.7252658e-18
 5.5681812e-21 4.9037886e-22 1.0000000e+00 7.3296265e-22 9.8181189e-22
 1.0719990e-18], sum to 1.0000
[2019-04-10 14:30:36,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7230
[2019-04-10 14:30:36,496] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.8, 63.0, 165.5, 0.0, 19.0, 28.05011011934405, 1.199209672617, 0.0, 0.0, 50.0, 20.70038232281144], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1166400.0000, 
sim time next is 1167600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 19.0, 28.072211432705, 1.207327914587604, 0.0, 0.0, 50.0, 20.712270693403347], 
processed observation next is [0.0, 0.5217391304347826, 0.9787626962142197, 0.6366666666666667, 0.563888888888889, 0.0, 0.08333333333333333, 0.8393509527254167, 0.902442638195868, 0.0, 0.0, 0.7, 0.20712270693403348], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.7327566], dtype=float32), 2.4377413]. 
=============================================
[2019-04-10 14:30:38,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6185349e-20 1.6555940e-19 1.0513770e-18 6.3672734e-19 4.9606352e-19
 3.1205828e-22 1.3085962e-23 1.0000000e+00 5.2905585e-23 3.1541035e-23
 2.6568679e-19], sum to 1.0000
[2019-04-10 14:30:38,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4621
[2019-04-10 14:30:38,864] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.60315913142728, 1.106586607241726, 0.0, 1.0, 50.0, 29.665069368256525], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1135200.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 27.60494370668655, 1.102216265661089, 0.0, 1.0, 50.0, 29.64004335088413], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8004119755572127, 0.8674054218870296, 0.0, 1.0, 0.7, 0.2964004335088413], 
reward next is 0.7036, 
noisyNet noise sample is [array([1.0023769], dtype=float32), 0.16542688]. 
=============================================
[2019-04-10 14:30:40,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0749945e-21 8.2472182e-20 7.9265885e-19 2.0586601e-19 1.7705183e-18
 4.0614904e-23 5.5520953e-24 1.0000000e+00 1.0676744e-23 9.6991355e-24
 7.6450911e-18], sum to 1.0000
[2019-04-10 14:30:40,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4862
[2019-04-10 14:30:40,775] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 19.0, 27.23817092334277, 0.9907318336630643, 0.0, 1.0, 50.0, 35.811213504913795], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1312800.0000, 
sim time next is 1314000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 27.20157384438043, 1.0052686125672, 0.0, 1.0, 50.0, 36.10425129953941], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7667978203650359, 0.8350895375223999, 0.0, 1.0, 0.7, 0.36104251299539414], 
reward next is 0.6390, 
noisyNet noise sample is [array([1.2099587], dtype=float32), 0.93541354]. 
=============================================
[2019-04-10 14:30:40,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.38257 ]
 [67.470345]
 [67.5573  ]
 [67.636566]
 [67.73133 ]], R is [[67.24771881]
 [67.21713257]
 [67.18733215]
 [67.15139771]
 [67.12761688]].
[2019-04-10 14:30:41,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1925911e-21 7.4094122e-21 2.3609240e-18 7.9538793e-19 3.6668917e-20
 2.5423929e-22 8.0715980e-24 1.0000000e+00 4.7534379e-23 3.3597531e-24
 8.8631322e-22], sum to 1.0000
[2019-04-10 14:30:41,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2745
[2019-04-10 14:30:41,824] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 28.15428232166812, 1.232461962699699, 0.0, 0.0, 50.0, 19.696831867566456], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1172400.0000, 
sim time next is 1173600.0000, 
raw observation next is [18.3, 65.0, 143.5, 0.0, 19.0, 28.18092436273368, 1.239103123650292, 0.0, 0.0, 50.0, 19.05286544690917], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.47833333333333333, 0.0, 0.08333333333333333, 0.8484103635611401, 0.9130343745500973, 0.0, 0.0, 0.7, 0.1905286544690917], 
reward next is 0.8095, 
noisyNet noise sample is [array([-0.16763559], dtype=float32), -0.5144111]. 
=============================================
[2019-04-10 14:30:42,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.9049376e-19 5.2663714e-17 4.8242166e-19 6.7203157e-17 4.1423493e-18
 2.8878418e-21 8.7272579e-21 1.0000000e+00 1.1879562e-21 1.3846566e-20
 1.0092743e-15], sum to 1.0000
[2019-04-10 14:30:42,235] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0405
[2019-04-10 14:30:42,259] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 114.5, 0.0, 22.5, 28.02398588674108, 1.085908722322588, 1.0, 1.0, 50.0, 26.402315599481526], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1335600.0000, 
sim time next is 1336800.0000, 
raw observation next is [1.1, 92.0, 122.8333333333333, 0.0, 22.5, 28.05426636427529, 1.094624361756219, 1.0, 1.0, 50.0, 26.154414999038828], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.40944444444444433, 0.0, 0.375, 0.8378555303562741, 0.8648747872520731, 1.0, 1.0, 0.7, 0.2615441499903883], 
reward next is 0.7385, 
noisyNet noise sample is [array([-2.0200021], dtype=float32), 0.057996783]. 
=============================================
[2019-04-10 14:30:48,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2521182e-18 7.5840915e-18 1.3003490e-18 1.4916668e-16 3.1429432e-17
 1.5869284e-21 1.8570564e-21 1.0000000e+00 7.0410927e-22 2.6707867e-21
 4.9333173e-16], sum to 1.0000
[2019-04-10 14:30:48,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8645
[2019-04-10 14:30:48,331] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 22.5, 27.14768516411144, 0.8405092717982411, 1.0, 1.0, 50.0, 32.24722757264249], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1418400.0000, 
sim time next is 1419600.0000, 
raw observation next is [0.0, 95.0, 67.66666666666667, 0.0, 22.5, 27.21182902144154, 0.8487743042074651, 1.0, 1.0, 50.0, 31.913920112452296], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.22555555555555556, 0.0, 0.375, 0.7676524184534618, 0.782924768069155, 1.0, 1.0, 0.7, 0.31913920112452293], 
reward next is 0.6809, 
noisyNet noise sample is [array([-0.39661452], dtype=float32), -0.18971547]. 
=============================================
[2019-04-10 14:30:51,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6066710e-20 6.6312547e-19 2.3620926e-20 6.8936293e-18 4.7932131e-20
 1.6643997e-22 9.2075504e-23 1.0000000e+00 2.5847346e-23 4.6764437e-22
 5.0482990e-18], sum to 1.0000
[2019-04-10 14:30:51,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8044
[2019-04-10 14:30:51,210] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.133333333333333, 69.66666666666667, 87.5, 700.8333333333334, 22.5, 27.28470768808731, 1.057042210963074, 1.0, 1.0, 50.0, 29.46997157121117], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1516800.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 22.5, 27.77980177512332, 1.073037983052848, 1.0, 1.0, 50.0, 5.946463909025859], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.375, 0.8149834812602768, 0.8576793276842828, 1.0, 1.0, 0.7, 0.059464639090258585], 
reward next is 0.9405, 
noisyNet noise sample is [array([-1.9926143], dtype=float32), 0.6969659]. 
=============================================
[2019-04-10 14:30:51,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.27599]
 [70.88853]
 [70.62305]
 [70.06125]
 [69.30859]], R is [[71.48054504]
 [71.47103882]
 [71.57991028]
 [71.62962341]
 [71.63374329]].
[2019-04-10 14:31:07,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2737017e-19 5.7631617e-18 4.3341268e-19 2.2675886e-17 1.1253381e-16
 9.5482644e-22 8.6814645e-22 1.0000000e+00 4.7616255e-22 1.2444451e-21
 1.3224715e-16], sum to 1.0000
[2019-04-10 14:31:07,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4318
[2019-04-10 14:31:07,200] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.8, 90.66666666666667, 0.0, 0.0, 19.0, 27.56777099848409, 1.036802423234779, 0.0, 1.0, 50.0, 30.127505127458093], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1644000.0000, 
sim time next is 1645200.0000, 
raw observation next is [6.6, 93.0, 0.0, 0.0, 19.0, 27.51912508488477, 1.040827081162547, 0.0, 1.0, 50.0, 33.09524476984576], 
processed observation next is [1.0, 0.043478260869565216, 0.6454293628808865, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7932604237403975, 0.8469423603875157, 0.0, 1.0, 0.7, 0.3309524476984576], 
reward next is 0.6690, 
noisyNet noise sample is [array([-0.72605413], dtype=float32), -0.5376365]. 
=============================================
[2019-04-10 14:31:25,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8977017e-16 5.6359655e-14 2.4493623e-16 4.0468015e-14 1.5105584e-15
 1.2794323e-18 5.3716112e-18 1.0000000e+00 3.3876001e-18 2.9416505e-17
 2.0018464e-13], sum to 1.0000
[2019-04-10 14:31:25,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-10 14:31:25,565] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.2, 62.0, 116.1666666666667, 0.0, 22.5, 26.7090272703836, 0.5540806484361644, 1.0, 1.0, 50.0, 45.65210681922942], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1952400.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 22.5, 26.877464121734, 0.5879879451509405, 1.0, 1.0, 50.0, 45.575053096154036], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 0.375, 0.7397886768111667, 0.6959959817169802, 1.0, 1.0, 0.7, 0.4557505309615404], 
reward next is 0.5442, 
noisyNet noise sample is [array([1.2245681], dtype=float32), -0.9432386]. 
=============================================
[2019-04-10 14:31:28,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5578993e-19 4.2446327e-16 4.1056653e-17 1.5397338e-16 2.2496856e-16
 1.3636056e-20 2.3299382e-20 1.0000000e+00 9.4072319e-21 4.0279534e-20
 7.6314910e-16], sum to 1.0000
[2019-04-10 14:31:28,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9562
[2019-04-10 14:31:28,363] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 19.0, 25.78716800638491, 0.4532825628569423, 0.0, 1.0, 50.0, 39.645775451661095], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1981200.0000, 
sim time next is 1982400.0000, 
raw observation next is [-5.8, 83.0, 0.0, 0.0, 19.0, 25.514289069556, 0.4155617811019028, 0.0, 1.0, 50.0, 39.25907844163865], 
processed observation next is [1.0, 0.9565217391304348, 0.30193905817174516, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6261907557963333, 0.6385205937006343, 0.0, 1.0, 0.7, 0.3925907844163865], 
reward next is 0.6074, 
noisyNet noise sample is [array([-0.11646484], dtype=float32), -1.0737252]. 
=============================================
[2019-04-10 14:31:30,412] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4655131e-17 5.0712176e-15 1.1233221e-16 3.3255193e-15 5.4510094e-15
 2.9268054e-19 1.1783626e-18 1.0000000e+00 8.6283230e-19 1.5534018e-18
 6.3899434e-14], sum to 1.0000
[2019-04-10 14:31:30,413] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3741
[2019-04-10 14:31:30,452] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 19.0, 24.204454638523, 0.2050685721116006, 0.0, 1.0, 50.0, 39.11023160790755], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2065200.0000, 
sim time next is 2066400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 24.17297792841991, 0.1985537711917306, 0.0, 1.0, 50.0, 39.02489734154464], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.5144148273683259, 0.5661845903972436, 0.0, 1.0, 0.7, 0.3902489734154464], 
reward next is 0.6098, 
noisyNet noise sample is [array([0.04478505], dtype=float32), -1.0110958]. 
=============================================
[2019-04-10 14:31:30,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5414809e-18 1.6148273e-17 3.8721291e-17 4.7810971e-16 7.4982014e-16
 3.6732199e-20 6.4895847e-20 1.0000000e+00 2.0326889e-20 4.8132583e-20
 5.0464751e-16], sum to 1.0000
[2019-04-10 14:31:30,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9013
[2019-04-10 14:31:30,861] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.4, 89.33333333333334, 0.0, 0.0, 19.0, 23.25692193836717, -0.08064014676455931, 0.0, 1.0, 50.0, 41.57161026232017], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2086800.0000, 
sim time next is 2088000.0000, 
raw observation next is [-5.6, 91.0, 0.0, 0.0, 19.0, 23.28690198006537, -0.05940666569242858, 0.0, 1.0, 50.0, 41.752972234053516], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.91, 0.0, 0.0, 0.08333333333333333, 0.44057516500544747, 0.4801977781025238, 0.0, 1.0, 0.7, 0.41752972234053515], 
reward next is 0.5825, 
noisyNet noise sample is [array([-1.8180072], dtype=float32), 0.29321495]. 
=============================================
[2019-04-10 14:31:30,870] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[61.061283]
 [61.07189 ]
 [61.088852]
 [61.121185]
 [61.211098]], R is [[61.04148865]
 [61.01536179]
 [60.99103165]
 [60.96881866]
 [60.94924927]].
[2019-04-10 14:31:31,451] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7824550e-17 1.1093714e-16 5.5761211e-17 4.2599132e-16 7.2079799e-16
 9.6198953e-20 2.1000645e-19 1.0000000e+00 4.2755657e-20 1.6346212e-19
 1.3321887e-15], sum to 1.0000
[2019-04-10 14:31:31,451] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9222
[2019-04-10 14:31:31,502] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 19.0, 23.39154845134821, -0.02374884632203488, 0.0, 1.0, 50.0, 40.798143066743705], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2080800.0000, 
sim time next is 2082000.0000, 
raw observation next is [-4.666666666666667, 86.0, 0.0, 0.0, 19.0, 23.45833147345098, -0.02167549061862932, 0.0, 1.0, 50.0, 40.81720618052893], 
processed observation next is [1.0, 0.08695652173913043, 0.3333333333333333, 0.86, 0.0, 0.0, 0.08333333333333333, 0.4548609561209149, 0.4927748364604569, 0.0, 1.0, 0.7, 0.40817206180528925], 
reward next is 0.5918, 
noisyNet noise sample is [array([-0.15330485], dtype=float32), -0.22883816]. 
=============================================
[2019-04-10 14:31:31,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[60.476807]
 [60.525177]
 [60.59674 ]
 [60.664455]
 [60.699207]], R is [[60.39818954]
 [60.38622665]
 [60.37496185]
 [60.36429214]
 [60.35409546]].
[2019-04-10 14:31:31,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.70880903e-16 1.18796890e-15 1.08021834e-16 1.18226934e-13
 2.38063269e-16 9.12769404e-19 1.83150009e-18 1.00000000e+00
 8.65740205e-19 1.73252284e-17 2.82879477e-14], sum to 1.0000
[2019-04-10 14:31:31,684] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8390
[2019-04-10 14:31:31,721] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 22.5, 24.76118699903036, 0.2296809078359426, 1.0, 1.0, 50.0, 37.73156268934167], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2115600.0000, 
sim time next is 2116800.0000, 
raw observation next is [-6.7, 64.0, 222.0, 117.5, 22.5, 24.89116802089685, 0.2508881665456958, 1.0, 1.0, 50.0, 37.64418087034553], 
processed observation next is [1.0, 0.5217391304347826, 0.2770083102493075, 0.64, 0.74, 0.1298342541436464, 0.375, 0.5742640017414041, 0.5836293888485652, 1.0, 1.0, 0.7, 0.3764418087034553], 
reward next is 0.6236, 
noisyNet noise sample is [array([-0.69606876], dtype=float32), -0.050804053]. 
=============================================
[2019-04-10 14:31:50,929] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4215920e-20 1.7708365e-17 4.6714785e-18 1.2958287e-17 9.8638681e-17
 3.1231569e-21 1.8735429e-21 1.0000000e+00 2.0017694e-21 1.1092756e-21
 7.1450386e-17], sum to 1.0000
[2019-04-10 14:31:50,932] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0046
[2019-04-10 14:31:50,968] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 19.0, 23.35199704325757, -0.08146892986550451, 0.0, 1.0, 50.0, 42.377090489141516], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2616000.0000, 
sim time next is 2617200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 19.0, 23.35271648448724, -0.08276999829884746, 0.0, 1.0, 50.0, 42.57563261241235], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4460597070406032, 0.47241000056705085, 0.0, 1.0, 0.7, 0.4257563261241235], 
reward next is 0.5742, 
noisyNet noise sample is [array([0.9770425], dtype=float32), 0.97820246]. 
=============================================
[2019-04-10 14:31:55,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9253262e-20 7.2973828e-20 2.6547100e-19 1.7553876e-17 1.9653765e-19
 1.1205369e-22 6.7767241e-23 1.0000000e+00 5.6274722e-24 2.5790137e-22
 1.6873527e-19], sum to 1.0000
[2019-04-10 14:31:55,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8223
[2019-04-10 14:31:55,402] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 22.5, 26.1741773846248, 0.5093799854610782, 1.0, 1.0, 50.0, 32.75320889974029], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2554800.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 22.5, 26.32107787561291, 0.5316420567270501, 1.0, 1.0, 50.0, 32.65414726556491], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.375, 0.6934231563010759, 0.6772140189090168, 1.0, 1.0, 0.7, 0.3265414726556491], 
reward next is 0.6735, 
noisyNet noise sample is [array([0.30813986], dtype=float32), -0.815675]. 
=============================================
[2019-04-10 14:31:55,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.82242 ]
 [71.601845]
 [71.20988 ]
 [70.87512 ]
 [70.66773 ]], R is [[71.79051208]
 [71.74507904]
 [71.68741608]
 [71.62654114]
 [71.56832123]].
[2019-04-10 14:31:58,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2111349e-16 6.0692905e-15 4.0917227e-15 1.2009967e-14 3.7060815e-14
 3.5111034e-18 7.0860253e-18 1.0000000e+00 3.4919090e-18 1.7419530e-17
 1.8025219e-13], sum to 1.0000
[2019-04-10 14:31:58,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2765
[2019-04-10 14:31:58,706] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.3, 79.0, 18.66666666666666, 6.666666666666667, 22.5, 23.27833134174454, -0.09345819851164423, 0.0, 1.0, 50.0, 42.77305422887558], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2619600.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 42.0, 4.0, 22.5, 23.26132306853209, -0.098510294594135, 1.0, 1.0, 50.0, 42.705090388186434], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.14, 0.004419889502762431, 0.375, 0.4384435890443408, 0.46716323513528835, 1.0, 1.0, 0.7, 0.42705090388186434], 
reward next is 0.5729, 
noisyNet noise sample is [array([-0.4905662], dtype=float32), 0.75660884]. 
=============================================
[2019-04-10 14:32:01,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9111285e-17 2.5514765e-15 7.0134233e-16 2.7196783e-14 1.0759055e-14
 2.9852845e-18 2.5578277e-17 1.0000000e+00 4.6537103e-18 4.4609265e-17
 3.9642947e-13], sum to 1.0000
[2019-04-10 14:32:01,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1098
[2019-04-10 14:32:01,123] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 64.0, 0.0, 0.0, 19.0, 24.48834856169877, 0.267327053198338, 0.0, 1.0, 50.0, 49.670655570533654], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2754000.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 19.0, 24.42946795309413, 0.2549709080219824, 0.0, 1.0, 50.0, 48.052525772700164], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.5357889960911774, 0.5849903026739941, 0.0, 1.0, 0.7, 0.48052525772700166], 
reward next is 0.5195, 
noisyNet noise sample is [array([-0.05698266], dtype=float32), -0.7653129]. 
=============================================
[2019-04-10 14:32:03,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8084778e-17 3.5760302e-16 1.2938092e-16 9.7482862e-16 9.0975351e-16
 1.5650112e-19 5.7212302e-20 1.0000000e+00 5.2109089e-20 1.2741228e-19
 2.5221935e-15], sum to 1.0000
[2019-04-10 14:32:03,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4068
[2019-04-10 14:32:03,856] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.26130125087543, -0.1075355163949077, 0.0, 1.0, 50.0, 39.44398688644033], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2784000.0000, 
sim time next is 2785200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.19626066706997, -0.1242634575978691, 0.0, 1.0, 50.0, 39.53542814079287], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.43302172225583097, 0.4585788474673769, 0.0, 1.0, 0.7, 0.39535428140792866], 
reward next is 0.6046, 
noisyNet noise sample is [array([0.72545254], dtype=float32), 0.073802285]. 
=============================================
[2019-04-10 14:32:07,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.95250193e-18 2.45375651e-17 3.15420941e-17 4.64869363e-16
 4.09920404e-16 3.35085179e-20 1.23209829e-20 1.00000000e+00
 1.02754985e-20 9.85497683e-20 1.34790248e-15], sum to 1.0000
[2019-04-10 14:32:07,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2212
[2019-04-10 14:32:07,631] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 19.0, 24.17551396799046, 0.1083178194907521, 0.0, 1.0, 50.0, 52.509600422964496], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2874000.0000, 
sim time next is 2875200.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 19.0, 24.1798606840428, 0.1273873547294028, 0.0, 1.0, 50.0, 51.96295026763305], 
processed observation next is [1.0, 0.2608695652173913, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.5149883903369, 0.5424624515764677, 0.0, 1.0, 0.7, 0.5196295026763305], 
reward next is 0.4804, 
noisyNet noise sample is [array([1.9952997], dtype=float32), -0.30100346]. 
=============================================
[2019-04-10 14:32:10,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.93168029e-17 1.27874984e-17 5.27789538e-17 6.78445650e-16
 5.31140281e-17 1.52341362e-19 1.09054636e-20 1.00000000e+00
 7.83494975e-21 1.33472429e-19 1.29256606e-16], sum to 1.0000
[2019-04-10 14:32:10,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5504
[2019-04-10 14:32:10,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 19.0, 23.01908209008298, -0.101598120728354, 0.0, 1.0, 50.0, 35.41423813464823], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3058800.0000, 
sim time next is 3060000.0000, 
raw observation next is [-4.0, 54.0, 102.5, 697.0, 19.0, 23.14557336441561, -0.07226739601055375, 0.0, 1.0, 50.0, 34.520560616662166], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.3416666666666667, 0.7701657458563536, 0.08333333333333333, 0.4287977803679676, 0.4759108679964821, 0.0, 1.0, 0.7, 0.34520560616662166], 
reward next is 0.6548, 
noisyNet noise sample is [array([-0.95547116], dtype=float32), -1.0690126]. 
=============================================
[2019-04-10 14:32:11,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[60.781742]
 [60.434895]
 [60.03418 ]
 [59.717022]
 [59.394844]], R is [[61.09225082]
 [61.12718582]
 [61.15256119]
 [61.17143631]
 [61.1820488 ]].
[2019-04-10 14:32:16,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6289713e-19 8.4243015e-19 8.9120005e-19 2.3022524e-17 4.5775778e-18
 4.8014982e-22 6.2920713e-23 1.0000000e+00 1.4972656e-23 3.1473639e-21
 9.8061608e-18], sum to 1.0000
[2019-04-10 14:32:16,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0141
[2019-04-10 14:32:16,731] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 26.79807193886057, 0.9044470139565638, 0.0, 1.0, 50.0, 33.94265150783866], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3194400.0000, 
sim time next is 3195600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 26.72750220955905, 0.8885948210243811, 0.0, 1.0, 50.0, 33.98472755745182], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7272918507965874, 0.7961982736747938, 0.0, 1.0, 0.7, 0.3398472755745182], 
reward next is 0.6602, 
noisyNet noise sample is [array([-0.920259], dtype=float32), 0.052599885]. 
=============================================
[2019-04-10 14:32:17,145] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.59300362e-19 8.94571663e-20 6.47006044e-19 1.10863055e-17
 7.23110228e-19 1.35551941e-21 2.18215145e-23 1.00000000e+00
 8.27496914e-23 1.33205260e-21 2.82987380e-18], sum to 1.0000
[2019-04-10 14:32:17,145] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7390
[2019-04-10 14:32:17,220] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 94.66666666666667, 0.0, 0.0, 19.0, 24.47527495954761, 0.168475452342212, 0.0, 1.0, 50.0, 37.87596407230228], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3097200.0000, 
sim time next is 3098400.0000, 
raw observation next is [-1.0, 97.33333333333333, 0.0, 0.0, 19.0, 24.46282763628334, 0.1625948806733057, 0.0, 1.0, 50.0, 37.89941551022822], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.5385689696902783, 0.5541982935577686, 0.0, 1.0, 0.7, 0.37899415510228224], 
reward next is 0.6210, 
noisyNet noise sample is [array([1.503729], dtype=float32), -2.4632134]. 
=============================================
[2019-04-10 14:32:18,991] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1455098e-17 2.9295564e-17 4.8881497e-17 4.5765873e-16 1.0542979e-16
 1.8447646e-20 7.8171573e-21 1.0000000e+00 4.7846061e-21 4.1523123e-20
 6.5532335e-16], sum to 1.0000
[2019-04-10 14:32:18,991] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-10 14:32:19,020] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.0, 70.0, 0.0, 0.0, 19.0, 25.33115480758989, 0.4801957051140368, 0.0, 1.0, 50.0, 40.286394054674], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3286800.0000, 
sim time next is 3288000.0000, 
raw observation next is [-7.333333333333334, 72.33333333333334, 0.0, 0.0, 19.0, 25.30575157668769, 0.4669468257732403, 0.0, 1.0, 50.0, 40.4531202370501], 
processed observation next is [1.0, 0.043478260869565216, 0.2594644506001847, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.6088126313906409, 0.6556489419244135, 0.0, 1.0, 0.7, 0.404531202370501], 
reward next is 0.5955, 
noisyNet noise sample is [array([-0.7643934], dtype=float32), -0.9970333]. 
=============================================
[2019-04-10 14:32:19,041] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[62.338066]
 [62.87134 ]
 [63.537727]
 [64.27765 ]
 [64.956154]], R is [[61.96347427]
 [61.940979  ]
 [61.92066193]
 [61.90333176]
 [61.88974762]].
[2019-04-10 14:32:22,186] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.5182449e-18 8.9862255e-17 2.3329222e-16 2.4601981e-15 6.2564077e-16
 1.9264880e-19 3.7808099e-20 1.0000000e+00 2.9663692e-20 4.8338879e-19
 4.8983466e-15], sum to 1.0000
[2019-04-10 14:32:22,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4329
[2019-04-10 14:32:22,218] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.6666666666666667, 100.0, 0.0, 0.0, 19.0, 26.76694956318527, 0.8677313531041196, 0.0, 1.0, 50.0, 34.46631297954522], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3201600.0000, 
sim time next is 3202800.0000, 
raw observation next is [0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 26.73560539230861, 0.840039773602847, 0.0, 1.0, 50.0, 34.7156749486909], 
processed observation next is [1.0, 0.043478260869565216, 0.4718374884579871, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7279671160257175, 0.7800132578676157, 0.0, 1.0, 0.7, 0.34715674948690906], 
reward next is 0.6528, 
noisyNet noise sample is [array([-1.3903064], dtype=float32), 0.06790256]. 
=============================================
[2019-04-10 14:32:22,449] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7319295e-19 1.8491084e-16 9.7490200e-18 3.5015701e-16 1.1114040e-17
 8.9304713e-21 4.6106058e-21 1.0000000e+00 8.4089776e-21 2.0206217e-19
 7.8338901e-17], sum to 1.0000
[2019-04-10 14:32:22,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9228
[2019-04-10 14:32:22,504] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 26.48932454335916, 0.78449859555626, 0.0, 1.0, 50.0, 37.41560404904069], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3273600.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 19.0, 26.41526900578169, 0.7694894006069518, 0.0, 1.0, 50.0, 37.18799089071519], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.08333333333333333, 0.701272417148474, 0.7564964668689839, 0.0, 1.0, 0.7, 0.3718799089071519], 
reward next is 0.6281, 
noisyNet noise sample is [array([-1.642639], dtype=float32), 0.18928687]. 
=============================================
[2019-04-10 14:32:23,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6032257e-18 4.2976314e-17 4.1472507e-18 1.0384085e-15 5.6939333e-18
 6.3373647e-21 2.0593816e-20 1.0000000e+00 2.8081899e-21 5.1586576e-19
 2.5129904e-17], sum to 1.0000
[2019-04-10 14:32:23,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6685
[2019-04-10 14:32:23,490] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.266666666666667, 77.0, 112.3333333333333, 801.1666666666666, 22.5, 27.41323843196734, 0.9996326419237259, 1.0, 1.0, 50.0, 34.797333251954456], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3237600.0000, 
sim time next is 3238800.0000, 
raw observation next is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 22.5, 27.38022571398015, 0.9836403562192162, 1.0, 1.0, 50.0, 25.28568097930622], 
processed observation next is [1.0, 0.4782608695652174, 0.4035087719298246, 0.74, 0.37777777777777766, 0.8983425414364641, 0.375, 0.7816854761650124, 0.8278801187397388, 1.0, 1.0, 0.7, 0.2528568097930622], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.22282709], dtype=float32), 0.45797196]. 
=============================================
[2019-04-10 14:32:26,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4283244e-16 8.4706615e-14 1.6666293e-15 1.1428368e-13 1.0721484e-14
 9.4134924e-18 2.9923093e-17 1.0000000e+00 1.2843229e-17 7.9621752e-17
 1.8595192e-14], sum to 1.0000
[2019-04-10 14:32:26,840] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4972
[2019-04-10 14:32:26,903] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.08763521894198, 0.6381442165648036, 0.0, 1.0, 50.0, 36.004689877426486], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3358800.0000, 
sim time next is 3360000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 25.99589700527971, 0.6213729933929238, 0.0, 1.0, 50.0, 36.173105453572774], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6663247504399757, 0.7071243311309746, 0.0, 1.0, 0.7, 0.36173105453572774], 
reward next is 0.6383, 
noisyNet noise sample is [array([-0.3532934], dtype=float32), 0.69013405]. 
=============================================
[2019-04-10 14:32:26,924] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[51.4494  ]
 [51.494175]
 [51.67604 ]
 [52.23567 ]
 [52.100285]], R is [[51.57948303]
 [51.7036438 ]
 [51.82804871]
 [51.95289612]
 [52.07794952]].
[2019-04-10 14:32:29,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7164051e-18 2.0436235e-17 5.0721991e-17 7.8938304e-16 2.3103477e-16
 4.3409088e-20 2.3438537e-20 1.0000000e+00 5.4486851e-21 1.1013918e-19
 6.3302774e-16], sum to 1.0000
[2019-04-10 14:32:29,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4082
[2019-04-10 14:32:29,785] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 70.0, 0.0, 0.0, 19.0, 26.88656123871143, 0.818810451258298, 0.0, 1.0, 50.0, 38.345882266494186], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3537600.0000, 
sim time next is 3538800.0000, 
raw observation next is [-1.0, 66.0, 0.0, 0.0, 19.0, 26.7457627697206, 0.8010880828995681, 0.0, 1.0, 50.0, 38.38409798719718], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7288135641433833, 0.7670293609665227, 0.0, 1.0, 0.7, 0.3838409798719718], 
reward next is 0.6162, 
noisyNet noise sample is [array([0.38461795], dtype=float32), -1.2225996]. 
=============================================
[2019-04-10 14:32:30,671] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-10 14:32:30,671] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:32:30,675] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:32:30,676] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run6
[2019-04-10 14:32:30,674] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:32:30,695] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:32:30,699] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run6
[2019-04-10 14:32:30,715] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:32:30,717] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:32:30,720] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run6
[2019-04-10 14:32:47,492] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([-0.07407624], dtype=float32), -0.54379284]
[2019-04-10 14:32:47,492] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-11.13333333333333, 55.0, 0.0, 0.0, 19.0, 21.20178562954455, -0.6194334876932036, 0.0, 1.0, 50.0, 43.774951575314674]
[2019-04-10 14:32:47,493] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-10 14:32:47,493] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [3.7452587e-17 2.6721883e-17 1.3534819e-16 1.0524821e-15 1.8393226e-16
 3.5842328e-19 2.0089768e-20 1.0000000e+00 3.3314150e-20 1.4134923e-19
 1.5995926e-16], sampled 0.15451761965563204
[2019-04-10 14:34:02,010] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([-0.07407624], dtype=float32), -0.54379284]
[2019-04-10 14:34:02,010] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.0, 61.0, 112.0, 790.0, 22.5, 26.59177551723548, 0.8380980955414148, 1.0, 1.0, 50.0, 40.18290122211471]
[2019-04-10 14:34:02,010] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-10 14:34:02,012] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [4.8667320e-18 2.1614959e-17 1.9895246e-17 4.5544885e-15 1.4258732e-18
 5.1312954e-20 1.1451390e-20 1.0000000e+00 4.9568277e-21 4.9295094e-19
 3.8139573e-18], sampled 0.3003933743408509
[2019-04-10 14:34:22,628] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2824.7400 141975.2428 1220.7053
[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,658] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:22,815] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,360] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2741.8289 150712.3196 941.3419
[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:48,849] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2704.0892 154398.2722 786.3333
[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:48,897] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,109] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:34:49,902] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 250000, evaluation results [250000.0, 2741.828868918773, 150712.31962660837, 941.3419215034918, 2824.7399698979193, 141975.2427887979, 1220.7052505585646, 2704.0891666007647, 154398.27216610115, 786.3333053506527]
[2019-04-10 14:34:51,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9703780e-17 4.5049441e-17 6.4490236e-17 2.3034836e-16 3.4518496e-16
 1.6820315e-19 5.7396803e-20 1.0000000e+00 3.1617457e-20 1.3877031e-19
 2.3293298e-16], sum to 1.0000
[2019-04-10 14:34:51,700] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0661
[2019-04-10 14:34:51,783] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.36405408864396, 0.1521382327916153, 0.0, 1.0, 50.0, 40.32673503026831], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3394800.0000, 
sim time next is 3396000.0000, 
raw observation next is [-2.666666666666667, 63.33333333333334, 0.0, 0.0, 22.5, 24.30978724464933, 0.1542536815964578, 1.0, 1.0, 50.0, 40.183693833074244], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.6333333333333334, 0.0, 0.0, 0.375, 0.5258156037207774, 0.5514178938654859, 1.0, 1.0, 0.7, 0.40183693833074247], 
reward next is 0.5982, 
noisyNet noise sample is [array([1.2377552], dtype=float32), 0.63193554]. 
=============================================
[2019-04-10 14:34:51,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.957867]
 [60.909958]
 [60.820683]
 [60.726406]
 [60.66363 ]], R is [[61.32012558]
 [61.30365753]
 [61.28667068]
 [61.26963425]
 [61.25356293]].
[2019-04-10 14:34:58,820] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2864258e-18 7.5406676e-20 1.2683727e-17 8.3594846e-17 2.3818032e-17
 1.4878960e-20 8.9875738e-22 1.0000000e+00 9.8244873e-22 1.7879790e-21
 3.2652136e-18], sum to 1.0000
[2019-04-10 14:34:58,822] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4073
[2019-04-10 14:34:58,865] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 24.87109375442503, 0.3464636475158401, 0.0, 1.0, 50.0, 38.46473686871844], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3567600.0000, 
sim time next is 3568800.0000, 
raw observation next is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 24.78652218325594, 0.3259950221136462, 0.0, 1.0, 50.0, 38.7116813720222], 
processed observation next is [0.0, 0.30434782608695654, 0.28716528162511545, 0.7, 0.0, 0.0, 0.08333333333333333, 0.5655435152713283, 0.6086650073712154, 0.0, 1.0, 0.7, 0.387116813720222], 
reward next is 0.6129, 
noisyNet noise sample is [array([0.16243872], dtype=float32), 1.6910642]. 
=============================================
[2019-04-10 14:35:05,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.31920941e-17 9.75043283e-18 2.92512452e-16 1.36035143e-15
 5.84872015e-17 3.50133766e-19 1.17964316e-20 1.00000000e+00
 1.12636215e-20 1.25436938e-19 4.67230090e-17], sum to 1.0000
[2019-04-10 14:35:05,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-10 14:35:05,202] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 24.96022873449899, 0.3667850308928862, 0.0, 1.0, 50.0, 38.25743673088097], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3566400.0000, 
sim time next is 3567600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 24.87109375442503, 0.3464636475158401, 0.0, 1.0, 50.0, 38.46473686871844], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.5725911462020857, 0.61548788250528, 0.0, 1.0, 0.7, 0.3846473686871844], 
reward next is 0.6154, 
noisyNet noise sample is [array([0.15556033], dtype=float32), -0.19648172]. 
=============================================
[2019-04-10 14:35:14,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1971923e-18 4.7160445e-18 1.6757056e-17 2.8168017e-16 2.5040527e-17
 2.6946324e-20 2.2656212e-21 1.0000000e+00 3.1767595e-21 1.4421127e-20
 9.0947198e-17], sum to 1.0000
[2019-04-10 14:35:14,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-10 14:35:14,551] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.5511134185108, 0.6681743263729417, 0.0, 1.0, 50.0, 36.32912484745961], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3718800.0000, 
sim time next is 3720000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 26.55030301229924, 0.6654416197827192, 0.0, 1.0, 50.0, 36.625801186095], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7125252510249366, 0.7218138732609064, 0.0, 1.0, 0.7, 0.36625801186095003], 
reward next is 0.6337, 
noisyNet noise sample is [array([-0.48317492], dtype=float32), 0.80728275]. 
=============================================
[2019-04-10 14:35:14,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.37201 ]
 [64.87827 ]
 [65.36738 ]
 [65.938965]
 [66.43832 ]], R is [[63.91745758]
 [63.91498947]
 [63.90903854]
 [63.90629578]
 [63.90385056]].
[2019-04-10 14:35:17,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3031377e-17 2.6125158e-19 2.5956114e-17 1.6225774e-16 5.9274684e-17
 3.8264861e-20 1.2129541e-21 1.0000000e+00 3.3024363e-22 1.3173532e-20
 9.4155441e-17], sum to 1.0000
[2019-04-10 14:35:17,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-10 14:35:17,727] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 19.0, 27.56317109205167, 0.9144032690621954, 0.0, 1.0, 50.0, 32.79147259556588], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3698400.0000, 
sim time next is 3699600.0000, 
raw observation next is [3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 27.5146974679268, 0.9033424658737653, 0.0, 1.0, 50.0, 33.09554621875539], 
processed observation next is [0.0, 0.8260869565217391, 0.5549399815327793, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.7928914556605667, 0.8011141552912551, 0.0, 1.0, 0.7, 0.3309554621875539], 
reward next is 0.6690, 
noisyNet noise sample is [array([-1.0123328], dtype=float32), -0.41415632]. 
=============================================
[2019-04-10 14:35:19,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8719611e-18 5.3830780e-18 9.9534445e-18 1.7059180e-15 4.1383873e-17
 2.6645397e-20 9.6843200e-22 1.0000000e+00 1.5835360e-21 1.3482443e-19
 1.1768723e-16], sum to 1.0000
[2019-04-10 14:35:19,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8141
[2019-04-10 14:35:19,404] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 19.0, 26.73085988888522, 0.722232521554524, 0.0, 1.0, 50.0, 36.08669924719411], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3712800.0000, 
sim time next is 3714000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 26.61701095133887, 0.7018451409809415, 0.0, 1.0, 50.0, 36.273022280040024], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7180842459449058, 0.7339483803269805, 0.0, 1.0, 0.7, 0.36273022280040024], 
reward next is 0.6373, 
noisyNet noise sample is [array([0.18732904], dtype=float32), 0.43232784]. 
=============================================
[2019-04-10 14:35:19,437] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[63.081886]
 [63.274433]
 [63.66308 ]
 [64.00809 ]
 [64.26439 ]], R is [[62.99780273]
 [63.00695801]
 [63.0190506 ]
 [63.03412247]
 [63.05149841]].
[2019-04-10 14:35:27,700] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0852929e-19 4.7735818e-19 3.4175836e-18 2.3823212e-17 6.6392568e-18
 6.5383564e-22 1.3582006e-21 1.0000000e+00 2.8731713e-22 2.5395154e-21
 4.1064326e-17], sum to 1.0000
[2019-04-10 14:35:27,701] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9625
[2019-04-10 14:35:27,807] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.02535185042546, 0.6178719706852277, 0.0, 1.0, 50.0, 40.40854038887043], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3805200.0000, 
sim time next is 3806400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.01846575266006, 0.6175038966943128, 0.0, 1.0, 50.0, 40.62014696274317], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6682054793883383, 0.7058346322314376, 0.0, 1.0, 0.7, 0.4062014696274317], 
reward next is 0.5938, 
noisyNet noise sample is [array([-0.85894746], dtype=float32), 1.4626782]. 
=============================================
[2019-04-10 14:35:38,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6457277e-18 7.8004025e-17 4.5174034e-17 3.8538141e-16 1.5397632e-16
 2.3636250e-20 3.9497894e-20 1.0000000e+00 1.0732582e-20 3.6504230e-20
 3.2849941e-16], sum to 1.0000
[2019-04-10 14:35:38,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-10 14:35:38,296] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-13.0, 69.0, 0.0, 0.0, 19.0, 23.93724172465254, 0.08240020860796371, 0.0, 1.0, 50.0, 41.155363419938396], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3992400.0000, 
sim time next is 3993600.0000, 
raw observation next is [-13.0, 67.0, 0.0, 0.0, 19.0, 23.81508658259503, 0.05886974285011603, 0.0, 1.0, 50.0, 41.179052934830096], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.67, 0.0, 0.0, 0.08333333333333333, 0.4845905485495858, 0.5196232476167053, 0.0, 1.0, 0.7, 0.41179052934830096], 
reward next is 0.5882, 
noisyNet noise sample is [array([0.17582127], dtype=float32), 1.0061731]. 
=============================================
[2019-04-10 14:35:51,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5543909e-18 6.2131422e-17 6.0474910e-17 1.6339941e-16 2.2386168e-16
 1.0778205e-20 1.3502406e-20 1.0000000e+00 8.7760002e-21 3.0517649e-20
 3.2506390e-16], sum to 1.0000
[2019-04-10 14:35:51,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6290
[2019-04-10 14:35:51,492] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 24.89746482682526, 0.3027747523936956, 0.0, 1.0, 50.0, 37.458699307540535], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4065600.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 24.97855225576422, 0.3021178128076887, 0.0, 1.0, 50.0, 37.662288527463225], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.581546021313685, 0.6007059376025629, 0.0, 1.0, 0.7, 0.3766228852746323], 
reward next is 0.6234, 
noisyNet noise sample is [array([0.639293], dtype=float32), 0.9768859]. 
=============================================
[2019-04-10 14:35:56,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8688705e-18 5.3931524e-17 3.5888178e-18 5.5737395e-17 1.8503494e-17
 2.3458204e-21 1.8218811e-21 1.0000000e+00 2.2813433e-22 3.1116216e-21
 7.7537952e-17], sum to 1.0000
[2019-04-10 14:35:56,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1739
[2019-04-10 14:35:57,094] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 43.0, 0.0, 0.0, 19.0, 26.68215538611019, 0.7382960007462483, 0.0, 1.0, 50.0, 33.57045540160032], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4143600.0000, 
sim time next is 4144800.0000, 
raw observation next is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 26.5590343794348, 0.7141476449058706, 0.0, 1.0, 50.0, 33.856688164600925], 
processed observation next is [1.0, 1.0, 0.4533702677747, 0.4266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7132528649528999, 0.7380492149686235, 0.0, 1.0, 0.7, 0.33856688164600923], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.4317698], dtype=float32), -0.31441498]. 
=============================================
[2019-04-10 14:36:09,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1241616e-20 4.0568669e-20 3.7923532e-19 1.0439042e-17 9.2673325e-19
 4.9359186e-22 3.4866693e-23 1.0000000e+00 2.6531352e-23 7.3882405e-22
 7.0496542e-18], sum to 1.0000
[2019-04-10 14:36:09,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7918
[2019-04-10 14:36:09,254] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 19.0, 27.58188435623862, 0.9063941266484852, 0.0, 1.0, 50.0, 31.375849167344725], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4299600.0000, 
sim time next is 4300800.0000, 
raw observation next is [6.066666666666666, 67.33333333333334, 0.0, 0.0, 19.0, 27.53475414920691, 0.8981509588944349, 0.0, 1.0, 50.0, 31.894456463480445], 
processed observation next is [0.0, 0.782608695652174, 0.6306555863342568, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.7945628457672426, 0.7993836529648116, 0.0, 1.0, 0.7, 0.3189445646348045], 
reward next is 0.6811, 
noisyNet noise sample is [array([1.5196848], dtype=float32), 1.7113124]. 
=============================================
[2019-04-10 14:36:13,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.0309164e-21 2.2181693e-19 3.0622837e-20 3.7854587e-19 8.9603730e-19
 7.7572624e-23 7.0705622e-23 1.0000000e+00 1.9509112e-23 4.6355199e-23
 9.1316645e-19], sum to 1.0000
[2019-04-10 14:36:13,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7546
[2019-04-10 14:36:13,595] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 22.5, 27.35026007612444, 0.9434292748178866, 1.0, 1.0, 50.0, 36.51759074794441], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4432800.0000, 
sim time next is 4434000.0000, 
raw observation next is [2.0, 80.0, 20.0, 38.66666666666666, 22.5, 27.42239436480563, 0.9497905776435891, 1.0, 1.0, 50.0, 33.7497334595126], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.06666666666666667, 0.04272559852670349, 0.375, 0.7851995304004692, 0.8165968592145297, 1.0, 1.0, 0.7, 0.337497334595126], 
reward next is 0.6625, 
noisyNet noise sample is [array([-1.5072827], dtype=float32), -0.9596186]. 
=============================================
[2019-04-10 14:36:13,726] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[67.84982]
 [66.25143]
 [66.14597]
 [66.07196]
 [65.97361]], R is [[68.41351318]
 [68.36420441]
 [68.33149719]
 [68.29134369]
 [68.25744629]].
[2019-04-10 14:36:17,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8091379e-21 2.6355068e-20 1.3797214e-20 4.6213692e-19 4.4014368e-20
 1.3988242e-23 1.0321289e-23 1.0000000e+00 2.3759506e-24 2.3502339e-23
 4.4650082e-19], sum to 1.0000
[2019-04-10 14:36:17,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-10 14:36:17,856] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 22.5, 27.15184816155457, 0.7984754768746671, 1.0, 1.0, 50.0, 33.62566603337926], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4527600.0000, 
sim time next is 4528800.0000, 
raw observation next is [1.0, 61.0, 172.0, 9.0, 22.5, 27.29284816773413, 0.8288568121109875, 1.0, 1.0, 50.0, 31.342082881494527], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.61, 0.5733333333333334, 0.009944751381215469, 0.375, 0.7744040139778443, 0.7762856040369958, 1.0, 1.0, 0.7, 0.31342082881494526], 
reward next is 0.6866, 
noisyNet noise sample is [array([0.11503702], dtype=float32), -0.24282208]. 
=============================================
[2019-04-10 14:36:18,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4443039e-20 9.7319598e-19 2.5386837e-20 4.7563340e-18 2.9073446e-18
 2.5268442e-23 1.5166445e-22 1.0000000e+00 2.4654210e-23 3.6250385e-22
 2.3190705e-17], sum to 1.0000
[2019-04-10 14:36:18,663] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4951
[2019-04-10 14:36:18,731] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.12283033586, 0.9283126274814361, 1.0, 1.0, 50.0, 36.554496643557435], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4478400.0000, 
sim time next is 4479600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.05522798833784, 0.9167853961984153, 0.0, 1.0, 50.0, 37.16307644367748], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7546023323614867, 0.8055951320661384, 0.0, 1.0, 0.7, 0.37163076443677484], 
reward next is 0.6284, 
noisyNet noise sample is [array([0.4398753], dtype=float32), 0.25108564]. 
=============================================
[2019-04-10 14:36:20,432] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1643650e-19 2.7640230e-19 5.5179072e-18 3.5039180e-18 2.8398891e-17
 1.3362863e-22 3.0130796e-22 1.0000000e+00 2.6875961e-23 1.5319488e-22
 1.0480859e-17], sum to 1.0000
[2019-04-10 14:36:20,434] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0563
[2019-04-10 14:36:20,466] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 19.0, 26.4487147551981, 0.6910539747294949, 0.0, 1.0, 50.0, 32.173216007205596], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4586400.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 19.0, 26.44967693111909, 0.6890236618939484, 0.0, 1.0, 50.0, 32.19772310922851], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7041397442599241, 0.7296745539646494, 0.0, 1.0, 0.7, 0.3219772310922851], 
reward next is 0.6780, 
noisyNet noise sample is [array([-1.7396802], dtype=float32), -0.06873859]. 
=============================================
[2019-04-10 14:36:23,300] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1944030e-18 1.1114343e-16 1.2974759e-17 3.6998060e-16 7.5809429e-17
 1.6680625e-20 1.8177863e-20 1.0000000e+00 2.9373274e-21 3.7028485e-20
 1.1384300e-15], sum to 1.0000
[2019-04-10 14:36:23,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5073
[2019-04-10 14:36:23,353] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 19.0, 26.53235392520571, 0.7734476902275119, 0.0, 1.0, 50.0, 39.38226512516826], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4489200.0000, 
sim time next is 4490400.0000, 
raw observation next is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 26.45249748256472, 0.7574595868439489, 0.0, 1.0, 50.0, 39.44774745723557], 
processed observation next is [1.0, 1.0, 0.4524469067405356, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7043747902137266, 0.752486528947983, 0.0, 1.0, 0.7, 0.3944774745723557], 
reward next is 0.6055, 
noisyNet noise sample is [array([-0.5132731], dtype=float32), -2.0032928]. 
=============================================
[2019-04-10 14:36:26,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3831642e-19 5.4663267e-18 2.5458324e-19 7.4592554e-17 1.0383673e-18
 5.1891033e-22 5.3142063e-22 1.0000000e+00 1.2080356e-22 1.1558470e-21
 3.1242459e-18], sum to 1.0000
[2019-04-10 14:36:26,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0406
[2019-04-10 14:36:26,498] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.17369662905708, 0.8832047559022073, 1.0, 1.0, 50.0, 30.49147277681209], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4564800.0000, 
sim time next is 4566000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 27.13550089471742, 0.8770398491295858, 0.0, 1.0, 50.0, 30.105736491337495], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7612917412264517, 0.7923466163765286, 0.0, 1.0, 0.7, 0.301057364913375], 
reward next is 0.6989, 
noisyNet noise sample is [array([-1.5294175], dtype=float32), 0.79813796]. 
=============================================
[2019-04-10 14:36:26,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.75229 ]
 [68.37056 ]
 [67.63256 ]
 [68.412155]
 [68.40747 ]], R is [[67.30633545]
 [67.32835388]
 [67.34039307]
 [67.36903381]
 [67.39404297]].
[2019-04-10 14:36:28,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3484497e-20 6.7683935e-19 8.4717667e-20 6.0497029e-18 1.3623533e-19
 9.5342461e-23 1.3476816e-22 1.0000000e+00 1.4391540e-23 2.0522822e-22
 5.8588553e-18], sum to 1.0000
[2019-04-10 14:36:28,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2240
[2019-04-10 14:36:29,061] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.3333333333333333, 68.33333333333334, 121.0, 11.0, 22.5, 27.10459975688101, 0.7762882768666736, 1.0, 1.0, 50.0, 33.4451266800189], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4526400.0000, 
sim time next is 4527600.0000, 
raw observation next is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 22.5, 27.15184816155457, 0.7984754768746671, 1.0, 1.0, 50.0, 33.62566603337926], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.6466666666666667, 0.46444444444444427, 0.0033149171270718224, 0.375, 0.762654013462881, 0.7661584922915557, 1.0, 1.0, 0.7, 0.3362566603337926], 
reward next is 0.6637, 
noisyNet noise sample is [array([-0.48512605], dtype=float32), -0.41086057]. 
=============================================
[2019-04-10 14:36:34,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4347771e-19 7.1436683e-19 1.5374257e-18 2.3043537e-16 9.1437553e-20
 1.7879382e-21 1.8530078e-22 1.0000000e+00 2.1840264e-22 4.6288454e-21
 7.0001317e-18], sum to 1.0000
[2019-04-10 14:36:34,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3487
[2019-04-10 14:36:34,062] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.110223024625157e-16, 44.0, 130.0, 830.3333333333334, 19.0, 25.37268766098756, 0.5202743459948335, 0.0, 1.0, 50.0, 32.715571790034765], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4794000.0000, 
sim time next is 4795200.0000, 
raw observation next is [1.0, 43.0, 146.0, 813.0, 19.0, 25.50436017247686, 0.5502173691045487, 0.0, 1.0, 50.0, 32.23271153548048], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.43, 0.4866666666666667, 0.8983425414364641, 0.08333333333333333, 0.6253633477064051, 0.6834057897015162, 0.0, 1.0, 0.7, 0.32232711535480485], 
reward next is 0.6777, 
noisyNet noise sample is [array([0.06509176], dtype=float32), -1.0369786]. 
=============================================
[2019-04-10 14:36:38,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6202252e-18 3.6354618e-17 3.1302361e-17 2.2459558e-16 1.3892624e-16
 1.2637928e-19 2.0142397e-21 1.0000000e+00 1.4004702e-21 4.2244647e-20
 2.1130445e-17], sum to 1.0000
[2019-04-10 14:36:38,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1969
[2019-04-10 14:36:38,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 24.7535686164563, 0.2554775715169117, 0.0, 1.0, 50.0, 36.895881722150904], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4861200.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 24.71779268001576, 0.2445123295642392, 0.0, 1.0, 50.0, 36.92812030545031], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.5598160566679798, 0.5815041098547464, 0.0, 1.0, 0.7, 0.36928120305450307], 
reward next is 0.6307, 
noisyNet noise sample is [array([1.5398983], dtype=float32), -0.32976997]. 
=============================================
[2019-04-10 14:36:41,655] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4765329e-17 8.7961989e-16 1.2990415e-16 1.4875372e-14 5.0333798e-17
 5.0355638e-19 1.3335691e-19 1.0000000e+00 5.2756329e-20 2.2982237e-18
 3.6436830e-16], sum to 1.0000
[2019-04-10 14:36:41,655] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1998
[2019-04-10 14:36:41,682] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.0, 34.0, 57.5, 367.0, 19.0, 27.03445131982693, 0.8385969326994215, 0.0, 1.0, 50.0, 29.807679464073942], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4813200.0000, 
sim time next is 4814400.0000, 
raw observation next is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 19.0, 27.03576295445989, 0.8327194160048443, 0.0, 1.0, 50.0, 30.1339294010539], 
processed observation next is [0.0, 0.7391304347826086, 0.5364727608494922, 0.36, 0.1372222222222222, 0.27145488029465936, 0.08333333333333333, 0.7529802462049909, 0.7775731386682815, 0.0, 1.0, 0.7, 0.301339294010539], 
reward next is 0.6987, 
noisyNet noise sample is [array([-0.62410337], dtype=float32), 2.2319455]. 
=============================================
[2019-04-10 14:36:52,321] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:52,574] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:52,911] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:53,113] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:53,289] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:53,289] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:53,296] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run5
[2019-04-10 14:36:53,895] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:53,895] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:53,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run5
[2019-04-10 14:36:54,153] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:54,435] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:55,152] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:55,152] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:55,154] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run5
[2019-04-10 14:36:56,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0624702e-21 3.5857370e-19 1.1665050e-20 6.3193170e-18 1.1029632e-21
 3.5643733e-23 1.4399611e-23 1.0000000e+00 3.3292486e-24 2.1023296e-22
 4.2041122e-20], sum to 1.0000
[2019-04-10 14:36:56,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3381
[2019-04-10 14:36:56,138] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 22.5, 28.6756147020015, 1.02172040205853, 1.0, 1.0, 50.0, 21.56304442301814], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4977600.0000, 
sim time next is 4978800.0000, 
raw observation next is [8.0, 26.0, 100.5, 796.5, 22.5, 27.66239689852978, 1.068531177261517, 1.0, 1.0, 50.0, 8.85163378542711], 
processed observation next is [1.0, 0.6521739130434783, 0.6842105263157896, 0.26, 0.335, 0.8801104972375691, 0.375, 0.8051997415441484, 0.8561770590871722, 1.0, 1.0, 0.7, 0.0885163378542711], 
reward next is 0.9115, 
noisyNet noise sample is [array([0.6147232], dtype=float32), -1.7084274]. 
=============================================
[2019-04-10 14:36:56,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:57,148] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:57,910] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:57,910] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:57,937] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run5
[2019-04-10 14:36:58,132] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:58,375] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:58,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:58,914] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:59,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:59,133] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:59,136] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run5
[2019-04-10 14:36:59,369] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:59,669] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:36:59,669] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:36:59,671] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run5
[2019-04-10 14:36:59,720] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:36:59,731] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:36:59,919] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:00,129] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:00,368] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:00,368] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:00,370] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run5
[2019-04-10 14:37:00,413] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:00,580] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:00,745] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:00,745] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:00,747] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run5
[2019-04-10 14:37:00,882] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:01,115] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:01,116] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:01,117] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run5
[2019-04-10 14:37:01,564] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:01,564] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:01,566] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run5
[2019-04-10 14:37:02,732] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:02,871] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:02,916] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:03,078] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,242] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,242] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,396] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:03,405] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:03,522] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_3 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:37:03,588] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,719] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:03,719] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:03,721] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run5
[2019-04-10 14:37:03,776] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,813] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_3 ERROR:Aborted (core dumped)

[2019-04-10 14:37:03,873] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:03,873] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:03,875] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run5
[2019-04-10 14:37:03,945] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:03,945] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:03,948] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run5
[2019-04-10 14:37:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:04,399] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run5
[2019-04-10 14:37:04,457] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:04,457] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:04,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run5
[2019-04-10 14:37:04,533] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 14:37:04,533] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:04,535] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run5
[2019-04-10 14:37:08,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1112545e-18 2.3976743e-17 1.1156853e-18 4.4407899e-17 2.7361493e-19
 1.9626775e-21 9.5136101e-23 1.0000000e+00 5.1100740e-23 9.7167404e-21
 2.4242814e-17], sum to 1.0000
[2019-04-10 14:37:08,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9141
[2019-04-10 14:37:08,331] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 22.5, 23.16513133184509, -0.1394799237870904, 1.0, 1.0, 50.0, 44.44915569036223], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 118800.0000, 
sim time next is 120000.0000, 
raw observation next is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 22.5, 23.29240569336269, -0.1023427216681208, 1.0, 1.0, 50.0, 44.60266750295006], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6533333333333334, 0.14555555555555552, 0.0016574585635359116, 0.375, 0.44103380778022405, 0.46588575944395977, 1.0, 1.0, 0.7, 0.4460266750295006], 
reward next is 0.5540, 
noisyNet noise sample is [array([1.2035528], dtype=float32), 1.6461382]. 
=============================================
[2019-04-10 14:37:08,335] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.32025 ]
 [64.37808 ]
 [63.921143]
 [63.72274 ]
 [62.37762 ]], R is [[64.3013916 ]
 [64.21389008]
 [64.12827301]
 [64.0401535 ]
 [63.95837784]].
[2019-04-10 14:37:13,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.90429764e-17 3.16089436e-17 5.19779121e-18 7.36586257e-16
 7.87916727e-19 1.02649204e-20 2.52339847e-21 1.00000000e+00
 1.56154410e-22 1.97192504e-19 1.00938931e-16], sum to 1.0000
[2019-04-10 14:37:13,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1244
[2019-04-10 14:37:13,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3321300e-18 5.4211370e-19 1.0037175e-17 7.7139965e-17 2.5777206e-18
 3.4757984e-20 9.6436157e-23 1.0000000e+00 1.3170001e-22 9.5136199e-21
 2.0858391e-17], sum to 1.0000
[2019-04-10 14:37:13,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0684
[2019-04-10 14:37:13,644] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 22.6608414823415, -0.2201909539803889, 0.0, 1.0, 50.0, 37.207645944191924], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 15600.0000, 
sim time next is 16800.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 22.67388101616481, -0.2158909751539319, 0.0, 1.0, 50.0, 37.082333698030965], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.3894900846804008, 0.42803634161535603, 0.0, 1.0, 0.7, 0.37082333698030967], 
reward next is 0.6292, 
noisyNet noise sample is [array([-0.9283644], dtype=float32), -1.443646]. 
=============================================
[2019-04-10 14:37:13,650] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.3, 75.0, 101.5, 0.0, 22.5, 23.83668180370336, -0.07010928058881992, 1.0, 1.0, 50.0, 51.473590573844305], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 208800.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 24.10293571320952, -0.02453114085687771, 1.0, 1.0, 50.0, 51.151092865616576], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 0.375, 0.5085779761007932, 0.4918229530477074, 1.0, 1.0, 0.7, 0.5115109286561658], 
reward next is 0.4885, 
noisyNet noise sample is [array([0.97583795], dtype=float32), -0.13438648]. 
=============================================
[2019-04-10 14:37:13,654] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[62.57255]
 [62.73133]
 [62.91074]
 [63.14588]
 [63.44832]], R is [[62.18186951]
 [62.04531479]
 [61.9078598 ]
 [61.77024078]
 [61.63436127]].
[2019-04-10 14:37:18,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0171737e-15 9.2062181e-15 1.0103100e-16 1.6964030e-13 1.4789750e-16
 6.1619430e-18 1.3417436e-19 1.0000000e+00 3.0539078e-20 1.8720394e-17
 5.3869275e-15], sum to 1.0000
[2019-04-10 14:37:18,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1139
[2019-04-10 14:37:18,761] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.8, 86.0, 187.0, 24.5, 22.5, 23.86674668679184, 0.01738636299708483, 1.0, 1.0, 50.0, 43.08826085640882], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 126000.0000, 
sim time next is 127200.0000, 
raw observation next is [-8.0, 77.66666666666667, 185.0, 16.83333333333333, 22.5, 23.93433322781647, 0.0322963507614927, 1.0, 1.0, 50.0, 42.84017111830265], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.7766666666666667, 0.6166666666666667, 0.018600368324125226, 0.375, 0.49452776898470585, 0.5107654502538309, 1.0, 1.0, 0.7, 0.4284017111830265], 
reward next is 0.5716, 
noisyNet noise sample is [array([2.9521658], dtype=float32), 0.026367487]. 
=============================================
[2019-04-10 14:37:25,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4965122e-15 2.3546214e-16 7.3851036e-17 3.0316370e-14 9.0824762e-18
 6.9185211e-19 3.5919917e-20 1.0000000e+00 1.9674675e-21 3.2664737e-17
 2.2830492e-15], sum to 1.0000
[2019-04-10 14:37:25,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7870
[2019-04-10 14:37:25,660] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.833333333333334, 65.0, 133.0, 0.0, 22.5, 25.07633481176258, 0.09883883534341568, 1.0, 1.0, 50.0, 50.100633438798226], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 217200.0000, 
sim time next is 218400.0000, 
raw observation next is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 22.5, 24.13141779357282, 0.1264140980970954, 1.0, 1.0, 50.0, 50.16578133583782], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333333, 0.65, 0.44111111111111095, 0.0, 0.375, 0.510951482797735, 0.5421380326990318, 1.0, 1.0, 0.7, 0.5016578133583782], 
reward next is 0.4983, 
noisyNet noise sample is [array([0.86615103], dtype=float32), 0.7807418]. 
=============================================
[2019-04-10 14:37:29,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.92130102e-15 4.53529833e-14 3.92955419e-16 1.56293679e-13
 1.02142406e-16 1.33598387e-17 7.34668526e-19 1.00000000e+00
 8.87535587e-20 2.55292580e-16 1.79906224e-14], sum to 1.0000
[2019-04-10 14:37:29,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5847
[2019-04-10 14:37:29,800] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 24.10293571320952, -0.02453114085687771, 1.0, 1.0, 50.0, 51.151092865616576], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 210000.0000, 
sim time next is 211200.0000, 
raw observation next is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 22.5, 24.26083026140775, 0.01522235706723468, 1.0, 1.0, 50.0, 50.85845988421423], 
processed observation next is [1.0, 0.43478260869565216, 0.28070175438596495, 0.73, 0.4294444444444443, 0.0, 0.375, 0.5217358551173126, 0.5050741190224116, 1.0, 1.0, 0.7, 0.5085845988421422], 
reward next is 0.4914, 
noisyNet noise sample is [array([-1.2068688], dtype=float32), -0.026641924]. 
=============================================
[2019-04-10 14:37:41,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.50500603e-17 3.25016939e-15 1.58147550e-16 1.46283369e-14
 1.60138831e-15 6.36253885e-19 2.79097744e-19 1.00000000e+00
 3.01534966e-19 1.25430297e-18 1.39765984e-14], sum to 1.0000
[2019-04-10 14:37:41,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7231
[2019-04-10 14:37:41,607] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-16.53333333333333, 80.0, 0.0, 0.0, 22.5, 20.56296236069137, -0.7780556340471683, 1.0, 1.0, 50.0, 61.63621637719397], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 373200.0000, 
sim time next is 374400.0000, 
raw observation next is [-16.7, 81.0, 12.5, 263.5, 22.5, 20.58457149143824, -0.7348896914011421, 0.0, 1.0, 50.0, 58.74087937646951], 
processed observation next is [1.0, 0.34782608695652173, 0.0, 0.81, 0.041666666666666664, 0.29116022099447514, 0.375, 0.21538095761985332, 0.2550367695329526, 0.0, 1.0, 0.7, 0.5874087937646951], 
reward next is 0.4126, 
noisyNet noise sample is [array([-0.47304326], dtype=float32), 0.96390384]. 
=============================================
[2019-04-10 14:37:45,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0059715e-17 6.0966456e-16 9.1816935e-17 2.7596339e-16 1.1219732e-16
 9.7474110e-20 5.4084723e-20 1.0000000e+00 4.3307038e-20 1.3115924e-19
 2.6482521e-15], sum to 1.0000
[2019-04-10 14:37:45,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5889
[2019-04-10 14:37:45,864] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 19.0, 22.58112601567358, -0.3315440817100037, 0.0, 1.0, 50.0, 43.947026917254334], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 444000.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 19.0, 22.42029990093197, -0.3629695957336441, 0.0, 1.0, 50.0, 44.137125939525035], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.08333333333333333, 0.3683583250776641, 0.379010134755452, 0.0, 1.0, 0.7, 0.44137125939525035], 
reward next is 0.5586, 
noisyNet noise sample is [array([-0.05640136], dtype=float32), -0.2735936]. 
=============================================
[2019-04-10 14:37:52,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9840840e-18 6.9881555e-17 2.2184244e-18 2.0448645e-16 3.8179769e-17
 1.0641792e-20 5.3091231e-21 1.0000000e+00 1.2408945e-21 3.1196102e-20
 1.2223312e-15], sum to 1.0000
[2019-04-10 14:37:52,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6620
[2019-04-10 14:37:52,758] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 19.0, 25.42169444707796, 0.367185488647546, 0.0, 1.0, 50.0, 36.70467484217703], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 518400.0000, 
sim time next is 519600.0000, 
raw observation next is [4.2, 94.33333333333334, 0.0, 0.0, 19.0, 25.36699431194358, 0.358365232708341, 0.0, 1.0, 50.0, 36.71216303430081], 
processed observation next is [0.0, 0.0, 0.5789473684210527, 0.9433333333333335, 0.0, 0.0, 0.08333333333333333, 0.6139161926619648, 0.6194550775694471, 0.0, 1.0, 0.7, 0.3671216303430081], 
reward next is 0.6329, 
noisyNet noise sample is [array([-1.6873595], dtype=float32), -1.174006]. 
=============================================
[2019-04-10 14:37:54,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1442403e-18 1.1069106e-17 5.3550365e-19 1.2329611e-16 1.1721712e-18
 4.2315430e-21 2.6737836e-21 1.0000000e+00 8.8957519e-22 2.2257374e-20
 4.1563618e-16], sum to 1.0000
[2019-04-10 14:37:54,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1418
[2019-04-10 14:37:54,706] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.5, 23.74800065443623, -0.09193731790208581, 1.0, 1.0, 50.0, 38.76780760168111], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 816000.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 22.5, 23.84861099193852, -0.0671364992208963, 1.0, 1.0, 50.0, 38.46274498118286], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.375, 0.4873842493282101, 0.4776211669263679, 1.0, 1.0, 0.7, 0.38462744981182856], 
reward next is 0.6154, 
noisyNet noise sample is [array([-0.9283731], dtype=float32), -1.2230432]. 
=============================================
[2019-04-10 14:37:57,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2013216e-18 8.9191477e-18 2.9103533e-17 5.7989563e-17 2.1903145e-17
 1.0217532e-20 8.2110827e-21 1.0000000e+00 7.0358876e-21 1.7099726e-20
 3.9729180e-16], sum to 1.0000
[2019-04-10 14:37:57,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9549
[2019-04-10 14:37:57,418] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.2, 72.0, 0.0, 0.0, 19.0, 22.68562560691975, -0.2643186956894022, 0.0, 1.0, 50.0, 37.77868126525753], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 884400.0000, 
sim time next is 885600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 22.57988803885552, -0.2705508707537702, 0.0, 1.0, 50.0, 37.70750779425991], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.3816573365712932, 0.4098163764154099, 0.0, 1.0, 0.7, 0.3770750779425991], 
reward next is 0.6229, 
noisyNet noise sample is [array([-0.9732149], dtype=float32), 1.87518]. 
=============================================
[2019-04-10 14:37:57,719] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.74172108e-17 3.11836569e-17 1.02565598e-17 2.73217968e-16
 1.95030693e-17 1.15134636e-19 8.68696840e-21 1.00000000e+00
 1.19131045e-20 1.05171737e-19 1.78002121e-16], sum to 1.0000
[2019-04-10 14:37:57,719] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1371
[2019-04-10 14:37:57,767] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 22.51532848964503, -0.3141526935833661, 0.0, 1.0, 50.0, 40.6554086245943], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 679200.0000, 
sim time next is 680400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 22.49426394755879, -0.3172600552602563, 0.0, 1.0, 50.0, 40.60790288704166], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.3745219956298991, 0.39424664824658123, 0.0, 1.0, 0.7, 0.4060790288704166], 
reward next is 0.5939, 
noisyNet noise sample is [array([-1.6705946], dtype=float32), -1.457126]. 
=============================================
[2019-04-10 14:37:59,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8030055e-18 2.1637079e-16 4.2049687e-17 1.9444552e-15 2.5138260e-16
 6.7489246e-20 1.6590141e-19 1.0000000e+00 8.9023808e-20 3.4632266e-19
 4.6070859e-15], sum to 1.0000
[2019-04-10 14:37:59,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7934
[2019-04-10 14:37:59,435] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.366666666666667, 65.0, 0.0, 0.0, 19.0, 23.19665245890681, -0.08256633965922815, 0.0, 1.0, 50.0, 40.52852530845314], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 771600.0000, 
sim time next is 772800.0000, 
raw observation next is [-6.533333333333333, 66.0, 0.0, 0.0, 19.0, 23.12647090819473, -0.09868669382475549, 0.0, 1.0, 50.0, 40.425378899123245], 
processed observation next is [1.0, 0.9565217391304348, 0.2816251154201293, 0.66, 0.0, 0.0, 0.08333333333333333, 0.42720590901622746, 0.4671044353917482, 0.0, 1.0, 0.7, 0.40425378899123243], 
reward next is 0.5957, 
noisyNet noise sample is [array([0.05720966], dtype=float32), 0.05104439]. 
=============================================
[2019-04-10 14:38:04,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0946825e-19 3.7176851e-18 1.0794675e-18 2.5096279e-17 6.7693264e-18
 3.5909623e-21 9.9434440e-22 1.0000000e+00 1.0524387e-21 3.8830673e-21
 5.7383974e-17], sum to 1.0000
[2019-04-10 14:38:04,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5545
[2019-04-10 14:38:04,595] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 19.0, 22.53401346460721, -0.2648148236305536, 0.0, 1.0, 50.0, 38.12251737078627], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 876000.0000, 
sim time next is 877200.0000, 
raw observation next is [-1.366666666666667, 77.0, 0.0, 0.0, 19.0, 22.58584553966945, -0.2603891225764752, 0.0, 1.0, 50.0, 38.03574440194025], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.77, 0.0, 0.0, 0.08333333333333333, 0.3821537949724541, 0.4132036258078416, 0.0, 1.0, 0.7, 0.38035744401940247], 
reward next is 0.6196, 
noisyNet noise sample is [array([0.06257498], dtype=float32), 0.20762782]. 
=============================================
[2019-04-10 14:38:09,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8064564e-20 6.3097751e-19 2.1508247e-20 8.3329273e-18 1.3653187e-19
 4.5112589e-23 2.5124760e-22 1.0000000e+00 2.6218753e-23 3.6194561e-22
 1.5095093e-16], sum to 1.0000
[2019-04-10 14:38:09,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2958
[2019-04-10 14:38:09,132] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 22.5, 24.82715029067985, 0.3018262868184944, 1.0, 1.0, 50.0, 35.05704393667584], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 926400.0000, 
sim time next is 927600.0000, 
raw observation next is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 22.5, 25.05263510151675, 0.3218713681475434, 1.0, 1.0, 50.0, 34.67496207553707], 
processed observation next is [1.0, 0.7391304347826086, 0.5900277008310251, 0.9466666666666668, 0.0, 0.0, 0.375, 0.5877195917930624, 0.6072904560491811, 1.0, 1.0, 0.7, 0.3467496207553707], 
reward next is 0.6533, 
noisyNet noise sample is [array([-0.00575269], dtype=float32), 1.7117825]. 
=============================================
[2019-04-10 14:38:09,555] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2131901e-21 6.9548457e-21 1.7812063e-20 2.1287461e-19 4.8810616e-20
 3.5979910e-24 1.4827942e-23 1.0000000e+00 2.4756262e-24 8.9219790e-24
 4.8544919e-19], sum to 1.0000
[2019-04-10 14:38:09,557] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5083
[2019-04-10 14:38:09,586] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [9.8, 86.33333333333334, 0.0, 0.0, 22.5, 24.84026512943292, 0.3120701119045374, 1.0, 1.0, 50.0, 34.82445734529744], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 976800.0000, 
sim time next is 978000.0000, 
raw observation next is [9.600000000000001, 89.66666666666667, 0.0, 0.0, 22.5, 24.8237171926916, 0.3124115242753743, 1.0, 1.0, 50.0, 34.88159133989183], 
processed observation next is [1.0, 0.30434782608695654, 0.7285318559556788, 0.8966666666666667, 0.0, 0.0, 0.375, 0.5686430993909667, 0.6041371747584581, 1.0, 1.0, 0.7, 0.34881591339891826], 
reward next is 0.6512, 
noisyNet noise sample is [array([1.3032724], dtype=float32), -1.0026939]. 
=============================================
[2019-04-10 14:38:09,603] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[71.27531 ]
 [70.771515]
 [70.6382  ]
 [70.52733 ]
 [70.44648 ]], R is [[71.01353455]
 [70.95515442]
 [70.89686584]
 [70.83866119]
 [70.78144836]].
[2019-04-10 14:38:12,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0794376e-20 6.1736541e-20 2.9307717e-19 7.2341928e-19 3.4098075e-19
 1.0956868e-22 2.4632399e-23 1.0000000e+00 5.8562932e-23 1.5874726e-22
 1.0035410e-18], sum to 1.0000
[2019-04-10 14:38:12,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6306
[2019-04-10 14:38:12,820] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [10.16666666666667, 78.33333333333333, 0.0, 0.0, 19.0, 27.6635322808261, 1.125714183247155, 0.0, 1.0, 50.0, 28.70224035795023], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1129200.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 19.0, 27.63814704005815, 1.119837990594431, 0.0, 1.0, 50.0, 29.30152470268196], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.08333333333333333, 0.8031789200048459, 0.8732793301981436, 0.0, 1.0, 0.7, 0.2930152470268196], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.9291902], dtype=float32), -2.024771]. 
=============================================
[2019-04-10 14:38:12,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0005830e-22 7.2002738e-19 3.6025836e-21 2.5675827e-18 1.8330229e-19
 1.8794393e-23 3.8984988e-23 1.0000000e+00 3.4821878e-24 1.6877364e-22
 6.0784797e-18], sum to 1.0000
[2019-04-10 14:38:12,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2132
[2019-04-10 14:38:12,963] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.0, 86.0, 121.3333333333333, 0.0, 22.5, 26.94023107476806, 0.7728172627534043, 1.0, 1.0, 50.0, 31.19020309972243], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 992400.0000, 
sim time next is 993600.0000, 
raw observation next is [12.2, 86.0, 124.0, 0.0, 22.5, 27.02717442777988, 0.8027599938213368, 1.0, 1.0, 50.0, 30.50548328398486], 
processed observation next is [1.0, 0.5217391304347826, 0.8005540166204987, 0.86, 0.41333333333333333, 0.0, 0.375, 0.7522645356483233, 0.7675866646071122, 1.0, 1.0, 0.7, 0.3050548328398486], 
reward next is 0.6949, 
noisyNet noise sample is [array([-0.5775208], dtype=float32), -0.35717082]. 
=============================================
[2019-04-10 14:38:23,308] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.68682474e-20 9.97327464e-20 1.10262715e-20 5.23382403e-18
 2.03445750e-20 4.34472906e-23 6.27004870e-24 1.00000000e+00
 7.29244241e-24 1.77306939e-22 1.26516886e-17], sum to 1.0000
[2019-04-10 14:38:23,308] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-10 14:38:23,335] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 19.0, 27.44246775437475, 1.095691055063847, 0.0, 1.0, 50.0, 34.703031674814945], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1294800.0000, 
sim time next is 1296000.0000, 
raw observation next is [4.4, 96.0, 0.0, 0.0, 19.0, 27.38827532646658, 1.084943247621848, 0.0, 1.0, 50.0, 35.016177684230314], 
processed observation next is [1.0, 0.0, 0.5844875346260389, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7823562772055483, 0.8616477492072826, 0.0, 1.0, 0.7, 0.35016177684230315], 
reward next is 0.6498, 
noisyNet noise sample is [array([0.01082008], dtype=float32), -1.0661286]. 
=============================================
[2019-04-10 14:38:23,341] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[67.73796]
 [67.85125]
 [67.9505 ]
 [67.96465]
 [67.99304]], R is [[67.81630707]
 [67.79111481]
 [67.77661896]
 [67.7631073 ]
 [67.76008606]].
[2019-04-10 14:38:24,161] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4797346e-19 1.1269011e-17 5.2138716e-20 2.2353130e-17 3.0641682e-18
 4.3891090e-22 2.6746186e-22 1.0000000e+00 1.0967740e-22 6.2908301e-21
 2.0384003e-16], sum to 1.0000
[2019-04-10 14:38:24,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2139
[2019-04-10 14:38:24,196] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 22.5, 26.96100551136471, 0.9162059351315124, 1.0, 1.0, 50.0, 37.12685952175951], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1324800.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 22.5, 26.91997196300557, 0.9564988069774979, 1.0, 1.0, 50.0, 34.31838583592316], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 0.375, 0.7433309969171308, 0.818832935659166, 1.0, 1.0, 0.7, 0.3431838583592316], 
reward next is 0.6568, 
noisyNet noise sample is [array([-1.6637346], dtype=float32), 0.53378445]. 
=============================================
[2019-04-10 14:38:24,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[66.13911 ]
 [65.63246 ]
 [65.01425 ]
 [63.811337]
 [63.873974]], R is [[66.52526093]
 [66.48873901]
 [66.45588684]
 [66.42160034]
 [66.38919067]].
[2019-04-10 14:38:28,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9192788e-19 2.5958291e-17 1.2969803e-18 8.2866510e-17 6.2595528e-18
 2.3605691e-21 3.7131439e-21 1.0000000e+00 1.2586340e-21 2.1530576e-20
 2.1743329e-15], sum to 1.0000
[2019-04-10 14:38:28,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6720
[2019-04-10 14:38:28,466] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 22.5, 25.7877697327211, 0.6264959199393335, 1.0, 1.0, 50.0, 35.725637970205305], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1408800.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 22.5, 25.86219211251193, 0.6198247784242797, 1.0, 1.0, 50.0, 35.71121653143132], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.375, 0.6551826760426609, 0.7066082594747599, 1.0, 1.0, 0.7, 0.35711216531431317], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.38460946], dtype=float32), -1.0387915]. 
=============================================
[2019-04-10 14:38:28,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[60.114616]
 [58.803738]
 [58.836704]
 [58.886955]
 [58.955063]], R is [[60.82027054]
 [60.85480881]
 [60.88912964]
 [60.92370224]
 [60.95907974]].
[2019-04-10 14:38:28,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.30324755e-19 6.36674565e-17 4.13490915e-20 5.47858363e-17
 4.87672505e-20 3.59477074e-22 1.76211912e-22 1.00000000e+00
 1.06195975e-22 1.17717075e-20 3.10731204e-16], sum to 1.0000
[2019-04-10 14:38:28,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8965
[2019-04-10 14:38:28,907] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 22.5, 27.33622030889822, 0.8941559201891242, 1.0, 1.0, 50.0, 27.671836099911964], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1436400.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 22.5, 27.44694585337922, 0.9203359130463967, 1.0, 1.0, 50.0, 30.677685390444235], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.375, 0.7872454877816016, 0.8067786376821323, 1.0, 1.0, 0.7, 0.30677685390444237], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.33160374], dtype=float32), 0.038502242]. 
=============================================
[2019-04-10 14:38:29,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.5160732e-17 3.6949261e-16 2.0524909e-17 2.9489791e-15 8.3672376e-17
 2.7634326e-19 3.7450105e-20 1.0000000e+00 8.3980184e-20 7.1310083e-19
 5.8151112e-15], sum to 1.0000
[2019-04-10 14:38:29,745] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4230
[2019-04-10 14:38:29,793] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 19.0, 24.19378383196808, 0.1862210423589788, 0.0, 1.0, 50.0, 42.68048840862683], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1796400.0000, 
sim time next is 1797600.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 19.0, 24.14237972847425, 0.1737723360975594, 0.0, 1.0, 50.0, 42.75061701068781], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.08333333333333333, 0.5118649773728542, 0.5579241120325198, 0.0, 1.0, 0.7, 0.42750617010687814], 
reward next is 0.5725, 
noisyNet noise sample is [array([-0.7075853], dtype=float32), -0.6900787]. 
=============================================
[2019-04-10 14:38:31,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.35753829e-20 4.95173269e-18 2.04416725e-19 2.42212628e-18
 1.29554144e-18 1.00892041e-22 4.39958607e-22 1.00000000e+00
 3.11489649e-23 1.30944574e-22 1.22920826e-17], sum to 1.0000
[2019-04-10 14:38:31,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2454
[2019-04-10 14:38:31,385] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 19.0, 25.9169455434107, 0.6342646825882297, 0.0, 1.0, 50.0, 33.13916121000605], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1480800.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 19.0, 26.00897602216482, 0.6258642246837249, 0.0, 1.0, 50.0, 32.904235134498656], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.667414668513735, 0.7086214082279083, 0.0, 1.0, 0.7, 0.32904235134498655], 
reward next is 0.6710, 
noisyNet noise sample is [array([0.3997355], dtype=float32), -0.7831001]. 
=============================================
[2019-04-10 14:38:31,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.291374]
 [68.268616]
 [68.269394]
 [68.28771 ]
 [68.29861 ]], R is [[68.28888702]
 [68.2746048 ]
 [68.26409912]
 [68.25410461]
 [68.24344635]].
[2019-04-10 14:38:40,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4775652e-16 4.9676682e-16 4.4965512e-16 5.2878790e-14 2.1193907e-16
 4.4244874e-18 9.9784162e-19 1.0000000e+00 3.8652627e-19 1.2735652e-17
 4.3802117e-14], sum to 1.0000
[2019-04-10 14:38:40,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7560
[2019-04-10 14:38:40,482] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 84.33333333333333, 0.0, 0.0, 19.0, 25.51854941637818, 0.5370912523843728, 0.0, 1.0, 50.0, 40.42702961734193], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1744800.0000, 
sim time next is 1746000.0000, 
raw observation next is [-0.6, 83.0, 0.0, 0.0, 19.0, 25.47255563988811, 0.5256995630684366, 0.0, 1.0, 50.0, 40.537726652981036], 
processed observation next is [0.0, 0.21739130434782608, 0.44598337950138506, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6227129699906758, 0.6752331876894789, 0.0, 1.0, 0.7, 0.4053772665298104], 
reward next is 0.5946, 
noisyNet noise sample is [array([-0.5545384], dtype=float32), -0.5764814]. 
=============================================
[2019-04-10 14:38:40,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[52.41764 ]
 [52.490517]
 [52.579987]
 [52.705517]
 [52.81386 ]], R is [[52.41915512]
 [52.49069214]
 [52.56261444]
 [52.6349144 ]
 [52.70756149]].
[2019-04-10 14:38:41,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3089554e-16 5.7663780e-16 9.5327841e-17 5.2522726e-15 5.5027363e-16
 5.8230904e-19 2.6030667e-18 1.0000000e+00 3.7243404e-19 6.8507148e-18
 5.0458640e-14], sum to 1.0000
[2019-04-10 14:38:41,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1488
[2019-04-10 14:38:41,992] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 24.32823230594118, 0.09551455955290837, 0.0, 1.0, 50.0, 38.71279875879336], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2010000.0000, 
sim time next is 2011200.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 19.0, 24.1422633187501, 0.07353951219003807, 0.0, 1.0, 50.0, 38.77371105959665], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5118552765625083, 0.5245131707300127, 0.0, 1.0, 0.7, 0.3877371105959665], 
reward next is 0.6123, 
noisyNet noise sample is [array([0.39866927], dtype=float32), 2.4830863]. 
=============================================
[2019-04-10 14:38:42,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3790223e-17 1.4510892e-15 1.3299184e-16 8.6739155e-15 1.8160433e-16
 9.5640628e-19 2.6589868e-19 1.0000000e+00 1.4617366e-19 2.1784060e-18
 3.5396321e-15], sum to 1.0000
[2019-04-10 14:38:42,661] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9373
[2019-04-10 14:38:42,710] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 19.0, 24.96831911239158, 0.3932572475330445, 0.0, 1.0, 50.0, 41.266506690081314], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1766400.0000, 
sim time next is 1767600.0000, 
raw observation next is [-2.3, 87.0, 108.0, 0.0, 19.0, 24.92167035975854, 0.3861159279451256, 0.0, 1.0, 50.0, 41.298761708745864], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.87, 0.36, 0.0, 0.08333333333333333, 0.5768058633132117, 0.6287053093150419, 0.0, 1.0, 0.7, 0.4129876170874586], 
reward next is 0.5870, 
noisyNet noise sample is [array([-0.48070726], dtype=float32), -1.262509]. 
=============================================
[2019-04-10 14:38:56,352] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-10 14:38:56,363] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:38:56,364] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:38:56,365] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run7
[2019-04-10 14:38:56,387] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:38:56,389] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:38:56,389] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:38:56,393] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:38:56,398] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run7
[2019-04-10 14:38:56,413] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/1/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run7
