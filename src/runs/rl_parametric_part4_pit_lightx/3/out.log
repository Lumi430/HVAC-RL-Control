Using TensorFlow backend.
[2019-04-10 14:30:13,869] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v3', activation='linear', agent_num=5, check_args_only=False, clip_norm=1.0, debug_log_prob=0.001, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-Repeat-v2', eval_act_func='part4_v4', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=50000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part4_v2', model_dir='None', model_param=[13, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-Repeat-v2-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_v7', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=15.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=11, test_env=['Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'], test_mode='Multiple', train_act_func='part4_v4', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=5.0, weight_initer='glorot_uniform', window_len=10)
[2019-04-10 14:30:13,870] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-10 14:30:13.924376: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-10 14:30:42,462] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-10 14:30:42,463] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-Repeat-v2', 'Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'] ...
[2019-04-10 14:30:42,486] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation worker starts!
[2019-04-10 14:30:42,499] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation worker starts!
[2019-04-10 14:30:42,515] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation worker starts!
[2019-04-10 14:30:42,515] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:42,516] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-10 14:30:42,597] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:42,599] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run1
[2019-04-10 14:30:43,517] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:43,518] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-10 14:30:43,628] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:43,630] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run1
[2019-04-10 14:30:44,519] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:44,520] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-10 14:30:44,631] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:44,633] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run1
[2019-04-10 14:30:45,522] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:45,523] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-10 14:30:45,626] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:45,628] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run1
[2019-04-10 14:30:46,524] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:46,524] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-10 14:30:46,672] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:46,674] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run1
[2019-04-10 14:30:47,539] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:47,540] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-10 14:30:47,666] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:47,668] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run1
[2019-04-10 14:30:48,542] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:48,543] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-10 14:30:48,721] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:48,722] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run1
[2019-04-10 14:30:49,545] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:49,546] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-10 14:30:49,671] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:49,673] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run1
[2019-04-10 14:30:50,547] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:50,547] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-10 14:30:50,650] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:50,652] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run1
[2019-04-10 14:30:51,548] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:51,549] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-10 14:30:51,618] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 14:30:51,618] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:30:51,619] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:51,620] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run1
[2019-04-10 14:30:51,633] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:30:51,641] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:51,646] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run1
[2019-04-10 14:30:51,635] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:30:51,740] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:51,777] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run1
[2019-04-10 14:30:52,132] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:52,134] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run1
[2019-04-10 14:30:52,562] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:52,563] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-10 14:30:53,002] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:53,003] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run1
[2019-04-10 14:30:53,564] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:53,564] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-10 14:30:53,663] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:53,665] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run1
[2019-04-10 14:30:54,565] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:54,566] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-10 14:30:54,764] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:54,765] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run1
[2019-04-10 14:30:55,567] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:55,568] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-10 14:30:55,747] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:55,749] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run1
[2019-04-10 14:30:56,569] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:56,571] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-10 14:30:56,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:56,917] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run1
[2019-04-10 14:30:57,571] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 14:30:57,572] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-10 14:30:57,824] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:30:57,825] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run1
[2019-04-10 14:32:52,966] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2876.7574 136891.4858 1302.6702
[2019-04-10 14:32:52,987] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:32:53,132] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:33:02,386] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2779.1379 146920.1504 1025.4067
[2019-04-10 14:33:02,418] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:33:02,518] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:33:03,900] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2747.6180 150117.6604 824.1965
[2019-04-10 14:33:03,921] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:33:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:33:04,923] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 2779.137897986038, 146920.15037538836, 1025.406684181856, 2876.757448872553, 136891.4857545646, 1302.6701806478445, 2747.61800476671, 150117.66035179837, 824.1964955583167]
[2019-04-10 14:33:08,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01017734 0.07634705 0.14929137 0.0598741  0.31085685 0.04028249
 0.01067814 0.04034935 0.10693911 0.14078029 0.05442385], sum to 1.0000
[2019-04-10 14:33:08,405] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3478
[2019-04-10 14:33:08,550] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 19.0, 23.89021007373489, 0.1294379928432644, 0.0, 1.0, 65.0, 65.72792729949703], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 46800.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 24.10829797216622, 0.1667940549721489, 0.0, 1.0, 35.0, 41.105779439781145], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.08333333333333333, 0.5090248310138517, 0.5555980183240496, 0.0, 1.0, 0.4, 0.4110577943978114], 
reward next is 0.5889, 
noisyNet noise sample is [array([-0.105198], dtype=float32), -0.34396583]. 
=============================================
[2019-04-10 14:33:08,571] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[1.1864679]
 [1.124983 ]
 [1.1176225]
 [1.1903969]
 [1.1669263]], R is [[1.73651409]
 [2.06186962]
 [2.73578525]
 [3.36247492]
 [3.91031575]].
[2019-04-10 14:33:11,772] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03268918 0.08527196 0.14156032 0.059493   0.2199602  0.05152765
 0.0222862  0.02705169 0.16381596 0.15184928 0.04449452], sum to 1.0000
[2019-04-10 14:33:11,772] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4626
[2019-04-10 14:33:11,913] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.799999999999999, 69.66666666666666, 69.16666666666666, 5.999999999999998, 22.5, 24.78589512540132, 0.1857155279677976, 1.0, 1.0, 35.0, 35.41832640798335], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 121200.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 22.5, 24.72971129082294, 0.1670827611120992, 1.0, 1.0, 35.0, 32.07117051591229], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.375, 0.5608092742352451, 0.5556942537040331, 1.0, 1.0, 0.4, 0.32071170515912295], 
reward next is 0.6793, 
noisyNet noise sample is [array([0.8422167], dtype=float32), 0.46472713]. 
=============================================
[2019-04-10 14:33:17,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00904948 0.07300466 0.13966124 0.05044446 0.32841754 0.08800886
 0.00661717 0.02279611 0.17116566 0.07719411 0.03364073], sum to 1.0000
[2019-04-10 14:33:17,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1393
[2019-04-10 14:33:17,293] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.84805308783553, -0.2326392331409442, 0.0, 1.0, 20.0, 38.17449370659451], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 244800.0000, 
sim time next is 246000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 22.63577137110031, -0.2790313462963014, 0.0, 1.0, 25.0, 34.29285770158155], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.38631428092502595, 0.4069895512345662, 0.0, 1.0, 0.2, 0.34292857701581553], 
reward next is 0.6571, 
noisyNet noise sample is [array([-0.13745858], dtype=float32), -0.29676655]. 
=============================================
[2019-04-10 14:33:17,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[2.1066825]
 [1.9288172]
 [2.2678256]
 [2.0709968]
 [2.184454 ]], R is [[2.78784466]
 [3.37822127]
 [3.90799689]
 [4.30595446]
 [4.783463  ]].
[2019-04-10 14:33:18,609] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01370061 0.06368318 0.10432648 0.04380461 0.39076233 0.04321972
 0.01481654 0.02833983 0.15823765 0.09826743 0.0408416 ], sum to 1.0000
[2019-04-10 14:33:18,609] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0146
[2019-04-10 14:33:18,748] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 20.661416571572, -0.6842079926851882, 0.0, 1.0, 35.0, 22.976409308728847], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 266400.0000, 
sim time next is 267600.0000, 
raw observation next is [-7.833333333333334, 69.66666666666667, 0.0, 0.0, 19.0, 20.46100990775653, -0.7287723543540063, 0.0, 1.0, 20.0, 20.928887039500083], 
processed observation next is [1.0, 0.08695652173913043, 0.2456140350877193, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.20508415897971086, 0.25707588188199787, 0.0, 1.0, 0.1, 0.20928887039500083], 
reward next is 0.7907, 
noisyNet noise sample is [array([2.4411147], dtype=float32), -0.5268607]. 
=============================================
[2019-04-10 14:33:30,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00102379 0.02554271 0.05784861 0.00847684 0.7262159  0.01225842
 0.00158264 0.01683291 0.09447445 0.03474071 0.021003  ], sum to 1.0000
[2019-04-10 14:33:30,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8305
[2019-04-10 14:33:30,685] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 20.24277330013351, -0.7598022980656537, 0.0, 1.0, 55.0, 60.40975654479317], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 363600.0000, 
sim time next is 364800.0000, 
raw observation next is [-15.8, 74.66666666666667, 0.0, 0.0, 19.0, 20.47501157778802, -0.7507737047875964, 0.0, 1.0, 40.0, 42.191100477859266], 
processed observation next is [1.0, 0.21739130434782608, 0.024930747922437636, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.20625096481566838, 0.24974209840413453, 0.0, 1.0, 0.5, 0.42191100477859267], 
reward next is 0.5781, 
noisyNet noise sample is [array([-0.00829749], dtype=float32), 0.5598702]. 
=============================================
[2019-04-10 14:33:35,305] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7806780e-04 1.0525457e-02 2.6015470e-02 2.4291943e-03 8.3194482e-01
 6.3579832e-03 4.6072496e-04 7.9379380e-03 8.9269988e-02 1.2400440e-02
 1.2379898e-02], sum to 1.0000
[2019-04-10 14:33:35,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1305
[2019-04-10 14:33:35,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 19.0, 19.85920478326019, -0.9130131163948568, 0.0, 1.0, 35.0, 35.77074584449644], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 444000.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 19.0, 19.84393516705752, -0.9484078161867541, 0.0, 1.0, 35.0, 29.622466221992738], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.08333333333333333, 0.15366126392145998, 0.18386406127108199, 0.0, 1.0, 0.4, 0.2962246622199274], 
reward next is 0.7038, 
noisyNet noise sample is [array([-1.0535464], dtype=float32), -0.57600605]. 
=============================================
[2019-04-10 14:33:35,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4755928e-05 5.8483104e-03 2.5323927e-02 2.3246526e-03 8.9684057e-01
 6.0303542e-03 7.0865855e-05 3.4412786e-03 5.0405756e-02 4.7215307e-03
 4.9380283e-03], sum to 1.0000
[2019-04-10 14:33:35,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-10 14:33:35,808] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 19.66265263226354, -0.9634272080135381, 0.0, 1.0, 35.0, 27.235260836552495], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 428400.0000, 
sim time next is 429600.0000, 
raw observation next is [-11.7, 54.00000000000001, 0.0, 0.0, 19.0, 19.51837024535337, -0.9941460334730138, 0.0, 1.0, 35.0, 27.45942486144992], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.12653085377944753, 0.16861798884232873, 0.0, 1.0, 0.4, 0.2745942486144992], 
reward next is 0.7254, 
noisyNet noise sample is [array([1.6543646], dtype=float32), 0.17940761]. 
=============================================
[2019-04-10 14:33:36,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0581718e-04 1.3818024e-02 1.1919791e-02 3.9484277e-03 8.4767002e-01
 8.1239687e-03 4.9989950e-04 9.1546169e-03 9.1681987e-02 5.4114950e-03
 7.5659361e-03], sum to 1.0000
[2019-04-10 14:33:36,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8425
[2019-04-10 14:33:36,382] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 22.5, 18.91873887908471, -1.145014387734706, 1.0, 1.0, 35.0, 33.18499882125279], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 462000.0000, 
sim time next is 463200.0000, 
raw observation next is [-6.733333333333333, 35.33333333333334, 29.5, 0.0, 22.5, 19.49644964894963, -1.069002781275151, 1.0, 1.0, 35.0, 32.9695281545232], 
processed observation next is [1.0, 0.34782608695652173, 0.2760849492151431, 0.35333333333333344, 0.09833333333333333, 0.0, 0.375, 0.12470413741246904, 0.14366573957494966, 1.0, 1.0, 0.4, 0.32969528154523203], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93106323], dtype=float32), 1.866571]. 
=============================================
[2019-04-10 14:33:36,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.07311396e-04 1.39653385e-02 2.59875562e-02 4.06420138e-03
 7.98676193e-01 8.78105685e-03 7.75309920e-04 1.27978735e-02
 1.05283156e-01 1.46969547e-02 1.45650934e-02], sum to 1.0000
[2019-04-10 14:33:36,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0571
[2019-04-10 14:33:36,855] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 44.0, 0.0, 0.0, 19.0, 17.82694900762384, -1.385502220063641, 0.0, 1.0, 35.0, 29.599534423290365], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 453600.0000, 
sim time next is 454800.0000, 
raw observation next is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 19.0, 17.80722018420402, -1.399857033305431, 0.0, 1.0, 35.0, 29.498286703173783], 
processed observation next is [1.0, 0.2608695652173913, 0.20960295475530935, 0.4366666666666667, 0.0, 0.0, 0.08333333333333333, -0.016064984649664993, 0.03338098889818967, 0.0, 1.0, 0.4, 0.2949828670317378], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5730542], dtype=float32), 0.29503807]. 
=============================================
[2019-04-10 14:33:41,270] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8883904e-07 3.5405977e-04 1.2306281e-03 7.1101480e-05 9.7614551e-01
 2.1669602e-04 2.9898587e-07 1.9203038e-04 2.1013059e-02 3.0307841e-04
 4.7334557e-04], sum to 1.0000
[2019-04-10 14:33:41,270] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6294
[2019-04-10 14:33:41,494] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 22.5, 21.38486192537403, -0.5972855058145435, 1.0, 1.0, 35.0, 27.56477368016936], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 502800.0000, 
sim time next is 504000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 22.5, 21.33567976402667, -0.6028759079797193, 1.0, 1.0, 35.0, 27.523803414692434], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.96, 0.0, 0.0, 0.375, 0.27797331366888905, 0.29904136400676024, 1.0, 1.0, 0.4, 0.2752380341469243], 
reward next is 0.7391, 
noisyNet noise sample is [array([0.22578833], dtype=float32), -1.1083872]. 
=============================================
[2019-04-10 14:33:41,516] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[14.8193655]
 [14.755591 ]
 [14.943656 ]
 [14.853908 ]
 [14.757787 ]], R is [[15.63861656]
 [16.2223568 ]
 [16.7848587 ]
 [17.34261513]
 [17.90877724]].
[2019-04-10 14:33:41,848] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6001811e-07 4.3640734e-04 1.6770111e-03 8.6747714e-05 9.6996295e-01
 3.0219278e-04 5.4285022e-07 4.2959230e-04 2.6205575e-02 2.5808241e-04
 6.4077869e-04], sum to 1.0000
[2019-04-10 14:33:41,848] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1764
[2019-04-10 14:33:41,877] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 20.90817012647598, -0.6807808677011141, 0.0, 1.0, 35.0, 22.873518430089497], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 510000.0000, 
sim time next is 511200.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 19.0, 20.86075218099106, -0.6912357396830754, 0.0, 1.0, 35.0, 22.864503261396763], 
processed observation next is [1.0, 0.9565217391304348, 0.5373961218836566, 0.92, 0.0, 0.0, 0.08333333333333333, 0.23839601508258834, 0.2695880867723082, 0.0, 1.0, 0.4, 0.22864503261396762], 
reward next is 0.7714, 
noisyNet noise sample is [array([-0.40375897], dtype=float32), -2.4462864]. 
=============================================
[2019-04-10 14:33:42,262] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7010: loss 2.4773
[2019-04-10 14:33:42,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7010: learning rate 0.0001
[2019-04-10 14:33:43,358] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7425: loss 12.3501
[2019-04-10 14:33:43,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7425: learning rate 0.0001
[2019-04-10 14:33:44,228] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7774: loss 0.1867
[2019-04-10 14:33:44,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7774: learning rate 0.0001
[2019-04-10 14:33:44,285] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7796: loss 2.7774
[2019-04-10 14:33:44,287] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7798: learning rate 0.0001
[2019-04-10 14:33:44,305] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7809: loss 5.1431
[2019-04-10 14:33:44,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7810: learning rate 0.0001
[2019-04-10 14:33:44,591] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7913: loss 0.1309
[2019-04-10 14:33:44,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7913: learning rate 0.0001
[2019-04-10 14:33:44,676] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 7945: loss 0.0451
[2019-04-10 14:33:44,677] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 7945: learning rate 0.0001
[2019-04-10 14:33:44,912] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8036: loss 3.9353
[2019-04-10 14:33:44,914] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8037: loss 6.4145
[2019-04-10 14:33:44,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8037: learning rate 0.0001
[2019-04-10 14:33:44,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8036: learning rate 0.0001
[2019-04-10 14:33:44,928] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8041: loss 2.6104
[2019-04-10 14:33:44,929] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8041: learning rate 0.0001
[2019-04-10 14:33:44,971] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8059: loss 0.2193
[2019-04-10 14:33:44,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8059: learning rate 0.0001
[2019-04-10 14:33:45,051] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8085: loss 3.3600
[2019-04-10 14:33:45,054] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8085: learning rate 0.0001
[2019-04-10 14:33:45,386] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8207: loss 0.4979
[2019-04-10 14:33:45,388] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8207: learning rate 0.0001
[2019-04-10 14:33:45,554] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 8259: loss 4.0893
[2019-04-10 14:33:45,559] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 8260: learning rate 0.0001
[2019-04-10 14:33:46,002] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8379: loss 0.1079
[2019-04-10 14:33:46,022] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8379: learning rate 0.0001
[2019-04-10 14:33:46,498] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8478: loss 0.0671
[2019-04-10 14:33:46,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8478: learning rate 0.0001
[2019-04-10 14:33:51,107] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8925459e-07 7.6552341e-04 1.4370390e-03 6.5712899e-05 9.6206057e-01
 3.1037687e-04 2.6098319e-06 5.2378728e-04 3.3242136e-02 7.7093800e-04
 8.2072418e-04], sum to 1.0000
[2019-04-10 14:33:51,107] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3428
[2019-04-10 14:33:51,134] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 19.0, 18.10695491296873, -1.363714995993184, 0.0, 1.0, 35.0, 26.056166014050042], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 704400.0000, 
sim time next is 705600.0000, 
raw observation next is [-2.8, 75.0, 0.0, 0.0, 19.0, 18.04339895707777, -1.380389388647585, 0.0, 1.0, 35.0, 26.09162794149476], 
processed observation next is [1.0, 0.17391304347826086, 0.38504155124653744, 0.75, 0.0, 0.0, 0.08333333333333333, 0.0036165797564808124, 0.0398702037841383, 0.0, 1.0, 0.4, 0.2609162794149476], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.31706437], dtype=float32), 1.2348456]. 
=============================================
[2019-04-10 14:33:52,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.39665602e-07 2.66330346e-04 7.29383668e-04 2.81360863e-05
 9.79457080e-01 1.44900885e-04 5.54234020e-07 2.22039977e-04
 1.87182669e-02 1.07309694e-04 3.25836183e-04], sum to 1.0000
[2019-04-10 14:33:52,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-10 14:33:52,161] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 24.16666666666667, 0.0, 22.5, 18.48846879802558, -1.301172204583772, 1.0, 1.0, 35.0, 31.614563391581285], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 721200.0000, 
sim time next is 722400.0000, 
raw observation next is [-2.3, 76.0, 41.0, 8.166666666666664, 22.5, 18.83764385730343, -1.247297233908218, 1.0, 1.0, 35.0, 31.163092773620185], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.13666666666666666, 0.009023941068139961, 0.375, 0.06980365477528583, 0.08423425536392733, 1.0, 1.0, 0.4, 0.3116309277362019], 
reward next is 0.8081, 
noisyNet noise sample is [array([0.4200353], dtype=float32), 1.2007862]. 
=============================================
[2019-04-10 14:34:09,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9963242e-10 9.1706834e-06 2.7311684e-05 5.3235749e-07 9.9175054e-01
 3.0719405e-06 7.2901996e-10 8.7634962e-06 8.1398562e-03 1.7406807e-06
 5.8984802e-05], sum to 1.0000
[2019-04-10 14:34:09,512] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9134
[2019-04-10 14:34:09,592] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 19.0, 20.64848917911286, -0.6960554573139777, 0.0, 1.0, 35.0, 21.14171706339442], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 958800.0000, 
sim time next is 960000.0000, 
raw observation next is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 19.0, 20.66675664841003, -0.6985210615889045, 0.0, 1.0, 35.0, 21.081940800211115], 
processed observation next is [1.0, 0.08695652173913043, 0.6657433056325024, 0.8066666666666665, 0.0, 0.0, 0.08333333333333333, 0.22222972070083577, 0.26715964613703186, 0.0, 1.0, 0.4, 0.21081940800211116], 
reward next is 0.7892, 
noisyNet noise sample is [array([-0.78806627], dtype=float32), -1.331766]. 
=============================================
[2019-04-10 14:34:09,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[30.014647]
 [30.562052]
 [30.908707]
 [31.141953]
 [32.04378 ]], R is [[29.63599396]
 [30.12821579]
 [30.61499405]
 [31.09614563]
 [31.5724926 ]].
[2019-04-10 14:34:16,993] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4248732e-12 7.0594098e-07 5.4365564e-06 3.6287766e-08 9.9769598e-01
 2.8196212e-07 2.7044207e-11 6.9653515e-07 2.2919208e-03 4.9372431e-07
 4.4038752e-06], sum to 1.0000
[2019-04-10 14:34:16,994] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2597
[2019-04-10 14:34:17,096] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 81.33333333333334, 0.0, 0.0, 19.0, 23.9406004483963, 0.2206172574956223, 0.0, 1.0, 35.0, 19.903032683122124], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1149600.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 19.0, 23.9623401762742, 0.2234711026266412, 0.0, 1.0, 35.0, 17.305131347330235], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.4968616813561833, 0.5744903675422137, 0.0, 1.0, 0.4, 0.17305131347330235], 
reward next is 0.8269, 
noisyNet noise sample is [array([-0.2808999], dtype=float32), -0.61889213]. 
=============================================
[2019-04-10 14:34:17,722] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 14751: loss 4.6380
[2019-04-10 14:34:17,722] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 14751: learning rate 0.0001
[2019-04-10 14:34:18,360] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15050: loss 4.4220
[2019-04-10 14:34:18,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15052: learning rate 0.0001
[2019-04-10 14:34:18,457] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15104: loss 4.3935
[2019-04-10 14:34:18,460] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15105: learning rate 0.0001
[2019-04-10 14:34:19,438] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15613: loss 4.2469
[2019-04-10 14:34:19,444] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15613: learning rate 0.0001
[2019-04-10 14:34:19,582] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15693: loss 4.2068
[2019-04-10 14:34:19,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15693: learning rate 0.0001
[2019-04-10 14:34:19,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.5010544e-14 1.4718712e-07 4.5448319e-06 8.8721857e-09 9.9892443e-01
 2.8251089e-08 7.1706351e-13 3.0661602e-07 1.0688605e-03 7.6596059e-08
 1.6360584e-06], sum to 1.0000
[2019-04-10 14:34:19,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-10 14:34:19,742] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.2, 67.0, 106.5, 0.0, 19.0, 24.27445963080643, 0.2846493330957156, 0.0, 0.0, 35.0, 18.72331996720768], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1159200.0000, 
sim time next is 1160400.0000, 
raw observation next is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 19.0, 24.31785082716183, 0.2977270438469929, 0.0, 0.0, 35.0, 18.58362794381138], 
processed observation next is [0.0, 0.43478260869565216, 0.9492151431209604, 0.6633333333333334, 0.4072222222222223, 0.0, 0.08333333333333333, 0.5264875689301526, 0.5992423479489976, 0.0, 0.0, 0.4, 0.1858362794381138], 
reward next is 0.8142, 
noisyNet noise sample is [array([0.34609333], dtype=float32), -1.2190503]. 
=============================================
[2019-04-10 14:34:20,121] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15970: loss 4.1570
[2019-04-10 14:34:20,123] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15972: learning rate 0.0001
[2019-04-10 14:34:20,394] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16114: loss 4.0464
[2019-04-10 14:34:20,396] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16115: learning rate 0.0001
[2019-04-10 14:34:20,722] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16285: loss 3.8905
[2019-04-10 14:34:20,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16285: learning rate 0.0001
[2019-04-10 14:34:20,772] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16313: loss 3.9264
[2019-04-10 14:34:20,788] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16318: learning rate 0.0001
[2019-04-10 14:34:20,792] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16319: loss 3.8455
[2019-04-10 14:34:20,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16319: learning rate 0.0001
[2019-04-10 14:34:20,819] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16336: loss 4.0369
[2019-04-10 14:34:20,825] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 16336: loss 3.7921
[2019-04-10 14:34:20,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16337: learning rate 0.0001
[2019-04-10 14:34:20,830] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 16337: learning rate 0.0001
[2019-04-10 14:34:20,858] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 16353: loss 3.8889
[2019-04-10 14:34:20,861] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 16354: learning rate 0.0001
[2019-04-10 14:34:21,017] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16437: loss 3.9098
[2019-04-10 14:34:21,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16438: learning rate 0.0001
[2019-04-10 14:34:21,299] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16581: loss 4.0313
[2019-04-10 14:34:21,299] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16581: learning rate 0.0001
[2019-04-10 14:34:21,370] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16617: loss 3.8033
[2019-04-10 14:34:21,373] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16618: learning rate 0.0001
[2019-04-10 14:34:21,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3497165e-14 1.6352926e-07 3.0398028e-06 3.5263956e-09 9.9838603e-01
 8.4430262e-08 5.7150243e-13 1.7329947e-07 1.6089301e-03 4.8739665e-08
 1.4613981e-06], sum to 1.0000
[2019-04-10 14:34:21,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-10 14:34:21,683] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 79.33333333333333, 0.0, 0.0, 19.0, 24.91429569904504, 0.4521890129501618, 0.0, 0.0, 35.0, 18.169677990889888], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1212000.0000, 
sim time next is 1213200.0000, 
raw observation next is [16.1, 80.0, 0.0, 0.0, 19.0, 24.94602344898994, 0.4489709356486801, 0.0, 0.0, 35.0, 18.27359278661627], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.8, 0.0, 0.0, 0.08333333333333333, 0.5788352874158283, 0.6496569785495601, 0.0, 0.0, 0.4, 0.1827359278661627], 
reward next is 0.8173, 
noisyNet noise sample is [array([-0.31804827], dtype=float32), 1.725094]. 
=============================================
[2019-04-10 14:34:22,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7152008e-21 1.0452262e-11 1.6702675e-10 3.5462427e-14 9.9999416e-01
 2.6633357e-12 8.5920071e-20 1.6174430e-11 5.8674455e-06 2.2568585e-12
 1.6339956e-10], sum to 1.0000
[2019-04-10 14:34:22,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1227
[2019-04-10 14:34:22,926] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 24.32578427359257, 0.08693104771867384, 1.0, 1.0, 35.0, 21.70166617062198], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1358400.0000, 
sim time next is 1359600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 22.40133785755127, 0.003400620226914203, 1.0, 1.0, 35.0, 22.207616503547854], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.36677815479593906, 0.501133540075638, 1.0, 1.0, 0.4, 0.22207616503547853], 
reward next is 0.7779, 
noisyNet noise sample is [array([0.328555], dtype=float32), -1.4490384]. 
=============================================
[2019-04-10 14:34:34,251] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9810184e-23 2.5696849e-13 4.8139940e-11 2.5057249e-16 9.9999952e-01
 2.9234376e-13 7.7731405e-22 3.0193228e-13 5.2084170e-07 4.4105327e-14
 5.7765837e-11], sum to 1.0000
[2019-04-10 14:34:34,252] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0542
[2019-04-10 14:34:34,383] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 79.33333333333334, 0.0, 0.0, 19.0, 24.89734207680022, 0.4413706970122938, 0.0, 1.0, 35.0, 18.311098487531886], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1632000.0000, 
sim time next is 1633200.0000, 
raw observation next is [6.8, 82.66666666666666, 0.0, 0.0, 19.0, 24.85266374295225, 0.4331044830901156, 0.0, 1.0, 35.0, 18.502614117377547], 
processed observation next is [1.0, 0.9130434782608695, 0.6509695290858727, 0.8266666666666665, 0.0, 0.0, 0.08333333333333333, 0.5710553119126874, 0.6443681610300386, 0.0, 1.0, 0.4, 0.18502614117377547], 
reward next is 0.8150, 
noisyNet noise sample is [array([1.1036699], dtype=float32), 0.9259732]. 
=============================================
[2019-04-10 14:34:36,429] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.85392489e-24 1.00554764e-13 4.22667413e-12 4.03968700e-16
 1.00000000e+00 2.31285870e-14 1.20161502e-22 1.10968214e-13
 4.97833064e-08 9.08730742e-15 2.56491811e-12], sum to 1.0000
[2019-04-10 14:34:36,430] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5332
[2019-04-10 14:34:36,453] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 61.0, 208.0, 168.5, 22.5, 25.3772349256875, 0.3978735496002381, 1.0, 1.0, 35.0, 19.696048426212105], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1594800.0000, 
sim time next is 1596000.0000, 
raw observation next is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 22.5, 25.54596054579965, 0.4424838865140668, 1.0, 1.0, 35.0, 16.80767817430823], 
processed observation next is [1.0, 0.4782608695652174, 0.7433056325023084, 0.5966666666666667, 0.7111111111111109, 0.24548802946593007, 0.375, 0.6288300454833043, 0.6474946288380222, 1.0, 1.0, 0.4, 0.1680767817430823], 
reward next is 0.8319, 
noisyNet noise sample is [array([-0.5299369], dtype=float32), 0.8074885]. 
=============================================
[2019-04-10 14:34:36,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.691315]
 [75.67156 ]
 [74.76223 ]
 [72.919685]
 [71.29424 ]], R is [[77.76137543]
 [77.7868042 ]
 [77.83424377]
 [77.87876892]
 [77.92145538]].
[2019-04-10 14:34:36,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0036254e-18 6.2501629e-11 7.3409661e-09 1.2443061e-12 9.9999058e-01
 3.8165110e-11 2.5986878e-17 3.4609518e-10 9.3597682e-06 5.3896731e-11
 9.1720525e-09], sum to 1.0000
[2019-04-10 14:34:36,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0153
[2019-04-10 14:34:36,545] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.333333333333334, 80.0, 0.0, 0.0, 19.0, 23.59919467523277, 0.02826733383897006, 0.0, 1.0, 35.0, 18.76586396377172], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1579200.0000, 
sim time next is 1580400.0000, 
raw observation next is [5.5, 79.0, 0.0, 0.0, 19.0, 23.51481309169582, 0.01111402123353989, 0.0, 1.0, 35.0, 18.749437809084093], 
processed observation next is [1.0, 0.30434782608695654, 0.6149584487534627, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4595677576413184, 0.5037046737445133, 0.0, 1.0, 0.4, 0.18749437809084094], 
reward next is 0.8125, 
noisyNet noise sample is [array([-0.702673], dtype=float32), 0.14541996]. 
=============================================
[2019-04-10 14:34:37,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.15822325e-23 5.98268248e-14 4.77130072e-11 2.92830248e-16
 9.99998689e-01 6.40173515e-13 2.11098181e-22 4.21144131e-13
 1.29158764e-06 1.38875944e-13 3.51529257e-11], sum to 1.0000
[2019-04-10 14:34:37,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2498
[2019-04-10 14:34:37,309] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 57.0, 182.5, 186.5, 22.5, 25.86911335711114, 0.50313027486763, 1.0, 1.0, 35.0, 18.618757984729328], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1598400.0000, 
sim time next is 1599600.0000, 
raw observation next is [12.33333333333333, 54.33333333333334, 160.1666666666667, 144.8333333333333, 22.5, 25.97668610940296, 0.513925976141263, 1.0, 1.0, 35.0, 18.41520455918375], 
processed observation next is [1.0, 0.5217391304347826, 0.8042474607571561, 0.5433333333333334, 0.5338888888888891, 0.16003683241252298, 0.375, 0.6647238424502465, 0.6713086587137543, 1.0, 1.0, 0.4, 0.18415204559183748], 
reward next is 0.8158, 
noisyNet noise sample is [array([-0.85902786], dtype=float32), 1.859614]. 
=============================================
[2019-04-10 14:34:37,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.40168125e-22 1.39600733e-13 5.43174221e-11 8.67085870e-16
 9.99999046e-01 5.84113487e-13 3.25207875e-21 2.24543867e-13
 9.02219142e-07 1.01488215e-13 6.87183860e-12], sum to 1.0000
[2019-04-10 14:34:37,321] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5053
[2019-04-10 14:34:37,357] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 22.5, 24.36567005883218, 0.2009584891248829, 1.0, 1.0, 35.0, 21.58633702227231], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1676400.0000, 
sim time next is 1677600.0000, 
raw observation next is [1.5, 92.0, 59.5, 0.0, 22.5, 24.53600760726743, 0.2224207747700105, 1.0, 1.0, 35.0, 22.40054230806987], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.92, 0.19833333333333333, 0.0, 0.375, 0.5446673006056191, 0.5741402582566701, 1.0, 1.0, 0.4, 0.2240054230806987], 
reward next is 0.7760, 
noisyNet noise sample is [array([1.3464829], dtype=float32), 0.9685215]. 
=============================================
[2019-04-10 14:34:38,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5561683e-22 3.2219121e-12 5.9007646e-11 2.1787515e-14 9.9999821e-01
 5.1302699e-12 8.3241948e-21 8.0226358e-13 1.7869395e-06 4.9307010e-13
 4.9063052e-11], sum to 1.0000
[2019-04-10 14:34:38,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3542
[2019-04-10 14:34:38,748] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.8, 82.66666666666666, 0.0, 0.0, 19.0, 24.84047688508824, 0.430192144061974, 0.0, 1.0, 35.0, 18.497687566968033], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1633200.0000, 
sim time next is 1634400.0000, 
raw observation next is [6.6, 86.0, 0.0, 0.0, 19.0, 24.78231486817843, 0.4184977173781435, 0.0, 1.0, 35.0, 18.699226391585917], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.86, 0.0, 0.0, 0.08333333333333333, 0.565192905681536, 0.6394992391260478, 0.0, 1.0, 0.4, 0.18699226391585916], 
reward next is 0.8130, 
noisyNet noise sample is [array([-0.23646145], dtype=float32), -1.1462845]. 
=============================================
[2019-04-10 14:34:39,534] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 22729: loss 0.1207
[2019-04-10 14:34:39,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 22729: learning rate 0.0001
[2019-04-10 14:34:40,892] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23318: loss 0.1393
[2019-04-10 14:34:40,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23319: learning rate 0.0001
[2019-04-10 14:34:41,114] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23412: loss 0.1436
[2019-04-10 14:34:41,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23412: learning rate 0.0001
[2019-04-10 14:34:41,592] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23635: loss 0.1661
[2019-04-10 14:34:41,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23635: learning rate 0.0001
[2019-04-10 14:34:41,782] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23721: loss 0.1349
[2019-04-10 14:34:41,789] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23721: learning rate 0.0001
[2019-04-10 14:34:42,358] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23968: loss 0.0957
[2019-04-10 14:34:42,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23968: learning rate 0.0001
[2019-04-10 14:34:42,531] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 24036: loss 0.0939
[2019-04-10 14:34:42,532] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 24036: learning rate 0.0001
[2019-04-10 14:34:42,601] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24069: loss 0.1367
[2019-04-10 14:34:42,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24069: learning rate 0.0001
[2019-04-10 14:34:42,613] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24072: loss 0.1349
[2019-04-10 14:34:42,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24072: learning rate 0.0001
[2019-04-10 14:34:42,673] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24101: loss 0.0903
[2019-04-10 14:34:42,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24103: learning rate 0.0001
[2019-04-10 14:34:42,678] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 24105: loss 0.1216
[2019-04-10 14:34:42,679] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 24106: learning rate 0.0001
[2019-04-10 14:34:42,845] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24168: loss 0.0837
[2019-04-10 14:34:42,846] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24168: learning rate 0.0001
[2019-04-10 14:34:42,906] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24192: loss 0.1020
[2019-04-10 14:34:42,907] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24192: learning rate 0.0001
[2019-04-10 14:34:43,043] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24246: loss 0.1016
[2019-04-10 14:34:43,043] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24246: learning rate 0.0001
[2019-04-10 14:34:43,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24561: loss 0.0910
[2019-04-10 14:34:43,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24561: learning rate 0.0001
[2019-04-10 14:34:44,224] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24622: loss 0.1238
[2019-04-10 14:34:44,225] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24622: learning rate 0.0001
[2019-04-10 14:34:55,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6134227e-18 3.9553522e-10 3.9229193e-09 1.0859225e-12 9.9993551e-01
 4.1949394e-10 1.3164471e-16 1.2642763e-10 6.4435997e-05 6.7471043e-11
 1.2426891e-08], sum to 1.0000
[2019-04-10 14:34:55,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5385
[2019-04-10 14:34:55,718] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 109.5, 0.0, 22.5, 20.13959468892636, -0.9929782131630808, 1.0, 1.0, 35.0, 31.388316661799045], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2025600.0000, 
sim time next is 2026800.0000, 
raw observation next is [-5.6, 83.0, 124.5, 0.0, 22.5, 20.24308389272084, -0.9622347999701849, 1.0, 1.0, 35.0, 31.250747985917894], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.83, 0.415, 0.0, 0.375, 0.18692365772673666, 0.17925506667660504, 1.0, 1.0, 0.4, 0.3125074798591789], 
reward next is 0.5481, 
noisyNet noise sample is [array([1.1774063], dtype=float32), 2.325855]. 
=============================================
[2019-04-10 14:34:58,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3399241e-19 1.3960011e-10 3.0481073e-09 1.7210730e-12 9.9997199e-01
 2.0369813e-10 1.5255591e-17 1.9401712e-10 2.7988513e-05 5.3949002e-11
 5.0672910e-09], sum to 1.0000
[2019-04-10 14:34:58,462] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0035
[2019-04-10 14:34:58,893] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 152.0, 0.0, 22.5, 20.46558832186712, -0.8822466941287844, 1.0, 1.0, 35.0, 29.976680111132907], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2034000.0000, 
sim time next is 2035200.0000, 
raw observation next is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 22.5, 20.53182444552134, -0.8693795020393026, 1.0, 1.0, 35.0, 29.901527646453136], 
processed observation next is [1.0, 0.5652173913043478, 0.34349030470914127, 0.79, 0.4977777777777776, 0.0, 0.375, 0.21098537046011176, 0.2102068326535658, 1.0, 1.0, 0.4, 0.29901527646453135], 
reward next is 0.5187, 
noisyNet noise sample is [array([0.44760922], dtype=float32), -0.07228157]. 
=============================================
[2019-04-10 14:35:04,501] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2771946e-13 3.7420911e-08 8.6446823e-07 1.4022586e-09 9.9948692e-01
 2.4352481e-08 9.0400690e-13 7.8754688e-08 5.1151105e-04 1.5662831e-08
 5.2381722e-07], sum to 1.0000
[2019-04-10 14:35:04,502] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6703
[2019-04-10 14:35:04,737] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 86.0, 0.0, 0.0, 19.0, 18.70125896418924, -1.168291895203869, 0.0, 1.0, 35.0, 26.683461988126417], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2082000.0000, 
sim time next is 2083200.0000, 
raw observation next is [-4.833333333333333, 86.0, 0.0, 0.0, 19.0, 18.65191874902509, -1.192291080066407, 0.0, 1.0, 35.0, 26.83477694178325], 
processed observation next is [1.0, 0.08695652173913043, 0.3287165281625116, 0.86, 0.0, 0.0, 0.08333333333333333, 0.05432656241875744, 0.10256963997786432, 0.0, 1.0, 0.4, 0.2683477694178325], 
reward next is 0.4338, 
noisyNet noise sample is [array([-0.3701808], dtype=float32), 1.0151393]. 
=============================================
[2019-04-10 14:35:06,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5351826e-12 5.3167497e-07 3.3366171e-06 1.5699031e-08 9.9822646e-01
 3.0877547e-07 5.3522943e-11 5.9646163e-07 1.7649678e-03 5.2071118e-07
 3.3212705e-06], sum to 1.0000
[2019-04-10 14:35:06,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8829
[2019-04-10 14:35:06,903] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 18.61673036875165, -1.210815742865295, 0.0, 1.0, 35.0, 26.657565576366878], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2170800.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 18.60401119507121, -1.242622051140154, 0.0, 1.0, 35.0, 26.76307288340316], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.05033426625593407, 0.08579264961994866, 0.0, 1.0, 0.4, 0.2676307288340316], 
reward next is 0.3171, 
noisyNet noise sample is [array([-1.7269022], dtype=float32), 0.39984503]. 
=============================================
[2019-04-10 14:35:06,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[38.90901 ]
 [39.233566]
 [40.093067]
 [40.612736]
 [40.97117 ]], R is [[38.87231827]
 [38.96431732]
 [38.83379745]
 [39.37089157]
 [38.9771843 ]].
[2019-04-10 14:35:10,578] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1489033e-17 1.0937017e-09 2.1463618e-08 1.7045743e-11 9.9979752e-01
 9.8126340e-10 3.1694557e-16 6.0883421e-10 2.0243262e-04 1.6083536e-09
 5.2469030e-08], sum to 1.0000
[2019-04-10 14:35:10,578] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0167
[2019-04-10 14:35:10,709] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333334, 67.0, 92.5, 0.0, 22.5, 21.53237281213814, -0.7356113617217184, 1.0, 1.0, 35.0, 28.95442025525361], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2128800.0000, 
sim time next is 2130000.0000, 
raw observation next is [-4.666666666666667, 66.0, 76.0, 0.0, 22.5, 20.9716741325647, -0.7172463996874461, 1.0, 1.0, 35.0, 29.128579466506743], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.66, 0.25333333333333335, 0.0, 0.375, 0.2476395110470584, 0.2609178667708513, 1.0, 1.0, 0.4, 0.29128579466506743], 
reward next is 0.7662, 
noisyNet noise sample is [array([-1.040174], dtype=float32), -0.66758513]. 
=============================================
[2019-04-10 14:35:10,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.871502]
 [56.99114 ]
 [57.219025]
 [57.577442]
 [57.914345]], R is [[56.93745804]
 [56.99292755]
 [57.2316246 ]
 [57.42834854]
 [57.69026947]].
[2019-04-10 14:35:10,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3721115e-11 4.2046238e-07 1.1939272e-05 3.0587934e-08 9.9794573e-01
 3.4076623e-07 6.5003412e-11 5.5374363e-07 2.0338888e-03 3.5450574e-07
 6.7786104e-06], sum to 1.0000
[2019-04-10 14:35:10,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4308
[2019-04-10 14:35:10,951] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 18.60245915161275, -1.243059046940205, 0.0, 1.0, 35.0, 26.76424125466507], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2172000.0000, 
sim time next is 2173200.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 18.34194137008864, -1.269212247173307, 0.0, 1.0, 35.0, 26.574078982131045], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.02849511417405341, 0.07692925094223102, 0.0, 1.0, 0.4, 0.26574078982131044], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8900312], dtype=float32), -0.028360166]. 
=============================================
[2019-04-10 14:35:12,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3324203e-12 7.3713579e-07 3.1036532e-06 1.8529011e-08 9.9788338e-01
 5.1148851e-07 1.3125322e-10 6.7181594e-07 2.1072861e-03 3.6121537e-07
 4.0012055e-06], sum to 1.0000
[2019-04-10 14:35:12,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-10 14:35:12,495] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 18.09923631271713, -1.323311089566208, 0.0, 1.0, 35.0, 26.551672563130435], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2176800.0000, 
sim time next is 2178000.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 19.0, 18.02060612296045, -1.336552793566393, 0.0, 1.0, 35.0, 26.525491284315223], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.75, 0.0, 0.0, 0.08333333333333333, 0.0017171769133709585, 0.05448240214453567, 0.0, 1.0, 0.4, 0.2652549128431522], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00150128], dtype=float32), 1.483218]. 
=============================================
[2019-04-10 14:35:12,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[37.151604]
 [37.18544 ]
 [37.189117]
 [37.173615]
 [37.176807]], R is [[36.78604507]
 [36.41818619]
 [36.05400467]
 [35.69346619]
 [35.33653259]].
[2019-04-10 14:35:27,202] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31173: loss 0.2509
[2019-04-10 14:35:27,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31173: learning rate 0.0001
[2019-04-10 14:35:29,477] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31553: loss -0.5522
[2019-04-10 14:35:29,483] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31554: learning rate 0.0001
[2019-04-10 14:35:29,726] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31606: loss 0.0142
[2019-04-10 14:35:29,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31606: learning rate 0.0001
[2019-04-10 14:35:30,262] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31694: loss 1.0565
[2019-04-10 14:35:30,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31695: learning rate 0.0001
[2019-04-10 14:35:30,616] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31742: loss 0.2561
[2019-04-10 14:35:30,617] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31742: learning rate 0.0001
[2019-04-10 14:35:32,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32053: loss 0.3361
[2019-04-10 14:35:32,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32053: learning rate 0.0001
[2019-04-10 14:35:32,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32073: loss 0.2848
[2019-04-10 14:35:32,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32073: learning rate 0.0001
[2019-04-10 14:35:34,214] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32240: loss 0.3892
[2019-04-10 14:35:34,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32240: learning rate 0.0001
[2019-04-10 14:35:34,228] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 32242: loss 0.4259
[2019-04-10 14:35:34,229] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 32242: learning rate 0.0001
[2019-04-10 14:35:34,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.00691677e-08 1.18103046e-04 1.69847612e-04 1.11516911e-05
 8.89779687e-01 5.60384397e-05 1.40412013e-07 2.51434452e-04
 1.09149002e-01 8.06781100e-05 3.83912731e-04], sum to 1.0000
[2019-04-10 14:35:34,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6489
[2019-04-10 14:35:34,487] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 19.0, 17.98594321008725, -1.36841680659125, 0.0, 1.0, 35.0, 27.741490865651304], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2427600.0000, 
sim time next is 2428800.0000, 
raw observation next is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 17.89476563916222, -1.390878785708811, 0.0, 1.0, 35.0, 27.864831708618702], 
processed observation next is [0.0, 0.08695652173913043, 0.2511542012927055, 0.5433333333333333, 0.0, 0.0, 0.08333333333333333, -0.008769530069815032, 0.03637373809706301, 0.0, 1.0, 0.4, 0.27864831708618704], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.88629204], dtype=float32), 0.8209019]. 
=============================================
[2019-04-10 14:35:34,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7990974e-08 9.4174640e-05 2.2755028e-04 5.7473694e-06 9.3179798e-01
 7.7547993e-05 8.7730065e-08 1.2961196e-04 6.7403503e-02 4.4377746e-05
 2.1944927e-04], sum to 1.0000
[2019-04-10 14:35:34,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1641
[2019-04-10 14:35:34,628] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 19.0, 17.38141286008079, -1.495025717389304, 0.0, 1.0, 35.0, 28.52201198134589], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2436000.0000, 
sim time next is 2437200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 19.0, 17.29866769807789, -1.509791565358566, 0.0, 1.0, 35.0, 28.531714510312767], 
processed observation next is [0.0, 0.21739130434782608, 0.2299168975069252, 0.61, 0.0, 0.0, 0.08333333333333333, -0.05844435849350926, -0.0032638551195220225, 0.0, 1.0, 0.4, 0.28531714510312767], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.89673936], dtype=float32), 0.018518617]. 
=============================================
[2019-04-10 14:35:34,770] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 32314: loss -0.5447
[2019-04-10 14:35:34,770] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 32314: learning rate 0.0001
[2019-04-10 14:35:34,885] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32330: loss 2.1195
[2019-04-10 14:35:34,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32330: learning rate 0.0001
[2019-04-10 14:35:34,961] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32337: loss 0.2878
[2019-04-10 14:35:34,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32337: learning rate 0.0001
[2019-04-10 14:35:35,262] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32375: loss 1.9273
[2019-04-10 14:35:35,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32375: learning rate 0.0001
[2019-04-10 14:35:35,295] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32382: loss 0.3925
[2019-04-10 14:35:35,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32382: learning rate 0.0001
[2019-04-10 14:35:35,486] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32416: loss 0.7055
[2019-04-10 14:35:35,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32416: learning rate 0.0001
[2019-04-10 14:35:35,684] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.87823620e-09 2.67042924e-05 1.68080660e-04 2.29770126e-06
 7.98238158e-01 1.24451371e-05 1.84089402e-08 7.36851944e-05
 2.01339081e-01 2.83497138e-05 1.11177105e-04], sum to 1.0000
[2019-04-10 14:35:35,685] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7095
[2019-04-10 14:35:35,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.05111159e-10 1.84616947e-05 6.79313162e-05 7.84421843e-07
 9.40718532e-01 1.29186119e-05 1.06183529e-09 1.19975675e-05
 5.90958223e-02 4.25451117e-06 6.93188631e-05], sum to 1.0000
[2019-04-10 14:35:35,906] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8225
[2019-04-10 14:35:35,843] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 19.0, 17.71397105461258, -1.411868244681958, 0.0, 1.0, 35.0, 40.450637367794236], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2446800.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 19.0, 17.91987591387851, -1.394561281308746, 0.0, 1.0, 35.0, 35.68855991464709], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 0.08333333333333333, -0.006677007176790835, 0.035146239563751326, 0.0, 1.0, 0.4, 0.3568855991464709], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28438237], dtype=float32), -0.27358785]. 
=============================================
[2019-04-10 14:35:35,913] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[28.493338]
 [28.272451]
 [27.578222]
 [27.490572]
 [27.034615]], R is [[29.69593811]
 [30.39897919]
 [30.68392372]
 [31.14242554]
 [30.83100128]].
[2019-04-10 14:35:35,970] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 43.0, 0.0, 0.0, 19.0, 19.16193388953811, -1.085686502960919, 0.0, 1.0, 35.0, 34.25695241437936], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2419200.0000, 
sim time next is 2420400.0000, 
raw observation next is [-5.8, 44.66666666666667, 0.0, 0.0, 19.0, 19.23909129599431, -1.091178271268541, 0.0, 1.0, 35.0, 28.763986470676585], 
processed observation next is [0.0, 0.0, 0.30193905817174516, 0.4466666666666667, 0.0, 0.0, 0.08333333333333333, 0.10325760799952584, 0.13627390957715302, 0.0, 1.0, 0.4, 0.28763986470676584], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.85189366], dtype=float32), -0.38104165]. 
=============================================
[2019-04-10 14:35:36,202] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32533: loss -0.0673
[2019-04-10 14:35:36,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32533: learning rate 0.0001
[2019-04-10 14:35:37,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.97250860e-09 3.32805794e-05 9.36639626e-05 1.34274308e-06
 8.84128511e-01 2.45844185e-05 1.28010385e-08 5.02742769e-05
 1.15582108e-01 1.99006972e-05 6.62717357e-05], sum to 1.0000
[2019-04-10 14:35:37,027] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8871
[2019-04-10 14:35:37,077] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.3, 53.0, 0.0, 0.0, 19.0, 18.91822307206398, -1.201289107428349, 0.0, 1.0, 35.0, 24.064760856172185], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2426400.0000, 
sim time next is 2427600.0000, 
raw observation next is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 19.0, 18.69898070562883, -1.187590712424706, 0.0, 1.0, 55.0, 58.336461141441745], 
processed observation next is [0.0, 0.08695652173913043, 0.25577100646352724, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.058248392135735884, 0.10413642919176465, 0.0, 1.0, 0.8, 0.5833646114144174], 
reward next is 0.4054, 
noisyNet noise sample is [array([0.65024525], dtype=float32), 1.3280947]. 
=============================================
[2019-04-10 14:35:44,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3256770e-15 1.8730693e-08 2.0715645e-07 1.3834703e-10 9.2596251e-01
 5.0704299e-09 3.9849639e-15 6.1051386e-09 7.4037053e-02 2.7321163e-09
 2.7743314e-07], sum to 1.0000
[2019-04-10 14:35:44,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4086
[2019-04-10 14:35:44,768] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 22.5, 22.99446618305687, -0.262996585361972, 1.0, 1.0, 35.0, 24.10157203781399], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2571600.0000, 
sim time next is 2572800.0000, 
raw observation next is [-0.2333333333333333, 35.66666666666667, 0.0, 0.0, 22.5, 22.78082122380371, -0.292215540339177, 1.0, 1.0, 35.0, 24.560487012466155], 
processed observation next is [1.0, 0.782608695652174, 0.456140350877193, 0.3566666666666667, 0.0, 0.0, 0.375, 0.39840176865030913, 0.40259481988694096, 1.0, 1.0, 0.4, 0.24560487012466153], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.7320056], dtype=float32), 0.34120083]. 
=============================================
[2019-04-10 14:35:45,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3099933e-16 4.5469841e-09 1.3598714e-07 3.9841273e-11 9.6256560e-01
 3.5899148e-09 3.1024842e-16 2.8255818e-09 3.7434224e-02 2.3209332e-09
 1.6985533e-08], sum to 1.0000
[2019-04-10 14:35:45,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0333
[2019-04-10 14:35:45,598] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 36.0, 0.0, 0.0, 22.5, 23.17365656144042, -0.1207681298063092, 1.0, 1.0, 35.0, 23.391188185264415], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2574000.0000, 
sim time next is 2575200.0000, 
raw observation next is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 22.5, 23.27333602449258, -0.1249472934866631, 0.0, 1.0, 35.0, 23.845778752676736], 
processed observation next is [1.0, 0.8260869565217391, 0.43582640812557716, 0.3866666666666667, 0.0, 0.0, 0.375, 0.4394446687077149, 0.4583509021711123, 0.0, 1.0, 0.4, 0.23845778752676736], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.59895486], dtype=float32), 0.13620472]. 
=============================================
[2019-04-10 14:36:03,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0620021e-13 1.9961193e-07 3.6340111e-06 7.7916731e-09 5.8188897e-01
 4.3319949e-07 2.5976673e-12 5.7091336e-07 4.1809851e-01 2.5205486e-07
 7.4520281e-06], sum to 1.0000
[2019-04-10 14:36:03,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3405
[2019-04-10 14:36:03,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 22.72176536217763, -0.2972152936616245, 0.0, 1.0, 35.0, 36.39065841319301], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2787600.0000, 
sim time next is 2788800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 22.41288179683449, -0.3869571848209775, 0.0, 1.0, 35.0, 43.24014199561553], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.36774014973620756, 0.3710142717263409, 0.0, 1.0, 0.4, 0.43240141995615533], 
reward next is 0.5676, 
noisyNet noise sample is [array([-0.223311], dtype=float32), 0.28360376]. 
=============================================
[2019-04-10 14:36:11,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6289021e-20 5.1394507e-11 8.7455565e-10 9.0154277e-14 7.1403675e-02
 2.1289838e-10 4.6231161e-19 2.9435926e-10 9.2859638e-01 2.4310871e-11
 2.5830043e-09], sum to 1.0000
[2019-04-10 14:36:11,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0863
[2019-04-10 14:36:11,852] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.0, 24.0, 106.5, 0.0, 22.5, 27.11388898036651, 0.698417092236248, 1.0, 1.0, 35.0, 44.62597985017467], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2818800.0000, 
sim time next is 2820000.0000, 
raw observation next is [6.866666666666667, 24.33333333333333, 98.83333333333333, 0.0, 22.5, 27.26735815595877, 0.725538267291213, 1.0, 1.0, 55.0, 52.51210658522696], 
processed observation next is [1.0, 0.6521739130434783, 0.6528162511542014, 0.2433333333333333, 0.32944444444444443, 0.0, 0.375, 0.7722798463298975, 0.741846089097071, 1.0, 1.0, 0.8, 0.5251210658522696], 
reward next is 0.4749, 
noisyNet noise sample is [array([0.5309006], dtype=float32), 0.34134826]. 
=============================================
[2019-04-10 14:36:11,901] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.47769]
 [71.64222]
 [72.27976]
 [72.70217]
 [72.85353]], R is [[69.49880219]
 [69.3575592 ]
 [69.1139679 ]
 [68.84734344]
 [68.63725281]].
[2019-04-10 14:36:15,684] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7227778e-13 1.9008128e-07 3.0592137e-06 2.0981148e-09 9.2635229e-02
 9.9208073e-08 3.3010050e-13 5.8568759e-07 9.0735924e-01 1.7006550e-07
 1.3852459e-06], sum to 1.0000
[2019-04-10 14:36:15,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8929
[2019-04-10 14:36:15,801] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.69906645400174, 0.3084986553104767, 0.0, 1.0, 55.0, 45.95217686349902], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2959200.0000, 
sim time next is 2960400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.60385445734745, 0.2698926484799839, 0.0, 1.0, 55.0, 59.12106017001915], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5503212047789541, 0.5899642161599946, 0.0, 1.0, 0.8, 0.5912106017001915], 
reward next is 0.4088, 
noisyNet noise sample is [array([0.14061823], dtype=float32), -0.12587753]. 
=============================================
[2019-04-10 14:36:16,398] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 38790: loss 3.1725
[2019-04-10 14:36:16,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 38791: learning rate 0.0001
[2019-04-10 14:36:17,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1937025e-16 4.2829432e-10 2.4944786e-08 8.0978644e-12 3.0102450e-02
 6.0229949e-10 4.0844935e-16 1.2238477e-08 9.6989745e-01 3.4947647e-09
 6.1288397e-08], sum to 1.0000
[2019-04-10 14:36:17,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2256
[2019-04-10 14:36:18,136] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 25.54582932884109, 0.4277735147303057, 0.0, 1.0, 55.0, 59.35181229702388], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2865600.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 25.53691019175197, 0.4188641953127434, 0.0, 1.0, 55.0, 59.39687606010527], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.6280758493126642, 0.6396213984375811, 0.0, 1.0, 0.8, 0.5939687606010527], 
reward next is 0.4060, 
noisyNet noise sample is [array([0.43885896], dtype=float32), -1.2616384]. 
=============================================
[2019-04-10 14:36:18,484] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39230: loss -0.7363
[2019-04-10 14:36:18,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39230: learning rate 0.0001
[2019-04-10 14:36:18,607] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39257: loss 0.0342
[2019-04-10 14:36:18,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39257: learning rate 0.0001
[2019-04-10 14:36:18,835] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39304: loss -1.6451
[2019-04-10 14:36:18,836] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39304: learning rate 0.0001
[2019-04-10 14:36:19,279] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39398: loss -1.0238
[2019-04-10 14:36:19,284] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39398: learning rate 0.0001
[2019-04-10 14:36:19,343] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.87906699e-13 1.60445467e-07 5.39732764e-06 4.51491378e-09
 1.14189215e-01 2.29226671e-07 1.51552685e-12 5.38502661e-07
 8.85801911e-01 1.71688072e-07 2.28032150e-06], sum to 1.0000
[2019-04-10 14:36:19,343] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6085
[2019-04-10 14:36:19,427] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 77.0, 14.0, 15.66666666666666, 19.0, 24.29682929841483, 0.2307365150886666, 0.0, 1.0, 55.0, 64.22456944772321], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2965200.0000, 
sim time next is 2966400.0000, 
raw observation next is [-4.0, 77.0, 42.0, 29.0, 19.0, 24.27874240012959, 0.2395188759730903, 0.0, 1.0, 55.0, 64.17942507451448], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.77, 0.14, 0.032044198895027624, 0.08333333333333333, 0.5232285333441326, 0.5798396253243635, 0.0, 1.0, 0.8, 0.6417942507451448], 
reward next is 0.3582, 
noisyNet noise sample is [array([-1.2879796], dtype=float32), -0.35031906]. 
=============================================
[2019-04-10 14:36:20,881] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39748: loss 1.9194
[2019-04-10 14:36:20,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39748: learning rate 0.0001
[2019-04-10 14:36:21,335] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39857: loss 2.3059
[2019-04-10 14:36:21,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39857: learning rate 0.0001
[2019-04-10 14:36:21,514] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39898: loss 2.6617
[2019-04-10 14:36:21,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39900: learning rate 0.0001
[2019-04-10 14:36:22,866] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40205: loss 3.2000
[2019-04-10 14:36:22,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40205: learning rate 0.0001
[2019-04-10 14:36:23,749] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40420: loss 3.5911
[2019-04-10 14:36:23,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40420: learning rate 0.0001
[2019-04-10 14:36:23,969] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 40473: loss 3.0430
[2019-04-10 14:36:23,969] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 40473: learning rate 0.0001
[2019-04-10 14:36:24,058] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 40491: loss 1.4890
[2019-04-10 14:36:24,061] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 40491: learning rate 0.0001
[2019-04-10 14:36:24,159] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40517: loss 3.6607
[2019-04-10 14:36:24,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40517: learning rate 0.0001
[2019-04-10 14:36:25,451] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40811: loss 3.5445
[2019-04-10 14:36:25,451] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40811: learning rate 0.0001
[2019-04-10 14:36:25,563] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40832: loss 3.7302
[2019-04-10 14:36:25,564] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40832: learning rate 0.0001
[2019-04-10 14:36:25,997] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40925: loss 3.1771
[2019-04-10 14:36:26,006] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40925: learning rate 0.0001
[2019-04-10 14:36:27,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5968900e-16 1.3035198e-09 3.6998177e-08 1.7173788e-11 3.2229067e-03
 7.5345169e-10 2.8824425e-16 1.1528211e-09 9.9677700e-01 2.7318012e-09
 1.1143719e-07], sum to 1.0000
[2019-04-10 14:36:27,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9582
[2019-04-10 14:36:28,013] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 19.0, 25.64418346439698, 0.5212901620002958, 0.0, 1.0, 55.0, 37.18056817951772], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3072000.0000, 
sim time next is 3073200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 107.3333333333333, 800.8333333333334, 19.0, 25.73908992743332, 0.542740826156196, 0.0, 1.0, 55.0, 37.37750653980504], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.3577777777777777, 0.8848987108655617, 0.08333333333333333, 0.6449241606194432, 0.6809136087187321, 0.0, 1.0, 0.8, 0.3737750653980504], 
reward next is 0.6262, 
noisyNet noise sample is [array([1.7676992], dtype=float32), -0.61492836]. 
=============================================
[2019-04-10 14:36:32,352] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1891789e-18 2.8308492e-10 7.3082260e-09 2.7440683e-13 9.7850217e-03
 3.6499301e-10 3.1576677e-17 3.7806816e-10 9.9021494e-01 3.6003478e-10
 1.3181619e-08], sum to 1.0000
[2019-04-10 14:36:32,353] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3010
[2019-04-10 14:36:32,387] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 22.5, 26.63367442167833, 0.7980028929077673, 1.0, 1.0, 55.0, 41.95969705844533], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3224400.0000, 
sim time next is 3225600.0000, 
raw observation next is [-3.0, 92.0, 43.0, 226.0, 22.5, 26.62791700755474, 0.8246559934438652, 1.0, 1.0, 55.0, 42.345897871137296], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.14333333333333334, 0.24972375690607734, 0.375, 0.718993083962895, 0.7748853311479551, 1.0, 1.0, 0.8, 0.42345897871137295], 
reward next is 0.5765, 
noisyNet noise sample is [array([0.00539661], dtype=float32), -0.62012184]. 
=============================================
[2019-04-10 14:36:35,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.04654276e-16 2.47013721e-09 1.68383920e-08 7.87925974e-12
 1.01840105e-02 4.61557875e-10 4.94819919e-16 4.97122388e-09
 9.89815950e-01 3.51057183e-09 4.14757437e-08], sum to 1.0000
[2019-04-10 14:36:35,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1709
[2019-04-10 14:36:35,961] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 19.0, 26.12099273528779, 0.643905585637757, 0.0, 1.0, 55.0, 46.44571897274279], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3290400.0000, 
sim time next is 3291600.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 19.0, 26.11312227492705, 0.6322564386930446, 0.0, 1.0, 55.0, 46.46600328859454], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6760935229105876, 0.7107521462310148, 0.0, 1.0, 0.8, 0.46466003288594543], 
reward next is 0.5353, 
noisyNet noise sample is [array([0.7279818], dtype=float32), -0.0021356775]. 
=============================================
[2019-04-10 14:36:36,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.18323723e-18 1.83708687e-10 7.92057886e-09 1.57256846e-12
 4.11948236e-03 3.16518756e-10 2.45392377e-17 8.78570827e-10
 9.95880485e-01 1.06825535e-10 8.10657053e-09], sum to 1.0000
[2019-04-10 14:36:36,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0604
[2019-04-10 14:36:36,739] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 22.5, 26.70755408530905, 0.8158826119377635, 1.0, 1.0, 55.0, 41.82535313899532], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3224400.0000, 
sim time next is 3225600.0000, 
raw observation next is [-3.0, 92.0, 43.0, 226.0, 22.5, 26.69929092116953, 0.8421610187627365, 1.0, 1.0, 55.0, 42.05225738366006], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.14333333333333334, 0.24972375690607734, 0.375, 0.724940910097461, 0.7807203395875789, 1.0, 1.0, 0.8, 0.42052257383660063], 
reward next is 0.5795, 
noisyNet noise sample is [array([-1.7296913], dtype=float32), -1.1975259]. 
=============================================
[2019-04-10 14:36:37,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7273292e-19 2.3047047e-11 1.4208450e-09 2.2009960e-13 6.7787484e-04
 4.9853219e-11 8.2512900e-19 4.7546144e-11 9.9932218e-01 3.5031054e-11
 2.1602620e-09], sum to 1.0000
[2019-04-10 14:36:37,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3193
[2019-04-10 14:36:37,207] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.0, 97.66666666666667, 0.0, 0.0, 19.0, 27.5434997703954, 1.070352734070701, 0.0, 1.0, 55.0, 36.35613962025246], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3190800.0000, 
sim time next is 3192000.0000, 
raw observation next is [2.0, 95.33333333333334, 0.0, 0.0, 19.0, 27.36673741093678, 1.053213040123065, 0.0, 1.0, 55.0, 35.534419616477265], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.7805614509113982, 0.8510710133743551, 0.0, 1.0, 0.8, 0.35534419616477264], 
reward next is 0.6447, 
noisyNet noise sample is [array([-0.7496047], dtype=float32), 0.7014528]. 
=============================================
[2019-04-10 14:36:37,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.378975]
 [64.58755 ]
 [65.206375]
 [65.98457 ]
 [66.85455 ]], R is [[63.99490356]
 [63.99139404]
 [64.07744598]
 [64.15777588]
 [64.23758698]].
[2019-04-10 14:36:42,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5016807e-19 2.0769478e-11 1.5636092e-10 6.5322341e-14 1.3976669e-03
 3.1208189e-11 1.6701422e-18 2.2656746e-11 9.9860233e-01 1.5467631e-11
 1.9498860e-09], sum to 1.0000
[2019-04-10 14:36:42,951] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8858
[2019-04-10 14:36:43,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 50.0, 110.0, 776.0, 22.5, 27.69224922965608, 0.9226829691241241, 1.0, 1.0, 55.0, 24.338338207595015], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3333600.0000, 
sim time next is 3334800.0000, 
raw observation next is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 22.5, 27.77868265705011, 0.9454937463987347, 1.0, 1.0, 55.0, 24.623834641183805], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.5, 0.3577777777777777, 0.8397790055248618, 0.375, 0.8148902214208423, 0.8151645821329115, 1.0, 1.0, 0.8, 0.24623834641183806], 
reward next is 0.7538, 
noisyNet noise sample is [array([1.323643], dtype=float32), 0.7939915]. 
=============================================
[2019-04-10 14:36:47,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7382390e-18 2.6628410e-11 1.7764636e-09 1.0069440e-12 6.3063490e-04
 6.2136497e-11 6.7793953e-18 6.4621350e-11 9.9936932e-01 9.1377517e-11
 2.5326874e-09], sum to 1.0000
[2019-04-10 14:36:47,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7637
[2019-04-10 14:36:47,214] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.36173479066029, 0.8783693389202564, 1.0, 1.0, 55.0, 36.53354770720118], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3350400.0000, 
sim time next is 3351600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.20384485437822, 0.8728679446743847, 1.0, 1.0, 55.0, 33.109075703448], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7669870711981851, 0.7909559815581283, 1.0, 1.0, 0.8, 0.33109075703448], 
reward next is 0.6689, 
noisyNet noise sample is [array([-1.309579], dtype=float32), -0.31049788]. 
=============================================
[2019-04-10 14:36:51,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1987430e-14 4.7352529e-08 2.3111248e-07 6.0738042e-10 1.6712997e-03
 1.5869645e-08 1.5162708e-13 6.1932987e-08 9.9832755e-01 3.0842280e-08
 8.4921498e-07], sum to 1.0000
[2019-04-10 14:36:51,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1298
[2019-04-10 14:36:52,014] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.71475675284124, 0.76750240367287, 0.0, 1.0, 55.0, 42.48605782455665], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3553200.0000, 
sim time next is 3554400.0000, 
raw observation next is [-3.333333333333333, 67.0, 0.0, 0.0, 19.0, 26.70621768457149, 0.75076894842728, 0.0, 1.0, 55.0, 43.065934605987735], 
processed observation next is [0.0, 0.13043478260869565, 0.37026777469990774, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7255181403809576, 0.7502563161424267, 0.0, 1.0, 0.8, 0.4306593460598773], 
reward next is 0.5693, 
noisyNet noise sample is [array([-0.26952174], dtype=float32), 0.8705989]. 
=============================================
[2019-04-10 14:36:52,669] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 46714: loss 0.1064
[2019-04-10 14:36:52,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 46714: learning rate 0.0001
[2019-04-10 14:36:54,780] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47173: loss 0.1435
[2019-04-10 14:36:54,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47173: learning rate 0.0001
[2019-04-10 14:36:55,244] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.39392186e-19 2.05051826e-11 4.21261193e-10 1.64960439e-13
 3.38331702e-05 1.42527535e-11 1.05201140e-18 1.01886260e-11
 9.99966145e-01 1.97919511e-11 2.15072168e-10], sum to 1.0000
[2019-04-10 14:36:55,244] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7451
[2019-04-10 14:36:55,277] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47283: loss 0.1801
[2019-04-10 14:36:55,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47283: learning rate 0.0001
[2019-04-10 14:36:55,284] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 22.5, 27.48486509794171, 0.8936509392039231, 1.0, 1.0, 55.0, 32.73011529643447], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3489600.0000, 
sim time next is 3490800.0000, 
raw observation next is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 22.5, 27.69447760185948, 0.9423294888291384, 1.0, 1.0, 55.0, 28.684646350574305], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6366666666666667, 0.33555555555555566, 0.7587476979742174, 0.375, 0.8078731334882899, 0.8141098296097128, 1.0, 1.0, 0.8, 0.28684646350574305], 
reward next is 0.7132, 
noisyNet noise sample is [array([-0.36811697], dtype=float32), 0.6726973]. 
=============================================
[2019-04-10 14:36:55,498] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47327: loss -3.5670
[2019-04-10 14:36:55,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47328: learning rate 0.0001
[2019-04-10 14:36:56,031] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47421: loss 0.1373
[2019-04-10 14:36:56,033] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47421: learning rate 0.0001
[2019-04-10 14:36:56,836] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47543: loss 0.1900
[2019-04-10 14:36:56,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47543: learning rate 0.0001
[2019-04-10 14:36:57,434] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.8870738e-17 3.3351977e-10 2.3770683e-08 4.4244361e-12 6.6864881e-04
 7.4703782e-10 2.7488612e-16 2.2690616e-09 9.9933136e-01 1.0527750e-09
 3.9160224e-08], sum to 1.0000
[2019-04-10 14:36:57,434] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-10 14:36:57,481] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 55.0, 116.0, 819.5, 19.0, 26.67929152054669, 0.791673261198032, 0.0, 1.0, 55.0, 36.44875645078697], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3585600.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 19.0, 26.71185851135976, 0.8037969932818206, 0.0, 1.0, 55.0, 36.239819088070846], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 0.08333333333333333, 0.7259882092799801, 0.7679323310939402, 0.0, 1.0, 0.8, 0.36239819088070846], 
reward next is 0.6376, 
noisyNet noise sample is [array([-0.10568222], dtype=float32), -1.1500555]. 
=============================================
[2019-04-10 14:36:57,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6482819e-14 2.2021947e-08 9.0633769e-08 1.1939072e-09 4.4454541e-03
 1.5166052e-08 6.8578755e-14 4.7368630e-08 9.9555367e-01 5.9875568e-08
 6.3699804e-07], sum to 1.0000
[2019-04-10 14:36:57,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0509
[2019-04-10 14:36:57,608] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.15634680110822, 0.8401569097044402, 0.0, 1.0, 55.0, 35.559708110469074], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3609600.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.18209985670195, 0.8361564172653863, 0.0, 1.0, 55.0, 34.07015593638038], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7651749880584958, 0.7787188057551288, 0.0, 1.0, 0.8, 0.3407015593638038], 
reward next is 0.6593, 
noisyNet noise sample is [array([-0.05523742], dtype=float32), 0.024713807]. 
=============================================
[2019-04-10 14:36:58,110] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47806: loss 0.1611
[2019-04-10 14:36:58,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47806: learning rate 0.0001
[2019-04-10 14:36:58,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5025909e-14 3.2496683e-08 4.2556999e-07 8.8150248e-10 2.8324293e-03
 1.9892768e-08 1.0871407e-13 1.2336710e-07 9.9716645e-01 2.7572844e-08
 4.6471928e-07], sum to 1.0000
[2019-04-10 14:36:58,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7008
[2019-04-10 14:36:58,197] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 27.06739552018899, 0.755211478917878, 0.0, 1.0, 55.0, 35.77550657340677], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3630000.0000, 
sim time next is 3631200.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 27.13504552869046, 0.7581272752576047, 0.0, 1.0, 55.0, 34.91018582133168], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7612537940575382, 0.7527090917525349, 0.0, 1.0, 0.8, 0.3491018582133168], 
reward next is 0.6509, 
noisyNet noise sample is [array([-0.2207569], dtype=float32), -0.50236094]. 
=============================================
[2019-04-10 14:36:58,227] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47830: loss 0.1504
[2019-04-10 14:36:58,233] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47830: learning rate 0.0001
[2019-04-10 14:37:01,217] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 48297: loss 0.2518
[2019-04-10 14:37:01,218] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 48297: learning rate 0.0001
[2019-04-10 14:37:01,583] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48335: loss 0.2310
[2019-04-10 14:37:01,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48335: learning rate 0.0001
[2019-04-10 14:37:02,627] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48431: loss 0.2946
[2019-04-10 14:37:02,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48431: learning rate 0.0001
[2019-04-10 14:37:03,770] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 48567: loss 0.2844
[2019-04-10 14:37:03,770] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 48567: learning rate 0.0001
[2019-04-10 14:37:04,073] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48614: loss 0.2409
[2019-04-10 14:37:04,074] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48614: learning rate 0.0001
[2019-04-10 14:37:05,850] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48824: loss 0.3383
[2019-04-10 14:37:05,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48824: learning rate 0.0001
[2019-04-10 14:37:07,145] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48961: loss 0.4206
[2019-04-10 14:37:07,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48961: learning rate 0.0001
[2019-04-10 14:37:08,468] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0667209e-16 3.3200498e-10 1.3615184e-08 3.4943378e-11 6.4884452e-04
 2.5509035e-09 7.9404865e-16 3.6467112e-09 9.9935108e-01 1.0642291e-09
 3.5519378e-08], sum to 1.0000
[2019-04-10 14:37:08,468] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3477
[2019-04-10 14:37:08,547] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 19.0, 27.65106139239758, 0.946464978310518, 0.0, 1.0, 55.0, 32.2566415653625], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3700800.0000, 
sim time next is 3702000.0000, 
raw observation next is [2.666666666666667, 62.66666666666667, 0.0, 0.0, 19.0, 27.6059334306066, 0.9400223210804564, 0.0, 1.0, 55.0, 32.9072962713032], 
processed observation next is [0.0, 0.8695652173913043, 0.5364727608494922, 0.6266666666666667, 0.0, 0.0, 0.08333333333333333, 0.8004944525505501, 0.8133407736934855, 0.0, 1.0, 0.8, 0.32907296271303205], 
reward next is 0.6709, 
noisyNet noise sample is [array([1.7569474], dtype=float32), 0.08513838]. 
=============================================
[2019-04-10 14:37:08,723] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 49134: loss 0.3947
[2019-04-10 14:37:08,741] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 49134: learning rate 0.0001
[2019-04-10 14:37:08,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.02619 ]
 [53.016373]
 [53.13303 ]
 [53.3103  ]
 [53.60599 ]], R is [[53.25333405]
 [53.39823532]
 [53.54916763]
 [53.70586014]
 [53.88311768]].
[2019-04-10 14:37:17,391] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 14:37:17,393] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-10 14:37:17,393] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:17,395] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run2
[2019-04-10 14:37:17,502] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-10 14:37:17,502] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:17,504] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run2
[2019-04-10 14:37:17,661] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-10 14:37:17,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 14:37:17,663] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_lightx/3/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run2
[2019-04-10 14:38:02,850] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.1661987], dtype=float32), 0.15574282]
[2019-04-10 14:38:02,851] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [19.75972464333334, 71.24978360333333, 0.0, 0.0, 19.0, 28.43249961508772, 1.278840611248852, 0.0, 0.0, 55.0, 19.819015828936674]
[2019-04-10 14:38:02,851] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-10 14:38:02,852] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [1.2857887e-15 6.1494378e-09 2.0504217e-07 2.2603097e-10 1.4338393e-03
 1.0688306e-08 4.8219032e-15 1.3104758e-08 9.9856573e-01 5.8923173e-09
 2.2593628e-07], sampled 0.9515224303642088
[2019-04-10 14:39:16,057] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2758.3182 149017.4015 1436.0085
[2019-04-10 14:39:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:16,077] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:16,185] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:16,185] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:21,052] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2601.1458 164685.4199 981.3313
[2019-04-10 14:39:21,074] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:21,074] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:21,190] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:21,190] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:24,280] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2649.5085 159857.1684 1184.7525
[2019-04-10 14:39:24,301] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:24,301] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-10 14:39:24,406] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:24,406] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-10 14:39:25,303] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 50000, evaluation results [50000.0, 2649.5084981848445, 159857.16844770202, 1184.752494781404, 2758.3181968519402, 149017.40148652828, 1436.0085436718596, 2601.1458011520062, 164685.4198847997, 981.3313374241211]
[2019-04-10 14:39:25,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0687696e-17 9.8200642e-11 1.3305319e-09 9.3063111e-13 2.0983985e-04
 2.6373201e-10 8.4805739e-17 1.9426659e-10 9.9979013e-01 1.3880508e-10
 1.0064347e-08], sum to 1.0000
[2019-04-10 14:39:25,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2395
[2019-04-10 14:39:25,405] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.61726080041678, 0.7153440915808313, 0.0, 1.0, 55.0, 46.3428499901514], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3812400.0000, 
sim time next is 3813600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.57251023314459, 0.7073730469259325, 0.0, 1.0, 55.0, 45.685824268496404], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7143758527620493, 0.7357910156419775, 0.0, 1.0, 0.8, 0.45685824268496406], 
reward next is 0.5431, 
noisyNet noise sample is [array([-0.8441438], dtype=float32), 0.8212601]. 
=============================================
[2019-04-10 14:39:26,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6071746e-17 1.7717657e-10 3.5401253e-08 1.7488067e-11 3.5057458e-04
 8.3815338e-10 3.3049550e-16 1.9209176e-09 9.9964941e-01 8.1975793e-10
 4.2408320e-08], sum to 1.0000
[2019-04-10 14:39:26,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-10 14:39:26,146] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.666666666666667, 62.66666666666667, 0.0, 0.0, 19.0, 27.60690483374835, 0.9402992836310707, 0.0, 1.0, 55.0, 32.897499779933156], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3702000.0000, 
sim time next is 3703200.0000, 
raw observation next is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 19.0, 27.59308452941931, 0.9264507107313542, 0.0, 1.0, 55.0, 32.6684888352553], 
processed observation next is [0.0, 0.8695652173913043, 0.5272391505078486, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.7994237107849426, 0.8088169035771181, 0.0, 1.0, 0.8, 0.32668488835255305], 
reward next is 0.6733, 
noisyNet noise sample is [array([-0.24013038], dtype=float32), 0.21311915]. 
=============================================
[2019-04-10 14:39:27,647] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0838881e-22 2.2076165e-13 1.1832477e-11 6.7028724e-16 5.1104638e-05
 7.4822429e-13 4.4927604e-21 4.0844376e-13 9.9994886e-01 3.5209921e-13
 4.1364835e-11], sum to 1.0000
[2019-04-10 14:39:27,648] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7578
[2019-04-10 14:39:27,744] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 22.5, 28.77430129039073, 1.050521193206371, 1.0, 1.0, 55.0, 36.19361939360425], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3772800.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 22.5, 27.34533623494719, 1.103020378389635, 1.0, 1.0, 55.0, 11.218057677485007], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.375, 0.7787780195789326, 0.8676734594632117, 1.0, 1.0, 0.8, 0.11218057677485006], 
reward next is 0.8878, 
noisyNet noise sample is [array([-0.4117752], dtype=float32), 1.6986817]. 
=============================================
[2019-04-10 14:39:27,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.18367]
 [72.29595]
 [72.57073]
 [72.8024 ]
 [73.04008]], R is [[71.81691742]
 [71.73680878]
 [71.85334778]
 [71.97058868]
 [72.19228363]].
[2019-04-10 14:39:29,655] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.1876026e-21 1.6450049e-12 3.4703230e-11 2.8173353e-15 2.4707444e-05
 1.3251381e-12 9.7840458e-21 1.9887067e-12 9.9997532e-01 2.5085292e-12
 1.8358447e-10], sum to 1.0000
[2019-04-10 14:39:29,655] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7220
[2019-04-10 14:39:29,707] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 60.0, 117.0, 828.5, 22.5, 27.97702352609592, 0.7446953014967258, 1.0, 1.0, 55.0, 29.512945985062046], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3844800.0000, 
sim time next is 3846000.0000, 
raw observation next is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 22.5, 27.54336912869961, 0.9947472636882114, 1.0, 1.0, 55.0, 20.850606531697913], 
processed observation next is [1.0, 0.5217391304347826, 0.4533702677747, 0.5700000000000001, 0.39, 0.9202578268876612, 0.375, 0.7952807607249675, 0.8315824212294038, 1.0, 1.0, 0.8, 0.20850606531697913], 
reward next is 0.7915, 
noisyNet noise sample is [array([-2.807824], dtype=float32), 1.5418338]. 
=============================================
[2019-04-10 14:39:29,716] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[70.52826 ]
 [70.499054]
 [70.50784 ]
 [70.391235]
 [70.23487 ]], R is [[70.53308868]
 [70.53263092]
 [70.60354614]
 [70.55379486]
 [70.61460114]].
[2019-04-10 14:39:36,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5005036e-16 4.3814793e-10 3.2594848e-08 1.0756048e-11 2.8479178e-04
 1.7449651e-09 2.5530861e-15 2.5580607e-09 9.9971515e-01 1.6017464e-09
 1.2723052e-08], sum to 1.0000
[2019-04-10 14:39:36,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5688
[2019-04-10 14:39:36,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 25.53144866738155, 0.4654654144062322, 0.0, 1.0, 55.0, 46.25603520333506], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3982800.0000, 
sim time next is 3984000.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 25.4670395525992, 0.4408321774242938, 0.0, 1.0, 55.0, 46.262857316660146], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6222532960499333, 0.6469440591414313, 0.0, 1.0, 0.8, 0.46262857316660144], 
reward next is 0.5374, 
noisyNet noise sample is [array([-1.0060831], dtype=float32), 0.011554296]. 
=============================================
[2019-04-10 14:39:36,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[50.277138]
 [50.24976 ]
 [50.050587]
 [49.785946]
 [50.175613]], R is [[50.028862  ]
 [50.06601334]
 [50.10286331]
 [50.13993454]
 [50.1769371 ]].
[2019-04-10 14:39:37,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9596664e-16 7.2023781e-10 3.1000482e-08 3.6980998e-11 2.8065025e-04
 1.0545124e-09 1.9035566e-15 2.4318438e-09 9.9971932e-01 4.0741863e-10
 5.4288538e-08], sum to 1.0000
[2019-04-10 14:39:37,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7176
[2019-04-10 14:39:37,121] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-13.0, 63.0, 46.5, 222.0, 22.5, 24.50469180263268, 0.2269474137301466, 1.0, 1.0, 55.0, 44.96192569542835], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4003200.0000, 
sim time next is 4004400.0000, 
raw observation next is [-12.33333333333333, 59.66666666666667, 77.5, 370.0, 22.5, 24.60996891189613, 0.3106865539697093, 1.0, 1.0, 55.0, 44.75939586382287], 
processed observation next is [1.0, 0.34782608695652173, 0.12096029547553101, 0.5966666666666667, 0.25833333333333336, 0.4088397790055249, 0.375, 0.5508307426580107, 0.6035621846565697, 1.0, 1.0, 0.8, 0.44759395863822865], 
reward next is 0.5524, 
noisyNet noise sample is [array([-0.90385413], dtype=float32), -1.7617447]. 
=============================================
[2019-04-10 14:39:40,033] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 54604: loss 0.2956
[2019-04-10 14:39:40,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 54604: learning rate 0.0001
[2019-04-10 14:39:40,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7458650e-18 4.8758279e-11 1.6912604e-09 1.4461239e-12 8.4625999e-04
 2.5024485e-10 1.3376263e-17 1.2699704e-10 9.9915373e-01 1.1313160e-10
 3.7237493e-09], sum to 1.0000
[2019-04-10 14:39:40,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5696
[2019-04-10 14:39:40,701] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 24.0, 43.5, 370.5, 22.5, 27.58125913315442, 0.9281957154909429, 1.0, 1.0, 55.0, 18.151835115742212], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4035600.0000, 
sim time next is 4036800.0000, 
raw observation next is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 22.5, 27.95452289580188, 0.9332094906639892, 1.0, 1.0, 55.0, 37.67033995291013], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.24666666666666662, 0.09277777777777776, 0.2786372007366483, 0.375, 0.8295435746501566, 0.8110698302213297, 1.0, 1.0, 0.8, 0.3767033995291013], 
reward next is 0.6233, 
noisyNet noise sample is [array([1.190462], dtype=float32), 0.08453351]. 
=============================================
[2019-04-10 14:39:42,086] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55241: loss 0.2477
[2019-04-10 14:39:42,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55245: learning rate 0.0001
[2019-04-10 14:39:42,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55252: loss 0.2526
[2019-04-10 14:39:42,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55252: learning rate 0.0001
[2019-04-10 14:39:42,225] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55282: loss 0.2480
[2019-04-10 14:39:42,227] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55282: learning rate 0.0001
[2019-04-10 14:39:42,368] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55325: loss 0.2350
[2019-04-10 14:39:42,370] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55327: learning rate 0.0001
[2019-04-10 14:39:43,528] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55711: loss 0.2153
[2019-04-10 14:39:43,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55711: learning rate 0.0001
[2019-04-10 14:39:43,786] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55797: loss 0.1892
[2019-04-10 14:39:43,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55797: learning rate 0.0001
[2019-04-10 14:39:44,281] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55964: loss 0.1886
[2019-04-10 14:39:44,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55964: learning rate 0.0001
[2019-04-10 14:39:44,409] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 56006: loss 0.1921
[2019-04-10 14:39:44,414] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 56006: learning rate 0.0001
[2019-04-10 14:39:44,494] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3038545e-15 5.7386798e-09 1.5282414e-07 5.4460703e-11 2.1629720e-03
 9.2283745e-09 1.4438632e-15 1.0117810e-08 9.9783689e-01 6.0057737e-09
 5.0039073e-08], sum to 1.0000
[2019-04-10 14:39:44,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2793
[2019-04-10 14:39:44,534] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 45.66666666666667, 0.0, 0.0, 19.0, 27.08762372643471, 0.7824213425783202, 0.0, 1.0, 55.0, 38.28861987468916], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4225200.0000, 
sim time next is 4226400.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 19.0, 27.09712395350487, 0.7739544309317555, 0.0, 1.0, 55.0, 37.77914479856703], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7580936627920725, 0.7579848103105852, 0.0, 1.0, 0.8, 0.3777914479856703], 
reward next is 0.6222, 
noisyNet noise sample is [array([-0.96976227], dtype=float32), 2.2405856]. 
=============================================
[2019-04-10 14:39:45,343] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56316: loss 0.1785
[2019-04-10 14:39:45,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56316: learning rate 0.0001
[2019-04-10 14:39:45,721] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56456: loss 0.1692
[2019-04-10 14:39:45,722] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56456: learning rate 0.0001
[2019-04-10 14:39:45,871] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 56510: loss 0.1753
[2019-04-10 14:39:45,871] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 56510: learning rate 0.0001
[2019-04-10 14:39:45,956] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56540: loss 0.1662
[2019-04-10 14:39:45,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56540: learning rate 0.0001
[2019-04-10 14:39:46,534] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56743: loss 0.1693
[2019-04-10 14:39:46,534] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56743: learning rate 0.0001
[2019-04-10 14:39:47,484] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 57079: loss 0.1599
[2019-04-10 14:39:47,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 57079: learning rate 0.0001
[2019-04-10 14:39:48,004] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 57271: loss 0.1596
[2019-04-10 14:39:48,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 57271: learning rate 0.0001
[2019-04-10 14:39:49,934] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2906097e-23 4.4305454e-14 2.3870365e-12 1.1751190e-16 6.9978291e-06
 1.1312500e-13 3.7739043e-23 2.3886473e-13 9.9999297e-01 2.6571195e-14
 1.8393950e-11], sum to 1.0000
[2019-04-10 14:39:49,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5103
[2019-04-10 14:39:49,961] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 22.5, 28.05061232506452, 1.101314322950371, 1.0, 1.0, 55.0, 17.581850600422616], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4458000.0000, 
sim time next is 4459200.0000, 
raw observation next is [0.0, 87.33333333333334, 82.66666666666667, 0.0, 22.5, 28.36211455042603, 1.110263017834796, 1.0, 1.0, 55.0, 23.90617033514919], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8733333333333334, 0.27555555555555555, 0.0, 0.375, 0.8635095458688357, 0.8700876726115987, 1.0, 1.0, 0.8, 0.2390617033514919], 
reward next is 0.7609, 
noisyNet noise sample is [array([1.0179152], dtype=float32), 1.2572289]. 
=============================================
[2019-04-10 14:39:50,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4102120e-18 4.1265791e-10 1.7001512e-09 1.8770927e-12 9.1431117e-05
 2.8402136e-10 8.4208409e-17 8.9692209e-10 9.9990857e-01 2.1366924e-10
 8.0215656e-09], sum to 1.0000
[2019-04-10 14:39:50,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3064
[2019-04-10 14:39:50,161] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.0, 26.83552752044191, 0.6767008807091387, 0.0, 1.0, 55.0, 40.88637452728682], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4255200.0000, 
sim time next is 4256400.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.81246752918103, 0.6731195182483507, 0.0, 1.0, 55.0, 40.992835783806356], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7343722940984193, 0.7243731727494502, 0.0, 1.0, 0.8, 0.4099283578380636], 
reward next is 0.5901, 
noisyNet noise sample is [array([0.23438838], dtype=float32), 1.2241634]. 
=============================================
[2019-04-10 14:39:52,313] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.4071244e-20 7.4626581e-12 2.5553828e-10 1.2610223e-13 2.3379143e-05
 1.1110339e-11 6.4126825e-19 5.9119702e-11 9.9997663e-01 1.0613375e-11
 2.8954805e-09], sum to 1.0000
[2019-04-10 14:39:52,314] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8326
[2019-04-10 14:39:52,359] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 19.0, 27.91736320363643, 1.083271125469037, 0.0, 1.0, 55.0, 27.02036139932579], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4413600.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 19.0, 27.89160300218059, 1.075636828175192, 0.0, 1.0, 55.0, 28.752845072257134], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.08333333333333333, 0.8243002501817159, 0.8585456093917306, 0.0, 1.0, 0.8, 0.2875284507225713], 
reward next is 0.7125, 
noisyNet noise sample is [array([-1.0273671], dtype=float32), -1.209313]. 
=============================================
[2019-04-10 14:39:55,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5463219e-21 1.4796909e-12 1.3812014e-11 1.3663888e-15 2.3516393e-05
 6.4993436e-12 2.9891997e-20 3.9968810e-12 9.9997652e-01 9.4557988e-13
 3.1161018e-11], sum to 1.0000
[2019-04-10 14:39:55,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7682
[2019-04-10 14:39:55,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.0, 52.0, 187.0, 24.0, 22.5, 26.00643025197526, 0.828434815806407, 1.0, 1.0, 55.0, 29.880445818999846], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4539600.0000, 
sim time next is 4540800.0000, 
raw observation next is [2.333333333333333, 51.0, 227.0, 40.0, 22.5, 27.69687596300466, 0.9609192790398443, 1.0, 1.0, 55.0, 19.842728658824516], 
processed observation next is [1.0, 0.5652173913043478, 0.5272391505078486, 0.51, 0.7566666666666667, 0.04419889502762431, 0.375, 0.8080729969170551, 0.8203064263466148, 1.0, 1.0, 0.8, 0.19842728658824516], 
reward next is 0.8016, 
noisyNet noise sample is [array([0.82827944], dtype=float32), -0.28090367]. 
=============================================
[2019-04-10 14:39:59,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7411716e-18 7.7717201e-11 4.8854663e-09 1.2647836e-12 4.3835415e-05
 1.0538266e-10 1.7155125e-17 3.6822173e-10 9.9995613e-01 6.1386562e-11
 4.0575703e-09], sum to 1.0000
[2019-04-10 14:39:59,749] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4126
[2019-04-10 14:39:59,774] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 27.43426765348554, 0.9581708509818555, 0.0, 1.0, 55.0, 32.85087206192489], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4662000.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 27.44097018154266, 0.9568628568089489, 0.0, 1.0, 55.0, 32.14815858998686], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.08333333333333333, 0.786747515128555, 0.8189542856029829, 0.0, 1.0, 0.8, 0.32148158589986864], 
reward next is 0.6785, 
noisyNet noise sample is [array([-1.0854369], dtype=float32), 0.6051714]. 
=============================================
[2019-04-10 14:40:02,254] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4109737e-22 2.9717151e-14 2.7883024e-12 7.1188344e-16 1.2251456e-06
 1.4791391e-13 2.7456267e-21 7.1625372e-13 9.9999881e-01 5.6048170e-14
 3.5974255e-12], sum to 1.0000
[2019-04-10 14:40:02,257] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-10 14:40:02,289] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 72.0, 88.16666666666667, 13.83333333333333, 22.5, 27.97419282256416, 1.008040955554055, 1.0, 1.0, 55.0, 27.341500481038167], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4725600.0000, 
sim time next is 4726800.0000, 
raw observation next is [1.0, 72.0, 64.5, 19.5, 22.5, 28.08638473480573, 1.028796890547955, 1.0, 1.0, 55.0, 28.578061631930872], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.72, 0.215, 0.02154696132596685, 0.375, 0.8405320612338109, 0.8429322968493184, 1.0, 1.0, 0.8, 0.28578061631930873], 
reward next is 0.7142, 
noisyNet noise sample is [array([-1.3229207], dtype=float32), -0.3633993]. 
=============================================
