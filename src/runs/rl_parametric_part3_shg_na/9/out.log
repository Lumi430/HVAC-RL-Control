Using TensorFlow backend.
[2019-03-26 23:55:16,127] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-26 23:55:16,127] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-26 23:55:16.170086: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-26 23:55:35,028] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-26 23:55:35,028] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-26 23:55:35,037] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-26 23:55:35,042] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-26 23:55:35,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-26 23:55:35,052] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-26 23:55:35,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-26 23:55:35,056] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:35,056] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-26 23:55:35,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:35,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-26 23:55:36,057] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:36,060] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-26 23:55:36,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-26 23:55:36,365] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 23:55:36,365] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:55:36,366] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:55:36,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,366] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:55:36,367] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,367] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:55:36,368] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,368] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,366] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:55:36,369] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:36,372] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-26 23:55:36,379] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-26 23:55:36,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-26 23:55:36,387] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-26 23:55:36,405] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-26 23:55:37,061] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:37,062] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-26 23:55:37,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:37,143] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-26 23:55:38,063] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:38,067] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-26 23:55:38,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:38,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-26 23:55:39,067] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:39,071] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-26 23:55:39,135] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:39,136] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-26 23:55:40,073] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:40,082] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-26 23:55:40,143] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:40,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-26 23:55:41,078] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:41,081] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-26 23:55:41,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:41,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-26 23:55:42,084] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:42,088] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-26 23:55:42,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:42,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-26 23:55:43,087] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:43,094] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-26 23:55:43,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:43,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-26 23:55:44,093] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:44,096] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-26 23:55:44,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:44,160] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-26 23:55:45,096] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:45,100] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-26 23:55:45,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:45,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-26 23:55:46,101] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:46,106] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-26 23:55:46,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:46,166] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-26 23:55:47,104] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:47,110] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-26 23:55:47,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:47,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-26 23:55:48,111] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:48,116] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-26 23:55:48,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:48,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-26 23:55:49,116] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:49,120] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-26 23:55:49,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:49,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-26 23:55:50,120] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 23:55:50,125] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-26 23:55:50,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:55:50,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-26 23:56:00,159] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:00,161] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.93333333333334, 35.33333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5473943346076103, 6.911200000000001, 6.9112, 168.912956510431, 493686.3430818132, 493686.3430818126, 157572.0827081275]
[2019-03-26 23:56:00,164] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:56:00,167] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.15044384 0.15512371 0.32964134 0.18922286 0.17556827], sampled 0.5578893713427794
[2019-03-26 23:56:02,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:02,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.4, 78.66666666666667, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 526524.474933236, 526524.474933236, 239201.4773710336]
[2019-03-26 23:56:02,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:56:02,151] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.12917192 0.14416754 0.28154594 0.22431262 0.22080205], sampled 0.9146567037718518
[2019-03-26 23:56:08,537] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:08,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.93333333333333, 86.33333333333334, 1.0, 2.0, 0.5436796324941591, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9192914186339718, 6.9112, 6.9112, 168.9129564632759, 1555262.117751719, 1555262.117751719, 330338.7738762386]
[2019-03-26 23:56:08,539] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:56:08,541] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.10482359 0.16200079 0.3407459  0.21249413 0.17993559], sampled 0.1462024909206484
[2019-03-26 23:56:26,211] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:26,212] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.85, 59.0, 1.0, 2.0, 0.296068053993634, 1.0, 2.0, 0.296068053993634, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 827460.9950987766, 827460.9950987766, 249615.9151998947]
[2019-03-26 23:56:26,213] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:56:26,216] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.17540134 0.15577126 0.26521772 0.24236979 0.1612399 ], sampled 0.9944672601467588
[2019-03-26 23:56:47,971] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:47,972] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 70.0, 1.0, 2.0, 0.9349914162172932, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989548008412267, 6.9112, 168.9124898879725, 2204014.042061564, 2148431.421911109, 444553.3718088109]
[2019-03-26 23:56:47,973] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:56:47,976] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.12147649 0.11664151 0.3117113  0.17983869 0.270332  ], sampled 0.9904597625119044
[2019-03-26 23:56:53,716] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:56:53,718] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.18224446333333, 85.01402279333334, 1.0, 2.0, 0.2776486079077732, 1.0, 2.0, 0.2776486079077732, 1.0, 2.0, 0.4796360354036567, 6.911200000000001, 6.9112, 171.5212843490159, 1164157.660742367, 1164157.660742366, 295482.9604199152]
[2019-03-26 23:56:53,718] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:56:53,720] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.15193357 0.10757869 0.32133463 0.2081405  0.21101259], sampled 0.9108869048658126
[2019-03-26 23:57:14,697] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:57:14,699] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.66666666666666, 94.16666666666667, 1.0, 2.0, 0.2421541439738791, 0.0, 1.0, 0.0, 1.0, 1.0, 0.40388871795906, 6.9112, 6.9112, 168.912956510431, 676738.6020357349, 676738.6020357349, 203888.9868775463]
[2019-03-26 23:57:14,700] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:57:14,702] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.11771605 0.11322135 0.3790457  0.23697701 0.15303989], sampled 0.4625723707031184
[2019-03-26 23:57:25,838] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:57:25,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.88832346, 94.48574152, 1.0, 2.0, 0.2000278372103529, 1.0, 1.0, 0.2000278372103529, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 610010.8992916497, 610010.8992916497, 242191.0873155043]
[2019-03-26 23:57:25,842] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:57:25,845] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.1125887  0.15116471 0.2853907  0.22220092 0.22865501], sampled 0.8010381319604634
[2019-03-26 23:57:29,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 23:57:29,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.63333333333334, 83.66666666666667, 1.0, 2.0, 0.5565833941960787, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9626238738528183, 6.911199999999999, 6.9112, 168.9126991045388, 1556105.453854697, 1556105.453854697, 339729.7679968454]
[2019-03-26 23:57:29,565] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:57:29,569] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.13784209 0.0992061  0.37059554 0.22199406 0.17036223], sampled 0.79242453914926
[2019-03-26 23:57:33,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3514.2112 3497802968.1001 1313.0000
[2019-03-26 23:57:33,256] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3688.2057 3284667392.8626 832.0000
[2019-03-26 23:57:33,262] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3578.0453 3342506979.4898 1186.0000
[2019-03-26 23:57:33,353] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3658.2205 3206815193.4468 742.0000
[2019-03-26 23:57:33,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3734.0335 3170375089.9447 597.0000
[2019-03-26 23:57:34,482] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3514.2111656732945, 3497802968.100052, 1313.0, 3688.205654764485, 3284667392.8626304, 832.0, 3734.033508004338, 3170375089.9446564, 597.0, 3578.045319921008, 3342506979.489752, 1186.0, 3658.220548856175, 3206815193.4468307, 742.0]
[2019-03-26 23:57:41,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.04981362 0.1484872  0.44041124 0.12699929 0.23428868], sum to 1.0000
[2019-03-26 23:57:41,779] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3849
[2019-03-26 23:57:41,892] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.3, 89.0, 1.0, 2.0, 0.1728332839228221, 0.0, 2.0, 0.0, 1.0, 1.0, 0.3084313759768742, 6.9112, 6.9112, 168.912956510431, 536162.2300257292, 536162.2300257292, 195473.7437646182], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 87000.0000, 
sim time next is 87600.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.1727943710667832, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3082698711962065, 6.911199999999999, 6.9112, 168.912956510431, 535782.427160749, 535782.4271607496, 195441.8134295495], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 1.0, 1.0, 0.003366712128654434, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.15642667219049575, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14882845198909694, 0.1488284519890971, 0.29170419914858137], 
reward next is 0.7083, 
noisyNet noise sample is [array([0.40288287], dtype=float32), -1.0411725]. 
=============================================
[2019-03-26 23:57:48,927] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.03428145 0.04507705 0.6933075  0.1838608  0.04347324], sum to 1.0000
[2019-03-26 23:57:48,936] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8304
[2019-03-26 23:57:49,048] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.48333333333333, 93.0, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2665166175488694, 6.911199999999999, 6.9112, 168.912956510431, 469284.0919048819, 469284.0919048826, 186562.913989374], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 204600.0000, 
sim time next is 205200.0000, 
raw observation next is [20.5, 93.0, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 470120.2340613435, 470120.2340613435, 227649.1621049824], 
processed observation next is [0.0, 0.391304347826087, 0.1706161137440759, 0.93, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.13058895390592876, 0.13058895390592876, 0.33977486881340657], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47114438], dtype=float32), 0.8984186]. 
=============================================
[2019-03-26 23:57:54,351] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7922: loss 0.1615
[2019-03-26 23:57:54,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7923: learning rate 0.0001
[2019-03-26 23:57:54,458] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7948: loss 0.0974
[2019-03-26 23:57:54,463] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7949: learning rate 0.0001
[2019-03-26 23:57:54,483] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7959: loss 0.0819
[2019-03-26 23:57:54,485] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7960: learning rate 0.0001
[2019-03-26 23:57:54,493] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7967: loss 0.0613
[2019-03-26 23:57:54,494] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7967: loss 0.0259
[2019-03-26 23:57:54,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7967: learning rate 0.0001
[2019-03-26 23:57:54,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7968: learning rate 0.0001
[2019-03-26 23:57:54,519] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7976: loss 0.0668
[2019-03-26 23:57:54,520] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7976: loss -3.5276
[2019-03-26 23:57:54,521] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7976: learning rate 0.0001
[2019-03-26 23:57:54,523] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7976: learning rate 0.0001
[2019-03-26 23:57:54,540] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7984: loss 0.0482
[2019-03-26 23:57:54,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7984: learning rate 0.0001
[2019-03-26 23:57:54,546] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7985: loss 0.0642
[2019-03-26 23:57:54,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7985: learning rate 0.0001
[2019-03-26 23:57:54,568] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7996: loss 0.0613
[2019-03-26 23:57:54,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7998: learning rate 0.0001
[2019-03-26 23:57:54,577] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8000: loss 0.0530
[2019-03-26 23:57:54,579] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8000: learning rate 0.0001
[2019-03-26 23:57:54,580] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8000: loss -2.6132
[2019-03-26 23:57:54,582] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8000: learning rate 0.0001
[2019-03-26 23:57:54,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8004: loss 0.0288
[2019-03-26 23:57:54,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8005: learning rate 0.0001
[2019-03-26 23:57:54,645] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8031: loss -2.7166
[2019-03-26 23:57:54,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8031: learning rate 0.0001
[2019-03-26 23:57:54,649] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8031: loss 0.0156
[2019-03-26 23:57:54,652] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8031: learning rate 0.0001
[2019-03-26 23:57:54,669] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8039: loss 0.0038
[2019-03-26 23:57:54,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8039: learning rate 0.0001
[2019-03-26 23:57:57,197] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0584034e-08 9.9992085e-01 7.5543001e-05 1.7859543e-08 3.6107081e-06], sum to 1.0000
[2019-03-26 23:57:57,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3059
[2019-03-26 23:57:57,316] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 86.0, 1.0, 2.0, 0.26683578064081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 433317.0466089039, 433317.0466089046, 162490.0351588493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 344400.0000, 
sim time next is 345000.0000, 
raw observation next is [20.61666666666667, 86.0, 1.0, 2.0, 0.2667518460789443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433298.7257239028, 433298.7257239028, 162486.9835625219], 
processed observation next is [0.0, 1.0, 0.1761453396524489, 0.86, 1.0, 1.0, 0.11656848925174013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12036075714552856, 0.12036075714552856, 0.24251788591421178], 
reward next is 0.7575, 
noisyNet noise sample is [array([-2.304104], dtype=float32), 0.36619475]. 
=============================================
[2019-03-26 23:57:57,335] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.002556]
 [76.974106]
 [76.92504 ]
 [76.88157 ]
 [76.83966 ]], R is [[77.00744629]
 [76.99485016]
 [76.98231506]
 [76.969841  ]
 [76.95739746]].
[2019-03-26 23:58:01,093] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7386280e-08 9.9999952e-01 3.3252172e-07 6.3052774e-10 1.2195167e-07], sum to 1.0000
[2019-03-26 23:58:01,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7770
[2019-03-26 23:58:01,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 81.5, 1.0, 2.0, 0.2903614062941328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466868.1970245438, 466868.1970245438, 164742.51961318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [21.66666666666666, 81.33333333333334, 1.0, 2.0, 0.2887871776284492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 465252.1205791599, 465252.1205791593, 164632.2609034044], 
processed observation next is [1.0, 0.7391304347826086, 0.22590837282780388, 0.8133333333333335, 1.0, 1.0, 0.14311708148005925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12923670016087774, 0.12923670016087757, 0.24571979239314093], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.02170086], dtype=float32), 0.7083707]. 
=============================================
[2019-03-26 23:58:02,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1987960e-07 9.9859172e-01 1.4036549e-03 2.4458098e-09 4.1148223e-06], sum to 1.0000
[2019-03-26 23:58:02,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6988
[2019-03-26 23:58:02,747] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 85.66666666666667, 1.0, 2.0, 0.2425464766337609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399465.4648517597, 399465.4648517604, 160169.9418480094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 433200.0000, 
sim time next is 433800.0000, 
raw observation next is [19.65, 85.5, 1.0, 2.0, 0.2416396750564468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398145.4098398074, 398145.4098398067, 160078.15637442], 
processed observation next is [1.0, 0.0, 0.13033175355450236, 0.855, 1.0, 1.0, 0.0863128615137913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11059594717772428, 0.11059594717772409, 0.23892262145435822], 
reward next is 0.7611, 
noisyNet noise sample is [array([-1.0882585], dtype=float32), -1.79554]. 
=============================================
[2019-03-26 23:58:07,657] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7635813e-09 9.9999690e-01 2.6758366e-06 4.4569312e-10 4.2537934e-07], sum to 1.0000
[2019-03-26 23:58:07,666] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8849
[2019-03-26 23:58:07,774] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [18.8, 87.0, 1.0, 2.0, 0.2299358285746499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381481.9020643638, 381481.9020643638, 158786.4280343929], 
processed observation next is [1.0, 0.0, 0.09004739336492901, 0.87, 1.0, 1.0, 0.07221184165620467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10596719501787884, 0.10596719501787884, 0.2369946687080491], 
reward next is 0.7630, 
noisyNet noise sample is [array([2.012542], dtype=float32), -1.2986952]. 
=============================================
[2019-03-26 23:58:12,350] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15868: loss 0.3728
[2019-03-26 23:58:12,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15872: learning rate 0.0001
[2019-03-26 23:58:12,476] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15928: loss 0.2683
[2019-03-26 23:58:12,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15928: learning rate 0.0001
[2019-03-26 23:58:12,499] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15935: loss 0.2869
[2019-03-26 23:58:12,505] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15936: learning rate 0.0001
[2019-03-26 23:58:12,521] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 15944: loss 0.2547
[2019-03-26 23:58:12,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 15945: learning rate 0.0001
[2019-03-26 23:58:12,548] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15955: loss 0.1930
[2019-03-26 23:58:12,548] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15955: loss 0.1964
[2019-03-26 23:58:12,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15955: learning rate 0.0001
[2019-03-26 23:58:12,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15955: learning rate 0.0001
[2019-03-26 23:58:12,589] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15978: loss 0.2263
[2019-03-26 23:58:12,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15978: learning rate 0.0001
[2019-03-26 23:58:12,607] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15986: loss 0.2010
[2019-03-26 23:58:12,609] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15987: learning rate 0.0001
[2019-03-26 23:58:12,625] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15995: loss 0.2488
[2019-03-26 23:58:12,628] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15995: loss 0.1327
[2019-03-26 23:58:12,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15995: learning rate 0.0001
[2019-03-26 23:58:12,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15995: loss 0.2347
[2019-03-26 23:58:12,633] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15995: learning rate 0.0001
[2019-03-26 23:58:12,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15996: learning rate 0.0001
[2019-03-26 23:58:12,675] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16014: loss 0.1244
[2019-03-26 23:58:12,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16014: learning rate 0.0001
[2019-03-26 23:58:12,686] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16016: loss 0.1723
[2019-03-26 23:58:12,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16016: learning rate 0.0001
[2019-03-26 23:58:12,789] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16063: loss 0.0717
[2019-03-26 23:58:12,792] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16064: learning rate 0.0001
[2019-03-26 23:58:12,824] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16079: loss 0.0922
[2019-03-26 23:58:12,831] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16080: learning rate 0.0001
[2019-03-26 23:58:12,849] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16090: loss 0.0804
[2019-03-26 23:58:12,851] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16090: learning rate 0.0001
[2019-03-26 23:58:13,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1604104e-06 9.9967182e-01 2.0018838e-04 8.6586178e-09 1.2674535e-04], sum to 1.0000
[2019-03-26 23:58:13,962] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9943
[2019-03-26 23:58:14,069] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.98333333333333, 92.0, 1.0, 2.0, 0.2056873960517206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 343855.8416639633, 343855.841663964, 155796.2085989093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 619800.0000, 
sim time next is 620400.0000, 
raw observation next is [16.96666666666667, 92.0, 1.0, 2.0, 0.2007718088497139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 335655.8152497928, 335655.8152497928, 155379.4642532544], 
processed observation next is [1.0, 0.17391304347826086, 0.003159557661927487, 0.92, 1.0, 1.0, 0.0370744684936312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09323772645827579, 0.09323772645827579, 0.2319096481391857], 
reward next is 0.7681, 
noisyNet noise sample is [array([-0.00803604], dtype=float32), -0.46017402]. 
=============================================
[2019-03-26 23:58:16,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.6234977e-09 9.9999726e-01 2.6309528e-06 6.6979783e-10 1.4164605e-07], sum to 1.0000
[2019-03-26 23:58:16,171] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2983
[2019-03-26 23:58:16,281] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.33333333333334, 1.0, 2.0, 0.5585032590207535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918411.6264856434, 918411.626485644, 206990.280047354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [24.7, 53.5, 1.0, 2.0, 0.5948755988889678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977866.2161074358, 977866.2161074365, 214516.1061950178], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.535, 1.0, 1.0, 0.511898311914419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2716295044742877, 0.2716295044742879, 0.3201732928283848], 
reward next is 0.6798, 
noisyNet noise sample is [array([-0.05912642], dtype=float32), 1.1244972]. 
=============================================
[2019-03-26 23:58:19,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0010602e-07 9.9996877e-01 2.3554370e-05 1.6473932e-08 6.9418593e-06], sum to 1.0000
[2019-03-26 23:58:19,793] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7287
[2019-03-26 23:58:19,904] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.41666666666666, 78.16666666666667, 1.0, 2.0, 0.2474378110729192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408681.5302615968, 408681.5302615968, 160598.0716330438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 717000.0000, 
sim time next is 717600.0000, 
raw observation next is [20.63333333333333, 77.33333333333334, 1.0, 2.0, 0.2388806219487062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 394101.6041391643, 394101.6041391649, 159797.1353427625], 
processed observation next is [1.0, 0.30434782608695654, 0.17693522906793036, 0.7733333333333334, 1.0, 1.0, 0.08298870114301952, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10947266781643454, 0.1094726678164347, 0.23850318707875], 
reward next is 0.7615, 
noisyNet noise sample is [array([1.6701843], dtype=float32), -0.3782195]. 
=============================================
[2019-03-26 23:58:26,963] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3859724e-10 9.9999964e-01 3.0391180e-07 4.6417575e-11 1.6066803e-08], sum to 1.0000
[2019-03-26 23:58:26,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0901
[2019-03-26 23:58:27,089] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 72.5, 1.0, 2.0, 0.3101979174747647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490296.4679412794, 490296.46794128, 166300.871713252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 838200.0000, 
sim time next is 838800.0000, 
raw observation next is [23.9, 73.0, 1.0, 2.0, 0.3114317052683492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491778.4320690652, 491778.4320690645, 166399.7076514102], 
processed observation next is [0.0, 0.7391304347826086, 0.33175355450236965, 0.73, 1.0, 1.0, 0.1703996449016255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1366051200191848, 0.1366051200191846, 0.24835777261404507], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.3176125], dtype=float32), 0.9538439]. 
=============================================
[2019-03-26 23:58:28,704] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.1578645e-10 9.9999940e-01 5.7201953e-07 8.4559520e-10 2.0936586e-09], sum to 1.0000
[2019-03-26 23:58:28,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2813
[2019-03-26 23:58:28,721] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 89.0, 1.0, 2.0, 0.3091584772471531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489622.0235564462, 489622.0235564456, 166271.5381219746], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [21.5, 89.0, 1.0, 2.0, 0.3087097295269451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489279.8158706811, 489279.8158706804, 166253.6816097139], 
processed observation next is [0.0, 1.0, 0.21800947867298584, 0.89, 1.0, 1.0, 0.16712015605656033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13591105996407807, 0.13591105996407787, 0.24813982329808046], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.02423041], dtype=float32), 0.32795462]. 
=============================================
[2019-03-26 23:58:30,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7367118e-09 9.9999893e-01 1.0354513e-06 1.6337114e-09 4.4576677e-08], sum to 1.0000
[2019-03-26 23:58:30,398] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23871: loss 0.0011
[2019-03-26 23:58:30,402] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23873: learning rate 0.0001
[2019-03-26 23:58:30,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8188
[2019-03-26 23:58:30,417] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.292241266409421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466670.2044195034, 466670.204419504, 164700.6656892739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 892800.0000, 
sim time next is 893400.0000, 
raw observation next is [22.5, 79.00000000000001, 1.0, 2.0, 0.2925385832152782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466982.5633899804, 466982.5633899804, 164720.0440739929], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.7900000000000001, 1.0, 1.0, 0.14763684724732312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.129717378719439, 0.129717378719439, 0.24585081205073567], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.34045845], dtype=float32), -1.7341375]. 
=============================================
[2019-03-26 23:58:30,459] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23894: loss 0.0063
[2019-03-26 23:58:30,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23894: learning rate 0.0001
[2019-03-26 23:58:30,544] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 23921: loss 0.0161
[2019-03-26 23:58:30,547] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 23921: learning rate 0.0001
[2019-03-26 23:58:30,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23938: loss 0.0380
[2019-03-26 23:58:30,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23938: learning rate 0.0001
[2019-03-26 23:58:30,632] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23961: loss 0.0169
[2019-03-26 23:58:30,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23961: learning rate 0.0001
[2019-03-26 23:58:30,636] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23961: loss 0.0173
[2019-03-26 23:58:30,640] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23963: loss 0.0091
[2019-03-26 23:58:30,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23964: learning rate 0.0001
[2019-03-26 23:58:30,643] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23967: learning rate 0.0001
[2019-03-26 23:58:30,670] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23975: loss 0.0078
[2019-03-26 23:58:30,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23976: learning rate 0.0001
[2019-03-26 23:58:30,695] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23983: loss 0.0003
[2019-03-26 23:58:30,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23984: learning rate 0.0001
[2019-03-26 23:58:30,706] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23988: loss 0.0022
[2019-03-26 23:58:30,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23988: learning rate 0.0001
[2019-03-26 23:58:30,726] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23995: loss 0.0012
[2019-03-26 23:58:30,730] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23998: learning rate 0.0001
[2019-03-26 23:58:30,845] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24054: loss 0.0085
[2019-03-26 23:58:30,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24054: learning rate 0.0001
[2019-03-26 23:58:30,852] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24056: loss 0.0028
[2019-03-26 23:58:30,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24056: learning rate 0.0001
[2019-03-26 23:58:30,854] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24056: loss 0.0008
[2019-03-26 23:58:30,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24057: learning rate 0.0001
[2019-03-26 23:58:30,896] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24072: loss 0.0254
[2019-03-26 23:58:30,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24072: learning rate 0.0001
[2019-03-26 23:58:30,978] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24110: loss 0.0279
[2019-03-26 23:58:30,983] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24111: learning rate 0.0001
[2019-03-26 23:58:32,958] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 23:58:32,959] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:58:32,961] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:58:32,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:58:32,962] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:58:32,965] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:58:32,963] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:58:32,967] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:58:32,962] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:58:32,969] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:58:32,970] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:58:32,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-26 23:58:32,999] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-26 23:58:33,020] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-26 23:58:33,021] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-26 23:58:33,065] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-26 23:58:57,712] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.032861143]
[2019-03-26 23:58:57,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.31666666666667, 99.0, 1.0, 2.0, 0.4465584681719386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.0587700157, 643133.0587700164, 178103.1880650497]
[2019-03-26 23:58:57,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:58:57,717] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.9133511e-11 9.9999988e-01 1.2330419e-07 2.6140614e-11 4.6494724e-09], sampled 0.07912188908830642
[2019-03-26 23:59:48,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.032861143]
[2019-03-26 23:59:48,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.77070423166667, 67.87563722333334, 1.0, 2.0, 0.891329135351416, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987233109387, 6.9112, 168.9123157234353, 2142899.763951773, 2075654.693560574, 431702.9525767452]
[2019-03-26 23:59:48,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:59:48,846] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1440352e-11 1.0000000e+00 5.0532762e-08 5.3452182e-12 1.1979295e-09], sampled 0.030949903080813468
[2019-03-26 23:59:48,847] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2142899.763951773 W.
[2019-03-27 00:00:26,576] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9389 2779156670.5327 933.0000
[2019-03-27 00:00:26,875] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:00:26,978] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-27 00:00:27,022] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5431 2927343822.9089 1338.0000
[2019-03-27 00:00:27,057] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.5516 2842568787.9044 1131.0000
[2019-03-27 00:00:28,071] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 25000, evaluation results [25000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8253.543067978462, 2927343822.9089026, 1338.0, 8659.93888523734, 2779156670.5326896, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8494.551607788804, 2842568787.9044414, 1131.0]
[2019-03-27 00:00:29,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2834804e-11 1.0000000e+00 3.5276884e-08 3.4279236e-11 3.2478386e-08], sum to 1.0000
[2019-03-27 00:00:29,580] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8449
[2019-03-27 00:00:29,585] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 93.16666666666666, 1.0, 2.0, 0.3316361779264139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514014.6031773683, 514014.6031773683, 167830.4349445109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3315436675871201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513924.8772747177, 513924.8772747171, 167825.1107573162], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19463092480375918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14275691035408825, 0.14275691035408808, 0.25048523993629285], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.41943198], dtype=float32), -2.2909555]. 
=============================================
[2019-03-27 00:00:41,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1536733e-09 9.9999976e-01 2.5046094e-07 1.3336086e-09 2.1244579e-08], sum to 1.0000
[2019-03-27 00:00:41,875] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1882
[2019-03-27 00:00:41,880] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.88333333333333, 63.5, 1.0, 2.0, 0.5262258908200603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801873.4452989831, 801873.4452989837, 195883.366619986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [27.0, 63.0, 1.0, 2.0, 0.5250589993288421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799527.1029328268, 799527.1029328268, 195610.9273368673], 
processed observation next is [1.0, 0.5652173913043478, 0.4786729857819906, 0.63, 1.0, 1.0, 0.42778192690221944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22209086192578523, 0.22209086192578523, 0.29195660796547357], 
reward next is 0.7080, 
noisyNet noise sample is [array([-1.7652714], dtype=float32), -0.12652466]. 
=============================================
[2019-03-27 00:00:41,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.4183 ]
 [68.41625]
 [68.41032]
 [68.36573]
 [68.31849]], R is [[68.49713135]
 [68.51979828]
 [68.53791046]
 [68.52805328]
 [68.51067352]].
[2019-03-27 00:00:42,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6481185e-09 9.9999607e-01 3.2690843e-06 2.2126239e-09 7.2640535e-07], sum to 1.0000
[2019-03-27 00:00:42,998] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7262
[2019-03-27 00:00:43,121] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 67.0, 1.0, 2.0, 0.3443976029830777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532773.1660277712, 532773.1660277712, 169292.8305185069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1191600.0000, 
sim time next is 1192200.0000, 
raw observation next is [25.55, 68.0, 1.0, 2.0, 0.3459098047016798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535056.3827896409, 535056.3827896416, 169476.4749503628], 
processed observation next is [1.0, 0.8260869565217391, 0.40995260663507116, 0.68, 1.0, 1.0, 0.21193952373696362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14862677299712249, 0.14862677299712268, 0.2529499626124818], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.30224591], dtype=float32), -0.9986373]. 
=============================================
[2019-03-27 00:00:43,499] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31875: loss 0.0194
[2019-03-27 00:00:43,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31875: learning rate 0.0001
[2019-03-27 00:00:43,539] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31891: loss 0.0217
[2019-03-27 00:00:43,541] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31892: learning rate 0.0001
[2019-03-27 00:00:43,558] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31899: loss 0.0279
[2019-03-27 00:00:43,559] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31899: learning rate 0.0001
[2019-03-27 00:00:43,602] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31918: loss 0.0177
[2019-03-27 00:00:43,607] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31919: learning rate 0.0001
[2019-03-27 00:00:43,664] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31947: loss 0.0014
[2019-03-27 00:00:43,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31949: learning rate 0.0001
[2019-03-27 00:00:43,698] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31962: loss 0.0083
[2019-03-27 00:00:43,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31962: learning rate 0.0001
[2019-03-27 00:00:43,720] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31973: loss 0.0014
[2019-03-27 00:00:43,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31973: learning rate 0.0001
[2019-03-27 00:00:43,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31979: loss 0.0232
[2019-03-27 00:00:43,743] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31979: learning rate 0.0001
[2019-03-27 00:00:43,768] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31991: loss 0.0370
[2019-03-27 00:00:43,770] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31991: learning rate 0.0001
[2019-03-27 00:00:43,819] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32012: loss 0.0517
[2019-03-27 00:00:43,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32012: learning rate 0.0001
[2019-03-27 00:00:43,834] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32020: loss 0.0555
[2019-03-27 00:00:43,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32021: learning rate 0.0001
[2019-03-27 00:00:43,841] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32023: loss 0.0550
[2019-03-27 00:00:43,845] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32023: learning rate 0.0001
[2019-03-27 00:00:43,910] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32053: loss 0.0860
[2019-03-27 00:00:43,913] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32053: loss 0.0636
[2019-03-27 00:00:43,917] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32053: learning rate 0.0001
[2019-03-27 00:00:43,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32053: learning rate 0.0001
[2019-03-27 00:00:43,980] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32082: loss 0.0297
[2019-03-27 00:00:43,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32083: learning rate 0.0001
[2019-03-27 00:00:44,064] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 32122: loss 0.0035
[2019-03-27 00:00:44,067] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 32122: learning rate 0.0001
[2019-03-27 00:00:46,519] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.5454270e-12 1.0000000e+00 9.1434931e-09 3.5927911e-10 2.5184612e-09], sum to 1.0000
[2019-03-27 00:00:46,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6983
[2019-03-27 00:00:46,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2440816.208599915 W.
[2019-03-27 00:00:46,542] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.45, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.281933456766287, 6.9112, 168.9054382303731, 2440816.208599915, 1468414.000708911, 313184.9742941042], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1247400.0000, 
sim time next is 1248000.0000, 
raw observation next is [27.66666666666667, 72.33333333333333, 1.0, 2.0, 0.4664419444138964, 1.0, 1.0, 0.4664419444138964, 1.0, 1.0, 0.7833036873631721, 6.9112, 6.9112, 170.5573041426782, 1956481.009203154, 1956481.009203154, 388128.3938793784], 
processed observation next is [1.0, 0.43478260869565216, 0.5102685624012641, 0.7233333333333333, 1.0, 1.0, 0.35715896917336915, 1.0, 0.5, 0.35715896917336915, 1.0, 0.5, 0.7357362041014293, 0.0, 0.0, 0.8375144448122397, 0.543466947000876, 0.543466947000876, 0.579296110267729], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2695326], dtype=float32), -0.093387924]. 
=============================================
[2019-03-27 00:00:46,555] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.34742 ]
 [68.169235]
 [68.34268 ]
 [68.20318 ]
 [68.170334]], R is [[66.93191528]
 [66.26259613]
 [66.07463837]
 [65.41389465]
 [65.32529449]].
[2019-03-27 00:00:51,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0464887e-16 1.0000000e+00 2.8347927e-16 4.9723646e-18 4.1879699e-16], sum to 1.0000
[2019-03-27 00:00:51,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9860
[2019-03-27 00:00:51,166] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 95.0, 1.0, 2.0, 0.7341156852523142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1088893.433748887, 1088893.433748887, 236718.4646992754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1327200.0000, 
sim time next is 1327800.0000, 
raw observation next is [23.01666666666667, 95.0, 1.0, 2.0, 0.7439501344991756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1104125.174937807, 1104125.174937807, 239185.5781017794], 
processed observation next is [1.0, 0.34782608695652173, 0.2898894154818327, 0.95, 1.0, 1.0, 0.691506186143585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30670143748272416, 0.30670143748272416, 0.35699340015190956], 
reward next is 0.6430, 
noisyNet noise sample is [array([1.1613139], dtype=float32), -0.72823393]. 
=============================================
[2019-03-27 00:00:54,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2654742e-15 1.0000000e+00 2.6762702e-16 3.1373372e-17 3.0709468e-16], sum to 1.0000
[2019-03-27 00:00:54,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4421
[2019-03-27 00:00:54,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 95.16666666666667, 1.0, 2.0, 0.3180440035163916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502811.7905038574, 502811.7905038581, 167233.9838712227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1375800.0000, 
sim time next is 1376400.0000, 
raw observation next is [20.83333333333333, 95.33333333333334, 1.0, 2.0, 0.3171240389280713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501519.2736703553, 501519.2736703559, 167139.9895251932], 
processed observation next is [1.0, 0.9565217391304348, 0.1864139020537123, 0.9533333333333335, 1.0, 1.0, 0.1772578782265919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1393109093528765, 0.13931090935287663, 0.2494626709331242], 
reward next is 0.7505, 
noisyNet noise sample is [array([-0.77747434], dtype=float32), 0.12711082]. 
=============================================
[2019-03-27 00:01:01,341] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39869: loss 0.0385
[2019-03-27 00:01:01,344] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39869: learning rate 0.0001
[2019-03-27 00:01:01,350] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39873: loss 0.0529
[2019-03-27 00:01:01,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39874: learning rate 0.0001
[2019-03-27 00:01:01,388] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39891: loss 0.0505
[2019-03-27 00:01:01,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39891: learning rate 0.0001
[2019-03-27 00:01:01,393] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39891: loss 0.0380
[2019-03-27 00:01:01,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39892: learning rate 0.0001
[2019-03-27 00:01:01,404] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39894: loss 0.0278
[2019-03-27 00:01:01,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39894: learning rate 0.0001
[2019-03-27 00:01:01,522] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39948: loss 0.0285
[2019-03-27 00:01:01,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39949: learning rate 0.0001
[2019-03-27 00:01:01,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39966: loss 0.0062
[2019-03-27 00:01:01,565] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39966: loss 0.0043
[2019-03-27 00:01:01,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39966: learning rate 0.0001
[2019-03-27 00:01:01,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39966: learning rate 0.0001
[2019-03-27 00:01:01,620] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39987: loss 0.0007
[2019-03-27 00:01:01,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39988: learning rate 0.0001
[2019-03-27 00:01:01,647] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40000: loss 0.0006
[2019-03-27 00:01:01,650] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40000: learning rate 0.0001
[2019-03-27 00:01:01,714] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40031: loss 0.0064
[2019-03-27 00:01:01,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40031: learning rate 0.0001
[2019-03-27 00:01:01,804] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40067: loss 0.0198
[2019-03-27 00:01:01,805] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40068: loss 0.0103
[2019-03-27 00:01:01,809] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40069: learning rate 0.0001
[2019-03-27 00:01:01,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40069: learning rate 0.0001
[2019-03-27 00:01:01,848] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40086: loss 0.0189
[2019-03-27 00:01:01,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40087: learning rate 0.0001
[2019-03-27 00:01:01,937] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40124: loss 0.0149
[2019-03-27 00:01:01,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40126: learning rate 0.0001
[2019-03-27 00:01:02,022] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40163: loss 0.0009
[2019-03-27 00:01:02,023] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40163: learning rate 0.0001
[2019-03-27 00:01:07,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2427748e-16 1.0000000e+00 2.1669635e-15 1.2974667e-16 4.5556333e-16], sum to 1.0000
[2019-03-27 00:01:07,811] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9368
[2019-03-27 00:01:07,925] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.08333333333334, 85.0, 1.0, 2.0, 0.817020652302006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223013.416345706, 1223013.416345706, 259234.0670737856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1605000.0000, 
sim time next is 1605600.0000, 
raw observation next is [24.1, 85.0, 1.0, 2.0, 0.7733287877246773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1156952.56078432, 1156952.560784321, 247672.1545933295], 
processed observation next is [1.0, 0.6086956521739131, 0.3412322274881518, 0.85, 1.0, 1.0, 0.7269021538851533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3213757113289778, 0.3213757113289781, 0.36965993222885], 
reward next is 0.6303, 
noisyNet noise sample is [array([-0.93483883], dtype=float32), -1.2243034]. 
=============================================
[2019-03-27 00:01:11,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2473917e-16 1.0000000e+00 2.5836890e-16 7.0114932e-17 1.4391872e-15], sum to 1.0000
[2019-03-27 00:01:11,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4530
[2019-03-27 00:01:11,581] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 98.0, 1.0, 2.0, 0.4648490107739431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664753.0375699161, 664753.0375699154, 180221.710398359], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1666800.0000, 
sim time next is 1667400.0000, 
raw observation next is [23.61666666666667, 98.0, 1.0, 2.0, 0.4484092435607479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 640969.2873152549, 640969.2873152556, 177762.1315016774], 
processed observation next is [1.0, 0.30434782608695654, 0.31832543443917877, 0.98, 1.0, 1.0, 0.3354328235671662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17804702425423746, 0.17804702425423766, 0.26531661418160807], 
reward next is 0.7347, 
noisyNet noise sample is [array([2.1522338], dtype=float32), -0.14706369]. 
=============================================
[2019-03-27 00:01:11,742] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3160707e-15 1.0000000e+00 1.3252246e-15 2.5816889e-16 2.8223302e-16], sum to 1.0000
[2019-03-27 00:01:11,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8892
[2019-03-27 00:01:11,747] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.48333333333333, 99.0, 1.0, 2.0, 0.4689554287411384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 670649.5228793566, 670649.5228793573, 180843.2579851706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1662600.0000, 
sim time next is 1663200.0000, 
raw observation next is [23.5, 99.0, 1.0, 2.0, 0.4684580988681915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669463.7835423907, 669463.7835423913, 180708.0322407645], 
processed observation next is [1.0, 0.2608695652173913, 0.31279620853080575, 0.99, 1.0, 1.0, 0.35958807092553197, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18596216209510852, 0.18596216209510869, 0.26971348095636494], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.3350377], dtype=float32), 0.3182974]. 
=============================================
[2019-03-27 00:01:19,086] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47813: loss 0.0008
[2019-03-27 00:01:19,087] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47813: learning rate 0.0001
[2019-03-27 00:01:19,137] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47833: loss 0.0016
[2019-03-27 00:01:19,139] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47834: learning rate 0.0001
[2019-03-27 00:01:19,211] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47868: loss 0.0199
[2019-03-27 00:01:19,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47868: learning rate 0.0001
[2019-03-27 00:01:19,216] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47869: loss 0.0220
[2019-03-27 00:01:19,219] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47869: learning rate 0.0001
[2019-03-27 00:01:19,288] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47904: loss 0.0422
[2019-03-27 00:01:19,293] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47904: learning rate 0.0001
[2019-03-27 00:01:19,385] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47945: loss 0.0383
[2019-03-27 00:01:19,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47945: learning rate 0.0001
[2019-03-27 00:01:19,455] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47975: loss 0.0421
[2019-03-27 00:01:19,455] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47975: loss 0.0216
[2019-03-27 00:01:19,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47975: learning rate 0.0001
[2019-03-27 00:01:19,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47976: learning rate 0.0001
[2019-03-27 00:01:19,482] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47987: loss 0.0166
[2019-03-27 00:01:19,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47987: learning rate 0.0001
[2019-03-27 00:01:19,534] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48005: loss 0.0048
[2019-03-27 00:01:19,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48008: learning rate 0.0001
[2019-03-27 00:01:19,547] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48010: loss 0.0030
[2019-03-27 00:01:19,553] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48013: learning rate 0.0001
[2019-03-27 00:01:19,622] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48048: loss 0.0007
[2019-03-27 00:01:19,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48048: learning rate 0.0001
[2019-03-27 00:01:19,688] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48075: loss 0.0041
[2019-03-27 00:01:19,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48075: learning rate 0.0001
[2019-03-27 00:01:19,778] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48117: loss 0.0006
[2019-03-27 00:01:19,779] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48117: learning rate 0.0001
[2019-03-27 00:01:19,885] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48163: loss 0.0106
[2019-03-27 00:01:19,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48163: learning rate 0.0001
[2019-03-27 00:01:19,915] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48177: loss 0.0138
[2019-03-27 00:01:19,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48177: learning rate 0.0001
[2019-03-27 00:01:20,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3042756e-15 1.0000000e+00 6.9523364e-18 2.8679646e-18 2.2710252e-17], sum to 1.0000
[2019-03-27 00:01:20,829] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3943
[2019-03-27 00:01:20,838] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 96.0, 1.0, 2.0, 0.3493305945499476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535988.9577849462, 535988.9577849468, 169420.9558366583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1828800.0000, 
sim time next is 1829400.0000, 
raw observation next is [21.88333333333333, 96.33333333333333, 1.0, 2.0, 0.3623963527946852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555663.2742153206, 555663.2742153206, 171050.950247655], 
processed observation next is [1.0, 0.17391304347826086, 0.2361769352290678, 0.9633333333333333, 1.0, 1.0, 0.2318028346923918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15435090950425573, 0.15435090950425573, 0.25529992574276866], 
reward next is 0.7447, 
noisyNet noise sample is [array([1.8472528], dtype=float32), -1.4614575]. 
=============================================
[2019-03-27 00:01:23,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2897710e-12 1.0000000e+00 3.3660195e-13 2.6418917e-14 2.1388529e-14], sum to 1.0000
[2019-03-27 00:01:23,636] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1005
[2019-03-27 00:01:23,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1796895.653158506 W.
[2019-03-27 00:01:23,762] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 86.0, 1.0, 2.0, 0.6440913247976465, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98140834030486, 6.9112, 168.9125380593711, 1796895.653158506, 1747087.563552421, 373880.7797654913], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1872000.0000, 
sim time next is 1872600.0000, 
raw observation next is [26.98333333333333, 86.33333333333334, 1.0, 2.0, 0.5748959455053531, 1.0, 1.0, 0.5748959455053531, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1607331.03208493, 1607331.03208493, 325161.4924307864], 
processed observation next is [1.0, 0.6956521739130435, 0.4778830963665086, 0.8633333333333334, 1.0, 1.0, 0.48782644036789524, 1.0, 0.5, 0.48782644036789524, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4464808422458139, 0.4464808422458139, 0.48531566034445733], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11497378], dtype=float32), -0.99893546]. 
=============================================
[2019-03-27 00:01:24,070] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:01:24,072] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:01:24,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:01:24,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:01:24,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:01:24,075] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:01:24,077] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:01:24,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:01:24,076] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:01:24,080] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:01:24,081] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:01:24,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-27 00:01:24,098] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-27 00:01:24,138] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-27 00:01:24,139] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-27 00:01:24,176] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-27 00:01:32,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.024681916]
[2019-03-27 00:01:32,765] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.69768512, 92.65570265000001, 1.0, 2.0, 0.2248193296466605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 374110.8591274213, 374110.8591274213, 158127.4762735885]
[2019-03-27 00:01:32,768] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:01:32,771] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0800336e-12 1.0000000e+00 1.8044138e-13 1.0209464e-13 4.3449229e-14], sampled 0.2677459872185869
[2019-03-27 00:03:17,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:03:17,566] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:03:17,612] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:03:17,703] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3814 2842575533.4654 1131.0000
[2019-03-27 00:03:17,794] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:03:18,812] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 50000, evaluation results [50000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8495.381403764231, 2842575533.4653883, 1131.0]
[2019-03-27 00:03:29,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.06306460e-17 1.00000000e+00 1.33863664e-17 1.62492167e-20
 1.27211215e-19], sum to 1.0000
[2019-03-27 00:03:29,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3391
[2019-03-27 00:03:29,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 88.5, 1.0, 2.0, 0.4819775380918156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673480.742453085, 673480.7424530843, 180820.3905261952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2057400.0000, 
sim time next is 2058000.0000, 
raw observation next is [25.53333333333333, 88.66666666666666, 1.0, 2.0, 0.480554741605886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671492.0006293404, 671492.0006293404, 180605.3452978999], 
processed observation next is [0.0, 0.8260869565217391, 0.4091627172195892, 0.8866666666666666, 1.0, 1.0, 0.37416233928420006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18652555573037233, 0.18652555573037233, 0.26956021686253717], 
reward next is 0.7304, 
noisyNet noise sample is [array([-0.4808293], dtype=float32), -0.50211585]. 
=============================================
[2019-03-27 00:03:29,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.7063  ]
 [75.65781 ]
 [75.6124  ]
 [75.56179 ]
 [75.501236]], R is [[75.71787262]
 [75.69081116]
 [75.66371155]
 [75.63656616]
 [75.60941315]].
[2019-03-27 00:03:31,192] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4594821e-16 1.0000000e+00 3.8984069e-17 2.6489539e-19 2.3712394e-19], sum to 1.0000
[2019-03-27 00:03:31,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4434
[2019-03-27 00:03:31,206] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 96.33333333333333, 1.0, 2.0, 0.4578627826517767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647524.3351276477, 647524.3351276484, 178258.0009826797], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [24.05, 96.5, 1.0, 2.0, 0.4581625852114105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 647824.8115850058, 647824.8115850058, 178285.9516952029], 
processed observation next is [0.0, 0.13043478260869565, 0.3388625592417062, 0.965, 1.0, 1.0, 0.34718383760410904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1799513365513905, 0.1799513365513905, 0.2660984353659745], 
reward next is 0.7339, 
noisyNet noise sample is [array([-0.12232452], dtype=float32), -0.09756747]. 
=============================================
[2019-03-27 00:03:31,806] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55825: loss 0.0109
[2019-03-27 00:03:31,809] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55826: loss 0.0148
[2019-03-27 00:03:31,811] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55827: learning rate 0.0001
[2019-03-27 00:03:31,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55827: learning rate 0.0001
[2019-03-27 00:03:31,866] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55846: loss 0.0001
[2019-03-27 00:03:31,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55847: learning rate 0.0001
[2019-03-27 00:03:31,953] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55884: loss 0.0029
[2019-03-27 00:03:31,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55886: learning rate 0.0001
[2019-03-27 00:03:32,013] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55913: loss 0.0065
[2019-03-27 00:03:32,014] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55914: loss 0.0082
[2019-03-27 00:03:32,016] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55916: learning rate 0.0001
[2019-03-27 00:03:32,017] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55916: learning rate 0.0001
[2019-03-27 00:03:32,060] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55934: loss 0.0116
[2019-03-27 00:03:32,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55934: learning rate 0.0001
[2019-03-27 00:03:32,065] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55935: loss 0.0052
[2019-03-27 00:03:32,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55936: learning rate 0.0001
[2019-03-27 00:03:32,147] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55969: loss 0.0031
[2019-03-27 00:03:32,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55970: learning rate 0.0001
[2019-03-27 00:03:32,194] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55990: loss 0.0012
[2019-03-27 00:03:32,197] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55990: learning rate 0.0001
[2019-03-27 00:03:32,262] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56021: loss 0.0022
[2019-03-27 00:03:32,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56022: learning rate 0.0001
[2019-03-27 00:03:32,316] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56044: loss 0.0050
[2019-03-27 00:03:32,321] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56046: learning rate 0.0001
[2019-03-27 00:03:32,471] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56113: loss 0.0117
[2019-03-27 00:03:32,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56113: learning rate 0.0001
[2019-03-27 00:03:32,584] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56165: loss 0.0089
[2019-03-27 00:03:32,588] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56165: learning rate 0.0001
[2019-03-27 00:03:32,626] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56182: loss 0.0008
[2019-03-27 00:03:32,630] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56183: learning rate 0.0001
[2019-03-27 00:03:32,762] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56245: loss 0.0002
[2019-03-27 00:03:32,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56245: learning rate 0.0001
[2019-03-27 00:03:33,305] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6539951e-16 1.0000000e+00 6.6586091e-18 3.6590667e-20 9.0196332e-19], sum to 1.0000
[2019-03-27 00:03:33,313] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3260
[2019-03-27 00:03:33,318] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.83333333333334, 1.0, 2.0, 0.5684791045238861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794397.0062718627, 794397.006271862, 195021.0877695475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.5688649918539387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794936.4499355687, 794936.4499355693, 195089.4460440906], 
processed observation next is [0.0, 0.6086956521739131, 0.6208530805687204, 0.77, 1.0, 1.0, 0.48056023114932367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.220815680537658, 0.22081568053765815, 0.2911782776777472], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.78282243], dtype=float32), 0.9798434]. 
=============================================
[2019-03-27 00:03:33,334] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[74.68034]
 [74.65859]
 [74.63677]
 [74.60625]
 [74.58282]], R is [[74.69316101]
 [74.65515137]
 [74.61777496]
 [74.58132935]
 [74.54573059]].
[2019-03-27 00:03:36,603] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1767461e-17 1.0000000e+00 1.8740472e-17 8.7010347e-20 1.0403020e-18], sum to 1.0000
[2019-03-27 00:03:36,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0991
[2019-03-27 00:03:36,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 96.83333333333334, 1.0, 2.0, 0.5637536282232145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787791.14109876, 787791.14109876, 194182.0596976558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2177400.0000, 
sim time next is 2178000.0000, 
raw observation next is [24.5, 97.0, 1.0, 2.0, 0.558473430911198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780409.8657194518, 780409.8657194518, 193259.4443489104], 
processed observation next is [1.0, 0.21739130434782608, 0.3601895734597157, 0.97, 1.0, 1.0, 0.4680402782062627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2167805182554033, 0.2167805182554033, 0.28844693186404535], 
reward next is 0.7116, 
noisyNet noise sample is [array([-1.3335721], dtype=float32), -0.40298712]. 
=============================================
[2019-03-27 00:03:36,633] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.70588 ]
 [71.69936 ]
 [71.67595 ]
 [71.57803 ]
 [71.551254]], R is [[71.86933136]
 [71.86081696]
 [71.84971619]
 [71.83465576]
 [71.8190155 ]].
[2019-03-27 00:03:38,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8023863e-11 1.0000000e+00 6.1951238e-12 1.4303073e-11 5.0846464e-13], sum to 1.0000
[2019-03-27 00:03:38,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5771
[2019-03-27 00:03:38,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1673039.563895731 W.
[2019-03-27 00:03:39,027] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.26666666666667, 68.0, 1.0, 2.0, 0.398919774119023, 1.0, 2.0, 0.398919774119023, 1.0, 1.0, 0.692791722859513, 6.9112, 6.9112, 170.5573041426782, 1673039.563895731, 1673039.563895731, 352291.3037201929], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2205600.0000, 
sim time next is 2206200.0000, 
raw observation next is [31.38333333333333, 67.5, 1.0, 2.0, 0.3978347567945067, 1.0, 2.0, 0.3978347567945067, 1.0, 2.0, 0.6909074065875404, 6.911199999999999, 6.9112, 170.5573041426782, 1668485.540335212, 1668485.540335213, 351693.1188183388], 
processed observation next is [1.0, 0.5217391304347826, 0.6864139020537123, 0.675, 1.0, 1.0, 0.27449970698133336, 1.0, 1.0, 0.27449970698133336, 1.0, 1.0, 0.6230578129116344, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.46346820564867, 0.4634682056486703, 0.5249151027139385], 
reward next is 0.4751, 
noisyNet noise sample is [array([-0.15016411], dtype=float32), -1.5190332]. 
=============================================
[2019-03-27 00:03:39,334] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.6978725e-10 1.0000000e+00 3.9696937e-11 3.1959806e-11 8.4438565e-13], sum to 1.0000
[2019-03-27 00:03:39,345] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2172
[2019-03-27 00:03:39,352] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.63333333333333, 68.66666666666667, 1.0, 2.0, 0.3959623504498354, 1.0, 2.0, 0.3959623504498354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1106798.720229462, 1106798.720229462, 271124.4155638096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [31.46666666666667, 69.33333333333334, 1.0, 2.0, 0.5346943965416157, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747169.3948859446, 747169.394885944, 189213.9201299547], 
processed observation next is [1.0, 0.7391304347826086, 0.6903633491311217, 0.6933333333333335, 1.0, 1.0, 0.4393908392067658, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2075470541349846, 0.20754705413498445, 0.2824088360148578], 
reward next is 0.7176, 
noisyNet noise sample is [array([-0.44815457], dtype=float32), -1.3190799]. 
=============================================
[2019-03-27 00:03:49,572] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63756: loss 21.4380
[2019-03-27 00:03:49,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63756: learning rate 0.0001
[2019-03-27 00:03:49,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63811: loss 18.7360
[2019-03-27 00:03:49,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63811: learning rate 0.0001
[2019-03-27 00:03:49,843] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63876: loss 15.0438
[2019-03-27 00:03:49,846] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63877: learning rate 0.0001
[2019-03-27 00:03:49,859] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63885: loss 12.2897
[2019-03-27 00:03:49,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63885: learning rate 0.0001
[2019-03-27 00:03:49,884] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63896: loss 15.8787
[2019-03-27 00:03:49,887] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63896: learning rate 0.0001
[2019-03-27 00:03:49,923] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 63912: loss 10.1721
[2019-03-27 00:03:49,927] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 63913: learning rate 0.0001
[2019-03-27 00:03:49,938] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63918: loss 25.7932
[2019-03-27 00:03:49,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63918: learning rate 0.0001
[2019-03-27 00:03:49,979] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63936: loss 14.4387
[2019-03-27 00:03:49,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63937: learning rate 0.0001
[2019-03-27 00:03:49,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63937: loss 13.9577
[2019-03-27 00:03:49,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63937: learning rate 0.0001
[2019-03-27 00:03:50,120] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64000: loss 15.9456
[2019-03-27 00:03:50,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64000: learning rate 0.0001
[2019-03-27 00:03:50,180] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64024: loss 12.8239
[2019-03-27 00:03:50,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64025: learning rate 0.0001
[2019-03-27 00:03:50,273] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64067: loss 11.1013
[2019-03-27 00:03:50,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64068: learning rate 0.0001
[2019-03-27 00:03:50,482] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64157: loss 18.1033
[2019-03-27 00:03:50,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64157: learning rate 0.0001
[2019-03-27 00:03:50,571] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64201: loss 7.8397
[2019-03-27 00:03:50,576] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64201: learning rate 0.0001
[2019-03-27 00:03:50,721] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64269: loss 16.0704
[2019-03-27 00:03:50,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64269: learning rate 0.0001
[2019-03-27 00:03:50,752] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64283: loss 12.7346
[2019-03-27 00:03:50,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64285: learning rate 0.0001
[2019-03-27 00:03:58,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0575075e-10 1.0000000e+00 3.1795713e-14 2.0815691e-11 1.8324952e-15], sum to 1.0000
[2019-03-27 00:03:58,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0615
[2019-03-27 00:03:58,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1941410.983379893 W.
[2019-03-27 00:03:58,493] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.98333333333333, 76.66666666666667, 1.0, 2.0, 0.4628523726133875, 1.0, 2.0, 0.4628523726133875, 1.0, 2.0, 0.7994227919768063, 6.9112, 6.9112, 170.5573041426782, 1941410.983379893, 1941410.983379893, 389680.5404217609], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2548200.0000, 
sim time next is 2548800.0000, 
raw observation next is [29.1, 76.0, 1.0, 2.0, 0.4795406733978763, 1.0, 2.0, 0.4795406733978763, 1.0, 2.0, 0.8285951391664133, 6.9112, 6.9112, 170.5573041426782, 2011474.933223007, 2011474.933223007, 400561.4041335129], 
processed observation next is [1.0, 0.5217391304347826, 0.5781990521327015, 0.76, 1.0, 1.0, 0.3729405703588871, 1.0, 1.0, 0.3729405703588871, 1.0, 1.0, 0.7909696819102601, 0.0, 0.0, 0.8375144448122397, 0.5587430370063908, 0.5587430370063908, 0.5978528419903177], 
reward next is 0.4021, 
noisyNet noise sample is [array([1.1882554], dtype=float32), 0.8169039]. 
=============================================
[2019-03-27 00:04:01,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2171770e-13 1.0000000e+00 1.6249366e-17 3.4055393e-14 2.9470388e-19], sum to 1.0000
[2019-03-27 00:04:01,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5971
[2019-03-27 00:04:01,345] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 91.5, 1.0, 2.0, 0.5004310294897539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 699274.8109756339, 699274.8109756333, 183663.2336015775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2590200.0000, 
sim time next is 2590800.0000, 
raw observation next is [25.4, 91.66666666666666, 1.0, 2.0, 0.4971431653930173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694679.0281956465, 694679.0281956465, 183149.2888863903], 
processed observation next is [1.0, 1.0, 0.4028436018957346, 0.9166666666666665, 1.0, 1.0, 0.3941483920397798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1929663967210129, 0.1929663967210129, 0.2733571475916273], 
reward next is 0.7266, 
noisyNet noise sample is [array([1.2629436], dtype=float32), 0.90431917]. 
=============================================
[2019-03-27 00:04:04,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1666181e-15 1.0000000e+00 5.5250867e-19 1.8279022e-14 1.3917420e-20], sum to 1.0000
[2019-03-27 00:04:04,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7021
[2019-03-27 00:04:04,054] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.5, 1.0, 2.0, 0.49055919603352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685476.0028912062, 685476.0028912062, 182130.4274093183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.495270425329538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692061.3174084985, 692061.3174084985, 182858.6997948849], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.8233333333333335, 1.0, 1.0, 0.39189207871028675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.192239254835694, 0.192239254835694, 0.27292343252967893], 
reward next is 0.7271, 
noisyNet noise sample is [array([0.68628687], dtype=float32), 0.84064883]. 
=============================================
[2019-03-27 00:04:05,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6198564e-15 1.0000000e+00 3.7979414e-19 1.5729935e-14 5.3346623e-21], sum to 1.0000
[2019-03-27 00:04:05,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5698
[2019-03-27 00:04:05,050] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 89.0, 1.0, 2.0, 0.4750088158621437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666012.16823512, 666012.16823512, 180066.1552623873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2656200.0000, 
sim time next is 2656800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4657907796082175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657914.0540058315, 657914.0540058315, 179319.7866438999], 
processed observation next is [0.0, 0.782608695652174, 0.38388625592417064, 0.89, 1.0, 1.0, 0.35637443326291274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18275390389050875, 0.18275390389050875, 0.26764147260283566], 
reward next is 0.7324, 
noisyNet noise sample is [array([0.02037997], dtype=float32), 1.0112334]. 
=============================================
[2019-03-27 00:04:07,247] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71709: loss 0.2853
[2019-03-27 00:04:07,250] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71710: learning rate 0.0001
[2019-03-27 00:04:07,399] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71779: loss 0.3389
[2019-03-27 00:04:07,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71780: learning rate 0.0001
[2019-03-27 00:04:07,562] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71853: loss 0.2356
[2019-03-27 00:04:07,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71855: learning rate 0.0001
[2019-03-27 00:04:07,578] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71860: loss 0.2339
[2019-03-27 00:04:07,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71860: learning rate 0.0001
[2019-03-27 00:04:07,643] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71888: loss 0.1583
[2019-03-27 00:04:07,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71889: learning rate 0.0001
[2019-03-27 00:04:07,657] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71893: loss 0.1306
[2019-03-27 00:04:07,661] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71893: learning rate 0.0001
[2019-03-27 00:04:07,688] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71907: loss 0.1122
[2019-03-27 00:04:07,692] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71909: learning rate 0.0001
[2019-03-27 00:04:07,696] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71910: loss 0.1192
[2019-03-27 00:04:07,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71910: learning rate 0.0001
[2019-03-27 00:04:07,743] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71930: loss 0.1115
[2019-03-27 00:04:07,751] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71931: learning rate 0.0001
[2019-03-27 00:04:07,984] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72042: loss 0.0339
[2019-03-27 00:04:07,986] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72043: learning rate 0.0001
[2019-03-27 00:04:08,006] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72050: loss 0.0241
[2019-03-27 00:04:08,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72050: learning rate 0.0001
[2019-03-27 00:04:08,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72058: loss 0.0144
[2019-03-27 00:04:08,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72060: learning rate 0.0001
[2019-03-27 00:04:08,319] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72190: loss 0.0001
[2019-03-27 00:04:08,324] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72191: learning rate 0.0001
[2019-03-27 00:04:08,350] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72206: loss 0.0002
[2019-03-27 00:04:08,353] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72207: learning rate 0.0001
[2019-03-27 00:04:08,402] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72225: loss 0.0012
[2019-03-27 00:04:08,403] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72225: learning rate 0.0001
[2019-03-27 00:04:08,546] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72292: loss 0.0175
[2019-03-27 00:04:08,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72294: learning rate 0.0001
[2019-03-27 00:04:09,290] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0570163e-15 1.0000000e+00 5.9905454e-18 9.6038595e-15 7.9768161e-20], sum to 1.0000
[2019-03-27 00:04:09,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3509
[2019-03-27 00:04:09,307] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 1.0, 1.0, 1.0, 0.28817071973506386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16784845999691392, 0.16784845999691392, 0.2608206684207806], 
reward next is 0.7392, 
noisyNet noise sample is [array([-1.1737794], dtype=float32), -0.18392451]. 
=============================================
[2019-03-27 00:04:14,540] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 00:04:14,542] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:04:14,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:04:14,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:04:14,544] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:04:14,544] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:04:14,548] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:04:14,548] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:04:14,550] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:04:14,545] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:04:14,552] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:04:14,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-27 00:04:14,570] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-27 00:04:14,571] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-27 00:04:14,633] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-27 00:04:14,649] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-27 00:04:23,224] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.031900115]
[2019-03-27 00:04:23,225] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.7, 76.0, 1.0, 2.0, 0.2252862276839105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 375084.5095942381, 375084.5095942381, 158123.2685116621]
[2019-03-27 00:04:23,226] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:04:23,229] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3285260e-12 1.0000000e+00 3.0439364e-15 7.0818447e-12 1.5566243e-16], sampled 0.08032955708702261
[2019-03-27 00:04:32,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.031900115]
[2019-03-27 00:04:32,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.36666666666667, 89.33333333333333, 1.0, 2.0, 0.2838041463645256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 459752.6407140289, 459752.6407140295, 164241.6910741603]
[2019-03-27 00:04:32,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:04:32,076] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4005781e-12 1.0000000e+00 1.3341519e-15 4.0195659e-12 6.1253393e-17], sampled 0.5316723502606098
[2019-03-27 00:04:38,889] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.031900115]
[2019-03-27 00:04:38,890] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.149261795, 91.397471585, 1.0, 2.0, 0.587777247393791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821374.8043060721, 821374.8043060721, 198483.4140063955]
[2019-03-27 00:04:38,891] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:04:38,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9674689e-12 1.0000000e+00 2.2338067e-15 5.6260361e-12 9.6002966e-17], sampled 0.5489067433870921
[2019-03-27 00:05:52,754] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.031900115]
[2019-03-27 00:05:52,755] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.1, 62.33333333333334, 1.0, 2.0, 0.5943162742865821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830516.1785573763, 830516.1785573763, 199691.4788653437]
[2019-03-27 00:05:52,756] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:05:52,759] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1913374e-12 1.0000000e+00 4.6017987e-15 9.2129247e-12 1.4429797e-16], sampled 0.5206177549860533
[2019-03-27 00:06:00,019] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.031900115]
[2019-03-27 00:06:00,020] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.4, 81.0, 1.0, 2.0, 0.864692656027394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1208563.60255087, 1208563.60255087, 260413.3044066343]
[2019-03-27 00:06:00,021] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:06:00,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9723567e-12 1.0000000e+00 9.4200084e-15 1.9867927e-11 2.6291659e-16], sampled 0.6240062013360248
[2019-03-27 00:06:07,896] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6673 3164135639.9928 1778.0000
[2019-03-27 00:06:08,423] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.8054 3007666199.2025 1766.0000
[2019-03-27 00:06:08,492] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-03-27 00:06:08,506] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-03-27 00:06:08,520] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:06:09,533] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 75000, evaluation results [75000.0, 7882.667340294582, 3164135639.9928446, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.987755389953, 2779190307.3384666, 933.0, 7996.805443399168, 3007666199.202481, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-03-27 00:06:09,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9018146e-12 1.0000000e+00 4.2630203e-15 1.0375553e-11 8.2807734e-17], sum to 1.0000
[2019-03-27 00:06:09,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9636
[2019-03-27 00:06:09,572] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 85.5, 1.0, 2.0, 0.5874426808244066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919498.511964977, 919498.511964977, 210048.3238018114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2802600.0000, 
sim time next is 2803200.0000, 
raw observation next is [22.66666666666666, 84.66666666666666, 1.0, 2.0, 0.6201661007753495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969315.0156245605, 969315.0156245598, 216795.5202452995], 
processed observation next is [1.0, 0.43478260869565216, 0.27330173775671385, 0.8466666666666666, 1.0, 1.0, 0.5423687961148789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26925417100682236, 0.2692541710068222, 0.3235754033511933], 
reward next is 0.6764, 
noisyNet noise sample is [array([1.5232755], dtype=float32), -0.19095074]. 
=============================================
[2019-03-27 00:06:16,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2926857e-14 1.0000000e+00 4.8736241e-18 6.5055173e-14 4.9260131e-19], sum to 1.0000
[2019-03-27 00:06:16,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6707
[2019-03-27 00:06:16,098] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3186198199692449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504778.9420151702, 504778.9420151702, 167401.0737023378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.3158403466178733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500954.1734709924, 500954.1734709924, 167123.6546730745], 
processed observation next is [1.0, 0.8260869565217391, 0.1706161137440759, 0.97, 1.0, 1.0, 0.17571126098538953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13915393707527568, 0.13915393707527568, 0.2494382905568276], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.05171477], dtype=float32), -0.14891995]. 
=============================================
[2019-03-27 00:06:18,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8492037e-14 1.0000000e+00 1.0297993e-18 7.8736549e-15 9.5760321e-20], sum to 1.0000
[2019-03-27 00:06:18,667] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4848
[2019-03-27 00:06:18,671] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.553290195321537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869356.8270014918, 869356.8270014912, 203553.8672014843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2980200.0000, 
sim time next is 2980800.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.5808879828875657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912708.653975482, 912708.653975482, 209038.4272906284], 
processed observation next is [1.0, 0.5217391304347826, 0.2417061611374408, 0.88, 1.0, 1.0, 0.4950457625151394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2535301816598561, 0.2535301816598561, 0.31199765267257973], 
reward next is 0.6880, 
noisyNet noise sample is [array([1.4610788], dtype=float32), 1.0445296]. 
=============================================
[2019-03-27 00:06:19,782] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79632: loss 0.5477
[2019-03-27 00:06:19,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79632: learning rate 0.0001
[2019-03-27 00:06:20,238] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79800: loss 0.5427
[2019-03-27 00:06:20,239] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79800: loss 0.5380
[2019-03-27 00:06:20,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79801: learning rate 0.0001
[2019-03-27 00:06:20,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79801: learning rate 0.0001
[2019-03-27 00:06:20,539] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79872: loss 0.6165
[2019-03-27 00:06:20,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79872: learning rate 0.0001
[2019-03-27 00:06:20,658] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79889: loss 0.5508
[2019-03-27 00:06:20,661] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79890: learning rate 0.0001
[2019-03-27 00:06:20,749] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79897: loss 0.5558
[2019-03-27 00:06:20,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79897: learning rate 0.0001
[2019-03-27 00:06:20,857] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79919: loss 0.5590
[2019-03-27 00:06:20,860] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79919: learning rate 0.0001
[2019-03-27 00:06:20,982] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79946: loss 0.4428
[2019-03-27 00:06:20,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79946: learning rate 0.0001
[2019-03-27 00:06:21,083] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79960: loss 0.4556
[2019-03-27 00:06:21,084] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79960: loss 0.3583
[2019-03-27 00:06:21,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79960: learning rate 0.0001
[2019-03-27 00:06:21,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79960: learning rate 0.0001
[2019-03-27 00:06:21,463] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80075: loss 0.2449
[2019-03-27 00:06:21,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80075: learning rate 0.0001
[2019-03-27 00:06:21,633] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80096: loss 0.2575
[2019-03-27 00:06:21,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80096: learning rate 0.0001
[2019-03-27 00:06:21,863] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80170: loss 0.3788
[2019-03-27 00:06:21,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80170: learning rate 0.0001
[2019-03-27 00:06:22,072] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80227: loss 0.5151
[2019-03-27 00:06:22,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80230: learning rate 0.0001
[2019-03-27 00:06:22,188] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80246: loss 0.4740
[2019-03-27 00:06:22,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80248: learning rate 0.0001
[2019-03-27 00:06:22,291] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80262: loss 0.5012
[2019-03-27 00:06:22,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80263: learning rate 0.0001
[2019-03-27 00:06:24,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9730958e-14 1.0000000e+00 6.3022037e-17 1.5898100e-13 5.4448466e-19], sum to 1.0000
[2019-03-27 00:06:24,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9984
[2019-03-27 00:06:24,375] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.5764305647285458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868067.4489496684, 868067.4489496684, 204174.1835280488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5250409983552279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2534094491690596, 0.2534094491690596, 0.31345196911197226], 
reward next is 0.6865, 
noisyNet noise sample is [array([-2.8225117], dtype=float32), -0.8435312]. 
=============================================
[2019-03-27 00:06:28,302] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6066510e-15 1.0000000e+00 3.0225735e-17 9.3074616e-14 2.3816112e-18], sum to 1.0000
[2019-03-27 00:06:28,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4713
[2019-03-27 00:06:28,318] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3843210724165375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578860.9898432888, 578860.9898432888, 172773.2748123322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3849212062235661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579764.8643923954, 579764.8643923948, 172854.1836680601], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2589412123175496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16104579566455426, 0.1610457956645541, 0.2579913189075524], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.84270734], dtype=float32), 0.008024733]. 
=============================================
[2019-03-27 00:06:31,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6719880e-13 1.0000000e+00 8.9115722e-16 6.4874703e-12 1.5115202e-16], sum to 1.0000
[2019-03-27 00:06:31,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7724
[2019-03-27 00:06:31,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.7554163433148172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1115007.890253888, 1115007.890253889, 241220.8087337572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3146400.0000, 
sim time next is 3147000.0000, 
raw observation next is [24.16666666666666, 88.0, 1.0, 2.0, 0.7142218169628064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051333.316920916, 1051333.316920916, 230970.0905097878], 
processed observation next is [1.0, 0.43478260869565216, 0.34439178515007873, 0.88, 1.0, 1.0, 0.6556889360997668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29203703247803225, 0.29203703247803225, 0.34473147837281765], 
reward next is 0.6553, 
noisyNet noise sample is [array([0.69590336], dtype=float32), 0.30781928]. 
=============================================
[2019-03-27 00:06:31,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.64529 ]
 [59.72013 ]
 [59.896023]
 [60.041   ]
 [60.130264]], R is [[59.72637939]
 [59.76908493]
 [59.80640793]
 [59.85400009]
 [59.92424393]].
[2019-03-27 00:06:31,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7422241e-11 1.0000000e+00 1.6255877e-13 2.3729105e-10 5.2025005e-15], sum to 1.0000
[2019-03-27 00:06:31,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4928
[2019-03-27 00:06:31,267] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.9645215274016441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510402, 1348181.00620739, 1348181.00620739, 288298.3131831085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3162600.0000, 
sim time next is 3163200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 1.02216592542432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1428808.944309621, 1428808.944309621, 305748.3130581075], 
processed observation next is [1.0, 0.6086956521739131, 0.4312796208530806, 0.84, 1.0, 1.0, 1.0267059342461688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3968913734193391, 0.3968913734193391, 0.4563407657583694], 
reward next is 0.5437, 
noisyNet noise sample is [array([0.43919688], dtype=float32), 0.035145063]. 
=============================================
[2019-03-27 00:06:38,760] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87648: loss 0.0321
[2019-03-27 00:06:38,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87648: learning rate 0.0001
[2019-03-27 00:06:39,044] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87777: loss 0.0064
[2019-03-27 00:06:39,049] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87777: learning rate 0.0001
[2019-03-27 00:06:39,137] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 87816: loss 0.0045
[2019-03-27 00:06:39,140] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 87818: learning rate 0.0001
[2019-03-27 00:06:39,179] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87836: loss 0.0037
[2019-03-27 00:06:39,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87837: learning rate 0.0001
[2019-03-27 00:06:39,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87890: loss 0.0087
[2019-03-27 00:06:39,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87890: learning rate 0.0001
[2019-03-27 00:06:39,344] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87910: loss 0.0167
[2019-03-27 00:06:39,347] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87912: loss 0.0251
[2019-03-27 00:06:39,350] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87912: learning rate 0.0001
[2019-03-27 00:06:39,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87910: learning rate 0.0001
[2019-03-27 00:06:39,405] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87936: loss 0.0480
[2019-03-27 00:06:39,408] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87936: learning rate 0.0001
[2019-03-27 00:06:39,451] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87956: loss 0.0468
[2019-03-27 00:06:39,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87957: learning rate 0.0001
[2019-03-27 00:06:39,486] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87970: loss 0.0387
[2019-03-27 00:06:39,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87970: learning rate 0.0001
[2019-03-27 00:06:39,765] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88099: loss 0.0113
[2019-03-27 00:06:39,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88099: learning rate 0.0001
[2019-03-27 00:06:39,773] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88100: loss 0.0202
[2019-03-27 00:06:39,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88100: learning rate 0.0001
[2019-03-27 00:06:39,881] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88150: loss 0.0267
[2019-03-27 00:06:39,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88151: learning rate 0.0001
[2019-03-27 00:06:40,103] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88249: loss 0.0125
[2019-03-27 00:06:40,107] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88250: learning rate 0.0001
[2019-03-27 00:06:40,166] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88273: loss 0.0175
[2019-03-27 00:06:40,167] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88273: learning rate 0.0001
[2019-03-27 00:06:40,199] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88287: loss 0.0282
[2019-03-27 00:06:40,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88289: learning rate 0.0001
[2019-03-27 00:06:42,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2808533e-16 1.0000000e+00 2.2646632e-20 1.8624011e-18 1.1959223e-21], sum to 1.0000
[2019-03-27 00:06:42,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6777
[2019-03-27 00:06:42,203] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5855459607091553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818255.5467687334, 818255.546768734, 198084.2774686287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [30.0, 79.00000000000001, 1.0, 2.0, 0.5863806676398069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819422.435591174, 819422.435591174, 198236.2585279407], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7900000000000001, 1.0, 1.0, 0.5016634549877191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22761734321977056, 0.22761734321977056, 0.2958750127282697], 
reward next is 0.7041, 
noisyNet noise sample is [array([2.0635834], dtype=float32), -0.7381738]. 
=============================================
[2019-03-27 00:06:42,215] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.534294]
 [81.42283 ]
 [81.34871 ]
 [81.27079 ]
 [81.18601 ]], R is [[81.50930023]
 [81.39855957]
 [81.28847504]
 [81.17910767]
 [81.07044983]].
[2019-03-27 00:06:48,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7240293e-08 1.0000000e+00 4.3348758e-13 4.4761350e-08 8.3928154e-16], sum to 1.0000
[2019-03-27 00:06:48,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5178
[2019-03-27 00:06:48,748] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5163185270364803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721482.6658881874, 721482.665888188, 186192.4860737829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5163119925786053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 186191.4310685439], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41724336455253647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2004093143888005, 0.2004093143888005, 0.27789765831125957], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.649215], dtype=float32), -0.032205924]. 
=============================================
[2019-03-27 00:06:48,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[55.687344]
 [56.29556 ]
 [56.946564]
 [57.312946]
 [57.828617]], R is [[55.46973801]
 [55.63714218]
 [55.80281067]
 [55.96674347]
 [56.12908936]].
[2019-03-27 00:06:49,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.8841887e-10 1.0000000e+00 9.6712485e-15 5.5560045e-09 2.2271262e-16], sum to 1.0000
[2019-03-27 00:06:49,048] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3371
[2019-03-27 00:06:49,053] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.7802758105508892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090515.280487867, 1090515.280487866, 239123.9517923286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3465000.0000, 
sim time next is 3465600.0000, 
raw observation next is [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.7623205926511752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1065408.437642088, 1065408.437642088, 234869.0653913397], 
processed observation next is [1.0, 0.08695652173913043, 0.4628751974723541, 0.8233333333333335, 1.0, 1.0, 0.713639268254428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2959467882339133, 0.2959467882339133, 0.3505508438676712], 
reward next is 0.6494, 
noisyNet noise sample is [array([2.6954138], dtype=float32), -0.68652]. 
=============================================
[2019-03-27 00:06:53,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7741010e-09 9.9999988e-01 2.8988201e-12 1.6427356e-07 5.8414344e-15], sum to 1.0000
[2019-03-27 00:06:53,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4861
[2019-03-27 00:06:53,113] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.5552167309940984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775857.2964768948, 775857.2964768953, 192699.7982324186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3526200.0000, 
sim time next is 3526800.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5547762969872104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775241.6114318797, 775241.611431879, 192623.499144762], 
processed observation next is [1.0, 0.8260869565217391, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.4635858999845909, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21534489206441101, 0.21534489206441082, 0.2874977599175552], 
reward next is 0.7125, 
noisyNet noise sample is [array([0.34245658], dtype=float32), 1.7477084]. 
=============================================
[2019-03-27 00:06:56,856] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95735: loss -87.6535
[2019-03-27 00:06:56,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95736: learning rate 0.0001
[2019-03-27 00:06:56,994] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95798: loss -87.2938
[2019-03-27 00:06:56,998] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95798: learning rate 0.0001
[2019-03-27 00:06:57,145] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 95864: loss -122.2011
[2019-03-27 00:06:57,148] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 95865: learning rate 0.0001
[2019-03-27 00:06:57,233] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95901: loss -35.6479
[2019-03-27 00:06:57,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95901: learning rate 0.0001
[2019-03-27 00:06:57,237] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95901: loss 45.2388
[2019-03-27 00:06:57,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95902: learning rate 0.0001
[2019-03-27 00:06:57,253] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95909: loss 46.8766
[2019-03-27 00:06:57,254] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95909: learning rate 0.0001
[2019-03-27 00:06:57,339] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95950: loss -127.6382
[2019-03-27 00:06:57,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95950: learning rate 0.0001
[2019-03-27 00:06:57,364] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95957: loss -8.4670
[2019-03-27 00:06:57,364] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95957: learning rate 0.0001
[2019-03-27 00:06:57,450] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95997: loss -45.4735
[2019-03-27 00:06:57,453] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95997: learning rate 0.0001
[2019-03-27 00:06:57,477] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96009: loss 11.9227
[2019-03-27 00:06:57,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96009: learning rate 0.0001
[2019-03-27 00:06:57,589] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96054: loss 2.9922
[2019-03-27 00:06:57,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96056: learning rate 0.0001
[2019-03-27 00:06:57,640] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96076: loss -48.8982
[2019-03-27 00:06:57,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96077: learning rate 0.0001
[2019-03-27 00:06:57,740] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96120: loss -19.9635
[2019-03-27 00:06:57,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96120: learning rate 0.0001
[2019-03-27 00:06:57,872] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96181: loss -16.2516
[2019-03-27 00:06:57,875] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96181: learning rate 0.0001
[2019-03-27 00:06:57,923] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96203: loss -54.9215
[2019-03-27 00:06:57,926] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96203: learning rate 0.0001
[2019-03-27 00:06:57,946] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0599603e-05 8.9762795e-01 2.9337767e-07 1.0233111e-01 1.1710298e-10], sum to 1.0000
[2019-03-27 00:06:57,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2723
[2019-03-27 00:06:57,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2541753.07983694 W.
[2019-03-27 00:06:57,973] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 61.66666666666667, 1.0, 2.0, 0.6058237696352652, 1.0, 2.0, 0.6058237696352652, 1.0, 2.0, 1.03, 6.936061457278865, 6.9112, 170.5573041426782, 2541753.07983694, 2523943.806688666, 490099.7989803067], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3598800.0000, 
sim time next is 3599400.0000, 
raw observation next is [33.0, 62.33333333333333, 1.0, 2.0, 0.8872081540743265, 1.0, 2.0, 0.8872081540743265, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2481480.409886425, 2481480.409886424, 464564.3388707807], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.6233333333333333, 1.0, 1.0, 0.8641062097281043, 1.0, 1.0, 0.8641062097281043, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6893001138573402, 0.68930011385734, 0.6933796102548966], 
reward next is 0.3066, 
noisyNet noise sample is [array([1.0186679], dtype=float32), 1.146454]. 
=============================================
[2019-03-27 00:06:58,019] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96247: loss -28.7009
[2019-03-27 00:06:58,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96247: learning rate 0.0001
[2019-03-27 00:07:05,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9602277e-10 9.9985325e-01 3.6567867e-12 1.4673080e-04 3.6775973e-15], sum to 1.0000
[2019-03-27 00:07:05,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6617
[2019-03-27 00:07:05,644] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 75.66666666666666, 1.0, 2.0, 0.7779722481120304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087510.913913973, 1087510.913913973, 238602.1432485454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [27.66666666666667, 74.83333333333334, 1.0, 2.0, 0.7895435816668988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1103474.668945929, 1103474.668945929, 241356.8499119864], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012641, 0.7483333333333334, 1.0, 1.0, 0.7464380502010828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3065207413738692, 0.3065207413738692, 0.36023410434624836], 
reward next is 0.6398, 
noisyNet noise sample is [array([0.05007788], dtype=float32), 0.10022002]. 
=============================================
[2019-03-27 00:07:06,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8400537e-09 9.9961036e-01 2.4164117e-11 3.8964936e-04 1.4005972e-14], sum to 1.0000
[2019-03-27 00:07:06,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1420
[2019-03-27 00:07:06,053] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2341574.71270409 W.
[2019-03-27 00:07:06,058] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.8372342403979813, 1.0, 1.0, 0.8372342403979813, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2341574.71270409, 2341574.712704091, 438394.1342195612], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3745800.0000, 
sim time next is 3746400.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.7377131130834993, 1.0, 2.0, 0.7377131130834993, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2062983.959265002, 2062983.959265002, 390678.1550055029], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.6839917025102401, 1.0, 1.0, 0.6839917025102401, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5730510997958339, 0.5730510997958339, 0.5831017238888103], 
reward next is 0.4169, 
noisyNet noise sample is [array([0.3790314], dtype=float32), 0.41095337]. 
=============================================
[2019-03-27 00:07:06,383] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 00:07:06,385] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:07:06,386] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:07:06,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:07:06,387] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:07:06,388] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:07:06,389] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:07:06,389] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:07:06,390] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:07:06,392] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:07:06,393] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:07:06,409] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-27 00:07:06,430] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-27 00:07:06,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-27 00:07:06,477] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-27 00:07:06,493] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-27 00:07:20,133] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:20,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.80638813833333, 92.52874008666666, 1.0, 2.0, 0.2446872003028276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 404845.4928530201, 404845.4928530194, 160285.2029469355]
[2019-03-27 00:07:20,138] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:07:20,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.6905392e-12 9.9999988e-01 2.3977692e-14 6.6353401e-08 1.6059870e-17], sampled 0.2815062514032779
[2019-03-27 00:07:29,225] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:29,227] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.99680459333334, 97.37075122666667, 1.0, 2.0, 0.4549085089921631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645598.431818324, 645598.431818324, 178116.8204049911]
[2019-03-27 00:07:29,229] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:07:29,231] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8182573e-11 9.9999964e-01 2.4607299e-13 3.6152426e-07 3.4454415e-16], sampled 0.5837744805105801
[2019-03-27 00:07:36,897] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:36,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.35, 79.5, 1.0, 2.0, 0.5019271728438487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701366.1296260753, 701366.1296260753, 183898.32651929]
[2019-03-27 00:07:36,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:07:36,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3228423e-11 9.9999928e-01 3.0092319e-13 6.9017932e-07 2.5868135e-16], sampled 0.9819207872066633
[2019-03-27 00:07:41,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:41,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.39174873, 70.73802945, 1.0, 2.0, 0.4189322156461287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599708.425430024, 599708.425430024, 173733.8892233408]
[2019-03-27 00:07:41,425] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:07:41,428] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1859895e-09 9.9997735e-01 3.1206233e-12 2.2654393e-05 1.9754382e-15], sampled 0.14085572014363124
[2019-03-27 00:07:45,896] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:45,897] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.01666666666667, 75.33333333333333, 1.0, 2.0, 0.575918220197218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804796.4211451918, 804796.4211451918, 196344.6453576986]
[2019-03-27 00:07:45,898] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:07:45,900] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1327935e-10 9.9999893e-01 4.0561575e-13 1.1001707e-06 2.4433126e-16], sampled 0.6233034163804626
[2019-03-27 00:07:47,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:07:47,165] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.45535284, 100.0, 1.0, 2.0, 0.2920057100804507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470167.9025119431, 470167.9025119431, 164971.9369485484]
[2019-03-27 00:07:47,165] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:07:47,168] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1100701e-11 9.9999976e-01 9.4038187e-14 1.8034950e-07 6.3015056e-17], sampled 0.139695695869959
[2019-03-27 00:08:17,095] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:17,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.472600875, 62.17457505500001, 1.0, 2.0, 0.8015090088387318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1120206.500664324, 1120206.500664323, 244283.8786780325]
[2019-03-27 00:08:17,098] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:08:17,102] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.066060e-10 9.999912e-01 4.218462e-12 8.872793e-06 4.250787e-15], sampled 0.5001757545020832
[2019-03-27 00:08:18,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:18,435] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98132509333334, 70.50235650666667, 1.0, 2.0, 0.7718585230597385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1089664.678015163, 1089664.678015163, 238635.7434789683]
[2019-03-27 00:08:18,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:08:18,441] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9175945e-09 9.9997592e-01 1.0921571e-11 2.4075938e-05 1.2258886e-14], sampled 0.8130209021925577
[2019-03-27 00:08:20,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:20,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.61695489166667, 71.61445131833334, 1.0, 2.0, 0.6060827841335261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 846965.6239720264, 846965.6239720258, 201884.328755056]
[2019-03-27 00:08:20,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:08:20,509] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2074252e-10 9.9999797e-01 1.1520576e-12 2.0830119e-06 1.3107572e-15], sampled 0.7532052638464409
[2019-03-27 00:08:23,792] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:23,794] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.07019554833334, 87.11123601666667, 1.0, 2.0, 0.8574490717668496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1198433.676243125, 1198433.676243125, 258505.9265380291]
[2019-03-27 00:08:23,794] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:08:23,796] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1488765e-09 9.9998379e-01 5.1960580e-12 1.6268692e-05 5.0784144e-15], sampled 0.8460904172453554
[2019-03-27 00:08:27,095] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:27,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.78786084, 79.09409268, 1.0, 2.0, 0.6352234197773844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887704.9941717508, 887704.9941717508, 207493.4298799532]
[2019-03-27 00:08:27,099] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:08:27,102] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6673303e-10 9.9999619e-01 7.9223200e-13 3.7847367e-06 4.9169908e-16], sampled 0.9842085933880295
[2019-03-27 00:08:28,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:28,171] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.8, 53.0, 1.0, 2.0, 0.5123013271457002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715867.3014275094, 715867.3014275087, 185545.9111822582]
[2019-03-27 00:08:28,172] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:28,175] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4544158e-10 9.9999809e-01 1.1936808e-12 1.9123436e-06 1.0404088e-15], sampled 0.350702058697164
[2019-03-27 00:08:29,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:29,606] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.58333333333333, 95.0, 1.0, 2.0, 0.525615003447051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734477.6778275277, 734477.6778275283, 187705.958538583]
[2019-03-27 00:08:29,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:29,611] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5049156e-11 9.9999893e-01 3.6636918e-13 1.0211213e-06 1.5570741e-16], sampled 0.8183330863955944
[2019-03-27 00:08:30,915] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:30,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.9, 72.33333333333334, 1.0, 2.0, 0.599518948616268, 0.0, 2.0, 0.0, 1.0, 2.0, 1.028037705988106, 6.911199999999999, 6.9112, 168.9128557139978, 1676240.208728159, 1676240.20872816, 364528.1486243267]
[2019-03-27 00:08:30,918] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:08:30,922] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3845001e-08 9.9909508e-01 1.1349950e-10 9.0488035e-04 9.6579320e-14], sampled 0.38748566855281863
[2019-03-27 00:08:30,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1676240.208728159 W.
[2019-03-27 00:08:31,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:31,896] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 94.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.973174445121463, 6.9112, 168.9124672572736, 1497751.714228048, 1453785.037212107, 311352.3118751975]
[2019-03-27 00:08:31,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:08:31,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9771970e-09 9.9990475e-01 4.0752818e-11 9.5292256e-05 6.8751299e-14], sampled 0.7477050723989828
[2019-03-27 00:08:39,526] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:39,529] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.08333333333334, 71.66666666666667, 1.0, 2.0, 0.5545054565263946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774863.0022857768, 774863.0022857768, 192575.9873837465]
[2019-03-27 00:08:39,530] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:39,535] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4643524e-10 9.9999619e-01 1.3169164e-12 3.8591447e-06 7.6754309e-16], sampled 0.04477325190581749
[2019-03-27 00:08:48,245] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:48,246] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.51666666666667, 67.5, 1.0, 2.0, 0.6941607250093221, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.955785802276656, 6.9112, 168.9126546822336, 1866957.612121972, 1835326.965982486, 384897.3716472084]
[2019-03-27 00:08:48,251] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:48,254] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2166231e-07 9.9869764e-01 8.7618618e-10 1.3021851e-03 1.9478538e-12], sampled 0.11495354028722127
[2019-03-27 00:08:48,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1866957.612121972 W.
[2019-03-27 00:08:49,736] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:49,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.33333333333333, 78.83333333333334, 1.0, 2.0, 0.5702872232265747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796924.632898325, 796924.6328983256, 195340.5692170507]
[2019-03-27 00:08:49,741] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:08:49,745] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0171916e-10 9.9999774e-01 7.8915211e-13 2.2213715e-06 3.7873628e-16], sampled 0.6908112036352768
[2019-03-27 00:08:56,691] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03248104]
[2019-03-27 00:08:56,694] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.502987345, 88.21721442500001, 1.0, 2.0, 0.4352471785100961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 630633.3764788562, 630633.3764788556, 176953.3395345332]
[2019-03-27 00:08:56,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:08:56,699] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.4885238e-11 9.9999917e-01 4.3840051e-13 8.1217661e-07 5.1224227e-16], sampled 0.4005224415609919
[2019-03-27 00:08:59,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6332 2927028281.9576 1332.0000
[2019-03-27 00:09:00,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1929 3007563816.2842 1755.0000
[2019-03-27 00:09:00,150] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.0248 3163587560.2905 1769.0000
[2019-03-27 00:09:00,498] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9044 2779269203.0389 933.0000
[2019-03-27 00:09:00,554] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.8316 2842461926.5882 1125.0000
[2019-03-27 00:09:01,570] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 100000, evaluation results [100000.0, 7884.024788568769, 3163587560.2904778, 1769.0, 8254.633183974462, 2927028281.9576015, 1332.0, 8658.904358503349, 2779269203.038908, 933.0, 7999.192910120978, 3007563816.284236, 1755.0, 8493.831580443491, 2842461926.5881743, 1125.0]
[2019-03-27 00:09:03,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.09486665e-11 9.99999046e-01 3.99722386e-15 9.77537525e-07
 7.36522600e-19], sum to 1.0000
[2019-03-27 00:09:03,431] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1467
[2019-03-27 00:09:03,437] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 79.0, 1.0, 2.0, 0.534595523560122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747031.1835200633, 747031.1835200626, 189193.6894458492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3793200.0000, 
sim time next is 3793800.0000, 
raw observation next is [28.16666666666667, 79.0, 1.0, 2.0, 0.5283272826043334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738269.0474789258, 738269.0474789252, 188153.1214616688], 
processed observation next is [1.0, 0.9130434782608695, 0.5339652448657191, 0.79, 1.0, 1.0, 0.43171961759558236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20507473541081273, 0.20507473541081256, 0.2808255544204012], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.21942207], dtype=float32), 1.4189179]. 
=============================================
[2019-03-27 00:09:09,724] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103674: loss 0.0753
[2019-03-27 00:09:09,726] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103674: learning rate 0.0001
[2019-03-27 00:09:09,979] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103790: loss 0.0265
[2019-03-27 00:09:09,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103790: learning rate 0.0001
[2019-03-27 00:09:10,164] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103870: loss 0.0063
[2019-03-27 00:09:10,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103871: learning rate 0.0001
[2019-03-27 00:09:10,209] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 103895: loss 0.0139
[2019-03-27 00:09:10,210] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 103895: learning rate 0.0001
[2019-03-27 00:09:10,245] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103908: loss 0.0157
[2019-03-27 00:09:10,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103908: learning rate 0.0001
[2019-03-27 00:09:10,284] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103929: loss 0.0449
[2019-03-27 00:09:10,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103930: learning rate 0.0001
[2019-03-27 00:09:10,308] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103941: loss 0.0504
[2019-03-27 00:09:10,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103941: learning rate 0.0001
[2019-03-27 00:09:10,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7406388e-14 1.0000000e+00 1.1904724e-16 1.2968626e-09 1.0850279e-19], sum to 1.0000
[2019-03-27 00:09:10,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4783
[2019-03-27 00:09:10,362] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.586725941692012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819905.1160749003, 819905.1160749003, 198298.557284411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3888600.0000, 
sim time next is 3889200.0000, 
raw observation next is [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5839132199405105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815973.0404311869, 815973.0404311869, 197786.8549785381], 
processed observation next is [0.0, 0.0, 0.5576619273301741, 0.8566666666666667, 1.0, 1.0, 0.49869062643434997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2266591778975519, 0.2266591778975519, 0.29520426116199716], 
reward next is 0.7048, 
noisyNet noise sample is [array([2.6250029], dtype=float32), 0.020952914]. 
=============================================
[2019-03-27 00:09:10,368] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103966: loss 0.0467
[2019-03-27 00:09:10,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103967: learning rate 0.0001
[2019-03-27 00:09:10,399] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103980: loss 0.0567
[2019-03-27 00:09:10,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103981: learning rate 0.0001
[2019-03-27 00:09:10,510] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104029: loss 0.0529
[2019-03-27 00:09:10,511] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104029: loss 0.0418
[2019-03-27 00:09:10,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104029: learning rate 0.0001
[2019-03-27 00:09:10,513] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104029: learning rate 0.0001
[2019-03-27 00:09:10,602] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104065: loss 0.0624
[2019-03-27 00:09:10,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104066: learning rate 0.0001
[2019-03-27 00:09:10,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104069: loss 0.0464
[2019-03-27 00:09:10,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104069: learning rate 0.0001
[2019-03-27 00:09:10,952] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 104183: loss 0.0165
[2019-03-27 00:09:10,954] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 104183: learning rate 0.0001
[2019-03-27 00:09:11,027] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104213: loss 0.0018
[2019-03-27 00:09:11,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104214: learning rate 0.0001
[2019-03-27 00:09:11,121] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104255: loss 0.0005
[2019-03-27 00:09:11,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104255: learning rate 0.0001
[2019-03-27 00:09:14,070] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5006180e-16 1.0000000e+00 1.9890142e-19 1.5913323e-10 7.2641821e-23], sum to 1.0000
[2019-03-27 00:09:14,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7014
[2019-03-27 00:09:14,087] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6214420417559785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868438.0727718945, 868438.0727718945, 204809.1119490259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3970800.0000, 
sim time next is 3971400.0000, 
raw observation next is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6345517343025823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886765.9427293071, 886765.9427293071, 207359.2051816386], 
processed observation next is [0.0, 1.0, 0.6603475513428123, 0.7983333333333335, 1.0, 1.0, 0.5597008847019064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2463238729803631, 0.2463238729803631, 0.309491351017371], 
reward next is 0.6905, 
noisyNet noise sample is [array([-0.66999394], dtype=float32), -1.4641898]. 
=============================================
[2019-03-27 00:09:15,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8779678e-11 9.9999976e-01 5.0837438e-13 2.8405748e-07 3.2464728e-17], sum to 1.0000
[2019-03-27 00:09:15,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7912
[2019-03-27 00:09:15,879] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5867293978527562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819909.9476621918, 819909.9476621918, 198299.2891513203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3981000.0000, 
sim time next is 3981600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5860023686779927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818893.5877447528, 818893.5877447533, 198166.8269466856], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5012076731060153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2274704410402091, 0.22747044104020925, 0.29577138350251586], 
reward next is 0.7042, 
noisyNet noise sample is [array([-0.76851624], dtype=float32), -0.15456377]. 
=============================================
[2019-03-27 00:09:20,834] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9491280e-13 9.9999988e-01 1.4639045e-15 9.4071666e-08 9.5173939e-21], sum to 1.0000
[2019-03-27 00:09:20,841] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8946
[2019-03-27 00:09:20,846] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 86.83333333333333, 1.0, 2.0, 0.8560973863711036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196543.397142065, 1196543.397142065, 258146.9177549946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4071000.0000, 
sim time next is 4071600.0000, 
raw observation next is [27.4, 87.0, 1.0, 2.0, 0.8924119824338216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1247329.107210822, 1247329.107210823, 267859.2786927132], 
processed observation next is [1.0, 0.13043478260869565, 0.4976303317535545, 0.87, 1.0, 1.0, 0.8703758824503874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34648030755856163, 0.34648030755856196, 0.3997899681980794], 
reward next is 0.6002, 
noisyNet noise sample is [array([-0.41629836], dtype=float32), 1.1631511]. 
=============================================
[2019-03-27 00:09:22,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9766220e-12 9.9932337e-01 3.7623703e-14 6.7662972e-04 8.6947539e-19], sum to 1.0000
[2019-03-27 00:09:22,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5576
[2019-03-27 00:09:22,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3053541.526237371 W.
[2019-03-27 00:09:22,561] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.83333333333333, 71.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.995417848664037, 6.9112, 168.9069319423394, 3053541.526237371, 2284387.502662514, 473570.869845471], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [33.0, 71.0, 1.0, 2.0, 1.012506584788245, 1.0, 1.0, 1.012506584788245, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2832331.603239266, 2832331.603239265, 536600.069079177], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.71, 1.0, 1.0, 1.0150681744436687, 1.0, 0.5, 1.0150681744436687, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7867587786775739, 0.7867587786775736, 0.800895625491309], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3775922], dtype=float32), 0.12830047]. 
=============================================
[2019-03-27 00:09:22,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[50.153183]
 [49.425507]
 [47.66269 ]
 [45.971138]
 [45.272224]], R is [[49.79253387]
 [49.29460907]
 [49.10336304]
 [48.87934875]
 [48.39923477]].
[2019-03-27 00:09:22,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4134931e-10 9.9538535e-01 7.6392798e-13 4.6146112e-03 3.9954170e-17], sum to 1.0000
[2019-03-27 00:09:22,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1443
[2019-03-27 00:09:22,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2501400.513175678 W.
[2019-03-27 00:09:22,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.83333333333333, 71.0, 1.0, 2.0, 0.8943231027891237, 1.0, 2.0, 0.8943231027891237, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2501400.513175678, 2501400.513175678, 468417.2979488353], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [33.0, 71.0, 1.0, 2.0, 0.6431538801149381, 1.0, 2.0, 0.6421669795717315, 1.0, 1.0, 1.03, 7.005093250320455, 6.9112, 170.5573041426782, 2694396.599020276, 2627137.044482504, 503975.3731734703], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.71, 1.0, 1.0, 0.5700649158011303, 1.0, 1.0, 0.5688758790020861, 1.0, 0.5, 1.0365853658536586, 0.009389325032045459, 0.0, 0.8375144448122397, 0.7484434997278545, 0.7297602901340289, 0.7522020495126422], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3686879], dtype=float32), 1.0121787]. 
=============================================
[2019-03-27 00:09:22,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[48.910137]
 [47.325714]
 [46.693573]
 [45.167088]
 [44.216614]], R is [[48.57957458]
 [48.09378052]
 [47.61284256]
 [47.4037323 ]
 [46.93837357]].
[2019-03-27 00:09:23,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8672106e-09 1.6017851e-01 1.5522964e-12 8.3982146e-01 5.1132206e-17], sum to 1.0000
[2019-03-27 00:09:23,344] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3821
[2019-03-27 00:09:23,351] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3399028.4601762 W.
[2019-03-27 00:09:23,357] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 64.0, 1.0, 2.0, 0.978573269309165, 1.0, 2.0, 0.8098766741688449, 1.0, 1.0, 1.03, 7.005119707687289, 6.9112, 170.5573041426782, 3399028.4601762, 3331749.953150348, 623774.4075690391], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4114800.0000, 
sim time next is 4115400.0000, 
raw observation next is [35.83333333333334, 64.5, 1.0, 2.0, 1.001162373416377, 1.0, 2.0, 0.8211712262224508, 1.0, 2.0, 1.03, 7.005121490553877, 6.9112, 170.5573041426782, 3446496.784453954, 3379217.000288242, 633351.5587238773], 
processed observation next is [1.0, 0.6521739130434783, 0.8973143759873622, 0.645, 1.0, 1.0, 1.0014004498992495, 1.0, 1.0, 0.7845436460511455, 1.0, 1.0, 1.0365853658536586, 0.009392149055387655, 0.0, 0.8375144448122397, 0.9573602179038762, 0.9386713889689561, 0.9453008339162348], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0397907], dtype=float32), -0.12498708]. 
=============================================
[2019-03-27 00:09:23,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6754607e-11 9.1761672e-01 1.2850259e-14 8.2383260e-02 2.2739360e-19], sum to 1.0000
[2019-03-27 00:09:23,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9099
[2019-03-27 00:09:23,998] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.62128216746955, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868214.5641327193, 868214.5641327199, 204779.7931549513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4125600.0000, 
sim time next is 4126200.0000, 
raw observation next is [32.66666666666666, 72.33333333333334, 1.0, 2.0, 0.6269024741403783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876071.9353101765, 876071.9353101765, 205866.3755992966], 
processed observation next is [1.0, 0.782608695652174, 0.7472353870458132, 0.7233333333333334, 1.0, 1.0, 0.5504849086028654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2433533153639379, 0.2433533153639379, 0.30726324716312925], 
reward next is 0.6927, 
noisyNet noise sample is [array([0.1095585], dtype=float32), -1.6407034]. 
=============================================
[2019-03-27 00:09:25,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0050110e-14 1.0000000e+00 6.5775993e-17 4.2042338e-08 1.7636456e-22], sum to 1.0000
[2019-03-27 00:09:25,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4881
[2019-03-27 00:09:25,174] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5796733832508436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810045.9411018521, 810045.9411018527, 197020.3200530737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4137600.0000, 
sim time next is 4138200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5799068443955615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810372.3083294906, 810372.3083294906, 197062.4012206825], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49386366794645964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22510341898041406, 0.22510341898041406, 0.29412298689654104], 
reward next is 0.7059, 
noisyNet noise sample is [array([2.213028], dtype=float32), 0.4203266]. 
=============================================
[2019-03-27 00:09:27,840] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111717: loss -90.8136
[2019-03-27 00:09:27,841] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111718: learning rate 0.0001
[2019-03-27 00:09:28,196] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111874: loss -148.6551
[2019-03-27 00:09:28,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111876: learning rate 0.0001
[2019-03-27 00:09:28,285] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111911: loss -89.1365
[2019-03-27 00:09:28,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111911: learning rate 0.0001
[2019-03-27 00:09:28,376] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111953: loss -103.1007
[2019-03-27 00:09:28,379] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111954: learning rate 0.0001
[2019-03-27 00:09:28,401] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111967: loss -136.0557
[2019-03-27 00:09:28,403] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111967: loss -99.9575
[2019-03-27 00:09:28,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111967: learning rate 0.0001
[2019-03-27 00:09:28,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111967: learning rate 0.0001
[2019-03-27 00:09:28,419] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111972: loss -80.6441
[2019-03-27 00:09:28,420] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111972: loss -93.7578
[2019-03-27 00:09:28,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111972: learning rate 0.0001
[2019-03-27 00:09:28,424] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111973: learning rate 0.0001
[2019-03-27 00:09:28,454] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111986: loss -105.5256
[2019-03-27 00:09:28,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111986: learning rate 0.0001
[2019-03-27 00:09:28,467] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 111989: loss -90.7402
[2019-03-27 00:09:28,469] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 111990: learning rate 0.0001
[2019-03-27 00:09:28,510] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112008: loss -166.3932
[2019-03-27 00:09:28,512] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112009: learning rate 0.0001
[2019-03-27 00:09:28,528] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112017: loss -84.1404
[2019-03-27 00:09:28,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112017: learning rate 0.0001
[2019-03-27 00:09:28,628] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112064: loss -155.6797
[2019-03-27 00:09:28,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112064: learning rate 0.0001
[2019-03-27 00:09:28,795] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112136: loss -99.5966
[2019-03-27 00:09:28,800] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112136: learning rate 0.0001
[2019-03-27 00:09:28,819] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 112144: loss -82.2902
[2019-03-27 00:09:28,823] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 112146: learning rate 0.0001
[2019-03-27 00:09:28,851] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112158: loss -92.6065
[2019-03-27 00:09:28,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112158: learning rate 0.0001
[2019-03-27 00:09:28,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4293510e-08 3.7723985e-01 1.1754003e-11 6.2276018e-01 3.8223833e-16], sum to 1.0000
[2019-03-27 00:09:28,872] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7970
[2019-03-27 00:09:28,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 51.5, 1.0, 2.0, 1.0043091480825, 1.0, 2.0, 1.0043091480825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2809374.748173898, 2809374.748173898, 531598.0710256002], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4206600.0000, 
sim time next is 4207200.0000, 
raw observation next is [36.0, 51.0, 1.0, 2.0, 0.9735031506200406, 1.0, 2.0, 0.9735031506200406, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2723106.573406268, 2723106.573406268, 513182.791855268], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.51, 1.0, 1.0, 0.9680760850843863, 1.0, 1.0, 0.9680760850843863, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7564184926128522, 0.7564184926128522, 0.7659444654556239], 
reward next is 0.2341, 
noisyNet noise sample is [array([-0.0181114], dtype=float32), -1.2534192]. 
=============================================
[2019-03-27 00:09:35,834] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6277218e-12 9.9998963e-01 7.6114135e-15 1.0366240e-05 8.0994791e-19], sum to 1.0000
[2019-03-27 00:09:35,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7323
[2019-03-27 00:09:35,848] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6253906125947705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873958.298074756, 873958.298074756, 205571.8518761977], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4317600.0000, 
sim time next is 4318200.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6246589710612662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872935.4382042255, 872935.4382042255, 205430.1732259692], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5477818928448991, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2424820661678404, 0.2424820661678404, 0.3066121988447301], 
reward next is 0.6934, 
noisyNet noise sample is [array([0.44110242], dtype=float32), 0.25717983]. 
=============================================
[2019-03-27 00:09:38,371] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5214648e-10 9.9983299e-01 1.8927358e-13 1.6692918e-04 6.7867628e-17], sum to 1.0000
[2019-03-27 00:09:38,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4537
[2019-03-27 00:09:38,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2709857.622577364 W.
[2019-03-27 00:09:38,392] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.6505156787209885, 1.0, 2.0, 0.6458478788747568, 1.0, 2.0, 1.03, 7.005093830687723, 6.9112, 170.5573041426782, 2709857.622577364, 2642597.65229891, 506155.66228282], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.9188960550244549, 1.0, 2.0, 0.9188960550244549, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2570201.167244846, 2570201.167244846, 481921.2371805999], 
processed observation next is [1.0, 0.5652173913043478, 0.95260663507109, 0.54, 1.0, 1.0, 0.9022844036439216, 1.0, 1.0, 0.9022844036439216, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7139447686791239, 0.7139447686791239, 0.719285428627761], 
reward next is 0.2807, 
noisyNet noise sample is [array([-0.10353305], dtype=float32), -1.5009364]. 
=============================================
[2019-03-27 00:09:42,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5825051e-19 1.0000000e+00 5.2386872e-22 1.3275861e-17 5.0977224e-26], sum to 1.0000
[2019-03-27 00:09:42,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3055
[2019-03-27 00:09:42,960] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6385049556519021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 892292.7649808782, 892292.7649808788, 208139.5210176102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4448400.0000, 
sim time next is 4449000.0000, 
raw observation next is [32.83333333333333, 71.66666666666667, 1.0, 2.0, 0.6404039290390139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894947.6451074779, 894947.6451074779, 208515.4691915082], 
processed observation next is [0.0, 0.4782608695652174, 0.7551342812006318, 0.7166666666666667, 1.0, 1.0, 0.5667517217337517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24859656808541053, 0.24859656808541053, 0.3112171181962809], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.73217577], dtype=float32), 1.4875247]. 
=============================================
[2019-03-27 00:09:42,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.112976]
 [69.08379 ]
 [69.0326  ]
 [68.99034 ]
 [68.95872 ]], R is [[69.10961151]
 [69.10786438]
 [69.10656738]
 [69.10497284]
 [69.10127258]].
[2019-03-27 00:09:45,595] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119661: loss 0.6504
[2019-03-27 00:09:45,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119661: learning rate 0.0001
[2019-03-27 00:09:45,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8660011e-22 1.0000000e+00 4.1133006e-24 5.0029378e-19 3.5927016e-29], sum to 1.0000
[2019-03-27 00:09:45,902] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7090
[2019-03-27 00:09:45,907] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5087838850569756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710950.5380309998, 710950.5380310004, 184983.2191573847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4492800.0000, 
sim time next is 4493400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5079805750720062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709827.6556061554, 709827.655606156, 184855.410145665], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4072055121349472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1971743487794876, 0.19717434877948778, 0.2759035972323358], 
reward next is 0.7241, 
noisyNet noise sample is [array([-1.0285505], dtype=float32), 0.57185894]. 
=============================================
[2019-03-27 00:09:45,967] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119828: loss 0.6855
[2019-03-27 00:09:45,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119828: learning rate 0.0001
[2019-03-27 00:09:46,079] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119880: loss 0.6969
[2019-03-27 00:09:46,080] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119881: loss 0.6826
[2019-03-27 00:09:46,083] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119881: learning rate 0.0001
[2019-03-27 00:09:46,088] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119882: learning rate 0.0001
[2019-03-27 00:09:46,188] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119926: loss 0.3210
[2019-03-27 00:09:46,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119926: learning rate 0.0001
[2019-03-27 00:09:46,296] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119973: loss 0.2308
[2019-03-27 00:09:46,299] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119975: learning rate 0.0001
[2019-03-27 00:09:46,331] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119989: loss 0.1765
[2019-03-27 00:09:46,332] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119989: loss 0.1933
[2019-03-27 00:09:46,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119990: learning rate 0.0001
[2019-03-27 00:09:46,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119990: learning rate 0.0001
[2019-03-27 00:09:46,346] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119999: loss 0.1522
[2019-03-27 00:09:46,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119999: learning rate 0.0001
[2019-03-27 00:09:46,352] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119999: loss 0.1404
[2019-03-27 00:09:46,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120000: learning rate 0.0001
[2019-03-27 00:09:46,378] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120010: loss 0.1073
[2019-03-27 00:09:46,380] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120010: learning rate 0.0001
[2019-03-27 00:09:46,405] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120021: loss 0.1045
[2019-03-27 00:09:46,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120021: learning rate 0.0001
[2019-03-27 00:09:46,628] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120121: loss 0.0127
[2019-03-27 00:09:46,632] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120121: learning rate 0.0001
[2019-03-27 00:09:46,728] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120167: loss 0.0171
[2019-03-27 00:09:46,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120167: learning rate 0.0001
[2019-03-27 00:09:46,745] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120175: loss 0.0214
[2019-03-27 00:09:46,748] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120176: learning rate 0.0001
[2019-03-27 00:09:46,776] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120187: loss 0.0129
[2019-03-27 00:09:46,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120188: learning rate 0.0001
[2019-03-27 00:09:47,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4241619e-20 1.0000000e+00 6.5052429e-22 2.5237225e-16 1.9013886e-25], sum to 1.0000
[2019-03-27 00:09:47,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9794
[2019-03-27 00:09:47,452] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5118758042310636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715272.494265225, 715272.494265225, 185477.5739025362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4518000.0000, 
sim time next is 4518600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5123566830453368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715944.6793896905, 715944.6793896911, 185554.6149372619], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4124779313799238, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19887352205269182, 0.19887352205269196, 0.27694718647352523], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.99005246], dtype=float32), -1.140696]. 
=============================================
[2019-03-27 00:09:48,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2620730e-20 1.0000000e+00 7.1333372e-22 2.7564882e-16 8.8874234e-26], sum to 1.0000
[2019-03-27 00:09:48,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6602
[2019-03-27 00:09:48,126] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5150205109398079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719668.2561756148, 719668.2561756148, 185983.1662604543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4528800.0000, 
sim time next is 4529400.0000, 
raw observation next is [28.16666666666667, 79.00000000000001, 1.0, 2.0, 0.520589465356836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727452.7462166852, 727452.7462166859, 186885.1744092829], 
processed observation next is [0.0, 0.43478260869565216, 0.5339652448657191, 0.7900000000000001, 1.0, 1.0, 0.42239694621305546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20207020728241254, 0.20207020728241273, 0.27893309613325806], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.7949031], dtype=float32), 1.528386]. 
=============================================
[2019-03-27 00:09:48,907] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1836672e-18 1.0000000e+00 7.1605770e-21 1.8173669e-16 1.0551948e-24], sum to 1.0000
[2019-03-27 00:09:48,917] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9892
[2019-03-27 00:09:48,921] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.66666666666667, 52.16666666666667, 1.0, 2.0, 0.5310261909823446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742041.7401686083, 742041.7401686077, 188600.2116308676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4542600.0000, 
sim time next is 4543200.0000, 
raw observation next is [34.0, 50.0, 1.0, 2.0, 0.5248473823490418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733404.658236917, 733404.658236917, 187580.9054157611], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.5, 1.0, 1.0, 0.42752696668559254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2037235161769214, 0.2037235161769214, 0.279971500620539], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.61941653], dtype=float32), 0.4303756]. 
=============================================
[2019-03-27 00:09:50,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0053977e-23 1.0000000e+00 9.2598576e-23 1.8380013e-19 8.8046680e-29], sum to 1.0000
[2019-03-27 00:09:50,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3101
[2019-03-27 00:09:50,635] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5477374752111377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765402.0533916256, 765402.0533916249, 191413.3666113194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4573800.0000, 
sim time next is 4574400.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.54855875032214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766550.1078980914, 766550.1078980914, 191553.7051416827], 
processed observation next is [0.0, 0.9565217391304348, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4560948799061927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2129305855272476, 0.2129305855272476, 0.2859010524502727], 
reward next is 0.7141, 
noisyNet noise sample is [array([1.6530992], dtype=float32), -0.35780194]. 
=============================================
[2019-03-27 00:09:55,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5833512e-15 1.0000000e+00 3.0566514e-17 3.8204400e-09 3.1591264e-22], sum to 1.0000
[2019-03-27 00:09:55,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8569
[2019-03-27 00:09:55,472] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 70.0, 1.0, 2.0, 0.4826137343085554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 676107.6275046763, 676107.627504677, 181140.1339086928], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4651200.0000, 
sim time next is 4651800.0000, 
raw observation next is [27.33333333333334, 74.0, 1.0, 2.0, 0.4745648029814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669080.7530330656, 669080.753033065, 180474.7162654271], 
processed observation next is [1.0, 0.8695652173913043, 0.4944707740916275, 0.74, 1.0, 1.0, 0.36694554576081423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1858557647314071, 0.18585576473140694, 0.26936524815735385], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.46136963], dtype=float32), 0.071223386]. 
=============================================
[2019-03-27 00:09:57,524] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 00:09:57,525] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:09:57,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:09:57,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:09:57,528] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:09:57,529] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:09:57,531] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:09:57,530] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:09:57,532] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:09:57,533] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:09:57,535] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:09:57,554] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-27 00:09:57,555] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-27 00:09:57,604] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-27 00:09:57,604] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-27 00:09:57,604] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-27 00:10:06,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:10:06,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.06666666666667, 58.0, 1.0, 2.0, 0.6372104790311179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044985.718161123, 1044985.718161123, 223828.8376568352]
[2019-03-27 00:10:06,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:10:06,808] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.10939856e-13 1.00000000e+00 3.26110914e-16 4.77826978e-09
 1.63864802e-19], sampled 0.708885652482605
[2019-03-27 00:10:19,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:10:19,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.4, 55.0, 1.0, 2.0, 0.2830434922381998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 459575.704913065, 459575.7049130656, 164213.2884589604]
[2019-03-27 00:10:19,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:10:19,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0632688e-15 1.0000000e+00 2.3249065e-18 3.4073640e-11 7.2837479e-22], sampled 0.12618934997999032
[2019-03-27 00:10:22,569] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:10:22,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.63113588666667, 97.95883288666667, 1.0, 2.0, 0.5028161005956234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702608.6804139629, 702608.6804139629, 184037.8713019594]
[2019-03-27 00:10:22,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:10:22,573] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9637327e-14 1.0000000e+00 6.1951791e-17 1.3973436e-09 2.0578738e-20], sampled 0.798389647077144
[2019-03-27 00:10:28,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:10:28,954] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.25477994333333, 83.92057136666668, 1.0, 2.0, 0.5942799977919059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830465.4648290299, 830465.4648290306, 199684.8850238808]
[2019-03-27 00:10:28,955] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:10:28,958] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7412448e-14 1.0000000e+00 5.1322236e-17 1.1853809e-09 1.6707754e-20], sampled 0.6576791139985942
[2019-03-27 00:11:00,106] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:11:00,108] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.93801424, 65.287697485, 1.0, 2.0, 0.8394145719055676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1173213.41140246, 1173213.41140246, 253820.6518378031]
[2019-03-27 00:11:00,109] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:11:00,111] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3946221e-13 1.0000000e+00 2.4465983e-16 1.6912415e-08 3.7902831e-20], sampled 0.10347762961509488
[2019-03-27 00:11:04,393] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:11:04,394] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.43333333333333, 46.83333333333334, 1.0, 2.0, 0.5737929192189173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801825.3739319649, 801825.3739319649, 195965.2895210493]
[2019-03-27 00:11:04,396] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:11:04,399] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3196535e-14 1.0000000e+00 7.3591189e-17 1.4646528e-08 3.9970171e-21], sampled 0.06718932563586033
[2019-03-27 00:11:37,270] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03249964]
[2019-03-27 00:11:37,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.65990710833334, 66.44169554166666, 1.0, 2.0, 0.3489938770403301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544934.4787179483, 544934.4787179483, 170410.9729823025]
[2019-03-27 00:11:37,273] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:11:37,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.5665799e-16 1.0000000e+00 1.3963913e-18 5.9615125e-11 2.5728624e-22], sampled 0.33615070599485875
[2019-03-27 00:11:51,243] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4308 2779149198.4617 933.0000
[2019-03-27 00:11:51,504] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 00:11:51,606] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9868 2842589762.7160 1131.0000
[2019-03-27 00:11:51,623] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3824 3007846956.3309 1766.0000
[2019-03-27 00:11:51,626] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4154 3164026557.5509 1778.0000
[2019-03-27 00:11:52,642] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 125000, evaluation results [125000.0, 7883.415430077599, 3164026557.5508585, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8661.430759135581, 2779149198.4616733, 933.0, 7997.382411637957, 3007846956.330909, 1766.0, 8495.98682279325, 2842589762.7159967, 1131.0]
[2019-03-27 00:11:53,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6209412e-08 9.6453035e-01 2.7598982e-11 3.5469674e-02 3.3255012e-14], sum to 1.0000
[2019-03-27 00:11:53,903] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4078
[2019-03-27 00:11:53,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2092206.473711751 W.
[2019-03-27 00:11:53,916] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 67.33333333333334, 1.0, 2.0, 0.7481527486007662, 1.0, 1.0, 0.7481527486007662, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2092206.473711751, 2092206.47371175, 395420.3116643218], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4707600.0000, 
sim time next is 4708200.0000, 
raw observation next is [31.0, 66.66666666666666, 1.0, 2.0, 0.8693116752113511, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.99442590228311, 6.9112, 168.9124612056698, 2112082.212938582, 2053039.06657763, 426122.9659108023], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.6666666666666665, 1.0, 1.0, 0.8425441870016278, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00832259022831101, 0.0, 0.8294375129791738, 0.5866895035940506, 0.5702886296048972, 0.6360044267325408], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48036137], dtype=float32), 0.36956182]. 
=============================================
[2019-03-27 00:11:55,652] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.3172611e-14 9.9999869e-01 9.3943279e-17 1.3116056e-06 3.2834405e-21], sum to 1.0000
[2019-03-27 00:11:55,659] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3227
[2019-03-27 00:11:55,663] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5176365907795337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 186406.7668423664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4730400.0000, 
sim time next is 4731000.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5197230506336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726241.6359888734, 726241.635988874, 186744.9725685951], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.4213530730525808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20173378777468706, 0.20173378777468723, 0.27872383965461955], 
reward next is 0.7213, 
noisyNet noise sample is [array([1.0797331], dtype=float32), 0.50553256]. 
=============================================
[2019-03-27 00:11:55,680] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[56.877274]
 [56.301437]
 [56.37859 ]
 [55.277798]
 [52.99472 ]], R is [[56.7996521 ]
 [56.95343781]
 [57.10607147]
 [57.25777817]
 [57.40953064]].
[2019-03-27 00:11:57,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4507475e-13 1.0000000e+00 7.3557262e-16 1.6315675e-09 2.6216280e-21], sum to 1.0000
[2019-03-27 00:11:57,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9088
[2019-03-27 00:11:57,788] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.83333333333334, 1.0, 2.0, 0.5919076776101134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827149.0195213107, 827149.0195213107, 199241.0098672827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5713417938296217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798398.853453197, 798398.853453197, 195523.5098540154], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.48354432991520685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22177745929255474, 0.22177745929255474, 0.29182613411047076], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.08950454], dtype=float32), 0.4226114]. 
=============================================
[2019-03-27 00:11:58,788] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127734: loss -182.7245
[2019-03-27 00:11:58,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127735: learning rate 0.0001
[2019-03-27 00:11:58,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6533729e-13 9.9999905e-01 2.1567106e-15 9.7728866e-07 5.5045070e-19], sum to 1.0000
[2019-03-27 00:11:58,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7943
[2019-03-27 00:11:58,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2509740.130025761 W.
[2019-03-27 00:11:58,885] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.75, 64.0, 1.0, 2.0, 0.5982011772122784, 1.0, 2.0, 0.5982011772122784, 1.0, 1.0, 1.03, 6.92117975514876, 6.9112, 170.5573041426782, 2509740.130025761, 2502591.22546222, 487326.2150559167], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4800600.0000, 
sim time next is 4801200.0000, 
raw observation next is [31.66666666666667, 64.33333333333333, 1.0, 2.0, 0.5908286366682558, 1.0, 2.0, 0.5908286366682558, 1.0, 2.0, 1.02607395187688, 6.911200000000001, 6.9112, 170.5573041426782, 2478778.114770778, 2478778.114770778, 483635.7291446989], 
processed observation next is [1.0, 0.5652173913043478, 0.6998420221169038, 0.6433333333333333, 1.0, 1.0, 0.5070224538171756, 1.0, 1.0, 0.5070224538171756, 1.0, 1.0, 1.0317975022888781, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6885494763252161, 0.6885494763252161, 0.7218443718577595], 
reward next is 0.2782, 
noisyNet noise sample is [array([-0.30763173], dtype=float32), -0.62048024]. 
=============================================
[2019-03-27 00:11:59,152] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127895: loss -158.7118
[2019-03-27 00:11:59,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127895: learning rate 0.0001
[2019-03-27 00:11:59,178] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127904: loss -194.2710
[2019-03-27 00:11:59,180] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127904: loss -251.1916
[2019-03-27 00:11:59,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127904: learning rate 0.0001
[2019-03-27 00:11:59,184] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127904: learning rate 0.0001
[2019-03-27 00:11:59,278] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127949: loss -226.9604
[2019-03-27 00:11:59,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127950: learning rate 0.0001
[2019-03-27 00:11:59,295] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127957: loss -220.6872
[2019-03-27 00:11:59,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127958: learning rate 0.0001
[2019-03-27 00:11:59,323] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127970: loss -230.9597
[2019-03-27 00:11:59,325] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127970: loss -199.6443
[2019-03-27 00:11:59,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127970: learning rate 0.0001
[2019-03-27 00:11:59,326] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127970: loss -147.4604
[2019-03-27 00:11:59,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127970: learning rate 0.0001
[2019-03-27 00:11:59,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127970: learning rate 0.0001
[2019-03-27 00:11:59,342] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127976: loss -108.4027
[2019-03-27 00:11:59,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127977: learning rate 0.0001
[2019-03-27 00:11:59,361] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127983: loss -220.1640
[2019-03-27 00:11:59,364] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127983: learning rate 0.0001
[2019-03-27 00:11:59,372] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127985: loss -223.4667
[2019-03-27 00:11:59,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127986: learning rate 0.0001
[2019-03-27 00:11:59,658] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128114: loss -149.9339
[2019-03-27 00:11:59,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128114: learning rate 0.0001
[2019-03-27 00:11:59,709] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128137: loss -223.4565
[2019-03-27 00:11:59,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128139: learning rate 0.0001
[2019-03-27 00:11:59,771] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128159: loss -180.0297
[2019-03-27 00:11:59,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128159: learning rate 0.0001
[2019-03-27 00:11:59,829] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128191: loss -254.6424
[2019-03-27 00:11:59,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128191: learning rate 0.0001
[2019-03-27 00:12:02,951] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6949938e-12 1.0000000e+00 5.6692559e-15 1.0889583e-09 7.6545128e-19], sum to 1.0000
[2019-03-27 00:12:02,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3377
[2019-03-27 00:12:02,963] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.83333333333333, 1.0, 2.0, 0.6571396177117861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918345.4412641564, 918345.441264157, 211864.4852656514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4860600.0000, 
sim time next is 4861200.0000, 
raw observation next is [27.0, 85.66666666666667, 1.0, 2.0, 0.6381011400471721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 891728.2070959712, 891728.2070959718, 208050.6939751662], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8566666666666667, 1.0, 1.0, 0.5639772771652676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2477022797488809, 0.24770227974888107, 0.31052342384353165], 
reward next is 0.6895, 
noisyNet noise sample is [array([-1.1545032], dtype=float32), 0.7438986]. 
=============================================
[2019-03-27 00:12:03,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2557779e-13 1.0000000e+00 1.9732761e-15 7.9841520e-09 7.4841017e-19], sum to 1.0000
[2019-03-27 00:12:03,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1810
[2019-03-27 00:12:03,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1899543.920827473 W.
[2019-03-27 00:12:03,675] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.6793195115101276, 1.0, 2.0, 0.6793195115101276, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1899543.920827473, 1899543.920827473, 365395.5993607887], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4875000.0000, 
sim time next is 4875600.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7305088715789844, 1.0, 2.0, 0.7305088715789844, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2042818.389482042, 2042818.389482042, 387456.8148903496], 
processed observation next is [1.0, 0.43478260869565216, 0.6366508688783573, 0.6866666666666668, 1.0, 1.0, 0.6753118934686558, 1.0, 1.0, 0.6753118934686558, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5674495526339006, 0.5674495526339006, 0.578293753567686], 
reward next is 0.4217, 
noisyNet noise sample is [array([0.0152954], dtype=float32), -0.5408538]. 
=============================================
[2019-03-27 00:12:06,955] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.6418614e-19 1.0000000e+00 9.0207245e-21 2.5030607e-16 1.7069630e-26], sum to 1.0000
[2019-03-27 00:12:06,965] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0350
[2019-03-27 00:12:06,973] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5016176043346112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700933.41246658, 700933.4124665793, 183849.7965545334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4932000.0000, 
sim time next is 4932600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.9555799819787751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9373764001279, 1335674.782295708, 1335674.782295708, 285685.7156987255], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9464819059985242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8295598579901884, 0.3710207728599189, 0.3710207728599189, 0.42639659059511265], 
reward next is 0.5736, 
noisyNet noise sample is [array([-0.11503038], dtype=float32), 1.1320664]. 
=============================================
[2019-03-27 00:12:07,182] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8120245e-17 1.0000000e+00 2.4333867e-19 7.7941681e-14 3.3641522e-24], sum to 1.0000
[2019-03-27 00:12:07,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2405
[2019-03-27 00:12:07,195] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5048256229970798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705417.6133108041, 705417.6133108041, 184355.5239324152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4930800.0000, 
sim time next is 4931400.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5033067450059844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703294.5079751044, 703294.5079751044, 184115.6964239863], 
processed observation next is [1.0, 0.043478260869565216, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.40157439157347513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1953595855486401, 0.1953595855486401, 0.27479954690147207], 
reward next is 0.7252, 
noisyNet noise sample is [array([1.4416902], dtype=float32), -0.22837931]. 
=============================================
[2019-03-27 00:12:10,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3652944e-14 1.0000000e+00 9.1600036e-17 4.6418083e-09 2.3327930e-20], sum to 1.0000
[2019-03-27 00:12:10,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1149
[2019-03-27 00:12:10,538] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5175459498903098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723198.4008884656, 723198.4008884649, 186391.9799378255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.5206798587619019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727579.1019253226, 727579.1019253231, 186900.4308882074], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7133333333333334, 1.0, 1.0, 0.4225058539300023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20210530609036736, 0.20210530609036753, 0.2789558669973245], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.11661073], dtype=float32), 0.38280156]. 
=============================================
[2019-03-27 00:12:12,668] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.60796073e-20 1.00000000e+00 1.65134099e-21 1.05555744e-16
 2.38571284e-25], sum to 1.0000
[2019-03-27 00:12:12,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0199
[2019-03-27 00:12:12,686] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 89.0, 1.0, 2.0, 0.4946341087728147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691171.8773856262, 691171.8773856262, 182759.5327117348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5022600.0000, 
sim time next is 5023200.0000, 
raw observation next is [25.66666666666667, 89.0, 1.0, 2.0, 0.4911405031623354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686288.5466313515, 686288.5466313515, 182219.3397804183], 
processed observation next is [0.0, 0.13043478260869565, 0.4154818325434442, 0.89, 1.0, 1.0, 0.3869162688702836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19063570739759764, 0.19063570739759764, 0.2719691638513706], 
reward next is 0.7280, 
noisyNet noise sample is [array([-1.3479587], dtype=float32), 1.1741564]. 
=============================================
[2019-03-27 00:12:12,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3343323e-19 1.0000000e+00 8.1333380e-20 1.3988993e-15 4.0762959e-24], sum to 1.0000
[2019-03-27 00:12:12,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0528
[2019-03-27 00:12:12,843] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.4839063894074186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676176.838331621, 676176.8383316203, 181113.0564080392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5028000.0000, 
sim time next is 5028600.0000, 
raw observation next is [25.83333333333334, 89.0, 1.0, 2.0, 0.4888727929782466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683118.7735822961, 683118.7735822955, 181871.2613208176], 
processed observation next is [0.0, 0.17391304347826086, 0.42338072669826254, 0.89, 1.0, 1.0, 0.38418408792559827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18975521488397115, 0.18975521488397099, 0.2714496437624143], 
reward next is 0.7286, 
noisyNet noise sample is [array([-0.04784163], dtype=float32), 1.5475082]. 
=============================================
[2019-03-27 00:12:13,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.19882166e-23 1.00000000e+00 1.00664016e-23 3.45636359e-19
 1.27597433e-29], sum to 1.0000
[2019-03-27 00:12:13,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3930
[2019-03-27 00:12:13,658] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5180843330622806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723950.9727686801, 723950.9727686801, 186478.4444092323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5052600.0000, 
sim time next is 5053200.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5183199466982512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724280.3223779524, 724280.3223779524, 186516.61634275], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.4196625863834352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20118897843832012, 0.20118897843832012, 0.27838300946679106], 
reward next is 0.7216, 
noisyNet noise sample is [array([0.74288076], dtype=float32), -1.1344446]. 
=============================================
[2019-03-27 00:12:14,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9874597e-21 1.0000000e+00 5.9255484e-24 3.5680534e-19 1.0311981e-27], sum to 1.0000
[2019-03-27 00:12:14,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6902
[2019-03-27 00:12:14,216] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5177056254068089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723421.6011666113, 723421.6011666113, 186417.1241739583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.5227724539329466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730504.2237892565, 730504.2237892571, 187241.4677359061], 
processed observation next is [0.0, 0.5217391304347826, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4250270529312609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20291783994146015, 0.20291783994146032, 0.2794648772177703], 
reward next is 0.7205, 
noisyNet noise sample is [array([2.0401614], dtype=float32), 1.5889387]. 
=============================================
[2019-03-27 00:12:14,226] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.64134 ]
 [76.51841 ]
 [76.453575]
 [76.38876 ]
 [76.32603 ]], R is [[76.6921463 ]
 [76.64698792]
 [76.60218048]
 [76.5577774 ]
 [76.51387024]].
[2019-03-27 00:12:16,378] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135625: loss 10.2035
[2019-03-27 00:12:16,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135625: learning rate 0.0001
[2019-03-27 00:12:16,907] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135863: loss 7.4722
[2019-03-27 00:12:16,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135863: learning rate 0.0001
[2019-03-27 00:12:16,933] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135873: loss 7.5535
[2019-03-27 00:12:16,936] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135873: learning rate 0.0001
[2019-03-27 00:12:16,980] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135892: loss 7.4829
[2019-03-27 00:12:16,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135892: learning rate 0.0001
[2019-03-27 00:12:17,065] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135934: loss 6.9591
[2019-03-27 00:12:17,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135935: learning rate 0.0001
[2019-03-27 00:12:17,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135952: loss 6.5553
[2019-03-27 00:12:17,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135953: learning rate 0.0001
[2019-03-27 00:12:17,191] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135987: loss 6.3219
[2019-03-27 00:12:17,192] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135988: loss 6.5968
[2019-03-27 00:12:17,193] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135989: learning rate 0.0001
[2019-03-27 00:12:17,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135990: learning rate 0.0001
[2019-03-27 00:12:17,213] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135999: loss 6.0971
[2019-03-27 00:12:17,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 136000: learning rate 0.0001
[2019-03-27 00:12:17,246] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136012: loss 6.3069
[2019-03-27 00:12:17,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136012: learning rate 0.0001
[2019-03-27 00:12:17,275] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136025: loss 5.7971
[2019-03-27 00:12:17,277] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136025: loss 6.3746
[2019-03-27 00:12:17,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136025: learning rate 0.0001
[2019-03-27 00:12:17,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136026: learning rate 0.0001
[2019-03-27 00:12:17,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136116: loss 5.8621
[2019-03-27 00:12:17,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136116: learning rate 0.0001
[2019-03-27 00:12:17,588] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136163: loss 5.8997
[2019-03-27 00:12:17,594] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136167: learning rate 0.0001
[2019-03-27 00:12:17,623] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136183: loss 6.2025
[2019-03-27 00:12:17,629] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136183: learning rate 0.0001
[2019-03-27 00:12:17,665] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136198: loss 5.6874
[2019-03-27 00:12:17,669] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136198: learning rate 0.0001
[2019-03-27 00:12:19,385] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7654833e-22 1.0000000e+00 1.5559057e-24 7.0276295e-20 8.1029518e-29], sum to 1.0000
[2019-03-27 00:12:19,395] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4086
[2019-03-27 00:12:19,401] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5682425839325336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794066.3670615365, 794066.3670615365, 194979.8072214978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5140800.0000, 
sim time next is 5141400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.5779105018786459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807581.5226610766, 807581.5226610772, 196702.8963192595], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.4914584359983685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22432820073918794, 0.2243282007391881, 0.2935864124168052], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.06043178], dtype=float32), -0.6931102]. 
=============================================
[2019-03-27 00:12:21,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1738382e-24 1.0000000e+00 5.1765188e-26 1.6638792e-22 2.9332913e-32], sum to 1.0000
[2019-03-27 00:12:21,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2604
[2019-03-27 00:12:21,368] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5234427971094541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731441.26076313, 731441.2607631307, 187350.4849528408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5167800.0000, 
sim time next is 5168400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5227833462935365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730519.4496325778, 730519.4496325784, 187242.60886876], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.42504017625727286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20292206934238274, 0.20292206934238288, 0.2794665804011343], 
reward next is 0.7205, 
noisyNet noise sample is [array([-1.8010228], dtype=float32), -0.22756885]. 
=============================================
[2019-03-27 00:12:34,346] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143670: loss 38.0805
[2019-03-27 00:12:34,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143671: learning rate 0.0001
[2019-03-27 00:12:34,750] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143847: loss -21.6839
[2019-03-27 00:12:34,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143847: learning rate 0.0001
[2019-03-27 00:12:34,803] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 143871: loss 37.0024
[2019-03-27 00:12:34,804] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 143871: learning rate 0.0001
[2019-03-27 00:12:34,937] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143929: loss 23.3266
[2019-03-27 00:12:34,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143930: learning rate 0.0001
[2019-03-27 00:12:35,005] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143960: loss 17.2800
[2019-03-27 00:12:35,006] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143961: loss 26.0771
[2019-03-27 00:12:35,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143961: learning rate 0.0001
[2019-03-27 00:12:35,012] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143961: learning rate 0.0001
[2019-03-27 00:12:35,051] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143981: loss -35.2450
[2019-03-27 00:12:35,053] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143981: loss 42.9240
[2019-03-27 00:12:35,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143981: learning rate 0.0001
[2019-03-27 00:12:35,055] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143981: learning rate 0.0001
[2019-03-27 00:12:35,071] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143990: loss 3.2013
[2019-03-27 00:12:35,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143993: learning rate 0.0001
[2019-03-27 00:12:35,107] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144004: loss 8.0165
[2019-03-27 00:12:35,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144004: learning rate 0.0001
[2019-03-27 00:12:35,134] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144015: loss -36.4146
[2019-03-27 00:12:35,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144015: learning rate 0.0001
[2019-03-27 00:12:35,178] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144034: loss 50.7349
[2019-03-27 00:12:35,180] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144034: learning rate 0.0001
[2019-03-27 00:12:35,338] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144103: loss 61.2588
[2019-03-27 00:12:35,339] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144104: learning rate 0.0001
[2019-03-27 00:12:35,407] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144136: loss 39.0650
[2019-03-27 00:12:35,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144137: learning rate 0.0001
[2019-03-27 00:12:35,441] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144153: loss 20.2007
[2019-03-27 00:12:35,443] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144154: learning rate 0.0001
[2019-03-27 00:12:35,460] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144161: loss -40.3944
[2019-03-27 00:12:35,462] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144162: learning rate 0.0001
[2019-03-27 00:12:38,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1315069e-14 1.0000000e+00 3.5122607e-15 3.4709215e-13 2.1757391e-19], sum to 1.0000
[2019-03-27 00:12:38,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1298
[2019-03-27 00:12:38,727] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.15, 89.0, 1.0, 2.0, 0.7952089889672792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111396.847317342, 1111396.847317342, 242741.3637489601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5463000.0000, 
sim time next is 5463600.0000, 
raw observation next is [28.33333333333334, 88.0, 1.0, 2.0, 0.7928086402714412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1108040.331113781, 1108040.331113781, 242156.5278565023], 
processed observation next is [1.0, 0.21739130434782608, 0.5418641390205374, 0.88, 1.0, 1.0, 0.7503718557487242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30778898086493917, 0.30778898086493917, 0.3614276535171676], 
reward next is 0.6386, 
noisyNet noise sample is [array([0.29286698], dtype=float32), 0.8635633]. 
=============================================
[2019-03-27 00:12:41,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3919242e-14 1.0000000e+00 5.0149067e-16 7.9528981e-14 3.2553370e-19], sum to 1.0000
[2019-03-27 00:12:41,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3468
[2019-03-27 00:12:41,201] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.2, 59.0, 1.0, 2.0, 0.5421744223301029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757625.53488525, 757625.5348852506, 190469.7265458136], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5508000.0000, 
sim time next is 5508600.0000, 
raw observation next is [32.93333333333334, 60.5, 1.0, 2.0, 0.5506815927695967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769517.6196698534, 769517.6196698528, 191919.2281587604], 
processed observation next is [1.0, 0.782608695652174, 0.7598736176935231, 0.605, 1.0, 1.0, 0.45865252140915264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21375489435273703, 0.2137548943527369, 0.28644660919217974], 
reward next is 0.7136, 
noisyNet noise sample is [array([0.10557499], dtype=float32), -0.7211251]. 
=============================================
[2019-03-27 00:12:45,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0665794e-14 1.0000000e+00 1.3683084e-16 2.6907311e-11 6.4105852e-18], sum to 1.0000
[2019-03-27 00:12:45,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2186
[2019-03-27 00:12:45,197] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2292196.50290376 W.
[2019-03-27 00:12:45,200] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.9, 65.0, 1.0, 2.0, 0.5463967664481932, 1.0, 2.0, 0.5463967664481932, 1.0, 2.0, 0.9489104871486479, 6.911199999999999, 6.9112, 170.5573041426782, 2292196.50290376, 2292196.502903761, 448713.1385293565], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5567400.0000, 
sim time next is 5568000.0000, 
raw observation next is [32.1, 64.0, 1.0, 2.0, 0.8079692912912843, 1.0, 2.0, 0.8079692912912843, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2259652.577151449, 2259652.577151449, 423771.8115606554], 
processed observation next is [1.0, 0.43478260869565216, 0.7203791469194314, 0.64, 1.0, 1.0, 0.7686377003509449, 1.0, 1.0, 0.7686377003509449, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6276812714309581, 0.6276812714309581, 0.6324952411353066], 
reward next is 0.3675, 
noisyNet noise sample is [array([-0.63268125], dtype=float32), 0.63767946]. 
=============================================
[2019-03-27 00:12:45,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[39.846695]
 [38.64365 ]
 [37.696938]
 [35.91197 ]
 [34.366684]], R is [[40.69358444]
 [40.6169281 ]
 [40.5335083 ]
 [40.50053787]
 [40.40700531]].
[2019-03-27 00:12:48,496] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 00:12:48,502] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:12:48,504] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:12:48,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:12:48,505] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:12:48,505] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:12:48,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:12:48,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:12:48,508] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:12:48,508] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:12:48,509] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:12:48,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-27 00:12:48,528] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-27 00:12:48,564] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-27 00:12:48,565] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-27 00:12:48,592] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-27 00:12:52,296] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03259278]
[2019-03-27 00:12:52,298] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.68333333333333, 74.66666666666667, 1.0, 2.0, 0.2416961920936398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399988.3317455532, 399988.3317455532, 159989.6215637999]
[2019-03-27 00:12:52,300] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:12:52,302] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1633758e-21 1.0000000e+00 2.6104271e-24 2.5423360e-21 2.3025318e-26], sampled 0.8484457229345392
[2019-03-27 00:13:43,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03259278]
[2019-03-27 00:13:43,285] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.55, 82.0, 1.0, 2.0, 0.5590075789533165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781156.5577928477, 781156.5577928477, 193357.269734205]
[2019-03-27 00:13:43,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:13:43,288] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4578571e-24 1.0000000e+00 2.9421182e-27 1.3634676e-24 2.4721363e-30], sampled 0.32763899124154705
[2019-03-27 00:14:29,362] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03259278]
[2019-03-27 00:14:29,364] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.03333333333334, 45.0, 1.0, 2.0, 0.6487827433861749, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.938876808913984, 6.9112, 168.9127686107666, 1843035.941021825, 1823401.082057941, 380506.9341185349]
[2019-03-27 00:14:29,367] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:14:29,370] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.095185e-22 1.000000e+00 1.716878e-24 8.147645e-22 2.481901e-27], sampled 0.608425441811582
[2019-03-27 00:14:29,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1843035.941021825 W.
[2019-03-27 00:14:33,533] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03259278]
[2019-03-27 00:14:33,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.53333333333333, 73.66666666666666, 1.0, 2.0, 0.3868699793266774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583089.1592438482, 583089.1592438476, 173164.1113373199]
[2019-03-27 00:14:33,535] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:14:33,538] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0514634e-23 1.0000000e+00 6.0071345e-26 5.7964174e-23 2.4962373e-28], sampled 0.9796385520250886
[2019-03-27 00:14:38,674] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.03259278]
[2019-03-27 00:14:38,676] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.26106240833333, 96.11548840833333, 1.0, 2.0, 0.505834386358515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706827.6768244195, 706827.6768244195, 184513.0554092159]
[2019-03-27 00:14:38,677] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:14:38,678] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.1989653e-24 1.0000000e+00 6.1811963e-27 8.2577163e-24 2.2169145e-29], sampled 0.6779767430807563
[2019-03-27 00:14:42,344] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5588 3007666311.2491 1766.0000
[2019-03-27 00:14:42,447] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-27 00:14:42,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-03-27 00:14:42,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-27 00:14:42,569] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 00:14:43,585] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 150000, evaluation results [150000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8255.065342017213, 2927317329.746172, 1338.0, 8659.987755389953, 2779190307.3384666, 933.0, 7997.558800637214, 3007666311.249115, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 00:14:46,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151526: loss 0.1494
[2019-03-27 00:14:46,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151527: learning rate 0.0001
[2019-03-27 00:14:47,596] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 151822: loss 0.0372
[2019-03-27 00:14:47,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 151822: learning rate 0.0001
[2019-03-27 00:14:47,622] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151833: loss 0.0341
[2019-03-27 00:14:47,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151833: learning rate 0.0001
[2019-03-27 00:14:47,732] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151882: loss 0.0348
[2019-03-27 00:14:47,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151884: learning rate 0.0001
[2019-03-27 00:14:47,822] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151922: loss 0.0070
[2019-03-27 00:14:47,823] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151922: learning rate 0.0001
[2019-03-27 00:14:47,852] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151933: loss 0.0022
[2019-03-27 00:14:47,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151934: learning rate 0.0001
[2019-03-27 00:14:47,904] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151957: loss 0.0005
[2019-03-27 00:14:47,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151958: learning rate 0.0001
[2019-03-27 00:14:47,945] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151978: loss 0.0008
[2019-03-27 00:14:47,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151978: learning rate 0.0001
[2019-03-27 00:14:48,054] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 152023: loss 0.0122
[2019-03-27 00:14:48,056] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 152025: learning rate 0.0001
[2019-03-27 00:14:48,062] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152028: loss 0.0090
[2019-03-27 00:14:48,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152028: learning rate 0.0001
[2019-03-27 00:14:48,138] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152064: loss 0.0123
[2019-03-27 00:14:48,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152064: learning rate 0.0001
[2019-03-27 00:14:48,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152081: loss 0.0066
[2019-03-27 00:14:48,181] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152081: loss 0.0191
[2019-03-27 00:14:48,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152081: learning rate 0.0001
[2019-03-27 00:14:48,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152081: learning rate 0.0001
[2019-03-27 00:14:48,432] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152194: loss 0.0016
[2019-03-27 00:14:48,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152194: learning rate 0.0001
[2019-03-27 00:14:48,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152233: loss 0.0125
[2019-03-27 00:14:48,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152234: learning rate 0.0001
[2019-03-27 00:14:48,546] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152243: loss 0.0126
[2019-03-27 00:14:48,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152243: learning rate 0.0001
[2019-03-27 00:14:49,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8128450e-24 1.0000000e+00 3.1120766e-26 3.9378532e-23 4.2348521e-29], sum to 1.0000
[2019-03-27 00:14:49,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7862
[2019-03-27 00:14:49,868] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 79.16666666666667, 1.0, 2.0, 0.5203001505872328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727048.329947903, 727048.3299479038, 186837.6735789017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5728200.0000, 
sim time next is 5728800.0000, 
raw observation next is [28.13333333333333, 78.33333333333334, 1.0, 2.0, 0.5211864883841889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728287.2904269149, 728287.2904269149, 186982.0259376628], 
processed observation next is [0.0, 0.30434782608695654, 0.532385466034755, 0.7833333333333334, 1.0, 1.0, 0.42311625106528783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20230202511858747, 0.20230202511858747, 0.2790776506532281], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.2494768], dtype=float32), -0.21927176]. 
=============================================
[2019-03-27 00:14:59,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9417118e-14 1.0000000e+00 1.2555250e-16 1.8654859e-08 1.0492375e-18], sum to 1.0000
[2019-03-27 00:14:59,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5707
[2019-03-27 00:14:59,515] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 90.16666666666667, 1.0, 2.0, 0.6862671686628288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959069.2553212187, 959069.2553212187, 217909.5682786445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5897400.0000, 
sim time next is 5898000.0000, 
raw observation next is [27.2, 89.33333333333334, 1.0, 2.0, 0.7328978633174595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024267.817487945, 1024267.817487945, 228107.2523535618], 
processed observation next is [1.0, 0.2608695652173913, 0.4881516587677725, 0.8933333333333334, 1.0, 1.0, 0.6781901967680235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2845188381910958, 0.2845188381910958, 0.34045858560233105], 
reward next is 0.6595, 
noisyNet noise sample is [array([0.80819035], dtype=float32), -2.05677]. 
=============================================
[2019-03-27 00:14:59,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.379097]
 [52.437084]
 [52.49909 ]
 [52.641895]
 [52.71266 ]], R is [[52.21001434]
 [52.36267853]
 [52.51576996]
 [52.66123581]
 [52.80798721]].
[2019-03-27 00:15:04,877] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159561: loss -62.8326
[2019-03-27 00:15:04,885] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159561: learning rate 0.0001
[2019-03-27 00:15:05,493] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 159834: loss 2.9366
[2019-03-27 00:15:05,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 159834: learning rate 0.0001
[2019-03-27 00:15:05,573] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159871: loss -69.7760
[2019-03-27 00:15:05,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159871: learning rate 0.0001
[2019-03-27 00:15:05,626] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159895: loss -97.2516
[2019-03-27 00:15:05,631] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159896: learning rate 0.0001
[2019-03-27 00:15:05,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159897: loss 23.5906
[2019-03-27 00:15:05,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159899: learning rate 0.0001
[2019-03-27 00:15:05,788] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159967: loss -66.4963
[2019-03-27 00:15:05,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159967: learning rate 0.0001
[2019-03-27 00:15:05,831] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159983: loss -121.7641
[2019-03-27 00:15:05,833] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159983: loss -38.0100
[2019-03-27 00:15:05,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159983: learning rate 0.0001
[2019-03-27 00:15:05,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159983: learning rate 0.0001
[2019-03-27 00:15:05,856] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159992: loss 76.9745
[2019-03-27 00:15:05,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159992: learning rate 0.0001
[2019-03-27 00:15:05,953] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 160035: loss 76.9393
[2019-03-27 00:15:05,956] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 160035: learning rate 0.0001
[2019-03-27 00:15:05,998] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160055: loss 13.5588
[2019-03-27 00:15:06,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160056: learning rate 0.0001
[2019-03-27 00:15:06,046] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160074: loss -11.6956
[2019-03-27 00:15:06,048] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160074: learning rate 0.0001
[2019-03-27 00:15:06,086] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160087: loss -52.2288
[2019-03-27 00:15:06,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160087: learning rate 0.0001
[2019-03-27 00:15:06,200] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160141: loss -69.8074
[2019-03-27 00:15:06,205] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160141: learning rate 0.0001
[2019-03-27 00:15:06,270] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160168: loss -62.4238
[2019-03-27 00:15:06,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160168: learning rate 0.0001
[2019-03-27 00:15:06,420] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160236: loss 18.7170
[2019-03-27 00:15:06,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160237: learning rate 0.0001
[2019-03-27 00:15:07,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9731393e-19 1.0000000e+00 5.9127712e-23 3.0782654e-16 1.6432636e-24], sum to 1.0000
[2019-03-27 00:15:07,939] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5588
[2019-03-27 00:15:07,947] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 84.66666666666667, 1.0, 2.0, 0.5383256757850722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752245.455993728, 752245.4559937286, 189819.0953174918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6034800.0000, 
sim time next is 6035400.0000, 
raw observation next is [27.75, 85.0, 1.0, 2.0, 0.5381012223055038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751931.6982087173, 751931.6982087173, 189781.4013220653], 
processed observation next is [1.0, 0.8695652173913043, 0.514218009478673, 0.85, 1.0, 1.0, 0.443495448560848, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20886991616908812, 0.20886991616908812, 0.28325582286875417], 
reward next is 0.7167, 
noisyNet noise sample is [array([-1.1373873], dtype=float32), -0.36111376]. 
=============================================
[2019-03-27 00:15:13,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7207384e-24 1.0000000e+00 5.9199617e-26 4.1549880e-21 1.0602808e-28], sum to 1.0000
[2019-03-27 00:15:13,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9242
[2019-03-27 00:15:13,503] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5269159403329983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736296.1956722644, 736296.1956722644, 187920.8085717417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6123600.0000, 
sim time next is 6124200.0000, 
raw observation next is [27.21666666666667, 86.16666666666667, 1.0, 2.0, 0.5281266562988742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737988.600777555, 737988.6007775556, 188120.4078167687], 
processed observation next is [1.0, 0.9130434782608695, 0.48894154818325447, 0.8616666666666667, 1.0, 1.0, 0.43147789915527013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20499683354932083, 0.204996833549321, 0.2807767280847294], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.6457134], dtype=float32), -0.4456829]. 
=============================================
[2019-03-27 00:15:19,404] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4372969e-19 1.0000000e+00 2.2024913e-22 5.2788215e-17 2.9485478e-25], sum to 1.0000
[2019-03-27 00:15:19,416] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0574
[2019-03-27 00:15:19,424] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.0, 1.0, 2.0, 0.5247655867243359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733290.3202400721, 733290.3202400721, 187567.5961967095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6232800.0000, 
sim time next is 6233400.0000, 
raw observation next is [26.5, 91.0, 1.0, 2.0, 0.5248070539752634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733348.285226549, 733348.2852265484, 187574.3951885303], 
processed observation next is [0.0, 0.13043478260869565, 0.4549763033175356, 0.91, 1.0, 1.0, 0.42747837828344987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20370785700737473, 0.20370785700737457, 0.27996178386347803], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.39377695], dtype=float32), 0.77115846]. 
=============================================
[2019-03-27 00:15:22,782] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167555: loss 0.1113
[2019-03-27 00:15:22,786] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167556: learning rate 0.0001
[2019-03-27 00:15:23,184] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167735: loss 0.0514
[2019-03-27 00:15:23,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167736: learning rate 0.0001
[2019-03-27 00:15:23,308] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167788: loss 0.1215
[2019-03-27 00:15:23,314] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167790: learning rate 0.0001
[2019-03-27 00:15:23,434] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167850: loss 0.1131
[2019-03-27 00:15:23,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167850: learning rate 0.0001
[2019-03-27 00:15:23,452] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167857: loss 0.1018
[2019-03-27 00:15:23,453] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167857: learning rate 0.0001
[2019-03-27 00:15:23,609] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167928: loss 0.0199
[2019-03-27 00:15:23,613] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167928: learning rate 0.0001
[2019-03-27 00:15:23,697] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167966: loss 0.0020
[2019-03-27 00:15:23,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167966: learning rate 0.0001
[2019-03-27 00:15:23,701] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167966: loss 0.0007
[2019-03-27 00:15:23,703] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167966: learning rate 0.0001
[2019-03-27 00:15:23,757] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167991: loss 0.0021
[2019-03-27 00:15:23,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167993: learning rate 0.0001
[2019-03-27 00:15:23,814] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168017: loss 0.0007
[2019-03-27 00:15:23,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168017: learning rate 0.0001
[2019-03-27 00:15:24,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168113: loss 0.0025
[2019-03-27 00:15:24,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168114: learning rate 0.0001
[2019-03-27 00:15:24,075] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168136: loss 0.0010
[2019-03-27 00:15:24,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168137: learning rate 0.0001
[2019-03-27 00:15:24,107] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168150: loss 0.0013
[2019-03-27 00:15:24,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168151: learning rate 0.0001
[2019-03-27 00:15:24,226] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168204: loss 0.0018
[2019-03-27 00:15:24,230] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168205: learning rate 0.0001
[2019-03-27 00:15:24,301] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168234: loss 0.0016
[2019-03-27 00:15:24,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168235: learning rate 0.0001
[2019-03-27 00:15:24,493] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168321: loss 0.0013
[2019-03-27 00:15:24,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168322: learning rate 0.0001
[2019-03-27 00:15:25,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4527146e-19 1.0000000e+00 6.6584398e-23 3.0333967e-17 2.1036019e-25], sum to 1.0000
[2019-03-27 00:15:25,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4301
[2019-03-27 00:15:25,114] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.33333333333334, 1.0, 2.0, 0.5240657996925021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732312.1235873987, 732312.1235873987, 187452.4993230246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6327600.0000, 
sim time next is 6328200.0000, 
raw observation next is [27.1, 85.66666666666667, 1.0, 2.0, 0.5237910485583507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731928.0632159087, 731928.0632159094, 187407.5278585231], 
processed observation next is [0.0, 0.21739130434782608, 0.4834123222748816, 0.8566666666666667, 1.0, 1.0, 0.42625427537150684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.203313350893308, 0.20331335089330818, 0.2797127281470494], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.6654411], dtype=float32), 0.1168796]. 
=============================================
[2019-03-27 00:15:25,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6372226e-20 1.0000000e+00 8.6619767e-23 4.7250484e-18 1.5116525e-24], sum to 1.0000
[2019-03-27 00:15:25,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4076
[2019-03-27 00:15:25,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.5348506041666883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747387.7525456378, 747387.7525456378, 189236.3696040671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6314400.0000, 
sim time next is 6315000.0000, 
raw observation next is [27.26666666666667, 86.16666666666667, 1.0, 2.0, 0.5340542116640158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746274.5013215193, 746274.5013215187, 189103.4894171029], 
processed observation next is [0.0, 0.08695652173913043, 0.4913112164297, 0.8616666666666667, 1.0, 1.0, 0.4386195321253202, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20729847258931092, 0.20729847258931075, 0.28224401405537747], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.5088489], dtype=float32), 1.5647277]. 
=============================================
[2019-03-27 00:15:25,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.46431 ]
 [71.4476  ]
 [71.59243 ]
 [71.76886 ]
 [71.910385]], R is [[71.6504364 ]
 [71.65148926]
 [71.65265656]
 [71.65398407]
 [71.65542603]].
[2019-03-27 00:15:26,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5758577e-22 1.0000000e+00 9.5334264e-27 1.2130340e-19 1.6385281e-28], sum to 1.0000
[2019-03-27 00:15:26,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1953
[2019-03-27 00:15:26,905] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 63.0, 1.0, 2.0, 0.529586121827304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740028.7252333068, 740028.7252333062, 188361.4660533321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6353400.0000, 
sim time next is 6354000.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5250703621992528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733716.3506362874, 733716.3506362874, 187617.2523374262], 
processed observation next is [0.0, 0.5652173913043478, 0.6682464454976303, 0.63, 1.0, 1.0, 0.4277956171075335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20381009739896871, 0.20381009739896871, 0.28002574975735256], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.27212968], dtype=float32), 1.2051538]. 
=============================================
[2019-03-27 00:15:26,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.08603]
 [82.98511]
 [82.87662]
 [82.75759]
 [82.64941]], R is [[83.15802002]
 [83.04530334]
 [82.93262482]
 [82.82000732]
 [82.70690918]].
[2019-03-27 00:15:29,284] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.3922474e-21 1.0000000e+00 3.4602599e-24 1.4476480e-18 1.2709005e-25], sum to 1.0000
[2019-03-27 00:15:29,293] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5462
[2019-03-27 00:15:29,301] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.43333333333333, 82.0, 1.0, 2.0, 0.5214930887779144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728715.8697678195, 728715.8697678202, 187031.657905314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6385200.0000, 
sim time next is 6385800.0000, 
raw observation next is [27.41666666666666, 82.0, 1.0, 2.0, 0.5203567118000646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727127.3935614872, 727127.3935614872, 186846.5989268792], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690361, 0.82, 1.0, 1.0, 0.4221165202410417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20197983154485755, 0.20197983154485755, 0.2788755207863869], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.429156], dtype=float32), -1.6060238]. 
=============================================
[2019-03-27 00:15:39,399] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 00:15:39,402] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:15:39,405] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:15:39,406] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:15:39,406] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:15:39,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:15:39,407] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:15:39,409] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:15:39,408] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:15:39,413] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:15:39,414] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:15:39,435] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-27 00:15:39,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-27 00:15:39,484] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-27 00:15:39,517] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-27 00:15:39,517] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-27 00:15:42,400] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:15:42,401] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.45, 41.0, 1.0, 2.0, 0.5189419386405054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860113.2020122558, 860113.2020122558, 199106.0793195296]
[2019-03-27 00:15:42,403] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:15:42,406] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5101118e-19 1.0000000e+00 5.0492876e-23 2.9650729e-17 2.6232846e-24], sampled 0.27820879403316645
[2019-03-27 00:16:05,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:06,000] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.86150212, 86.92422493000001, 1.0, 2.0, 0.4541396050048687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648997.7453867041, 648997.7453867041, 178576.4822267327]
[2019-03-27 00:16:06,000] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:16:06,005] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2742472e-19 1.0000000e+00 1.5189142e-22 2.0904204e-16 7.1593371e-24], sampled 0.45428985282250245
[2019-03-27 00:16:19,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:19,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.86666666666667, 81.33333333333334, 1.0, 2.0, 0.5702620593357758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796889.4554539786, 796889.4554539786, 195335.9291042338]
[2019-03-27 00:16:19,924] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:16:19,926] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8423728e-21 1.0000000e+00 2.8749760e-25 4.5916015e-19 9.3276601e-27], sampled 0.36157391071543954
[2019-03-27 00:16:25,684] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:25,685] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 94.0, 1.0, 2.0, 0.489418320808139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683881.303761665, 683881.303761665, 181954.5824861134]
[2019-03-27 00:16:25,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:16:25,688] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0253370e-21 1.0000000e+00 2.4773632e-25 1.2618658e-19 5.6512994e-27], sampled 0.36441436018751194
[2019-03-27 00:16:45,012] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:45,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.16666666666667, 41.00000000000001, 1.0, 2.0, 0.7033234107955039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 982916.6550079953, 982916.6550079953, 221567.3894262204]
[2019-03-27 00:16:45,015] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:16:45,017] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1063016e-19 1.0000000e+00 3.0758599e-23 3.1225436e-17 1.0821029e-24], sampled 0.7481286203841666
[2019-03-27 00:16:47,015] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:47,015] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.90694570333334, 68.77437166666667, 1.0, 2.0, 0.5017741486308611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701152.2312235505, 701152.2312235499, 183875.9998833519]
[2019-03-27 00:16:47,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:16:47,019] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2047779e-22 1.0000000e+00 2.8342592e-26 9.3288673e-20 3.5031919e-28], sampled 0.9067271934194209
[2019-03-27 00:16:54,554] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:54,555] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.61614595166667, 84.75397669166668, 1.0, 2.0, 0.400006171516136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606690.4754434671, 606690.4754434665, 175404.0127746816]
[2019-03-27 00:16:54,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:16:54,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2011969e-22 1.0000000e+00 1.0055991e-25 1.8880448e-19 3.4557828e-27], sampled 0.14186843045292252
[2019-03-27 00:16:57,987] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.029682053]
[2019-03-27 00:16:57,988] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.03333333333333, 64.33333333333334, 1.0, 2.0, 0.9088037089718873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1270253.618540988, 1270253.618540989, 272375.6171700581]
[2019-03-27 00:16:57,989] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:16:57,990] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0332747e-21 1.0000000e+00 1.0749236e-24 5.7848985e-19 1.8073054e-26], sampled 0.35917187804576156
[2019-03-27 00:17:33,803] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:17:33,862] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:17:33,970] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:17:34,071] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:17:34,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 00:17:35,118] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 175000, evaluation results [175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 00:17:35,520] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2804954e-18 1.0000000e+00 3.9842148e-21 1.5772175e-14 9.4995080e-22], sum to 1.0000
[2019-03-27 00:17:35,534] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6189
[2019-03-27 00:17:35,540] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 90.5, 1.0, 2.0, 0.6800205814721221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 950335.64476138, 950335.6447613795, 216590.1856917202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6575400.0000, 
sim time next is 6576000.0000, 
raw observation next is [26.06666666666667, 90.66666666666667, 1.0, 2.0, 0.6568285128143575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 917910.487686318, 917910.487686318, 211801.0630103037], 
processed observation next is [1.0, 0.08695652173913043, 0.4344391785150081, 0.9066666666666667, 1.0, 1.0, 0.586540376884768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25497513546842165, 0.25497513546842165, 0.31612098956761747], 
reward next is 0.6839, 
noisyNet noise sample is [array([-0.34481415], dtype=float32), 1.6630045]. 
=============================================
[2019-03-27 00:17:35,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[61.892178]
 [61.42343 ]
 [61.201607]
 [62.280163]
 [62.148186]], R is [[62.19693756]
 [62.25170135]
 [62.27300644]
 [62.25926208]
 [62.35944366]].
[2019-03-27 00:17:36,187] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175487: loss -256.3479
[2019-03-27 00:17:36,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175488: learning rate 0.0001
[2019-03-27 00:17:36,636] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175684: loss -176.6722
[2019-03-27 00:17:36,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175685: learning rate 0.0001
[2019-03-27 00:17:36,832] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175767: loss -155.0449
[2019-03-27 00:17:36,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175768: learning rate 0.0001
[2019-03-27 00:17:36,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175799: loss -188.6498
[2019-03-27 00:17:36,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175799: learning rate 0.0001
[2019-03-27 00:17:36,982] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175836: loss -165.6995
[2019-03-27 00:17:36,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175837: learning rate 0.0001
[2019-03-27 00:17:37,051] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175870: loss -269.8750
[2019-03-27 00:17:37,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175870: learning rate 0.0001
[2019-03-27 00:17:37,163] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175917: loss -246.3074
[2019-03-27 00:17:37,166] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175917: learning rate 0.0001
[2019-03-27 00:17:37,276] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175967: loss -226.0737
[2019-03-27 00:17:37,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175967: learning rate 0.0001
[2019-03-27 00:17:37,294] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175976: loss -108.7226
[2019-03-27 00:17:37,295] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175976: learning rate 0.0001
[2019-03-27 00:17:37,485] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176059: loss -204.3598
[2019-03-27 00:17:37,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176060: learning rate 0.0001
[2019-03-27 00:17:37,568] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176095: loss -173.6537
[2019-03-27 00:17:37,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176095: learning rate 0.0001
[2019-03-27 00:17:37,666] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176140: loss -122.9432
[2019-03-27 00:17:37,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176140: learning rate 0.0001
[2019-03-27 00:17:37,718] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176162: loss -144.0518
[2019-03-27 00:17:37,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176163: learning rate 0.0001
[2019-03-27 00:17:37,939] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176259: loss -183.8266
[2019-03-27 00:17:37,940] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176259: learning rate 0.0001
[2019-03-27 00:17:37,958] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176267: loss -163.4888
[2019-03-27 00:17:37,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176267: learning rate 0.0001
[2019-03-27 00:17:38,060] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176316: loss -170.7440
[2019-03-27 00:17:38,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176317: learning rate 0.0001
[2019-03-27 00:17:51,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0312583e-20 1.0000000e+00 3.6535241e-23 4.7215913e-17 2.4336763e-25], sum to 1.0000
[2019-03-27 00:17:51,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7859
[2019-03-27 00:17:51,805] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 82.0, 1.0, 2.0, 0.353800904892167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544747.3487628093, 544747.3487628093, 170200.9782192346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6847200.0000, 
sim time next is 6847800.0000, 
raw observation next is [23.71666666666667, 81.66666666666667, 1.0, 2.0, 0.3558915212501213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546967.3904653498, 546967.3904653498, 170356.7308014311], 
processed observation next is [0.0, 0.2608695652173913, 0.32306477093206964, 0.8166666666666668, 1.0, 1.0, 0.22396568825315818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15193538624037495, 0.15193538624037495, 0.2542637773155688], 
reward next is 0.7457, 
noisyNet noise sample is [array([-1.1388814], dtype=float32), 0.048563924]. 
=============================================
[2019-03-27 00:17:53,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4163363e-23 1.0000000e+00 2.6358795e-27 1.1785091e-21 1.3374136e-29], sum to 1.0000
[2019-03-27 00:17:53,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9479
[2019-03-27 00:17:53,626] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 37.0, 1.0, 2.0, 0.2719459181547744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 440749.8059422675, 440749.8059422681, 162980.1806510437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6879600.0000, 
sim time next is 6880200.0000, 
raw observation next is [29.65, 38.0, 1.0, 2.0, 0.2775613735108938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 448367.320643843, 448367.320643843, 163490.4800756555], 
processed observation next is [0.0, 0.6521739130434783, 0.6042654028436019, 0.38, 1.0, 1.0, 0.1295920162781853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12454647795662305, 0.12454647795662305, 0.24401564190396344], 
reward next is 0.7560, 
noisyNet noise sample is [array([-1.2851118], dtype=float32), 0.6084342]. 
=============================================
[2019-03-27 00:17:54,083] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183478: loss 0.0895
[2019-03-27 00:17:54,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183478: learning rate 0.0001
[2019-03-27 00:17:54,377] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183608: loss 0.0917
[2019-03-27 00:17:54,379] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183609: learning rate 0.0001
[2019-03-27 00:17:54,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1489036e-22 1.0000000e+00 4.2822288e-26 3.4550472e-18 2.8728993e-28], sum to 1.0000
[2019-03-27 00:17:54,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1710
[2019-03-27 00:17:54,806] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 48.0, 1.0, 2.0, 0.334012715020036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519897.311109479, 519897.3111094784, 168357.2941940529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6886200.0000, 
sim time next is 6886800.0000, 
raw observation next is [29.2, 49.0, 1.0, 2.0, 0.3404554802626267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 527584.7760305761, 527584.7760305768, 168901.6468024454], 
processed observation next is [0.0, 0.7391304347826086, 0.5829383886255924, 0.49, 1.0, 1.0, 0.20536804850918877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14655132667516002, 0.14655132667516022, 0.2520920101529036], 
reward next is 0.7479, 
noisyNet noise sample is [array([0.21759625], dtype=float32), 0.22222699]. 
=============================================
[2019-03-27 00:17:54,809] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183802: loss 0.0671
[2019-03-27 00:17:54,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183802: learning rate 0.0001
[2019-03-27 00:17:54,816] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183804: loss 0.0446
[2019-03-27 00:17:54,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183805: learning rate 0.0001
[2019-03-27 00:17:54,890] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183836: loss 0.0348
[2019-03-27 00:17:54,895] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183837: learning rate 0.0001
[2019-03-27 00:17:54,946] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183862: loss 0.0220
[2019-03-27 00:17:54,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183862: learning rate 0.0001
[2019-03-27 00:17:55,038] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183900: loss 0.0151
[2019-03-27 00:17:55,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183901: learning rate 0.0001
[2019-03-27 00:17:55,157] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183955: loss 0.0162
[2019-03-27 00:17:55,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183955: learning rate 0.0001
[2019-03-27 00:17:55,250] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183992: loss 0.0056
[2019-03-27 00:17:55,256] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183992: learning rate 0.0001
[2019-03-27 00:17:55,414] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184066: loss 0.0037
[2019-03-27 00:17:55,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184066: learning rate 0.0001
[2019-03-27 00:17:55,565] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184133: loss 0.0024
[2019-03-27 00:17:55,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184133: learning rate 0.0001
[2019-03-27 00:17:55,619] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184155: loss 0.0068
[2019-03-27 00:17:55,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184156: learning rate 0.0001
[2019-03-27 00:17:55,756] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184218: loss 0.0156
[2019-03-27 00:17:55,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184219: learning rate 0.0001
[2019-03-27 00:17:55,930] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184296: loss 0.0032
[2019-03-27 00:17:55,933] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184297: learning rate 0.0001
[2019-03-27 00:17:56,047] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184345: loss 0.0020
[2019-03-27 00:17:56,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184346: learning rate 0.0001
[2019-03-27 00:17:56,108] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184378: loss 0.0021
[2019-03-27 00:17:56,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184379: learning rate 0.0001
[2019-03-27 00:18:08,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2652087e-15 1.0000000e+00 8.9780917e-20 4.3492661e-11 1.7071376e-20], sum to 1.0000
[2019-03-27 00:18:08,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6169
[2019-03-27 00:18:08,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2151868.761937186 W.
[2019-03-27 00:18:08,638] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.865800084982329, 6.9112, 168.9075676307972, 2151868.761937186, 1474664.212596213, 314626.0437618887], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7117800.0000, 
sim time next is 7118400.0000, 
raw observation next is [27.7, 71.33333333333334, 1.0, 2.0, 0.6166416644805994, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.911694863106829, 6.9112, 168.9121673675936, 1734552.224321608, 1734201.153041923, 369882.0315017942], 
processed observation next is [1.0, 0.391304347826087, 0.5118483412322274, 0.7133333333333334, 1.0, 1.0, 0.5381224873260234, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 4.948631068293352e-05, 0.0, 0.8294360700997, 0.4818200623115578, 0.48172254251164526, 0.5520627335847674], 
reward next is 0.4455, 
noisyNet noise sample is [array([-2.1901884], dtype=float32), -0.13927378]. 
=============================================
[2019-03-27 00:18:10,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5256043e-15 9.9999988e-01 2.5293011e-17 1.7598065e-07 8.5715671e-19], sum to 1.0000
[2019-03-27 00:18:10,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7582
[2019-03-27 00:18:10,365] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1811301.012361468 W.
[2019-03-27 00:18:10,371] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.15, 85.0, 1.0, 2.0, 0.6477885197717308, 1.0, 2.0, 0.6477885197717308, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1811301.012361468, 1811301.012361468, 352563.1043722734], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7144200.0000, 
sim time next is 7144800.0000, 
raw observation next is [26.13333333333333, 85.33333333333333, 1.0, 2.0, 0.4085737415993875, 1.0, 2.0, 0.4085737415993875, 1.0, 1.0, 0.6901391053851593, 6.9112, 6.9112, 170.5573041426782, 1713559.949125608, 1713559.949125608, 354842.4766299297], 
processed observation next is [1.0, 0.6956521739130435, 0.43759873617693507, 0.8533333333333333, 1.0, 1.0, 0.28743824289082825, 1.0, 1.0, 0.28743824289082825, 1.0, 0.5, 0.622120860225804, 0.0, 0.0, 0.8375144448122397, 0.4759888747571133, 0.4759888747571133, 0.529615636761089], 
reward next is 0.4704, 
noisyNet noise sample is [array([-0.82576144], dtype=float32), 0.26110756]. 
=============================================
[2019-03-27 00:18:11,393] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5875727e-18 1.0000000e+00 6.8129347e-23 1.1334263e-13 1.9191481e-24], sum to 1.0000
[2019-03-27 00:18:11,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7510
[2019-03-27 00:18:11,404] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.71666666666667, 86.0, 1.0, 2.0, 0.473376425093435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661640.8689527683, 661640.8689527677, 179553.2254955652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7171800.0000, 
sim time next is 7172400.0000, 
raw observation next is [25.73333333333333, 86.0, 1.0, 2.0, 0.4736779639699703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661879.9016697747, 661879.9016697753, 179574.5086698887], 
processed observation next is [1.0, 0.0, 0.41864139020537117, 0.86, 1.0, 1.0, 0.3658770650240606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1838555282416041, 0.18385552824160423, 0.2680216547311772], 
reward next is 0.7320, 
noisyNet noise sample is [array([0.4960176], dtype=float32), 0.24529062]. 
=============================================
[2019-03-27 00:18:12,016] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191470: loss 0.0810
[2019-03-27 00:18:12,020] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191471: learning rate 0.0001
[2019-03-27 00:18:12,428] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191659: loss 0.1299
[2019-03-27 00:18:12,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191660: learning rate 0.0001
[2019-03-27 00:18:12,466] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191675: loss 0.0938
[2019-03-27 00:18:12,469] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191675: learning rate 0.0001
[2019-03-27 00:18:12,731] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191788: loss 0.0856
[2019-03-27 00:18:12,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191788: learning rate 0.0001
[2019-03-27 00:18:12,867] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191853: loss 0.0829
[2019-03-27 00:18:12,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191854: learning rate 0.0001
[2019-03-27 00:18:12,875] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191856: loss 0.1195
[2019-03-27 00:18:12,876] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191856: learning rate 0.0001
[2019-03-27 00:18:12,990] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191908: loss 0.0551
[2019-03-27 00:18:12,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191908: learning rate 0.0001
[2019-03-27 00:18:13,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191923: loss 0.0593
[2019-03-27 00:18:13,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191924: learning rate 0.0001
[2019-03-27 00:18:13,129] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191967: loss 0.0573
[2019-03-27 00:18:13,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191967: learning rate 0.0001
[2019-03-27 00:18:13,224] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192009: loss 0.0788
[2019-03-27 00:18:13,225] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192010: learning rate 0.0001
[2019-03-27 00:18:13,521] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192141: loss 0.0788
[2019-03-27 00:18:13,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192142: learning rate 0.0001
[2019-03-27 00:18:13,535] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192146: loss 0.0915
[2019-03-27 00:18:13,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192146: learning rate 0.0001
[2019-03-27 00:18:13,624] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192185: loss 0.0804
[2019-03-27 00:18:13,627] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192186: learning rate 0.0001
[2019-03-27 00:18:13,690] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192214: loss 0.0490
[2019-03-27 00:18:13,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192214: learning rate 0.0001
[2019-03-27 00:18:13,923] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192320: loss 0.0785
[2019-03-27 00:18:13,925] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192320: learning rate 0.0001
[2019-03-27 00:18:14,000] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192353: loss 0.0399
[2019-03-27 00:18:14,002] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192354: learning rate 0.0001
[2019-03-27 00:18:20,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3954135e-20 1.0000000e+00 4.2765076e-22 5.0449980e-14 1.1462117e-24], sum to 1.0000
[2019-03-27 00:18:20,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9100
[2019-03-27 00:18:20,361] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 73.33333333333334, 1.0, 2.0, 0.3889834030130439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584808.1625891158, 584808.1625891165, 173276.4010678578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7330800.0000, 
sim time next is 7331400.0000, 
raw observation next is [25.6, 73.5, 1.0, 2.0, 0.3878582133267932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583848.2853143984, 583848.285314399, 173211.2121714892], 
processed observation next is [1.0, 0.8695652173913043, 0.4123222748815167, 0.735, 1.0, 1.0, 0.26247977509252196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16218007925399955, 0.16218007925399974, 0.2585241972708794], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.1841764], dtype=float32), 0.022829726]. 
=============================================
[2019-03-27 00:18:21,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0095919e-18 1.0000000e+00 1.9813248e-22 9.0699139e-13 2.6767404e-23], sum to 1.0000
[2019-03-27 00:18:21,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6025
[2019-03-27 00:18:21,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 82.0, 1.0, 2.0, 0.3771318744214461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.9689907632, 571243.9689907638, 172194.9392325905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7357800.0000, 
sim time next is 7358400.0000, 
raw observation next is [24.1, 83.0, 1.0, 2.0, 0.3776804476273991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570742.1987594216, 570742.1987594216, 172110.4719952524], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.83, 1.0, 1.0, 0.25021740677999893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15853949965539488, 0.15853949965539488, 0.25688130148545135], 
reward next is 0.7431, 
noisyNet noise sample is [array([-0.5030869], dtype=float32), 0.14455387]. 
=============================================
[2019-03-27 00:18:25,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0609653e-19 1.0000000e+00 1.0285640e-23 2.2283789e-13 3.2234824e-25], sum to 1.0000
[2019-03-27 00:18:25,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6949
[2019-03-27 00:18:25,214] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 86.5, 1.0, 2.0, 0.2814994678713811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455956.5888777226, 455956.5888777226, 163986.9808121778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7408200.0000, 
sim time next is 7408800.0000, 
raw observation next is [20.8, 86.0, 1.0, 2.0, 0.2817799132103656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 456381.3243507242, 456381.3243507236, 164015.811752537], 
processed observation next is [1.0, 0.782608695652174, 0.1848341232227489, 0.86, 1.0, 1.0, 0.13467459422935615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1267725900974234, 0.12677259009742323, 0.24479971903363731], 
reward next is 0.7552, 
noisyNet noise sample is [array([-2.917036], dtype=float32), 0.11811475]. 
=============================================
[2019-03-27 00:18:26,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.52316334e-19 1.00000000e+00 4.19251273e-22 1.32963065e-14
 4.59799001e-23], sum to 1.0000
[2019-03-27 00:18:26,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3497
[2019-03-27 00:18:26,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 89.66666666666667, 1.0, 2.0, 0.5466142941974074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 881804.8437253291, 881804.8437253286, 204049.5315351732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7402800.0000, 
sim time next is 7403400.0000, 
raw observation next is [20.5, 89.5, 1.0, 2.0, 0.5140196600526521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830045.3471967152, 830045.3471967159, 197907.1468202413], 
processed observation next is [1.0, 0.6956521739130435, 0.1706161137440759, 0.895, 1.0, 1.0, 0.41448151813572537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23056815199908756, 0.23056815199908776, 0.29538380122424074], 
reward next is 0.7046, 
noisyNet noise sample is [array([-0.79499084], dtype=float32), -0.7625738]. 
=============================================
[2019-03-27 00:18:29,908] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199480: loss 0.0018
[2019-03-27 00:18:29,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199480: learning rate 0.0001
[2019-03-27 00:18:30,271] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199645: loss 0.0212
[2019-03-27 00:18:30,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199645: learning rate 0.0001
[2019-03-27 00:18:30,397] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199703: loss 0.0620
[2019-03-27 00:18:30,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199704: learning rate 0.0001
[2019-03-27 00:18:30,588] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199790: loss 0.0014
[2019-03-27 00:18:30,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199790: learning rate 0.0001
[2019-03-27 00:18:30,599] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199794: loss 0.0031
[2019-03-27 00:18:30,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199794: learning rate 0.0001
[2019-03-27 00:18:30,726] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199849: loss 0.0032
[2019-03-27 00:18:30,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199849: learning rate 0.0001
[2019-03-27 00:18:30,893] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199922: loss 0.0046
[2019-03-27 00:18:30,894] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199922: learning rate 0.0001
[2019-03-27 00:18:30,919] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199933: loss 0.0022
[2019-03-27 00:18:30,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199933: learning rate 0.0001
[2019-03-27 00:18:31,016] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199976: loss 0.0073
[2019-03-27 00:18:31,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199977: learning rate 0.0001
[2019-03-27 00:18:31,067] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 00:18:31,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:18:31,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:18:31,074] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:18:31,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:18:31,076] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:18:31,077] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:18:31,077] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:18:31,081] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:18:31,083] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:18:31,085] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:18:31,095] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-27 00:18:31,115] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-27 00:18:31,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-27 00:18:31,116] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-27 00:18:31,116] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-27 00:18:40,157] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:18:40,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.89834467333333, 69.89696857166666, 1.0, 2.0, 0.2262291370359184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377454.1967817037, 377454.1967817037, 157960.7829887491]
[2019-03-27 00:18:40,159] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:18:40,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.2286506e-19 1.0000000e+00 3.7783262e-22 3.7661101e-14 1.7291897e-23], sampled 0.3996273400652055
[2019-03-27 00:19:03,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:03,599] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 87.0, 1.0, 2.0, 0.6954466490191373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 971903.5948098795, 971903.5948098802, 219864.8691363885]
[2019-03-27 00:19:03,602] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:19:03,604] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1835810e-18 1.0000000e+00 8.6634345e-22 1.6498334e-13 3.6958291e-23], sampled 0.6874608051125941
[2019-03-27 00:19:05,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:05,390] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.40957741, 79.13390455999999, 1.0, 2.0, 0.5432605013416023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759143.7459884267, 759143.7459884273, 190650.9336538815]
[2019-03-27 00:19:05,391] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:19:05,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0269895e-20 1.0000000e+00 2.2586882e-24 6.9089058e-16 3.4708589e-26], sampled 0.4622701138862749
[2019-03-27 00:19:20,668] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:20,671] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.33999367, 69.743166575, 1.0, 2.0, 0.8044746299159448, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981807756706, 6.9112, 168.912315911564, 2021335.855151793, 1954094.633603019, 408987.716920933]
[2019-03-27 00:19:20,672] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:19:20,675] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9931085e-17 1.0000000e+00 2.0377897e-20 5.9432082e-12 1.4562252e-21], sampled 0.34400365742181216
[2019-03-27 00:19:20,678] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2021335.855151793 W.
[2019-03-27 00:19:38,771] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:38,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [38.08333333333334, 41.66666666666667, 1.0, 2.0, 0.6301511989327649, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984236050906887, 6.9112, 168.9125061731749, 1761958.327532429, 1710144.177453798, 370261.4028645354]
[2019-03-27 00:19:38,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:19:38,776] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6863184e-17 1.0000000e+00 4.0560757e-21 3.9460856e-12 2.7576894e-22], sampled 0.27651648675347884
[2019-03-27 00:19:38,778] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1761958.327532429 W.
[2019-03-27 00:19:40,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:40,040] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.1, 53.0, 1.0, 2.0, 0.8346776828467508, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983694165918, 6.9112, 168.9123159862313, 2063607.28306336, 1996364.723206317, 416593.170495428]
[2019-03-27 00:19:40,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:19:40,045] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5615651e-20 1.0000000e+00 1.0397360e-23 2.8941161e-14 2.7225747e-25], sampled 0.0651090064449874
[2019-03-27 00:19:40,047] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2063607.28306336 W.
[2019-03-27 00:19:40,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:19:40,761] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.4846914292085788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677274.1475613822, 677274.1475613815, 181232.0632363257]
[2019-03-27 00:19:40,763] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:19:40,767] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0528393e-19 1.0000000e+00 2.2942948e-22 2.4007165e-14 6.0535166e-24], sampled 0.7775203512727759
[2019-03-27 00:20:03,453] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0272903]
[2019-03-27 00:20:03,454] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.53654530666667, 70.23902558833333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.69227330811928, 6.9112, 168.9084015007977, 2008239.834828964, 1454134.490325621, 311351.9304462517]
[2019-03-27 00:20:03,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:20:03,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2191209e-18 1.0000000e+00 6.5493398e-22 9.1142521e-14 1.5301199e-23], sampled 0.348686117832547
[2019-03-27 00:20:03,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2008239.834828964 W.
[2019-03-27 00:20:24,647] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3261 2927382873.9314 1338.0000
[2019-03-27 00:20:24,922] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 00:20:25,036] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9503 2779224067.3859 933.0000
[2019-03-27 00:20:25,149] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1321 2842434697.4997 1131.0000
[2019-03-27 00:20:25,172] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6622 3164092912.7028 1778.0000
[2019-03-27 00:20:26,191] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 200000, evaluation results [200000.0, 7882.66218059418, 3164092912.7028494, 1778.0, 8254.326098134321, 2927382873.931414, 1338.0, 8659.950259991894, 2779224067.385866, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.132106361554, 2842434697.4996533, 1131.0]
[2019-03-27 00:20:26,310] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200058: loss 0.0710
[2019-03-27 00:20:26,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200059: learning rate 0.0001
[2019-03-27 00:20:26,606] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200189: loss 0.0455
[2019-03-27 00:20:26,608] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200190: loss 0.0265
[2019-03-27 00:20:26,610] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200190: learning rate 0.0001
[2019-03-27 00:20:26,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200190: learning rate 0.0001
[2019-03-27 00:20:26,665] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200212: loss 0.0215
[2019-03-27 00:20:26,673] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200216: learning rate 0.0001
[2019-03-27 00:20:26,701] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200228: loss 0.0293
[2019-03-27 00:20:26,704] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200228: learning rate 0.0001
[2019-03-27 00:20:27,008] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200367: loss 0.0670
[2019-03-27 00:20:27,011] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200367: learning rate 0.0001
[2019-03-27 00:20:27,212] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200456: loss 0.0404
[2019-03-27 00:20:27,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200458: learning rate 0.0001
[2019-03-27 00:20:42,915] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207491: loss 0.0483
[2019-03-27 00:20:42,919] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207491: learning rate 0.0001
[2019-03-27 00:20:43,154] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207600: loss 0.0170
[2019-03-27 00:20:43,157] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207602: learning rate 0.0001
[2019-03-27 00:20:43,397] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207707: loss 0.0274
[2019-03-27 00:20:43,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207710: learning rate 0.0001
[2019-03-27 00:20:43,448] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207731: loss 0.0238
[2019-03-27 00:20:43,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207731: learning rate 0.0001
[2019-03-27 00:20:43,587] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207791: loss 0.0186
[2019-03-27 00:20:43,588] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207791: learning rate 0.0001
[2019-03-27 00:20:43,705] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207849: loss 0.0160
[2019-03-27 00:20:43,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207849: learning rate 0.0001
[2019-03-27 00:20:43,845] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207911: loss 0.0200
[2019-03-27 00:20:43,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207911: learning rate 0.0001
[2019-03-27 00:20:43,932] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207946: loss 0.0151
[2019-03-27 00:20:43,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207947: learning rate 0.0001
[2019-03-27 00:20:44,038] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207993: loss 0.0150
[2019-03-27 00:20:44,041] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207995: learning rate 0.0001
[2019-03-27 00:20:44,247] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208088: loss 0.0189
[2019-03-27 00:20:44,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208089: learning rate 0.0001
[2019-03-27 00:20:44,386] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208150: loss 0.0394
[2019-03-27 00:20:44,388] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208150: learning rate 0.0001
[2019-03-27 00:20:44,402] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208156: loss 0.0257
[2019-03-27 00:20:44,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208156: learning rate 0.0001
[2019-03-27 00:20:44,415] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208159: loss 0.0250
[2019-03-27 00:20:44,418] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208160: learning rate 0.0001
[2019-03-27 00:20:44,731] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208304: loss 0.0346
[2019-03-27 00:20:44,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208304: learning rate 0.0001
[2019-03-27 00:20:44,770] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208318: loss 0.0585
[2019-03-27 00:20:44,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208319: learning rate 0.0001
[2019-03-27 00:20:44,957] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208401: loss 0.0944
[2019-03-27 00:20:44,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208401: learning rate 0.0001
[2019-03-27 00:20:47,664] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0206459e-18 1.0000000e+00 1.1126999e-21 3.3013665e-14 1.2144535e-22], sum to 1.0000
[2019-03-27 00:20:47,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8026
[2019-03-27 00:20:47,676] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.01666666666667, 92.66666666666667, 1.0, 2.0, 0.9778363665412079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1366804.093822836, 1366804.093822837, 292244.4060780596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7870200.0000, 
sim time next is 7870800.0000, 
raw observation next is [26.03333333333333, 92.33333333333334, 1.0, 2.0, 0.8849227226535293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1236855.230653404, 1236855.230653404, 265822.6574180614], 
processed observation next is [1.0, 0.08695652173913043, 0.4328593996840442, 0.9233333333333335, 1.0, 1.0, 0.8613526778958185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3435708974037233, 0.3435708974037233, 0.3967502349523304], 
reward next is 0.6032, 
noisyNet noise sample is [array([-0.9736176], dtype=float32), 0.95106846]. 
=============================================
[2019-03-27 00:20:47,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2944931e-17 1.0000000e+00 6.8109672e-21 1.2324922e-14 4.3536736e-22], sum to 1.0000
[2019-03-27 00:20:47,869] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6062
[2019-03-27 00:20:47,877] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.0, 1.0, 2.0, 0.5075820176449558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709270.5446718894, 709270.5446718894, 184792.8573793201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7864200.0000, 
sim time next is 7864800.0000, 
raw observation next is [26.16666666666666, 90.33333333333334, 1.0, 2.0, 0.5079851915844985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709834.1086543124, 709834.1086543124, 184856.9613210805], 
processed observation next is [1.0, 0.0, 0.4391785150078987, 0.9033333333333334, 1.0, 1.0, 0.40721107419819097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19717614129286457, 0.19717614129286457, 0.27590591241952317], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.18116787], dtype=float32), 0.39424318]. 
=============================================
[2019-03-27 00:20:50,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6375573e-12 9.9998748e-01 3.5930388e-15 1.2512920e-05 1.7525239e-15], sum to 1.0000
[2019-03-27 00:20:50,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2506
[2019-03-27 00:20:50,795] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1775137.038237345 W.
[2019-03-27 00:20:50,800] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.46666666666667, 72.66666666666666, 1.0, 2.0, 0.6348656437206245, 1.0, 2.0, 0.6348656437206245, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1775137.038237345, 1775137.038237344, 347485.3306000138], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7901400.0000, 
sim time next is 7902000.0000, 
raw observation next is [29.6, 72.0, 1.0, 2.0, 0.6680837093173609, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.986489992202251, 6.9112, 168.9125084211333, 1830467.5472082, 1777054.377152263, 378601.9281036093], 
processed observation next is [1.0, 0.4782608695652174, 0.6018957345971565, 0.72, 1.0, 1.0, 0.60010085459923, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007528999220225074, 0.0, 0.8294377448287149, 0.5084632075578334, 0.4936262158756286, 0.5650775046322527], 
reward next is 0.0585, 
noisyNet noise sample is [array([0.8068877], dtype=float32), 0.30162457]. 
=============================================
[2019-03-27 00:20:50,814] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[42.97663 ]
 [42.254166]
 [41.837547]
 [41.845963]
 [41.226906]], R is [[42.57750702]
 [42.6330986 ]
 [42.68461227]
 [42.25776672]
 [41.83518982]].
[2019-03-27 00:20:52,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0150632e-20 1.0000000e+00 1.0009195e-23 2.4805128e-17 4.2676522e-25], sum to 1.0000
[2019-03-27 00:20:52,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0477
[2019-03-27 00:20:52,475] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.31666666666666, 85.50000000000001, 1.0, 2.0, 0.5281917196004554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738079.5499012971, 738079.5499012977, 188131.1494959067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7938600.0000, 
sim time next is 7939200.0000, 
raw observation next is [27.23333333333333, 86.0, 1.0, 2.0, 0.526670531433898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735953.1499972278, 735953.1499972272, 187880.5169773799], 
processed observation next is [1.0, 0.9130434782608695, 0.4897314375987361, 0.86, 1.0, 1.0, 0.42972353184806983, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20443143055478552, 0.20443143055478535, 0.2804186820557909], 
reward next is 0.7196, 
noisyNet noise sample is [array([1.2332683], dtype=float32), 0.081474304]. 
=============================================
[2019-03-27 00:20:52,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:52,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:52,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-27 00:20:53,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-27 00:20:53,345] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,347] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-27 00:20:53,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,365] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-27 00:20:53,428] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,428] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-27 00:20:53,451] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,452] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-27 00:20:53,612] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,612] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-27 00:20:53,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,649] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-27 00:20:53,668] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,671] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-27 00:20:53,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,720] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-27 00:20:53,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-27 00:20:53,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-27 00:20:53,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,848] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-27 00:20:53,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-27 00:20:53,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:20:53,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:20:53,966] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-27 00:20:54,018] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-27 00:20:54,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4673412e-17 1.0000000e+00 7.9829610e-21 2.7970080e-13 1.0166866e-21], sum to 1.0000
[2019-03-27 00:20:54,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0531
[2019-03-27 00:20:54,760] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 62.5, 1.0, 2.0, 0.883599305018736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1322282.517607203, 1322282.517607203, 277843.0599871189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 48600.0000, 
sim time next is 49200.0000, 
raw observation next is [27.8, 62.33333333333333, 1.0, 2.0, 0.9097782116964279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1358494.337185492, 1358494.337185492, 285163.0603203893], 
processed observation next is [1.0, 0.5652173913043478, 0.5165876777251186, 0.6233333333333333, 1.0, 1.0, 0.8912990502366601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3773595381070811, 0.3773595381070811, 0.4256165079408795], 
reward next is 0.5744, 
noisyNet noise sample is [array([0.61227053], dtype=float32), -1.0811373]. 
=============================================
[2019-03-27 00:21:07,489] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1417965e-22 1.0000000e+00 1.2961342e-26 5.6680089e-20 1.0317726e-27], sum to 1.0000
[2019-03-27 00:21:07,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4460
[2019-03-27 00:21:07,507] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 89.33333333333334, 1.0, 2.0, 0.3094223519876886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490924.9593247549, 490924.9593247549, 166383.7612187934], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 217200.0000, 
sim time next is 217800.0000, 
raw observation next is [21.5, 89.0, 1.0, 2.0, 0.3098882878448609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491389.417390003, 491389.4173900037, 166412.9069071262], 
processed observation next is [0.0, 0.5217391304347826, 0.21800947867298584, 0.89, 1.0, 1.0, 0.1685401058371818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13649706038611195, 0.13649706038611215, 0.24837747299571072], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.3238296], dtype=float32), -1.9907526]. 
=============================================
[2019-03-27 00:21:13,768] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.7285568e-21 1.0000000e+00 2.3578721e-26 2.0555054e-19 1.1970779e-27], sum to 1.0000
[2019-03-27 00:21:13,776] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9592
[2019-03-27 00:21:13,783] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 81.0, 1.0, 2.0, 0.2944966427733454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470279.2579332673, 470279.2579332667, 164952.6699789232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [22.11666666666667, 81.5, 1.0, 2.0, 0.2935981594479545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 469009.0832647583, 469009.0832647576, 164866.0181386909], 
processed observation next is [0.0, 0.782608695652174, 0.24723538704581383, 0.815, 1.0, 1.0, 0.14891344511801743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1302803009068773, 0.13028030090687712, 0.24606868378909091], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.84581006], dtype=float32), -0.05848568]. 
=============================================
[2019-03-27 00:21:19,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5168323e-21 1.0000000e+00 1.0570239e-24 3.1317513e-18 1.3990790e-25], sum to 1.0000
[2019-03-27 00:21:19,505] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4184
[2019-03-27 00:21:19,510] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.85, 85.0, 1.0, 2.0, 0.2492489939481574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410050.3107986607, 410050.3107986607, 160830.0660650723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 426600.0000, 
sim time next is 427200.0000, 
raw observation next is [19.8, 85.33333333333333, 1.0, 2.0, 0.2481551778884649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 408293.7364843456, 408293.7364843456, 160722.1153555817], 
processed observation next is [1.0, 0.9565217391304348, 0.13744075829383895, 0.8533333333333333, 1.0, 1.0, 0.0941628649258613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1134149268012071, 0.1134149268012071, 0.23988375426206227], 
reward next is 0.7601, 
noisyNet noise sample is [array([-1.095411], dtype=float32), 0.9795723]. 
=============================================
[2019-03-27 00:21:23,217] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:21:23,219] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:21:23,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:21:23,221] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:21:23,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:21:23,222] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:21:23,224] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:21:23,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:21:23,224] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:21:23,228] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:21:23,229] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:21:23,242] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-27 00:21:23,262] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-27 00:21:23,264] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-27 00:21:23,313] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-27 00:21:23,332] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-27 00:21:35,398] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:21:35,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.94164719, 88.62196373333335, 1.0, 2.0, 0.3391845352839547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531531.7510774952, 531531.7510774952, 169367.3320029771]
[2019-03-27 00:21:35,403] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:21:35,408] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.6737133e-21 1.0000000e+00 7.5731658e-25 4.2785069e-18 1.9120809e-25], sampled 0.0300345709019193
[2019-03-27 00:21:41,475] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:21:41,476] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.19430024166667, 70.890589015, 1.0, 2.0, 0.6650297268877456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 929376.6071811764, 929376.6071811764, 213476.5154790817]
[2019-03-27 00:21:41,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:21:41,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2316579e-20 1.0000000e+00 4.1889114e-24 4.1890385e-17 1.2040043e-24], sampled 0.7351922142703236
[2019-03-27 00:22:04,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:22:04,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.52320652, 97.67367833, 1.0, 2.0, 0.5228560123115219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731326.6885551455, 731326.6885551461, 187343.4730647671]
[2019-03-27 00:22:04,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:22:04,359] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.352045e-20 1.000000e+00 1.197528e-23 5.808609e-17 3.658557e-24], sampled 0.21388533842714275
[2019-03-27 00:22:12,283] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:22:12,285] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.95, 64.0, 1.0, 2.0, 0.5316428786612293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742903.7843167511, 742903.7843167504, 188702.1777232302]
[2019-03-27 00:22:12,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:22:12,287] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.5477757e-20 1.0000000e+00 4.9735637e-24 2.9785179e-17 9.6921029e-25], sampled 0.17545179033066216
[2019-03-27 00:22:19,064] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:22:19,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.08168695666667, 65.25552313333333, 1.0, 2.0, 0.6149531993260869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859366.5307954609, 859366.5307954609, 203564.5279069585]
[2019-03-27 00:22:19,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:22:19,068] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0626602e-20 1.0000000e+00 1.7692233e-24 3.0406375e-17 4.5858111e-25], sampled 0.23774173802860732
[2019-03-27 00:22:41,234] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:22:41,235] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.73333333333333, 61.66666666666667, 1.0, 2.0, 0.984963165812649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1376772.279929413, 1376772.279929413, 294382.9538425413]
[2019-03-27 00:22:41,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:22:41,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.22538018e-22 1.00000000e+00 1.40970787e-25 5.57411631e-19
 1.03829965e-26], sampled 0.9144805400173159
[2019-03-27 00:22:49,754] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:22:49,756] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.96831043, 87.96019714, 1.0, 2.0, 0.3834679494924087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585453.7614820879, 585453.7614820886, 173580.2920227353]
[2019-03-27 00:22:49,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:22:49,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.4272677e-23 1.0000000e+00 6.0641159e-27 3.4671114e-20 5.7949879e-28], sampled 0.10366217001228561
[2019-03-27 00:23:13,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:23:13,619] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 86.0, 1.0, 2.0, 1.022714880974363, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005994637420365, 6.9112, 168.9122086183457, 2326796.045651196, 2259545.76504943, 470369.6708374362]
[2019-03-27 00:23:13,621] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:23:13,625] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.2205185e-20 1.0000000e+00 7.1851644e-24 1.3766329e-16 1.7370232e-24], sampled 0.028415525361694782
[2019-03-27 00:23:13,626] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2326796.045651196 W.
[2019-03-27 00:23:16,790] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018939793]
[2019-03-27 00:23:16,791] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.93333333333334, 71.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.102114901984447, 6.9112, 168.9062582318291, 2299165.678086167, 1454323.16222595, 311283.9080384432]
[2019-03-27 00:23:16,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:23:16,796] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8815078e-21 1.0000000e+00 4.7369968e-25 7.2506940e-17 1.3095487e-25], sampled 0.2676488597017278
[2019-03-27 00:23:16,799] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2299165.678086167 W.
[2019-03-27 00:23:16,998] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 00:23:17,307] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:23:17,387] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9559 3007647868.8583 1766.0000
[2019-03-27 00:23:17,424] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 00:23:17,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:23:18,451] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 225000, evaluation results [225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.955910945021, 3007647868.858331, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 00:23:20,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1921174e-19 1.0000000e+00 1.7946799e-24 2.9176563e-17 2.9758597e-25], sum to 1.0000
[2019-03-27 00:23:20,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0737
[2019-03-27 00:23:20,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.06666666666667, 77.66666666666667, 1.0, 2.0, 0.2335263237162743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 387063.5965167027, 387063.5965167021, 159162.1169401951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 594600.0000, 
sim time next is 595200.0000, 
raw observation next is [19.93333333333333, 78.33333333333334, 1.0, 2.0, 0.2323642358406649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 385292.552640969, 385292.5526409696, 159036.8003015741], 
processed observation next is [1.0, 0.9130434782608695, 0.14375987361769343, 0.7833333333333334, 1.0, 1.0, 0.07513763354296975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10702570906693583, 0.107025709066936, 0.23736835865906583], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.2933714], dtype=float32), -0.5783611]. 
=============================================
[2019-03-27 00:23:26,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6772360e-19 1.0000000e+00 5.1305463e-22 1.4288483e-15 7.8337466e-23], sum to 1.0000
[2019-03-27 00:23:26,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0189
[2019-03-27 00:23:26,216] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.13333333333333, 91.0, 1.0, 2.0, 0.2286987242212161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 382335.3404763004, 382335.3404762998, 157779.9089168127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 623400.0000, 
sim time next is 624000.0000, 
raw observation next is [17.36666666666667, 90.0, 1.0, 2.0, 0.2157828236034253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 360580.0990561401, 360580.0990561408, 156757.1322968522], 
processed observation next is [1.0, 0.21739130434782608, 0.02211690363349157, 0.9, 1.0, 1.0, 0.05516002843786179, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10016113862670559, 0.10016113862670578, 0.23396586909977943], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.11749868], dtype=float32), 1.2390916]. 
=============================================
[2019-03-27 00:23:26,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[65.66211]
 [65.78445]
 [65.74468]
 [65.76261]
 [65.89614]], R is [[65.73273468]
 [65.83992004]
 [65.94983673]
 [66.05865479]
 [66.16629028]].
[2019-03-27 00:23:27,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5942706e-18 1.0000000e+00 2.1178325e-22 1.9206144e-15 4.3482108e-23], sum to 1.0000
[2019-03-27 00:23:27,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9723
[2019-03-27 00:23:27,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 69.0, 1.0, 2.0, 0.4320835532460939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711115.6809246483, 711115.6809246477, 184346.6503951903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [22.16666666666667, 68.16666666666667, 1.0, 2.0, 0.4488592894162586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738340.9935510228, 738340.9935510228, 187039.087585057], 
processed observation next is [1.0, 0.391304347826087, 0.24960505529225935, 0.6816666666666668, 1.0, 1.0, 0.3359750474894682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20509472043083968, 0.20509472043083968, 0.27916281729112985], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.67984223], dtype=float32), 0.40164337]. 
=============================================
[2019-03-27 00:23:31,334] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2102719e-18 1.0000000e+00 4.1638215e-23 1.8921439e-15 6.5831017e-23], sum to 1.0000
[2019-03-27 00:23:31,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6440
[2019-03-27 00:23:31,348] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.51666666666667, 92.0, 1.0, 2.0, 0.2141490871248178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357128.8032411602, 357128.8032411602, 156965.2098925418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [17.5, 92.0, 1.0, 2.0, 0.21247095903468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 354364.6273674652, 354364.6273674652, 156808.9730928492], 
processed observation next is [1.0, 0.21739130434782608, 0.028436018957346036, 0.92, 1.0, 1.0, 0.05116983016226504, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09843461871318478, 0.09843461871318478, 0.23404324342216298], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.2667796], dtype=float32), -1.0893215]. 
=============================================
[2019-03-27 00:23:54,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4357354e-21 1.0000000e+00 2.9132531e-24 1.0074000e-17 8.3505863e-26], sum to 1.0000
[2019-03-27 00:23:54,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2195
[2019-03-27 00:23:54,902] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 69.0, 1.0, 2.0, 0.3304070660160498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512324.9343247216, 512324.9343247223, 167705.3275194789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [25.13333333333333, 69.5, 1.0, 2.0, 0.332286620568118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515907.662497577, 515907.6624975776, 168005.839660841], 
processed observation next is [1.0, 0.7391304347826086, 0.3902053712480251, 0.695, 1.0, 1.0, 0.19552604887725056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14330768402710473, 0.1433076840271049, 0.25075498456841944], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.50413954], dtype=float32), -0.43724647]. 
=============================================
[2019-03-27 00:23:54,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.51251]
 [81.44542]
 [81.1706 ]
 [80.41989]
 [79.17482]], R is [[81.35427094]
 [81.29042053]
 [81.22785187]
 [81.16507721]
 [81.08480835]].
[2019-03-27 00:24:02,125] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0096595e-20 1.0000000e+00 9.0231204e-24 2.3928239e-16 1.7335621e-23], sum to 1.0000
[2019-03-27 00:24:02,132] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6745
[2019-03-27 00:24:02,140] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 89.33333333333334, 1.0, 2.0, 0.3432941827803873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532884.6881421336, 532884.6881421336, 169352.8989846993], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1214400.0000, 
sim time next is 1215000.0000, 
raw observation next is [22.25, 89.5, 1.0, 2.0, 0.3420193081045516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531317.8825995304, 531317.8825995297, 169237.7872638267], 
processed observation next is [1.0, 0.043478260869565216, 0.2535545023696683, 0.895, 1.0, 1.0, 0.2072521784392188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14758830072209178, 0.1475883007220916, 0.2525937123340697], 
reward next is 0.7474, 
noisyNet noise sample is [array([-0.4733858], dtype=float32), 1.4727316]. 
=============================================
[2019-03-27 00:24:02,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.14946 ]
 [69.961945]
 [69.70006 ]
 [69.36924 ]
 [69.07175 ]], R is [[70.40779114]
 [70.45095062]
 [70.49351501]
 [70.53555298]
 [70.57715607]].
[2019-03-27 00:24:03,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0617560e-19 1.0000000e+00 2.2243515e-22 5.7272437e-17 2.5309827e-23], sum to 1.0000
[2019-03-27 00:24:03,602] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5756
[2019-03-27 00:24:03,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 91.83333333333334, 1.0, 2.0, 0.5596482083041264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795960.9681756579, 795960.9681756579, 195251.3883863686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1311000.0000, 
sim time next is 1311600.0000, 
raw observation next is [24.53333333333333, 91.66666666666667, 1.0, 2.0, 0.5137428404186826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730800.3274593552, 730800.3274593552, 187428.4019170274], 
processed observation next is [1.0, 0.17391304347826086, 0.36176935229067925, 0.9166666666666667, 1.0, 1.0, 0.4141480005044368, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20300009096093202, 0.20300009096093202, 0.27974388345824985], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.5057461], dtype=float32), -2.4149244]. 
=============================================
[2019-03-27 00:24:04,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3023672e-14 1.0000000e+00 5.9565015e-18 8.6164392e-10 9.9922492e-18], sum to 1.0000
[2019-03-27 00:24:04,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3629
[2019-03-27 00:24:04,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1898696.284913115 W.
[2019-03-27 00:24:04,637] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.23333333333333, 72.16666666666667, 1.0, 2.0, 0.6790166462547704, 1.0, 2.0, 0.6790166462547704, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1898696.284913115, 1898696.284913115, 365260.4113713186], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [28.26666666666667, 72.33333333333334, 1.0, 2.0, 0.3090153002163886, 1.0, 2.0, 0.3090153002163886, 1.0, 1.0, 0.5209017891698987, 6.9112, 6.9112, 170.5573041426782, 1295759.040860924, 1295759.040860924, 306597.6538669664], 
processed observation next is [1.0, 0.5217391304347826, 0.53870458135861, 0.7233333333333334, 1.0, 1.0, 0.16748831351372123, 1.0, 1.0, 0.16748831351372123, 1.0, 0.5, 0.4157338892315838, 0.0, 0.0, 0.8375144448122397, 0.3599330669058122, 0.3599330669058122, 0.4576084386074125], 
reward next is 0.5424, 
noisyNet noise sample is [array([-0.25143823], dtype=float32), 0.9135316]. 
=============================================
[2019-03-27 00:24:04,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[51.852654]
 [51.553604]
 [51.177765]
 [51.49151 ]
 [50.991703]], R is [[52.90019226]
 [52.8260231 ]
 [52.78718185]
 [52.72827911]
 [52.65743637]].
[2019-03-27 00:24:05,288] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8658976e-13 1.0000000e+00 4.2002875e-17 2.5633033e-09 1.2215649e-18], sum to 1.0000
[2019-03-27 00:24:05,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2801
[2019-03-27 00:24:05,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1981674.732008067 W.
[2019-03-27 00:24:05,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.4724427952847953, 1.0, 1.0, 0.4724427952847953, 1.0, 2.0, 0.8016887286230145, 6.911199999999999, 6.9112, 170.5573041426782, 1981674.732008067, 1981674.732008068, 393379.4517705348], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.6973453389592479, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.97018596813283, 6.9112, 168.9126044277129, 1871413.98973931, 1829567.401030883, 385270.7338357415], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.635355830071383, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005898596813282975, 0.0, 0.8294382162649726, 0.5198372193720305, 0.5082131669530231, 0.5750309460234948], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5224148], dtype=float32), -0.80832994]. 
=============================================
[2019-03-27 00:24:05,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1315773e-15 1.0000000e+00 1.0847932e-17 6.3870284e-11 5.7945528e-18], sum to 1.0000
[2019-03-27 00:24:05,654] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5398
[2019-03-27 00:24:05,658] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 92.83333333333333, 1.0, 2.0, 0.5427297502722095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773743.6861823428, 773743.6861823434, 192522.4575212592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1307400.0000, 
sim time next is 1308000.0000, 
raw observation next is [24.36666666666667, 92.66666666666667, 1.0, 2.0, 0.5058709776170744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720844.650781309, 720844.6507813084, 186305.1775416525], 
processed observation next is [1.0, 0.13043478260869565, 0.3538704581358612, 0.9266666666666667, 1.0, 1.0, 0.40466382845430654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20023462521703028, 0.20023462521703012, 0.2780674291666455], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.3279182], dtype=float32), -0.38404867]. 
=============================================
[2019-03-27 00:24:05,668] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.898624]
 [52.86553 ]
 [52.85135 ]
 [52.963467]
 [52.89766 ]], R is [[53.10879898]
 [53.29036713]
 [53.47370911]
 [53.65355682]
 [53.82814026]].
[2019-03-27 00:24:10,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6175592e-16 1.0000000e+00 6.1765718e-21 2.8753074e-12 9.5774843e-22], sum to 1.0000
[2019-03-27 00:24:10,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4250
[2019-03-27 00:24:10,292] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.5704487401589996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906774.0291150539, 906774.0291150539, 207844.0228276239], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1347000.0000, 
sim time next is 1347600.0000, 
raw observation next is [21.43333333333334, 88.33333333333334, 1.0, 2.0, 0.5523849785694962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878398.0276198094, 878398.0276198094, 204283.3605974592], 
processed observation next is [1.0, 0.6086956521739131, 0.21484992101105885, 0.8833333333333334, 1.0, 1.0, 0.4607047934572243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24399945211661372, 0.24399945211661372, 0.304900538205163], 
reward next is 0.6951, 
noisyNet noise sample is [array([0.4681272], dtype=float32), -1.5147045]. 
=============================================
[2019-03-27 00:24:11,505] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1904567e-19 1.0000000e+00 1.0377825e-23 4.7296901e-15 2.3317573e-23], sum to 1.0000
[2019-03-27 00:24:11,518] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1431
[2019-03-27 00:24:11,525] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1448400.0000, 
sim time next is 1449000.0000, 
raw observation next is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
processed observation next is [0.0, 0.782608695652174, 0.39810426540284366, 0.8, 1.0, 1.0, 0.2920263341904347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16842418548608884, 0.16842418548608867, 0.2609946092827937], 
reward next is 0.7390, 
noisyNet noise sample is [array([0.04639652], dtype=float32), -0.02982494]. 
=============================================
[2019-03-27 00:24:11,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.57175 ]
 [77.52155 ]
 [77.438515]
 [77.31911 ]
 [77.24595 ]], R is [[77.58465576]
 [77.54758453]
 [77.51113892]
 [77.47473145]
 [77.43843079]].
[2019-03-27 00:24:14,219] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 00:24:14,220] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:24:14,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:24:14,223] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:24:14,226] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:24:14,227] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:24:14,229] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:24:14,230] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:24:14,232] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:24:14,232] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:24:14,234] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:24:14,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-27 00:24:14,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-27 00:24:14,268] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-27 00:24:14,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-27 00:24:14,310] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-27 00:24:15,306] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:24:15,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.38333333333333, 74.16666666666667, 1.0, 2.0, 0.3921674869265577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591677.9098760757, 591677.9098760757, 173959.0777869822]
[2019-03-27 00:24:15,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:24:15,308] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7409315e-05 9.9976963e-01 6.1994092e-07 2.1170208e-04 6.9816593e-07], sampled 0.967127679213277
[2019-03-27 00:24:49,422] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:24:49,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.46255883, 90.90938013333334, 1.0, 2.0, 0.4973824479655644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695013.4971665042, 695013.4971665036, 183186.4749045656]
[2019-03-27 00:24:49,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:24:49,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.1140628e-20 1.0000000e+00 1.2576194e-24 3.4807032e-15 6.4149656e-25], sampled 0.257747088424628
[2019-03-27 00:24:58,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:24:58,993] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.5, 53.0, 1.0, 2.0, 0.7989589618634577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1116640.625349778, 1116640.625349778, 243653.1909459946]
[2019-03-27 00:24:58,994] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:24:58,997] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.469417e-19 1.000000e+00 4.753164e-24 1.780896e-14 3.206009e-24], sampled 0.32168333596443566
[2019-03-27 00:25:05,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:25:05,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.76666666666667, 57.0, 1.0, 2.0, 0.5198296683888379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726390.6706091019, 726390.6706091012, 186760.5212599874]
[2019-03-27 00:25:05,735] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:25:05,738] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5949464e-20 1.0000000e+00 2.2517103e-25 6.7046988e-16 1.2591405e-25], sampled 0.3830610651371412
[2019-03-27 00:25:15,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:25:15,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.10959047, 74.25862117, 1.0, 2.0, 0.6643376661175461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928409.0320588959, 928409.0320588959, 213345.8894090843]
[2019-03-27 00:25:15,377] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:25:15,379] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.1429574e-21 1.0000000e+00 8.9658116e-26 4.6317006e-16 4.8805813e-26], sampled 0.6787770794216255
[2019-03-27 00:26:06,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:26:06,201] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.73333333333333, 84.0, 1.0, 2.0, 0.7483556512141517, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.003894910314562, 6.9112, 168.9123320407243, 1942797.625419508, 1877036.908847446, 395557.6733067048]
[2019-03-27 00:26:06,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:26:06,205] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1139705e-18 1.0000000e+00 6.2473707e-23 1.6907340e-13 6.3324528e-23], sampled 0.29948981656907503
[2019-03-27 00:26:06,206] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1942797.625419508 W.
[2019-03-27 00:26:09,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:26:09,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.25, 86.16666666666667, 1.0, 2.0, 0.5422440248838785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757722.8310286747, 757722.8310286741, 190478.6689506821]
[2019-03-27 00:26:09,561] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:26:09,564] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3263235e-20 1.0000000e+00 3.1737250e-25 9.9381645e-16 1.8332271e-25], sampled 0.7242707275323937
[2019-03-27 00:26:12,728] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015494908]
[2019-03-27 00:26:12,729] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.76606153, 96.92693771, 1.0, 2.0, 0.4528987855256678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 647845.3279399124, 647845.3279399119, 178474.123578476]
[2019-03-27 00:26:12,731] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:26:12,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8945394e-20 1.0000000e+00 1.4933803e-24 4.2708333e-15 8.4928745e-25], sampled 0.9158666937579597
[2019-03-27 00:26:18,068] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:26:18,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:26:18,128] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7743 2842558600.5870 1131.0000
[2019-03-27 00:26:18,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 00:26:18,251] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 00:26:19,268] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.774252025001, 2842558600.586998, 1131.0]
[2019-03-27 00:26:21,046] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4601015e-19 1.0000000e+00 6.6988648e-25 1.2337945e-15 1.6942035e-25], sum to 1.0000
[2019-03-27 00:26:21,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9332
[2019-03-27 00:26:21,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 69.83333333333334, 1.0, 2.0, 0.4519402142382168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639298.9911641382, 639298.9911641382, 177417.5316918393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1440600.0000, 
sim time next is 1441200.0000, 
raw observation next is [27.86666666666667, 69.66666666666667, 1.0, 2.0, 0.4480711582908161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636194.6363583707, 636194.6363583714, 177165.4249158481], 
processed observation next is [0.0, 0.6956521739130435, 0.519747235387046, 0.6966666666666668, 1.0, 1.0, 0.3350254919166459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17672073232176963, 0.17672073232176982, 0.2644260073370867], 
reward next is 0.7356, 
noisyNet noise sample is [array([0.7387696], dtype=float32), 0.8656034]. 
=============================================
[2019-03-27 00:26:22,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1418439e-18 1.0000000e+00 1.2829424e-24 2.4535999e-15 1.8443740e-24], sum to 1.0000
[2019-03-27 00:26:22,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7105
[2019-03-27 00:26:22,737] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.66666666666667, 1.0, 2.0, 0.3104452244773311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492129.4430516197, 492129.4430516197, 166464.6233860319], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1485600.0000, 
sim time next is 1486200.0000, 
raw observation next is [20.26666666666667, 98.83333333333333, 1.0, 2.0, 0.3089550858985022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490307.2586242497, 490307.2586242491, 166340.6201551674], 
processed observation next is [0.0, 0.17391304347826086, 0.15955766192733034, 0.9883333333333333, 1.0, 1.0, 0.1674157661427737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13619646072895825, 0.13619646072895808, 0.24826958232114535], 
reward next is 0.7517, 
noisyNet noise sample is [array([-1.126501], dtype=float32), -0.977458]. 
=============================================
[2019-03-27 00:26:25,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9756535e-20 1.0000000e+00 3.8347053e-24 3.8092938e-16 1.0454986e-24], sum to 1.0000
[2019-03-27 00:26:25,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4559
[2019-03-27 00:26:25,097] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 51.0, 1.0, 2.0, 0.3526428651476576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538711.9606284414, 538711.9606284414, 169568.5944270689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1513800.0000, 
sim time next is 1514400.0000, 
raw observation next is [29.36666666666667, 51.0, 1.0, 2.0, 0.3545495643181173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540696.9328101261, 540696.9328101267, 169702.3073243424], 
processed observation next is [0.0, 0.5217391304347826, 0.5908372827804109, 0.51, 1.0, 1.0, 0.22234887267243048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15019359244725725, 0.1501935924472574, 0.25328702585722745], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.6549877], dtype=float32), -1.9463387]. 
=============================================
[2019-03-27 00:26:25,990] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4435259e-19 1.0000000e+00 1.4256193e-23 1.2421398e-14 4.4888010e-25], sum to 1.0000
[2019-03-27 00:26:26,003] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3651
[2019-03-27 00:26:26,011] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 59.0, 1.0, 2.0, 0.3437258197622903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531896.1454583206, 531896.14545832, 169226.5476227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1530000.0000, 
sim time next is 1530600.0000, 
raw observation next is [26.9, 60.16666666666667, 1.0, 2.0, 0.3436552026726148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531968.7533513719, 531968.7533513724, 169237.7290554589], 
processed observation next is [0.0, 0.7391304347826086, 0.4739336492890995, 0.6016666666666667, 1.0, 1.0, 0.2092231357501383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14776909815315883, 0.147769098153159, 0.2525936254559088], 
reward next is 0.7474, 
noisyNet noise sample is [array([1.8142453], dtype=float32), 1.3977414]. 
=============================================
[2019-03-27 00:26:26,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0013875e-19 1.0000000e+00 3.5737521e-24 5.2611961e-15 2.3520174e-24], sum to 1.0000
[2019-03-27 00:26:26,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7488
[2019-03-27 00:26:26,688] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 81.5, 1.0, 2.0, 0.3584798453612946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549739.9703249993, 549739.9703249993, 170553.3653356346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1540200.0000, 
sim time next is 1540800.0000, 
raw observation next is [23.6, 83.0, 1.0, 2.0, 0.3586410818911617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549815.2703252132, 549815.2703252139, 170554.578030716], 
processed observation next is [0.0, 0.8695652173913043, 0.3175355450236968, 0.83, 1.0, 1.0, 0.22727841191706225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1527264639792259, 0.1527264639792261, 0.2545590716876358], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.2971246], dtype=float32), 0.08484166]. 
=============================================
[2019-03-27 00:26:39,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1769340e-18 1.0000000e+00 1.0388887e-21 5.1464567e-17 1.6908553e-22], sum to 1.0000
[2019-03-27 00:26:39,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7387
[2019-03-27 00:26:39,721] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.71666666666667, 82.66666666666667, 1.0, 2.0, 0.7788576083047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1191096.788939322, 1191096.788939322, 252253.2850973595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [23.63333333333334, 83.33333333333334, 1.0, 2.0, 0.7611438823765887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163799.272541831, 1163799.272541831, 247621.4332862092], 
processed observation next is [1.0, 0.4782608695652174, 0.3191153238546607, 0.8333333333333335, 1.0, 1.0, 0.7122215450320346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3232775757060642, 0.3232775757060642, 0.3695842287853869], 
reward next is 0.6304, 
noisyNet noise sample is [array([-0.73664427], dtype=float32), -0.5512258]. 
=============================================
[2019-03-27 00:26:40,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.24719456e-17 1.00000000e+00 1.09237783e-21 2.08845056e-17
 9.42511004e-23], sum to 1.0000
[2019-03-27 00:26:40,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5335
[2019-03-27 00:26:40,233] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 85.33333333333333, 1.0, 2.0, 0.6590610138828128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1039346.497714092, 1039346.497714092, 226309.5044178158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [21.93333333333333, 86.66666666666667, 1.0, 2.0, 0.6317802615002365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997703.614510872, 997703.614510872, 220280.7405242832], 
processed observation next is [1.0, 0.6086956521739131, 0.23854660347551332, 0.8666666666666667, 1.0, 1.0, 0.5563617608436584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27713989291968666, 0.27713989291968666, 0.32877722466310927], 
reward next is 0.6712, 
noisyNet noise sample is [array([0.04859792], dtype=float32), -0.804676]. 
=============================================
[2019-03-27 00:26:40,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0087108e-18 1.0000000e+00 3.2522431e-21 5.4489275e-16 1.3385770e-22], sum to 1.0000
[2019-03-27 00:26:40,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7945
[2019-03-27 00:26:40,622] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 85.0, 1.0, 2.0, 0.8158929262739981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1219510.515088458, 1219510.515088459, 258696.1748876208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [23.96666666666667, 84.0, 1.0, 2.0, 0.7937308504470925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195037.300128591, 1195037.300128591, 253907.9875041888], 
processed observation next is [1.0, 0.43478260869565216, 0.33491311216429714, 0.84, 1.0, 1.0, 0.7514829523458946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33195480559127527, 0.33195480559127527, 0.37896714552863997], 
reward next is 0.6210, 
noisyNet noise sample is [array([1.088656], dtype=float32), -0.3108343]. 
=============================================
[2019-03-27 00:26:40,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4578797e-19 1.0000000e+00 7.7194773e-23 1.2321713e-16 7.8542135e-23], sum to 1.0000
[2019-03-27 00:26:40,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9379
[2019-03-27 00:26:40,789] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.65, 84.5, 1.0, 2.0, 0.6980661025658906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090535.943897861, 1090535.943897861, 234538.0644342992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1776600.0000, 
sim time next is 1777200.0000, 
raw observation next is [22.56666666666667, 84.33333333333334, 1.0, 2.0, 0.6642716028660758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1040452.423537808, 1040452.423537808, 226840.2808088394], 
processed observation next is [1.0, 0.5652173913043478, 0.26856240126382325, 0.8433333333333334, 1.0, 1.0, 0.5955079552603323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2890145620938356, 0.2890145620938356, 0.3385675832967752], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.24768497], dtype=float32), -1.1373773]. 
=============================================
[2019-03-27 00:26:42,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3612596e-23 1.0000000e+00 2.5742301e-27 2.8082206e-22 2.0746375e-29], sum to 1.0000
[2019-03-27 00:26:42,523] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4353
[2019-03-27 00:26:42,528] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 95.33333333333334, 1.0, 2.0, 0.3407480664824477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530656.300366194, 530656.300366194, 169219.3343202951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1809600.0000, 
sim time next is 1810200.0000, 
raw observation next is [21.48333333333333, 95.66666666666666, 1.0, 2.0, 0.3424430376351973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532482.1838473212, 532482.1838473206, 169345.0301547416], 
processed observation next is [1.0, 0.9565217391304348, 0.21721958925750387, 0.9566666666666666, 1.0, 1.0, 0.20776269594602081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.147911717735367, 0.14791171773536685, 0.2527537763503606], 
reward next is 0.7472, 
noisyNet noise sample is [array([0.1143954], dtype=float32), 1.2129009]. 
=============================================
[2019-03-27 00:26:53,811] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0883936e-20 1.0000000e+00 6.2979690e-26 5.9947718e-19 5.2827057e-26], sum to 1.0000
[2019-03-27 00:26:53,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9456
[2019-03-27 00:26:53,825] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 95.66666666666666, 1.0, 2.0, 0.4799949164347742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 670709.4949860029, 670709.4949860035, 180521.0908855588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2010000.0000, 
sim time next is 2010600.0000, 
raw observation next is [24.75, 95.5, 1.0, 2.0, 0.4817375651242758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673145.3152481278, 673145.3152481284, 180784.2955881653], 
processed observation next is [0.0, 0.2608695652173913, 0.3720379146919432, 0.955, 1.0, 1.0, 0.3755874278605732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18698480979114662, 0.18698480979114676, 0.2698273068480079], 
reward next is 0.7302, 
noisyNet noise sample is [array([1.0294706], dtype=float32), -0.3212796]. 
=============================================
[2019-03-27 00:27:01,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3143891e-22 1.0000000e+00 1.4554829e-26 1.4633489e-19 5.5916849e-27], sum to 1.0000
[2019-03-27 00:27:01,323] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9505
[2019-03-27 00:27:01,329] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.2, 76.33333333333333, 1.0, 2.0, 0.5726510587551965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800229.122817712, 800229.1228177126, 195761.9802924602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2126400.0000, 
sim time next is 2127000.0000, 
raw observation next is [30.25, 76.16666666666667, 1.0, 2.0, 0.5739163567625523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 801997.9322146008, 801997.9322146003, 195987.6671177533], 
processed observation next is [0.0, 0.6086956521739131, 0.6327014218009479, 0.7616666666666667, 1.0, 1.0, 0.4866462129669304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22277720339294468, 0.22277720339294452, 0.2925189061459004], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.2681771], dtype=float32), -1.4278607]. 
=============================================
[2019-03-27 00:27:01,344] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.71274 ]
 [78.67779 ]
 [78.640396]
 [78.601746]
 [78.54164 ]], R is [[78.66192627]
 [78.58312225]
 [78.5055542 ]
 [78.42910004]
 [78.3535614 ]].
[2019-03-27 00:27:03,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2734675e-18 1.0000000e+00 4.7563583e-23 5.6164690e-17 3.3195216e-24], sum to 1.0000
[2019-03-27 00:27:03,464] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3056
[2019-03-27 00:27:03,469] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 90.33333333333334, 1.0, 2.0, 0.5363365448326775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749464.9038800414, 749464.9038800414, 189484.9939448163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [26.68333333333333, 90.66666666666667, 1.0, 2.0, 0.5344547091604308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746834.3436235257, 746834.3436235257, 189170.4358686113], 
processed observation next is [0.0, 0.9130434782608695, 0.4636650868878356, 0.9066666666666667, 1.0, 1.0, 0.4391020592294347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20745398433986825, 0.20745398433986825, 0.28234393413225567], 
reward next is 0.7177, 
noisyNet noise sample is [array([1.1333624], dtype=float32), 3.0329058]. 
=============================================
[2019-03-27 00:27:04,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6066906e-16 1.0000000e+00 4.2171407e-19 5.6348936e-13 6.5536918e-21], sum to 1.0000
[2019-03-27 00:27:04,303] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0267
[2019-03-27 00:27:04,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 82.0, 1.0, 2.0, 0.7579716888774716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059327.439861486, 1059327.439861486, 233856.7730923202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [28.4, 81.50000000000001, 1.0, 2.0, 0.7946998139054229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1110684.843900213, 1110684.843900212, 242614.7917788742], 
processed observation next is [1.0, 0.34782608695652173, 0.5450236966824644, 0.8150000000000002, 1.0, 1.0, 0.7526503781993047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3085235677500592, 0.3085235677500589, 0.36211162952070775], 
reward next is 0.6379, 
noisyNet noise sample is [array([0.35695106], dtype=float32), 0.88283294]. 
=============================================
[2019-03-27 00:27:04,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1889629e-15 1.0000000e+00 1.5723477e-19 4.1381924e-12 3.8304876e-20], sum to 1.0000
[2019-03-27 00:27:04,971] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9111
[2019-03-27 00:27:04,975] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 82.0, 1.0, 2.0, 0.7579716888774716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059327.439861486, 1059327.439861486, 233856.7730923202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [28.4, 81.50000000000001, 1.0, 2.0, 0.7946998139054229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1110684.843900213, 1110684.843900212, 242614.7917788742], 
processed observation next is [1.0, 0.34782608695652173, 0.5450236966824644, 0.8150000000000002, 1.0, 1.0, 0.7526503781993047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3085235677500592, 0.3085235677500589, 0.36211162952070775], 
reward next is 0.6379, 
noisyNet noise sample is [array([0.13308728], dtype=float32), -2.5982525]. 
=============================================
[2019-03-27 00:27:09,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7248036e-14 1.0000000e+00 5.8133089e-17 8.1112106e-10 7.6743981e-18], sum to 1.0000
[2019-03-27 00:27:09,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1483
[2019-03-27 00:27:09,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2792873.004640939 W.
[2019-03-27 00:27:09,158] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.98333333333333, 63.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.619543245443499, 6.9112, 168.9093953511246, 2792873.004640939, 2290360.521627498, 474830.9909731605], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [31.96666666666667, 63.33333333333334, 1.0, 2.0, 0.4733473754225923, 1.0, 1.0, 0.4733473754225923, 1.0, 2.0, 0.8220478527399441, 6.9112, 6.9112, 170.5573041426782, 1985472.535035645, 1985472.535035645, 397204.2009173032], 
processed observation next is [1.0, 0.5217391304347826, 0.7140600315955767, 0.6333333333333334, 1.0, 1.0, 0.3654787655693883, 1.0, 0.5, 0.3654787655693883, 1.0, 1.0, 0.7829851862682243, 0.0, 0.0, 0.8375144448122397, 0.5515201486210125, 0.5515201486210125, 0.5928420909213481], 
reward next is 0.4072, 
noisyNet noise sample is [array([0.19980563], dtype=float32), -0.56143254]. 
=============================================
[2019-03-27 00:27:10,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4758978e-16 1.0000000e+00 9.4100408e-19 3.8225772e-12 1.4881124e-19], sum to 1.0000
[2019-03-27 00:27:10,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6092
[2019-03-27 00:27:10,174] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 74.0, 1.0, 2.0, 0.6684329564310884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 934134.6996199704, 934134.699619971, 214177.4271087158], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2275200.0000, 
sim time next is 2275800.0000, 
raw observation next is [28.8, 73.33333333333334, 1.0, 2.0, 0.76646997880906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1071210.485815609, 1071210.485815609, 235845.3600782987], 
processed observation next is [1.0, 0.34782608695652173, 0.5639810426540285, 0.7333333333333334, 1.0, 1.0, 0.7186385286856144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29755846828211363, 0.29755846828211363, 0.35200800011686373], 
reward next is 0.6480, 
noisyNet noise sample is [array([-1.637178], dtype=float32), 0.54377663]. 
=============================================
[2019-03-27 00:27:10,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2923332e-16 1.0000000e+00 1.4789896e-19 3.7165267e-12 5.5693735e-20], sum to 1.0000
[2019-03-27 00:27:10,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1277
[2019-03-27 00:27:10,452] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.33333333333334, 1.0, 2.0, 0.6254545890678294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 874047.7394175718, 874047.7394175711, 205574.9690723456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2269200.0000, 
sim time next is 2269800.0000, 
raw observation next is [27.15, 80.5, 1.0, 2.0, 0.634765193559009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887064.3698499117, 887064.3698499117, 207391.9909037694], 
processed observation next is [1.0, 0.2608695652173913, 0.485781990521327, 0.805, 1.0, 1.0, 0.5599580645289264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24640676940275325, 0.24640676940275325, 0.3095402849309991], 
reward next is 0.6905, 
noisyNet noise sample is [array([-1.6024238], dtype=float32), 0.7472316]. 
=============================================
[2019-03-27 00:27:13,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2186620e-22 1.0000000e+00 1.9509827e-26 1.4444298e-21 8.9203568e-29], sum to 1.0000
[2019-03-27 00:27:13,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7862
[2019-03-27 00:27:13,092] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.03333333333333, 77.33333333333334, 1.0, 2.0, 0.5760035941345235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804915.7691199516, 804915.7691199511, 196360.841515998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2319600.0000, 
sim time next is 2320200.0000, 
raw observation next is [29.95, 77.5, 1.0, 2.0, 0.5752929894162296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803922.383998383, 803922.383998383, 196233.4834917669], 
processed observation next is [1.0, 0.8695652173913043, 0.6184834123222749, 0.775, 1.0, 1.0, 0.48830480652557784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22331177333288416, 0.22331177333288416, 0.2928857962563685], 
reward next is 0.7071, 
noisyNet noise sample is [array([-1.9136068], dtype=float32), -0.0007278108]. 
=============================================
[2019-03-27 00:27:13,299] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.7304390e-23 1.0000000e+00 1.1955419e-24 2.4176468e-20 9.7749631e-28], sum to 1.0000
[2019-03-27 00:27:13,310] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0722
[2019-03-27 00:27:13,316] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 78.0, 1.0, 2.0, 0.5704048126491973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797089.0150994578, 797089.0150994578, 195362.0085995898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [29.6, 78.33333333333333, 1.0, 2.0, 0.5689007990590577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794986.5059331937, 794986.5059331937, 195095.3099613687], 
processed observation next is [1.0, 0.9130434782608695, 0.6018957345971565, 0.7833333333333333, 1.0, 1.0, 0.4806033723603104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2208295849814427, 0.2208295849814427, 0.29118702979308764], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.5050289], dtype=float32), -0.09191421]. 
=============================================
[2019-03-27 00:27:15,098] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 00:27:15,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:27:15,100] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:27:15,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:27:15,102] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:27:15,103] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:27:15,102] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:27:15,105] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:27:15,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:27:15,105] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:27:15,108] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:27:15,123] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-27 00:27:15,123] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-27 00:27:15,123] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-27 00:27:15,178] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-27 00:27:15,178] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-27 00:27:18,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:27:18,479] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.8, 96.0, 1.0, 2.0, 0.3263997564430149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515268.4718229907, 515268.4718229913, 168168.4146442934]
[2019-03-27 00:27:18,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:27:18,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5359805e-20 1.0000000e+00 4.5003271e-23 7.0770562e-18 3.0156525e-24], sampled 0.5740721832153974
[2019-03-27 00:27:24,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:27:24,868] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.3, 82.0, 1.0, 2.0, 0.3152690997967275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500933.0249660626, 500933.024966062, 167135.6255174932]
[2019-03-27 00:27:24,869] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:27:24,872] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5712436e-20 1.0000000e+00 3.9273819e-23 5.2618104e-18 2.9782006e-24], sampled 0.7798775055513589
[2019-03-27 00:27:32,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:27:32,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.25, 92.66666666666666, 1.0, 2.0, 0.2809421102786107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 452812.3938076269, 452812.3938076269, 163788.0986402168]
[2019-03-27 00:27:32,443] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:27:32,447] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9723567e-19 1.0000000e+00 4.0379647e-22 7.5556808e-17 3.5269886e-23], sampled 0.20478678412496854
[2019-03-27 00:27:41,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:27:41,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.4, 94.0, 1.0, 2.0, 0.4600946989538006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649294.4116061226, 649294.4116061226, 178406.6916801751]
[2019-03-27 00:27:41,076] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:27:41,080] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.1735622e-19 1.0000000e+00 6.8917746e-22 2.0248455e-16 4.8331034e-23], sampled 0.6988276159621566
[2019-03-27 00:28:05,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:28:05,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.16768281333333, 79.69077941333335, 1.0, 2.0, 0.7136574784187956, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.946169259168265, 6.9112, 168.9122649895067, 1918258.558018374, 1893450.263579683, 392384.9373898586]
[2019-03-27 00:28:05,347] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:28:05,350] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2922060e-17 1.0000000e+00 5.1012590e-20 3.4352865e-15 2.0101331e-21], sampled 0.6960520413813324
[2019-03-27 00:28:05,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1918258.558018374 W.
[2019-03-27 00:28:12,749] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.018286152]
[2019-03-27 00:28:12,749] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.60329850666666, 84.25136790666667, 1.0, 2.0, 0.8657280757403274, 1.0, 1.0, 0.8657280757403274, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2421312.128420465, 2421312.128420464, 453645.5962648742]
[2019-03-27 00:28:12,751] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:28:12,753] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1704878e-15 1.0000000e+00 2.9698596e-18 4.9399394e-11 1.9216365e-19], sampled 0.9894284360295039
[2019-03-27 00:28:12,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2421312.128420465 W.
[2019-03-27 00:29:09,092] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 00:29:09,293] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0344 2842500170.3949 1131.0000
[2019-03-27 00:29:09,299] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2953 3007596012.4260 1766.0000
[2019-03-27 00:29:09,341] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 00:29:09,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 00:29:10,487] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 275000, evaluation results [275000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.295259250187, 3007596012.4259605, 1766.0, 8496.034361333246, 2842500170.394918, 1131.0]
[2019-03-27 00:29:15,720] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2377725e-15 1.0000000e+00 1.4518068e-18 1.0670498e-11 1.1350416e-18], sum to 1.0000
[2019-03-27 00:29:15,728] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5765
[2019-03-27 00:29:15,733] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.33333333333334, 1.0, 2.0, 0.7423467209443687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1037479.601694007, 1037479.601694007, 230251.8085865166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2445600.0000, 
sim time next is 2446200.0000, 
raw observation next is [27.7, 85.5, 1.0, 2.0, 0.7473744893047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044509.702720558, 1044509.702720558, 231403.9013687684], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.855, 1.0, 1.0, 0.6956319148250139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2901415840890439, 0.2901415840890439, 0.3453789572668185], 
reward next is 0.6546, 
noisyNet noise sample is [array([-0.6469049], dtype=float32), 0.5742935]. 
=============================================
[2019-03-27 00:29:19,718] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0748672e-18 1.0000000e+00 8.2829997e-21 5.0129750e-16 1.2652622e-22], sum to 1.0000
[2019-03-27 00:29:19,727] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1151
[2019-03-27 00:29:19,734] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 95.0, 1.0, 2.0, 0.5444001895171413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760736.8987499457, 760736.8987499464, 190845.0597908571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2512800.0000, 
sim time next is 2513400.0000, 
raw observation next is [26.38333333333333, 95.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.359698216321023, 6.9112, 169.6265963146485, 1773492.994362928, 1453968.645634479, 311482.866855632], 
processed observation next is [1.0, 0.08695652173913043, 0.44944707740916257, 0.9516666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04484982163210231, 0.0, 0.8329442432967267, 0.4926369428785911, 0.4038801793429108, 0.46489980127706265], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.62359923], dtype=float32), -0.96167165]. 
=============================================
[2019-03-27 00:29:25,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6539289e-20 1.0000000e+00 2.3067539e-24 1.7050453e-16 4.8899413e-24], sum to 1.0000
[2019-03-27 00:29:25,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7286
[2019-03-27 00:29:25,348] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 92.0, 1.0, 2.0, 0.4326088736147158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627353.6987812736, 627353.6987812736, 176645.1972888044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2605800.0000, 
sim time next is 2606400.0000, 
raw observation next is [24.0, 92.0, 1.0, 2.0, 0.4316987058440918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626446.7788350862, 626446.7788350862, 176567.4866850642], 
processed observation next is [0.0, 0.17391304347826086, 0.3364928909952607, 0.92, 1.0, 1.0, 0.3152996455952914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17401299412085727, 0.17401299412085727, 0.26353356221651375], 
reward next is 0.7365, 
noisyNet noise sample is [array([-1.0131612], dtype=float32), -0.4220542]. 
=============================================
[2019-03-27 00:29:25,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4179974e-20 1.0000000e+00 1.7625543e-24 3.9735697e-16 9.4508426e-25], sum to 1.0000
[2019-03-27 00:29:25,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2867
[2019-03-27 00:29:25,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 92.0, 1.0, 2.0, 0.4346233721842535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629440.7439681643, 629440.7439681637, 176828.0432622833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2604600.0000, 
sim time next is 2605200.0000, 
raw observation next is [24.03333333333333, 92.0, 1.0, 2.0, 0.4335841862253737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628352.3139503202, 628352.3139503202, 176732.0910779584], 
processed observation next is [0.0, 0.13043478260869565, 0.3380726698262243, 0.92, 1.0, 1.0, 0.31757130870526956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1745423094306445, 0.1745423094306445, 0.2637792404148633], 
reward next is 0.7362, 
noisyNet noise sample is [array([-1.150377], dtype=float32), 0.44878876]. 
=============================================
[2019-03-27 00:29:32,047] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7972673e-19 1.0000000e+00 9.5434065e-25 6.6282777e-17 1.9540971e-25], sum to 1.0000
[2019-03-27 00:29:32,057] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0898
[2019-03-27 00:29:32,062] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4300009904569671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624140.126048276, 624140.1260482754, 176346.2269402324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.428758649458126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622337.012059926, 622337.0120599266, 176170.4536607454], 
processed observation next is [0.0, 0.43478260869565216, 0.28909952606635075, 1.0, 1.0, 1.0, 0.311757408985694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17287139223886835, 0.17287139223886852, 0.26294097561305285], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.6877106], dtype=float32), -0.06789258]. 
=============================================
[2019-03-27 00:29:32,761] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0194017e-20 1.0000000e+00 2.7674449e-24 3.9688282e-16 6.0775339e-24], sum to 1.0000
[2019-03-27 00:29:32,769] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5837
[2019-03-27 00:29:32,776] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3939518268025217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587835.3044585264, 587835.3044585264, 173419.3759234499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2740200.0000, 
sim time next is 2740800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3940222073388642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587940.3788081195, 587940.3788081189, 173428.985432352], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26990627390224603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1633167718911443, 0.16331677189114413, 0.25884923198858506], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.3544531], dtype=float32), 1.9573581]. 
=============================================
[2019-03-27 00:29:36,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6187127e-17 1.0000000e+00 1.1107697e-22 2.3423402e-15 4.8440326e-22], sum to 1.0000
[2019-03-27 00:29:36,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8973
[2019-03-27 00:29:36,438] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3434163734979812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531381.400996574, 531381.400996574, 169184.0196654944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.346177024953328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534923.2906912548, 534923.2906912548, 169449.9368409843], 
processed observation next is [1.0, 0.17391304347826086, 0.22590837282780438, 0.96, 1.0, 1.0, 0.21226147584738317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.148589802969793, 0.148589802969793, 0.2529103534940064], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.3317923], dtype=float32), 0.9137404]. 
=============================================
[2019-03-27 00:29:43,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4954299e-19 1.0000000e+00 1.0600333e-22 1.2122090e-14 1.6185473e-23], sum to 1.0000
[2019-03-27 00:29:43,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1091
[2019-03-27 00:29:43,655] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 99.0, 1.0, 2.0, 0.308800462644727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491248.2889387395, 491248.2889387395, 166428.6719962212], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2924400.0000, 
sim time next is 2925000.0000, 
raw observation next is [20.25, 98.5, 1.0, 2.0, 0.3098181933047057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492572.1679135509, 492572.1679135509, 166521.3419519773], 
processed observation next is [1.0, 0.8695652173913043, 0.1587677725118484, 0.985, 1.0, 1.0, 0.1684556545839828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13682560219820858, 0.13682560219820858, 0.24853931634623475], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.14128156], dtype=float32), -2.029256]. 
=============================================
[2019-03-27 00:29:43,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.03249 ]
 [74.957016]
 [74.88009 ]
 [74.71982 ]
 [74.61903 ]], R is [[75.09670258]
 [75.09733582]
 [75.09798431]
 [75.09862518]
 [75.09905243]].
[2019-03-27 00:29:53,426] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9325982e-16 1.0000000e+00 3.0506941e-20 6.2109627e-13 6.5377396e-20], sum to 1.0000
[2019-03-27 00:29:53,428] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6607
[2019-03-27 00:29:53,435] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8859886559846346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287727.92825234, 1287727.928252339, 273280.8439283373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3074400.0000, 
sim time next is 3075000.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.8404968436560365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1219732.275116041, 1219732.275116041, 260476.2360193947], 
processed observation next is [1.0, 0.6086956521739131, 0.28909952606635075, 1.0, 1.0, 1.0, 0.8078275224771524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33881452086556696, 0.33881452086556696, 0.3887705015214846], 
reward next is 0.6112, 
noisyNet noise sample is [array([-0.11000296], dtype=float32), 1.8380767]. 
=============================================
[2019-03-27 00:29:53,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[59.694218]
 [59.932056]
 [60.27118 ]
 [60.69343 ]
 [61.382847]], R is [[59.78726196]
 [59.78150558]
 [59.78092957]
 [59.78764725]
 [59.79973221]].
[2019-03-27 00:30:04,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.97833315e-23 1.00000000e+00 3.29251745e-26 1.90087229e-19
 1.11417944e-26], sum to 1.0000
[2019-03-27 00:30:04,155] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5849
[2019-03-27 00:30:04,159] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4228097815742998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619072.0669224252, 619072.0669224246, 176004.9039168248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3299400.0000, 
sim time next is 3300000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4225399448233598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618677.5883841622, 618677.5883841629, 175967.0038830437], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 1.0, 1.0, 0.30426499376308414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17185488566226728, 0.17185488566226748, 0.2626373192284234], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.57525796], dtype=float32), -0.13645104]. 
=============================================
[2019-03-27 00:30:04,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.58887]
 [74.67707]
 [74.72898]
 [74.68383]
 [74.69149]], R is [[74.42559052]
 [74.41864014]
 [74.41166687]
 [74.40457153]
 [74.39716339]].
[2019-03-27 00:30:06,134] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 00:30:06,136] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:30:06,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:30:06,139] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:30:06,139] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:30:06,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:30:06,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:30:06,143] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:30:06,144] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:30:06,144] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:30:06,146] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:30:06,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-27 00:30:06,185] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-27 00:30:06,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-27 00:30:06,187] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-27 00:30:06,255] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-27 00:30:50,841] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:30:50,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.25560858666667, 100.0, 1.0, 2.0, 0.3390655967760194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524668.4527550468, 524668.4527550461, 168645.9655794528]
[2019-03-27 00:30:50,843] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:30:50,847] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.03624185e-19 1.00000000e+00 8.01061744e-24 7.30577415e-18
 6.94250924e-24], sampled 0.9388057377367283
[2019-03-27 00:30:56,483] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:30:56,485] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.50328127333333, 77.74691101333333, 1.0, 2.0, 0.5323449673210711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743885.2080853801, 743885.2080853801, 188818.8147019089]
[2019-03-27 00:30:56,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:30:56,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2129732e-20 1.0000000e+00 6.9859416e-25 4.6788598e-19 3.1398626e-25], sampled 0.961171693454102
[2019-03-27 00:31:01,015] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:01,019] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5172292827914047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722755.7521721631, 722755.7521721631, 186339.6530535103]
[2019-03-27 00:31:01,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:31:01,023] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0358109e-20 1.0000000e+00 3.6437931e-24 2.2249926e-18 1.5362896e-24], sampled 0.38690386429686996
[2019-03-27 00:31:06,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:06,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.086401085, 85.13102502000001, 1.0, 2.0, 0.5557798274276109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776644.4525014764, 776644.4525014758, 192796.9141931242]
[2019-03-27 00:31:06,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:31:06,164] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6612923e-21 1.0000000e+00 7.5494808e-26 4.5078083e-20 2.3445798e-26], sampled 0.25762090669035964
[2019-03-27 00:31:06,409] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:06,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.23180915, 85.01884099, 1.0, 2.0, 0.6635405165924305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 927294.5326877388, 927294.5326877388, 213182.2599532481]
[2019-03-27 00:31:06,412] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:31:06,414] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.9655347e-20 1.0000000e+00 6.1533273e-24 3.6145655e-18 2.4630038e-24], sampled 0.307756580349952
[2019-03-27 00:31:17,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:17,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5190113034278284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725246.7274278646, 725246.7274278653, 186628.4037743669]
[2019-03-27 00:31:17,494] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:31:17,495] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5770257e-23 1.0000000e+00 1.1192695e-27 6.3397795e-22 1.3920974e-28], sampled 0.6973981254755303
[2019-03-27 00:31:24,236] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:24,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.50824568, 86.76960038, 1.0, 2.0, 0.6514336249160803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 910367.9610338407, 910367.9610338407, 210720.2424339481]
[2019-03-27 00:31:24,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:31:24,243] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.5454371e-22 1.0000000e+00 4.8583093e-26 1.5175257e-20 1.0397424e-26], sampled 0.5429030767424435
[2019-03-27 00:31:47,204] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:47,204] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.7, 76.0, 1.0, 2.0, 0.5926015403547696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828119.0215132664, 828119.0215132664, 199375.9503110467]
[2019-03-27 00:31:47,206] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:31:47,209] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.2466975e-21 1.0000000e+00 4.1760497e-25 4.0902941e-19 1.3222132e-25], sampled 0.9981818954896343
[2019-03-27 00:31:51,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.015649306]
[2019-03-27 00:31:51,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.86509924, 95.81580578, 1.0, 2.0, 0.3899357872257898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583628.06632831, 583628.0663283094, 173091.6894298214]
[2019-03-27 00:31:51,531] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:31:51,535] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1605712e-19 1.0000000e+00 9.5718126e-24 7.5949772e-18 7.1465126e-24], sampled 0.9098785286739985
[2019-03-27 00:31:58,726] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 00:32:00,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:32:00,244] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1684 3164071917.9177 1778.0000
[2019-03-27 00:32:00,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7746 2842563444.1824 1131.0000
[2019-03-27 00:32:00,386] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:32:01,400] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 300000, evaluation results [300000.0, 7884.168443086554, 3164071917.9176674, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.774567725599, 2842563444.1823883, 1131.0]
[2019-03-27 00:32:02,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7621194e-16 1.0000000e+00 2.3872314e-19 1.3601971e-13 1.7911006e-19], sum to 1.0000
[2019-03-27 00:32:02,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6201
[2019-03-27 00:32:02,093] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.8525567548986385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191591.978612995, 1191591.978612995, 257219.8526596784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [26.33333333333334, 92.33333333333334, 1.0, 2.0, 0.8219963233802569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1148855.509134595, 1148855.509134595, 249381.2279087122], 
processed observation next is [1.0, 0.21739130434782608, 0.44707740916271754, 0.9233333333333335, 1.0, 1.0, 0.7855377390123577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3191265303151653, 0.3191265303151653, 0.372210787923451], 
reward next is 0.6278, 
noisyNet noise sample is [array([0.691919], dtype=float32), 0.46812764]. 
=============================================
[2019-03-27 00:32:03,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6670121e-21 1.0000000e+00 3.0651261e-26 1.6115009e-21 1.0368702e-27], sum to 1.0000
[2019-03-27 00:32:03,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8123
[2019-03-27 00:32:03,133] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5350653751139779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747687.9740799333, 747687.9740799333, 189272.5834599001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3362400.0000, 
sim time next is 3363000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4419344861785306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20836683766327363, 0.20836683766327346, 0.2829308381689719], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.8045235], dtype=float32), -0.6818353]. 
=============================================
[2019-03-27 00:32:03,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.53466 ]
 [76.5052  ]
 [76.516685]
 [76.52949 ]
 [76.56179 ]], R is [[76.47081757]
 [76.4236145 ]
 [76.37763214]
 [76.33275604]
 [76.28878021]].
[2019-03-27 00:32:07,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.2119984e-14 1.0000000e+00 5.9189273e-17 6.6683464e-10 5.5575581e-18], sum to 1.0000
[2019-03-27 00:32:07,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3830
[2019-03-27 00:32:07,272] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.8525567548986385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191591.978612995, 1191591.978612995, 257219.8526596784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [26.33333333333334, 92.33333333333334, 1.0, 2.0, 0.8219963233802569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1148855.509134595, 1148855.509134595, 249381.2279087122], 
processed observation next is [1.0, 0.21739130434782608, 0.44707740916271754, 0.9233333333333335, 1.0, 1.0, 0.7855377390123577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3191265303151653, 0.3191265303151653, 0.372210787923451], 
reward next is 0.6278, 
noisyNet noise sample is [array([-0.7478735], dtype=float32), 0.83588636]. 
=============================================
[2019-03-27 00:32:10,924] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5522479e-12 9.9999654e-01 2.2497883e-17 3.4452119e-06 2.5626450e-16], sum to 1.0000
[2019-03-27 00:32:10,932] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6859
[2019-03-27 00:32:10,936] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5023997800195541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702026.7437834226, 702026.7437834232, 183972.8808807829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3462600.0000, 
sim time next is 3463200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5016900063401247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701034.6165113848, 701034.6165113841, 183861.1716263127], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39962651366280083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19473183791982912, 0.19473183791982893, 0.27441965914375027], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.6072603], dtype=float32), -1.295408]. 
=============================================
[2019-03-27 00:32:12,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0272960e-13 9.9999702e-01 5.8897642e-17 2.9682381e-06 4.3514822e-16], sum to 1.0000
[2019-03-27 00:32:12,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2420
[2019-03-27 00:32:12,972] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 78.16666666666667, 1.0, 2.0, 0.8500554522355913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1188094.02996442, 1188094.02996442, 256566.9197807291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [28.0, 77.33333333333334, 1.0, 2.0, 0.855749390220978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196056.738696544, 1196056.738696544, 258053.4722672796], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.7733333333333334, 1.0, 1.0, 0.8262040846035879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3322379829712622, 0.3322379829712622, 0.3851544362198203], 
reward next is 0.6148, 
noisyNet noise sample is [array([1.3551825], dtype=float32), 1.0679195]. 
=============================================
[2019-03-27 00:32:16,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3625750e-16 1.0000000e+00 4.3659166e-21 3.9869524e-13 5.7495192e-20], sum to 1.0000
[2019-03-27 00:32:16,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8431
[2019-03-27 00:32:16,913] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.494663753188603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691213.3141815951, 691213.3141815951, 182763.8815167615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3546000.0000, 
sim time next is 3546600.0000, 
raw observation next is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4918174229934308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687234.7373787031, 687234.7373787038, 182323.7222424673], 
processed observation next is [1.0, 0.043478260869565216, 0.5181674565560824, 0.7483333333333334, 1.0, 1.0, 0.38773183493184427, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19089853816075086, 0.19089853816075106, 0.27212495857084673], 
reward next is 0.7279, 
noisyNet noise sample is [array([0.0569937], dtype=float32), 2.13627]. 
=============================================
[2019-03-27 00:32:18,550] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1475863e-17 1.0000000e+00 3.6675334e-21 3.5986390e-15 1.7732733e-21], sum to 1.0000
[2019-03-27 00:32:18,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0393
[2019-03-27 00:32:18,564] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5234008158471993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731382.5773714219, 731382.5773714213, 187343.8294250153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5245965319221539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733054.0070592929, 733054.0070592923, 187539.6007289533], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4272247372556071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2036261130720258, 0.20362611307202563, 0.27990985183425865], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.22347774], dtype=float32), 1.2373827]. 
=============================================
[2019-03-27 00:32:18,620] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5943589e-11 4.3240812e-02 8.7928627e-14 9.5675915e-01 3.2097751e-13], sum to 1.0000
[2019-03-27 00:32:18,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8933
[2019-03-27 00:32:18,634] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.66666666666667, 1.0, 2.0, 0.910375626292744, 1.0, 2.0, 0.910375626292744, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2546344.790225822, 2546344.790225822, 477183.2714978341], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3595800.0000, 
sim time next is 3596400.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.9052683097551107, 1.0, 2.0, 0.9052683097551107, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2532045.015845256, 2532045.015845256, 474372.6451921837], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.8858654334398924, 1.0, 1.0, 0.8858654334398924, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7033458377347933, 0.7033458377347933, 0.7080188734211698], 
reward next is 0.2920, 
noisyNet noise sample is [array([0.34282982], dtype=float32), -1.0414271]. 
=============================================
[2019-03-27 00:32:30,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6136891e-23 1.0000000e+00 1.2367032e-28 9.5778032e-24 4.2134272e-26], sum to 1.0000
[2019-03-27 00:32:30,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5812
[2019-03-27 00:32:30,662] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3869400.0000, 
sim time next is 3870000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5719960276763714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 799313.430501277, 799313.4305012776, 195645.613357322], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4843325634655076, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22203150847257694, 0.2220315084725771, 0.2920083781452567], 
reward next is 0.7080, 
noisyNet noise sample is [array([-0.56613445], dtype=float32), 0.23377335]. 
=============================================
[2019-03-27 00:32:30,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.00366]
 [73.00938]
 [72.98666]
 [72.95046]
 [72.9074 ]], R is [[73.00228119]
 [72.98236084]
 [72.9644165 ]
 [72.94815063]
 [72.93352509]].
[2019-03-27 00:32:31,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9586457e-23 1.0000000e+00 2.5099610e-26 8.1746722e-21 1.8404502e-26], sum to 1.0000
[2019-03-27 00:32:31,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2784
[2019-03-27 00:32:31,370] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5166814000258863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721989.9022757778, 721989.9022757784, 186251.0924673056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3808800.0000, 
sim time next is 3809400.0000, 
raw observation next is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.5187210980084552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724841.0670143329, 724841.0670143323, 186581.4908043624], 
processed observation next is [0.0, 0.08695652173913043, 0.5181674565560824, 0.8066666666666668, 1.0, 1.0, 0.4201459012150063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20134474083731468, 0.20134474083731452, 0.2784798370214364], 
reward next is 0.7215, 
noisyNet noise sample is [array([1.3543072], dtype=float32), 0.8226482]. 
=============================================
[2019-03-27 00:32:37,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2163654e-21 1.0000000e+00 1.4052140e-25 7.2097765e-21 7.7007640e-25], sum to 1.0000
[2019-03-27 00:32:37,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3146
[2019-03-27 00:32:37,076] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5829586620148277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814638.6088747102, 814638.6088747096, 197613.973473719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3885600.0000, 
sim time next is 3886200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5821198693267234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813466.0136017958, 813466.0136017958, 197462.0988921315], 
processed observation next is [0.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49652996304424507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22596278155605437, 0.22596278155605437, 0.2947195505852709], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.7343128], dtype=float32), 0.692638]. 
=============================================
[2019-03-27 00:32:43,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8630033e-11 9.9996436e-01 8.0841117e-13 3.5627188e-05 5.7244959e-13], sum to 1.0000
[2019-03-27 00:32:43,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7934
[2019-03-27 00:32:43,419] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.8993734095448372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1257064.892096279, 1257064.89209628, 269768.7844799874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3996000.0000, 
sim time next is 3996600.0000, 
raw observation next is [29.16666666666667, 84.0, 1.0, 2.0, 0.9836277700152309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1374904.467449562, 1374904.467449563, 293981.007786133], 
processed observation next is [1.0, 0.2608695652173913, 0.581358609794629, 0.84, 1.0, 1.0, 0.9802744217050975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3819179076248783, 0.38191790762487865, 0.4387776235613925], 
reward next is 0.5612, 
noisyNet noise sample is [array([-1.3645619], dtype=float32), -1.6542709]. 
=============================================
[2019-03-27 00:32:46,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7626974e-22 1.0000000e+00 2.8829711e-25 2.2192080e-21 1.9484188e-24], sum to 1.0000
[2019-03-27 00:32:46,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2434
[2019-03-27 00:32:46,530] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5449808257091711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761548.5621678563, 761548.5621678557, 190943.7741778573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5452465447021885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761920.0073329152, 761920.0073329152, 190988.9409948862], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4521042707255283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21164444648136532, 0.21164444648136532, 0.28505812088788984], 
reward next is 0.7149, 
noisyNet noise sample is [array([0.8792311], dtype=float32), -0.18588573]. 
=============================================
[2019-03-27 00:32:46,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[60.971478]
 [61.115864]
 [61.189594]
 [61.87945 ]
 [62.239986]], R is [[60.69757843]
 [60.80561066]
 [60.91220856]
 [61.01737976]
 [61.12115479]].
[2019-03-27 00:32:48,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3185724e-12 9.9999404e-01 2.6348230e-16 6.0051320e-06 1.5959764e-14], sum to 1.0000
[2019-03-27 00:32:48,520] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0088
[2019-03-27 00:32:48,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2609783.713728781 W.
[2019-03-27 00:32:48,534] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 75.0, 1.0, 2.0, 0.6220218718782308, 1.0, 2.0, 0.6220218718782308, 1.0, 1.0, 1.03, 6.967686521353075, 6.9112, 170.5573041426782, 2609783.713728781, 2569320.120758792, 496102.7223024963], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4098600.0000, 
sim time next is 4099200.0000, 
raw observation next is [31.66666666666666, 73.66666666666666, 1.0, 2.0, 0.9673973573219534, 1.0, 2.0, 0.9673973573219534, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2706008.80362324, 2706008.80362324, 509602.3940500843], 
processed observation next is [1.0, 0.43478260869565216, 0.6998420221169034, 0.7366666666666666, 1.0, 1.0, 0.9607197076168114, 1.0, 1.0, 0.9607197076168114, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7516691121175667, 0.7516691121175667, 0.7606005881344542], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0521811], dtype=float32), -1.9497563]. 
=============================================
[2019-03-27 00:32:51,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8806279e-19 1.0000000e+00 5.6347571e-23 1.7824130e-16 3.8627068e-22], sum to 1.0000
[2019-03-27 00:32:51,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9162
[2019-03-27 00:32:51,727] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.5806682511980742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811436.7183846717, 811436.7183846717, 197199.6546098688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4147800.0000, 
sim time next is 4148400.0000, 
raw observation next is [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5785132676295686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808424.1578612714, 808424.1578612714, 196811.2420558016], 
processed observation next is [1.0, 0.0, 0.5576619273301741, 0.8566666666666667, 1.0, 1.0, 0.4921846597946609, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2245622660725754, 0.2245622660725754, 0.29374812247134563], 
reward next is 0.7063, 
noisyNet noise sample is [array([-1.4445776], dtype=float32), -2.2303913]. 
=============================================
[2019-03-27 00:32:54,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5892104e-08 2.9160762e-01 4.4861081e-12 7.0839232e-01 7.6341050e-10], sum to 1.0000
[2019-03-27 00:32:54,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6802
[2019-03-27 00:32:54,532] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.83333333333333, 72.33333333333334, 1.0, 2.0, 1.006691114379357, 1.0, 2.0, 1.006691114379357, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2816045.38200189, 2816045.38200189, 533053.7163115365], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4179000.0000, 
sim time next is 4179600.0000, 
raw observation next is [33.0, 71.0, 1.0, 2.0, 0.9891688472211839, 1.0, 2.0, 0.9891688472211839, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2766975.567434217, 2766975.567434216, 522486.2267635362], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.71, 1.0, 1.0, 0.9869504183387758, 1.0, 1.0, 0.9869504183387758, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7686043242872824, 0.7686043242872822, 0.7798301891993078], 
reward next is 0.2202, 
noisyNet noise sample is [array([1.3365797], dtype=float32), -0.62695014]. 
=============================================
[2019-03-27 00:32:57,235] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 00:32:57,237] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:32:57,239] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:32:57,240] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:32:57,241] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:32:57,243] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:32:57,243] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:32:57,244] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:32:57,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:32:57,247] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:32:57,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:32:57,260] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-27 00:32:57,282] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-27 00:32:57,308] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-27 00:32:57,310] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-27 00:32:57,343] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-27 00:33:48,404] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.016660206]
[2019-03-27 00:33:48,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.76666666666667, 50.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.398227720025437, 6.9112, 168.9098957208874, 1799499.542568397, 1453991.576677149, 311354.1002021267]
[2019-03-27 00:33:48,406] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:33:48,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9802634e-18 1.0000000e+00 3.3508610e-22 2.7800050e-16 8.3148155e-21], sampled 0.08221986751063393
[2019-03-27 00:33:48,410] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1799499.542568397 W.
[2019-03-27 00:33:52,789] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.016660206]
[2019-03-27 00:33:52,791] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [38.12925471, 63.62299231000001, 1.0, 2.0, 0.8231433609377523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1150459.523988485, 1150459.523988486, 249688.7173187104]
[2019-03-27 00:33:52,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:33:52,794] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6707105e-21 1.0000000e+00 4.4247075e-25 2.3722424e-20 1.6704734e-23], sampled 0.011858218519186892
[2019-03-27 00:34:48,046] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.016660206]
[2019-03-27 00:34:48,048] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.98548662, 97.93924975, 1.0, 2.0, 0.5105779785199619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713458.3612855441, 713458.3612855435, 185269.7590077122]
[2019-03-27 00:34:48,050] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:34:48,054] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9968956e-21 1.0000000e+00 3.7839025e-25 1.1347201e-20 1.3394243e-23], sampled 0.4181603214586519
[2019-03-27 00:34:50,406] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6842 2927263131.0635 1338.0000
[2019-03-27 00:34:50,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4166 2842558242.3972 1131.0000
[2019-03-27 00:34:51,064] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-03-27 00:34:51,246] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 00:34:51,358] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:34:52,374] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 325000, evaluation results [325000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8253.684238692544, 2927263131.0635424, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.416582222564, 2842558242.397172, 1131.0]
[2019-03-27 00:34:53,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8187321e-09 9.9784124e-01 1.8376396e-12 2.1587857e-03 6.0150225e-11], sum to 1.0000
[2019-03-27 00:34:53,875] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7106
[2019-03-27 00:34:53,876] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2523864.056303533 W.
[2019-03-27 00:34:53,888] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666667, 81.0, 1.0, 2.0, 0.601564244494629, 1.0, 1.0, 0.601564244494629, 1.0, 2.0, 1.03, 6.927745475162372, 6.9112, 170.5573041426782, 2523864.056303533, 2512011.859417827, 488548.6091730649], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4263600.0000, 
sim time next is 4264200.0000, 
raw observation next is [32.0, 79.5, 1.0, 2.0, 1.013862126170973, 1.0, 2.0, 1.013862126170973, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2836127.826547209, 2836127.82654721, 537433.1432005098], 
processed observation next is [1.0, 0.34782608695652173, 0.7156398104265403, 0.795, 1.0, 1.0, 1.0167013568324976, 1.0, 1.0, 1.0167013568324976, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7878132851520026, 0.7878132851520028, 0.8021390197022534], 
reward next is 0.1979, 
noisyNet noise sample is [array([-1.1840869], dtype=float32), -0.40062585]. 
=============================================
[2019-03-27 00:34:54,791] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2427448e-09 7.8148216e-01 2.3115160e-12 2.1851778e-01 1.1816488e-10], sum to 1.0000
[2019-03-27 00:34:54,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9746
[2019-03-27 00:34:54,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3464710.536497538 W.
[2019-03-27 00:34:54,809] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.33333333333334, 59.00000000000001, 1.0, 2.0, 1.009829658931219, 1.0, 2.0, 0.8255048689798717, 1.0, 1.0, 1.03, 7.005122174663614, 6.9112, 170.5573041426782, 3464710.536497538, 3397430.262276194, 637076.1582135517], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4274400.0000, 
sim time next is 4275000.0000, 
raw observation next is [36.5, 58.5, 1.0, 2.0, 0.8800319353316829, 1.0, 2.0, 0.7606060071801042, 1.0, 2.0, 1.03, 7.005111931802603, 6.9112, 170.5573041426782, 3191976.632473479, 3124703.69563013, 584226.5131010239], 
processed observation next is [1.0, 0.4782608695652174, 0.9289099526066351, 0.585, 1.0, 1.0, 0.8554601630502203, 1.0, 1.0, 0.7115735026266315, 1.0, 1.0, 1.0365853658536586, 0.009391193180260337, 0.0, 0.8375144448122397, 0.8866601756870776, 0.8679732487861472, 0.8719798703000357], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1108458], dtype=float32), 0.683531]. 
=============================================
[2019-03-27 00:34:54,828] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[25.788649]
 [26.981672]
 [27.512999]
 [28.223522]
 [27.999191]], R is [[26.35800743]
 [26.09442711]
 [25.83348274]
 [25.57514763]
 [25.31939697]].
[2019-03-27 00:34:57,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2133342e-20 1.0000000e+00 4.2524273e-24 4.2199205e-21 3.8089022e-23], sum to 1.0000
[2019-03-27 00:34:57,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4800
[2019-03-27 00:34:57,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6213418748053967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868298.0366210239, 868298.0366210239, 204789.8230371206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4314000.0000, 
sim time next is 4314600.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6212517794063316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868172.0807918498, 868172.0807918498, 204772.4759817558], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5436768426582308, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24115891133106937, 0.24115891133106937, 0.3056305611667997], 
reward next is 0.6944, 
noisyNet noise sample is [array([0.3589229], dtype=float32), 0.24119565]. 
=============================================
[2019-03-27 00:35:01,234] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.28826804e-17 1.00000000e+00 2.56654343e-19 5.47267511e-16
 6.45241906e-17], sum to 1.0000
[2019-03-27 00:35:01,241] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9238
[2019-03-27 00:35:01,247] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 65.0, 1.0, 2.0, 0.5286764970710881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738757.1992219356, 738757.1992219363, 188215.4128288801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4383000.0000, 
sim time next is 4383600.0000, 
raw observation next is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5435444189643169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759540.6298902467, 759540.6298902467, 190702.9414442459], 
processed observation next is [1.0, 0.7391304347826086, 0.7314375987361774, 0.6566666666666667, 1.0, 1.0, 0.4500535168244782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2109835083028463, 0.2109835083028463, 0.28463125588693416], 
reward next is 0.7154, 
noisyNet noise sample is [array([1.3832072], dtype=float32), -0.9364119]. 
=============================================
[2019-03-27 00:35:03,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0000789e-23 1.0000000e+00 7.2277286e-29 8.2884826e-25 5.9018641e-23], sum to 1.0000
[2019-03-27 00:35:03,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6479
[2019-03-27 00:35:03,456] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5856808067897477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818444.0564191804, 818444.0564191804, 198108.2894874195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4421400.0000, 
sim time next is 4422000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5853686592339303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818007.6861553146, 818007.6861553151, 198051.4949600791], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5004441677517233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22722435726536516, 0.22722435726536533, 0.29559924620907324], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.03583286], dtype=float32), -1.4437973]. 
=============================================
[2019-03-27 00:35:03,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.810394]
 [72.96353 ]
 [72.91765 ]
 [72.74785 ]
 [72.63148 ]], R is [[72.51084137]
 [72.49005127]
 [72.46930695]
 [72.44756317]
 [72.42485046]].
[2019-03-27 00:35:05,071] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5851261e-26 1.0000000e+00 2.2116511e-30 1.1870827e-27 2.8819594e-25], sum to 1.0000
[2019-03-27 00:35:05,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6516
[2019-03-27 00:35:05,090] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5521228776564142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771532.3899624025, 771532.3899624019, 192165.0534474425], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4479000.0000, 
sim time next is 4479600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.551201893325089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770244.9467074851, 770244.9467074844, 192006.7182166297], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45927938954829994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2139569296409681, 0.2139569296409679, 0.286577191368104], 
reward next is 0.7134, 
noisyNet noise sample is [array([-0.64936966], dtype=float32), 0.9008994]. 
=============================================
[2019-03-27 00:35:08,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4765697e-22 1.0000000e+00 6.1576886e-26 8.2434942e-23 2.4526110e-20], sum to 1.0000
[2019-03-27 00:35:08,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4262
[2019-03-27 00:35:08,166] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5079039030453599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 184843.2213397491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4501200.0000, 
sim time next is 4501800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5078265170962295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709612.3104869892, 709612.3104869892, 184830.9208811251], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40701990011593914, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19711453069083035, 0.19711453069083035, 0.27586704609123147], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.9879], dtype=float32), 0.96430844]. 
=============================================
[2019-03-27 00:35:11,285] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7786606e-28 1.0000000e+00 1.8645622e-33 1.8027453e-28 5.5405137e-27], sum to 1.0000
[2019-03-27 00:35:11,289] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6109
[2019-03-27 00:35:11,293] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 51.0, 1.0, 2.0, 0.5252597767891714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733981.1239060323, 733981.1239060316, 187649.1260269228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [34.0, 51.5, 1.0, 2.0, 0.5282437871637812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738152.332860504, 738152.332860504, 188140.407343605], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.515, 1.0, 1.0, 0.4316190206792544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20504231468347334, 0.20504231468347334, 0.2808065781247836], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.13924564], dtype=float32), -0.51223755]. 
=============================================
[2019-03-27 00:35:11,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.089066]
 [84.949135]
 [84.92393 ]
 [84.77048 ]
 [84.67601 ]], R is [[85.08226013]
 [84.95136261]
 [84.81769562]
 [84.68955231]
 [84.56116486]].
[2019-03-27 00:35:14,262] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6805632e-11 1.0000000e+00 9.3614695e-16 7.4980280e-09 2.2777377e-12], sum to 1.0000
[2019-03-27 00:35:14,273] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6229
[2019-03-27 00:35:14,280] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.9548348060375872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104219, 1334632.669241796, 1334632.669241795, 285468.5481291201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4596000.0000, 
sim time next is 4596600.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.9444198705847181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1320066.00821465, 1320066.00821465, 282450.5917637523], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.94, 1.0, 1.0, 0.9330359886562869, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3666850022818472, 0.3666850022818472, 0.4215680474085855], 
reward next is 0.5784, 
noisyNet noise sample is [array([1.0123376], dtype=float32), 0.49985817]. 
=============================================
[2019-03-27 00:35:23,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7044493e-21 1.0000000e+00 4.3695043e-26 2.7806542e-22 1.0359227e-17], sum to 1.0000
[2019-03-27 00:35:23,201] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8093
[2019-03-27 00:35:23,207] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.519705927438546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726217.7004945856, 726217.7004945861, 186741.0906508938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4743000.0000, 
sim time next is 4743600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5222056435240531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729711.910394294, 729711.9103942934, 187148.1098549811], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4243441488241603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20269775288730388, 0.2026977528873037, 0.2793255370969867], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.9336866], dtype=float32), 0.9195655]. 
=============================================
[2019-03-27 00:35:25,683] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1187939e-15 1.0000000e+00 3.3920676e-22 5.3613557e-14 1.0546354e-13], sum to 1.0000
[2019-03-27 00:35:25,694] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1147
[2019-03-27 00:35:25,699] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 78.0, 1.0, 2.0, 0.8144763231887183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138339.617053048, 1138339.617053049, 247497.0031894951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [29.66666666666666, 76.5, 1.0, 2.0, 0.813570798455795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137073.347465906, 1137073.347465906, 247271.1869853839], 
processed observation next is [1.0, 0.30434782608695654, 0.6050552922590835, 0.765, 1.0, 1.0, 0.7753865041636084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31585370762941833, 0.31585370762941833, 0.36906147311251325], 
reward next is 0.6309, 
noisyNet noise sample is [array([0.04340072], dtype=float32), -0.3557035]. 
=============================================
[2019-03-27 00:35:30,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2171628e-09 9.9246937e-01 2.3272207e-15 7.4742781e-03 5.6366236e-05], sum to 1.0000
[2019-03-27 00:35:30,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8632
[2019-03-27 00:35:30,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2191033.073093451 W.
[2019-03-27 00:35:30,880] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5223040511151092, 1.0, 2.0, 0.5223040511151092, 1.0, 2.0, 0.9049009412286088, 6.9112, 6.9112, 170.5573041426782, 2191033.073093451, 2191033.073093451, 430501.9531153713], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5471744695792197, 1.0, 2.0, 0.5471744695792197, 1.0, 2.0, 0.9482725077239458, 6.9112, 6.9112, 170.5573041426782, 2295462.052173244, 2295462.052173244, 448904.6462527548], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4544270717821923, 1.0, 1.0, 0.4544270717821923, 1.0, 1.0, 0.9369176923462753, 0.0, 0.0, 0.8375144448122397, 0.637628347825901, 0.637628347825901, 0.6700069347056041], 
reward next is 0.3300, 
noisyNet noise sample is [array([1.7572575], dtype=float32), -0.7009111]. 
=============================================
[2019-03-27 00:35:31,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1120018e-08 9.9012989e-01 2.7136857e-14 9.8107355e-03 5.9335620e-05], sum to 1.0000
[2019-03-27 00:35:31,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8768
[2019-03-27 00:35:31,152] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2077735.237091096 W.
[2019-03-27 00:35:31,162] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4953219921872968, 1.0, 2.0, 0.4953219921872968, 1.0, 2.0, 0.8575925685116226, 6.911199999999999, 6.9112, 170.5573041426782, 2077735.237091096, 2077735.237091096, 411458.9350089998], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4878600.0000, 
sim time next is 4879200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.9442790151686616, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992968933274291, 6.9112, 168.9124700191439, 2217012.838047738, 2159003.309588936, 447120.4494031041], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.932866283335737, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008176893327429102, 0.0, 0.8294375562573664, 0.615836899457705, 0.5997231415524823, 0.6673439543329912], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7322108], dtype=float32), 0.123292945]. 
=============================================
[2019-03-27 00:35:33,202] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0635310e-18 1.0000000e+00 1.1958247e-24 2.4714267e-17 1.6734476e-13], sum to 1.0000
[2019-03-27 00:35:33,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4704
[2019-03-27 00:35:33,223] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4994365494971811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697884.722825183, 697884.722825183, 183507.6516266482], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.815, 1.0, 1.0, 0.39691150541829046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1938568674514397, 0.1938568674514397, 0.2738920173532063], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.17357953], dtype=float32), -0.27277854]. 
=============================================
[2019-03-27 00:35:35,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5333110e-10 9.9827802e-01 1.0056625e-15 1.7188202e-03 3.1679551e-06], sum to 1.0000
[2019-03-27 00:35:35,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8412
[2019-03-27 00:35:35,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1697770.2191427 W.
[2019-03-27 00:35:35,899] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 69.33333333333334, 1.0, 2.0, 0.6072178311175327, 1.0, 2.0, 0.6072178311175327, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1697770.2191427, 1697770.219142699, 336931.4876090063], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4961400.0000, 
sim time next is 4962000.0000, 
raw observation next is [30.0, 68.66666666666667, 1.0, 2.0, 0.4668737242133044, 1.0, 2.0, 0.4668737242133044, 1.0, 1.0, 0.8018531422160698, 6.911199999999999, 6.9112, 170.5573041426782, 1958293.755780676, 1958293.755780677, 391491.5425231728], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6866666666666668, 1.0, 1.0, 0.3576791857991619, 1.0, 1.0, 0.3576791857991619, 1.0, 0.5, 0.7583574905074021, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5439704877168544, 0.5439704877168547, 0.5843157351092131], 
reward next is 0.4157, 
noisyNet noise sample is [array([-0.4435869], dtype=float32), 1.0243598]. 
=============================================
[2019-03-27 00:35:35,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[49.81961 ]
 [48.39918 ]
 [47.005257]
 [47.648937]
 [47.567917]], R is [[49.71025085]
 [49.71026611]
 [49.68623352]
 [49.18937302]
 [48.69747925]].
[2019-03-27 00:35:36,726] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.0042263e-08 6.1749476e-01 1.5624793e-14 3.8094085e-01 1.5642392e-03], sum to 1.0000
[2019-03-27 00:35:36,735] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6643
[2019-03-27 00:35:36,740] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 65.5, 1.0, 2.0, 0.638170591180186, 1.0, 2.0, 0.638170591180186, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1784385.639930454, 1784385.639930454, 348772.4191091742], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4966200.0000, 
sim time next is 4966800.0000, 
raw observation next is [30.13333333333333, 65.33333333333333, 1.0, 2.0, 0.6734879469196632, 1.0, 2.0, 0.6734879469196632, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1883223.105536301, 1883223.105536301, 362974.9361376561], 
processed observation next is [1.0, 0.4782608695652174, 0.6271721958925749, 0.6533333333333333, 1.0, 1.0, 0.6066119842405581, 1.0, 1.0, 0.6066119842405581, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5231175293156392, 0.5231175293156392, 0.5417536360263524], 
reward next is 0.4582, 
noisyNet noise sample is [array([-0.8692669], dtype=float32), -0.5973631]. 
=============================================
[2019-03-27 00:35:41,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6664453e-24 1.0000000e+00 5.7115190e-30 6.9566125e-24 8.5357648e-21], sum to 1.0000
[2019-03-27 00:35:41,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8476
[2019-03-27 00:35:41,337] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 63.0, 1.0, 2.0, 0.5346565109051925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747116.4357692233, 747116.4357692233, 189205.0323169144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5056800.0000, 
sim time next is 5057400.0000, 
raw observation next is [31.83333333333333, 63.0, 1.0, 2.0, 0.5393228908244629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753639.438542164, 753639.438542164, 189987.4066432035], 
processed observation next is [0.0, 0.5217391304347826, 0.7077409162717218, 0.63, 1.0, 1.0, 0.4449673383427264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20934428848393447, 0.20934428848393447, 0.2835632934973186], 
reward next is 0.7164, 
noisyNet noise sample is [array([-2.3314066], dtype=float32), -1.0742267]. 
=============================================
[2019-03-27 00:35:44,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4340952e-20 1.0000000e+00 1.9290108e-26 7.8480854e-20 3.7922701e-17], sum to 1.0000
[2019-03-27 00:35:44,031] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1425
[2019-03-27 00:35:44,037] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666666, 64.0, 1.0, 2.0, 0.5176329147832306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723319.9635912878, 723319.9635912871, 186405.0226921558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5136000.0000, 
sim time next is 5136600.0000, 
raw observation next is [30.83333333333334, 63.5, 1.0, 2.0, 0.5191838336691426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725487.8969776926, 725487.896977692, 186656.5505101178], 
processed observation next is [0.0, 0.43478260869565216, 0.6603475513428123, 0.635, 1.0, 1.0, 0.42070341405920786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20152441582713684, 0.20152441582713668, 0.27859186643301165], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.9282773], dtype=float32), -0.22928014]. 
=============================================
[2019-03-27 00:35:47,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2812319e-22 1.0000000e+00 1.0179054e-28 4.5552570e-21 4.4433926e-20], sum to 1.0000
[2019-03-27 00:35:47,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3116
[2019-03-27 00:35:47,068] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.00000000000001, 1.0, 2.0, 0.5525213899601028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772089.4704744226, 772089.4704744231, 192234.4598290622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5149200.0000, 
sim time next is 5149800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5509749634307706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769927.7218306224, 769927.7218306231, 191968.559560129], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45900598003707294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21386881161961735, 0.21386881161961754, 0.28652023814944627], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.16967678], dtype=float32), 0.070497416]. 
=============================================
[2019-03-27 00:35:47,648] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5245747e-21 1.0000000e+00 2.3385749e-28 9.9237763e-22 6.4092406e-18], sum to 1.0000
[2019-03-27 00:35:47,658] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9465
[2019-03-27 00:35:47,665] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5509749634307706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769927.7218306224, 769927.7218306231, 191968.559560129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5149800.0000, 
sim time next is 5150400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5510132275605634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769981.2111797953, 769981.2111797959, 191975.1303179837], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4590520813982691, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21388366977216536, 0.21388366977216552, 0.28653004525072195], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.03350487], dtype=float32), -1.4893528]. 
=============================================
[2019-03-27 00:35:48,280] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 00:35:48,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:35:48,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:35:48,284] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:35:48,286] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:35:48,288] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:35:48,289] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:35:48,290] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:35:48,291] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:35:48,292] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:35:48,294] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:35:48,307] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-27 00:35:48,326] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-27 00:35:48,349] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-27 00:35:48,350] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-27 00:35:48,350] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-27 00:36:34,523] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.013666615]
[2019-03-27 00:36:34,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 89.0, 1.0, 2.0, 0.4554664323995708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644889.3542668258, 644889.3542668258, 178005.615019745]
[2019-03-27 00:36:34,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:36:34,529] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4190896e-19 1.0000000e+00 7.4909591e-26 1.6581789e-19 6.3933716e-17], sampled 0.5647062995951023
[2019-03-27 00:36:35,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.013666615]
[2019-03-27 00:36:35,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.16666666666667, 83.16666666666667, 1.0, 2.0, 0.4360764697621042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632396.6356465198, 632396.6356465198, 177142.3477191151]
[2019-03-27 00:36:35,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:36:35,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5385983e-19 1.0000000e+00 6.2995548e-26 1.3983789e-19 5.7025659e-17], sampled 0.22730139059772014
[2019-03-27 00:37:41,964] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 00:37:42,248] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.0877 3163795148.0655 1774.0000
[2019-03-27 00:37:42,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.013666615]
[2019-03-27 00:37:42,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.75, 62.33333333333334, 1.0, 2.0, 0.4490731382213549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707127.0361396484, 707127.0361396477, 185267.8787305603]
[2019-03-27 00:37:42,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:37:42,281] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0632083e-19 1.0000000e+00 1.9631002e-25 1.7953897e-19 8.9784136e-17], sampled 0.1836037845298153
[2019-03-27 00:37:42,512] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 00:37:42,659] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0216 2842500949.6201 1131.0000
[2019-03-27 00:37:42,743] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5081 3007655539.1316 1766.0000
[2019-03-27 00:37:43,759] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 350000, evaluation results [350000.0, 7886.08769778697, 3163795148.065501, 1774.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7997.508103948408, 3007655539.131616, 1766.0, 8496.021631964326, 2842500949.6200757, 1131.0]
[2019-03-27 00:37:43,786] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.012908e-20 1.000000e+00 2.115194e-28 2.772701e-21 1.843655e-18], sum to 1.0000
[2019-03-27 00:37:43,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4939
[2019-03-27 00:37:43,793] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5234427971094541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731441.26076313, 731441.2607631307, 187350.4849528408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5167800.0000, 
sim time next is 5168400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5227833462935365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730519.4496325778, 730519.4496325784, 187242.60886876], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.42504017625727286, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20292206934238274, 0.20292206934238288, 0.2794665804011343], 
reward next is 0.7205, 
noisyNet noise sample is [array([-1.6339433], dtype=float32), 0.90720415]. 
=============================================
[2019-03-27 00:37:56,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3012989e-07 1.6738437e-02 5.4557518e-11 9.8325956e-01 1.0478976e-06], sum to 1.0000
[2019-03-27 00:37:57,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5091
[2019-03-27 00:37:57,011] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.1, 67.33333333333334, 1.0, 2.0, 0.8468794665521882, 1.0, 2.0, 0.8468794665521882, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2368576.022618104, 2368576.022618105, 443351.2490116378], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5389800.0000, 
sim time next is 5390400.0000, 
raw observation next is [33.3, 66.66666666666667, 1.0, 2.0, 0.8200154618118317, 1.0, 2.0, 0.8200154618118317, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2293373.08627115, 2293373.08627115, 429746.2227239732], 
processed observation next is [1.0, 0.391304347826087, 0.7772511848341231, 0.6666666666666667, 1.0, 1.0, 0.7831511588094358, 1.0, 1.0, 0.7831511588094358, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6370480795197639, 0.6370480795197639, 0.6414122727223481], 
reward next is 0.3586, 
noisyNet noise sample is [array([0.10128479], dtype=float32), -0.20111924]. 
=============================================
[2019-03-27 00:37:58,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1078882e-09 9.9999368e-01 9.0195646e-14 6.2768395e-06 1.9797755e-10], sum to 1.0000
[2019-03-27 00:37:58,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5648
[2019-03-27 00:37:58,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2727167.815392972 W.
[2019-03-27 00:37:58,052] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.66666666666666, 61.33333333333334, 1.0, 2.0, 0.9749534505380296, 1.0, 2.0, 0.9749534505380296, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2727167.815392972, 2727167.815392972, 514057.0586616262], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5402400.0000, 
sim time next is 5403000.0000, 
raw observation next is [36.83333333333334, 60.66666666666666, 1.0, 2.0, 0.9731284718398582, 1.0, 2.0, 0.9731284718398582, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2722057.371056861, 2722057.371056861, 512982.1994055336], 
processed observation next is [1.0, 0.5217391304347826, 0.9447077409162722, 0.6066666666666666, 1.0, 1.0, 0.967624664867299, 1.0, 1.0, 0.967624664867299, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7561270475157946, 0.7561270475157946, 0.7656450737396024], 
reward next is 0.2344, 
noisyNet noise sample is [array([0.9085902], dtype=float32), -0.29105014]. 
=============================================
[2019-03-27 00:37:58,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[17.795328]
 [16.827589]
 [15.694053]
 [14.242871]
 [13.518132]], R is [[18.04165459]
 [18.09398842]
 [18.14741707]
 [17.96594238]
 [17.78628349]].
[2019-03-27 00:38:07,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9200588e-10 1.3076757e-01 4.3467793e-16 8.6922526e-01 7.1053200e-06], sum to 1.0000
[2019-03-27 00:38:07,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2753
[2019-03-27 00:38:07,774] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.7, 60.33333333333334, 1.0, 2.0, 0.8472920334845337, 1.0, 2.0, 0.8472920334845337, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2369730.995339381, 2369730.995339381, 443551.8147222219], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5570400.0000, 
sim time next is 5571000.0000, 
raw observation next is [32.8, 59.5, 1.0, 2.0, 0.812375891785435, 1.0, 2.0, 0.812375891785435, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2271987.745457377, 2271987.745457377, 425941.8975659798], 
processed observation next is [1.0, 0.4782608695652174, 0.7535545023696681, 0.595, 1.0, 1.0, 0.7739468575728132, 1.0, 1.0, 0.7739468575728132, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6311077070714936, 0.6311077070714936, 0.6357341754716116], 
reward next is 0.3643, 
noisyNet noise sample is [array([-0.32955587], dtype=float32), -0.667461]. 
=============================================
[2019-03-27 00:38:07,791] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[49.131367]
 [48.25595 ]
 [48.84209 ]
 [48.623447]
 [48.334816]], R is [[49.64980698]
 [49.49129105]
 [49.26517105]
 [49.11787796]
 [48.99202347]].
[2019-03-27 00:38:21,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3375672e-12 9.9999952e-01 1.1536550e-17 4.7640759e-07 1.9530391e-09], sum to 1.0000
[2019-03-27 00:38:21,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5022
[2019-03-27 00:38:21,249] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 93.66666666666667, 1.0, 2.0, 0.8595539333973279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201377.249518551, 1201377.24951855, 259053.2830890009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5805600.0000, 
sim time next is 5806200.0000, 
raw observation next is [25.95, 93.83333333333334, 1.0, 2.0, 0.8472554202854963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1184178.336783012, 1184178.336783011, 255839.8913648883], 
processed observation next is [1.0, 0.17391304347826086, 0.42890995260663506, 0.9383333333333335, 1.0, 1.0, 0.8159703858861401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32893842688416997, 0.32893842688416974, 0.3818505841266989], 
reward next is 0.6181, 
noisyNet noise sample is [array([0.13393645], dtype=float32), 0.6134487]. 
=============================================
[2019-03-27 00:38:21,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6836012e-11 9.9999988e-01 2.8127519e-17 8.7374751e-08 9.4596372e-11], sum to 1.0000
[2019-03-27 00:38:21,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0396
[2019-03-27 00:38:21,507] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 95.0, 1.0, 2.0, 0.7774507243969447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1086564.913465274, 1086564.913465275, 238450.4391852209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5886000.0000, 
sim time next is 5886600.0000, 
raw observation next is [25.86666666666667, 95.0, 1.0, 2.0, 0.9543050645449079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333891.751063515, 1333891.751063515, 285311.5487068324], 
processed observation next is [1.0, 0.13043478260869565, 0.42496050552922615, 0.95, 1.0, 1.0, 0.9449458608974795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37052548640653193, 0.37052548640653193, 0.4258381323982573], 
reward next is 0.5742, 
noisyNet noise sample is [array([1.9040831], dtype=float32), 2.344946]. 
=============================================
[2019-03-27 00:38:22,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6145511e-11 9.9999964e-01 2.2995933e-16 3.2382545e-07 4.0976517e-10], sum to 1.0000
[2019-03-27 00:38:22,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8221
[2019-03-27 00:38:22,349] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.28333333333333, 82.16666666666667, 1.0, 2.0, 0.8528473451741722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191998.355556287, 1191998.355556287, 257297.132162842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5814600.0000, 
sim time next is 5815200.0000, 
raw observation next is [28.46666666666667, 81.33333333333334, 1.0, 2.0, 0.971369365972964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1357758.842865163, 1357758.842865164, 290324.7584659207], 
processed observation next is [1.0, 0.30434782608695654, 0.5481832543443919, 0.8133333333333335, 1.0, 1.0, 0.9655052602083903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3771552341292119, 0.3771552341292122, 0.4333205350237622], 
reward next is 0.5667, 
noisyNet noise sample is [array([0.25517148], dtype=float32), 1.7536793]. 
=============================================
[2019-03-27 00:38:28,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2396551e-09 8.7614633e-02 1.6872090e-15 9.1238487e-01 5.1076819e-07], sum to 1.0000
[2019-03-27 00:38:28,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9863
[2019-03-27 00:38:28,735] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.83333333333333, 74.66666666666666, 1.0, 2.0, 0.7710787619948316, 1.0, 2.0, 0.7710787619948316, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2156383.519356359, 2156383.519356359, 406038.1560444231], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5925000.0000, 
sim time next is 5925600.0000, 
raw observation next is [29.7, 75.0, 1.0, 2.0, 0.8243387554819392, 1.0, 2.0, 0.8243387554819392, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2305475.383517689, 2305475.38351769, 431893.5624803656], 
processed observation next is [1.0, 0.6086956521739131, 0.6066350710900474, 0.75, 1.0, 1.0, 0.7883599463637823, 1.0, 1.0, 0.7883599463637823, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6404098287549136, 0.6404098287549139, 0.6446172574333815], 
reward next is 0.3554, 
noisyNet noise sample is [array([0.57585406], dtype=float32), 1.227592]. 
=============================================
[2019-03-27 00:38:33,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1379500e-14 1.2457732e-05 6.3492600e-22 9.9998748e-01 2.9354774e-09], sum to 1.0000
[2019-03-27 00:38:33,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5784
[2019-03-27 00:38:33,556] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.8838151738505791, 1.0, 2.0, 0.8838151738505791, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2471981.013114442, 2471981.013114441, 462748.3861766959], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6004800.0000, 
sim time next is 6005400.0000, 
raw observation next is [32.0, 68.16666666666667, 1.0, 2.0, 0.9081734548007877, 1.0, 2.0, 0.9081734548007877, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2540178.996946533, 2540178.996946533, 475974.6448968645], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.6816666666666668, 1.0, 1.0, 0.8893656081937201, 1.0, 1.0, 0.8893656081937201, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7056052769295925, 0.7056052769295925, 0.7104099177565142], 
reward next is 0.2896, 
noisyNet noise sample is [array([0.64022386], dtype=float32), -0.3906718]. 
=============================================
[2019-03-27 00:38:35,843] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0461662e-14 1.0000000e+00 1.4677129e-22 3.0481651e-08 4.7899448e-11], sum to 1.0000
[2019-03-27 00:38:35,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6169
[2019-03-27 00:38:35,857] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 87.0, 1.0, 2.0, 0.536010203813433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749008.7213309497, 749008.7213309504, 189430.5460302233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6128400.0000, 
sim time next is 6129000.0000, 
raw observation next is [27.3, 87.0, 1.0, 2.0, 0.5366046962309151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749839.7450094966, 749839.7450094966, 189530.0478672836], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.87, 1.0, 1.0, 0.4416924050974881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2082888180581935, 0.2082888180581935, 0.28288066845863225], 
reward next is 0.7171, 
noisyNet noise sample is [array([-1.7014475], dtype=float32), 0.92133355]. 
=============================================
[2019-03-27 00:38:35,868] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.39329 ]
 [65.482445]
 [65.501976]
 [65.500885]
 [65.50229 ]], R is [[65.38145447]
 [65.44490814]
 [65.50769806]
 [65.56976318]
 [65.63082886]].
[2019-03-27 00:38:36,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4254301e-11 9.9992406e-01 2.3642670e-19 7.5950738e-05 2.0279078e-08], sum to 1.0000
[2019-03-27 00:38:36,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5748
[2019-03-27 00:38:36,444] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.5, 1.0, 2.0, 0.5283083874730912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738242.6348016327, 738242.6348016327, 188150.3274603031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6053400.0000, 
sim time next is 6054000.0000, 
raw observation next is [26.26666666666667, 92.66666666666667, 1.0, 2.0, 0.5280370825265823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737863.3895590453, 737863.3895590453, 188105.5431718198], 
processed observation next is [1.0, 0.043478260869565216, 0.44391785150079005, 0.9266666666666667, 1.0, 1.0, 0.4313699789476895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20496205265529036, 0.20496205265529036, 0.28075454204749223], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.53429145], dtype=float32), -1.2421293]. 
=============================================
[2019-03-27 00:38:36,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[56.298912]
 [56.309032]
 [56.415394]
 [56.48832 ]
 [56.49654 ]], R is [[56.23445892]
 [56.39129257]
 [56.54642105]
 [56.69977951]
 [56.85144043]].
[2019-03-27 00:38:38,074] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4389320e-09 9.8273003e-01 9.7530078e-18 1.7269865e-02 1.5386590e-07], sum to 1.0000
[2019-03-27 00:38:38,082] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8722
[2019-03-27 00:38:38,088] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 87.0, 1.0, 2.0, 0.6820130727227623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953121.4209567029, 953121.4209567023, 217011.0333146343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6071400.0000, 
sim time next is 6072000.0000, 
raw observation next is [27.5, 86.33333333333334, 1.0, 2.0, 0.7476511577549068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044896.55704804, 1044896.557048039, 231467.3212886032], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.8633333333333334, 1.0, 1.0, 0.6959652503071165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2902490436244555, 0.2902490436244553, 0.3454736138635869], 
reward next is 0.6545, 
noisyNet noise sample is [array([-0.5609381], dtype=float32), -0.66099757]. 
=============================================
[2019-03-27 00:38:38,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[56.551556]
 [56.99474 ]
 [57.449886]
 [58.133106]
 [58.844658]], R is [[56.32754517]
 [56.44037247]
 [56.55269241]
 [56.65704727]
 [56.76612091]].
[2019-03-27 00:38:39,644] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 00:38:39,647] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:38:39,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:38:39,649] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:38:39,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:38:39,651] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:38:39,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:38:39,653] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:38:39,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:38:39,655] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:38:39,655] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:38:39,670] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-27 00:38:39,691] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-27 00:38:39,709] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-27 00:38:39,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-27 00:38:39,728] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-27 00:39:14,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:14,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.15321164333333, 83.91175597333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 12.13337166307904, 6.9112, 173.5110001786674, 5261619.844996226, 1455982.085860959, 298562.644311241]
[2019-03-27 00:39:14,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:39:14,221] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.2924898e-10 9.9605370e-01 2.1274124e-17 3.9462182e-03 6.1506192e-08], sampled 0.334756398350441
[2019-03-27 00:39:14,223] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5261619.844996226 W.
[2019-03-27 00:39:16,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:16,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.3, 58.0, 1.0, 2.0, 0.6545630712922105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914743.1951347748, 914743.1951347748, 211341.1258601443]
[2019-03-27 00:39:16,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:39:16,515] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1439701e-13 9.9999952e-01 5.6215776e-21 4.7594440e-07 8.7174996e-11], sampled 0.05976233056048219
[2019-03-27 00:39:17,444] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:17,444] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.43991451, 85.78182159, 1.0, 2.0, 0.5669396526403822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792244.9622753155, 792244.9622753155, 194748.7543922054]
[2019-03-27 00:39:17,445] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:39:17,448] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0885343e-13 9.9999988e-01 1.8024078e-21 1.7452275e-07 3.6056744e-11], sampled 0.8588961161447678
[2019-03-27 00:39:20,140] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:20,141] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.7, 53.0, 1.0, 2.0, 0.7014815767303998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980341.4457287838, 980341.4457287838, 221163.594744463]
[2019-03-27 00:39:20,142] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:39:20,145] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8310050e-12 9.9999356e-01 2.2255446e-20 6.4570308e-06 4.7644039e-10], sampled 0.3198502302150693
[2019-03-27 00:39:36,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:36,952] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.6694322965600052, 1.0, 2.0, 0.6694322965600052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1871872.692685907, 1871872.692685907, 361320.5135970709]
[2019-03-27 00:39:36,953] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:39:36,955] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1412827e-10 6.0482126e-02 7.0627074e-19 9.3951732e-01 5.3080186e-07], sampled 0.8304468599449512
[2019-03-27 00:39:39,886] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:39,886] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.13333333333333, 62.33333333333333, 1.0, 2.0, 0.6048332634222515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 845218.7963007103, 845218.796300711, 201648.845603338]
[2019-03-27 00:39:39,887] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:39:39,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9358358e-18 1.0000000e+00 6.0166754e-26 3.5268817e-15 3.1193071e-16], sampled 0.7368210544990547
[2019-03-27 00:39:42,627] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:39:42,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.01400455333334, 58.64759827666667, 1.0, 2.0, 0.8311294708930671, 1.0, 2.0, 0.8311294708930671, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 2324472.879152024, 2324472.879152024, 435525.3856421465]
[2019-03-27 00:39:42,629] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:39:42,632] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5258607e-13 6.4499875e-07 4.6203374e-22 9.9999940e-01 4.2457060e-09], sampled 0.10665375452672021
[2019-03-27 00:40:16,562] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:40:16,564] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.89130418166667, 54.58290045833333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.935328335100046, 6.9112, 168.9126579772939, 1496377.593186794, 1479260.152086669, 315430.1311996973]
[2019-03-27 00:40:16,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:40:16,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.0362266e-11 9.9986148e-01 1.0089982e-18 1.3846834e-04 6.1225536e-09], sampled 0.7056977524703903
[2019-03-27 00:40:26,507] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:40:26,509] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.15, 62.5, 1.0, 2.0, 0.4652401629223046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661152.2435463929, 661152.2435463935, 179751.9117726144]
[2019-03-27 00:40:26,509] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:40:26,512] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6524309e-16 1.0000000e+00 7.2846295e-24 2.3803482e-11 1.3381849e-13], sampled 0.04355540396281632
[2019-03-27 00:40:29,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:40:29,903] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.1, 89.0, 1.0, 2.0, 0.7842461271504909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1096067.067967807, 1096067.067967808, 240083.6179986889]
[2019-03-27 00:40:29,904] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:40:29,906] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0896810e-11 9.9859196e-01 6.7599459e-19 1.4079951e-03 1.3560229e-08], sampled 0.15780332222575366
[2019-03-27 00:40:32,505] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.008959686]
[2019-03-27 00:40:32,507] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.55, 60.0, 1.0, 2.0, 0.3668810991005745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572630.9444721492, 572630.9444721498, 172730.9694120035]
[2019-03-27 00:40:32,508] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:40:32,511] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8474032e-15 1.0000000e+00 3.9843017e-23 9.5294141e-11 4.4598808e-13], sampled 0.6276150518042971
[2019-03-27 00:40:33,149] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8163.4590 3139276336.6356 1106.0000
[2019-03-27 00:40:33,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8845.7272 2766512256.5363 538.0000
[2019-03-27 00:40:33,204] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8526.8016 2908244719.1825 787.0000
[2019-03-27 00:40:33,258] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8265.8440 2987639326.8246 1187.0000
[2019-03-27 00:40:33,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8662.5470 2830259301.3339 750.0000
[2019-03-27 00:40:34,292] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 375000, evaluation results [375000.0, 8163.459036177847, 3139276336.6355915, 1106.0, 8526.80160085856, 2908244719.18247, 787.0, 8845.727231800654, 2766512256.5362654, 538.0, 8265.843975183318, 2987639326.8245997, 1187.0, 8662.547000676608, 2830259301.3338633, 750.0]
[2019-03-27 00:40:47,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5812787e-12 8.2059032e-06 4.9000479e-21 9.9999177e-01 2.1424880e-08], sum to 1.0000
[2019-03-27 00:40:47,620] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1889
[2019-03-27 00:40:47,626] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.2619353604101255, 1.0, 2.0, 0.2619353604101255, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 732037.0010661532, 732037.0010661532, 242945.1960760841], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6330600.0000, 
sim time next is 6331200.0000, 
raw observation next is [27.6, 82.33333333333334, 1.0, 2.0, 0.2621631239539388, 1.0, 2.0, 0.2621631239539388, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 732673.7546828656, 732673.7546828656, 242984.1055923517], 
processed observation next is [0.0, 0.2608695652173913, 0.5071090047393366, 0.8233333333333335, 1.0, 1.0, 0.11103990837823953, 1.0, 1.0, 0.11103990837823953, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2035204874119071, 0.2035204874119071, 0.3626628441676891], 
reward next is 0.6373, 
noisyNet noise sample is [array([0.38156977], dtype=float32), -0.64911866]. 
=============================================
[2019-03-27 00:40:49,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4116356e-13 1.1563821e-05 2.6141977e-21 9.9998844e-01 6.4573942e-09], sum to 1.0000
[2019-03-27 00:40:49,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8164
[2019-03-27 00:40:49,964] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.56666666666667, 82.0, 1.0, 2.0, 0.2627380260743837, 1.0, 2.0, 0.2627380260743837, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 734280.9977967433, 734280.9977967426, 243081.8768744585], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6381600.0000, 
sim time next is 6382200.0000, 
raw observation next is [27.53333333333333, 82.0, 1.0, 2.0, 0.2625040527515483, 1.0, 2.0, 0.2625040527515483, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 733626.8826601476, 733626.8826601476, 243041.5828266269], 
processed observation next is [0.0, 0.8695652173913043, 0.5039494470774091, 0.82, 1.0, 1.0, 0.11145066596572084, 1.0, 1.0, 0.11145066596572084, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20378524518337435, 0.20378524518337435, 0.36274863108451777], 
reward next is 0.6373, 
noisyNet noise sample is [array([-0.95071924], dtype=float32), 0.067305736]. 
=============================================
[2019-03-27 00:40:52,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7123244e-10 2.1322664e-06 9.3337644e-19 9.9999785e-01 1.3279761e-09], sum to 1.0000
[2019-03-27 00:40:52,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7501
[2019-03-27 00:40:52,225] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.2562262214808134, 1.0, 2.0, 0.2562262214808134, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 716076.2082465867, 716076.2082465867, 241976.7676586618], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6396000.0000, 
sim time next is 6396600.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.2568209403530155, 1.0, 2.0, 0.2568209403530155, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 717738.8269979004, 717738.8269979004, 242076.456411365], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.10460354259399457, 1.0, 1.0, 0.10460354259399457, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19937189638830566, 0.19937189638830566, 0.3613081438975597], 
reward next is 0.6387, 
noisyNet noise sample is [array([-0.26034942], dtype=float32), -1.4781356]. 
=============================================
[2019-03-27 00:41:01,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5323932e-14 8.2676010e-08 5.0021308e-21 9.9999988e-01 1.6374846e-09], sum to 1.0000
[2019-03-27 00:41:01,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2719
[2019-03-27 00:41:01,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.35, 83.0, 1.0, 2.0, 0.2573542117249791, 1.0, 2.0, 0.2573542117249791, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 719229.6629665539, 719229.6629665539, 242167.7673580711], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6561000.0000, 
sim time next is 6561600.0000, 
raw observation next is [27.3, 83.66666666666667, 1.0, 2.0, 0.2583142039963867, 1.0, 2.0, 0.2583142039963867, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 721913.4631258195, 721913.4631258195, 242329.7842293271], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.8366666666666667, 1.0, 1.0, 0.10640265541733339, 1.0, 1.0, 0.10640265541733339, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20053151753494985, 0.20053151753494985, 0.3616862451183987], 
reward next is 0.6383, 
noisyNet noise sample is [array([0.22372933], dtype=float32), -1.3116628]. 
=============================================
[2019-03-27 00:41:03,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9999880e-17 1.4589772e-14 9.8683393e-26 1.0000000e+00 1.7655226e-12], sum to 1.0000
[2019-03-27 00:41:03,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3370
[2019-03-27 00:41:03,199] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.26666666666667, 73.16666666666667, 1.0, 2.0, 0.7284685952813332, 1.0, 2.0, 0.7284685952813332, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2037107.470834273, 2037107.470834273, 386546.3433156313], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6603000.0000, 
sim time next is 6603600.0000, 
raw observation next is [29.43333333333334, 72.33333333333334, 1.0, 2.0, 0.7848197758553647, 1.0, 2.0, 0.7848197758553647, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2194850.736117726, 2194850.736117726, 412544.053846838], 
processed observation next is [1.0, 0.43478260869565216, 0.5939968404423385, 0.7233333333333334, 1.0, 1.0, 0.7407467178980297, 1.0, 1.0, 0.7407467178980297, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6096807600327018, 0.6096807600327018, 0.6157373938012508], 
reward next is 0.3843, 
noisyNet noise sample is [array([-0.6299067], dtype=float32), 1.3494455]. 
=============================================
[2019-03-27 00:41:06,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4392514e-13 1.3515500e-08 1.3851208e-21 1.0000000e+00 7.0170436e-10], sum to 1.0000
[2019-03-27 00:41:06,996] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9480
[2019-03-27 00:41:07,002] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.3, 93.0, 1.0, 2.0, 0.2476840720586319, 1.0, 2.0, 0.2476840720586319, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 692195.7340211804, 692195.7340211798, 240565.3676933809], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6658200.0000, 
sim time next is 6658800.0000, 
raw observation next is [25.23333333333333, 93.33333333333334, 1.0, 2.0, 0.2478522722791355, 1.0, 2.0, 0.2478522722791355, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 692665.950239507, 692665.9502395064, 240592.5551800431], 
processed observation next is [1.0, 0.043478260869565216, 0.39494470774091617, 0.9333333333333335, 1.0, 1.0, 0.09379791840859698, 1.0, 1.0, 0.09379791840859698, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19240720839986306, 0.1924072083998629, 0.35909336594036284], 
reward next is 0.6409, 
noisyNet noise sample is [array([1.9448313], dtype=float32), -0.32836065]. 
=============================================
[2019-03-27 00:41:14,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9229848e-13 1.0728507e-09 1.5706810e-19 1.0000000e+00 3.1078076e-10], sum to 1.0000
[2019-03-27 00:41:14,074] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3723
[2019-03-27 00:41:14,080] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.3, 57.0, 1.0, 2.0, 0.3718266869319198, 1.0, 2.0, 0.3718266869319198, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1139005.83029872, 1139005.83029872, 276816.8823679138], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6777000.0000, 
sim time next is 6777600.0000, 
raw observation next is [27.53333333333333, 56.0, 1.0, 2.0, 0.4358818296308107, 1.0, 2.0, 0.4358818296308107, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1332543.772060906, 1332543.772060906, 294357.2456633165], 
processed observation next is [1.0, 0.43478260869565216, 0.5039494470774091, 0.56, 1.0, 1.0, 0.320339553772061, 1.0, 1.0, 0.320339553772061, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3701510477946961, 0.3701510477946961, 0.4393391726318157], 
reward next is 0.5607, 
noisyNet noise sample is [array([1.8713449], dtype=float32), -0.22331853]. 
=============================================
[2019-03-27 00:41:15,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0869099e-15 3.8091836e-09 1.9477063e-23 1.0000000e+00 9.5400528e-11], sum to 1.0000
[2019-03-27 00:41:15,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5711
[2019-03-27 00:41:15,624] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.65, 51.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 494195.9945446511, 494195.9945446511, 233008.3044603946], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6802200.0000, 
sim time next is 6802800.0000, 
raw observation next is [28.5, 51.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 499390.4744632224, 499390.4744632218, 233928.6955387833], 
processed observation next is [1.0, 0.7391304347826086, 0.5497630331753555, 0.5133333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.13871957623978398, 0.13871957623978384, 0.34914730677430345], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6258735], dtype=float32), 0.64693236]. 
=============================================
[2019-03-27 00:41:21,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.67056007e-12 9.93672967e-01 1.12176604e-20 6.32700277e-03
 1.69314573e-09], sum to 1.0000
[2019-03-27 00:41:21,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5303
[2019-03-27 00:41:21,238] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 63.16666666666666, 1.0, 2.0, 0.3688237051575706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 562188.2769463515, 562188.2769463509, 171512.6995161288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6897000.0000, 
sim time next is 6897600.0000, 
raw observation next is [26.8, 64.0, 1.0, 2.0, 0.3693110611153672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562659.6548752574, 562659.6548752574, 171545.3113086715], 
processed observation next is [0.0, 0.8695652173913043, 0.4691943127962086, 0.64, 1.0, 1.0, 0.2401338085727316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15629434857646038, 0.15629434857646038, 0.256037778072644], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.7385344], dtype=float32), -0.21540682]. 
=============================================
[2019-03-27 00:41:21,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7626707e-11 9.6754539e-01 1.0513488e-19 3.2454599e-02 1.0412209e-08], sum to 1.0000
[2019-03-27 00:41:21,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4258
[2019-03-27 00:41:21,556] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 73.16666666666667, 1.0, 2.0, 0.3983812794120266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593862.1654945082, 593862.1654945075, 173955.9070893947], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6904200.0000, 
sim time next is 6904800.0000, 
raw observation next is [25.9, 74.0, 1.0, 2.0, 0.4008981062398433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596637.8098706974, 596637.8098706967, 174182.9389562755], 
processed observation next is [0.0, 0.9565217391304348, 0.42654028436018954, 0.74, 1.0, 1.0, 0.2781904894455943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16573272496408262, 0.16573272496408242, 0.25997453575563506], 
reward next is 0.7400, 
noisyNet noise sample is [array([1.0060484], dtype=float32), -2.6676424]. 
=============================================
[2019-03-27 00:41:30,067] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 00:41:30,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:41:30,072] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:41:30,072] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:41:30,074] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:41:30,074] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:41:30,075] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:41:30,076] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:41:30,076] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:41:30,078] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:41:30,080] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:41:30,099] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-27 00:41:30,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-27 00:41:30,138] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-27 00:41:30,139] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-27 00:41:30,139] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-27 00:41:48,869] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.012696652]
[2019-03-27 00:41:48,872] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.0, 61.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 402898.4181375442, 402898.4181375442, 214144.6134828695]
[2019-03-27 00:41:48,872] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:41:48,874] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3946002e-11 4.9929649e-06 6.9503396e-18 9.9999499e-01 5.1265636e-09], sampled 0.9787664196738057
[2019-03-27 00:43:23,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6856.6024 3475003372.9633 8.0000
[2019-03-27 00:43:24,447] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.2257 3511180034.7768 0.0000
[2019-03-27 00:43:24,527] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-03-27 00:43:24,646] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-03-27 00:43:24,670] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-03-27 00:43:25,686] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 400000, evaluation results [400000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6856.602408876326, 3475003372.9632854, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.225687268214, 3511180034.7768044, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-03-27 00:43:26,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0611720e-14 1.9781004e-14 4.8594311e-22 1.0000000e+00 8.4311106e-13], sum to 1.0000
[2019-03-27 00:43:26,440] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5209
[2019-03-27 00:43:26,447] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.41666666666666, 82.66666666666667, 1.0, 2.0, 0.6570108839166798, 1.0, 2.0, 0.6570108839166798, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1837110.049141783, 1837110.049141783, 356252.720249096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7138200.0000, 
sim time next is 7138800.0000, 
raw observation next is [26.4, 83.0, 1.0, 2.0, 0.654497092637276, 1.0, 2.0, 0.654497092637276, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1830075.07427458, 1830075.07427458, 355242.1269888284], 
processed observation next is [1.0, 0.6521739130434783, 0.45023696682464454, 0.83, 1.0, 1.0, 0.5837314369123806, 1.0, 1.0, 0.5837314369123806, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5083541872984945, 0.5083541872984945, 0.5302121298340723], 
reward next is 0.4698, 
noisyNet noise sample is [array([-0.57578707], dtype=float32), -0.25525013]. 
=============================================
[2019-03-27 00:43:32,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4867679e-13 2.5503374e-08 7.8790084e-21 1.0000000e+00 4.9234457e-09], sum to 1.0000
[2019-03-27 00:43:32,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7931
[2019-03-27 00:43:32,575] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.83333333333333, 84.0, 1.0, 2.0, 0.2375018993353634, 1.0, 2.0, 0.2375018993353634, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 663731.0972856721, 663731.0972856721, 238937.4547177321], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7163400.0000, 
sim time next is 7164000.0000, 
raw observation next is [25.8, 84.0, 1.0, 2.0, 0.2377303905273479, 1.0, 2.0, 0.2377303905273479, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 664369.8444955264, 664369.8444955264, 238973.012584808], 
processed observation next is [1.0, 0.9565217391304348, 0.42180094786729866, 0.84, 1.0, 1.0, 0.0816028801534312, 1.0, 1.0, 0.0816028801534312, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1845471790265351, 0.1845471790265351, 0.3566761381862806], 
reward next is 0.6433, 
noisyNet noise sample is [array([1.9211472], dtype=float32), 0.5455172]. 
=============================================
[2019-03-27 00:43:32,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.41772 ]
 [66.42504 ]
 [66.37809 ]
 [66.353645]
 [66.31025 ]], R is [[66.41010284]
 [66.38938141]
 [66.36882019]
 [66.34799194]
 [66.32707214]].
[2019-03-27 00:43:35,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3563755e-15 6.9624395e-13 2.0230096e-22 1.0000000e+00 5.3961089e-12], sum to 1.0000
[2019-03-27 00:43:35,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4318
[2019-03-27 00:43:35,834] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.36666666666666, 81.83333333333333, 1.0, 2.0, 0.4382290052594667, 1.0, 2.0, 0.4382290052594667, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1294564.916934284, 1294564.916934284, 290331.2022979846], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7231800.0000, 
sim time next is 7232400.0000, 
raw observation next is [24.4, 81.0, 1.0, 2.0, 0.4341348489083052, 1.0, 2.0, 0.4341348489083052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1286221.504317662, 1286221.504317662, 289563.3861344561], 
processed observation next is [1.0, 0.7391304347826086, 0.3554502369668246, 0.81, 1.0, 1.0, 0.31823475772084964, 1.0, 1.0, 0.31823475772084964, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3572837511993506, 0.3572837511993506, 0.432184158409636], 
reward next is 0.5678, 
noisyNet noise sample is [array([-0.8918098], dtype=float32), -0.21253061]. 
=============================================
[2019-03-27 00:43:39,977] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4203247e-11 1.4600882e-10 1.4292892e-16 1.0000000e+00 2.9488909e-10], sum to 1.0000
[2019-03-27 00:43:39,987] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1274
[2019-03-27 00:43:39,991] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.1, 86.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 506696.3504161629, 506696.3504161629, 234944.8435650677], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7284000.0000, 
sim time next is 7284600.0000, 
raw observation next is [22.15, 86.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 508738.4040204982, 508738.4040204982, 235319.2738123535], 
processed observation next is [1.0, 0.30434782608695654, 0.24881516587677724, 0.86, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14131622333902727, 0.14131622333902727, 0.35122279673485596], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7637925], dtype=float32), 0.19051272]. 
=============================================
[2019-03-27 00:43:42,371] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1949254e-16 2.4671647e-13 6.5536533e-25 1.0000000e+00 5.0035253e-13], sum to 1.0000
[2019-03-27 00:43:42,379] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9802
[2019-03-27 00:43:42,382] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.8, 60.0, 1.0, 2.0, 0.5076999299126592, 1.0, 2.0, 0.5076999299126592, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1504096.312694353, 1504096.312694353, 312456.8133008917], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7311600.0000, 
sim time next is 7312200.0000, 
raw observation next is [27.76666666666667, 60.33333333333333, 1.0, 2.0, 0.4092978839542063, 1.0, 2.0, 0.4092978839542063, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1214265.819806799, 1214265.819806799, 282773.0102848883], 
processed observation next is [1.0, 0.6521739130434783, 0.515007898894155, 0.6033333333333333, 1.0, 1.0, 0.2883107035592847, 1.0, 1.0, 0.2883107035592847, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3372960610574442, 0.3372960610574442, 0.42204926908192286], 
reward next is 0.5780, 
noisyNet noise sample is [array([1.8673894], dtype=float32), 1.4460726]. 
=============================================
[2019-03-27 00:43:45,956] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4044281e-15 1.3128861e-12 3.3621106e-25 1.0000000e+00 1.1553359e-13], sum to 1.0000
[2019-03-27 00:43:45,964] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0394
[2019-03-27 00:43:45,970] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.23333333333333, 93.0, 1.0, 2.0, 0.3546034139030059, 1.0, 2.0, 0.3546034139030059, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1103632.835953658, 1103632.835953658, 274148.9908134278], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7383000.0000, 
sim time next is 7383600.0000, 
raw observation next is [21.3, 93.0, 1.0, 2.0, 0.3587152246063949, 1.0, 2.0, 0.3587152246063949, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1114437.538284231, 1114437.538284231, 274999.4091356141], 
processed observation next is [1.0, 0.4782608695652174, 0.2085308056872039, 0.93, 1.0, 1.0, 0.22736774048963238, 1.0, 1.0, 0.22736774048963238, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3095659828567308, 0.3095659828567308, 0.4104468793068867], 
reward next is 0.5896, 
noisyNet noise sample is [array([0.7245994], dtype=float32), -0.905494]. 
=============================================
[2019-03-27 00:43:50,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1071137e-10 3.6834578e-07 1.9940316e-15 9.9999964e-01 5.7061009e-09], sum to 1.0000
[2019-03-27 00:43:50,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0423
[2019-03-27 00:43:50,240] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.8, 94.33333333333334, 1.0, 2.0, 0.1720755750762983, 1.0, 2.0, 0.1720755750762983, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 528607.3731789304, 528607.3731789297, 238453.8021644299], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7460400.0000, 
sim time next is 7461000.0000, 
raw observation next is [21.9, 94.0, 1.0, 2.0, 0.1732681518348022, 1.0, 2.0, 0.1732681518348022, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 531338.145317075, 531338.1453170755, 238467.955169909], 
processed observation next is [0.0, 0.34782608695652173, 0.23696682464454974, 0.94, 1.0, 1.0, 0.003937532331086984, 1.0, 1.0, 0.003937532331086984, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.14759392925474304, 0.1475939292547432, 0.3559223211491179], 
reward next is 0.6441, 
noisyNet noise sample is [array([-2.246806], dtype=float32), -0.44361168]. 
=============================================
[2019-03-27 00:43:50,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[28.435366]
 [28.463415]
 [28.488491]
 [28.460619]
 [28.466217]], R is [[28.77643013]
 [29.13276482]
 [29.48563194]
 [29.19077682]
 [28.89886856]].
[2019-03-27 00:43:55,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.03682545e-13 1.98107009e-05 1.21499600e-20 9.99980211e-01
 1.17873133e-09], sum to 1.0000
[2019-03-27 00:43:55,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0519
[2019-03-27 00:43:55,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.6, 90.0, 1.0, 2.0, 0.1971741723565774, 1.0, 2.0, 0.1971741723565774, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 581361.2693692988, 581361.2693692988, 238255.4122249956], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7543200.0000, 
sim time next is 7543800.0000, 
raw observation next is [23.65, 90.0, 1.0, 2.0, 0.1984020915170149, 1.0, 2.0, 0.1984020915170149, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 583921.782592143, 583921.782592143, 238257.9386491035], 
processed observation next is [0.0, 0.30434782608695654, 0.31990521327014215, 0.9, 1.0, 1.0, 0.034219387369897464, 1.0, 1.0, 0.034219387369897464, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16220049516448418, 0.16220049516448418, 0.35560886365537836], 
reward next is 0.6444, 
noisyNet noise sample is [array([1.1077106], dtype=float32), -1.5445867]. 
=============================================
[2019-03-27 00:43:58,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2918410e-12 3.9523069e-04 1.5758627e-19 9.9960476e-01 3.5847328e-10], sum to 1.0000
[2019-03-27 00:43:58,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1219
[2019-03-27 00:43:58,223] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.73333333333333, 88.33333333333334, 1.0, 2.0, 0.2442231630743741, 1.0, 2.0, 0.2442231630743741, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 682520.5514711868, 682520.5514711875, 240005.4629557482], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7593000.0000, 
sim time next is 7593600.0000, 
raw observation next is [25.66666666666667, 88.66666666666667, 1.0, 2.0, 0.2430695515527811, 1.0, 2.0, 0.2430695515527811, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 679295.5793621222, 679295.5793621222, 239820.6881345503], 
processed observation next is [0.0, 0.9130434782608695, 0.4154818325434442, 0.8866666666666667, 1.0, 1.0, 0.08803560428045913, 1.0, 1.0, 0.08803560428045913, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1886932164894784, 0.1886932164894784, 0.35794132557395564], 
reward next is 0.6421, 
noisyNet noise sample is [array([2.0952466], dtype=float32), 1.2143775]. 
=============================================
[2019-03-27 00:44:02,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5446874e-16 5.3628128e-16 2.2997701e-23 1.0000000e+00 3.7749128e-14], sum to 1.0000
[2019-03-27 00:44:02,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0747
[2019-03-27 00:44:02,747] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.73333333333333, 67.33333333333334, 1.0, 2.0, 0.7307087740177367, 1.0, 2.0, 0.7307087740177367, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2043377.93620013, 2043377.93620013, 387540.5264564602], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7659600.0000, 
sim time next is 7660200.0000, 
raw observation next is [29.61666666666667, 68.16666666666666, 1.0, 2.0, 0.7460225235823509, 1.0, 2.0, 0.7460225235823509, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2086243.50622154, 2086243.50622154, 394441.3955787329], 
processed observation next is [1.0, 0.6521739130434783, 0.6026856240126385, 0.6816666666666665, 1.0, 1.0, 0.6940030404606637, 1.0, 1.0, 0.6940030404606637, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.579512085061539, 0.579512085061539, 0.5887185008637804], 
reward next is 0.4113, 
noisyNet noise sample is [array([-0.6809737], dtype=float32), 0.34933698]. 
=============================================
[2019-03-27 00:44:03,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6478301e-13 1.9179991e-09 1.6162537e-21 1.0000000e+00 1.4619106e-10], sum to 1.0000
[2019-03-27 00:44:03,113] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6321
[2019-03-27 00:44:03,118] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.05, 91.0, 1.0, 2.0, 0.2383060365648883, 1.0, 2.0, 0.2383060365648883, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 665979.0648879553, 665979.064887956, 239064.6492112452], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7691400.0000, 
sim time next is 7692000.0000, 
raw observation next is [25.0, 91.33333333333333, 1.0, 2.0, 0.2388101075040206, 1.0, 2.0, 0.2388101075040206, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 667388.1988398252, 667388.1988398252, 239143.7909536462], 
processed observation next is [1.0, 0.0, 0.38388625592417064, 0.9133333333333333, 1.0, 1.0, 0.0829037439807477, 1.0, 1.0, 0.0829037439807477, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18538561078884033, 0.18538561078884033, 0.3569310312740988], 
reward next is 0.6431, 
noisyNet noise sample is [array([-1.2004559], dtype=float32), -0.2017074]. 
=============================================
[2019-03-27 00:44:03,130] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[56.150085]
 [56.006924]
 [56.024914]
 [56.53377 ]
 [57.674152]], R is [[56.32883453]
 [56.40873337]
 [56.48789978]
 [56.56618118]
 [56.64350891]].
[2019-03-27 00:44:03,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4943648e-13 2.1393141e-06 2.9777783e-21 9.9999785e-01 1.7215888e-10], sum to 1.0000
[2019-03-27 00:44:03,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4153
[2019-03-27 00:44:03,390] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.7, 81.0, 1.0, 2.0, 0.2556853082259846, 1.0, 2.0, 0.2556853082259846, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 714564.0127684042, 714564.0127684042, 241888.3888587037], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7759800.0000, 
sim time next is 7760400.0000, 
raw observation next is [27.53333333333333, 82.33333333333333, 1.0, 2.0, 0.2567685400330161, 1.0, 2.0, 0.2567685400330161, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 717592.3345621434, 717592.3345621434, 242070.0449530562], 
processed observation next is [1.0, 0.8260869565217391, 0.5039494470774091, 0.8233333333333333, 1.0, 1.0, 0.10454040967833261, 1.0, 1.0, 0.10454040967833261, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19933120404503984, 0.19933120404503984, 0.3612985745568003], 
reward next is 0.6387, 
noisyNet noise sample is [array([-0.66875607], dtype=float32), -0.22320814]. 
=============================================
[2019-03-27 00:44:03,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2028817e-15 1.5563049e-07 1.8207397e-23 9.9999988e-01 1.0854888e-09], sum to 1.0000
[2019-03-27 00:44:03,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3310
[2019-03-27 00:44:03,561] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.93333333333333, 88.16666666666667, 1.0, 2.0, 0.247874255648258, 1.0, 2.0, 0.247874255648258, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 692727.4063970895, 692727.4063970895, 240596.1394877602], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7678200.0000, 
sim time next is 7678800.0000, 
raw observation next is [25.9, 88.0, 1.0, 2.0, 0.2471480295467679, 1.0, 2.0, 0.2471480295467679, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 690697.1887118061, 690697.1887118061, 240477.8984082868], 
processed observation next is [1.0, 0.9130434782608695, 0.42654028436018954, 0.88, 1.0, 1.0, 0.09294943318887698, 1.0, 1.0, 0.09294943318887698, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19186033019772392, 0.19186033019772392, 0.3589222364302788], 
reward next is 0.6411, 
noisyNet noise sample is [array([1.6102301], dtype=float32), 0.51140034]. 
=============================================
[2019-03-27 00:44:09,449] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6209474e-13 6.5441182e-06 2.0784347e-21 9.9999344e-01 1.1349955e-10], sum to 1.0000
[2019-03-27 00:44:09,458] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2026
[2019-03-27 00:44:09,464] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 86.33333333333334, 1.0, 2.0, 0.2604288931747603, 1.0, 2.0, 0.2604288931747603, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 727825.4126947182, 727825.4126947176, 242688.5218218966], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7764000.0000, 
sim time next is 7764600.0000, 
raw observation next is [26.95, 86.66666666666666, 1.0, 2.0, 0.2606083817348671, 1.0, 2.0, 0.2606083817348671, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 728327.2029809713, 728327.2029809713, 242719.0173946501], 
processed observation next is [1.0, 0.8695652173913043, 0.476303317535545, 0.8666666666666666, 1.0, 1.0, 0.10916672498176759, 1.0, 1.0, 0.10916672498176759, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20231311193915869, 0.20231311193915869, 0.3622671901412688], 
reward next is 0.6377, 
noisyNet noise sample is [array([1.1266668], dtype=float32), 0.406245]. 
=============================================
[2019-03-27 00:44:15,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:15,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:15,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-27 00:44:16,538] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4462402e-13 7.6958493e-09 6.2260398e-20 1.0000000e+00 3.8335918e-10], sum to 1.0000
[2019-03-27 00:44:16,545] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6499
[2019-03-27 00:44:16,550] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.3, 82.0, 1.0, 2.0, 0.3470002318401512, 1.0, 2.0, 0.3470002318401512, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 969877.3229996485, 969877.3229996485, 259596.9458909415], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7891200.0000, 
sim time next is 7891800.0000, 
raw observation next is [27.43333333333333, 81.50000000000001, 1.0, 2.0, 0.4346242614630392, 1.0, 2.0, 0.4346242614630392, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1214928.188107137, 1214928.188107137, 281195.9997935722], 
processed observation next is [1.0, 0.34782608695652173, 0.49921011058451803, 0.8150000000000002, 1.0, 1.0, 0.31882441140125206, 1.0, 1.0, 0.31882441140125206, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3374800522519825, 0.3374800522519825, 0.41969552207995847], 
reward next is 0.5803, 
noisyNet noise sample is [array([-0.59968364], dtype=float32), 0.5350252]. 
=============================================
[2019-03-27 00:44:17,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4225945e-14 6.5024756e-11 8.1596471e-23 1.0000000e+00 6.5522969e-12], sum to 1.0000
[2019-03-27 00:44:17,839] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9076
[2019-03-27 00:44:17,844] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.63333333333333, 63.0, 1.0, 2.0, 0.3428383748250848, 1.0, 2.0, 0.3428383748250848, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 958239.5952707824, 958239.5952707824, 258684.0539853793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7924200.0000, 
sim time next is 7924800.0000, 
raw observation next is [30.46666666666667, 64.0, 1.0, 2.0, 0.2355991366271806, 1.0, 2.0, 0.2355991366271806, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 658411.9382188844, 658411.9382188838, 238648.7953582818], 
processed observation next is [1.0, 0.7391304347826086, 0.6429699842022119, 0.64, 1.0, 1.0, 0.0790351043700971, 1.0, 1.0, 0.0790351043700971, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.18289220506080123, 0.18289220506080106, 0.3561922318780325], 
reward next is 0.6438, 
noisyNet noise sample is [array([0.6511738], dtype=float32), -1.7064183]. 
=============================================
[2019-03-27 00:44:18,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:18,238] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:18,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-27 00:44:18,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:18,923] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:18,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-27 00:44:19,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-27 00:44:19,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,568] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-27 00:44:19,596] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-27 00:44:19,657] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,657] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-27 00:44:19,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-27 00:44:19,698] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-27 00:44:19,770] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-27 00:44:19,795] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,798] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-27 00:44:19,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,818] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,819] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,824] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-27 00:44:19,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-27 00:44:19,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-27 00:44:19,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-27 00:44:19,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 00:44:19,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:19,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-27 00:44:21,658] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 00:44:21,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:44:21,660] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:44:21,660] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:44:21,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:21,662] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:21,661] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:44:21,665] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:21,663] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:44:21,666] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:21,667] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:44:21,678] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-27 00:44:21,678] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-27 00:44:21,691] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-27 00:44:21,720] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-27 00:44:21,722] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-27 00:44:39,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0030034254]
[2019-03-27 00:44:39,073] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.75018424, 100.0, 1.0, 2.0, 0.1714365468240253, 1.0, 2.0, 0.1714365468240253, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 530498.8265606895, 530498.8265606895, 239424.9070252992]
[2019-03-27 00:44:39,074] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:44:39,079] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3874526e-12 4.0507333e-05 4.0713995e-20 9.9995947e-01 9.2363306e-10], sampled 0.20240467567076503
[2019-03-27 00:44:53,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0030034254]
[2019-03-27 00:44:53,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.770691445, 99.768101855, 1.0, 2.0, 0.273001329159426, 1.0, 2.0, 0.273001329159426, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 762970.7090423765, 762970.7090423771, 245360.6443481167]
[2019-03-27 00:44:53,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:44:53,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3062388e-12 2.0317072e-03 1.6705132e-20 9.9796826e-01 1.4794816e-09], sampled 0.8400640107093748
[2019-03-27 00:44:57,574] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0030034254]
[2019-03-27 00:44:57,577] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.6, 90.66666666666667, 1.0, 2.0, 0.1850040217322752, 1.0, 2.0, 0.1850040217322752, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 562098.4103437506, 562098.4103437506, 238976.0318003575]
[2019-03-27 00:44:57,580] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:44:57,583] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9418156e-12 1.7117723e-05 3.3767012e-20 9.9998283e-01 5.6004751e-10], sampled 0.7978434607840913
[2019-03-27 00:45:34,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0030034254]
[2019-03-27 00:45:34,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.492749705, 92.15933792166666, 1.0, 2.0, 0.2295874038782021, 1.0, 2.0, 0.2295874038782021, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 648513.5784515354, 648513.5784515354, 239007.7118257067]
[2019-03-27 00:45:34,154] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:45:34,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9243640e-13 3.6284528e-05 6.8294786e-21 9.9996376e-01 3.2510769e-10], sampled 0.7973327649238492
[2019-03-27 00:45:45,610] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), 0.0030034254]
[2019-03-27 00:45:45,612] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.234869, 92.78575777, 1.0, 2.0, 0.4453640408388917, 1.0, 2.0, 0.4453640408388917, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1244957.453693069, 1244957.453693069, 284634.9371502048]
[2019-03-27 00:45:45,613] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:45:45,616] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9406719e-15 1.7262305e-08 2.9987108e-23 1.0000000e+00 1.4192199e-11], sampled 0.20600644775340615
[2019-03-27 00:46:15,636] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6850.4660 3472244364.4945 8.0000
[2019-03-27 00:46:15,666] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6524.7577 3661758418.3011 228.0000
[2019-03-27 00:46:15,702] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6793.7404 3392744319.4891 9.0000
[2019-03-27 00:46:15,819] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6800.2516 3501759845.2436 0.0000
[2019-03-27 00:46:15,899] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.7687 3424115980.1041 33.0000
[2019-03-27 00:46:16,914] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 425000, evaluation results [425000.0, 6524.757737161184, 3661758418.3011227, 228.0, 6850.46595492213, 3472244364.494487, 8.0, 6793.740446980839, 3392744319.4891434, 9.0, 6800.251599719086, 3501759845.243636, 0.0, 6701.76872688821, 3424115980.1041427, 33.0]
[2019-03-27 00:46:17,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3378488e-08 2.5773115e-06 8.4235055e-14 9.9999642e-01 9.4471199e-07], sum to 1.0000
[2019-03-27 00:46:17,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7350
[2019-03-27 00:46:17,353] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.6, 85.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 439963.2363046724, 439963.2363046717, 221385.4091483465], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7200.0000, 
sim time next is 7800.0000, 
raw observation next is [20.66666666666667, 85.00000000000001, 1.0, 2.0, 0.188973342075356, 1.0, 2.0, 0.188973342075356, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 611375.0660107973, 611375.0660107973, 243984.8525613719], 
processed observation next is [1.0, 0.08695652173913043, 0.17851500789889443, 0.8500000000000001, 1.0, 1.0, 0.02285944828356142, 1.0, 1.0, 0.02285944828356142, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16982640722522147, 0.16982640722522147, 0.36415649636025654], 
reward next is 0.6358, 
noisyNet noise sample is [array([0.45085844], dtype=float32), 0.91309583]. 
=============================================
[2019-03-27 00:46:26,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9950553e-11 1.2021501e-02 1.7558601e-17 9.8797846e-01 1.5235232e-09], sum to 1.0000
[2019-03-27 00:46:26,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2643
[2019-03-27 00:46:26,999] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.7, 76.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 498476.0292013081, 498476.0292013081, 233533.2919915986], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 307800.0000, 
sim time next is 308400.0000, 
raw observation next is [23.73333333333333, 76.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 499745.8666513781, 499745.8666513781, 233785.4479276696], 
processed observation next is [0.0, 0.5652173913043478, 0.3238546603475513, 0.76, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.13881829629204948, 0.13881829629204948, 0.3489335043696561], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48278335], dtype=float32), 0.6280812]. 
=============================================
[2019-03-27 00:46:33,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8803712e-16 1.0000000e+00 5.6150725e-26 5.9658070e-09 1.0036283e-12], sum to 1.0000
[2019-03-27 00:46:33,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9622
[2019-03-27 00:46:33,525] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 86.0, 1.0, 2.0, 0.2767291524129772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446506.1808600551, 446506.1808600551, 163368.3651714692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [20.96666666666667, 86.0, 1.0, 2.0, 0.275679208189076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 445079.7360947166, 445079.736094716, 163273.9126578792], 
processed observation next is [0.0, 0.9130434782608695, 0.1927330173775673, 0.86, 1.0, 1.0, 0.1273243472157542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12363326002631016, 0.12363326002631, 0.24369240695205852], 
reward next is 0.7563, 
noisyNet noise sample is [array([1.5082816], dtype=float32), 0.033688154]. 
=============================================
[2019-03-27 00:46:49,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5000440e-15 9.9999595e-01 1.9579712e-23 4.0760642e-06 7.6318576e-11], sum to 1.0000
[2019-03-27 00:46:49,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8233
[2019-03-27 00:46:49,088] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 61.5, 1.0, 2.0, 0.4410394949101237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726687.4437295337, 726687.4437295337, 185761.3217983033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 556200.0000, 
sim time next is 556800.0000, 
raw observation next is [23.26666666666667, 60.0, 1.0, 2.0, 0.4543864610132667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748939.0051099604, 748939.0051099599, 187930.2080675032], 
processed observation next is [1.0, 0.43478260869565216, 0.3017377567140602, 0.6, 1.0, 1.0, 0.34263429037742976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20803861253054456, 0.2080386125305444, 0.28049284786194506], 
reward next is 0.7195, 
noisyNet noise sample is [array([-0.09671666], dtype=float32), 0.4354396]. 
=============================================
[2019-03-27 00:46:52,083] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3774247e-15 1.0000000e+00 2.8534109e-23 8.4973184e-09 5.7394736e-12], sum to 1.0000
[2019-03-27 00:46:52,091] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3753
[2019-03-27 00:46:52,095] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612000.0000, 
sim time next is 612600.0000, 
raw observation next is [17.18333333333333, 91.16666666666667, 1.0, 2.0, 0.245768296161783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410712.8942951671, 410712.8942951671, 159480.1815492728], 
processed observation next is [1.0, 0.08695652173913043, 0.013428120063191062, 0.9116666666666667, 1.0, 1.0, 0.0912871038093771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11408691508199087, 0.11408691508199087, 0.23803012171533253], 
reward next is 0.7620, 
noisyNet noise sample is [array([-0.35163638], dtype=float32), -0.5180127]. 
=============================================
[2019-03-27 00:46:55,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8954128e-13 9.9998915e-01 1.8075792e-22 1.0834450e-05 1.8140281e-10], sum to 1.0000
[2019-03-27 00:46:55,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8182
[2019-03-27 00:46:55,488] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.33333333333334, 1.0, 2.0, 0.5585032590207535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918411.6264856434, 918411.626485644, 206990.280047354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [24.7, 53.5, 1.0, 2.0, 0.5948755988889678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977866.2161074358, 977866.2161074365, 214516.1061950178], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.535, 1.0, 1.0, 0.511898311914419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2716295044742877, 0.2716295044742879, 0.3201732928283848], 
reward next is 0.6798, 
noisyNet noise sample is [array([-0.6851321], dtype=float32), -0.5511091]. 
=============================================
[2019-03-27 00:46:55,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.3374710e-15 1.0000000e+00 1.7138547e-24 8.8885859e-09 4.0786072e-12], sum to 1.0000
[2019-03-27 00:46:55,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2848
[2019-03-27 00:46:55,835] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.45, 71.5, 1.0, 2.0, 0.2442429345325114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402702.5802681952, 402702.5802681945, 160319.3379870274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [21.23333333333333, 72.66666666666667, 1.0, 2.0, 0.2432084654639678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 401205.2548112692, 401205.2548112686, 160211.5489642061], 
processed observation next is [1.0, 0.8260869565217391, 0.2053712480252764, 0.7266666666666667, 1.0, 1.0, 0.08820297043851541, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11144590411424143, 0.11144590411424128, 0.23912171487194941], 
reward next is 0.7609, 
noisyNet noise sample is [array([0.88880914], dtype=float32), 1.1890454]. 
=============================================
[2019-03-27 00:47:03,517] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5997437e-19 1.0000000e+00 4.5253834e-27 8.0583257e-12 6.0972507e-16], sum to 1.0000
[2019-03-27 00:47:03,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7213
[2019-03-27 00:47:03,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 89.66666666666667, 1.0, 2.0, 0.266010431261114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433403.7071040743, 433403.7071040743, 162465.6624087682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [20.15, 88.83333333333334, 1.0, 2.0, 0.2669562477402215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434605.4772025349, 434605.4772025342, 162550.7975552343], 
processed observation next is [0.0, 0.21739130434782608, 0.15402843601895733, 0.8883333333333334, 1.0, 1.0, 0.11681475631351984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1207237436673708, 0.12072374366737061, 0.2426131306794542], 
reward next is 0.7574, 
noisyNet noise sample is [array([1.4434975], dtype=float32), 0.43542007]. 
=============================================
[2019-03-27 00:47:12,610] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 00:47:12,611] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:47:12,612] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:47:12,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:47:12,616] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:47:12,617] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:47:12,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:47:12,621] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:47:12,622] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:47:12,621] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:47:12,625] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:47:12,639] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-27 00:47:12,639] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-27 00:47:12,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-27 00:47:12,682] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-27 00:47:12,723] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-27 00:47:17,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:47:17,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.2, 96.0, 1.0, 2.0, 0.2614105203169333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426112.745323793, 426112.745323793, 162000.0584266495]
[2019-03-27 00:47:17,599] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:47:17,601] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9635803e-17 1.0000000e+00 3.6424833e-26 3.0101860e-10 1.8257394e-13], sampled 0.6127906807744798
[2019-03-27 00:47:43,085] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:47:43,088] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.409018085, 90.16719695333333, 1.0, 2.0, 0.520858110654148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727828.2699716579, 727828.2699716579, 186926.2998132606]
[2019-03-27 00:47:43,089] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:47:43,092] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.2490075e-18 1.0000000e+00 4.4021767e-27 1.6111104e-10 6.3030669e-14], sampled 0.7078620570048677
[2019-03-27 00:48:04,699] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:48:04,701] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.2, 51.0, 1.0, 2.0, 0.9604826575478239, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005298078532809, 6.9112, 168.9123200424868, 2239691.561720336, 2172935.397742965, 451396.4068265355]
[2019-03-27 00:48:04,701] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:48:04,702] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4006379e-18 1.0000000e+00 1.6940450e-29 1.1691249e-08 1.0539577e-13], sampled 0.653065346991361
[2019-03-27 00:48:04,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2239691.561720336 W.
[2019-03-27 00:48:18,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:48:18,300] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.3, 50.0, 1.0, 2.0, 0.5733254362013952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801171.8609656424, 801171.8609656424, 195878.9499278]
[2019-03-27 00:48:18,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:48:18,306] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7862980e-18 1.0000000e+00 4.0526121e-28 6.5662878e-11 1.9856419e-14], sampled 0.34667639613947887
[2019-03-27 00:48:20,028] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:48:20,031] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.190764805, 76.91405144999999, 1.0, 2.0, 0.7444995156249562, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129563802082, 1040489.751399674, 1040489.751399674, 230742.0659260472]
[2019-03-27 00:48:20,033] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:48:20,035] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8794479e-16 9.9999046e-01 2.5246677e-26 9.5244959e-06 1.8800784e-11], sampled 0.6753313823960952
[2019-03-27 00:48:59,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003584808]
[2019-03-27 00:48:59,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.71666666666667, 86.0, 1.0, 2.0, 0.5188472331068782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725017.3835378976, 725017.383537897, 186601.1894594077]
[2019-03-27 00:48:59,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:48:59,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5657728e-17 1.0000000e+00 1.2047382e-26 1.4992680e-09 2.4375936e-13], sampled 0.22910109075552598
[2019-03-27 00:49:06,281] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8135.6186 2995827030.5252 1459.0000
[2019-03-27 00:49:06,404] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8757.0069 2771183937.2265 718.0000
[2019-03-27 00:49:06,514] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8389.2856 2916266785.0944 1048.0000
[2019-03-27 00:49:06,707] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8038.3693 3149586252.7306 1394.0000
[2019-03-27 00:49:06,739] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8572.7547 2836242712.1680 958.0000
[2019-03-27 00:49:07,753] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 450000, evaluation results [450000.0, 8038.3692653564585, 3149586252.730607, 1394.0, 8389.285598829643, 2916266785.0943627, 1048.0, 8757.006902643658, 2771183937.2264724, 718.0, 8135.618602679991, 2995827030.5251775, 1459.0, 8572.754681795588, 2836242712.168021, 958.0]
[2019-03-27 00:49:12,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1188953e-13 9.9997127e-01 1.3182628e-24 2.8701066e-05 3.4036416e-11], sum to 1.0000
[2019-03-27 00:49:12,289] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4719
[2019-03-27 00:49:12,296] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3573041311786649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549303.5648226578, 549303.5648226573, 170556.7196522344], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1015200.0000, 
sim time next is 1015800.0000, 
raw observation next is [21.71666666666667, 96.83333333333334, 1.0, 2.0, 0.3598122273471944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553296.8323661025, 553296.8323661019, 170896.1081969232], 
processed observation next is [1.0, 0.782608695652174, 0.22827804107424976, 0.9683333333333334, 1.0, 1.0, 0.22868943053878846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15369356454613958, 0.15369356454613942, 0.25506881820436295], 
reward next is 0.7449, 
noisyNet noise sample is [array([2.4326277], dtype=float32), 0.7717465]. 
=============================================
[2019-03-27 00:49:13,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2009988e-13 9.9996305e-01 1.7534779e-22 3.6982121e-05 5.6275345e-12], sum to 1.0000
[2019-03-27 00:49:13,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9459
[2019-03-27 00:49:13,949] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.0, 1.0, 2.0, 0.3497808518227349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537152.0915332764, 537152.091533277, 169531.2477243921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1047600.0000, 
sim time next is 1048200.0000, 
raw observation next is [21.56666666666667, 96.83333333333334, 1.0, 2.0, 0.3545090253303371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546747.7960604882, 546747.7960604876, 170392.7842107117], 
processed observation next is [1.0, 0.13043478260869565, 0.22116903633491333, 0.9683333333333334, 1.0, 1.0, 0.22230003051847846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15187438779458007, 0.1518743877945799, 0.25431758837419655], 
reward next is 0.7457, 
noisyNet noise sample is [array([1.3737199], dtype=float32), -0.5364856]. 
=============================================
[2019-03-27 00:49:16,808] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3452134e-13 9.9994624e-01 6.4173683e-23 5.3705280e-05 5.8900454e-11], sum to 1.0000
[2019-03-27 00:49:16,823] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8460
[2019-03-27 00:49:16,830] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 72.5, 1.0, 2.0, 0.3301714180145942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516698.9302897274, 516698.9302897268, 168180.979037402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [24.2, 73.0, 1.0, 2.0, 0.3295946731302897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516485.2663641399, 516485.2663641393, 168181.3163408892], 
processed observation next is [1.0, 0.8260869565217391, 0.3459715639810427, 0.73, 1.0, 1.0, 0.19228273871119236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14346812954559443, 0.14346812954559424, 0.2510168900610287], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.0539267], dtype=float32), -0.01262904]. 
=============================================
[2019-03-27 00:49:21,973] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.9731202e-14 9.9999917e-01 1.8691297e-21 8.7228528e-07 1.4074393e-10], sum to 1.0000
[2019-03-27 00:49:21,981] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4911
[2019-03-27 00:49:21,985] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.45, 57.83333333333333, 1.0, 2.0, 0.438165936429428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676160.7084975761, 676160.7084975761, 182190.6228728698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1185000.0000, 
sim time next is 1185600.0000, 
raw observation next is [27.3, 58.66666666666667, 1.0, 2.0, 0.3293212876343566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508207.406258325, 508207.4062583257, 167303.9016481441], 
processed observation next is [1.0, 0.7391304347826086, 0.4928909952606636, 0.5866666666666667, 1.0, 1.0, 0.19195335859561033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14116872396064584, 0.14116872396064603, 0.24970731589275239], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.80462193], dtype=float32), -0.1326857]. 
=============================================
[2019-03-27 00:49:31,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2935634e-13 9.9989402e-01 5.5956936e-22 1.0600701e-04 1.0768940e-10], sum to 1.0000
[2019-03-27 00:49:31,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8880
[2019-03-27 00:49:31,942] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 93.16666666666667, 1.0, 2.0, 0.3151404346517996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500459.3537640856, 500459.3537640856, 167096.4078086852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1361400.0000, 
sim time next is 1362000.0000, 
raw observation next is [20.96666666666667, 93.33333333333334, 1.0, 2.0, 0.3179549541097438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504321.7038382572, 504321.7038382572, 167376.3973238359], 
processed observation next is [1.0, 0.782608695652174, 0.1927330173775673, 0.9333333333333335, 1.0, 1.0, 0.178258980855113, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14008936217729365, 0.14008936217729365, 0.24981551839378494], 
reward next is 0.7502, 
noisyNet noise sample is [array([-0.6530178], dtype=float32), -1.969989]. 
=============================================
[2019-03-27 00:49:31,957] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.96004 ]
 [67.87634 ]
 [67.585434]
 [67.63373 ]
 [67.39159 ]], R is [[68.00414276]
 [68.07470703]
 [68.14540863]
 [68.21601105]
 [68.28646088]].
[2019-03-27 00:49:48,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3391384e-14 9.8491251e-01 2.1979225e-23 1.5087466e-02 3.2223854e-10], sum to 1.0000
[2019-03-27 00:49:48,681] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1391
[2019-03-27 00:49:48,685] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 98.0, 1.0, 2.0, 0.4211568343095108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614751.3162887272, 614751.3162887272, 175536.5775794529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1639800.0000, 
sim time next is 1640400.0000, 
raw observation next is [23.1, 98.0, 1.0, 2.0, 0.4212329881296998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 614862.2907928545, 614862.2907928539, 175547.2019075247], 
processed observation next is [1.0, 1.0, 0.2938388625592418, 0.98, 1.0, 1.0, 0.30269034714421655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17079508077579292, 0.17079508077579275, 0.2620107491157085], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.60254997], dtype=float32), 0.39526662]. 
=============================================
[2019-03-27 00:49:54,364] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.0966508e-14 9.9999964e-01 4.3176829e-22 3.2323058e-07 2.3066849e-11], sum to 1.0000
[2019-03-27 00:49:54,371] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6240
[2019-03-27 00:49:54,375] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 89.0, 1.0, 2.0, 0.5099836094240121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712627.5384505275, 712627.5384505275, 185175.3723690306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1717200.0000, 
sim time next is 1717800.0000, 
raw observation next is [26.33333333333333, 89.33333333333334, 1.0, 2.0, 0.5112618619308815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714414.3100991918, 714414.3100991918, 185379.5482666947], 
processed observation next is [1.0, 0.9130434782608695, 0.44707740916271704, 0.8933333333333334, 1.0, 1.0, 0.4111588697962428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19844841947199773, 0.19844841947199773, 0.2766858929353652], 
reward next is 0.7233, 
noisyNet noise sample is [array([-1.0323275], dtype=float32), 0.60723734]. 
=============================================
[2019-03-27 00:50:03,581] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 00:50:03,584] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:50:03,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:50:03,586] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:50:03,587] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:50:03,587] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:50:03,588] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:50:03,588] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:50:03,589] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:50:03,589] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:50:03,592] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:50:03,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-27 00:50:03,626] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-27 00:50:03,647] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-27 00:50:03,667] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-27 00:50:03,668] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-27 00:50:15,287] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0037664208]
[2019-03-27 00:50:15,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.4, 69.0, 1.0, 2.0, 0.6071000019488861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 913108.3554807594, 913108.35548076, 210147.9578186151]
[2019-03-27 00:50:15,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:50:15,294] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4661225e-14 9.9998629e-01 2.6398740e-23 1.3716066e-05 2.2323142e-12], sampled 0.5789021916096087
[2019-03-27 00:50:35,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0037664208]
[2019-03-27 00:50:35,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.8, 96.0, 1.0, 2.0, 0.6021880596426645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841520.8157285784, 841520.8157285777, 201146.3751646629]
[2019-03-27 00:50:35,916] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:50:35,920] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2235334e-13 9.9946195e-01 5.7491798e-23 5.3802074e-04 1.3237110e-11], sampled 0.8686551738662576
[2019-03-27 00:51:25,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0037664208]
[2019-03-27 00:51:25,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.26532622, 56.59557307, 1.0, 2.0, 0.5910049122682768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 825886.9784815122, 825886.9784815127, 199081.7646861386]
[2019-03-27 00:51:25,700] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:51:25,705] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5223817e-15 9.9999940e-01 1.0588111e-24 6.2371009e-07 1.4267057e-13], sampled 0.22916484452869879
[2019-03-27 00:51:38,533] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0037664208]
[2019-03-27 00:51:38,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 91.33333333333333, 1.0, 2.0, 0.501973089265814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701430.3119564517, 701430.3119564517, 183904.7902760261]
[2019-03-27 00:51:38,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:51:38,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5368870e-15 9.9999297e-01 2.4722209e-24 7.0356436e-06 5.2218950e-13], sampled 0.183159106353313
[2019-03-27 00:51:57,100] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8911.4609 2768899791.0211 395.0000
[2019-03-27 00:51:57,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8286.8714 3136457737.0061 795.0000
[2019-03-27 00:51:57,362] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8661.6145 2906609982.7703 527.0000
[2019-03-27 00:51:57,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8462.2621 2979506739.4324 763.0000
[2019-03-27 00:51:57,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8739.1478 2835197803.7113 563.0000
[2019-03-27 00:51:58,555] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 475000, evaluation results [475000.0, 8286.871380959796, 3136457737.006137, 795.0, 8661.61454834632, 2906609982.7702627, 527.0, 8911.460927570879, 2768899791.021127, 395.0, 8462.26205874325, 2979506739.432384, 763.0, 8739.147849046036, 2835197803.711292, 563.0]
[2019-03-27 00:52:06,203] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.5923442e-16 1.0000000e+00 9.6169255e-27 3.2497640e-09 9.4997955e-15], sum to 1.0000
[2019-03-27 00:52:06,213] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9381
[2019-03-27 00:52:06,219] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 95.66666666666667, 1.0, 2.0, 0.4658698185817562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655280.7119268682, 655280.7119268676, 178978.7878335293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2000400.0000, 
sim time next is 2001000.0000, 
raw observation next is [24.23333333333333, 95.83333333333333, 1.0, 2.0, 0.4650883855131099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654539.6954840053, 654539.6954840046, 178909.694666516], 
processed observation next is [0.0, 0.13043478260869565, 0.3475513428120062, 0.9583333333333333, 1.0, 1.0, 0.3555281753169999, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18181658207889034, 0.18181658207889015, 0.26702939502465073], 
reward next is 0.7330, 
noisyNet noise sample is [array([1.7048118], dtype=float32), -1.098354]. 
=============================================
[2019-03-27 00:52:06,231] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[76.28344 ]
 [76.360954]
 [76.51698 ]
 [76.51679 ]
 [76.38699 ]], R is [[76.222229  ]
 [76.19287109]
 [76.16370392]
 [76.13471985]
 [76.10591125]].
[2019-03-27 00:52:10,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2779325e-17 1.0000000e+00 4.0044244e-26 7.2876240e-11 4.4946475e-16], sum to 1.0000
[2019-03-27 00:52:10,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9171
[2019-03-27 00:52:10,166] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 95.5, 1.0, 2.0, 0.4674151160257047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655788.7725820044, 655788.772582005, 178992.472808752], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [24.33333333333334, 95.66666666666667, 1.0, 2.0, 0.4675708701703993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655868.527327139, 655868.5273271385, 178997.5540942362], 
processed observation next is [0.0, 0.043478260869565216, 0.35229067930489766, 0.9566666666666667, 1.0, 1.0, 0.3585191206872281, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1821857020353164, 0.18218570203531623, 0.26716052849886], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.6836742], dtype=float32), 0.7745509]. 
=============================================
[2019-03-27 00:52:15,195] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1417967e-14 9.9999988e-01 8.6980384e-22 1.2316916e-07 3.4502698e-12], sum to 1.0000
[2019-03-27 00:52:15,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0485
[2019-03-27 00:52:15,209] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 96.83333333333334, 1.0, 2.0, 0.5637536282232145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787791.14109876, 787791.14109876, 194182.0596976558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2177400.0000, 
sim time next is 2178000.0000, 
raw observation next is [24.5, 97.0, 1.0, 2.0, 0.558473430911198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780409.8657194518, 780409.8657194518, 193259.4443489104], 
processed observation next is [1.0, 0.21739130434782608, 0.3601895734597157, 0.97, 1.0, 1.0, 0.4680402782062627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2167805182554033, 0.2167805182554033, 0.28844693186404535], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.56700015], dtype=float32), 1.5553403]. 
=============================================
[2019-03-27 00:52:15,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.755287]
 [57.66151 ]
 [57.831516]
 [57.95556 ]
 [58.188545]], R is [[57.96349716]
 [58.0940361 ]
 [58.22060394]
 [58.34183121]
 [58.46112061]].
[2019-03-27 00:52:18,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7558627e-16 1.0000000e+00 4.3914514e-23 3.3831053e-11 1.7522049e-15], sum to 1.0000
[2019-03-27 00:52:18,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3080
[2019-03-27 00:52:18,155] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 87.16666666666667, 1.0, 2.0, 0.7610734100068578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1063664.519916042, 1063664.519916042, 234577.0383457468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2261400.0000, 
sim time next is 2262000.0000, 
raw observation next is [25.9, 87.33333333333334, 1.0, 2.0, 0.7136640862303004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 997374.8613126077, 997374.8613126082, 223819.1309060147], 
processed observation next is [1.0, 0.17391304347826086, 0.42654028436018954, 0.8733333333333334, 1.0, 1.0, 0.6550169713618077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27704857258683546, 0.27704857258683563, 0.33405840433733536], 
reward next is 0.6659, 
noisyNet noise sample is [array([-0.34427607], dtype=float32), 0.41183233]. 
=============================================
[2019-03-27 00:52:18,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.883884]
 [53.276142]
 [53.412685]
 [53.428722]
 [53.5026  ]], R is [[52.85224152]
 [52.97360229]
 [53.1255455 ]
 [53.27409363]
 [53.41604996]].
[2019-03-27 00:52:26,642] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3432697e-19 1.0000000e+00 2.4683263e-25 6.3636897e-16 1.1361506e-18], sum to 1.0000
[2019-03-27 00:52:26,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7733
[2019-03-27 00:52:26,652] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 77.66666666666667, 1.0, 2.0, 0.7111225497767187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 993821.2977888229, 993821.2977888235, 223263.03256951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2355600.0000, 
sim time next is 2356200.0000, 
raw observation next is [28.3, 77.0, 1.0, 2.0, 0.7166497822587589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001549.461716232, 1001549.461716232, 224478.8678559963], 
processed observation next is [1.0, 0.2608695652173913, 0.5402843601895735, 0.77, 1.0, 1.0, 0.6586141954924806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27820818381006446, 0.27820818381006446, 0.3350430863522333], 
reward next is 0.6650, 
noisyNet noise sample is [array([-1.0667607], dtype=float32), -0.1883673]. 
=============================================
[2019-03-27 00:52:34,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0787546e-26 1.0000000e+00 3.2462243e-33 6.7470058e-24 8.5868259e-25], sum to 1.0000
[2019-03-27 00:52:34,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2569
[2019-03-27 00:52:34,861] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4758616036700493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665060.7513070958, 665060.7513070963, 179916.7981240377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2624400.0000, 
sim time next is 2625000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4757732070636331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664937.0254277509, 664937.0254277509, 179903.5648397444], 
processed observation next is [0.0, 0.391304347826087, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3684014542935339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18470472928548637, 0.18470472928548637, 0.2685127833429021], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.75949854], dtype=float32), 0.04387999]. 
=============================================
[2019-03-27 00:52:34,876] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.50607 ]
 [76.45683 ]
 [76.440704]
 [76.41817 ]
 [76.40286 ]], R is [[76.48912048]
 [76.45569611]
 [76.42269135]
 [76.39012909]
 [76.35799408]].
[2019-03-27 00:52:38,570] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4204531e-11 9.9908447e-01 4.5247312e-17 9.1555400e-04 9.8463426e-10], sum to 1.0000
[2019-03-27 00:52:38,578] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5657
[2019-03-27 00:52:38,583] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2046609.457074848 W.
[2019-03-27 00:52:38,588] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.3, 80.66666666666667, 1.0, 2.0, 0.7318632580667458, 1.0, 2.0, 0.7318632580667458, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2046609.457074848, 2046609.457074848, 388061.4555683444], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [28.4, 80.0, 1.0, 2.0, 0.8109864189185876, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988994007105978, 6.9112, 168.9124933096485, 2030449.445654242, 1975259.850894001, 411201.0578863507], 
processed observation next is [1.0, 0.4782608695652174, 0.5450236966824644, 0.8, 1.0, 1.0, 0.7722727938778163, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007779400710597795, 0.0, 0.8294376706244064, 0.5640137349039561, 0.5486832919150003, 0.6137329222184339], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.15898694], dtype=float32), -0.20004962]. 
=============================================
[2019-03-27 00:52:43,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0405624e-28 1.0000000e+00 3.1652557e-36 1.4581045e-28 1.1457434e-25], sum to 1.0000
[2019-03-27 00:52:43,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1307
[2019-03-27 00:52:43,578] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4792904809376818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669724.8584926792, 669724.8584926798, 180414.8840751948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2634600.0000, 
sim time next is 2635200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4803232599636469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671168.442954809, 671168.4429548084, 180570.5467884914], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3738834457393336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18643567859855806, 0.18643567859855786, 0.26950827878879313], 
reward next is 0.7305, 
noisyNet noise sample is [array([1.5001525], dtype=float32), -0.9333058]. 
=============================================
[2019-03-27 00:52:45,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9859971e-26 1.0000000e+00 2.5262589e-36 1.0671190e-26 1.3126145e-25], sum to 1.0000
[2019-03-27 00:52:45,251] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6132
[2019-03-27 00:52:45,258] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 89.0, 1.0, 2.0, 0.4406684354096254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635592.2341938505, 635592.2341938505, 177369.741876003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2658600.0000, 
sim time next is 2659200.0000, 
raw observation next is [24.33333333333334, 89.0, 1.0, 2.0, 0.4329971793528517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628663.3576408663, 628663.3576408663, 176793.8160702822], 
processed observation next is [0.0, 0.782608695652174, 0.35229067930489766, 0.89, 1.0, 1.0, 0.31686407150945994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1746287104557962, 0.1746287104557962, 0.2638713672690779], 
reward next is 0.7361, 
noisyNet noise sample is [array([0.78706], dtype=float32), 1.1004657]. 
=============================================
[2019-03-27 00:52:51,011] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0896752e-28 1.0000000e+00 1.4422063e-35 1.1199785e-27 1.1698843e-25], sum to 1.0000
[2019-03-27 00:52:51,018] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2778
[2019-03-27 00:52:51,024] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 97.0, 1.0, 2.0, 0.3618353811237724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552027.6834196736, 552027.6834196736, 170659.8012869209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2755800.0000, 
sim time next is 2756400.0000, 
raw observation next is [22.0, 98.0, 1.0, 2.0, 0.3659199661941329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556163.915749069, 556163.9157490697, 170946.395624512], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.98, 1.0, 1.0, 0.236048152041124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1544899765969636, 0.15448997659696379, 0.2551438740664358], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.5380508], dtype=float32), -1.2278575]. 
=============================================
[2019-03-27 00:52:54,408] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 00:52:54,409] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:52:54,410] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:52:54,411] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:52:54,412] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:52:54,412] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:52:54,413] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:52:54,414] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:52:54,413] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:52:54,414] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:52:54,414] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:52:54,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-27 00:52:54,445] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-27 00:52:54,447] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-27 00:52:54,491] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-27 00:52:54,512] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-27 00:52:56,967] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003434515]
[2019-03-27 00:52:56,967] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.73333333333333, 73.5, 1.0, 2.0, 0.4421597818975433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640725.0104903362, 640725.0104903362, 177959.0368951401]
[2019-03-27 00:52:56,968] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:52:56,972] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.2736336e-24 1.0000000e+00 2.4625064e-31 1.1683914e-23 2.8166840e-23], sampled 0.16741850525210944
[2019-03-27 00:53:01,181] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003434515]
[2019-03-27 00:53:01,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.6, 75.0, 1.0, 2.0, 0.2560203251143444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418924.0461048051, 418924.0461048051, 161497.9383172683]
[2019-03-27 00:53:01,184] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:53:01,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0361193e-23 1.0000000e+00 1.9020439e-30 1.2986511e-22 2.0873940e-22], sampled 0.8877522286461683
[2019-03-27 00:53:41,620] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003434515]
[2019-03-27 00:53:41,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.59492341666667, 84.30685890333334, 1.0, 2.0, 0.8807549520427531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1231026.5665342, 1231026.5665342, 264701.1345528335]
[2019-03-27 00:53:41,622] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:53:41,624] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2830709e-22 1.0000000e+00 3.8482063e-29 1.6356300e-21 2.2695657e-21], sampled 0.8345619696611917
[2019-03-27 00:53:43,285] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003434515]
[2019-03-27 00:53:43,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.176345665, 86.724008555, 1.0, 2.0, 0.5087574932827148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710913.647073361, 710913.6470733604, 184978.686894246]
[2019-03-27 00:53:43,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 00:53:43,290] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4072217e-23 1.0000000e+00 6.8849108e-31 5.4023195e-23 8.3058357e-23], sampled 0.24503916692062955
[2019-03-27 00:54:15,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.003434515]
[2019-03-27 00:54:15,561] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.45, 94.5, 1.0, 2.0, 0.629641901732847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879901.7650858628, 879901.7650858628, 206397.7727407626]
[2019-03-27 00:54:15,563] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:54:15,564] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8421529e-23 1.0000000e+00 8.0356549e-31 6.1358207e-23 8.5144875e-23], sampled 0.3621493850366251
[2019-03-27 00:54:49,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9775 2779206587.5399 933.0000
[2019-03-27 00:54:49,700] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 00:54:49,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-27 00:54:49,781] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9141 3164264062.0750 1778.0000
[2019-03-27 00:54:49,880] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5144 2842492707.2201 1131.0000
[2019-03-27 00:54:50,897] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 500000, evaluation results [500000.0, 7881.914118866076, 3164264062.0749626, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8659.977453803283, 2779206587.539943, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.514395919725, 2842492707.2200756, 1131.0]
[2019-03-27 00:55:00,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9445815e-24 1.0000000e+00 7.2316060e-32 1.0584659e-21 1.0959161e-23], sum to 1.0000
[2019-03-27 00:55:00,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9894
[2019-03-27 00:55:00,888] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.5099191171375111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808332.3011028171, 808332.3011028176, 196071.1602527268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2986800.0000, 
sim time next is 2987400.0000, 
raw observation next is [20.83333333333333, 95.0, 1.0, 2.0, 0.5252021543128828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831588.4355212437, 831588.4355212437, 198793.9828026842], 
processed observation next is [1.0, 0.5652173913043478, 0.1864139020537123, 0.95, 1.0, 1.0, 0.4279544027866057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23099678764478992, 0.23099678764478992, 0.2967074370189316], 
reward next is 0.7033, 
noisyNet noise sample is [array([-0.55214757], dtype=float32), -1.6679331]. 
=============================================
[2019-03-27 00:55:07,247] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.323729e-24 1.000000e+00 9.060013e-34 8.608669e-25 3.878827e-24], sum to 1.0000
[2019-03-27 00:55:07,254] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0799
[2019-03-27 00:55:07,259] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.4113425000650328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607420.662355208, 607420.662355208, 175045.2776978148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3094800.0000, 
sim time next is 3095400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.4068317331916693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603477.5571900024, 603477.557190003, 174755.8674228257], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.95, 1.0, 1.0, 0.2853394375803245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16763265477500067, 0.16763265477500083, 0.2608296528698891], 
reward next is 0.7392, 
noisyNet noise sample is [array([0.57912743], dtype=float32), 0.042420167]. 
=============================================
[2019-03-27 00:55:14,765] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6068654e-26 1.0000000e+00 3.0125558e-34 2.7196952e-28 5.0197207e-27], sum to 1.0000
[2019-03-27 00:55:14,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7391
[2019-03-27 00:55:14,778] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4564397554793406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646267.9345350269, 646267.9345350263, 178147.4588664022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3216000.0000, 
sim time next is 3216600.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4566762021398829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 178181.9071402132], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3453930146263649, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17961178347060539, 0.17961178347060539, 0.26594314498539284], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.93272156], dtype=float32), 0.23806773]. 
=============================================
[2019-03-27 00:55:17,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4981735e-26 1.0000000e+00 2.4393441e-34 3.0163957e-27 9.3462927e-26], sum to 1.0000
[2019-03-27 00:55:17,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6894
[2019-03-27 00:55:17,807] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.5350485252847587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747664.420231969, 747664.4202319696, 189269.4674526293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [28.0, 80.66666666666667, 1.0, 2.0, 0.530733519724695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741632.6264427055, 741632.6264427049, 188551.2806606337], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8066666666666668, 1.0, 1.0, 0.4346186984634879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20600906290075152, 0.20600906290075138, 0.28141982188154285], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.41201675], dtype=float32), -0.99384344]. 
=============================================
[2019-03-27 00:55:26,946] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1234105e-18 1.0000000e+00 4.6820441e-24 1.2126889e-16 6.3508301e-18], sum to 1.0000
[2019-03-27 00:55:26,956] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7426
[2019-03-27 00:55:26,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 80.33333333333334, 1.0, 2.0, 0.6953607343293676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 971783.472008448, 971783.4720084474, 219843.3529516213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3555600.0000, 
sim time next is 3556200.0000, 
raw observation next is [26.58333333333334, 80.66666666666666, 1.0, 2.0, 0.6873813027547047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960626.9795823063, 960626.9795823058, 218141.3270338594], 
processed observation next is [1.0, 0.13043478260869565, 0.45892575039494504, 0.8066666666666665, 1.0, 1.0, 0.6233509671743429, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26684082766175177, 0.2668408276617516, 0.32558407019979013], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.18210201], dtype=float32), -0.6458717]. 
=============================================
[2019-03-27 00:55:28,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7486141e-09 9.9919707e-01 1.4313554e-14 8.0293784e-04 3.3991163e-09], sum to 1.0000
[2019-03-27 00:55:28,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7192
[2019-03-27 00:55:28,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2702678.178433403 W.
[2019-03-27 00:55:28,360] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.501277858167481, 6.9112, 168.9100299436093, 2702678.178433403, 2284063.889926457, 474789.7758081086], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3499800.0000, 
sim time next is 3500400.0000, 
raw observation next is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5345634359163883, 1.0, 1.0, 0.5345634359163883, 1.0, 2.0, 0.928359905357109, 6.9112, 6.9112, 170.5573041426782, 2242509.798956743, 2242509.798956743, 439870.9270851807], 
processed observation next is [1.0, 0.5217391304347826, 0.7314375987361774, 0.6566666666666667, 1.0, 1.0, 0.43923305532094975, 1.0, 0.5, 0.43923305532094975, 1.0, 1.0, 0.9126340309233036, 0.0, 0.0, 0.8375144448122397, 0.6229193885990952, 0.6229193885990952, 0.6565237717689264], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09172178], dtype=float32), 0.3659015]. 
=============================================
[2019-03-27 00:55:35,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4906570e-18 1.0000000e+00 8.0909839e-25 5.1897897e-17 3.4185760e-17], sum to 1.0000
[2019-03-27 00:55:35,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9899
[2019-03-27 00:55:35,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.0, 1.0, 2.0, 0.6731278364712743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940698.6977311402, 940698.6977311409, 215147.9812941711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3558000.0000, 
sim time next is 3558600.0000, 
raw observation next is [26.25, 82.5, 1.0, 2.0, 0.6740105933406464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941932.9008730919, 941932.9008730913, 215331.5916166144], 
processed observation next is [1.0, 0.17391304347826086, 0.4431279620853081, 0.825, 1.0, 1.0, 0.6072416787236703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2616480280203033, 0.2616480280203031, 0.3213904352486782], 
reward next is 0.6786, 
noisyNet noise sample is [array([0.04848159], dtype=float32), 2.1380901]. 
=============================================
[2019-03-27 00:55:37,772] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1656438e-20 1.0000000e+00 5.6229389e-26 3.9085843e-21 2.2627916e-18], sum to 1.0000
[2019-03-27 00:55:37,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6190
[2019-03-27 00:55:37,789] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5192437556441211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725571.6582813936, 725571.6582813943, 186666.0397329655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3624600.0000, 
sim time next is 3625200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5191185488877897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725396.6393389122, 725396.6393389116, 186645.7186011872], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42062475769613217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20149906648303118, 0.201499066483031, 0.278575699404757], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.4708206], dtype=float32), -0.8765904]. 
=============================================
[2019-03-27 00:55:40,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.800229e-28 1.000000e+00 3.142640e-34 8.820661e-31 1.910279e-26], sum to 1.0000
[2019-03-27 00:55:40,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9284
[2019-03-27 00:55:40,653] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 66.83333333333334, 1.0, 2.0, 0.5622856942543256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785739.0867143802, 785739.0867143802, 193931.2111422897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3780600.0000, 
sim time next is 3781200.0000, 
raw observation next is [31.66666666666667, 66.66666666666667, 1.0, 2.0, 0.5576751982178993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779294.0074643632, 779294.0074643632, 193126.7232787446], 
processed observation next is [1.0, 0.782608695652174, 0.6998420221169038, 0.6666666666666667, 1.0, 1.0, 0.46707855206975823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21647055762898978, 0.21647055762898978, 0.2882488407145442], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.5112489], dtype=float32), 0.54976135]. 
=============================================
[2019-03-27 00:55:41,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0691682e-07 9.9766576e-01 3.5721144e-13 2.3187075e-03 1.5487458e-05], sum to 1.0000
[2019-03-27 00:55:41,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2081
[2019-03-27 00:55:41,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2008351.18384229 W.
[2019-03-27 00:55:41,358] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.4787966610956633, 1.0, 2.0, 0.4787966610956633, 1.0, 2.0, 0.8268986896925943, 6.9112, 6.9112, 170.5573041426782, 2008351.18384229, 2008351.18384229, 399999.7611287605], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3662400.0000, 
sim time next is 3663000.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.7480662913262591, 1.0, 2.0, 0.7480662913262591, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2091964.460173571, 2091964.460173571, 395379.6608989409], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.72, 1.0, 1.0, 0.6964654112364568, 1.0, 1.0, 0.6964654112364568, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5811012389371031, 0.5811012389371031, 0.5901188968640909], 
reward next is 0.4099, 
noisyNet noise sample is [array([-0.7965949], dtype=float32), -0.1436541]. 
=============================================
[2019-03-27 00:55:41,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[31.77899 ]
 [31.672554]
 [31.685486]
 [33.063805]
 [35.221237]], R is [[31.96298981]
 [32.04634476]
 [31.72588158]
 [31.79714775]
 [31.89785385]].
[2019-03-27 00:55:42,047] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.2655018e-09 9.9981743e-01 1.5624161e-13 1.8249583e-04 1.2893298e-07], sum to 1.0000
[2019-03-27 00:55:42,056] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0512
[2019-03-27 00:55:42,065] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2701773.204656697 W.
[2019-03-27 00:55:42,070] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.9658847674975698, 1.0, 2.0, 0.9658847674975698, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2701773.204656697, 2701773.204656698, 508708.0352241846], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3684600.0000, 
sim time next is 3685200.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.6860330228700703, 1.0, 2.0, 0.6636065509492977, 1.0, 1.0, 1.03, 7.005096630898676, 6.9112, 170.5573041426782, 2784452.74758819, 2717190.771404725, 516952.9180631878], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.6217265335783979, 1.0, 1.0, 0.59470668789072, 1.0, 0.5, 1.0365853658536586, 0.009389663089867639, 0.0, 0.8375144448122397, 0.773459096552275, 0.7547752142790903, 0.7715715194972952], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03298185], dtype=float32), -1.5127237]. 
=============================================
[2019-03-27 00:55:46,746] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 00:55:46,747] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:55:46,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:55:46,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:55:46,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:55:46,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:55:46,750] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:55:46,751] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:55:46,753] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:55:46,753] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:55:46,751] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:55:46,776] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-27 00:55:46,798] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-27 00:55:46,800] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-27 00:55:46,849] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-27 00:55:46,876] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-27 00:55:53,572] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:55:53,574] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.75205902, 72.46291393999999, 1.0, 2.0, 0.2907919165002765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 481371.6469739987, 481371.6469739981, 165102.2346710436]
[2019-03-27 00:55:53,576] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:55:53,579] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5992497e-23 1.0000000e+00 7.8137521e-30 1.5260081e-26 4.4694148e-22], sampled 0.3558299748544461
[2019-03-27 00:56:08,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:56:08,005] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.39499508333333, 96.29420321333333, 1.0, 2.0, 0.294960565908081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477165.8549801549, 477165.8549801555, 165446.469576843]
[2019-03-27 00:56:08,007] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:56:08,010] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0262060e-23 1.0000000e+00 4.8146846e-30 6.4264712e-27 2.6397300e-22], sampled 0.823487441717355
[2019-03-27 00:56:09,738] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:56:09,738] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 61.0, 1.0, 2.0, 0.357913139627097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547127.1069043688, 547127.1069043695, 170280.8052184762]
[2019-03-27 00:56:09,739] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:56:09,742] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.7294035e-24 1.0000000e+00 1.7077541e-30 4.1286214e-27 1.4145528e-22], sampled 0.43621550604020165
[2019-03-27 00:56:17,797] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:56:17,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.13333333333333, 96.0, 1.0, 2.0, 0.4602961970323435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649894.0932472008, 649894.0932472001, 178476.709614409]
[2019-03-27 00:56:17,802] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:56:17,805] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.10002914e-23 1.00000000e+00 2.27105358e-30 4.92869971e-27
 1.71956708e-22], sampled 0.1150114142673423
[2019-03-27 00:56:58,511] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:56:58,512] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.10175894333333, 90.27486234, 1.0, 2.0, 0.5135159060036897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717565.073647611, 717565.0736476115, 185740.6136379818]
[2019-03-27 00:56:58,514] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:56:58,517] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2740091e-22 1.0000000e+00 4.5518683e-29 8.1055920e-26 1.7849942e-21], sampled 0.3394623480761728
[2019-03-27 00:56:58,972] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:56:58,972] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.76666666666667, 64.0, 1.0, 2.0, 0.6789346933630932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 948817.4272463896, 948817.4272463889, 216370.5943761389]
[2019-03-27 00:56:58,973] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:56:58,978] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1640693e-23 1.0000000e+00 9.9614363e-30 9.9026880e-27 3.3620515e-22], sampled 0.537587016560167
[2019-03-27 00:57:25,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:57:25,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 95.0, 1.0, 2.0, 0.4286546478759184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632409.6515784311, 632409.6515784311, 177419.2323338605]
[2019-03-27 00:57:25,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 00:57:25,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2782095e-23 1.0000000e+00 2.8465406e-30 5.0112069e-27 1.8490464e-22], sampled 0.654003288936016
[2019-03-27 00:57:27,855] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0025304642]
[2019-03-27 00:57:27,856] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.081574, 50.60030628, 1.0, 2.0, 0.4764000781780995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667383.2321681911, 667383.2321681911, 180200.1098135459]
[2019-03-27 00:57:27,858] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:57:27,860] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7213174e-24 1.0000000e+00 1.4935808e-30 1.9456008e-27 7.8689084e-23], sampled 0.7855021835246
[2019-03-27 00:57:39,999] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5883 2927390983.0870 1338.0000
[2019-03-27 00:57:40,747] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164139523.4469 1778.0000
[2019-03-27 00:57:40,933] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2711 2779256177.9474 933.0000
[2019-03-27 00:57:41,184] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-27 00:57:41,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.5529 2842640641.5065 1131.0000
[2019-03-27 00:57:42,292] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 525000, evaluation results [525000.0, 7883.41881391125, 3164139523.4469013, 1778.0, 8253.588268818321, 2927390983.087013, 1338.0, 8661.271088261434, 2779256177.947352, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8494.552924859718, 2842640641.5064535, 1131.0]
[2019-03-27 00:57:43,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7432347e-26 1.0000000e+00 4.0164497e-33 3.9768584e-32 7.5604324e-26], sum to 1.0000
[2019-03-27 00:57:43,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7170
[2019-03-27 00:57:43,990] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 69.0, 1.0, 2.0, 0.5395623729842705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753974.2050768668, 753974.2050768662, 190027.7037936371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3784800.0000, 
sim time next is 3785400.0000, 
raw observation next is [30.5, 70.5, 1.0, 2.0, 0.5423694498246604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757898.1602936431, 757898.1602936437, 190501.6773617952], 
processed observation next is [1.0, 0.8260869565217391, 0.6445497630331753, 0.705, 1.0, 1.0, 0.4486378913550125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21052726674823422, 0.21052726674823435, 0.2843308617340227], 
reward next is 0.7157, 
noisyNet noise sample is [array([1.909974], dtype=float32), 0.107721455]. 
=============================================
[2019-03-27 00:57:58,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7522585e-06 4.8378956e-01 6.4495187e-10 5.1605880e-01 1.4686138e-04], sum to 1.0000
[2019-03-27 00:57:58,051] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5207
[2019-03-27 00:57:58,057] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.021260311010145, 1.0, 2.0, 1.021260311010145, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2856846.808401423, 2856846.808401424, 541968.5050010285], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4023600.0000, 
sim time next is 4024200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 1.033376155358523, 1.0, 2.0, 1.033376155358523, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2890778.57085033, 2890778.570850331, 549484.2347369599], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6, 1.0, 1.0, 1.0402122353717145, 1.0, 1.0, 1.0402122353717145, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.802994047458425, 0.8029940474584253, 0.8201257234879998], 
reward next is 0.1799, 
noisyNet noise sample is [array([-1.1451486], dtype=float32), -1.7207125]. 
=============================================
[2019-03-27 00:58:13,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.5177588e-10 1.4293520e-02 5.9392304e-16 8.9635019e-04 9.8481017e-01], sum to 1.0000
[2019-03-27 00:58:13,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9064
[2019-03-27 00:58:13,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 0.4028599901900762, 1.0, 2.0, 0.4028599901900762, 1.0, 2.0, 0.6996345751255659, 6.9112, 6.9112, 170.5573041426782, 1689577.561033702, 1689577.561033702, 354478.6804737086], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4333800.0000, 
sim time next is 4334400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.3484115291381257, 1.0, 2.0, 0.3484115291381257, 1.0, 2.0, 0.6050756046595513, 6.9112, 6.9112, 170.5573041426782, 1461067.391034784, 1461067.391034784, 326171.0150729573], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 0.21495364956400684, 1.0, 1.0, 0.21495364956400684, 1.0, 1.0, 0.5183848837311601, 0.0, 0.0, 0.8375144448122397, 0.40585205306521777, 0.40585205306521777, 0.4868224105566527], 
reward next is 0.5132, 
noisyNet noise sample is [array([-0.00256784], dtype=float32), -0.7620922]. 
=============================================
[2019-03-27 00:58:21,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7622096e-26 1.0000000e+00 1.6703357e-35 1.8531174e-33 1.8741057e-23], sum to 1.0000
[2019-03-27 00:58:21,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3386
[2019-03-27 00:58:21,774] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 88.16666666666667, 1.0, 2.0, 0.6086994969410618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850623.793003844, 850623.793003844, 202377.6955014523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4417800.0000, 
sim time next is 4418400.0000, 
raw observation next is [29.0, 87.33333333333334, 1.0, 2.0, 0.6034250682298955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 843250.1454380996, 843250.145438099, 201385.6120817494], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.8733333333333334, 1.0, 1.0, 0.5221988773854163, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23423615151058322, 0.23423615151058308, 0.3005755404205215], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.29298732], dtype=float32), 1.3739535]. 
=============================================
[2019-03-27 00:58:23,826] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5172184e-27 1.0000000e+00 1.6392591e-36 5.6000115e-34 2.5448479e-22], sum to 1.0000
[2019-03-27 00:58:23,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6589
[2019-03-27 00:58:23,843] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 68.33333333333334, 1.0, 2.0, 0.6150307922152222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859475.006873966, 859475.006873966, 203579.8147868337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4454400.0000, 
sim time next is 4455000.0000, 
raw observation next is [32.5, 69.0, 1.0, 2.0, 0.6109733258951872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853802.6200466754, 853802.6200466754, 202808.4581071348], 
processed observation next is [0.0, 0.5652173913043478, 0.7393364928909952, 0.69, 1.0, 1.0, 0.5312931637291411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23716739445740984, 0.23716739445740984, 0.3026991912046788], 
reward next is 0.6973, 
noisyNet noise sample is [array([0.5117493], dtype=float32), 0.036333717]. 
=============================================
[2019-03-27 00:58:23,852] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.53259]
 [78.41554]
 [78.39963]
 [78.25995]
 [78.14941]], R is [[78.55241394]
 [78.46304321]
 [78.37030792]
 [78.28237915]
 [78.19389343]].
[2019-03-27 00:58:26,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8151405e-26 1.0000000e+00 5.6044568e-34 1.0644156e-33 3.1334076e-21], sum to 1.0000
[2019-03-27 00:58:26,438] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9045
[2019-03-27 00:58:26,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5078265170962295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709612.3104869892, 709612.3104869892, 184830.9208811251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4501800.0000, 
sim time next is 4502400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.507506380853706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709164.8182007936, 709164.8182007929, 184780.0542154979], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40663419379964577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19699022727799823, 0.19699022727799803, 0.275791125694773], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.29226768], dtype=float32), 0.8088806]. 
=============================================
[2019-03-27 00:58:26,934] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1010513e-27 1.0000000e+00 4.6403925e-35 4.6007844e-36 6.5001293e-23], sum to 1.0000
[2019-03-27 00:58:26,945] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9752
[2019-03-27 00:58:26,949] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 87.33333333333333, 1.0, 2.0, 0.4945839006506602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691101.6967614151, 691101.6967614151, 182751.5949901478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4509600.0000, 
sim time next is 4510200.0000, 
raw observation next is [26.0, 88.16666666666667, 1.0, 2.0, 0.4983186439381167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696322.1120910884, 696322.1120910884, 183332.8751632916], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.8816666666666667, 1.0, 1.0, 0.39556463125074304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19342280891419122, 0.19342280891419122, 0.27363115696013673], 
reward next is 0.7264, 
noisyNet noise sample is [array([0.17763633], dtype=float32), 0.1294231]. 
=============================================
[2019-03-27 00:58:31,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8603292e-11 9.9999988e-01 1.6744108e-14 1.1360294e-10 8.7976268e-08], sum to 1.0000
[2019-03-27 00:58:31,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3813
[2019-03-27 00:58:31,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2551482.90156435 W.
[2019-03-27 00:58:31,841] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 87.33333333333334, 1.0, 2.0, 0.6081404954272743, 1.0, 1.0, 0.6081404954272743, 1.0, 1.0, 1.03, 6.940584511454581, 6.9112, 170.5573041426782, 2551482.90156435, 2530433.58072393, 490948.5622020435], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4587600.0000, 
sim time next is 4588200.0000, 
raw observation next is [28.0, 86.5, 1.0, 2.0, 0.6966673213573812, 1.0, 2.0, 0.6966673213573812, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1948096.752186165, 1948096.752186166, 372704.0722185093], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.865, 1.0, 1.0, 0.6345389413944351, 1.0, 1.0, 0.6345389413944351, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5411379867183792, 0.5411379867183794, 0.5562747346544915], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0299774], dtype=float32), -0.20459442]. 
=============================================
[2019-03-27 00:58:38,078] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 00:58:38,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 00:58:38,081] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 00:58:38,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:58:38,081] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 00:58:38,082] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:58:38,083] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 00:58:38,084] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 00:58:38,083] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:58:38,086] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:58:38,086] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 00:58:38,107] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-27 00:58:38,128] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-27 00:58:38,128] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-27 00:58:38,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-27 00:58:38,193] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-27 00:58:41,838] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 00:58:41,841] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.95, 69.5, 1.0, 2.0, 0.2314552334155947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 384546.3297782692, 384546.3297782685, 158846.045339912]
[2019-03-27 00:58:41,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 00:58:41,846] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0897831e-23 1.0000000e+00 2.6282848e-30 4.2332188e-30 6.7108041e-19], sampled 0.9714530892974645
[2019-03-27 00:58:56,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 00:58:56,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.37946825, 56.64332406166666, 1.0, 2.0, 0.3378848046738988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533702.228270861, 533702.2282708616, 169616.2633980733]
[2019-03-27 00:58:56,528] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 00:58:56,530] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.00403531e-22 1.00000000e+00 1.00181080e-28 1.57324643e-28
 1.01728616e-17], sampled 0.8715133978429808
[2019-03-27 00:59:38,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 00:59:38,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 89.0, 1.0, 2.0, 0.9006624569647119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510424, 1258867.677126204, 1258867.677126204, 270122.7089204718]
[2019-03-27 00:59:38,690] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 00:59:38,692] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0707826e-18 1.0000000e+00 3.4436942e-24 1.9792699e-22 7.3391129e-14], sampled 0.38816205962259054
[2019-03-27 01:00:07,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 01:00:07,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.23794579, 94.85533081, 1.0, 2.0, 0.5787341779585672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808732.9791901442, 808732.9791901442, 196850.6953448979]
[2019-03-27 01:00:07,664] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:00:07,666] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.03924485e-22 1.00000000e+00 3.34794874e-29 1.17431339e-28
 6.85521135e-18], sampled 0.22423778107340664
[2019-03-27 01:00:14,683] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 01:00:14,686] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.44008523500001, 76.30308219666667, 1.0, 2.0, 0.4834119428827139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684959.8408445138, 684959.8408445131, 182252.9211379982]
[2019-03-27 01:00:14,688] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:00:14,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0035657e-22 1.0000000e+00 1.8146463e-29 2.0972998e-29 3.0585743e-18], sampled 0.7219016548487373
[2019-03-27 01:00:17,137] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.007819789]
[2019-03-27 01:00:17,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 70.0, 1.0, 2.0, 0.5608036151571666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783667.2625929243, 783667.2625929243, 193669.4815470427]
[2019-03-27 01:00:17,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:00:17,145] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0509212e-23 1.0000000e+00 8.6291592e-31 1.2234882e-30 5.0956213e-19], sampled 0.6128251447435303
[2019-03-27 01:00:32,518] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7668 3007741835.6152 1768.0000
[2019-03-27 01:00:32,562] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.6826 2779589864.6538 940.0000
[2019-03-27 01:00:32,752] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.9853 3164055675.8472 1797.0000
[2019-03-27 01:00:32,783] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.2792 2842621717.2849 1140.0000
[2019-03-27 01:00:32,842] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.6137 2927735703.4797 1357.0000
[2019-03-27 01:00:33,858] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 550000, evaluation results [550000.0, 7874.985268887379, 3164055675.8471546, 1797.0, 8247.613740834346, 2927735703.4797397, 1357.0, 8656.682578111016, 2779589864.6537886, 940.0, 7997.766805652719, 3007741835.6152134, 1768.0, 8493.27924336375, 2842621717.2848706, 1140.0]
[2019-03-27 01:00:36,242] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0072571e-16 1.0000000e+00 1.3916503e-22 8.8190953e-20 4.3692147e-10], sum to 1.0000
[2019-03-27 01:00:36,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1219
[2019-03-27 01:00:36,265] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 70.0, 1.0, 2.0, 0.5203137056501967, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564216358, 727067.2777777623, 727067.277777763, 186842.7814167855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [30.5, 70.0, 1.0, 2.0, 0.5069622612916547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104088, 708404.2383726215, 708404.2383726222, 184697.3306834354], 
processed observation next is [1.0, 0.7391304347826086, 0.6445497630331753, 0.7, 1.0, 1.0, 0.4059786280622345, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451521937, 0.19677895510350596, 0.19677895510350615, 0.27566765773647073], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.323492], dtype=float32), 2.280874]. 
=============================================
[2019-03-27 01:00:36,864] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8980551e-25 1.0000000e+00 1.5322566e-32 1.9384385e-33 3.9910293e-21], sum to 1.0000
[2019-03-27 01:00:36,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5490
[2019-03-27 01:00:36,880] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5172390205246137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722769.3639248497, 722769.3639248497, 186341.2278548926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5176297980677346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723315.606932321, 723315.606932321, 186404.4491686033], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41883108200931873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20092100192564472, 0.20092100192564472, 0.2782155957740348], 
reward next is 0.7218, 
noisyNet noise sample is [array([-1.3459436], dtype=float32), -0.41177642]. 
=============================================
[2019-03-27 01:00:37,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4268270e-19 1.0000000e+00 7.1644113e-26 8.3327748e-24 8.8551974e-13], sum to 1.0000
[2019-03-27 01:00:37,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4460
[2019-03-27 01:00:37,566] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6332765027489061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884983.1045701196, 884983.1045701191, 207098.9061980101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4766400.0000, 
sim time next is 4767000.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.658356801585593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 920047.1792237106, 920047.1792237106, 212110.0031102683], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.5883816886573409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25556866089547514, 0.25556866089547514, 0.3165820941944303], 
reward next is 0.6834, 
noisyNet noise sample is [array([-0.9654285], dtype=float32), -1.5844047]. 
=============================================
[2019-03-27 01:00:37,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[59.77945 ]
 [59.35587 ]
 [59.61761 ]
 [59.753643]
 [59.575672]], R is [[60.09595871]
 [60.18589783]
 [60.25179672]
 [60.3214035 ]
 [60.40274048]].
[2019-03-27 01:00:44,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1626317e-10 9.9792814e-01 3.5448738e-18 1.6270665e-09 2.0718914e-03], sum to 1.0000
[2019-03-27 01:00:44,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2039
[2019-03-27 01:00:44,544] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1796271.616554048 W.
[2019-03-27 01:00:44,551] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6424179511760737, 1.0, 2.0, 0.6424179511760737, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1796271.616554048, 1796271.616554048, 350444.3024575792], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.4304311390725083, 1.0, 2.0, 0.4304311390725083, 1.0, 1.0, 0.7381092040017135, 6.911199999999999, 6.9112, 170.5573041426782, 1805307.184545545, 1805307.184545546, 368933.0282631643], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.3137724567138654, 1.0, 1.0, 0.3137724567138654, 1.0, 0.5, 0.6806209804898945, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.501474217929318, 0.5014742179293183, 0.5506463108405437], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8321823], dtype=float32), 0.45361894]. 
=============================================
[2019-03-27 01:00:45,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2160945e-14 9.9999964e-01 2.7753574e-20 7.4768875e-17 3.8680875e-07], sum to 1.0000
[2019-03-27 01:00:45,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6524
[2019-03-27 01:00:45,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2289065.422991056 W.
[2019-03-27 01:00:45,390] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 65.0, 1.0, 2.0, 0.5456510858038404, 1.0, 2.0, 0.5456510858038404, 1.0, 2.0, 0.9469724881999382, 6.911199999999999, 6.9112, 170.5573041426782, 2289065.422991056, 2289065.422991057, 448022.2119316785], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4886400.0000, 
sim time next is 4887000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.8486561214013737, 1.0, 2.0, 0.8486561214013737, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2373549.739269516, 2373549.739269516, 444251.8891926032], 
processed observation next is [1.0, 0.5652173913043478, 0.6919431279620853, 0.645, 1.0, 1.0, 0.8176579775920165, 1.0, 1.0, 0.8176579775920165, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.65931937201931, 0.65931937201931, 0.6630625211829898], 
reward next is 0.3369, 
noisyNet noise sample is [array([0.3126169], dtype=float32), 0.3726178]. 
=============================================
[2019-03-27 01:00:45,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[50.221035]
 [50.89939 ]
 [52.70017 ]
 [54.21595 ]
 [55.53754 ]], R is [[48.28144073]
 [48.12993622]
 [48.04547501]
 [48.01399231]
 [47.53385162]].
[2019-03-27 01:00:47,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6497787e-22 1.0000000e+00 1.0672628e-31 5.4261318e-31 1.4206787e-16], sum to 1.0000
[2019-03-27 01:00:47,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9419
[2019-03-27 01:00:47,063] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.508993293196418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711243.2530233152, 711243.2530233152, 185017.2070422437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4918800.0000, 
sim time next is 4919400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5094168162108175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711835.2624259198, 711835.2624259191, 185084.6938995246], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4089359231455632, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19773201734053328, 0.19773201734053308, 0.27624581179033525], 
reward next is 0.7238, 
noisyNet noise sample is [array([1.0909717], dtype=float32), 1.1258445]. 
=============================================
[2019-03-27 01:00:47,751] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1717631e-20 1.0000000e+00 2.2492697e-27 1.2862887e-25 1.2360123e-13], sum to 1.0000
[2019-03-27 01:00:47,761] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4877
[2019-03-27 01:00:47,767] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5092025124121199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711535.7040147028, 711535.7040147034, 185050.4403527521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4929000.0000, 
sim time next is 4929600.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5076671417904816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709389.5327505049, 709389.5327505049, 184806.0331013235], 
processed observation next is [1.0, 0.043478260869565216, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.40682788167527895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19705264798625136, 0.19705264798625136, 0.2758299001512291], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.31612867], dtype=float32), 0.34613216]. 
=============================================
[2019-03-27 01:00:48,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5814324e-17 1.0000000e+00 1.0621659e-23 4.8202590e-20 1.7833016e-11], sum to 1.0000
[2019-03-27 01:00:48,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6304
[2019-03-27 01:00:48,029] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.6910034200028303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965691.251060816, 965691.251060816, 218913.5188449852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4949400.0000, 
sim time next is 4950000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.7016581958525254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980588.3901806338, 980588.3901806338, 221201.8509231201], 
processed observation next is [1.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.6405520431958137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27238566393906494, 0.27238566393906494, 0.3301520163031643], 
reward next is 0.6698, 
noisyNet noise sample is [array([0.42144942], dtype=float32), -0.6347143]. 
=============================================
[2019-03-27 01:00:48,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.606266]
 [57.70747 ]
 [57.780586]
 [58.032787]
 [58.13834 ]], R is [[57.87967682]
 [57.97414398]
 [58.06677246]
 [58.15377808]
 [58.24528885]].
[2019-03-27 01:00:51,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7724026e-13 9.9999976e-01 2.5989769e-19 5.0329995e-15 2.5361857e-07], sum to 1.0000
[2019-03-27 01:00:51,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9121
[2019-03-27 01:00:51,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2266304.382481748 W.
[2019-03-27 01:00:51,596] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.9794967835741463, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980802694087928, 6.9112, 168.9124852628286, 2266304.382481748, 2216925.973516351, 457959.0200941567], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4986000.0000, 
sim time next is 4986600.0000, 
raw observation next is [30.83333333333334, 64.16666666666667, 1.0, 2.0, 0.3666606198262264, 1.0, 1.0, 0.3666606198262264, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1024855.028510209, 1024855.028510209, 264054.4278663458], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.6416666666666667, 1.0, 1.0, 0.23694050581473058, 1.0, 0.5, 0.23694050581473058, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.28468195236394694, 0.28468195236394694, 0.3941110863676803], 
reward next is 0.6059, 
noisyNet noise sample is [array([2.2730837], dtype=float32), 0.5064412]. 
=============================================
[2019-03-27 01:00:54,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5753597e-26 1.0000000e+00 4.7821819e-35 2.5828857e-35 9.8078346e-20], sum to 1.0000
[2019-03-27 01:00:54,382] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7962
[2019-03-27 01:00:54,395] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 83.16666666666666, 1.0, 2.0, 0.5178608159439005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723638.5322214159, 723638.5322214159, 186441.3968319923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5168534716223304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722230.4299762567, 722230.4299762567, 186278.3530277768], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41789574894256676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20061956388229352, 0.20061956388229352, 0.27802739257877135], 
reward next is 0.7220, 
noisyNet noise sample is [array([1.5710902], dtype=float32), 0.22260489]. 
=============================================
[2019-03-27 01:00:55,081] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4774408e-27 1.0000000e+00 1.5727736e-34 2.5216976e-34 1.5069229e-20], sum to 1.0000
[2019-03-27 01:00:55,095] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9084
[2019-03-27 01:00:55,100] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5364795520307343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749664.8095138671, 749664.8095138664, 189509.7604375454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5040000.0000, 
sim time next is 5040600.0000, 
raw observation next is [28.33333333333334, 81.0, 1.0, 2.0, 0.5360444949806962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749056.655939734, 749056.6559397334, 189436.6097146941], 
processed observation next is [0.0, 0.34782608695652173, 0.5418641390205374, 0.81, 1.0, 1.0, 0.44101746383216406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20807129331659277, 0.2080712933165926, 0.28274120852939416], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.5897809], dtype=float32), 0.9720866]. 
=============================================
[2019-03-27 01:01:01,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0373342e-24 1.0000000e+00 2.1162551e-32 1.3337572e-34 1.9363967e-20], sum to 1.0000
[2019-03-27 01:01:01,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1901
[2019-03-27 01:01:01,822] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5161800.0000, 
sim time next is 5162400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5409040698056533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755849.733772564, 755849.7337725633, 190253.2434629186], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44687237325982326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20995825938126778, 0.20995825938126758, 0.28396006487002773], 
reward next is 0.7160, 
noisyNet noise sample is [array([-1.5991127], dtype=float32), -0.5772035]. 
=============================================
[2019-03-27 01:01:18,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7369968e-24 1.0000000e+00 1.8708784e-29 3.2360223e-34 1.1844590e-20], sum to 1.0000
[2019-03-27 01:01:18,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7816
[2019-03-27 01:01:18,321] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 92.0, 1.0, 2.0, 0.9402653770338888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1314255.456743802, 1314255.456743803, 281256.9444133639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5457000.0000, 
sim time next is 5457600.0000, 
raw observation next is [27.7, 92.0, 1.0, 2.0, 0.9211757236426978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1287556.723439653, 1287556.723439653, 275832.0907662032], 
processed observation next is [1.0, 0.17391304347826086, 0.5118483412322274, 0.92, 1.0, 1.0, 0.9050309923405998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3576546453999036, 0.3576546453999036, 0.41168968771075104], 
reward next is 0.5883, 
noisyNet noise sample is [array([0.532133], dtype=float32), 0.1757832]. 
=============================================
[2019-03-27 01:01:26,845] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1820601e-12 9.9999988e-01 5.1103033e-17 8.1444024e-17 1.5347943e-07], sum to 1.0000
[2019-03-27 01:01:26,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2834
[2019-03-27 01:01:26,866] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2242353.583402504 W.
[2019-03-27 01:01:26,870] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [34.06666666666667, 48.66666666666666, 1.0, 2.0, 0.8017893465759612, 1.0, 2.0, 0.8017893465759612, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2242353.583402504, 2242353.583402504, 420736.4921588595], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5581200.0000, 
sim time next is 5581800.0000, 
raw observation next is [34.0, 49.0, 1.0, 2.0, 0.9390454658086403, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.980610483548588, 6.9112, 168.9125430898987, 2209688.016700676, 2160445.951268851, 446016.6662192448], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.49, 1.0, 1.0, 0.9265608021790848, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0069410483548588255, 0.0, 0.8294379150682243, 0.6138022268612989, 0.6001238753524586, 0.6656965167451415], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.079815], dtype=float32), -1.2993525]. 
=============================================
[2019-03-27 01:01:29,827] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 01:01:29,829] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:01:29,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:01:29,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:01:29,830] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:01:29,832] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:01:29,832] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:01:29,834] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:01:29,835] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:01:29,836] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:01:29,835] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:01:29,847] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-27 01:01:29,867] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-27 01:01:29,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-27 01:01:29,870] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-27 01:01:29,888] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-27 01:01:40,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:01:40,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.98333333333333, 71.0, 1.0, 2.0, 0.3471651322318877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537757.8466935619, 537757.8466935612, 169717.6166666314]
[2019-03-27 01:01:40,116] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:01:40,119] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3348344e-28 1.0000000e+00 3.8615596e-36 0.0000000e+00 9.3585441e-23], sampled 0.21662236876916752
[2019-03-27 01:01:45,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:01:45,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.73448507, 99.10250355, 1.0, 2.0, 0.3360802386000906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 527564.7307243075, 527564.7307243069, 169071.2121293555]
[2019-03-27 01:01:45,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:01:45,359] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9603548e-29 1.0000000e+00 1.7558319e-37 0.0000000e+00 1.8138213e-23], sampled 0.20572095616090436
[2019-03-27 01:02:26,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:02:26,178] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.0, 71.0, 1.0, 2.0, 0.5482574571115514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766128.9321654997, 766128.9321654991, 191501.8538912426]
[2019-03-27 01:02:26,178] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:02:26,181] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4166388e-29 1.0000000e+00 6.2187193e-38 0.0000000e+00 1.0367616e-23], sampled 0.7950311939291572
[2019-03-27 01:02:26,449] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:02:26,451] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 72.33333333333334, 1.0, 2.0, 0.523814094753995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731960.2782930987, 731960.2782930987, 187410.7504360561]
[2019-03-27 01:02:26,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:02:26,455] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.1432603e-29 1.0000000e+00 9.3917373e-37 0.0000000e+00 3.0271682e-23], sampled 0.1506517670198667
[2019-03-27 01:02:32,572] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:02:32,573] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 79.0, 1.0, 2.0, 0.624147732083174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872220.7092239243, 872220.7092239243, 205331.2684898376]
[2019-03-27 01:02:32,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:02:32,577] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1735787e-29 1.0000000e+00 1.0325626e-37 0.0000000e+00 1.7950505e-23], sampled 0.8074337667542735
[2019-03-27 01:03:02,041] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:03:02,044] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.15626300833333, 87.40031690333332, 1.0, 2.0, 0.6906447314667122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965189.7487899394, 965189.7487899401, 218838.8474385834]
[2019-03-27 01:03:02,045] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:03:02,048] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1917353e-24 1.0000000e+00 7.1148753e-32 6.0975605e-36 1.4244155e-19], sampled 0.3407914285169449
[2019-03-27 01:03:12,275] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:03:12,275] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.95, 68.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.534478512500931, 6.9112, 168.9094894519611, 1896223.945249768, 1454057.792851546, 311356.9578151481]
[2019-03-27 01:03:12,276] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:03:12,279] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.5671085e-23 1.0000000e+00 5.4050005e-30 1.7580337e-33 7.0602570e-18], sampled 0.8106689093970725
[2019-03-27 01:03:12,282] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1896223.945249768 W.
[2019-03-27 01:03:15,322] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:03:15,324] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.76810859, 76.08559066333335, 1.0, 2.0, 0.409862596862985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605161.481023507, 605161.481023507, 174831.7276936005]
[2019-03-27 01:03:15,325] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:03:15,328] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9822645e-28 1.0000000e+00 1.5846358e-36 0.0000000e+00 4.5890532e-23], sampled 0.8045757264680756
[2019-03-27 01:03:17,087] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.008461194]
[2019-03-27 01:03:17,091] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.64890481, 70.13131773, 1.0, 2.0, 0.4094241463761982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 606028.2183985478, 606028.2183985485, 174956.3408039542]
[2019-03-27 01:03:17,092] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:03:17,094] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0458788e-29 1.0000000e+00 2.8995729e-38 0.0000000e+00 4.5898038e-24], sampled 0.7396721566108238
[2019-03-27 01:03:23,748] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.1805 2779328422.4969 936.0000
[2019-03-27 01:03:24,094] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.8827 2927353531.9115 1343.0000
[2019-03-27 01:03:24,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0346 2842627125.5837 1131.0000
[2019-03-27 01:03:24,326] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.2629 3164117606.7271 1777.0000
[2019-03-27 01:03:24,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0966 3007867509.5417 1766.0000
[2019-03-27 01:03:25,400] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 575000, evaluation results [575000.0, 7884.262862809629, 3164117606.727107, 1777.0, 8253.882662082837, 2927353531.9115014, 1343.0, 8659.180484913304, 2779328422.496893, 936.0, 7996.096616164504, 3007867509.5416894, 1766.0, 8496.034603432998, 2842627125.5836673, 1131.0]
[2019-03-27 01:03:29,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7873537e-25 1.0000000e+00 4.6793418e-34 0.0000000e+00 2.9738571e-21], sum to 1.0000
[2019-03-27 01:03:29,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5158
[2019-03-27 01:03:29,790] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 87.0, 1.0, 2.0, 0.5164692057412986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721693.2897524032, 721693.2897524037, 186216.2916922576], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5700000.0000, 
sim time next is 5700600.0000, 
raw observation next is [26.55, 87.0, 1.0, 2.0, 0.5155360072540192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720388.8335553042, 720388.8335553035, 186065.6733481382], 
processed observation next is [0.0, 1.0, 0.4573459715639811, 0.87, 1.0, 1.0, 0.4163084424747219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20010800932091782, 0.20010800932091763, 0.2777099602211018], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.5296627], dtype=float32), -1.3433735]. 
=============================================
[2019-03-27 01:03:31,101] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7698885e-28 1.0000000e+00 3.1779838e-36 0.0000000e+00 3.0700588e-22], sum to 1.0000
[2019-03-27 01:03:31,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2061
[2019-03-27 01:03:31,117] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.23333333333333, 62.66666666666666, 1.0, 2.0, 0.5545847629796474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774973.8651334906, 774973.8651334906, 192590.779138714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5762400.0000, 
sim time next is 5763000.0000, 
raw observation next is [32.06666666666666, 63.83333333333334, 1.0, 2.0, 0.5572636009613726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778718.6315937896, 778718.6315937903, 193054.878127043], 
processed observation next is [0.0, 0.6956521739130435, 0.7187993680884674, 0.6383333333333334, 1.0, 1.0, 0.46658265176068986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21631073099827489, 0.21631073099827508, 0.28814160914484027], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.49656847], dtype=float32), -0.010226062]. 
=============================================
[2019-03-27 01:03:31,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.19692 ]
 [75.193115]
 [75.16973 ]
 [75.12219 ]
 [75.19814 ]], R is [[75.15219116]
 [75.11322021]
 [75.07526398]
 [75.03768921]
 [74.99733734]].
[2019-03-27 01:03:31,935] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0104423e-27 1.0000000e+00 2.1932686e-37 0.0000000e+00 7.3664866e-22], sum to 1.0000
[2019-03-27 01:03:31,942] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5486
[2019-03-27 01:03:31,951] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.3, 61.33333333333334, 1.0, 2.0, 0.5196873384969101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726191.7161211448, 726191.7161211448, 186738.4154674287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5740800.0000, 
sim time next is 5741400.0000, 
raw observation next is [31.45, 60.66666666666666, 1.0, 2.0, 0.5198684069498711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726444.820945256, 726444.8209452567, 186767.8669291593], 
processed observation next is [0.0, 0.43478260869565216, 0.6895734597156398, 0.6066666666666666, 1.0, 1.0, 0.421528201144423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017902280403489, 0.2017902280403491, 0.2787580103420288], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.33649972], dtype=float32), -2.07533]. 
=============================================
[2019-03-27 01:03:34,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5478200e-11 9.9998629e-01 2.4319823e-17 1.2772322e-15 1.3691445e-05], sum to 1.0000
[2019-03-27 01:03:34,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3706
[2019-03-27 01:03:34,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2451725.775155379 W.
[2019-03-27 01:03:34,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 67.0, 1.0, 2.0, 0.876580352341822, 1.0, 1.0, 0.876580352341822, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2451725.775155379, 2451725.775155378, 458878.6384050247], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5828400.0000, 
sim time next is 5829000.0000, 
raw observation next is [31.95, 66.66666666666667, 1.0, 2.0, 0.5526209580053076, 1.0, 2.0, 0.5526209580053076, 1.0, 1.0, 0.9597198495117538, 6.9112, 6.9112, 170.5573041426782, 2318331.912882507, 2318331.912882507, 453442.0886848237], 
processed observation next is [1.0, 0.4782608695652174, 0.7132701421800948, 0.6666666666666667, 1.0, 1.0, 0.4609891060304911, 1.0, 1.0, 0.4609891060304911, 1.0, 0.5, 0.9508778652582364, 0.0, 0.0, 0.8375144448122397, 0.6439810869118074, 0.6439810869118074, 0.6767792368430204], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.78961325], dtype=float32), -1.3785459]. 
=============================================
[2019-03-27 01:03:34,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[34.45297 ]
 [34.639557]
 [34.848797]
 [35.735264]
 [35.774452]], R is [[33.40564728]
 [33.07159042]
 [32.74087524]
 [32.41346741]
 [32.08933258]].
[2019-03-27 01:03:44,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3727101e-24 1.0000000e+00 1.2954624e-33 4.8675600e-36 8.6946071e-18], sum to 1.0000
[2019-03-27 01:03:44,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8263
[2019-03-27 01:03:44,303] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 90.5, 1.0, 2.0, 0.5588406130134597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780923.1540094193, 780923.1540094193, 193328.5115715217], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5956200.0000, 
sim time next is 5956800.0000, 
raw observation next is [27.33333333333334, 91.0, 1.0, 2.0, 0.5601860517267772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782803.9611360257, 782803.9611360257, 193562.9574364171], 
processed observation next is [1.0, 0.9565217391304348, 0.4944707740916275, 0.91, 1.0, 1.0, 0.4701036767792496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21744554476000716, 0.21744554476000716, 0.28889993647226436], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.08103961], dtype=float32), 1.7859349]. 
=============================================
[2019-03-27 01:04:00,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2137860e-27 1.0000000e+00 6.3649084e-37 0.0000000e+00 4.2107248e-20], sum to 1.0000
[2019-03-27 01:04:00,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7175
[2019-03-27 01:04:00,512] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.75, 65.5, 1.0, 2.0, 0.5292678855813084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739583.8760587026, 739583.8760587032, 188309.0389110315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6265800.0000, 
sim time next is 6266400.0000, 
raw observation next is [30.76666666666667, 65.0, 1.0, 2.0, 0.5271507721093261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736624.4561835225, 736624.4561835225, 187959.5725366013], 
processed observation next is [0.0, 0.5217391304347826, 0.6571879936808849, 0.65, 1.0, 1.0, 0.43030213507147724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20461790449542291, 0.20461790449542291, 0.28053667542776317], 
reward next is 0.7195, 
noisyNet noise sample is [array([1.5838616], dtype=float32), -0.7556776]. 
=============================================
[2019-03-27 01:04:05,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2220012e-23 1.0000000e+00 7.2589127e-32 4.5939844e-34 4.1421460e-17], sum to 1.0000
[2019-03-27 01:04:05,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6201
[2019-03-27 01:04:05,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 64.0, 1.0, 2.0, 0.5462490005464246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763321.3286423986, 763321.3286423986, 191160.036054712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6349200.0000, 
sim time next is 6349800.0000, 
raw observation next is [31.83333333333333, 63.5, 1.0, 2.0, 0.547774198161312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765453.388110173, 765453.388110173, 191420.3098841449], 
processed observation next is [0.0, 0.4782608695652174, 0.7077409162717218, 0.635, 1.0, 1.0, 0.45514963633893013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21262594114171474, 0.21262594114171474, 0.2857019550509625], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.900485], dtype=float32), -0.027191259]. 
=============================================
[2019-03-27 01:04:08,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0422508e-24 1.0000000e+00 3.6175827e-35 3.1874507e-38 3.0414795e-20], sum to 1.0000
[2019-03-27 01:04:08,721] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9226
[2019-03-27 01:04:08,726] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.95, 62.83333333333333, 1.0, 2.0, 0.5292061733555125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739497.6111338534, 739497.611133854, 188298.0382992323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6354600.0000, 
sim time next is 6355200.0000, 
raw observation next is [30.9, 62.66666666666667, 1.0, 2.0, 0.5197484419251602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726277.12896356, 726277.1289635594, 186748.0054003445], 
processed observation next is [0.0, 0.5652173913043478, 0.6635071090047393, 0.6266666666666667, 1.0, 1.0, 0.4213836649700725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20174364693432223, 0.20174364693432206, 0.2787283662691709], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.26979423], dtype=float32), 0.9154196]. 
=============================================
[2019-03-27 01:04:17,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5458364e-12 9.9999475e-01 4.1348604e-18 3.5252052e-17 5.2965488e-06], sum to 1.0000
[2019-03-27 01:04:17,138] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1268
[2019-03-27 01:04:17,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2260472.422618384 W.
[2019-03-27 01:04:17,156] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.08333333333334, 62.5, 1.0, 2.0, 0.5388414488241036, 1.0, 2.0, 0.5388414488241036, 1.0, 2.0, 0.9262373342164777, 6.9112, 6.9112, 170.5573041426782, 2260472.422618384, 2260472.422618384, 441162.7042691929], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6619800.0000, 
sim time next is 6620400.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.898168935636685, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.9836332969732, 6.9112, 168.9125248843629, 2152473.521953797, 2101086.97942854, 434353.8415508231], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.63, 1.0, 1.0, 0.8773119706466084, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00724332969732, 0.0, 0.8294378256707091, 0.5979093116538325, 0.5836352720634833, 0.6482893157474972], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.185685], dtype=float32), 1.0682325]. 
=============================================
[2019-03-27 01:04:21,268] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:04:21,271] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:04:21,272] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:04:21,272] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:04:21,272] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:21,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:21,273] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:21,274] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:04:21,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:04:21,277] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:21,277] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:04:21,294] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-27 01:04:21,313] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-27 01:04:21,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-27 01:04:21,353] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-27 01:04:21,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-27 01:04:25,207] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:04:25,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.36666666666667, 89.66666666666667, 1.0, 2.0, 0.3091798375343857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490818.6857878945, 490818.6857878939, 166380.8258337787]
[2019-03-27 01:04:25,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:04:25,212] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7095604e-29 1.0000000e+00 3.4794429e-37 0.0000000e+00 2.6706575e-26], sampled 0.4193970940242012
[2019-03-27 01:04:37,456] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:04:37,457] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.52224601833334, 98.35973275333333, 1.0, 2.0, 0.3260901501386336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514090.7364392683, 514090.736439269, 168064.4377569428]
[2019-03-27 01:04:37,458] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:04:37,462] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6639267e-28 1.0000000e+00 4.4595392e-36 0.0000000e+00 4.7564807e-25], sampled 0.92284946150931
[2019-03-27 01:04:57,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:04:57,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.45, 92.16666666666667, 1.0, 2.0, 0.4704113332348431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669354.405715811, 669354.405715811, 180635.435244963]
[2019-03-27 01:04:57,960] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:04:57,962] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3061555e-26 1.0000000e+00 1.9550797e-34 0.0000000e+00 1.4498052e-23], sampled 0.34653303136005986
[2019-03-27 01:05:00,208] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:00,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.00631312, 90.72419564500001, 1.0, 2.0, 0.38697120602924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585591.1917046906, 585591.1917046906, 173456.1580553738]
[2019-03-27 01:05:00,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:05:00,214] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.137095e-29 1.000000e+00 4.860080e-37 0.000000e+00 9.197321e-26], sampled 0.03297449414822984
[2019-03-27 01:05:05,113] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:05,114] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.18478323666667, 84.815945355, 1.0, 2.0, 0.4373469047578686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674792.8690883178, 674792.8690883172, 182054.3538631312]
[2019-03-27 01:05:05,114] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:05:05,119] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.0218877e-28 1.0000000e+00 1.4930130e-35 0.0000000e+00 5.8143274e-25], sampled 0.11101631780621324
[2019-03-27 01:05:35,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:35,285] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.75, 84.0, 1.0, 2.0, 0.6230841827369727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870733.8336443375, 870733.8336443375, 205124.6088561116]
[2019-03-27 01:05:35,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:05:35,291] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6010918e-25 1.0000000e+00 6.0594265e-33 2.6947633e-37 3.3081100e-22], sampled 0.8543022391414018
[2019-03-27 01:05:42,965] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:42,967] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.63333333333333, 90.0, 1.0, 2.0, 0.7420855706389231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1037114.448308614, 1037114.448308614, 230191.1396054597]
[2019-03-27 01:05:42,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:05:42,971] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7456987e-24 1.0000000e+00 1.3297408e-31 1.0452694e-35 3.1236812e-21], sampled 0.6313008686451133
[2019-03-27 01:05:52,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:52,103] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.66688786833333, 80.71170818500002, 1.0, 2.0, 0.4804854461917372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 671395.141693862, 671395.1416938626, 180594.8537962307]
[2019-03-27 01:05:52,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:05:52,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.5813088e-26 1.0000000e+00 4.1053162e-34 1.6772096e-38 6.1044203e-23], sampled 0.32236620726530996
[2019-03-27 01:05:53,130] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:05:53,131] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.4, 67.0, 1.0, 2.0, 0.5380546316896897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751866.5703887285, 751866.5703887291, 189772.6368166341]
[2019-03-27 01:05:53,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:05:53,137] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9916255e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6003995e-28], sampled 0.7152963331533854
[2019-03-27 01:06:03,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:06:03,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 81.66666666666667, 1.0, 2.0, 0.8117461074276572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1134521.736762201, 1134521.7367622, 246818.5057662503]
[2019-03-27 01:06:03,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:06:03,984] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.5281968e-25 1.0000000e+00 5.4689992e-32 2.5015557e-36 1.1867921e-21], sampled 0.03121680294227347
[2019-03-27 01:06:07,773] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:06:07,774] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.43333333333333, 80.0, 1.0, 2.0, 0.4123104407429036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603519.7908392458, 603519.7908392464, 174521.6613810738]
[2019-03-27 01:06:07,775] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:06:07,779] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.3277442e-29 1.0000000e+00 4.3010643e-37 0.0000000e+00 5.7707721e-26], sampled 0.361473255946395
[2019-03-27 01:06:09,875] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.009630046]
[2019-03-27 01:06:09,875] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.5, 81.66666666666667, 1.0, 2.0, 0.5577744664865386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779432.7756042951, 779432.7756042951, 193142.5573905658]
[2019-03-27 01:06:09,876] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:06:09,879] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8910421e-26 1.0000000e+00 4.1837322e-34 0.0000000e+00 3.0424150e-23], sampled 0.17998592332904473
[2019-03-27 01:06:14,321] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164126190.6881 1778.0000
[2019-03-27 01:06:14,552] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 01:06:14,561] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 01:06:14,661] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7395 3007728464.4710 1766.0000
[2019-03-27 01:06:14,775] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 01:06:15,790] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 600000, evaluation results [600000.0, 7883.41877172488, 3164126190.6880827, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7996.739506574028, 3007728464.4709883, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 01:06:27,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.4267855e-26 1.0000000e+00 5.7225866e-34 0.0000000e+00 6.4320696e-25], sum to 1.0000
[2019-03-27 01:06:27,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6102
[2019-03-27 01:06:27,815] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.88333333333333, 79.66666666666667, 1.0, 2.0, 0.3714902160695102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587046.9243801547, 587046.924380154, 174058.8901870213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6763800.0000, 
sim time next is 6764400.0000, 
raw observation next is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
processed observation next is [1.0, 0.30434782608695654, 0.28909952606635075, 0.79, 1.0, 1.0, 0.21422772179071756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15259196970742325, 0.15259196970742325, 0.2550395607687796], 
reward next is 0.7450, 
noisyNet noise sample is [array([-1.9049067], dtype=float32), -0.3576778]. 
=============================================
[2019-03-27 01:06:31,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4677098e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5281370e-30], sum to 1.0000
[2019-03-27 01:06:31,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7847
[2019-03-27 01:06:31,634] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.3365327607482251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526828.3924248168, 526828.3924248168, 168981.12464931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6827400.0000, 
sim time next is 6828000.0000, 
raw observation next is [23.73333333333333, 76.33333333333333, 1.0, 2.0, 0.3340880907468066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523223.6843403777, 523223.6843403777, 168701.4906607152], 
processed observation next is [0.0, 0.0, 0.3238546603475513, 0.7633333333333333, 1.0, 1.0, 0.19769649487567062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14533991231677157, 0.14533991231677157, 0.25179326964285853], 
reward next is 0.7482, 
noisyNet noise sample is [array([-0.44090667], dtype=float32), -1.9286363]. 
=============================================
[2019-03-27 01:06:31,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.04309 ]
 [83.509415]
 [82.89797 ]
 [82.4137  ]
 [81.89865 ]], R is [[84.44741058]
 [84.35072327]
 [84.25497437]
 [84.16106415]
 [84.06839752]].
[2019-03-27 01:06:37,581] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4870068e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4178382e-28], sum to 1.0000
[2019-03-27 01:06:37,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3248
[2019-03-27 01:06:37,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 81.66666666666667, 1.0, 2.0, 0.4206809444509638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615312.0034052789, 615312.0034052783, 175626.2067373386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6934800.0000, 
sim time next is 6935400.0000, 
raw observation next is [25.45, 80.5, 1.0, 2.0, 0.4227334881001668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617252.3801111848, 617252.3801111855, 175782.2589802505], 
processed observation next is [0.0, 0.2608695652173913, 0.4052132701421801, 0.805, 1.0, 1.0, 0.30449817843393595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714589944753291, 0.1714589944753293, 0.26236158056753806], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.06078006], dtype=float32), -0.48226073]. 
=============================================
[2019-03-27 01:06:37,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0624123e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2329442e-30], sum to 1.0000
[2019-03-27 01:06:37,956] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6149
[2019-03-27 01:06:37,963] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 78.0, 1.0, 2.0, 0.4790616709608624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669405.035718534, 669405.0357185346, 180380.7580126619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7065000.0000, 
sim time next is 7065600.0000, 
raw observation next is [27.13333333333333, 78.66666666666666, 1.0, 2.0, 0.4810496341558501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672183.7460203886, 672183.746020388, 180680.3454985439], 
processed observation next is [1.0, 0.782608695652174, 0.484992101105845, 0.7866666666666666, 1.0, 1.0, 0.3747585953684941, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18671770722788572, 0.18671770722788555, 0.26967215746051326], 
reward next is 0.7303, 
noisyNet noise sample is [array([0.6638443], dtype=float32), 0.9042098]. 
=============================================
[2019-03-27 01:06:40,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3557055e-26 1.0000000e+00 1.9420208e-33 2.3113531e-38 4.9287663e-23], sum to 1.0000
[2019-03-27 01:06:40,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4908
[2019-03-27 01:06:40,240] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 76.0, 1.0, 2.0, 0.5917320944556285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846554.1892061256, 846554.1892061256, 201742.0874032477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7023000.0000, 
sim time next is 7023600.0000, 
raw observation next is [26.8, 75.0, 1.0, 2.0, 0.6364402611088918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910828.0310405577, 910828.031040557, 210518.0316832197], 
processed observation next is [1.0, 0.30434782608695654, 0.4691943127962086, 0.75, 1.0, 1.0, 0.5619762182034841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2530077864001549, 0.2530077864001547, 0.31420601743764137], 
reward next is 0.6858, 
noisyNet noise sample is [array([-0.7653519], dtype=float32), -0.79372025]. 
=============================================
[2019-03-27 01:06:41,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1151890e-25 1.0000000e+00 6.7111952e-32 1.5788486e-37 7.9258206e-22], sum to 1.0000
[2019-03-27 01:06:41,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1147
[2019-03-27 01:06:41,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 79.0, 1.0, 2.0, 0.4527797744168931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647631.4518774964, 647631.451877497, 178451.0905255409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7000200.0000, 
sim time next is 7000800.0000, 
raw observation next is [26.16666666666666, 79.33333333333333, 1.0, 2.0, 0.4525203593305189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646806.3773682071, 646806.3773682065, 178355.5033537819], 
processed observation next is [1.0, 0.0, 0.4391785150078987, 0.7933333333333333, 1.0, 1.0, 0.34038597509701074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17966843815783531, 0.17966843815783515, 0.2662022438116148], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.27520487], dtype=float32), -0.18769409]. 
=============================================
[2019-03-27 01:06:50,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2814485e-29 1.0000000e+00 9.5639338e-38 0.0000000e+00 1.1194890e-24], sum to 1.0000
[2019-03-27 01:06:50,525] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0858
[2019-03-27 01:06:50,529] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 84.0, 1.0, 2.0, 0.4793260540578593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670566.7363261781, 670566.7363261781, 180522.1158581421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7161000.0000, 
sim time next is 7161600.0000, 
raw observation next is [25.93333333333334, 84.0, 1.0, 2.0, 0.4804312696594108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673085.5141042876, 673085.5141042876, 180814.3123274779], 
processed observation next is [1.0, 0.9130434782608695, 0.42812006319115364, 0.84, 1.0, 1.0, 0.3740135779029046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1869681983623021, 0.1869681983623021, 0.26987210795145955], 
reward next is 0.7301, 
noisyNet noise sample is [array([-0.78760004], dtype=float32), 0.95593286]. 
=============================================
[2019-03-27 01:06:58,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0730115e-22 1.0000000e+00 1.0901492e-31 1.9633544e-33 5.9562745e-18], sum to 1.0000
[2019-03-27 01:06:58,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8089
[2019-03-27 01:06:58,786] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 61.5, 1.0, 2.0, 0.6553004294051561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 999761.3900415116, 999761.3900415116, 221975.4499605033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7302600.0000, 
sim time next is 7303200.0000, 
raw observation next is [27.3, 61.0, 1.0, 2.0, 0.5723463913351585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873091.0886159511, 873091.0886159511, 204640.9253716278], 
processed observation next is [1.0, 0.5217391304347826, 0.4928909952606636, 0.61, 1.0, 1.0, 0.48475468835561264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24252530239331976, 0.24252530239331976, 0.3054342169725788], 
reward next is 0.6946, 
noisyNet noise sample is [array([0.66483456], dtype=float32), -1.7763942]. 
=============================================
[2019-03-27 01:07:03,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.7430840e-29 1.0000000e+00 4.8044275e-38 0.0000000e+00 5.3571067e-24], sum to 1.0000
[2019-03-27 01:07:03,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2023
[2019-03-27 01:07:03,476] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 90.33333333333334, 1.0, 2.0, 0.347409159737246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540638.7765830187, 540638.7765830187, 170017.200224327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7366800.0000, 
sim time next is 7367400.0000, 
raw observation next is [21.85, 90.5, 1.0, 2.0, 0.3412572469574073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533333.0381592724, 533333.0381592731, 169479.696774071], 
processed observation next is [1.0, 0.2608695652173913, 0.23459715639810438, 0.905, 1.0, 1.0, 0.20633403247880394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14814806615535345, 0.14814806615535364, 0.25295477130458355], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.13190182], dtype=float32), -1.145932]. 
=============================================
[2019-03-27 01:07:04,471] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1085439e-24 1.0000000e+00 2.6667721e-31 2.2530293e-36 7.6185066e-19], sum to 1.0000
[2019-03-27 01:07:04,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3758
[2019-03-27 01:07:04,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 93.0, 1.0, 2.0, 0.6994920625006624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1103318.787937988, 1103318.787937988, 235905.5648011823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7383000.0000, 
sim time next is 7383600.0000, 
raw observation next is [21.3, 93.0, 1.0, 2.0, 0.7074684945793798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114112.989064285, 1114112.989064286, 237691.992370086], 
processed observation next is [1.0, 0.4782608695652174, 0.2085308056872039, 0.93, 1.0, 1.0, 0.6475524031076865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30947583029563475, 0.309475830295635, 0.3547641677165463], 
reward next is 0.6452, 
noisyNet noise sample is [array([1.6588904], dtype=float32), -0.703841]. 
=============================================
[2019-03-27 01:07:07,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3725922e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1790549e-25], sum to 1.0000
[2019-03-27 01:07:07,250] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4479
[2019-03-27 01:07:07,255] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 92.0, 1.0, 2.0, 0.3159488283097837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 498901.1413231689, 498901.1413231689, 166928.5734579208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7434000.0000, 
sim time next is 7434600.0000, 
raw observation next is [21.31666666666667, 91.83333333333334, 1.0, 2.0, 0.315471054679055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498204.7504799939, 498204.7504799945, 166877.7633131374], 
processed observation next is [0.0, 0.043478260869565216, 0.20932069510268583, 0.9183333333333334, 1.0, 1.0, 0.17526633093862048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13839020846666497, 0.13839020846666514, 0.24907128852707075], 
reward next is 0.7509, 
noisyNet noise sample is [array([0.7273434], dtype=float32), 1.5742681]. 
=============================================
[2019-03-27 01:07:08,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9746541e-28 1.0000000e+00 2.9533607e-36 0.0000000e+00 1.3255528e-23], sum to 1.0000
[2019-03-27 01:07:08,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6830
[2019-03-27 01:07:08,030] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 81.0, 1.0, 2.0, 0.4017569457259792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 592685.9122017049, 592685.9122017055, 173657.7022912811], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7479000.0000, 
sim time next is 7479600.0000, 
raw observation next is [25.2, 80.66666666666666, 1.0, 2.0, 0.4036288715111631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594362.9525237568, 594362.9525237568, 173779.8462242809], 
processed observation next is [0.0, 0.5652173913043478, 0.3933649289099526, 0.8066666666666665, 1.0, 1.0, 0.2814805680857387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.165100820145488, 0.165100820145488, 0.25937290481235953], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.9849064], dtype=float32), 1.7614292]. 
=============================================
[2019-03-27 01:07:10,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7733733e-30 1.0000000e+00 1.3936524e-38 0.0000000e+00 2.2565083e-25], sum to 1.0000
[2019-03-27 01:07:10,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6444
[2019-03-27 01:07:10,452] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 79.0, 1.0, 2.0, 0.4158893333616966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606903.4914427524, 606903.4914427524, 174784.5962774617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7496400.0000, 
sim time next is 7497000.0000, 
raw observation next is [25.55, 79.5, 1.0, 2.0, 0.4140619188921128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605155.6039935349, 605155.6039935349, 174647.3863554103], 
processed observation next is [0.0, 0.782608695652174, 0.40995260663507116, 0.795, 1.0, 1.0, 0.29405050468929256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16809877888709304, 0.16809877888709304, 0.2606677408289706], 
reward next is 0.7393, 
noisyNet noise sample is [array([0.676526], dtype=float32), -2.6263714]. 
=============================================
[2019-03-27 01:07:10,464] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.929535]
 [79.88492 ]
 [79.82782 ]
 [79.713554]
 [79.64958 ]], R is [[79.91437531]
 [79.85436249]
 [79.79473114]
 [79.73556519]
 [79.67699432]].
[2019-03-27 01:07:11,732] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:07:11,735] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:07:11,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:07:11,737] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:07:11,739] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:07:11,741] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:07:11,741] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:07:11,738] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:07:11,743] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:07:11,745] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:07:11,742] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:07:11,758] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-27 01:07:11,758] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-27 01:07:11,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-27 01:07:11,820] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-27 01:07:11,821] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-27 01:07:14,451] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:14,452] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.097714985, 87.70063987500001, 1.0, 2.0, 0.3485538260296018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541861.9218307158, 541861.9218307158, 170102.8971974821]
[2019-03-27 01:07:14,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:07:14,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.2410457e-27 1.0000000e+00 7.3957771e-35 0.0000000e+00 1.9040408e-22], sampled 0.20931208640765908
[2019-03-27 01:07:18,094] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:18,094] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.32963653, 67.80184227, 1.0, 2.0, 0.2542038564385961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421335.3735906028, 421335.3735906034, 161156.394394577]
[2019-03-27 01:07:18,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:07:18,099] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9310951e-27 1.0000000e+00 2.1462103e-35 0.0000000e+00 7.5402531e-23], sampled 0.21254609543695835
[2019-03-27 01:07:20,272] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:20,273] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.65, 75.5, 1.0, 2.0, 0.2735939382894838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446666.3946081329, 446666.3946081329, 163291.3211657284]
[2019-03-27 01:07:20,275] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:07:20,281] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1560209e-26 1.0000000e+00 3.9418141e-34 3.4989941e-38 1.4732558e-21], sampled 0.9759382759737825
[2019-03-27 01:07:20,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:20,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.65, 74.83333333333333, 1.0, 2.0, 0.293890872161298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473023.5060631822, 473023.5060631822, 165170.8009541166]
[2019-03-27 01:07:20,328] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:07:20,332] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6963165e-27 1.0000000e+00 5.7103254e-36 0.0000000e+00 4.9397022e-23], sampled 0.49023981570245134
[2019-03-27 01:07:23,924] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:23,925] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.28379149, 60.74108953666666, 1.0, 2.0, 0.4694706686228348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763074.5186043938, 763074.5186043938, 190275.2786790509]
[2019-03-27 01:07:23,927] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:07:23,931] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.3291346e-26 1.0000000e+00 5.5329114e-34 4.3185056e-38 1.5516227e-21], sampled 0.33300959059375734
[2019-03-27 01:07:25,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:25,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.88515229666667, 70.79165513, 1.0, 2.0, 0.3445118619406191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534860.6236192174, 534860.6236192181, 169514.8897195338]
[2019-03-27 01:07:25,964] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:07:25,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.4711660e-28 1.0000000e+00 2.3848532e-36 0.0000000e+00 2.3025089e-23], sampled 0.16555672045681247
[2019-03-27 01:07:33,431] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:33,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.97238405, 73.12426182, 1.0, 2.0, 0.5158809915249836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720871.0639726577, 720871.0639726582, 186121.8803008]
[2019-03-27 01:07:33,434] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:07:33,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3166503e-27 1.0000000e+00 4.2624380e-36 0.0000000e+00 5.5536237e-23], sampled 0.4363628792081067
[2019-03-27 01:07:51,812] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:07:51,813] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.45336155333334, 98.09168293333335, 1.0, 2.0, 0.5192281513847183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725549.846030556, 725549.8460305554, 186663.9644473814]
[2019-03-27 01:07:51,814] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:07:51,817] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5026171e-27 1.0000000e+00 1.9227500e-35 0.0000000e+00 1.0811508e-22], sampled 0.0602736956218487
[2019-03-27 01:08:05,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:08:05,311] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.18553806, 77.68082744333333, 1.0, 2.0, 0.4961547852523566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693297.4724144316, 693297.4724144316, 182994.8976288137]
[2019-03-27 01:08:05,312] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:08:05,316] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.7173781e-28 1.0000000e+00 1.2611668e-36 0.0000000e+00 5.1437716e-23], sampled 0.17957375753820548
[2019-03-27 01:09:02,610] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.013103993]
[2019-03-27 01:09:02,611] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.25, 77.0, 1.0, 2.0, 0.4816793336045208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104276, 673063.9209465954, 673063.9209465954, 180777.3779912238]
[2019-03-27 01:09:02,612] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:09:02,615] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.9056558e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5365269e-26], sampled 0.6052429538902073
[2019-03-27 01:09:05,375] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4546 2779216958.0617 934.0000
[2019-03-27 01:09:05,996] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.9470 2927473933.7898 1338.0000
[2019-03-27 01:09:06,122] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2767 3007680316.3481 1766.0000
[2019-03-27 01:09:06,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1326 2842568365.7369 1131.0000
[2019-03-27 01:09:06,199] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-27 01:09:07,218] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 625000, evaluation results [625000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8252.946952103406, 2927473933.7898183, 1338.0, 8660.454614036043, 2779216958.061703, 934.0, 7998.276703650782, 3007680316.348062, 1766.0, 8496.13258090819, 2842568365.7369456, 1131.0]
[2019-03-27 01:09:09,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2121674e-29 1.0000000e+00 5.0315404e-38 0.0000000e+00 3.1338076e-25], sum to 1.0000
[2019-03-27 01:09:09,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8989
[2019-03-27 01:09:09,647] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 83.16666666666667, 1.0, 2.0, 0.4283950986484798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617267.7866510127, 617267.7866510121, 175547.0529056282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7549800.0000, 
sim time next is 7550400.0000, 
raw observation next is [25.66666666666667, 82.33333333333334, 1.0, 2.0, 0.4333419437761548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621602.6372978551, 621602.6372978558, 175889.6279018857], 
processed observation next is [0.0, 0.391304347826087, 0.4154818325434442, 0.8233333333333335, 1.0, 1.0, 0.31727945033271665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1726673992494042, 0.17266739924940439, 0.2625218326893816], 
reward next is 0.7375, 
noisyNet noise sample is [array([1.4756984], dtype=float32), -0.37697196]. 
=============================================
[2019-03-27 01:09:15,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0593009e-14 9.9999976e-01 7.7173301e-22 1.1190933e-21 2.4598251e-07], sum to 1.0000
[2019-03-27 01:09:15,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4519
[2019-03-27 01:09:15,162] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.26666666666667, 70.66666666666667, 1.0, 2.0, 0.5361437267799758, 1.0, 2.0, 0.5361437267799758, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1498909.324970657, 1498909.324970657, 311852.9567267283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7639800.0000, 
sim time next is 7640400.0000, 
raw observation next is [28.53333333333333, 69.33333333333334, 1.0, 2.0, 1.001186632501421, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565103966, 1399464.236974146, 1399464.236974146, 299284.02459606], 
processed observation next is [1.0, 0.43478260869565216, 0.5513428120063191, 0.6933333333333335, 1.0, 1.0, 1.0014296777125553, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451521338, 0.38874006582615167, 0.38874006582615167, 0.4466925740239701], 
reward next is 0.5533, 
noisyNet noise sample is [array([1.9513721], dtype=float32), 1.4480648]. 
=============================================
[2019-03-27 01:09:16,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:16,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:16,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-27 01:09:17,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8760772e-16 9.9999678e-01 9.4203520e-27 3.0957759e-25 3.2106177e-06], sum to 1.0000
[2019-03-27 01:09:17,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2559
[2019-03-27 01:09:17,831] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 88.33333333333334, 1.0, 2.0, 0.4791598225705687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669542.2286759431, 669542.2286759437, 180394.9043858152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7686600.0000, 
sim time next is 7687200.0000, 
raw observation next is [25.4, 88.66666666666667, 1.0, 2.0, 0.4791948013832009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669591.1208661617, 669591.1208661622, 180400.1578883383], 
processed observation next is [1.0, 1.0, 0.4028436018957346, 0.8866666666666667, 1.0, 1.0, 0.37252385708819385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18599753357393378, 0.18599753357393395, 0.26925396699751986], 
reward next is 0.7307, 
noisyNet noise sample is [array([-1.3153037], dtype=float32), -0.7792224]. 
=============================================
[2019-03-27 01:09:23,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1541037e-11 9.9104530e-01 6.6898930e-20 5.8205424e-17 8.9547094e-03], sum to 1.0000
[2019-03-27 01:09:23,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8230
[2019-03-27 01:09:23,564] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 86.0, 1.0, 2.0, 0.4989857216511807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697254.5544403295, 697254.5544403295, 183437.3057700585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7781400.0000, 
sim time next is 7782000.0000, 
raw observation next is [26.4, 85.66666666666667, 1.0, 2.0, 0.4970971705667859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694614.7366797907, 694614.7366797907, 183142.50619799], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8566666666666667, 1.0, 1.0, 0.39409297658648906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19294853796660852, 0.19294853796660852, 0.27334702417610446], 
reward next is 0.7267, 
noisyNet noise sample is [array([-0.05017752], dtype=float32), -0.34604684]. 
=============================================
[2019-03-27 01:09:23,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[53.927277]
 [53.98962 ]
 [54.135494]
 [54.344555]
 [54.47039 ]], R is [[53.82926559]
 [54.01718521]
 [54.20272827]
 [54.38588715]
 [54.56673813]].
[2019-03-27 01:09:26,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:26,293] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:26,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-27 01:09:27,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3793132e-13 9.9998665e-01 9.4714472e-22 8.5581760e-22 1.3384023e-05], sum to 1.0000
[2019-03-27 01:09:27,051] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2580
[2019-03-27 01:09:27,055] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3819850059386332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576836.0541452381, 576836.0541452381, 172637.409321775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3803188587064072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574320.0518680543, 574320.0518680537, 172414.0207110183], 
processed observation next is [1.0, 0.8695652173913043, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2533962153089243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595333477411262, 0.15953334774112604, 0.25733435927017656], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.1076237], dtype=float32), -1.7852215]. 
=============================================
[2019-03-27 01:09:27,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7212763e-12 9.5054120e-01 3.8408019e-21 8.6233894e-19 4.9458724e-02], sum to 1.0000
[2019-03-27 01:09:27,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6674
[2019-03-27 01:09:27,651] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 89.66666666666667, 1.0, 2.0, 0.5072238097412914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708769.8352961168, 708769.8352961162, 184735.9396953024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7863600.0000, 
sim time next is 7864200.0000, 
raw observation next is [26.2, 90.0, 1.0, 2.0, 0.5075820176449558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709270.5446718894, 709270.5446718894, 184792.8573793201], 
processed observation next is [1.0, 0.0, 0.44075829383886256, 0.9, 1.0, 1.0, 0.40672532246380216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1970195957421915, 0.1970195957421915, 0.2758102348945076], 
reward next is 0.7242, 
noisyNet noise sample is [array([-1.252953], dtype=float32), 0.43002388]. 
=============================================
[2019-03-27 01:09:31,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:31,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:31,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-27 01:09:31,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:31,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:31,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-27 01:09:32,730] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:32,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:32,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-27 01:09:32,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:32,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:32,807] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-27 01:09:33,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-27 01:09:33,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-27 01:09:33,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-27 01:09:33,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,128] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-27 01:09:33,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-27 01:09:33,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-27 01:09:33,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:09:33,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:09:33,225] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-27 01:09:33,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-27 01:09:33,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-27 01:09:33,296] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-27 01:09:33,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7034149e-22 1.0000000e+00 3.7691352e-33 7.1345763e-33 3.6589106e-14], sum to 1.0000
[2019-03-27 01:09:33,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2562
[2019-03-27 01:09:33,851] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 87.33333333333334, 1.0, 2.0, 0.3184106743305473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503183.2770729339, 503183.2770729333, 167257.9517252924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [21.95, 87.0, 1.0, 2.0, 0.3201268575072396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505418.0504865164, 505418.0504865164, 167417.2512629884], 
processed observation next is [0.0, 0.5652173913043478, 0.2393364928909953, 0.87, 1.0, 1.0, 0.18087573193643328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14039390291292123, 0.14039390291292123, 0.24987649442237075], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.9782677], dtype=float32), -0.9682907]. 
=============================================
[2019-03-27 01:09:41,428] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3238857e-14 9.9538416e-01 5.5716185e-24 7.7133720e-20 4.6158009e-03], sum to 1.0000
[2019-03-27 01:09:41,436] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4303
[2019-03-27 01:09:41,440] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.0, 1.0, 2.0, 0.734012973731504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1108080.704503797, 1108080.704503796, 239083.8775810624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [22.9, 92.33333333333334, 1.0, 2.0, 0.7425352020826143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1119812.87522061, 1119812.87522061, 241048.7627609518], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9233333333333335, 1.0, 1.0, 0.6898014482923063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.311059132005725, 0.311059132005725, 0.35977427277754], 
reward next is 0.6402, 
noisyNet noise sample is [array([0.44633147], dtype=float32), 0.95590234]. 
=============================================
[2019-03-27 01:09:44,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8798178e-24 1.0000000e+00 1.4388776e-33 1.8562243e-32 3.9228183e-14], sum to 1.0000
[2019-03-27 01:09:44,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3874
[2019-03-27 01:09:44,123] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 76.83333333333334, 1.0, 2.0, 0.3162925045152192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497579.1848846025, 497579.1848846025, 166786.9531768659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 312600.0000, 
sim time next is 313200.0000, 
raw observation next is [23.4, 77.0, 1.0, 2.0, 0.3150392842453921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496097.0192038848, 496097.0192038848, 166688.1536447444], 
processed observation next is [0.0, 0.6521739130434783, 0.30805687203791465, 0.77, 1.0, 1.0, 0.17474612559685798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13780472755663467, 0.13780472755663467, 0.24878828902200656], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.4482723], dtype=float32), 0.54505134]. 
=============================================
[2019-03-27 01:09:45,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5080954e-23 1.0000000e+00 2.2619285e-33 3.3847195e-33 3.8528405e-14], sum to 1.0000
[2019-03-27 01:09:45,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1676
[2019-03-27 01:09:45,943] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 96.0, 1.0, 2.0, 0.2844410003306096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458693.0151236086, 458693.0151236086, 164184.2491094271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [19.81666666666667, 96.0, 1.0, 2.0, 0.2839575869409868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458056.6161098718, 458056.6161098718, 164140.8983375772], 
processed observation next is [0.0, 0.17391304347826086, 0.13823064770932092, 0.96, 1.0, 1.0, 0.13729829751926118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12723794891940882, 0.12723794891940882, 0.2449864154292197], 
reward next is 0.7550, 
noisyNet noise sample is [array([-0.20430602], dtype=float32), 0.0411551]. 
=============================================
[2019-03-27 01:10:01,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2102242e-16 9.9999607e-01 3.6685449e-26 1.5274262e-22 3.9630308e-06], sum to 1.0000
[2019-03-27 01:10:01,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2681
[2019-03-27 01:10:01,397] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 53.0, 1.0, 2.0, 0.6158825450571871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005456.406062563, 1005456.406062563, 218956.4809668626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 483000.0000, 
sim time next is 483600.0000, 
raw observation next is [25.3, 53.0, 1.0, 2.0, 0.6183887266573648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1010166.130353235, 1010166.130353234, 219526.8675808142], 
processed observation next is [1.0, 0.6086956521739131, 0.39810426540284366, 0.53, 1.0, 1.0, 0.5402273815148972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28060170287589864, 0.28060170287589836, 0.3276520411653943], 
reward next is 0.6723, 
noisyNet noise sample is [array([-3.6669295], dtype=float32), 0.015059803]. 
=============================================
[2019-03-27 01:10:02,330] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 01:10:02,333] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:10:02,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:10:02,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:10:02,336] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:10:02,337] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:10:02,337] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:10:02,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:10:02,340] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:10:02,340] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:10:02,341] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:10:02,365] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-27 01:10:02,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-27 01:10:02,394] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-27 01:10:02,394] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-27 01:10:02,410] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-27 01:10:04,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:04,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.83916762, 92.15186892833333, 1.0, 2.0, 0.3965806880922667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598027.1095442913, 598027.1095442907, 174529.1102661876]
[2019-03-27 01:10:04,703] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:10:04,705] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5806267e-19 1.0000000e+00 7.7882537e-29 1.1606379e-26 9.9302366e-10], sampled 0.23711947376125375
[2019-03-27 01:10:25,574] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:25,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.08347030333334, 51.41765997333333, 1.0, 2.0, 0.3446263808961164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537736.6766074009, 537736.6766074009, 169814.8758635948]
[2019-03-27 01:10:25,575] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:10:25,578] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1342486e-21 1.0000000e+00 4.6841531e-31 4.2780565e-30 1.4653736e-12], sampled 0.6166100089566704
[2019-03-27 01:10:32,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:32,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.961148045, 93.62663445666666, 1.0, 2.0, 0.4940532890543225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 690360.0112090299, 690360.0112090292, 182668.9631171635]
[2019-03-27 01:10:32,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:10:32,574] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.3050861e-20 1.0000000e+00 6.1558205e-30 4.4129858e-28 5.5661815e-11], sampled 0.2935480683549979
[2019-03-27 01:10:33,875] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:33,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 76.16666666666667, 1.0, 2.0, 0.5627679389941809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786413.2258867561, 786413.2258867561, 194015.3221651925]
[2019-03-27 01:10:33,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:10:33,879] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7589577e-20 1.0000000e+00 2.3572559e-30 1.0908873e-28 3.3392997e-11], sampled 0.09229083099477187
[2019-03-27 01:10:38,694] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:38,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.67461558, 88.170854505, 1.0, 2.0, 0.7194718265839928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1005495.259382423, 1005495.259382424, 225105.9710088605]
[2019-03-27 01:10:38,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:10:38,701] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2896097e-18 1.0000000e+00 3.9297548e-28 1.2429528e-25 4.2188919e-09], sampled 0.9871077413256807
[2019-03-27 01:10:38,773] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:38,777] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.55759637333334, 91.31696279333333, 1.0, 2.0, 0.5332490369158372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745148.9748676313, 745148.9748676307, 188966.9419169062]
[2019-03-27 01:10:38,777] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:10:38,779] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5431632e-19 1.0000000e+00 2.1812864e-30 2.4170296e-27 4.4184945e-09], sampled 0.5421121715013406
[2019-03-27 01:10:57,947] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:10:57,948] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 63.00000000000001, 1.0, 2.0, 0.6183279808014628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864084.53857606, 864084.5385760607, 204210.7737611511]
[2019-03-27 01:10:57,950] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:10:57,953] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1057294e-20 1.0000000e+00 4.6341952e-30 2.8569652e-28 7.1649985e-11], sampled 0.10260907661244012
[2019-03-27 01:11:04,491] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:11:04,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.3939266, 81.54071822, 1.0, 2.0, 0.5424471836548618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758006.8230288962, 758006.8230288968, 190516.8235028135]
[2019-03-27 01:11:04,495] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:11:04,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5923064e-26 1.0000000e+00 7.1888732e-37 3.4125609e-37 5.8923249e-16], sampled 0.37759793928650653
[2019-03-27 01:11:44,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.027751192]
[2019-03-27 01:11:44,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.83333333333333, 84.0, 1.0, 2.0, 0.4714607753006186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663420.109635108, 663420.1096351086, 179843.814024077]
[2019-03-27 01:11:44,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:11:44,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.5738797e-20 1.0000000e+00 4.9897395e-30 4.1017753e-28 1.5023216e-10], sampled 0.29319857568007424
[2019-03-27 01:11:56,101] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8062.7032 3156751792.5498 1363.0000
[2019-03-27 01:11:56,858] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8628.2949 2838253829.5950 822.0000
[2019-03-27 01:11:56,889] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8408.3758 2925076431.8988 986.0000
[2019-03-27 01:11:56,925] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8753.8034 2778838587.9835 689.0000
[2019-03-27 01:11:57,062] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8255.2422 2998442948.6408 1140.0000
[2019-03-27 01:11:58,078] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 650000, evaluation results [650000.0, 8062.703191269976, 3156751792.549828, 1363.0, 8408.375826293717, 2925076431.8987727, 986.0, 8753.803416964933, 2778838587.9835463, 689.0, 8255.242218299207, 2998442948.6407914, 1140.0, 8628.29492516339, 2838253829.5950313, 822.0]
[2019-03-27 01:11:59,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.5893245e-23 1.0000000e+00 7.4404137e-32 7.9735733e-32 1.0979265e-12], sum to 1.0000
[2019-03-27 01:11:59,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4771
[2019-03-27 01:11:59,462] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.05, 70.66666666666667, 1.0, 2.0, 0.3027321731922025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480197.6300833625, 480197.6300833632, 165602.6905125778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 910200.0000, 
sim time next is 910800.0000, 
raw observation next is [24.2, 70.0, 1.0, 2.0, 0.3041022323692932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481904.677424536, 481904.677424536, 165716.1897875097], 
processed observation next is [0.0, 0.5652173913043478, 0.3459715639810427, 0.7, 1.0, 1.0, 0.161568954661799, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13386241039570446, 0.13386241039570446, 0.2473375966977757], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.95681226], dtype=float32), -0.6814375]. 
=============================================
[2019-03-27 01:12:01,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.1181056e-17 9.9999964e-01 3.0231449e-27 1.1021073e-24 3.7394378e-07], sum to 1.0000
[2019-03-27 01:12:01,217] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1684
[2019-03-27 01:12:01,221] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.71666666666667, 87.0, 1.0, 2.0, 0.2263859627626152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 375861.624529921, 375861.6245299204, 158425.5745825521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 521400.0000, 
sim time next is 522000.0000, 
raw observation next is [18.7, 87.0, 1.0, 2.0, 0.2258242875174738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 374981.3737918419, 374981.3737918419, 158366.9582657497], 
processed observation next is [1.0, 0.043478260869565216, 0.08530805687203795, 0.87, 1.0, 1.0, 0.06725817773189613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10416149271995609, 0.10416149271995609, 0.2363685944264921], 
reward next is 0.7636, 
noisyNet noise sample is [array([-0.46081004], dtype=float32), 0.7060972]. 
=============================================
[2019-03-27 01:12:01,235] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[70.315506]
 [70.21641 ]
 [70.17598 ]
 [70.29048 ]
 [70.53723 ]], R is [[70.40533447]
 [70.46482086]
 [70.52362823]
 [70.58175659]
 [70.63920593]].
[2019-03-27 01:12:09,342] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7175146e-19 1.0000000e+00 2.5815538e-29 1.2009447e-24 3.4147469e-08], sum to 1.0000
[2019-03-27 01:12:09,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2562
[2019-03-27 01:12:09,362] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333334, 78.0, 1.0, 2.0, 0.5614630102908827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891187.9750266799, 891187.9750266799, 205939.0914790704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1077600.0000, 
sim time next is 1078200.0000, 
raw observation next is [23.1, 77.5, 1.0, 2.0, 0.5761142759169119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912568.9687445319, 912568.9687445319, 208725.1666380696], 
processed observation next is [1.0, 0.4782608695652174, 0.2938388625592418, 0.775, 1.0, 1.0, 0.48929430833362875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2534913802068144, 0.2534913802068144, 0.31153009945980537], 
reward next is 0.6885, 
noisyNet noise sample is [array([-0.1515854], dtype=float32), 0.49538088]. 
=============================================
[2019-03-27 01:12:10,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1312849e-22 1.0000000e+00 3.8916426e-34 6.0943578e-34 3.9045596e-14], sum to 1.0000
[2019-03-27 01:12:10,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9511
[2019-03-27 01:12:10,769] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 88.33333333333333, 1.0, 2.0, 0.3383860009893634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524331.8892428037, 524331.8892428043, 168640.3182976949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 938400.0000, 
sim time next is 939000.0000, 
raw observation next is [22.45, 88.66666666666667, 1.0, 2.0, 0.3377354240795242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523423.4149243106, 523423.41492431, 168570.9782972364], 
processed observation next is [0.0, 0.8695652173913043, 0.26303317535545023, 0.8866666666666667, 1.0, 1.0, 0.20209087238496895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14539539303453072, 0.14539539303453056, 0.2515984750705021], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.34599298], dtype=float32), 0.26825893]. 
=============================================
[2019-03-27 01:12:10,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.680664]
 [80.67529 ]
 [80.674416]
 [80.67445 ]
 [80.67207 ]], R is [[80.62792206]
 [80.56993866]
 [80.51248932]
 [80.45568848]
 [80.3996582 ]].
[2019-03-27 01:12:12,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.06055663e-19 1.00000000e+00 1.00386896e-28 1.48966906e-26
 7.80627951e-10], sum to 1.0000
[2019-03-27 01:12:12,045] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9904
[2019-03-27 01:12:12,051] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.61666666666667, 93.0, 1.0, 2.0, 0.2231216326482891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 371605.0903524174, 371605.090352418, 157897.7036187377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 701400.0000, 
sim time next is 702000.0000, 
raw observation next is [17.6, 93.0, 1.0, 2.0, 0.2214840591204185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368918.9041889188, 368918.9041889194, 157741.2396162729], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.93, 1.0, 1.0, 0.06202898689207046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10247747338581079, 0.10247747338581095, 0.23543468599443718], 
reward next is 0.7646, 
noisyNet noise sample is [array([-0.76142615], dtype=float32), -0.7750643]. 
=============================================
[2019-03-27 01:12:12,066] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.52198]
 [71.74001]
 [71.84222]
 [72.02639]
 [71.84893]], R is [[71.52508545]
 [71.57416534]
 [71.62259674]
 [71.67072296]
 [71.71688843]].
[2019-03-27 01:12:14,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1132014e-17 9.9999356e-01 8.9108821e-28 4.7035311e-24 6.4429569e-06], sum to 1.0000
[2019-03-27 01:12:14,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0002
[2019-03-27 01:12:14,883] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 62.33333333333334, 1.0, 2.0, 0.7860917604585286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1194518.974731555, 1194518.974731555, 253251.3744136685], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1171200.0000, 
sim time next is 1171800.0000, 
raw observation next is [27.3, 62.0, 1.0, 2.0, 0.8520371013654272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1293168.079774459, 1293168.07977446, 271151.0565299764], 
processed observation next is [1.0, 0.5652173913043478, 0.4928909952606636, 0.62, 1.0, 1.0, 0.8217314474282256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35921335549290523, 0.35921335549290556, 0.404703069447726], 
reward next is 0.5953, 
noisyNet noise sample is [array([-0.6749808], dtype=float32), 0.012394067]. 
=============================================
[2019-03-27 01:12:15,632] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2614083e-16 9.9823964e-01 2.3059521e-26 6.9144278e-21 1.7603684e-03], sum to 1.0000
[2019-03-27 01:12:15,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2298
[2019-03-27 01:12:15,650] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.0, 1.0, 2.0, 0.8641109400539346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1333435.593337787, 1333435.593337787, 277297.3716745664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1184400.0000, 
sim time next is 1185000.0000, 
raw observation next is [27.45, 57.83333333333333, 1.0, 2.0, 0.4381659364294249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676160.7084975761, 676160.7084975761, 182190.6228728698], 
processed observation next is [1.0, 0.7391304347826086, 0.5, 0.5783333333333333, 1.0, 1.0, 0.3230914896740059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18782241902710448, 0.18782241902710448, 0.2719263027953281], 
reward next is 0.7281, 
noisyNet noise sample is [array([0.9546495], dtype=float32), -0.5062687]. 
=============================================
[2019-03-27 01:12:15,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.1074  ]
 [77.979576]
 [77.94332 ]
 [77.81657 ]
 [77.56988 ]], R is [[79.76011658]
 [79.54863739]
 [79.33470917]
 [79.12280273]
 [78.91594696]].
[2019-03-27 01:12:20,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8013683e-23 1.0000000e+00 1.7568404e-33 4.8540708e-32 3.1879377e-15], sum to 1.0000
[2019-03-27 01:12:20,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0394
[2019-03-27 01:12:20,618] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 82.0, 1.0, 2.0, 0.3290152649457214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513191.5550425368, 513191.5550425375, 167863.1858485076], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 932400.0000, 
sim time next is 933000.0000, 
raw observation next is [23.03333333333334, 82.83333333333334, 1.0, 2.0, 0.3303007847433387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514730.1321471648, 514730.1321471654, 167969.7924274406], 
processed observation next is [0.0, 0.8260869565217391, 0.2906793048973147, 0.8283333333333335, 1.0, 1.0, 0.19313347559438396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14298059226310134, 0.1429805922631015, 0.2507011827275233], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.6373191], dtype=float32), -0.65869755]. 
=============================================
[2019-03-27 01:12:20,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.94308 ]
 [76.904015]
 [76.92436 ]
 [76.94469 ]
 [76.96198 ]], R is [[76.90393829]
 [76.88436127]
 [76.86495209]
 [76.84587097]
 [76.8272934 ]].
[2019-03-27 01:12:26,210] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9503962e-18 1.0000000e+00 1.3881885e-29 1.8339317e-27 1.2044453e-09], sum to 1.0000
[2019-03-27 01:12:26,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9763
[2019-03-27 01:12:26,221] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
processed observation next is [1.0, 0.7391304347826086, 0.18878357030015785, 0.9283333333333332, 1.0, 1.0, 0.1640022337309235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13529385465598642, 0.13529385465598642, 0.24794667966507133], 
reward next is 0.7521, 
noisyNet noise sample is [array([-0.3584671], dtype=float32), 0.041833527]. 
=============================================
[2019-03-27 01:12:29,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0626887e-14 8.9763910e-01 4.7538469e-25 2.3488668e-18 1.0236086e-01], sum to 1.0000
[2019-03-27 01:12:29,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7630
[2019-03-27 01:12:29,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1916497.789908991 W.
[2019-03-27 01:12:29,776] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.46666666666667, 73.0, 1.0, 2.0, 0.6853771774785081, 1.0, 2.0, 0.6853771774785081, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1916497.789908991, 1916497.789908991, 367919.0563802863], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1258800.0000, 
sim time next is 1259400.0000, 
raw observation next is [28.48333333333333, 73.0, 1.0, 2.0, 0.7381175170036708, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.971512424195472, 6.9112, 168.9126030501773, 1928470.03210565, 1885682.412123731, 394293.9596618753], 
processed observation next is [1.0, 0.5652173913043478, 0.5489731437598735, 0.73, 1.0, 1.0, 0.6844789361490009, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0060312424195472225, 0.0, 0.8294382095006422, 0.5356861200293472, 0.5238006700343697, 0.5884984472565303], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9664935], dtype=float32), -1.0903364]. 
=============================================
[2019-03-27 01:12:49,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3139424e-14 9.9993622e-01 4.4156095e-24 6.7744412e-20 6.3819345e-05], sum to 1.0000
[2019-03-27 01:12:49,872] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3804
[2019-03-27 01:12:49,876] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 91.33333333333334, 1.0, 2.0, 0.5442468249622961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 843652.4958574866, 843652.4958574872, 200698.2711923694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1342200.0000, 
sim time next is 1342800.0000, 
raw observation next is [22.0, 91.0, 1.0, 2.0, 0.562827691920754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875034.8951867357, 875034.8951867357, 204526.2362519844], 
processed observation next is [1.0, 0.5652173913043478, 0.2417061611374408, 0.91, 1.0, 1.0, 0.4732863758081373, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24306524866298213, 0.24306524866298213, 0.30526303918206626], 
reward next is 0.6947, 
noisyNet noise sample is [array([1.4761184], dtype=float32), 0.79876673]. 
=============================================
[2019-03-27 01:12:51,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4634877e-18 9.9993825e-01 1.4366542e-27 5.8936259e-23 6.1722661e-05], sum to 1.0000
[2019-03-27 01:12:51,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8822
[2019-03-27 01:12:51,371] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 86.66666666666667, 1.0, 2.0, 0.6317802615002365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997703.614510872, 997703.614510872, 220280.7405242832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1779600.0000, 
sim time next is 1780200.0000, 
raw observation next is [21.7, 88.0, 1.0, 2.0, 0.6384595995319078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009697.859320773, 1009697.859320773, 221894.1809250204], 
processed observation next is [1.0, 0.6086956521739131, 0.2274881516587678, 0.88, 1.0, 1.0, 0.5644091560625395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2804716275891036, 0.2804716275891036, 0.33118534466420957], 
reward next is 0.6688, 
noisyNet noise sample is [array([0.7632571], dtype=float32), -1.0824493]. 
=============================================
[2019-03-27 01:12:52,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2166264e-22 1.0000000e+00 9.5891704e-32 8.0503556e-30 7.9960157e-11], sum to 1.0000
[2019-03-27 01:12:52,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1686
[2019-03-27 01:12:52,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 97.0, 1.0, 2.0, 0.3089788912435167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491315.4062541475, 491315.4062541469, 166430.3348969855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [20.36666666666667, 97.16666666666667, 1.0, 2.0, 0.3077219938741986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489481.0402173451, 489481.0402173451, 166298.7417298428], 
processed observation next is [0.0, 0.043478260869565216, 0.1642969984202214, 0.9716666666666667, 1.0, 1.0, 0.16593011310144407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1359669556159292, 0.1359669556159292, 0.2482070772087206], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.2677197], dtype=float32), -0.4715974]. 
=============================================
[2019-03-27 01:12:53,945] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:12:53,948] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:12:53,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:12:53,949] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:12:53,950] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:12:53,950] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:12:53,951] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:12:53,952] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:12:53,954] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:12:53,953] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:12:53,956] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:12:53,975] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-27 01:12:53,996] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-27 01:12:53,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-27 01:12:54,017] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-27 01:12:54,064] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-27 01:13:24,236] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:13:24,237] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.26666666666667, 95.0, 1.0, 2.0, 0.3759467129885383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572185.5306716106, 572185.5306716106, 172357.367020018]
[2019-03-27 01:13:24,238] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:13:24,241] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3845978e-21 1.0000000e+00 4.0847629e-32 1.1506116e-29 1.3463294e-12], sampled 0.5308893454236444
[2019-03-27 01:13:27,872] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:13:27,872] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 92.0, 1.0, 2.0, 0.3913172690200777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586938.4197353853, 586938.4197353853, 173429.059207843]
[2019-03-27 01:13:27,874] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:13:27,876] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.1229774e-21 1.0000000e+00 2.3862698e-31 1.5007212e-28 1.0568958e-11], sampled 0.08381975115284124
[2019-03-27 01:13:30,661] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:13:30,662] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.97729449333333, 93.83087187333334, 1.0, 2.0, 0.7002407297683741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 978606.5260708167, 978606.5260708161, 220894.0021064473]
[2019-03-27 01:13:30,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:13:30,666] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.1086222e-20 1.0000000e+00 2.9302746e-30 3.8554101e-27 1.7333619e-10], sampled 0.5170944356080702
[2019-03-27 01:13:40,665] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:13:40,666] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.2, 55.0, 1.0, 2.0, 0.6810527572183426, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597410042959, 6.9112, 168.9123160664784, 1848615.228731723, 1781379.47494367, 380598.4984389898]
[2019-03-27 01:13:40,666] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:13:40,671] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.0740424e-19 9.9999416e-01 4.9312022e-30 5.0981428e-25 5.8638320e-06], sampled 0.2819901865996449
[2019-03-27 01:13:40,672] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1848615.228731723 W.
[2019-03-27 01:13:41,313] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:13:41,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.25, 56.5, 1.0, 2.0, 0.5881055329858125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821833.736382895, 821833.736382895, 198550.6658921391]
[2019-03-27 01:13:41,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:13:41,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2149297e-23 1.0000000e+00 2.1954998e-33 1.6390444e-31 8.6189695e-14], sampled 0.6584133044854324
[2019-03-27 01:14:11,425] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:14:11,426] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.1, 74.0, 1.0, 2.0, 0.597494075813787, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.9133040407457, 6.9112, 168.9128939083516, 1670574.258105362, 1669081.579738081, 364367.8705709893]
[2019-03-27 01:14:11,428] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:14:11,430] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.7802515e-20 1.0000000e+00 1.0603363e-30 7.4305621e-27 4.9854574e-08], sampled 0.8167303551476388
[2019-03-27 01:14:11,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1670574.258105362 W.
[2019-03-27 01:14:26,861] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:14:26,863] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.18800105333334, 51.62609497333334, 1.0, 2.0, 0.6394272557957337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 919891.8631428776, 919891.8631428782, 211739.7618379773]
[2019-03-27 01:14:26,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:14:26,867] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.5424751e-21 1.0000000e+00 3.5954949e-31 1.2445973e-28 1.2214108e-11], sampled 0.18925436056976364
[2019-03-27 01:14:29,552] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:14:29,553] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.26630901333333, 85.64989835333333, 1.0, 2.0, 0.4544588318558306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651088.5068313228, 651088.5068313228, 178831.1495488025]
[2019-03-27 01:14:29,554] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:14:29,555] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.4945563e-22 1.0000000e+00 1.3820518e-32 2.2189017e-30 6.3745233e-13], sampled 0.4024477904829743
[2019-03-27 01:14:47,831] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8708.7051 2778977623.8502 806.0000
[2019-03-27 01:14:48,169] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8031.0353 3157350238.5968 1462.0000
[2019-03-27 01:14:48,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8321.6575 2925227078.0268 1165.0000
[2019-03-27 01:14:48,247] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8574.9203 2839264668.4200 935.0000
[2019-03-27 01:14:48,323] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03215768]
[2019-03-27 01:14:48,323] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.6, 68.0, 1.0, 2.0, 0.3245894752276106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513378.4987589025, 513378.4987589025, 168040.3724990615]
[2019-03-27 01:14:48,323] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:14:48,324] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0644657e-22 1.0000000e+00 3.1131002e-32 2.1160473e-30 2.8664517e-13], sampled 0.02641200041328229
[2019-03-27 01:14:48,379] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8168.3220 3000560930.2929 1323.0000
[2019-03-27 01:14:49,398] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 675000, evaluation results [675000.0, 8031.035328215249, 3157350238.596781, 1462.0, 8321.65748518175, 2925227078.0268097, 1165.0, 8708.705090233567, 2778977623.850205, 806.0, 8168.322033451053, 3000560930.2929215, 1323.0, 8574.920328897337, 2839264668.419995, 935.0]
[2019-03-27 01:14:51,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.345846e-17 9.999999e-01 3.539232e-27 2.080913e-23 9.713913e-08], sum to 1.0000
[2019-03-27 01:14:51,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1425
[2019-03-27 01:14:51,220] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.45, 85.0, 1.0, 2.0, 0.5584358961648915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852950.3846322736, 852950.3846322736, 202073.3479479912], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1585800.0000, 
sim time next is 1586400.0000, 
raw observation next is [23.46666666666667, 85.0, 1.0, 2.0, 0.6278567526424211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958441.4162134654, 958441.4162134654, 216058.5645501286], 
processed observation next is [1.0, 0.34782608695652173, 0.31121642969984215, 0.85, 1.0, 1.0, 0.5516346417378568, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2662337267259626, 0.2662337267259626, 0.32247546947780387], 
reward next is 0.6775, 
noisyNet noise sample is [array([0.14854757], dtype=float32), 0.7941684]. 
=============================================
[2019-03-27 01:14:59,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.412333e-17 9.999974e-01 9.945209e-28 3.834233e-21 2.583794e-06], sum to 1.0000
[2019-03-27 01:14:59,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2090
[2019-03-27 01:14:59,401] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 85.0, 1.0, 2.0, 0.7647650731172642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1162568.02342012, 1162568.023420121, 247754.8895852361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1591200.0000, 
sim time next is 1591800.0000, 
raw observation next is [23.61666666666667, 85.00000000000001, 1.0, 2.0, 0.7915507349907526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1202668.116468185, 1202668.116468186, 254678.1702682478], 
processed observation next is [1.0, 0.43478260869565216, 0.31832543443917877, 0.8500000000000001, 1.0, 1.0, 0.7488563072177742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3340744767967181, 0.33407447679671837, 0.38011667204216093], 
reward next is 0.6199, 
noisyNet noise sample is [array([0.08177378], dtype=float32), -1.7389894]. 
=============================================
[2019-03-27 01:15:07,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2604777e-20 1.0000000e+00 1.3347939e-29 2.0439006e-28 5.5344618e-12], sum to 1.0000
[2019-03-27 01:15:07,318] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0369
[2019-03-27 01:15:07,324] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 90.33333333333334, 1.0, 2.0, 0.5109346140577616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713956.8749801518, 713956.8749801518, 185327.0788929491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1719600.0000, 
sim time next is 1720200.0000, 
raw observation next is [26.06666666666667, 90.66666666666667, 1.0, 2.0, 0.5102235314160604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712962.9068680446, 712962.9068680452, 185213.433450836], 
processed observation next is [1.0, 0.9130434782608695, 0.4344391785150081, 0.9066666666666667, 1.0, 1.0, 0.4099078691759763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19804525190779018, 0.19804525190779035, 0.27643796037438206], 
reward next is 0.7236, 
noisyNet noise sample is [array([1.2336588], dtype=float32), -0.31669074]. 
=============================================
[2019-03-27 01:15:07,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9419984e-13 9.9914956e-01 2.6034696e-21 2.2892495e-15 8.5045793e-04], sum to 1.0000
[2019-03-27 01:15:07,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0098
[2019-03-27 01:15:07,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 85.5, 1.0, 2.0, 0.8284904892261313, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1157936.968665215, 1157936.968665215, 251021.2000464722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1859400.0000, 
sim time next is 1860000.0000, 
raw observation next is [26.3, 85.33333333333334, 1.0, 2.0, 0.8682096268251136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213482.01119987, 1213482.01119987, 261339.6115699471], 
processed observation next is [1.0, 0.5217391304347826, 0.4454976303317536, 0.8533333333333334, 1.0, 1.0, 0.8412164178615826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3370783364444083, 0.3370783364444083, 0.3900591217461897], 
reward next is 0.6099, 
noisyNet noise sample is [array([-0.7477391], dtype=float32), -1.1009315]. 
=============================================
[2019-03-27 01:15:07,550] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[55.64918 ]
 [56.29546 ]
 [55.942024]
 [56.065384]
 [55.566776]], R is [[56.07087326]
 [56.13550949]
 [56.12711716]
 [56.06663132]
 [55.50596619]].
[2019-03-27 01:15:08,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9495922e-20 1.0000000e+00 1.6089705e-28 5.2191064e-27 4.3140183e-12], sum to 1.0000
[2019-03-27 01:15:08,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2157
[2019-03-27 01:15:08,122] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1975200.0000, 
sim time next is 1975800.0000, 
raw observation next is [22.95, 95.83333333333333, 1.0, 2.0, 0.4047787068330637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600682.9922490107, 600682.9922490107, 174504.5394089108], 
processed observation next is [1.0, 0.8695652173913043, 0.28672985781990523, 0.9583333333333333, 1.0, 1.0, 0.2828659118470647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1668563867358363, 0.1668563867358363, 0.26045453643121014], 
reward next is 0.7395, 
noisyNet noise sample is [array([-2.0574913], dtype=float32), 0.40169057]. 
=============================================
[2019-03-27 01:15:14,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0395763e-22 1.0000000e+00 1.6501362e-31 1.1733271e-30 1.3385359e-15], sum to 1.0000
[2019-03-27 01:15:14,300] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8759
[2019-03-27 01:15:14,305] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 93.16666666666666, 1.0, 2.0, 0.3460372273495639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535188.9095105969, 535188.9095105969, 169485.4041763998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [22.0, 93.0, 1.0, 2.0, 0.3453446244914873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533927.2220454625, 533927.2220454625, 169377.4132252798], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.93, 1.0, 1.0, 0.21125858372468348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1483131172348507, 0.1483131172348507, 0.2528021092914624], 
reward next is 0.7472, 
noisyNet noise sample is [array([1.6794186], dtype=float32), -0.44451717]. 
=============================================
[2019-03-27 01:15:22,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1646310e-16 1.0000000e+00 5.1955304e-26 1.9724262e-22 4.6481627e-08], sum to 1.0000
[2019-03-27 01:15:22,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6822
[2019-03-27 01:15:22,964] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 94.33333333333333, 1.0, 2.0, 0.3987670624760264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595809.0149580113, 595809.0149580113, 174175.5988094222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1971600.0000, 
sim time next is 1972200.0000, 
raw observation next is [22.8, 94.66666666666667, 1.0, 2.0, 0.3977044069292393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595420.8676647046, 595420.8676647046, 174174.446602265], 
processed observation next is [1.0, 0.8260869565217391, 0.2796208530805688, 0.9466666666666668, 1.0, 1.0, 0.2743426589508907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16539468546241792, 0.16539468546241792, 0.2599618606003955], 
reward next is 0.7400, 
noisyNet noise sample is [array([0.76239425], dtype=float32), -0.028905733]. 
=============================================
[2019-03-27 01:15:44,140] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9582526e-26 1.0000000e+00 1.0474088e-35 7.0775819e-37 1.5458373e-23], sum to 1.0000
[2019-03-27 01:15:44,150] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0138
[2019-03-27 01:15:44,156] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.3, 76.33333333333334, 1.0, 2.0, 0.5818054634330592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813026.4883291817, 813026.4883291817, 197405.3537050742], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [30.2, 76.66666666666667, 1.0, 2.0, 0.5805956275692469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811335.1940017126, 811335.1940017126, 197186.6646657431], 
processed observation next is [1.0, 0.9130434782608695, 0.6303317535545023, 0.7666666666666667, 1.0, 1.0, 0.49469352719186366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22537088722269794, 0.22537088722269794, 0.2943084547249897], 
reward next is 0.7057, 
noisyNet noise sample is [array([-0.6467363], dtype=float32), -0.123558044]. 
=============================================
[2019-03-27 01:15:45,298] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:15:45,300] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:15:45,301] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:15:45,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:15:45,304] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:15:45,306] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:15:45,306] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:15:45,308] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:15:45,309] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:15:45,309] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:15:45,310] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:15:45,328] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-27 01:15:45,328] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-27 01:15:45,373] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-27 01:15:45,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-27 01:15:45,417] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-27 01:16:05,019] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:16:05,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.35, 76.0, 1.0, 2.0, 0.2351842686955316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 389625.7892667522, 389625.7892667516, 159335.9372315606]
[2019-03-27 01:16:05,021] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:16:05,025] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0634712e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2193613e-28], sampled 0.14873858232979642
[2019-03-27 01:16:05,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:16:05,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.16666666666667, 70.16666666666667, 1.0, 2.0, 0.2545285350105946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417344.4070227034, 417344.4070227041, 161359.2172201032]
[2019-03-27 01:16:05,638] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:16:05,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.05262154e-32 1.00000000e+00 0.00000000e+00 0.00000000e+00
 2.33988541e-29], sampled 0.023343664398508435
[2019-03-27 01:16:06,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:16:06,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [17.55, 91.66666666666667, 1.0, 2.0, 0.2193740897386196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 365853.6351823279, 365853.6351823279, 157414.33461253]
[2019-03-27 01:16:06,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:16:06,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6374304e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1408073e-30], sampled 0.8424141962099497
[2019-03-27 01:16:08,510] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:16:08,511] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.85, 51.0, 1.0, 2.0, 0.3404076519279594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525745.5316471308, 525745.5316471303, 168701.1901762203]
[2019-03-27 01:16:08,511] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:16:08,513] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5434374e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9786666e-29], sampled 0.9275224230409035
[2019-03-27 01:16:47,728] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:16:47,729] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.07107877, 79.75803490666668, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 1.0, 2.0, 1.03, 9.682338284547239, 6.9112, 184.5923449428631, 5891114.946550219, 3742685.107307044, 692226.2226445178]
[2019-03-27 01:16:47,730] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:16:47,734] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6862348e-10 1.9453641e-07 1.2284469e-16 1.9337010e-06 9.9999785e-01], sampled 0.226378471782943
[2019-03-27 01:17:30,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:17:30,954] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.76666666666667, 56.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.58018094401832, 6.9112, 168.9088285040519, 2758702.07398498, 2284115.566293169, 474629.52860062]
[2019-03-27 01:17:30,958] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:17:30,962] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3005722e-20 1.0000000e+00 2.5108204e-28 3.3060597e-26 6.3058414e-15], sampled 0.6536742078985455
[2019-03-27 01:17:30,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2758702.07398498 W.
[2019-03-27 01:17:39,008] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6286 2927411639.5640 1341.0000
[2019-03-27 01:17:39,011] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.029647408]
[2019-03-27 01:17:39,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333333, 88.33333333333333, 1.0, 2.0, 0.5067662071800055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708130.1906442683, 708130.190644269, 184663.1231249614]
[2019-03-27 01:17:39,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:17:39,016] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6023416e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.8871646e-27], sampled 0.35704833162939
[2019-03-27 01:17:39,050] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.2888 2779315459.8620 938.0000
[2019-03-27 01:17:39,698] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6749 3164155944.7551 1778.0000
[2019-03-27 01:17:39,711] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0966 3007867509.5417 1766.0000
[2019-03-27 01:17:39,735] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4085 2842419894.5471 1132.0000
[2019-03-27 01:17:40,751] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 700000, evaluation results [700000.0, 7882.674907938477, 3164155944.7551346, 1778.0, 8253.628570102283, 2927411639.564007, 1341.0, 8658.288762081247, 2779315459.8620114, 938.0, 7996.096616164504, 3007867509.5416894, 1766.0, 8497.408497033466, 2842419894.547066, 1132.0]
[2019-03-27 01:17:41,697] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.12250685e-26 1.00000000e+00 4.20209384e-34 1.57050063e-35
 4.35706085e-23], sum to 1.0000
[2019-03-27 01:17:41,706] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2385
[2019-03-27 01:17:41,712] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 82.0, 1.0, 2.0, 0.6655486001863871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930102.0482365907, 930102.0482365907, 213582.6950682274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2349600.0000, 
sim time next is 2350200.0000, 
raw observation next is [27.13333333333333, 82.0, 1.0, 2.0, 0.6682611215593884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 933894.4548958407, 933894.4548958407, 214141.528037051], 
processed observation next is [1.0, 0.17391304347826086, 0.484992101105845, 0.82, 1.0, 1.0, 0.6003146042884198, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2594151263599558, 0.2594151263599558, 0.31961422095082237], 
reward next is 0.6804, 
noisyNet noise sample is [array([-0.09008788], dtype=float32), -1.4902127]. 
=============================================
[2019-03-27 01:17:42,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6788258e-12 1.0000000e+00 1.0524976e-16 1.9480784e-15 2.4150623e-08], sum to 1.0000
[2019-03-27 01:17:42,972] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5395
[2019-03-27 01:17:42,981] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2419892.359522703 W.
[2019-03-27 01:17:42,984] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.16666666666667, 60.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.103028881384962, 6.9112, 168.9119619351553, 2419892.359522703, 2283803.150138351, 475650.8101279366], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2392800.0000, 
sim time next is 2393400.0000, 
raw observation next is [33.18333333333334, 60.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.513075973620201, 6.9112, 168.9093715034236, 2711054.090942071, 2284071.622747834, 474757.5630521728], 
processed observation next is [1.0, 0.6956521739130435, 0.7717219589257506, 0.6016666666666666, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.060187597362020104, 0.0, 0.8294223411266731, 0.7530705808172419, 0.6344643396521761, 0.7085933776898101], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0450737], dtype=float32), -0.9118041]. 
=============================================
[2019-03-27 01:17:57,902] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9085203e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8162459e-31], sum to 1.0000
[2019-03-27 01:17:57,910] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4727
[2019-03-27 01:17:57,917] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 87.33333333333334, 1.0, 2.0, 0.4638925871243377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654358.2785045952, 654358.2785045946, 178926.8233017709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2618400.0000, 
sim time next is 2619000.0000, 
raw observation next is [25.5, 86.5, 1.0, 2.0, 0.4660689571230514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655977.7309307858, 655977.7309307863, 179061.9022285829], 
processed observation next is [0.0, 0.30434782608695654, 0.40758293838862564, 0.865, 1.0, 1.0, 0.3567095868952426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18221603636966271, 0.18221603636966288, 0.2672565704904223], 
reward next is 0.7327, 
noisyNet noise sample is [array([-0.14652346], dtype=float32), -1.9175535]. 
=============================================
[2019-03-27 01:17:57,931] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[77.148224]
 [77.20234 ]
 [77.25657 ]
 [77.26363 ]
 [77.32123 ]], R is [[77.0823822 ]
 [77.04450989]
 [77.00725555]
 [76.97085571]
 [76.93569946]].
[2019-03-27 01:17:59,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5101145e-29 1.0000000e+00 7.9864557e-38 0.0000000e+00 1.2842191e-26], sum to 1.0000
[2019-03-27 01:17:59,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0721
[2019-03-27 01:17:59,355] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3159174402727212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499659.2191652672, 499659.2191652672, 167001.6353500614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.3162247988953518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500056.7546586314, 500056.754658632, 167029.6317560708], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.17617445650042388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13890465407184205, 0.13890465407184222, 0.2492979578448818], 
reward next is 0.7507, 
noisyNet noise sample is [array([-1.248407], dtype=float32), -1.4394203]. 
=============================================
[2019-03-27 01:17:59,372] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.86476 ]
 [72.80015 ]
 [72.65618 ]
 [72.615555]
 [72.58954 ]], R is [[72.86975098]
 [72.8917923 ]
 [72.91353607]
 [72.93506622]
 [72.95655823]].
[2019-03-27 01:18:00,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0884286e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5754200e-29], sum to 1.0000
[2019-03-27 01:18:00,416] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0224
[2019-03-27 01:18:00,421] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 89.0, 1.0, 2.0, 0.4486449415494924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642721.1019697809, 642721.1019697809, 177975.758107507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [24.5, 89.0, 1.0, 2.0, 0.4406684354096254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635592.2341938505, 635592.2341938505, 177369.741876003], 
processed observation next is [0.0, 0.782608695652174, 0.3601895734597157, 0.89, 1.0, 1.0, 0.3261065486862956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1765533983871807, 0.1765533983871807, 0.2647309580238851], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.09662188], dtype=float32), -0.9476371]. 
=============================================
[2019-03-27 01:18:00,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2022666e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4236307e-30], sum to 1.0000
[2019-03-27 01:18:00,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6629
[2019-03-27 01:18:00,444] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4791437419458264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669519.7517283816, 669519.7517283816, 180392.6398217088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2706000.0000, 
sim time next is 2706600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4794286468688249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669917.9821609448, 669917.9821609443, 180435.5140849939], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3728055986371384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18608832837804024, 0.18608832837804007, 0.2693067374402894], 
reward next is 0.7307, 
noisyNet noise sample is [array([1.5809535], dtype=float32), -0.27988744]. 
=============================================
[2019-03-27 01:18:02,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1729232e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2316157e-30], sum to 1.0000
[2019-03-27 01:18:02,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0548
[2019-03-27 01:18:02,556] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666666, 100.0, 1.0, 2.0, 0.4681866242906509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658817.9564229068, 658817.9564229074, 179357.1657760982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2708400.0000, 
sim time next is 2709000.0000, 
raw observation next is [23.5, 100.0, 1.0, 2.0, 0.4591589106539988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650969.4285451074, 650969.4285451068, 178653.963590918], 
processed observation next is [0.0, 0.34782608695652173, 0.31279620853080575, 1.0, 1.0, 1.0, 0.348384229703613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18082484126252982, 0.18082484126252965, 0.2666477068521164], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.67652786], dtype=float32), -1.4684352]. 
=============================================
[2019-03-27 01:18:02,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.02382 ]
 [75.014404]
 [75.00655 ]
 [74.96605 ]
 [74.971596]], R is [[75.05849457]
 [75.04020691]
 [75.02110291]
 [75.00141907]
 [74.98209381]].
[2019-03-27 01:18:09,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.6528297e-30 1.0000000e+00 1.1887665e-38 0.0000000e+00 9.8567084e-27], sum to 1.0000
[2019-03-27 01:18:09,611] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2974
[2019-03-27 01:18:09,617] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.402772868211731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596731.3529397737, 596731.3529397737, 174111.2858917953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.3994368544729122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 593040.6507495865, 593040.6507495872, 173807.9887622953], 
processed observation next is [1.0, 0.8695652173913043, 0.30489731437598716, 0.9233333333333335, 1.0, 1.0, 0.276429945148087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16473351409710738, 0.16473351409710757, 0.25941490860044075], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.6468212], dtype=float32), 1.3620617]. 
=============================================
[2019-03-27 01:18:10,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8585153e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.3885245e-29], sum to 1.0000
[2019-03-27 01:18:10,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0554
[2019-03-27 01:18:10,705] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.402772868211731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596731.3529397737, 596731.3529397737, 174111.2858917953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.3994368544729122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 593040.6507495865, 593040.6507495872, 173807.9887622953], 
processed observation next is [1.0, 0.8695652173913043, 0.30489731437598716, 0.9233333333333335, 1.0, 1.0, 0.276429945148087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16473351409710738, 0.16473351409710757, 0.25941490860044075], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.15985695], dtype=float32), 0.6039239]. 
=============================================
[2019-03-27 01:18:11,207] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0381143e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3100498e-29], sum to 1.0000
[2019-03-27 01:18:11,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0399
[2019-03-27 01:18:11,221] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3512009365583582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540994.8831060061, 540994.8831060055, 169897.9269692278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2851800.0000, 
sim time next is 2852400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3505705274872588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540041.7948673894, 540041.79486739, 169819.9699637635], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2175548523942877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15001160968538593, 0.1500116096853861, 0.2534626417369604], 
reward next is 0.7465, 
noisyNet noise sample is [array([-0.41484138], dtype=float32), 1.6664817]. 
=============================================
[2019-03-27 01:18:13,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.3838153e-31 1.0000000e+00 1.2188580e-37 0.0000000e+00 1.6684703e-27], sum to 1.0000
[2019-03-27 01:18:13,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8239
[2019-03-27 01:18:13,361] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3474840441343431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535290.2104987665, 535290.2104987672, 169430.6731444829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2875800.0000, 
sim time next is 2876400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3516546787000042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541715.1016214251, 541715.1016214245, 169957.9511718156], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21886105867470387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15047641711706253, 0.15047641711706236, 0.25366858383853075], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.71789765], dtype=float32), 1.1211798]. 
=============================================
[2019-03-27 01:18:32,621] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.502619e-33 1.000000e+00 0.000000e+00 0.000000e+00 8.265850e-31], sum to 1.0000
[2019-03-27 01:18:32,634] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8481
[2019-03-27 01:18:32,640] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4554664323995708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644889.3542668258, 644889.3542668258, 178005.615019745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214200.0000, 
sim time next is 3214800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3435988257961173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1790261510823746, 0.1790261510823746, 0.2656194289465004], 
reward next is 0.7344, 
noisyNet noise sample is [array([-1.977363], dtype=float32), 0.5228329]. 
=============================================
[2019-03-27 01:18:34,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0937971e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9179483e-31], sum to 1.0000
[2019-03-27 01:18:34,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2120
[2019-03-27 01:18:34,071] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666667, 63.0, 1.0, 2.0, 0.5639949699404903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788128.5179131179, 788128.5179131179, 194231.0397517385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3246000.0000, 
sim time next is 3246600.0000, 
raw observation next is [32.83333333333333, 63.0, 1.0, 2.0, 0.5693407449678668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795601.5200955024, 795601.5200955024, 195174.2476920109], 
processed observation next is [0.0, 0.5652173913043478, 0.7551342812006318, 0.63, 1.0, 1.0, 0.48113342767212863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22100042224875066, 0.22100042224875066, 0.2913048473015088], 
reward next is 0.7087, 
noisyNet noise sample is [array([-1.3120592], dtype=float32), -0.16941226]. 
=============================================
[2019-03-27 01:18:36,574] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-27 01:18:36,579] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:18:36,580] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:18:36,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:18:36,582] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:18:36,583] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:18:36,585] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:18:36,586] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:18:36,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:18:36,592] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:18:36,594] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:18:36,607] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-27 01:18:36,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-27 01:18:36,655] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-27 01:18:36,656] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-27 01:18:36,657] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-27 01:19:05,417] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:19:05,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.93333333333333, 93.33333333333334, 1.0, 2.0, 0.3963516500100781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594623.7978677945, 594623.7978677938, 174135.9362407214]
[2019-03-27 01:19:05,419] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:19:05,423] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6536417e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3794151e-29], sampled 0.04549951158177035
[2019-03-27 01:19:32,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:19:32,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.33333333333333, 66.33333333333334, 1.0, 2.0, 0.5470945530413129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764503.3180307911, 764503.3180307911, 191304.4489190179]
[2019-03-27 01:19:32,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:19:32,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.8942882e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7119014e-36], sampled 0.8016093779779393
[2019-03-27 01:19:39,448] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:19:39,449] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.99772688, 78.40712283, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.390625832325364, 6.9112, 168.9045142974178, 2503946.543480917, 1454443.396559448, 310734.118670265]
[2019-03-27 01:19:39,451] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:19:39,455] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.6292535e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7434616e-30], sampled 0.060740389339717704
[2019-03-27 01:19:39,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2503946.543480917 W.
[2019-03-27 01:19:48,831] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:19:48,832] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.13333333333333, 65.33333333333333, 1.0, 2.0, 0.7373682592216466, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.974052111868055, 6.9112, 168.9125316694678, 1927421.503821985, 1882832.164609081, 394056.6088727798]
[2019-03-27 01:19:48,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:19:48,837] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6681567e-23 1.0000000e+00 8.0680901e-31 6.9458062e-28 1.0603671e-18], sampled 0.11560196761670949
[2019-03-27 01:19:48,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1927421.503821985 W.
[2019-03-27 01:19:49,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:19:49,168] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.791259925, 76.20824203000001, 1.0, 2.0, 0.5654170795248883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790116.5178820228, 790116.5178820228, 194479.9902805849]
[2019-03-27 01:19:49,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:19:49,170] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5605389e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4460132e-27], sampled 0.13573128321949857
[2019-03-27 01:20:30,445] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7241 2779361254.6677 934.0000
[2019-03-27 01:20:30,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.9992 3007928867.4828 1766.0000
[2019-03-27 01:20:30,902] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.028573804]
[2019-03-27 01:20:30,903] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.6, 73.83333333333334, 1.0, 2.0, 0.3603756504164839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557612.6220977145, 557612.622097715, 171350.3279408578]
[2019-03-27 01:20:30,904] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:20:30,907] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.6221068e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5764958e-29], sampled 0.588329643539711
[2019-03-27 01:20:30,910] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8248.4541 2927776967.2695 1355.0000
[2019-03-27 01:20:31,048] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3063 2842539581.6880 1136.0000
[2019-03-27 01:20:31,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0461 3164211057.9435 1790.0000
[2019-03-27 01:20:32,096] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 725000, evaluation results [725000.0, 7881.046139477268, 3164211057.9435225, 1790.0, 8248.454099464552, 2927776967.2694683, 1355.0, 8659.724054637785, 2779361254.667692, 934.0, 7995.999248431243, 3007928867.482793, 1766.0, 8495.306331088608, 2842539581.688029, 1136.0]
[2019-03-27 01:20:34,226] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8566045e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7385559e-29], sum to 1.0000
[2019-03-27 01:20:34,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7534
[2019-03-27 01:20:34,240] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 72.66666666666666, 1.0, 2.0, 0.5095282452493232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711991.0203410599, 711991.0203410599, 185102.8810049791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3318000.0000, 
sim time next is 3318600.0000, 
raw observation next is [29.0, 73.33333333333334, 1.0, 2.0, 0.5131737203342397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717086.7566957298, 717086.7566957292, 185686.3818998642], 
processed observation next is [0.0, 0.391304347826087, 0.5734597156398105, 0.7333333333333334, 1.0, 1.0, 0.41346231365571046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19919076574881384, 0.19919076574881367, 0.27714385358188687], 
reward next is 0.7229, 
noisyNet noise sample is [array([-0.64348835], dtype=float32), 1.4704956]. 
=============================================
[2019-03-27 01:20:35,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5015773e-11 9.8582911e-01 4.1158992e-17 3.1242855e-09 1.4170961e-02], sum to 1.0000
[2019-03-27 01:20:35,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0458
[2019-03-27 01:20:35,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2699752.507054531 W.
[2019-03-27 01:20:35,748] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.6457041172073875, 1.0, 2.0, 0.6434420981179563, 1.0, 1.0, 1.03, 7.005093451366728, 6.9112, 170.5573041426782, 2699752.507054531, 2632492.808499136, 504726.7124482589], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3603600.0000, 
sim time next is 3604200.0000, 
raw observation next is [32.83333333333334, 63.66666666666666, 1.0, 2.0, 0.4158722665594737, 1.0, 2.0, 0.4158722665594737, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1162481.326793341, 1162481.326793341, 276210.1342435081], 
processed observation next is [1.0, 0.7391304347826086, 0.7551342812006324, 0.6366666666666666, 1.0, 1.0, 0.2962316464571972, 1.0, 1.0, 0.2962316464571972, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3229114796648169, 0.3229114796648169, 0.4122539317067285], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.4401073], dtype=float32), -1.3239721]. 
=============================================
[2019-03-27 01:20:39,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7608254e-21 1.0000000e+00 2.8392435e-29 5.7719138e-27 3.2048743e-15], sum to 1.0000
[2019-03-27 01:20:39,702] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0425
[2019-03-27 01:20:39,709] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 80.33333333333334, 1.0, 2.0, 0.6953607343293676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 971783.472008448, 971783.4720084474, 219843.3529516213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3555600.0000, 
sim time next is 3556200.0000, 
raw observation next is [26.58333333333334, 80.66666666666666, 1.0, 2.0, 0.6873813027547047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960626.9795823063, 960626.9795823058, 218141.3270338594], 
processed observation next is [1.0, 0.13043478260869565, 0.45892575039494504, 0.8066666666666665, 1.0, 1.0, 0.6233509671743429, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26684082766175177, 0.2668408276617516, 0.32558407019979013], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.60863113], dtype=float32), -0.013758484]. 
=============================================
[2019-03-27 01:20:39,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8302779e-13 6.0001108e-05 2.2260779e-21 2.0464238e-08 9.9993992e-01], sum to 1.0000
[2019-03-27 01:20:39,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3306
[2019-03-27 01:20:39,876] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 70.33333333333334, 1.0, 2.0, 0.569280561967312, 1.0, 2.0, 0.569280561967312, 1.0, 2.0, 0.9886520721785365, 6.9112, 6.9112, 170.5573041426782, 2388288.358747222, 2388288.358747222, 466360.8286839278], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3406800.0000, 
sim time next is 3407400.0000, 
raw observation next is [31.5, 70.5, 1.0, 2.0, 0.5711738650834395, 1.0, 2.0, 0.5711738650834395, 1.0, 2.0, 0.9919401135663416, 6.911199999999999, 6.9112, 170.5573041426782, 2396238.901027008, 2396238.901027009, 467853.3634708512], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.705, 1.0, 1.0, 0.48334200612462586, 1.0, 1.0, 0.48334200612462586, 1.0, 1.0, 0.9901708702028554, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6656219169519466, 0.665621916951947, 0.6982886021953003], 
reward next is 0.3017, 
noisyNet noise sample is [array([0.13407405], dtype=float32), 0.106219314]. 
=============================================
[2019-03-27 01:20:50,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8460053e-18 1.0000000e+00 1.3790194e-23 2.3451136e-21 5.0659177e-12], sum to 1.0000
[2019-03-27 01:20:50,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8666
[2019-03-27 01:20:50,163] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 84.0, 1.0, 2.0, 1.003204192902753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402286.256768923, 1402286.256768923, 299908.4481120955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3997800.0000, 
sim time next is 3998400.0000, 
raw observation next is [29.66666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.929289861679488, 6.9112, 168.9126525651582, 1466597.264269796, 1453763.716882355, 311355.5335554355], 
processed observation next is [1.0, 0.2608695652173913, 0.6050552922590839, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0018089861679487563, 0.0, 0.8294384526418664, 0.4073881289638322, 0.40382325468954305, 0.46470975157527683], 
reward next is 0.4448, 
noisyNet noise sample is [array([-0.50440776], dtype=float32), -0.75767]. 
=============================================
[2019-03-27 01:20:56,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5712942e-10 7.0393871e-04 1.3496477e-16 2.6356935e-07 9.9929583e-01], sum to 1.0000
[2019-03-27 01:20:56,623] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1241
[2019-03-27 01:20:56,633] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.596933246980587, 1.0, 2.0, 0.596933246980587, 1.0, 2.0, 1.03, 6.918704392786211, 6.9112, 170.5573041426782, 2504415.225037994, 2499039.52322605, 486868.3902716148], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3682800.0000, 
sim time next is 3683400.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.6673424720993818, 1.0, 2.0, 0.6542612755639535, 1.0, 2.0, 1.03, 7.005095157281602, 6.9112, 170.5573041426782, 2745197.45626134, 2677936.535689728, 511211.2092645983], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.5992077977100986, 1.0, 1.0, 0.5834473199565705, 1.0, 1.0, 1.0365853658536586, 0.00938951572816018, 0.0, 0.8375144448122397, 0.7625548489614834, 0.7438712599138133, 0.7630018048725348], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06885771], dtype=float32), -0.9750936]. 
=============================================
[2019-03-27 01:21:14,368] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2512206e-18 1.0000000e+00 3.1580043e-23 2.5159070e-24 1.5705106e-15], sum to 1.0000
[2019-03-27 01:21:14,375] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6302
[2019-03-27 01:21:14,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.9114081067797528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1273896.022769302, 1273896.022769301, 273098.4465383361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3994800.0000, 
sim time next is 3995400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.9021881975391663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1261001.491076004, 1261001.491076004, 270543.7144273287], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.8821544548664655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35027819196555665, 0.35027819196555665, 0.40379658869750557], 
reward next is 0.5962, 
noisyNet noise sample is [array([0.3660248], dtype=float32), -0.31724626]. 
=============================================
[2019-03-27 01:21:26,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5909362e-19 1.0000000e+00 1.2549016e-25 9.0605440e-24 3.4469678e-16], sum to 1.0000
[2019-03-27 01:21:26,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3222
[2019-03-27 01:21:26,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1703713.217283246 W.
[2019-03-27 01:21:26,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 81.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.263297578968324, 6.9112, 168.9109766956921, 1703713.217283246, 1453926.004269188, 311358.1073930356], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [31.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5380098301895948, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9343451524696663, 6.911200000000001, 6.9112, 168.9126698163479, 1504140.370105423, 1504140.370105422, 329563.7949738437], 
processed observation next is [1.0, 0.30434782608695654, 0.6840442338072673, 0.8066666666666668, 1.0, 1.0, 0.44338533757782506, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9199331127678856, 8.881784197001253e-17, 0.0, 0.8294385373531055, 0.41781676947372864, 0.41781676947372837, 0.49188626115499057], 
reward next is 0.5081, 
noisyNet noise sample is [array([-1.7512426], dtype=float32), -1.557882]. 
=============================================
[2019-03-27 01:21:27,993] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 01:21:27,995] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:21:27,996] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:21:27,996] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:21:27,997] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:21:27,997] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:21:27,999] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:21:28,004] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:21:28,002] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:21:28,005] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:21:28,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:21:28,023] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-27 01:21:28,023] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-27 01:21:28,062] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-27 01:21:28,063] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-27 01:21:28,089] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-27 01:22:18,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:22:18,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.83333333333334, 80.66666666666666, 1.0, 2.0, 0.5010914477400578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700197.947278546, 700197.9472785455, 183766.3232391133]
[2019-03-27 01:22:18,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:22:18,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0496116e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.3947365e-31], sampled 0.6592030236160163
[2019-03-27 01:22:25,104] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:22:25,105] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.6, 60.0, 1.0, 2.0, 0.5557342165372906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776580.6927310515, 776580.6927310515, 192789.1223691348]
[2019-03-27 01:22:25,107] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:22:25,108] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5506547e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3095321e-32], sampled 0.033403785402386976
[2019-03-27 01:22:27,035] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:22:27,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.43448758333334, 67.49477447666666, 1.0, 2.0, 0.6061224390850996, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.936882904022017, 6.9112, 168.9126912997804, 1694718.148272867, 1676497.840830105, 365730.3204933237]
[2019-03-27 01:22:27,039] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:22:27,042] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.68873918e-27 1.00000000e+00 4.30194060e-34 4.76780767e-36
 1.09781575e-23], sampled 0.3775696778775103
[2019-03-27 01:22:27,044] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1694718.148272867 W.
[2019-03-27 01:22:37,847] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:22:37,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.80289152, 79.19795097, 1.0, 2.0, 0.5564084897820633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777523.2642259982, 777523.2642259988, 192905.2753874115]
[2019-03-27 01:22:37,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:22:37,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.9579115e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0966187e-30], sampled 0.3230323210351568
[2019-03-27 01:23:19,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:23:19,751] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.44371611, 92.40243148, 1.0, 2.0, 0.5037107894263738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703859.2855547417, 703859.2855547423, 184179.078377739]
[2019-03-27 01:23:19,754] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:23:19,759] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.4255965e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0455364e-30], sampled 0.2669262778541276
[2019-03-27 01:23:21,248] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1891 2842631158.9836 1132.0000
[2019-03-27 01:23:21,583] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.026775884]
[2019-03-27 01:23:21,585] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 90.0, 1.0, 2.0, 0.5075820176449558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709270.5446718894, 709270.5446718894, 184792.8573793201]
[2019-03-27 01:23:21,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:23:21,587] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1308347e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5875033e-29], sampled 0.4285051820774527
[2019-03-27 01:23:22,127] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.8219 2779448437.3049 937.0000
[2019-03-27 01:23:22,243] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2953 3007596012.4260 1766.0000
[2019-03-27 01:23:22,293] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8751 2927413537.9824 1341.0000
[2019-03-27 01:23:22,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.2051 3164156975.8057 1780.0000
[2019-03-27 01:23:23,340] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 750000, evaluation results [750000.0, 7882.205092115342, 3164156975.8057203, 1780.0, 8252.87510155748, 2927413537.9823656, 1341.0, 8657.82192428555, 2779448437.3049035, 937.0, 7998.295259250187, 3007596012.4259605, 1766.0, 8495.189086344117, 2842631158.983619, 1132.0]
[2019-03-27 01:23:23,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8527253e-29 1.0000000e+00 2.9164791e-38 0.0000000e+00 3.6255788e-29], sum to 1.0000
[2019-03-27 01:23:23,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0681
[2019-03-27 01:23:23,895] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.6320546486463455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 883274.8935925959, 883274.8935925965, 206871.0734202056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4219200.0000, 
sim time next is 4219800.0000, 
raw observation next is [32.66666666666666, 72.33333333333334, 1.0, 2.0, 0.6362255977738791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889106.092997451, 889106.092997451, 207689.8052303777], 
processed observation next is [1.0, 0.8695652173913043, 0.7472353870458132, 0.7233333333333334, 1.0, 1.0, 0.5617175876793724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24697391472151414, 0.24697391472151414, 0.30998478392593687], 
reward next is 0.6900, 
noisyNet noise sample is [array([1.2375723], dtype=float32), -1.839531]. 
=============================================
[2019-03-27 01:23:24,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7480127e-23 1.0000000e+00 5.3551701e-30 9.7060964e-31 1.2592596e-19], sum to 1.0000
[2019-03-27 01:23:24,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0514
[2019-03-27 01:23:24,876] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.598794308316437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836776.3885920225, 836776.3885920225, 200521.6796239286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4236000.0000, 
sim time next is 4236600.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5983498642354573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836155.0622102371, 836155.0622102377, 200439.0969074108], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5160841737776594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2322652950583992, 0.23226529505839938, 0.29916283120509074], 
reward next is 0.7008, 
noisyNet noise sample is [array([0.3871353], dtype=float32), 0.84205145]. 
=============================================
[2019-03-27 01:23:25,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9795244e-27 1.0000000e+00 5.1170679e-34 2.2128162e-37 1.0468669e-25], sum to 1.0000
[2019-03-27 01:23:25,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4298
[2019-03-27 01:23:25,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6199593662311755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866365.2535100605, 866365.2535100605, 204523.6742513632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6188710837720963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864843.8086701048, 864843.8086701054, 204314.6587111072], 
processed observation next is [1.0, 0.9565217391304348, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5408085346651763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24023439129725135, 0.2402343912972515, 0.30494725180762267], 
reward next is 0.6951, 
noisyNet noise sample is [array([-2.1987233], dtype=float32), -1.1671437]. 
=============================================
[2019-03-27 01:23:26,128] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3181900e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1927068e-31], sum to 1.0000
[2019-03-27 01:23:26,137] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5762
[2019-03-27 01:23:26,142] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 79.00000000000001, 1.0, 2.0, 0.520589465356836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727452.7462166852, 727452.7462166859, 186885.1744092829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4529400.0000, 
sim time next is 4530000.0000, 
raw observation next is [28.33333333333334, 79.0, 1.0, 2.0, 0.5251308930394936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733800.9636913787, 733800.9636913781, 187627.6321228274], 
processed observation next is [0.0, 0.43478260869565216, 0.5418641390205374, 0.79, 1.0, 1.0, 0.4278685458307151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20383360102538298, 0.2038336010253828, 0.28004124197436925], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.63466066], dtype=float32), -0.72787553]. 
=============================================
[2019-03-27 01:23:26,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.43131]
 [69.44588]
 [69.38847]
 [69.38713]
 [69.37219]], R is [[69.41756439]
 [69.44445801]
 [69.47242737]
 [69.50111389]
 [69.53044891]].
[2019-03-27 01:23:28,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1972590e-17 1.0000000e+00 3.3318690e-22 5.3551099e-22 1.1617824e-12], sum to 1.0000
[2019-03-27 01:23:28,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1259
[2019-03-27 01:23:28,485] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1685744.589174153 W.
[2019-03-27 01:23:28,493] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.6029201717882581, 1.0, 2.0, 0.6029201717882581, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1685744.589174153, 1685744.589174153, 335346.492306056], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4339800.0000, 
sim time next is 4340400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.5595512296758571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.971755439527635, 6.9112, 6.9112, 168.9129565103946, 1564409.092601331, 1564409.092601331, 342354.5742951269], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 0.4693388309347676, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.9655554140580913, 0.0, 0.0, 0.8294399451521239, 0.4345580812781475, 0.4345580812781475, 0.5109769765598908], 
reward next is 0.4890, 
noisyNet noise sample is [array([1.7327895], dtype=float32), 0.66778684]. 
=============================================
[2019-03-27 01:23:29,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3009134e-26 1.0000000e+00 4.7417302e-34 2.0754770e-36 4.7084804e-24], sum to 1.0000
[2019-03-27 01:23:29,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3967
[2019-03-27 01:23:29,274] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.621937577563629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869130.8457266962, 869130.8457266962, 204904.5806662329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4315200.0000, 
sim time next is 4315800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6229624248749964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870563.612440204, 870563.612440204, 205102.2556775321], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5457378612951764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24182322567783446, 0.24182322567783446, 0.30612276966795837], 
reward next is 0.6939, 
noisyNet noise sample is [array([0.7099439], dtype=float32), -1.0797647]. 
=============================================
[2019-03-27 01:23:37,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.106831e-33 1.000000e+00 0.000000e+00 0.000000e+00 5.221269e-30], sum to 1.0000
[2019-03-27 01:23:37,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2131
[2019-03-27 01:23:37,083] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6415212096858719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 896509.6744091983, 896509.6744091978, 208736.5282130332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4443000.0000, 
sim time next is 4443600.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6357747105557006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 888475.7281487472, 888475.7281487467, 207600.5647921237], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5611743500671091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.246798813374652, 0.24679881337465184, 0.3098515892419757], 
reward next is 0.6901, 
noisyNet noise sample is [array([-1.2152292], dtype=float32), -0.6037449]. 
=============================================
[2019-03-27 01:23:37,403] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.3134828e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3695938e-30], sum to 1.0000
[2019-03-27 01:23:37,416] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5963
[2019-03-27 01:23:37,422] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5642307618087112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788458.1365767573, 788458.1365767573, 194270.5614235827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4474800.0000, 
sim time next is 4475400.0000, 
raw observation next is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5616974823647708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784916.8146099654, 784916.8146099654, 193826.4778020406], 
processed observation next is [0.0, 0.8260869565217391, 0.5655608214849924, 0.7983333333333335, 1.0, 1.0, 0.4719246775479166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21803244850276818, 0.21803244850276818, 0.2892932504508069], 
reward next is 0.7107, 
noisyNet noise sample is [array([-1.239179], dtype=float32), -0.21044247]. 
=============================================
[2019-03-27 01:23:39,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1489925e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2836086e-30], sum to 1.0000
[2019-03-27 01:23:39,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7265
[2019-03-27 01:23:39,222] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5085719292308827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710654.2619121685, 710654.261912169, 184949.4775477625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4499400.0000, 
sim time next is 4500000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5079176052521707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709739.6352092845, 709739.6352092845, 184845.3994978373], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4071296448821333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971498986692457, 0.1971498986692457, 0.27588865596692136], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.13242388], dtype=float32), 0.007047375]. 
=============================================
[2019-03-27 01:23:39,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.18907 ]
 [70.192986]
 [70.2351  ]
 [70.23519 ]
 [70.1766  ]], R is [[70.30440521]
 [70.32531738]
 [70.34588623]
 [70.36621094]
 [70.38640594]].
[2019-03-27 01:23:46,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2182639e-13 1.0000000e+00 2.9352359e-19 8.3882023e-17 4.1099149e-08], sum to 1.0000
[2019-03-27 01:23:46,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8432
[2019-03-27 01:23:46,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1948096.752186165 W.
[2019-03-27 01:23:46,398] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 86.5, 1.0, 2.0, 0.6966673213573812, 1.0, 2.0, 0.6966673213573812, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1948096.752186165, 1948096.752186166, 372704.0722185093], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4588200.0000, 
sim time next is 4588800.0000, 
raw observation next is [28.0, 85.66666666666667, 1.0, 2.0, 0.6387217476197153, 1.0, 2.0, 0.6387217476197153, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1785928.010001174, 1785928.010001173, 348997.1556728061], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8566666666666667, 1.0, 1.0, 0.5647249971321872, 1.0, 1.0, 0.5647249971321872, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.496091113889215, 0.49609111388921473, 0.5208912771235912], 
reward next is 0.4791, 
noisyNet noise sample is [array([-0.78666764], dtype=float32), -1.5118515]. 
=============================================
[2019-03-27 01:23:55,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1608683e-18 1.0000000e+00 3.1013853e-27 2.1836294e-27 3.6513518e-11], sum to 1.0000
[2019-03-27 01:23:55,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4849
[2019-03-27 01:23:55,880] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4846202610063223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677174.6703606546, 677174.670360654, 181221.2402177604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4759200.0000, 
sim time next is 4759800.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.9870676619502732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1379715.836487639, 1379715.836487639, 295006.8123428341], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.9844188698196062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38325439902434416, 0.38325439902434416, 0.4403086751385583], 
reward next is 0.5597, 
noisyNet noise sample is [array([0.9432407], dtype=float32), 0.98429215]. 
=============================================
[2019-03-27 01:24:00,560] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3732880e-11 5.7464975e-05 2.0697296e-20 4.0633739e-11 9.9994254e-01], sum to 1.0000
[2019-03-27 01:24:00,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9219
[2019-03-27 01:24:00,575] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4311629889795182, 1.0, 1.0, 0.4311629889795182, 1.0, 2.0, 0.7365886913829678, 6.9112, 6.9112, 170.5573041426782, 1808379.287557022, 1808379.287557022, 368931.6240758324], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.4305903048121484, 1.0, 2.0, 0.4305903048121484, 1.0, 2.0, 0.738851980631496, 6.9112, 6.9112, 170.5573041426782, 1805975.317443936, 1805975.317443936, 369099.4378725134], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.31396422266523905, 1.0, 1.0, 0.31396422266523905, 1.0, 1.0, 0.6815268056481658, 0.0, 0.0, 0.8375144448122397, 0.5016598104010933, 0.5016598104010933, 0.550894683391811], 
reward next is 0.4491, 
noisyNet noise sample is [array([-1.1169195], dtype=float32), 0.40164918]. 
=============================================
[2019-03-27 01:24:15,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2316683e-23 1.0000000e+00 1.0574189e-32 2.5849272e-34 4.2814090e-20], sum to 1.0000
[2019-03-27 01:24:15,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0203
[2019-03-27 01:24:15,022] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5146236257301708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719113.4775577422, 719113.4775577415, 185918.4154986019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5099400.0000, 
sim time next is 5100000.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5131314479582318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717027.6671699791, 717027.6671699791, 185678.4695288258], 
processed observation next is [0.0, 0.0, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.41341138308220704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19917435199166086, 0.19917435199166086, 0.27713204407287434], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.491324], dtype=float32), -0.011891174]. 
=============================================
[2019-03-27 01:24:15,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.85892 ]
 [66.80163 ]
 [67.000595]
 [67.62886 ]
 [69.21857 ]], R is [[67.06230164]
 [67.11418915]
 [67.16516876]
 [67.21511841]
 [67.26457214]].
[2019-03-27 01:24:15,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8943963e-25 1.0000000e+00 2.3565778e-33 8.7145456e-37 3.8640685e-19], sum to 1.0000
[2019-03-27 01:24:15,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7390
[2019-03-27 01:24:15,593] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.45, 57.66666666666667, 1.0, 2.0, 0.5341408854167669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746395.6596899935, 746395.6596899942, 189120.6253044489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5507400.0000, 
sim time next is 5508000.0000, 
raw observation next is [33.2, 59.0, 1.0, 2.0, 0.5421744223300925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757625.5348852355, 757625.5348852355, 190469.7265467268], 
processed observation next is [1.0, 0.782608695652174, 0.7725118483412324, 0.59, 1.0, 1.0, 0.4484029184699909, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21045153746812098, 0.21045153746812098, 0.28428317395033853], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.06702412], dtype=float32), -0.44526464]. 
=============================================
[2019-03-27 01:24:15,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.21844 ]
 [63.97152 ]
 [61.950558]
 [61.272446]
 [59.810856]], R is [[65.63604736]
 [65.69741821]
 [65.75950623]
 [65.82310486]
 [65.77140808]].
[2019-03-27 01:24:16,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1677741e-27 1.0000000e+00 3.3831751e-38 0.0000000e+00 3.2353846e-24], sum to 1.0000
[2019-03-27 01:24:16,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8031
[2019-03-27 01:24:16,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 84.0, 1.0, 2.0, 0.4833168706571978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675352.8243392322, 675352.8243392328, 181023.1087109265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5119800.0000, 
sim time next is 5120400.0000, 
raw observation next is [26.33333333333334, 84.0, 1.0, 2.0, 0.4875456076470719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681263.6571862255, 681263.6571862255, 181667.5467462041], 
processed observation next is [0.0, 0.2608695652173913, 0.44707740916271754, 0.84, 1.0, 1.0, 0.38258506945430343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18923990477395153, 0.18923990477395153, 0.2711455921585136], 
reward next is 0.7289, 
noisyNet noise sample is [array([-1.6447096], dtype=float32), 1.4421194]. 
=============================================
[2019-03-27 01:24:17,423] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.2166848e-28 1.0000000e+00 3.1145873e-38 0.0000000e+00 5.2730267e-24], sum to 1.0000
[2019-03-27 01:24:17,433] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4892
[2019-03-27 01:24:17,437] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4805817283252084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671654.9019920448, 671654.9019920448, 180625.1653267668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5114400.0000, 
sim time next is 5115000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4807540498853487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671896.143399213, 671896.143399213, 180651.216108518], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.374402469741384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1866378176108925, 0.1866378176108925, 0.2696286807589821], 
reward next is 0.7304, 
noisyNet noise sample is [array([0.12142684], dtype=float32), -0.8464579]. 
=============================================
[2019-03-27 01:24:17,458] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.15728 ]
 [73.210846]
 [73.213356]
 [73.21455 ]
 [73.207436]], R is [[73.13983917]
 [73.13884735]
 [73.13767242]
 [73.13629913]
 [73.13478851]].
[2019-03-27 01:24:19,276] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 01:24:19,279] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:24:19,280] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:24:19,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:24:19,282] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:24:19,283] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:24:19,283] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:24:19,286] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:24:19,285] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:24:19,287] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:24:19,289] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:24:19,308] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-27 01:24:19,309] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-27 01:24:19,328] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-27 01:24:19,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-27 01:24:19,366] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-27 01:25:43,804] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.03118719]
[2019-03-27 01:25:43,807] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.72392671, 84.09513529, 1.0, 2.0, 0.6727524649355271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940173.8823927084, 940173.882392709, 215082.0005479628]
[2019-03-27 01:25:43,808] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:25:43,810] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6766002e-21 1.0000000e+00 9.1753259e-31 4.7834688e-31 4.7493018e-17], sampled 0.5551827863440743
[2019-03-27 01:26:13,489] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8567.5350 2839296693.7311 970.0000
[2019-03-27 01:26:13,539] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8699.5041 2779445197.0238 835.0000
[2019-03-27 01:26:13,657] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8009.1493 3158580950.9045 1556.0000
[2019-03-27 01:26:13,676] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8297.9535 2926273372.3719 1236.0000
[2019-03-27 01:26:13,864] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8172.0792 2999420573.0440 1305.0000
[2019-03-27 01:26:14,882] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 775000, evaluation results [775000.0, 8009.149316356765, 3158580950.904462, 1556.0, 8297.953453240425, 2926273372.371926, 1236.0, 8699.50407242728, 2779445197.0237646, 835.0, 8172.079232466473, 2999420573.044008, 1305.0, 8567.534964513085, 2839296693.7311354, 970.0]
[2019-03-27 01:26:14,926] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2201273e-25 1.0000000e+00 1.8873967e-34 1.5646932e-36 4.9427232e-21], sum to 1.0000
[2019-03-27 01:26:14,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0236
[2019-03-27 01:26:14,939] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4951180188941846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691848.2844061784, 691848.2844061791, 182833.909537162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5180400.0000, 
sim time next is 5181000.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.4955026115049634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692385.8662403717, 692385.8662403717, 182893.895497831], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.3921718210903174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19232940728899214, 0.19232940728899214, 0.2729759634295985], 
reward next is 0.7270, 
noisyNet noise sample is [array([1.2205846], dtype=float32), 0.31483516]. 
=============================================
[2019-03-27 01:26:14,982] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.14989 ]
 [67.05826 ]
 [67.04294 ]
 [67.039764]
 [67.04211 ]], R is [[67.19172668]
 [67.24692535]
 [67.30027771]
 [67.35182953]
 [67.40168762]].
[2019-03-27 01:26:15,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1076549e-28 1.0000000e+00 1.2216789e-37 0.0000000e+00 2.1552473e-24], sum to 1.0000
[2019-03-27 01:26:15,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9605
[2019-03-27 01:26:15,538] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5508516237762893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769755.305596947, 769755.3055969476, 191947.3824051682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5151600.0000, 
sim time next is 5152200.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5736702554996205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801653.8974371125, 801653.8974371132, 195941.9700258927], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.63, 1.0, 1.0, 0.48634970542122946, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2226816381769757, 0.2226816381769759, 0.29245070153118313], 
reward next is 0.7075, 
noisyNet noise sample is [array([-0.7146968], dtype=float32), -0.77249944]. 
=============================================
[2019-03-27 01:26:17,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5893627e-15 1.0000000e+00 7.8698089e-23 6.5537920e-21 1.4279855e-10], sum to 1.0000
[2019-03-27 01:26:17,394] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9924
[2019-03-27 01:26:17,400] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 75.66666666666666, 1.0, 2.0, 0.7755968226906139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1083972.578505252, 1083972.578505253, 238008.0061239538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5211600.0000, 
sim time next is 5212200.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.787049381784998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1099986.942618178, 1099986.942618178, 240755.897559858], 
processed observation next is [1.0, 0.30434782608695654, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.7434329901024072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3055519285050494, 0.3055519285050494, 0.35933716053710146], 
reward next is 0.6407, 
noisyNet noise sample is [array([0.16045903], dtype=float32), -1.6063296]. 
=============================================
[2019-03-27 01:26:29,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5806383e-22 1.0000000e+00 6.4786901e-31 4.2025932e-33 2.8379788e-20], sum to 1.0000
[2019-03-27 01:26:29,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4631
[2019-03-27 01:26:29,142] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 81.0, 1.0, 2.0, 0.6219566251502986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 869157.4747960069, 869157.4747960076, 204908.6164966634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5427000.0000, 
sim time next is 5427600.0000, 
raw observation next is [30.8, 81.66666666666666, 1.0, 2.0, 0.6272379793066877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876540.9842627418, 876540.9842627423, 205930.944461137], 
processed observation next is [1.0, 0.8260869565217391, 0.6587677725118484, 0.8166666666666665, 1.0, 1.0, 0.5508891316948044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2434836067396505, 0.24348360673965064, 0.3073596185987119], 
reward next is 0.6926, 
noisyNet noise sample is [array([-0.6527495], dtype=float32), -2.4373338]. 
=============================================
[2019-03-27 01:26:29,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4549675e-09 1.6179195e-03 5.8022269e-15 4.2394377e-09 9.9838209e-01], sum to 1.0000
[2019-03-27 01:26:29,228] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1534
[2019-03-27 01:26:29,234] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 71.0, 1.0, 2.0, 0.7401949708272976, 1.0, 2.0, 0.6906875249279115, 1.0, 2.0, 1.03, 7.005100901706097, 6.9112, 170.5573041426782, 2898214.684972556, 2830949.64943601, 534324.4447079507], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5418000.0000, 
sim time next is 5418600.0000, 
raw observation next is [30.9, 71.66666666666667, 1.0, 2.0, 0.2866932553653742, 1.0, 2.0, 0.2866932553653742, 1.0, 2.0, 0.49789137366131, 6.911199999999999, 6.9112, 170.5573041426782, 1202106.048201373, 1202106.048201374, 299040.6021287956], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7166666666666667, 1.0, 1.0, 0.14059428357274, 1.0, 1.0, 0.14059428357274, 1.0, 1.0, 0.38767240690403654, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3339183467226036, 0.3339183467226039, 0.4463292569086501], 
reward next is 0.5537, 
noisyNet noise sample is [array([1.6240289], dtype=float32), 2.6589463]. 
=============================================
[2019-03-27 01:26:33,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5608194e-14 1.0000000e+00 4.4773009e-21 3.9970670e-19 5.3198068e-10], sum to 1.0000
[2019-03-27 01:26:33,605] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7004
[2019-03-27 01:26:33,612] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 88.0, 1.0, 2.0, 0.7928086402714412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1108040.331113781, 1108040.331113781, 242156.5278565023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5463600.0000, 
sim time next is 5464200.0000, 
raw observation next is [28.51666666666667, 87.0, 1.0, 2.0, 0.800142292277075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1118295.341892051, 1118295.341892051, 243949.0979458549], 
processed observation next is [1.0, 0.21739130434782608, 0.5505529225908374, 0.87, 1.0, 1.0, 0.7592075810567168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3106375949700142, 0.3106375949700142, 0.36410313126247], 
reward next is 0.6359, 
noisyNet noise sample is [array([0.72985977], dtype=float32), -0.50976235]. 
=============================================
[2019-03-27 01:26:36,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8722778e-29 1.0000000e+00 2.1029541e-38 0.0000000e+00 3.5064028e-27], sum to 1.0000
[2019-03-27 01:26:36,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5886
[2019-03-27 01:26:36,819] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 87.33333333333334, 1.0, 2.0, 0.5383709707924642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752308.7727386561, 752308.7727386567, 189826.0672063862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5786400.0000, 
sim time next is 5787000.0000, 
raw observation next is [27.15, 87.5, 1.0, 2.0, 0.5386528356464708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752702.7845720698, 752702.7845720698, 189873.4473849325], 
processed observation next is [0.0, 1.0, 0.485781990521327, 0.875, 1.0, 1.0, 0.4441600429475552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20908410682557493, 0.20908410682557493, 0.28339320505213805], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.6280428], dtype=float32), 1.2146691]. 
=============================================
[2019-03-27 01:26:36,833] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.91715]
 [67.95508]
 [67.9686 ]
 [67.90729]
 [67.93105]], R is [[67.93245697]
 [67.96981049]
 [68.00692749]
 [68.04368591]
 [68.07983398]].
[2019-03-27 01:26:37,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4757665e-26 1.0000000e+00 3.3251050e-35 0.0000000e+00 5.6595492e-24], sum to 1.0000
[2019-03-27 01:26:37,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8074
[2019-03-27 01:26:37,161] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.35, 76.33333333333333, 1.0, 2.0, 0.576406859247662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805479.5113056763, 805479.5113056763, 196433.5117151019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5514600.0000, 
sim time next is 5515200.0000, 
raw observation next is [30.1, 78.0, 1.0, 2.0, 0.5793145604886455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809544.3246782325, 809544.3246782325, 196956.0427698605], 
processed observation next is [1.0, 0.8695652173913043, 0.6255924170616115, 0.78, 1.0, 1.0, 0.4931500728778861, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22487342352173126, 0.22487342352173126, 0.2939642429400903], 
reward next is 0.7060, 
noisyNet noise sample is [array([-0.3747515], dtype=float32), -0.9118805]. 
=============================================
[2019-03-27 01:26:39,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8377369e-16 1.0000000e+00 1.7352955e-23 1.3643535e-22 2.7551930e-10], sum to 1.0000
[2019-03-27 01:26:39,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9954
[2019-03-27 01:26:39,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 86.66666666666667, 1.0, 2.0, 0.808371934136521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129803.377829184, 1129803.377829185, 245976.7765173571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5554200.0000, 
sim time next is 5554800.0000, 
raw observation next is [27.7, 86.0, 1.0, 2.0, 0.8365889707277824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1169262.016483936, 1169262.016483935, 253091.0094335143], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.86, 1.0, 1.0, 0.8031192418407016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3247950045788711, 0.32479500457887084, 0.3777477752739019], 
reward next is 0.6223, 
noisyNet noise sample is [array([0.21892108], dtype=float32), 1.3769642]. 
=============================================
[2019-03-27 01:26:48,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2618824e-28 1.0000000e+00 5.1941735e-37 0.0000000e+00 7.4060628e-25], sum to 1.0000
[2019-03-27 01:26:48,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4621
[2019-03-27 01:26:48,225] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 80.0, 1.0, 2.0, 0.5186626171841852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724759.320264166, 724759.3202641654, 186571.6622047571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5727600.0000, 
sim time next is 5728200.0000, 
raw observation next is [27.96666666666667, 79.16666666666667, 1.0, 2.0, 0.5203001505872328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727048.329947903, 727048.3299479038, 186837.6735789017], 
processed observation next is [0.0, 0.30434782608695654, 0.524486571879937, 0.7916666666666667, 1.0, 1.0, 0.4220483742014852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20195786942997307, 0.20195786942997326, 0.2788621993714951], 
reward next is 0.7211, 
noisyNet noise sample is [array([1.1097468], dtype=float32), -0.02409488]. 
=============================================
[2019-03-27 01:26:54,466] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5054430e-13 6.7553524e-12 8.0564976e-23 4.4302431e-12 1.0000000e+00], sum to 1.0000
[2019-03-27 01:26:54,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9135
[2019-03-27 01:26:54,483] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.26666666666667, 76.83333333333333, 1.0, 2.0, 0.4993482221373194, 1.0, 2.0, 0.4993482221373194, 1.0, 2.0, 0.8672027248720221, 6.911199999999999, 6.9112, 170.5573041426782, 2094640.641468414, 2094640.641468415, 414694.9927348305], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6079800.0000, 
sim time next is 6080400.0000, 
raw observation next is [29.4, 76.0, 1.0, 2.0, 0.4617781924925392, 1.0, 2.0, 0.4617781924925392, 1.0, 2.0, 0.8015586650074812, 6.911199999999999, 6.9112, 170.5573041426782, 1936901.316171593, 1936901.316171594, 389655.6168329272], 
processed observation next is [1.0, 0.391304347826087, 0.5924170616113744, 0.76, 1.0, 1.0, 0.35153999095486654, 1.0, 1.0, 0.35153999095486654, 1.0, 1.0, 0.7579983719603428, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.538028143380998, 0.5380281433809984, 0.5815755475118317], 
reward next is 0.4184, 
noisyNet noise sample is [array([1.1511081], dtype=float32), 0.05127514]. 
=============================================
[2019-03-27 01:27:00,078] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.2102342e-12 1.5275976e-08 9.7178626e-19 1.8849920e-13 1.0000000e+00], sum to 1.0000
[2019-03-27 01:27:00,084] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3486
[2019-03-27 01:27:00,089] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.1, 70.0, 1.0, 2.0, 0.6081302072194177, 1.0, 2.0, 0.6081302072194177, 1.0, 2.0, 1.03, 6.940564425218279, 6.9112, 170.5573041426782, 2551439.692818321, 2530404.76056602, 490946.0589134315], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5914800.0000, 
sim time next is 5915400.0000, 
raw observation next is [31.96666666666667, 70.33333333333334, 1.0, 2.0, 0.6274087633336544, 1.0, 2.0, 0.6274087633336544, 1.0, 2.0, 1.03, 6.978204232589385, 6.9112, 170.5573041426782, 2632409.01455475, 2584411.157193054, 498131.3161787001], 
processed observation next is [1.0, 0.4782608695652174, 0.7140600315955767, 0.7033333333333335, 1.0, 1.0, 0.5510948955827161, 1.0, 1.0, 0.5510948955827161, 1.0, 1.0, 1.0365853658536586, 0.006700423258938493, 0.0, 0.8375144448122397, 0.7312247262652084, 0.7178919881091816, 0.7434795763861195], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47550306], dtype=float32), 0.27933612]. 
=============================================
[2019-03-27 01:27:10,834] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 01:27:10,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:27:10,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:27:10,837] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:27:10,838] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:27:10,838] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:27:10,842] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:27:10,843] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:27:10,843] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:27:10,839] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:27:10,848] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:27:10,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-27 01:27:10,865] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-27 01:27:10,906] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-27 01:27:10,929] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-27 01:27:10,946] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-27 01:27:15,796] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.035625603]
[2019-03-27 01:27:15,798] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 66.5, 1.0, 2.0, 0.2601758311856835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426625.2683187338, 426625.2683187338, 161929.2835327267]
[2019-03-27 01:27:15,799] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:27:15,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0778316e-26 1.0000000e+00 2.1101885e-36 0.0000000e+00 2.6720095e-21], sampled 0.9229325765851905
[2019-03-27 01:28:05,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.035625603]
[2019-03-27 01:28:05,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.41573502666667, 77.93851754166667, 1.0, 2.0, 0.5281791429566265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738061.9695641864, 738061.9695641864, 188128.8832831465]
[2019-03-27 01:28:05,420] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:28:05,422] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1618254e-18 1.0000000e+00 4.4797600e-29 2.1953703e-29 1.2520409e-12], sampled 0.876476914159003
[2019-03-27 01:28:14,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.035625603]
[2019-03-27 01:28:14,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.16666666666667, 59.83333333333333, 1.0, 2.0, 0.6964648996900635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 973327.2769809135, 973327.2769809135, 220089.3975259496]
[2019-03-27 01:28:14,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:28:14,084] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7206193e-20 1.0000000e+00 3.5867119e-32 1.7453320e-32 1.1606943e-13], sampled 0.8105639680970347
[2019-03-27 01:28:26,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.035625603]
[2019-03-27 01:28:26,371] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.8712444868087609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217726.216471143, 1217726.216471143, 262147.7242955125]
[2019-03-27 01:28:26,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:28:26,375] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3233484e-16 1.0000000e+00 2.2926979e-26 5.9990739e-26 1.7976707e-09], sampled 0.8474332870112765
[2019-03-27 01:29:03,110] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8585.0095 2930865881.7298 634.0000
[2019-03-27 01:29:03,129] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8860.9473 2786340976.0100 444.0000
[2019-03-27 01:29:03,447] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8531.9124 2992602390.1504 527.0000
[2019-03-27 01:29:03,573] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8262.3165 3153349020.7191 912.0000
[2019-03-27 01:29:03,603] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8772.7583 2840815438.0899 480.0000
[2019-03-27 01:29:04,620] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 800000, evaluation results [800000.0, 8262.316548483763, 3153349020.7190714, 912.0, 8585.009518285655, 2930865881.7297873, 634.0, 8860.947302473503, 2786340976.009976, 444.0, 8531.912439781094, 2992602390.1503563, 527.0, 8772.758305206, 2840815438.0898523, 480.0]
[2019-03-27 01:29:06,234] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5790507e-13 9.9998617e-01 2.7725211e-24 3.1803774e-21 1.3779469e-05], sum to 1.0000
[2019-03-27 01:29:06,242] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2683
[2019-03-27 01:29:06,246] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 86.0, 1.0, 2.0, 0.5269159403329983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736296.1956722644, 736296.1956722644, 187920.8085717417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6123600.0000, 
sim time next is 6124200.0000, 
raw observation next is [27.21666666666667, 86.16666666666667, 1.0, 2.0, 0.5281266562988742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737988.600777555, 737988.6007775556, 188120.4078167687], 
processed observation next is [1.0, 0.9130434782608695, 0.48894154818325447, 0.8616666666666667, 1.0, 1.0, 0.43147789915527013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20499683354932083, 0.204996833549321, 0.2807767280847294], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.34769368], dtype=float32), 0.99876934]. 
=============================================
[2019-03-27 01:29:10,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0751374e-23 1.0000000e+00 3.2063703e-35 2.3449156e-37 2.7621047e-19], sum to 1.0000
[2019-03-27 01:29:10,838] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5636
[2019-03-27 01:29:10,843] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.0, 1.0, 2.0, 0.5248070539752634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733348.285226549, 733348.2852265484, 187574.3951885303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6233400.0000, 
sim time next is 6234000.0000, 
raw observation next is [26.5, 91.0, 1.0, 2.0, 0.525009648198537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733631.4815570363, 733631.4815570363, 187607.6199639468], 
processed observation next is [0.0, 0.13043478260869565, 0.4549763033175356, 0.91, 1.0, 1.0, 0.42772246770908073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2037865226547323, 0.2037865226547323, 0.2800113730805176], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.7867476], dtype=float32), -0.34424776]. 
=============================================
[2019-03-27 01:29:10,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.45658]
 [72.35547]
 [72.30843]
 [72.16473]
 [71.94006]], R is [[72.64624023]
 [72.63981628]
 [72.63346863]
 [72.62715912]
 [72.62097931]].
[2019-03-27 01:29:15,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6368804e-23 1.0000000e+00 5.6711257e-34 1.2144594e-35 1.1586526e-17], sum to 1.0000
[2019-03-27 01:29:15,422] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3863
[2019-03-27 01:29:15,427] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 62.0, 1.0, 2.0, 0.5098208307436327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712400.0027689746, 712400.002768974, 185149.2394375378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6272400.0000, 
sim time next is 6273000.0000, 
raw observation next is [30.8, 62.0, 1.0, 2.0, 0.5088564819395575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711052.015433315, 711052.015433315, 184995.4657704985], 
processed observation next is [0.0, 0.6086956521739131, 0.6587677725118484, 0.62, 1.0, 1.0, 0.40826082161392463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1975144487314764, 0.1975144487314764, 0.27611263547835596], 
reward next is 0.7239, 
noisyNet noise sample is [array([-1.0177995], dtype=float32), -0.4443896]. 
=============================================
[2019-03-27 01:29:15,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.180466]
 [73.19147 ]
 [73.18988 ]
 [73.100914]
 [73.08383 ]], R is [[73.17887878]
 [73.17074585]
 [73.16233063]
 [73.1537323 ]
 [73.1446991 ]].
[2019-03-27 01:29:15,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9785998e-23 1.0000000e+00 5.5656145e-35 3.6045081e-36 5.7237927e-17], sum to 1.0000
[2019-03-27 01:29:15,699] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5754
[2019-03-27 01:29:15,710] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.25, 64.0, 1.0, 2.0, 0.5027214741685746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702476.4107185948, 702476.4107185954, 184023.7529082939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6282600.0000, 
sim time next is 6283200.0000, 
raw observation next is [30.1, 65.0, 1.0, 2.0, 0.5048420875278511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705440.6276468119, 705440.6276468113, 184358.3480066001], 
processed observation next is [0.0, 0.7391304347826086, 0.6255924170616115, 0.65, 1.0, 1.0, 0.4034242018407844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1959557299018922, 0.19595572990189203, 0.27516171344268675], 
reward next is 0.7248, 
noisyNet noise sample is [array([0.41558766], dtype=float32), -0.56423104]. 
=============================================
[2019-03-27 01:29:17,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5625752e-25 1.0000000e+00 1.2237937e-34 9.0761541e-37 5.9625913e-19], sum to 1.0000
[2019-03-27 01:29:17,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8454
[2019-03-27 01:29:17,227] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.66666666666667, 1.0, 2.0, 0.5220100882952503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729438.5545294686, 729438.5545294686, 187116.4846361777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6333600.0000, 
sim time next is 6334200.0000, 
raw observation next is [28.1, 79.0, 1.0, 2.0, 0.5217524332401644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729078.3928511643, 729078.3928511643, 187074.4467776832], 
processed observation next is [0.0, 0.30434782608695654, 0.5308056872037916, 0.79, 1.0, 1.0, 0.42379811233754744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2025217757919901, 0.2025217757919901, 0.2792155922054973], 
reward next is 0.7208, 
noisyNet noise sample is [array([-1.1151373], dtype=float32), -0.90125704]. 
=============================================
[2019-03-27 01:29:18,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5814855e-21 1.0000000e+00 5.7301065e-32 8.2408702e-34 1.1500756e-15], sum to 1.0000
[2019-03-27 01:29:18,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3792
[2019-03-27 01:29:18,538] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.83333333333334, 1.0, 2.0, 0.5321450761777206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743605.7875573632, 743605.7875573632, 188785.7250457925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6310200.0000, 
sim time next is 6310800.0000, 
raw observation next is [27.3, 86.0, 1.0, 2.0, 0.5333052148696199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745227.5040839148, 745227.5040839148, 188978.7343462476], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 1.0, 1.0, 0.4377171263489396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20700764002330965, 0.20700764002330965, 0.28205781245708594], 
reward next is 0.7179, 
noisyNet noise sample is [array([-1.5982199], dtype=float32), -0.4385907]. 
=============================================
[2019-03-27 01:29:22,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6611719e-21 1.0000000e+00 3.0629996e-33 2.1579923e-34 2.7581591e-17], sum to 1.0000
[2019-03-27 01:29:22,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4979
[2019-03-27 01:29:22,021] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.45, 82.0, 1.0, 2.0, 0.5227449343679943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730465.7556748218, 730465.7556748218, 187235.9598224992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6384600.0000, 
sim time next is 6385200.0000, 
raw observation next is [27.43333333333333, 82.0, 1.0, 2.0, 0.5214930887779144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728715.8697678195, 728715.8697678202, 187031.657905314], 
processed observation next is [0.0, 0.9130434782608695, 0.49921011058451803, 0.82, 1.0, 1.0, 0.4234856491300174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20242107493550543, 0.20242107493550562, 0.27915172821688655], 
reward next is 0.7208, 
noisyNet noise sample is [array([-1.0348114], dtype=float32), -1.4145615]. 
=============================================
[2019-03-27 01:29:32,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2373285e-22 1.0000000e+00 2.2443700e-32 8.7781179e-34 8.0434407e-17], sum to 1.0000
[2019-03-27 01:29:32,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5196
[2019-03-27 01:29:32,596] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.95, 68.5, 1.0, 2.0, 0.4841375757129702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676499.9847174002, 676499.9847174002, 181148.3032504881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6546600.0000, 
sim time next is 6547200.0000, 
raw observation next is [28.83333333333334, 69.0, 1.0, 2.0, 0.4826671817542217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 674444.7068110128, 674444.7068110134, 180925.1786570863], 
processed observation next is [1.0, 0.782608695652174, 0.5655608214849924, 0.69, 1.0, 1.0, 0.37670744789665267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.187345751891948, 0.18734575189194816, 0.27003758008520345], 
reward next is 0.7300, 
noisyNet noise sample is [array([1.6718534], dtype=float32), 0.10642202]. 
=============================================
[2019-03-27 01:29:33,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1574541e-19 1.0000000e+00 1.7207411e-30 4.0898988e-29 2.0258550e-11], sum to 1.0000
[2019-03-27 01:29:33,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6063
[2019-03-27 01:29:33,210] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 73.0, 1.0, 2.0, 0.4830780110132218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675018.9527168088, 675018.9527168088, 180987.1319754755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6552000.0000, 
sim time next is 6552600.0000, 
raw observation next is [27.95, 73.66666666666667, 1.0, 2.0, 0.4850318140300224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677749.9294035789, 677749.9294035796, 181284.0457140137], 
processed observation next is [1.0, 0.8695652173913043, 0.523696682464455, 0.7366666666666667, 1.0, 1.0, 0.37955640244581007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1882638692787719, 0.1882638692787721, 0.27057320255822936], 
reward next is 0.7294, 
noisyNet noise sample is [array([-0.41006416], dtype=float32), 1.9076879]. 
=============================================
[2019-03-27 01:29:37,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8742262e-17 1.0000000e+00 1.3792633e-28 2.3438721e-27 2.8870817e-10], sum to 1.0000
[2019-03-27 01:29:37,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4847
[2019-03-27 01:29:37,966] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.81666666666667, 95.0, 1.0, 2.0, 0.5489271738837114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767065.12491931, 767065.12491931, 191612.6361002953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6666600.0000, 
sim time next is 6667200.0000, 
raw observation next is [24.8, 95.0, 1.0, 2.0, 0.54289478418599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758632.516046979, 758632.5160469783, 190585.7856484646], 
processed observation next is [1.0, 0.17391304347826086, 0.3744075829383887, 0.95, 1.0, 1.0, 0.4492708243204699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21073125445749416, 0.21073125445749397, 0.2844563964902457], 
reward next is 0.7155, 
noisyNet noise sample is [array([-1.2053689], dtype=float32), -0.6455622]. 
=============================================
[2019-03-27 01:29:39,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5331767e-19 1.0000000e+00 3.0168630e-30 9.0029014e-32 7.9491866e-13], sum to 1.0000
[2019-03-27 01:29:39,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6675
[2019-03-27 01:29:39,766] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 67.0, 1.0, 2.0, 0.4582745650120442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648585.6294542779, 648585.6294542785, 178379.5010824531], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6719400.0000, 
sim time next is 6720000.0000, 
raw observation next is [28.26666666666667, 67.0, 1.0, 2.0, 0.4522053942050879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643380.8895357632, 643380.8895357632, 177930.5097580252], 
processed observation next is [1.0, 0.782608695652174, 0.53870458135861, 0.67, 1.0, 1.0, 0.34000649904227453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17871691375993423, 0.17871691375993423, 0.2655679250119779], 
reward next is 0.7344, 
noisyNet noise sample is [array([1.5234367], dtype=float32), 0.72853196]. 
=============================================
[2019-03-27 01:29:39,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.86631 ]
 [64.67603 ]
 [64.73556 ]
 [63.988518]
 [64.121796]], R is [[65.01898956]
 [65.10256195]
 [65.18476105]
 [65.26586151]
 [65.34624481]].
[2019-03-27 01:29:52,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7785128e-22 1.0000000e+00 1.0745789e-32 1.0473140e-33 4.3830677e-15], sum to 1.0000
[2019-03-27 01:29:52,390] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9988
[2019-03-27 01:29:52,395] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 79.66666666666667, 1.0, 2.0, 0.4163573938097708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611451.8569809191, 611451.8569809191, 175329.2391634624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6909600.0000, 
sim time next is 6910200.0000, 
raw observation next is [25.35, 80.0, 1.0, 2.0, 0.4167240385449648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612002.4918850659, 612002.4918850666, 175381.8049249348], 
processed observation next is [0.0, 1.0, 0.4004739336492892, 0.8, 1.0, 1.0, 0.29725787776501783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1700006921902961, 0.1700006921902963, 0.2617638879476639], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.5471397], dtype=float32), -0.35018963]. 
=============================================
[2019-03-27 01:29:56,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5035759e-24 1.0000000e+00 8.2336427e-34 8.8577632e-36 3.8300386e-18], sum to 1.0000
[2019-03-27 01:29:56,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2796
[2019-03-27 01:29:56,895] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 53.66666666666666, 1.0, 2.0, 0.4566937806234246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645723.409455838, 645723.4094558386, 178068.5655310678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6955800.0000, 
sim time next is 6956400.0000, 
raw observation next is [31.33333333333334, 53.33333333333334, 1.0, 2.0, 0.4607192371704784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649184.6980748904, 649184.6980748904, 178370.7886708213], 
processed observation next is [0.0, 0.5217391304347826, 0.6840442338072673, 0.5333333333333334, 1.0, 1.0, 0.35026414116925114, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18032908279858068, 0.18032908279858068, 0.2662250577176437], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.94777596], dtype=float32), -1.3490835]. 
=============================================
[2019-03-27 01:30:00,478] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 01:30:00,481] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:30:00,482] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:30:00,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:30:00,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:30:00,484] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:30:00,485] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:30:00,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:30:00,488] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:30:00,490] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:30:00,494] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:30:00,512] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-27 01:30:00,533] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-27 01:30:00,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-27 01:30:00,575] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-27 01:30:00,575] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-27 01:30:59,186] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.036181595]
[2019-03-27 01:30:59,188] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.268460865, 61.138723775, 1.0, 2.0, 0.8053756239930595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1125613.430779158, 1125613.430779157, 245235.9030754314]
[2019-03-27 01:30:59,189] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:30:59,192] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3327797e-14 9.9999893e-01 9.1624725e-25 4.2093167e-23 1.0867354e-06], sampled 0.6651327653848175
[2019-03-27 01:31:10,474] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.036181595]
[2019-03-27 01:31:10,475] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.73494725, 74.895281565, 1.0, 2.0, 0.535498440311725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748293.3425209662, 748293.3425209656, 189343.7791501515]
[2019-03-27 01:31:10,476] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:31:10,483] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7305634e-17 1.0000000e+00 5.3462761e-28 1.2724952e-27 1.4924682e-10], sampled 0.18827793407958615
[2019-03-27 01:31:44,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.036181595]
[2019-03-27 01:31:44,870] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.532833595, 90.749898495, 1.0, 2.0, 0.3738423605643421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571778.4401103669, 571778.4401103675, 172398.6542205903]
[2019-03-27 01:31:44,871] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:31:44,874] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4756940e-19 1.0000000e+00 1.3096758e-29 3.1126284e-30 2.8644841e-13], sampled 0.29933802989927183
[2019-03-27 01:31:54,609] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8866.5734 2787508789.7392 424.0000
[2019-03-27 01:31:55,097] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8280.9542 3153074719.3572 876.0000
[2019-03-27 01:31:55,131] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8567.2548 2993196189.6538 456.0000
[2019-03-27 01:31:55,147] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8788.6788 2842225738.5525 449.0000
[2019-03-27 01:31:55,154] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8589.4727 2932798873.0167 611.0000
[2019-03-27 01:31:56,169] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 825000, evaluation results [825000.0, 8280.954164936253, 3153074719.3572445, 876.0, 8589.472671712601, 2932798873.016743, 611.0, 8866.573357034364, 2787508789.7392483, 424.0, 8567.254757919161, 2993196189.653838, 456.0, 8788.678776329334, 2842225738.5525465, 449.0]
[2019-03-27 01:31:57,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4404668e-15 4.5765098e-08 1.1909121e-25 8.7882990e-20 1.0000000e+00], sum to 1.0000
[2019-03-27 01:31:57,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8135
[2019-03-27 01:31:57,588] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 56.5, 1.0, 2.0, 0.4834099076846651, 1.0, 2.0, 0.4834099076846651, 1.0, 2.0, 0.8099084117068944, 6.911200000000001, 6.9112, 170.5573041426782, 2027720.131633014, 2027720.131633013, 398563.5770330027], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7054200.0000, 
sim time next is 7054800.0000, 
raw observation next is [30.1, 58.0, 1.0, 2.0, 0.5090899113840547, 1.0, 2.0, 0.5090899113840547, 1.0, 2.0, 0.8544183054802984, 6.9112, 6.9112, 170.5573041426782, 2135545.319533319, 2135545.319533319, 415869.1705433652], 
processed observation next is [1.0, 0.6521739130434783, 0.6255924170616115, 0.58, 1.0, 1.0, 0.40854206190849957, 1.0, 1.0, 0.40854206190849957, 1.0, 1.0, 0.8224613481467052, 0.0, 0.0, 0.8375144448122397, 0.5932070332036998, 0.5932070332036998, 0.6207002545423361], 
reward next is 0.3793, 
noisyNet noise sample is [array([0.38437334], dtype=float32), -1.4519557]. 
=============================================
[2019-03-27 01:32:04,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1238047e-13 1.7562177e-10 1.6276427e-23 4.4511230e-15 1.0000000e+00], sum to 1.0000
[2019-03-27 01:32:04,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5440
[2019-03-27 01:32:04,344] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.456284973255303, 1.0, 2.0, 0.456284973255303, 1.0, 2.0, 0.7924161027979836, 6.9112, 6.9112, 170.5573041426782, 1913839.753747045, 1913839.753747045, 386233.2931682056], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7203600.0000, 
sim time next is 7204200.0000, 
raw observation next is [29.0, 83.16666666666667, 1.0, 2.0, 0.4843753331770951, 1.0, 2.0, 0.4843753331770951, 1.0, 2.0, 0.8411997683581567, 6.9112, 6.9112, 170.5573041426782, 2031773.56342269, 2031773.56342269, 404509.5285071398], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.8316666666666667, 1.0, 1.0, 0.37876546165915076, 1.0, 1.0, 0.37876546165915076, 1.0, 1.0, 0.8063411809245813, 0.0, 0.0, 0.8375144448122397, 0.5643815453951917, 0.5643815453951917, 0.6037455649360296], 
reward next is 0.3963, 
noisyNet noise sample is [array([-1.602955], dtype=float32), 0.8365976]. 
=============================================
[2019-03-27 01:32:08,552] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5304134e-21 1.0000000e+00 3.5132085e-32 5.9232414e-32 1.6171946e-13], sum to 1.0000
[2019-03-27 01:32:08,560] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9096
[2019-03-27 01:32:08,566] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 87.5, 1.0, 2.0, 0.817326810551353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1207507.023581168, 1207507.023581169, 257229.4271715355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7227000.0000, 
sim time next is 7227600.0000, 
raw observation next is [24.13333333333333, 87.0, 1.0, 2.0, 0.8823018199058289, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1305562.168384604, 1305562.168384604, 275469.1406428656], 
processed observation next is [1.0, 0.6521739130434783, 0.3428120063191152, 0.87, 1.0, 1.0, 0.8581949637419625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3626561578846122, 0.3626561578846122, 0.4111479711087547], 
reward next is 0.5889, 
noisyNet noise sample is [array([-0.6930695], dtype=float32), 1.1621102]. 
=============================================
[2019-03-27 01:32:14,788] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0735342e-17 1.0000000e+00 5.5310658e-30 3.5839529e-30 2.3223519e-12], sum to 1.0000
[2019-03-27 01:32:14,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5856
[2019-03-27 01:32:14,804] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333333, 78.0, 1.0, 2.0, 0.4173523204465318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638088.4572700417, 638088.4572700424, 178426.5306008872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7355400.0000, 
sim time next is 7356000.0000, 
raw observation next is [24.36666666666667, 79.0, 1.0, 2.0, 0.3781661619642417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576827.8756952665, 576827.8756952658, 172800.6040757502], 
processed observation next is [1.0, 0.13043478260869565, 0.3538704581358612, 0.79, 1.0, 1.0, 0.2508026047761948, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16022996547090737, 0.16022996547090718, 0.25791134936679133], 
reward next is 0.7421, 
noisyNet noise sample is [array([1.0441498], dtype=float32), 0.27713048]. 
=============================================
[2019-03-27 01:32:14,818] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.70804 ]
 [68.812546]
 [68.72528 ]
 [68.81997 ]
 [69.0848  ]], R is [[68.84844208]
 [68.89364624]
 [68.94309998]
 [68.98830414]
 [69.03230286]].
[2019-03-27 01:32:22,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0064641e-15 9.9999797e-01 5.2613941e-27 3.9451618e-26 2.0142913e-06], sum to 1.0000
[2019-03-27 01:32:22,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8384
[2019-03-27 01:32:22,888] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 92.33333333333333, 1.0, 2.0, 0.5029131377737938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723328.4567047626, 723328.4567047626, 186671.3806329673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7627800.0000, 
sim time next is 7628400.0000, 
raw observation next is [24.3, 92.0, 1.0, 2.0, 0.4815476531394288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690983.061003379, 690983.061003379, 183064.8518688829], 
processed observation next is [1.0, 0.30434782608695654, 0.3507109004739337, 0.92, 1.0, 1.0, 0.37535861824027567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1919397391676053, 0.1919397391676053, 0.2732311221923625], 
reward next is 0.7268, 
noisyNet noise sample is [array([2.4861345], dtype=float32), -0.8414316]. 
=============================================
[2019-03-27 01:32:23,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1054724e-22 1.0000000e+00 1.8649464e-33 3.1011936e-36 1.9155438e-15], sum to 1.0000
[2019-03-27 01:32:23,298] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9150
[2019-03-27 01:32:23,303] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 84.0, 1.0, 2.0, 0.3879838743994324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579066.6838532434, 579066.6838532427, 172627.6638041803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7474800.0000, 
sim time next is 7475400.0000, 
raw observation next is [24.45, 83.5, 1.0, 2.0, 0.3895191598809441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580439.909540628, 580439.9095406285, 172722.3176108013], 
processed observation next is [0.0, 0.5217391304347826, 0.3578199052132702, 0.835, 1.0, 1.0, 0.26448091551920977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16123330820572998, 0.16123330820573015, 0.2577945038967184], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.5006773], dtype=float32), 2.0947425]. 
=============================================
[2019-03-27 01:32:27,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.78188725e-12 2.22810339e-02 1.04405444e-20 5.13953354e-17
 9.77719009e-01], sum to 1.0000
[2019-03-27 01:32:27,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7107
[2019-03-27 01:32:27,398] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 60.0, 1.0, 2.0, 0.3124104834123229, 1.0, 2.0, 0.3124104834123229, 1.0, 2.0, 0.5272132911476717, 6.911199999999999, 6.9112, 170.5573041426782, 1310004.378637604, 1310004.378637604, 308081.3043605022], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7650000.0000, 
sim time next is 7650600.0000, 
raw observation next is [30.58333333333334, 59.66666666666666, 1.0, 2.0, 0.3287869561495443, 1.0, 2.0, 0.3287869561495443, 1.0, 2.0, 0.5549495581146381, 6.9112, 6.9112, 170.5573041426782, 1378718.6126977, 1378718.6126977, 315161.8463139408], 
processed observation next is [1.0, 0.5652173913043478, 0.6484992101105849, 0.5966666666666666, 1.0, 1.0, 0.1913095857223425, 1.0, 1.0, 0.1913095857223425, 1.0, 1.0, 0.4572555586763879, 0.0, 0.0, 0.8375144448122397, 0.3829773924160278, 0.3829773924160278, 0.4703908153939415], 
reward next is 0.5296, 
noisyNet noise sample is [array([-0.17660703], dtype=float32), 0.08450024]. 
=============================================
[2019-03-27 01:32:27,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:27,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:27,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-27 01:32:27,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5472857e-21 1.0000000e+00 6.2457189e-33 1.1121112e-34 3.3335066e-16], sum to 1.0000
[2019-03-27 01:32:27,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7092
[2019-03-27 01:32:27,625] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 62.33333333333333, 1.0, 2.0, 0.4392080589186769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628318.064137571, 628318.0641375717, 176505.2226159334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7564200.0000, 
sim time next is 7564800.0000, 
raw observation next is [29.0, 61.66666666666667, 1.0, 2.0, 0.4348498349056556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625057.65067362, 625057.65067362, 176266.2403404828], 
processed observation next is [0.0, 0.5652173913043478, 0.5734597156398105, 0.6166666666666667, 1.0, 1.0, 0.31909618663332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17362712518711668, 0.17362712518711668, 0.26308394080669073], 
reward next is 0.7369, 
noisyNet noise sample is [array([1.1558037], dtype=float32), 0.40622145]. 
=============================================
[2019-03-27 01:32:29,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.24361188e-12 9.99985576e-01 1.19410775e-23 2.78874347e-20
 1.44652031e-05], sum to 1.0000
[2019-03-27 01:32:29,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8735
[2019-03-27 01:32:29,353] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 93.83333333333334, 1.0, 2.0, 0.4595207864637266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650580.9040963266, 650580.904096326, 178591.7952665128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7606200.0000, 
sim time next is 7606800.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4587716862443483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650328.9176731273, 650328.9176731273, 178585.5180182038], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3479176942702991, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18064692157586867, 0.18064692157586867, 0.2665455492809012], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.5262236], dtype=float32), -0.10849448]. 
=============================================
[2019-03-27 01:32:30,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9377170e-15 1.0000000e+00 2.1800348e-26 3.8048631e-24 3.3885737e-08], sum to 1.0000
[2019-03-27 01:32:30,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2740
[2019-03-27 01:32:30,533] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 93.66666666666667, 1.0, 2.0, 0.4700282567033772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682312.3596196418, 682312.3596196418, 182274.0844446681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7625400.0000, 
sim time next is 7626000.0000, 
raw observation next is [23.9, 93.33333333333334, 1.0, 2.0, 0.4493217893935383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650747.1850006422, 650747.1850006428, 178962.3134321063], 
processed observation next is [1.0, 0.2608695652173913, 0.33175355450236965, 0.9333333333333335, 1.0, 1.0, 0.33653227637775696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18076310694462283, 0.180763106944623, 0.26710793049568105], 
reward next is 0.7329, 
noisyNet noise sample is [array([2.337271], dtype=float32), 1.3111353]. 
=============================================
[2019-03-27 01:32:30,548] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[65.7403 ]
 [65.92623]
 [65.85081]
 [65.82966]
 [65.69738]], R is [[65.8998642 ]
 [65.96881866]
 [66.04689789]
 [66.12462616]
 [66.20206451]].
[2019-03-27 01:32:31,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0086587e-14 3.2335641e-13 4.5346744e-22 1.0415828e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 01:32:31,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6721
[2019-03-27 01:32:31,157] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.2, 76.5, 1.0, 2.0, 0.3530290402590796, 1.0, 2.0, 0.3530290402590796, 1.0, 2.0, 0.5936548375791205, 6.911199999999999, 6.9112, 170.5573041426782, 1480444.344818838, 1480444.344818839, 326005.5360425235], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7637400.0000, 
sim time next is 7638000.0000, 
raw observation next is [27.46666666666667, 75.0, 1.0, 2.0, 0.3755379783373279, 1.0, 2.0, 0.3755379783373279, 1.0, 2.0, 0.6319451546265098, 6.9112, 6.9112, 170.5573041426782, 1574906.015678871, 1574906.015678871, 337065.1468573133], 
processed observation next is [1.0, 0.391304347826087, 0.500789889415482, 0.75, 1.0, 1.0, 0.24763611847870828, 1.0, 1.0, 0.24763611847870828, 1.0, 1.0, 0.5511526275933046, 0.0, 0.0, 0.8375144448122397, 0.43747389324413083, 0.43747389324413083, 0.5030823087422587], 
reward next is 0.4969, 
noisyNet noise sample is [array([0.06441951], dtype=float32), -0.37421358]. 
=============================================
[2019-03-27 01:32:31,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.82504 ]
 [55.451214]
 [56.20749 ]
 [56.835125]
 [58.623352]], R is [[54.21086502]
 [54.18218231]
 [54.15657806]
 [54.12601089]
 [54.09315872]].
[2019-03-27 01:32:35,743] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:35,744] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:35,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-27 01:32:37,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2815161e-17 1.0000000e+00 3.8421378e-27 3.9515992e-27 6.8785234e-11], sum to 1.0000
[2019-03-27 01:32:37,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3563
[2019-03-27 01:32:37,960] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 86.0, 1.0, 2.0, 0.3538382837698558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566489.6619203629, 566489.6619203629, 172316.7511963779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 18600.0000, 
sim time next is 19200.0000, 
raw observation next is [21.46666666666667, 86.0, 1.0, 2.0, 0.3241420795069231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518568.2885860677, 518568.2885860677, 168497.6843657705], 
processed observation next is [1.0, 0.21739130434782608, 0.21642969984202226, 0.86, 1.0, 1.0, 0.18571334880352178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14404674682946325, 0.14404674682946325, 0.25148908114294105], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.34649467], dtype=float32), -0.7386997]. 
=============================================
[2019-03-27 01:32:42,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:42,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:42,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-27 01:32:45,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:45,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:45,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-27 01:32:47,306] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6083197e-21 1.0000000e+00 1.4164525e-31 1.2139656e-32 9.0100525e-16], sum to 1.0000
[2019-03-27 01:32:47,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7625
[2019-03-27 01:32:47,316] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 86.0, 1.0, 2.0, 0.3538089022153792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546752.3518603382, 546752.3518603382, 170422.9375261928], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 77400.0000, 
sim time next is 78000.0000, 
raw observation next is [22.83333333333334, 86.33333333333334, 1.0, 2.0, 0.3523763629921491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 544875.285667902, 544875.2856679014, 170276.4894332824], 
processed observation next is [1.0, 0.9130434782608695, 0.2812006319115327, 0.8633333333333334, 1.0, 1.0, 0.21973055782186637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15135424601886166, 0.1513542460188615, 0.25414401407952597], 
reward next is 0.7459, 
noisyNet noise sample is [array([0.06017189], dtype=float32), -1.0734948]. 
=============================================
[2019-03-27 01:32:47,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.25548 ]
 [69.00922 ]
 [68.913414]
 [68.669716]
 [68.44588 ]], R is [[69.44661713]
 [69.49778748]
 [69.54814148]
 [69.59764862]
 [69.64656067]].
[2019-03-27 01:32:47,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:47,899] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:47,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-27 01:32:47,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:47,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:47,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-27 01:32:49,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3458812e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5949484e-25], sum to 1.0000
[2019-03-27 01:32:49,069] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7295
[2019-03-27 01:32:49,074] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 90.16666666666667, 1.0, 2.0, 0.2918847654377892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467566.0319162112, 467566.0319162112, 164780.12513733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 249000.0000, 
sim time next is 249600.0000, 
raw observation next is [20.83333333333333, 90.33333333333334, 1.0, 2.0, 0.2896693116080126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464138.3264318929, 464138.3264318929, 164544.0127841447], 
processed observation next is [0.0, 0.9130434782608695, 0.1864139020537123, 0.9033333333333334, 1.0, 1.0, 0.14417989350362967, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12892731289774803, 0.12892731289774803, 0.2455880787823055], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.4388088], dtype=float32), 1.2587898]. 
=============================================
[2019-03-27 01:32:49,101] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,102] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-27 01:32:49,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-27 01:32:49,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-27 01:32:49,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-27 01:32:49,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,615] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-27 01:32:49,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,655] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-27 01:32:49,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-27 01:32:49,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-27 01:32:49,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:49,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:49,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-27 01:32:50,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:32:50,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-27 01:32:50,495] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 01:32:50,500] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:32:50,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:32:50,502] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,502] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:32:50,503] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,503] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:32:50,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:32:50,503] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,503] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:32:50,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-27 01:32:50,523] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-27 01:32:50,542] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-27 01:32:50,564] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-27 01:32:50,602] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-27 01:33:01,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:33:01,224] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.23088722333333, 59.00416324333334, 1.0, 2.0, 0.2155390971439656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 359430.8471830676, 359430.8471830682, 157090.9957404532]
[2019-03-27 01:33:01,225] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:33:01,227] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.8296623e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1478979e-25], sampled 0.08742358705883957
[2019-03-27 01:33:24,653] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:33:24,654] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 92.0, 1.0, 2.0, 0.3914157689821272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587087.3592232054, 587087.3592232054, 173442.6384506016]
[2019-03-27 01:33:24,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:33:24,661] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6770586e-26 1.0000000e+00 1.8106776e-36 0.0000000e+00 1.3935157e-21], sampled 0.43499516796958426
[2019-03-27 01:33:30,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:33:30,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.93333333333333, 80.16666666666667, 1.0, 2.0, 0.5588547376111236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780942.898959829, 780942.8989598296, 193330.9081267355]
[2019-03-27 01:33:30,158] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:33:30,161] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.03559795e-26 1.00000000e+00 8.13711553e-37 0.00000000e+00
 2.47529810e-22], sampled 0.2291718876208405
[2019-03-27 01:33:46,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:33:46,246] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.76666666666667, 56.33333333333334, 1.0, 2.0, 1.031306061549943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1441593.929151647, 1441593.929151647, 308612.0234437712]
[2019-03-27 01:33:46,247] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:33:46,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9050846e-17 1.0000000e+00 7.2209257e-28 2.9755630e-26 1.3342323e-09], sampled 0.5068102560258159
[2019-03-27 01:34:29,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:34:29,383] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.6, 48.0, 1.0, 2.0, 0.5817098925905739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812892.8846337334, 812892.8846337334, 197388.6724922296]
[2019-03-27 01:34:29,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:34:29,387] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1990986e-26 1.0000000e+00 3.1012883e-36 0.0000000e+00 1.4995135e-21], sampled 0.7148391528544107
[2019-03-27 01:34:41,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.044915356]
[2019-03-27 01:34:41,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 86.33333333333334, 1.0, 2.0, 0.5206336275893874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727514.4780434177, 727514.4780434177, 186892.2380089225]
[2019-03-27 01:34:41,848] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:34:41,852] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9213384e-24 1.0000000e+00 2.5251521e-35 1.3305544e-36 2.6279487e-19], sampled 0.029565826541551155
[2019-03-27 01:34:44,459] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8010.7358 3158879771.4633 1506.0000
[2019-03-27 01:34:44,834] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8739.8025 2778990786.9225 750.0000
[2019-03-27 01:34:44,994] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8162.6603 3002071556.7333 1381.0000
[2019-03-27 01:34:45,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8604.6972 2839022003.8790 887.0000
[2019-03-27 01:34:45,088] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8358.9571 2927324318.4610 1127.0000
[2019-03-27 01:34:46,105] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 850000, evaluation results [850000.0, 8010.735830817663, 3158879771.4633336, 1506.0, 8358.957109942776, 2927324318.461037, 1127.0, 8739.802501199661, 2778990786.9224796, 750.0, 8162.660264769797, 3002071556.733322, 1381.0, 8604.697170737822, 2839022003.879024, 887.0]
[2019-03-27 01:34:49,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7478319e-25 1.0000000e+00 2.8091006e-36 2.6900682e-38 4.8138642e-20], sum to 1.0000
[2019-03-27 01:34:49,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3398
[2019-03-27 01:34:49,471] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 93.0, 1.0, 2.0, 0.2955235204301194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472817.2235251571, 472817.2235251571, 165141.3872876969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 207000.0000, 
sim time next is 207600.0000, 
raw observation next is [20.63333333333333, 93.0, 1.0, 2.0, 0.2963497847265903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473810.7724524142, 473810.7724524142, 165207.7891244677], 
processed observation next is [0.0, 0.391304347826087, 0.17693522906793036, 0.93, 1.0, 1.0, 0.15222865629709673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13161410345900396, 0.13161410345900396, 0.2465787897380115], 
reward next is 0.7534, 
noisyNet noise sample is [array([0.8931174], dtype=float32), -1.5059419]. 
=============================================
[2019-03-27 01:34:52,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2025076e-13 9.9172020e-01 1.3285970e-24 5.9205743e-21 8.2798004e-03], sum to 1.0000
[2019-03-27 01:34:52,585] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0865
[2019-03-27 01:34:52,590] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.0, 1.0, 2.0, 0.734012973731504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1108080.704503797, 1108080.704503796, 239083.8775810624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [22.9, 92.33333333333334, 1.0, 2.0, 0.7425352020826143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1119812.87522061, 1119812.87522061, 241048.7627609518], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9233333333333335, 1.0, 1.0, 0.6898014482923063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.311059132005725, 0.311059132005725, 0.35977427277754], 
reward next is 0.6402, 
noisyNet noise sample is [array([-0.69837624], dtype=float32), 0.32854223]. 
=============================================
[2019-03-27 01:34:55,780] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0411126e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8129754e-23], sum to 1.0000
[2019-03-27 01:34:55,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1307
[2019-03-27 01:34:55,791] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 87.0, 1.0, 2.0, 0.3115171818356229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493323.9484018233, 493323.9484018227, 166543.1612208481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 230400.0000, 
sim time next is 231000.0000, 
raw observation next is [21.76666666666667, 87.0, 1.0, 2.0, 0.310256711402336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491622.1101814056, 491622.1101814062, 166423.4228649743], 
processed observation next is [0.0, 0.6956521739130435, 0.23064770932069528, 0.87, 1.0, 1.0, 0.16898398964136868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13656169727261266, 0.13656169727261283, 0.24839316845518553], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.724365], dtype=float32), -0.3477565]. 
=============================================
[2019-03-27 01:34:55,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.261604]
 [79.21228 ]
 [79.225784]
 [79.24113 ]
 [79.242874]], R is [[79.19713593]
 [79.15659332]
 [79.11631775]
 [79.07633972]
 [79.03672028]].
[2019-03-27 01:34:57,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1343528e-19 1.0000000e+00 7.6669734e-31 7.1911678e-32 6.4022988e-11], sum to 1.0000
[2019-03-27 01:34:57,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8246
[2019-03-27 01:34:57,954] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.9, 87.0, 1.0, 2.0, 0.2340918634516594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 388019.8986728564, 388019.898672857, 159212.6025994365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [18.88333333333333, 87.0, 1.0, 2.0, 0.2338091448563937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 387618.3220044767, 387618.3220044767, 159178.8085734092], 
processed observation next is [1.0, 1.0, 0.09399684044233794, 0.87, 1.0, 1.0, 0.07687848777878759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10767175611235463, 0.10767175611235463, 0.2375803113035958], 
reward next is 0.7624, 
noisyNet noise sample is [array([-0.21349868], dtype=float32), 0.80144954]. 
=============================================
[2019-03-27 01:35:00,625] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.9293061e-24 1.0000000e+00 4.1229209e-36 2.3831876e-37 1.4392352e-17], sum to 1.0000
[2019-03-27 01:35:00,635] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7448
[2019-03-27 01:35:00,640] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 89.0, 1.0, 2.0, 0.2996505036284212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477729.2055036015, 477729.2055036021, 165467.9946676845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 242400.0000, 
sim time next is 243000.0000, 
raw observation next is [21.2, 89.0, 1.0, 2.0, 0.2985970192955962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476385.0334150187, 476385.0334150187, 165377.1718629935], 
processed observation next is [0.0, 0.8260869565217391, 0.20379146919431282, 0.89, 1.0, 1.0, 0.1549361678260195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13232917594861632, 0.13232917594861632, 0.2468315997955127], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.6911557], dtype=float32), -0.43760085]. 
=============================================
[2019-03-27 01:35:00,660] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[77.46338 ]
 [77.477036]
 [77.49571 ]
 [77.44234 ]
 [77.447586]], R is [[77.4500885 ]
 [77.42862701]
 [77.40732574]
 [77.38619232]
 [77.36524963]].
[2019-03-27 01:35:06,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4082673e-17 9.9999976e-01 5.1729955e-29 7.0130038e-28 2.1029507e-07], sum to 1.0000
[2019-03-27 01:35:06,015] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0728
[2019-03-27 01:35:06,020] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.15, 80.5, 1.0, 2.0, 0.5874190559732858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940185.5759398754, 940185.5759398754, 211811.7673065752], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [22.1, 81.0, 1.0, 2.0, 0.5941626311677893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 950651.4846372332, 950651.4846372338, 213200.4507746677], 
processed observation next is [1.0, 0.6956521739130435, 0.24644549763033188, 0.81, 1.0, 1.0, 0.5110393146599871, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2640698568436759, 0.26406985684367607, 0.31820962802189207], 
reward next is 0.6818, 
noisyNet noise sample is [array([0.37944806], dtype=float32), 2.9305959]. 
=============================================
[2019-03-27 01:35:08,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.03227015e-24 1.00000000e+00 1.61459071e-35 5.51248888e-37
 1.93712382e-17], sum to 1.0000
[2019-03-27 01:35:08,675] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7492
[2019-03-27 01:35:08,681] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333333, 78.66666666666667, 1.0, 2.0, 0.3587676915180267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585056.6533695918, 585056.6533695924, 173609.863459545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 376800.0000, 
sim time next is 377400.0000, 
raw observation next is [21.41666666666667, 78.33333333333333, 1.0, 2.0, 0.3726745707419122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607374.3786738176, 607374.3786738182, 175496.8364599389], 
processed observation next is [1.0, 0.34782608695652173, 0.2140600315955769, 0.7833333333333333, 1.0, 1.0, 0.24418622980953275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16871510518717156, 0.16871510518717173, 0.261935576805879], 
reward next is 0.7381, 
noisyNet noise sample is [array([0.3793476], dtype=float32), -2.1556804]. 
=============================================
[2019-03-27 01:35:10,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2453812e-20 1.0000000e+00 1.3513145e-32 1.4317551e-32 1.1359429e-10], sum to 1.0000
[2019-03-27 01:35:10,668] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0486
[2019-03-27 01:35:10,673] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.58333333333334, 81.16666666666666, 1.0, 2.0, 0.285048506549507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460102.2410224798, 460102.2410224804, 164278.6277934112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 409800.0000, 
sim time next is 410400.0000, 
raw observation next is [21.5, 81.0, 1.0, 2.0, 0.28165705529294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455468.6955350673, 455468.6955350667, 163961.1921212406], 
processed observation next is [1.0, 0.782608695652174, 0.21800947867298584, 0.81, 1.0, 1.0, 0.13452657264209636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12651908209307425, 0.12651908209307408, 0.2447181971958815], 
reward next is 0.7553, 
noisyNet noise sample is [array([1.5856808], dtype=float32), 1.0305372]. 
=============================================
[2019-03-27 01:35:13,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2870786e-28 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5309395e-23], sum to 1.0000
[2019-03-27 01:35:13,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8645
[2019-03-27 01:35:13,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666666, 82.0, 1.0, 2.0, 0.229338830578283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378904.9235685278, 378904.9235685278, 158880.38700392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [19.9, 82.0, 1.0, 2.0, 0.2303264801330011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380402.9795722726, 380402.9795722726, 158978.7952441557], 
processed observation next is [1.0, 0.2608695652173913, 0.14218009478672985, 0.82, 1.0, 1.0, 0.07268250618433868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10566749432563129, 0.10566749432563129, 0.23728178394650107], 
reward next is 0.7627, 
noisyNet noise sample is [array([-0.92247456], dtype=float32), -0.27282625]. 
=============================================
[2019-03-27 01:35:15,103] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7325886e-18 1.0000000e+00 4.5443731e-29 1.9215993e-27 8.4417477e-09], sum to 1.0000
[2019-03-27 01:35:15,112] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3555
[2019-03-27 01:35:15,118] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 53.0, 1.0, 2.0, 0.6183887266573648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1010166.130353235, 1010166.130353234, 219526.8675808142], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 483600.0000, 
sim time next is 484200.0000, 
raw observation next is [25.25, 53.0, 1.0, 2.0, 0.6298245629480145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029490.92409195, 1029490.92409195, 222093.0732988846], 
processed observation next is [1.0, 0.6086956521739131, 0.39573459715639814, 0.53, 1.0, 1.0, 0.5540054975277283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28596970113665277, 0.28596970113665277, 0.3314821989535591], 
reward next is 0.6685, 
noisyNet noise sample is [array([0.29061466], dtype=float32), 0.09239665]. 
=============================================
[2019-03-27 01:35:15,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4689060e-22 1.0000000e+00 1.2853894e-35 7.3782740e-36 3.8261474e-14], sum to 1.0000
[2019-03-27 01:35:15,350] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9561
[2019-03-27 01:35:15,355] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.8, 88.0, 1.0, 2.0, 0.2349968896338142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 389433.2932708868, 389433.2932708875, 159306.2788507443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 687600.0000, 
sim time next is 688200.0000, 
raw observation next is [18.73333333333333, 88.33333333333334, 1.0, 2.0, 0.2349277852836354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 389430.3212012993, 389430.3212012987, 159287.7401957771], 
processed observation next is [1.0, 1.0, 0.08688783570300151, 0.8833333333333334, 1.0, 1.0, 0.07822624732968118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10817508922258315, 0.10817508922258297, 0.23774289581459268], 
reward next is 0.7623, 
noisyNet noise sample is [array([1.1895787], dtype=float32), -2.1257644]. 
=============================================
[2019-03-27 01:35:20,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4592131e-18 1.0000000e+00 6.5042518e-29 4.1196069e-27 4.2110617e-09], sum to 1.0000
[2019-03-27 01:35:20,242] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6454
[2019-03-27 01:35:20,250] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 59.66666666666667, 1.0, 2.0, 0.7341314122123583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202735.656543506, 1202735.656543506, 247468.0571380187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [23.85, 60.0, 1.0, 2.0, 0.7446588331003309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1219329.711021105, 1219329.711021105, 250218.1769976782], 
processed observation next is [1.0, 0.6086956521739131, 0.3293838862559243, 0.6, 1.0, 1.0, 0.6923600398799167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3387026975058625, 0.3387026975058625, 0.3734599656681764], 
reward next is 0.6265, 
noisyNet noise sample is [array([0.936141], dtype=float32), 0.008660891]. 
=============================================
[2019-03-27 01:35:22,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3595944e-23 1.0000000e+00 6.1328378e-34 2.7282464e-36 4.2998781e-17], sum to 1.0000
[2019-03-27 01:35:22,818] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4412
[2019-03-27 01:35:22,823] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.16666666666667, 91.33333333333334, 1.0, 2.0, 0.2182630173528393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 364737.7274437897, 364737.7274437897, 156963.1787796891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 613200.0000, 
sim time next is 613800.0000, 
raw observation next is [17.15, 91.5, 1.0, 2.0, 0.2087223887586454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 348792.9853543976, 348792.9853543976, 156150.0011927825], 
processed observation next is [1.0, 0.08695652173913043, 0.011848341232227487, 0.915, 1.0, 1.0, 0.046653480432102885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09688694037622156, 0.09688694037622156, 0.23305970327280973], 
reward next is 0.7669, 
noisyNet noise sample is [array([0.17892775], dtype=float32), -0.84455365]. 
=============================================
[2019-03-27 01:35:28,534] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4692843e-27 1.0000000e+00 1.0006718e-37 0.0000000e+00 1.4443417e-21], sum to 1.0000
[2019-03-27 01:35:28,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0740
[2019-03-27 01:35:28,547] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.33333333333333, 1.0, 2.0, 0.2166460440961672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 361031.072193291, 361031.0721932903, 157267.4669381341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 704400.0000, 
sim time next is 705000.0000, 
raw observation next is [17.6, 92.16666666666667, 1.0, 2.0, 0.2156493060691995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 157168.2631591955], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9216666666666667, 1.0, 1.0, 0.05499916393879456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09983652937703905, 0.09983652937703892, 0.2345794972525306], 
reward next is 0.7654, 
noisyNet noise sample is [array([-0.95021397], dtype=float32), -1.989688]. 
=============================================
[2019-03-27 01:35:28,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.65461]
 [75.75339]
 [75.7489 ]
 [75.70453]
 [75.63783]], R is [[75.66268158]
 [75.67133331]
 [75.67967224]
 [75.68807983]
 [75.69587708]].
[2019-03-27 01:35:30,070] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9702614e-19 1.0000000e+00 9.9254351e-31 2.4293491e-29 9.2053188e-12], sum to 1.0000
[2019-03-27 01:35:30,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7925
[2019-03-27 01:35:30,083] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666666, 56.0, 1.0, 2.0, 0.5410158222457473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879874.7795319294, 879874.7795319294, 203320.0198753377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 733200.0000, 
sim time next is 733800.0000, 
raw observation next is [25.13333333333333, 55.5, 1.0, 2.0, 0.5358969499269023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871885.0325699777, 871885.0325699777, 202343.5722974583], 
processed observation next is [1.0, 0.4782608695652174, 0.3902053712480251, 0.555, 1.0, 1.0, 0.44083969870711115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2421902868249938, 0.2421902868249938, 0.3020053317872512], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.06876244], dtype=float32), -0.15861025]. 
=============================================
[2019-03-27 01:35:34,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2622425e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1414311e-23], sum to 1.0000
[2019-03-27 01:35:34,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3253
[2019-03-27 01:35:34,519] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 82.0, 1.0, 2.0, 0.2824387353538764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454906.8589584904, 454906.8589584911, 163928.4994987001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 804600.0000, 
sim time next is 805200.0000, 
raw observation next is [21.8, 81.33333333333333, 1.0, 2.0, 0.2841238818640062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457103.4032941213, 457103.4032941206, 164075.5186033361], 
processed observation next is [0.0, 0.30434782608695654, 0.23222748815165886, 0.8133333333333332, 1.0, 1.0, 0.13749865284820026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12697316758170038, 0.12697316758170019, 0.24488883373632253], 
reward next is 0.7551, 
noisyNet noise sample is [array([1.311081], dtype=float32), 0.647027]. 
=============================================
[2019-03-27 01:35:35,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1009871e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3751426e-24], sum to 1.0000
[2019-03-27 01:35:35,122] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1002
[2019-03-27 01:35:35,127] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 63.0, 1.0, 2.0, 0.2898091305429463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464338.8374976719, 464338.8374976725, 164557.6143666503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 825000.0000, 
sim time next is 825600.0000, 
raw observation next is [24.73333333333333, 63.00000000000001, 1.0, 2.0, 0.288240026047877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462102.2383976206, 462102.2383976199, 164406.2240309341], 
processed observation next is [0.0, 0.5652173913043478, 0.3712480252764612, 0.6300000000000001, 1.0, 1.0, 0.14245786270828556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12836173288822794, 0.12836173288822775, 0.24538242392676732], 
reward next is 0.7546, 
noisyNet noise sample is [array([-0.05506642], dtype=float32), 0.14262007]. 
=============================================
[2019-03-27 01:35:42,011] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 01:35:42,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:35:42,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:35:42,013] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:35:42,014] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:35:42,015] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:35:42,015] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:35:42,017] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:35:42,016] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:35:42,019] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:35:42,020] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:35:42,040] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-27 01:35:42,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-27 01:35:42,082] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-27 01:35:42,085] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-27 01:35:42,123] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-27 01:36:11,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:36:11,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 91.16666666666667, 1.0, 2.0, 0.4348435817334738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631981.4050644307, 631981.4050644307, 177137.5010512727]
[2019-03-27 01:36:11,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:36:11,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7848162e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0256108e-24], sampled 0.16722061651937314
[2019-03-27 01:36:17,349] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:36:17,350] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.57589958, 92.30564907, 1.0, 2.0, 0.6253848098405062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873950.1856220876, 873950.1856220876, 205569.7628694254]
[2019-03-27 01:36:17,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:36:17,354] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.93515232e-24 1.00000000e+00 1.35083755e-36 2.29609959e-36
 4.71716255e-17], sampled 0.9431238189336231
[2019-03-27 01:36:34,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:36:34,689] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.56666666666667, 50.83333333333333, 1.0, 2.0, 0.6925364963551357, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974817453462, 6.9112, 168.912316044056, 1864684.76457299, 1797448.502114379, 383031.223717117]
[2019-03-27 01:36:34,690] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:36:34,692] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2261377e-22 1.0000000e+00 7.2956051e-34 2.4963750e-32 1.4330926e-11], sampled 0.4937535269703235
[2019-03-27 01:36:34,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1864684.76457299 W.
[2019-03-27 01:36:42,277] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:36:42,280] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 77.0, 1.0, 2.0, 0.6090133766131698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851062.5981747742, 851062.5981747742, 202437.5336869241]
[2019-03-27 01:36:42,281] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:36:42,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1269889e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5081399e-23], sampled 0.8685249509309545
[2019-03-27 01:36:54,510] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:36:54,510] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.13666141, 71.0303246, 1.0, 2.0, 0.4370687652298836, 1.0, 2.0, 0.4370687652298836, 1.0, 2.0, 0.7590439043549326, 6.9112, 6.9112, 171.5212843490159, 1833161.60800345, 1833161.60800345, 374559.7952163322]
[2019-03-27 01:36:54,512] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:36:54,517] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2447191e-14 1.8074492e-03 1.3916949e-25 2.5384614e-18 9.9819261e-01], sampled 0.42936468027160646
[2019-03-27 01:37:06,650] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:06,653] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.53333333333333, 63.0, 1.0, 2.0, 0.6774822195405061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 946786.677029445, 946786.6770294455, 216070.2874853919]
[2019-03-27 01:37:06,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:37:06,659] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0845626e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7006991e-24], sampled 0.20669220031913338
[2019-03-27 01:37:08,274] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:08,276] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 92.66666666666667, 1.0, 2.0, 0.6213281476404795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868278.8456684981, 868278.8456684987, 204779.9012124394]
[2019-03-27 01:37:08,276] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:37:08,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1152280e-25 1.0000000e+00 1.8040864e-36 1.2955147e-36 4.3376520e-18], sampled 0.009242769075804569
[2019-03-27 01:37:09,337] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:09,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.70069493833333, 92.693262565, 1.0, 2.0, 0.504309274891965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704695.855443106, 704695.8554431053, 184274.204671952]
[2019-03-27 01:37:09,340] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:37:09,345] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5624535e-27 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7975337e-20], sampled 0.4601248943322518
[2019-03-27 01:37:09,345] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:09,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 93.0, 1.0, 2.0, 1.030843541314846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.2856565914384, 1440944.811024248, 1440944.811024248, 308531.7359730095]
[2019-03-27 01:37:09,348] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:37:09,352] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1055774e-24 1.0000000e+00 1.4287731e-36 1.2204638e-36 9.0769002e-18], sampled 0.33254677259861076
[2019-03-27 01:37:23,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:23,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.28333333333333, 82.0, 1.0, 2.0, 0.6981983772367837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975750.9685315274, 975750.9685315274, 220457.6498829335]
[2019-03-27 01:37:23,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:37:23,167] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0844397e-24 1.0000000e+00 1.8048297e-36 2.4997624e-36 4.4234264e-17], sampled 0.45403206120251993
[2019-03-27 01:37:34,211] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:34,212] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.2, 62.0, 1.0, 2.0, 0.3302193171379765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526602.0773655184, 526602.0773655191, 169110.573977417]
[2019-03-27 01:37:34,213] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:37:34,216] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5404694e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4064796e-25], sampled 0.03999907470521813
[2019-03-27 01:37:35,224] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.054542173]
[2019-03-27 01:37:35,225] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.34685425, 73.10367925333333, 1.0, 2.0, 0.4742693524069663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673475.5658862458, 673475.5658862452, 181045.6036652529]
[2019-03-27 01:37:35,226] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:37:35,229] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8838510e-26 1.0000000e+00 1.6581377e-38 0.0000000e+00 8.4414428e-20], sampled 0.06340889650036197
[2019-03-27 01:37:36,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7956.4049 3160400007.1203 1640.0000
[2019-03-27 01:37:36,354] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8292.3360 2926741298.4031 1246.0000
[2019-03-27 01:37:36,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8091.7588 3004033392.7201 1538.0000
[2019-03-27 01:37:36,527] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8693.0808 2779015254.4662 851.0000
[2019-03-27 01:37:36,572] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8551.5703 2840027129.2146 1008.0000
[2019-03-27 01:37:37,586] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 875000, evaluation results [875000.0, 7956.404906534341, 3160400007.1203203, 1640.0, 8292.336045753236, 2926741298.403079, 1246.0, 8693.080801219401, 2779015254.466175, 851.0, 8091.758804007221, 3004033392.7201085, 1538.0, 8551.570320083489, 2840027129.2146425, 1008.0]
[2019-03-27 01:37:44,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2921704e-24 1.0000000e+00 2.1498813e-35 3.4206043e-36 2.7354112e-17], sum to 1.0000
[2019-03-27 01:37:44,414] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9126
[2019-03-27 01:37:44,419] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 88.33333333333334, 1.0, 2.0, 0.2985829738507878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477188.8610938953, 477188.8610938959, 165444.4198335722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [21.15, 88.5, 1.0, 2.0, 0.2980085863276811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476556.6058885139, 476556.6058885139, 165402.6710470118], 
processed observation next is [1.0, 1.0, 0.2014218009478673, 0.885, 1.0, 1.0, 0.1542272124429893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13237683496903163, 0.13237683496903163, 0.24686965827912208], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.14750655], dtype=float32), 0.2234882]. 
=============================================
[2019-03-27 01:37:52,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6080516e-27 1.0000000e+00 2.0583115e-37 1.2585415e-38 3.5826962e-22], sum to 1.0000
[2019-03-27 01:37:52,242] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6233
[2019-03-27 01:37:52,251] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 96.33333333333333, 1.0, 2.0, 0.3166039960387995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501664.4559588066, 501664.4559588066, 167168.4678762208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1380000.0000, 
sim time next is 1380600.0000, 
raw observation next is [20.6, 96.5, 1.0, 2.0, 0.3155863708731341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500212.599310751, 500212.5993107517, 167062.6315749347], 
processed observation next is [1.0, 1.0, 0.17535545023696694, 0.965, 1.0, 1.0, 0.17540526611220977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1389479442529864, 0.1389479442529866, 0.24934721130587267], 
reward next is 0.7507, 
noisyNet noise sample is [array([1.2498487], dtype=float32), -1.3845606]. 
=============================================
[2019-03-27 01:37:55,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9450329e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9451486e-29], sum to 1.0000
[2019-03-27 01:37:55,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4180
[2019-03-27 01:37:55,806] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 69.66666666666667, 1.0, 2.0, 0.4480711582908161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636194.6363583707, 636194.6363583714, 177165.4249158481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [27.8, 69.5, 1.0, 2.0, 0.4437507989535847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632454.570169568, 632454.5701695685, 176853.6014965248], 
processed observation next is [0.0, 0.6956521739130435, 0.5165876777251186, 0.695, 1.0, 1.0, 0.3298202397031141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1756818250471022, 0.17568182504710236, 0.2639605992485445], 
reward next is 0.7360, 
noisyNet noise sample is [array([0.69929737], dtype=float32), 0.51988447]. 
=============================================
[2019-03-27 01:37:56,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.02348216e-26 1.00000000e+00 7.40712740e-35 2.16919601e-38
 5.32156199e-25], sum to 1.0000
[2019-03-27 01:37:56,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7198
[2019-03-27 01:37:56,944] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.36666666666667, 72.83333333333333, 1.0, 2.0, 0.9314206448281794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1301885.156718267, 1301885.156718267, 278724.1967509071], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1255800.0000, 
sim time next is 1256400.0000, 
raw observation next is [28.4, 73.0, 1.0, 2.0, 1.03262821974341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1443443.342415234, 1443443.342415234, 309022.4345501791], 
processed observation next is [1.0, 0.5652173913043478, 0.5450236966824644, 0.73, 1.0, 1.0, 1.0393111081245905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4009564840042317, 0.4009564840042317, 0.46122751425399866], 
reward next is 0.5388, 
noisyNet noise sample is [array([-2.931782], dtype=float32), 0.28418675]. 
=============================================
[2019-03-27 01:37:57,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1907807e-26 1.0000000e+00 2.3223526e-34 1.1403780e-36 3.2910154e-23], sum to 1.0000
[2019-03-27 01:37:57,156] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8859
[2019-03-27 01:37:57,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.7187401081279726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1092218.434412109, 1092218.434412109, 236227.0199749477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [22.4, 93.0, 1.0, 2.0, 0.77006259614506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1174780.441420588, 1174780.441420588, 249615.5148186972], 
processed observation next is [1.0, 0.5217391304347826, 0.2606635071090047, 0.93, 1.0, 1.0, 0.7229669833073012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32632790039460774, 0.32632790039460774, 0.37256046987865254], 
reward next is 0.6274, 
noisyNet noise sample is [array([-0.7453427], dtype=float32), 0.1682541]. 
=============================================
[2019-03-27 01:37:57,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1932426e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2525475e-29], sum to 1.0000
[2019-03-27 01:37:57,218] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5189
[2019-03-27 01:37:57,225] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 85.0, 1.0, 2.0, 0.4809180021870427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734962.9368181376, 734962.9368181371, 188286.2661040828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1585200.0000, 
sim time next is 1585800.0000, 
raw observation next is [23.45, 85.0, 1.0, 2.0, 0.5584358961648915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852950.3846322736, 852950.3846322736, 202073.3479479912], 
processed observation next is [1.0, 0.34782608695652173, 0.3104265402843602, 0.85, 1.0, 1.0, 0.4679950556203512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23693066239785376, 0.23693066239785376, 0.30160201186267344], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.17245457], dtype=float32), 1.2165638]. 
=============================================
[2019-03-27 01:37:58,363] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1811990e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6984615e-30], sum to 1.0000
[2019-03-27 01:37:58,369] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9416
[2019-03-27 01:37:58,376] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.4646238933654218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655924.6479070106, 655924.6479070106, 179103.355951271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1292400.0000, 
sim time next is 1293000.0000, 
raw observation next is [24.38333333333333, 94.00000000000001, 1.0, 2.0, 0.4636476081755364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654985.2056787602, 654985.2056787602, 179015.5343339711], 
processed observation next is [1.0, 1.0, 0.3546603475513427, 0.9400000000000002, 1.0, 1.0, 0.35379229900667036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18194033491076672, 0.18194033491076672, 0.2671873646775688], 
reward next is 0.7328, 
noisyNet noise sample is [array([-0.6219523], dtype=float32), -0.4028737]. 
=============================================
[2019-03-27 01:37:58,393] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.60911 ]
 [68.568085]
 [68.59527 ]
 [68.620285]
 [68.64681 ]], R is [[68.62819672]
 [68.67459869]
 [68.72045135]
 [68.76570129]
 [68.81027985]].
[2019-03-27 01:38:02,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8855116e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5682312e-29], sum to 1.0000
[2019-03-27 01:38:02,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3896
[2019-03-27 01:38:02,655] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 91.0, 1.0, 2.0, 0.330860947632158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 517485.2521073253, 517485.2521073253, 168234.9311779943], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1557000.0000, 
sim time next is 1557600.0000, 
raw observation next is [21.76666666666667, 91.0, 1.0, 2.0, 0.3298331534031852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516317.0468404786, 516317.0468404786, 168154.9518577245], 
processed observation next is [1.0, 0.0, 0.23064770932069528, 0.91, 1.0, 1.0, 0.19257006434118698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14342140190013294, 0.14342140190013294, 0.25097754008615597], 
reward next is 0.7490, 
noisyNet noise sample is [array([-0.7266643], dtype=float32), -1.6214647]. 
=============================================
[2019-03-27 01:38:07,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4057392e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7938996e-35], sum to 1.0000
[2019-03-27 01:38:07,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8202
[2019-03-27 01:38:07,501] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 56.0, 1.0, 2.0, 0.3499784286171283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537403.8140550866, 537403.8140550873, 169550.3510660284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1506600.0000, 
sim time next is 1507200.0000, 
raw observation next is [28.26666666666667, 54.33333333333334, 1.0, 2.0, 0.34763222361824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534753.6981337416, 534753.6981337416, 169363.2650387734], 
processed observation next is [0.0, 0.43478260869565216, 0.53870458135861, 0.5433333333333334, 1.0, 1.0, 0.21401472725089155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14854269392603933, 0.14854269392603933, 0.25278099259518416], 
reward next is 0.7472, 
noisyNet noise sample is [array([-0.7240718], dtype=float32), -0.68860674]. 
=============================================
[2019-03-27 01:38:11,048] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.9658418e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0818249e-34], sum to 1.0000
[2019-03-27 01:38:11,056] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6272
[2019-03-27 01:38:11,063] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 78.5, 1.0, 2.0, 0.3761821496943077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566772.5059340392, 566772.5059340392, 171707.9712444984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1499400.0000, 
sim time next is 1500000.0000, 
raw observation next is [25.06666666666667, 76.33333333333334, 1.0, 2.0, 0.371240135605223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 560304.8119079579, 560304.8119079586, 171176.2902761309], 
processed observation next is [0.0, 0.34782608695652173, 0.38704581358609813, 0.7633333333333334, 1.0, 1.0, 0.24245799470508797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1556402255299883, 0.1556402255299885, 0.2554870004121357], 
reward next is 0.7445, 
noisyNet noise sample is [array([-1.0079098], dtype=float32), -0.9704759]. 
=============================================
[2019-03-27 01:38:11,087] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.50289 ]
 [71.49845 ]
 [71.51339 ]
 [71.53225 ]
 [71.503654]], R is [[71.54444885]
 [71.57272339]
 [71.60057068]
 [71.62804413]
 [71.65520477]].
[2019-03-27 01:38:14,066] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0801667e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1574516e-31], sum to 1.0000
[2019-03-27 01:38:14,075] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0985
[2019-03-27 01:38:14,082] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 91.00000000000001, 1.0, 2.0, 0.3338555075879885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521271.1199704568, 521271.1199704568, 168508.7759005965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1555800.0000, 
sim time next is 1556400.0000, 
raw observation next is [21.83333333333334, 91.0, 1.0, 2.0, 0.3321920921994159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519121.8587113532, 519121.8587113532, 168351.5723837805], 
processed observation next is [1.0, 0.0, 0.23380726698262277, 0.91, 1.0, 1.0, 0.19541215927640473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14420051630870923, 0.14420051630870923, 0.25127100355788135], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.2379826], dtype=float32), 0.9327395]. 
=============================================
[2019-03-27 01:38:19,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3443278e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.0147071e-31], sum to 1.0000
[2019-03-27 01:38:19,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5638
[2019-03-27 01:38:19,361] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.83333333333333, 1.0, 2.0, 0.4164248089475659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611907.3553737071, 611907.3553737071, 175382.6263626735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1630200.0000, 
sim time next is 1630800.0000, 
raw observation next is [23.2, 96.0, 1.0, 2.0, 0.4160993969157197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610950.8839793513, 610950.8839793506, 175278.2734516194], 
processed observation next is [1.0, 0.9130434782608695, 0.29857819905213273, 0.96, 1.0, 1.0, 0.2965052974888189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16970857888315313, 0.16970857888315294, 0.26160936336062596], 
reward next is 0.7384, 
noisyNet noise sample is [array([-1.1911476], dtype=float32), -0.6740756]. 
=============================================
[2019-03-27 01:38:23,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4474016e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4573230e-34], sum to 1.0000
[2019-03-27 01:38:23,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5804
[2019-03-27 01:38:23,091] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 95.5, 1.0, 2.0, 0.4817375651242758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673145.3152481278, 673145.3152481284, 180784.2955881653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2010600.0000, 
sim time next is 2011200.0000, 
raw observation next is [24.83333333333333, 95.33333333333333, 1.0, 2.0, 0.4835651054196176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675699.8003626856, 675699.8003626862, 181061.2809112976], 
processed observation next is [0.0, 0.2608695652173913, 0.3759873617693521, 0.9533333333333333, 1.0, 1.0, 0.3777892836380935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1876943889896349, 0.18769438898963506, 0.27024071777805614], 
reward next is 0.7298, 
noisyNet noise sample is [array([-1.5894976], dtype=float32), 2.4923034]. 
=============================================
[2019-03-27 01:38:24,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0712557e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4107465e-35], sum to 1.0000
[2019-03-27 01:38:24,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8366
[2019-03-27 01:38:24,503] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 92.0, 1.0, 2.0, 0.4313916223341231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632436.7785811314, 632436.778581132, 177323.0443746817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1920600.0000, 
sim time next is 1921200.0000, 
raw observation next is [23.86666666666667, 91.33333333333334, 1.0, 2.0, 0.4351395411081841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 177774.0840418001], 
processed observation next is [1.0, 0.21739130434782608, 0.33017377567140627, 0.9133333333333334, 1.0, 1.0, 0.31944523025082416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17700108646222149, 0.17700108646222132, 0.2653344537937315], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.2865331], dtype=float32), 1.4218599]. 
=============================================
[2019-03-27 01:38:30,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3594019e-27 1.0000000e+00 1.4797387e-36 0.0000000e+00 7.6354167e-25], sum to 1.0000
[2019-03-27 01:38:30,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2696
[2019-03-27 01:38:30,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1951691.701059716 W.
[2019-03-27 01:38:30,608] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 72.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.612612558762084, 6.9112, 168.9094685242679, 1951691.701059716, 1454095.766489364, 311349.2136883793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2276400.0000, 
sim time next is 2277000.0000, 
raw observation next is [29.2, 72.0, 1.0, 2.0, 0.6935455950507654, 1.0, 1.0, 0.6935455950507654, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1939359.541631129, 1939359.541631128, 371369.0139346893], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.72, 1.0, 1.0, 0.6307778253623679, 1.0, 0.5, 0.6307778253623679, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5387109837864248, 0.5387109837864245, 0.5542821103502825], 
reward next is 0.4457, 
noisyNet noise sample is [array([0.3159025], dtype=float32), 0.11311253]. 
=============================================
[2019-03-27 01:38:30,621] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.28808 ]
 [65.989655]
 [66.38657 ]
 [66.12845 ]
 [66.23318 ]], R is [[60.41308975]
 [59.80895996]
 [59.85886383]
 [59.94060898]
 [60.00791168]].
[2019-03-27 01:38:33,574] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:38:33,577] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:38:33,578] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:38:33,579] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:38:33,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:38:33,581] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:38:33,583] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:38:33,584] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:38:33,586] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:38:33,588] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:38:33,584] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:38:33,599] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-27 01:38:33,619] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-27 01:38:33,638] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-27 01:38:33,654] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-27 01:38:33,677] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-27 01:38:37,622] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:38:37,623] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.43333333333334, 38.33333333333334, 1.0, 2.0, 0.221127183712802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 369572.5167972088, 369572.5167972093, 157181.303883275]
[2019-03-27 01:38:37,624] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:38:37,627] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9135658e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6888044e-36], sampled 0.5423679411590168
[2019-03-27 01:38:44,485] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:38:44,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.45, 77.5, 1.0, 2.0, 0.2442870918465227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403544.4234412894, 403544.4234412888, 160288.2708854308]
[2019-03-27 01:38:44,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:38:44,492] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.6881993e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0519154e-35], sampled 0.964529045264333
[2019-03-27 01:39:04,898] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:39:04,900] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.69454168333333, 92.31647631833334, 1.0, 2.0, 0.4635324465748837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 658327.873817016, 658327.8738170154, 179447.5853679654]
[2019-03-27 01:39:04,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:39:04,904] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.6193814e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0753242e-34], sampled 0.3769680664202427
[2019-03-27 01:39:12,954] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:39:12,955] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.5055479665789161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706427.3150664754, 706427.3150664754, 184470.1805756645]
[2019-03-27 01:39:12,955] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:39:12,958] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2350211e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2521941e-34], sampled 0.07346067972400172
[2019-03-27 01:39:24,543] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:39:24,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.6680389578119544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933583.8447510903, 933583.8447510909, 214094.3565840539]
[2019-03-27 01:39:24,545] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:39:24,547] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0798187e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6856208e-31], sampled 0.12690697374499482
[2019-03-27 01:39:37,269] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:39:37,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.78259422333333, 79.02929846833334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.974637524955435, 6.9112, 168.9126745870913, 1498790.435208193, 1453785.74676868, 311373.6085479101]
[2019-03-27 01:39:37,272] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:39:37,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7006938e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4774585e-38], sampled 0.8883174550738153
[2019-03-27 01:40:04,001] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.051213976]
[2019-03-27 01:40:04,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 63.0, 1.0, 2.0, 0.5053681343960709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706175.943087651, 706175.9430876516, 184441.5590048312]
[2019-03-27 01:40:04,003] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:40:04,005] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7939921e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5791474e-34], sampled 0.9226957278829998
[2019-03-27 01:40:27,366] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 01:40:27,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.4188 3164131021.0653 1778.0000
[2019-03-27 01:40:27,829] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6857 2927400807.7850 1338.0000
[2019-03-27 01:40:27,924] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9883 2779274382.2099 933.0000
[2019-03-27 01:40:28,017] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1326 2842568365.7369 1131.0000
[2019-03-27 01:40:29,034] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 900000, evaluation results [900000.0, 7883.4188199173295, 3164131021.06526, 1778.0, 8253.68573338827, 2927400807.784972, 1338.0, 8659.988291364358, 2779274382.209945, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.13258090819, 2842568365.7369456, 1131.0]
[2019-03-27 01:40:33,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2386635e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8123602e-36], sum to 1.0000
[2019-03-27 01:40:33,236] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0266
[2019-03-27 01:40:33,240] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 94.5, 1.0, 2.0, 0.4971613187518579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694704.4029410825, 694704.4029410825, 183152.7598269352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2014200.0000, 
sim time next is 2014800.0000, 
raw observation next is [25.4, 94.33333333333334, 1.0, 2.0, 0.500182295465546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698927.1293813366, 698927.1293813366, 183624.9002663266], 
processed observation next is [0.0, 0.30434782608695654, 0.4028436018957346, 0.9433333333333335, 1.0, 1.0, 0.39780999453680244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19414642482814906, 0.19414642482814906, 0.2740670153228755], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.35232306], dtype=float32), -0.026218971]. 
=============================================
[2019-03-27 01:40:35,459] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.7836658e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3867823e-34], sum to 1.0000
[2019-03-27 01:40:35,468] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2501
[2019-03-27 01:40:35,473] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 96.33333333333333, 1.0, 2.0, 0.412213088772087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607604.927948057, 607604.927948057, 175031.0478428343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1977600.0000, 
sim time next is 1978200.0000, 
raw observation next is [23.1, 96.5, 1.0, 2.0, 0.4148082964729517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610197.1089894206, 610197.1089894206, 175239.8477476627], 
processed observation next is [1.0, 0.9130434782608695, 0.2938388625592418, 0.965, 1.0, 1.0, 0.2949497547866888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16949919694150573, 0.16949919694150573, 0.26155201156367563], 
reward next is 0.7384, 
noisyNet noise sample is [array([-1.3708496], dtype=float32), 0.29911435]. 
=============================================
[2019-03-27 01:40:40,455] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2678503e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3104211e-36], sum to 1.0000
[2019-03-27 01:40:40,464] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4992
[2019-03-27 01:40:40,471] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.75, 73.16666666666667, 1.0, 2.0, 0.5706753174267406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797467.1629048804, 797467.1629048804, 195410.6037167161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2134200.0000, 
sim time next is 2134800.0000, 
raw observation next is [30.8, 73.0, 1.0, 2.0, 0.5711457773894059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798124.835167758, 798124.835167758, 195494.2186136233], 
processed observation next is [0.0, 0.7391304347826086, 0.6587677725118484, 0.73, 1.0, 1.0, 0.48330816552940464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.221701343102155, 0.221701343102155, 0.29178241584122877], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.50231475], dtype=float32), 1.4015635]. 
=============================================
[2019-03-27 01:40:57,098] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6508770e-23 1.0000000e+00 1.1766697e-32 2.9034654e-33 4.0726382e-19], sum to 1.0000
[2019-03-27 01:40:57,109] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8863
[2019-03-27 01:40:57,113] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333334, 81.83333333333334, 1.0, 2.0, 0.7702478047778449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076493.012394116, 1076493.012394115, 236737.7817726357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2343000.0000, 
sim time next is 2343600.0000, 
raw observation next is [27.6, 82.0, 1.0, 2.0, 0.7558563743247565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1056369.644282446, 1056369.644282446, 233362.6827635489], 
processed observation next is [1.0, 0.13043478260869565, 0.5071090047393366, 0.82, 1.0, 1.0, 0.7058510534033211, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29343601230067945, 0.29343601230067945, 0.3483025115873864], 
reward next is 0.6517, 
noisyNet noise sample is [array([0.7831678], dtype=float32), 0.815167]. 
=============================================
[2019-03-27 01:41:00,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0665215e-14 1.0000000e+00 6.1137012e-21 1.9297187e-17 3.2826023e-08], sum to 1.0000
[2019-03-27 01:41:00,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7055
[2019-03-27 01:41:00,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1869593.726714601 W.
[2019-03-27 01:41:00,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.63333333333333, 88.0, 1.0, 2.0, 0.6686179871538619, 1.0, 1.0, 0.6686179871538619, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1869593.726714601, 1869593.7267146, 360974.5794165434], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2470800.0000, 
sim time next is 2471400.0000, 
raw observation next is [26.75, 87.5, 1.0, 2.0, 0.6496626933439192, 1.0, 2.0, 0.6496626933439192, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1816545.887724719, 1816545.887724719, 353315.2142685113], 
processed observation next is [1.0, 0.6086956521739131, 0.4668246445497631, 0.875, 1.0, 1.0, 0.5779068594505051, 1.0, 1.0, 0.5779068594505051, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5045960799235331, 0.5045960799235331, 0.5273361406992706], 
reward next is 0.4727, 
noisyNet noise sample is [array([-0.4318172], dtype=float32), -0.14522365]. 
=============================================
[2019-03-27 01:41:01,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4425191e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3397892e-35], sum to 1.0000
[2019-03-27 01:41:01,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6784
[2019-03-27 01:41:01,483] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.384318671607873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 578856.8515791066, 578856.8515791061, 172772.88893812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3841677938209312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578629.998411667, 578629.998411667, 172752.6128654845], 
processed observation next is [0.0, 0.5217391304347826, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2580334865312424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16073055511435194, 0.16073055511435194, 0.25783972069475297], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.84024733], dtype=float32), 0.6850805]. 
=============================================
[2019-03-27 01:41:05,275] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0885583e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7544939e-29], sum to 1.0000
[2019-03-27 01:41:05,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0736
[2019-03-27 01:41:05,290] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3411634716109478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525558.265805555, 525558.2658055545, 168643.2708117133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2790000.0000, 
sim time next is 2790600.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.3761291680270729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579419.5824085322, 579419.5824085322, 173170.0991402411], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.24834839521334084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16094988400237004, 0.16094988400237004, 0.25846283453767327], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.02042775], dtype=float32), 1.5743333]. 
=============================================
[2019-03-27 01:41:06,829] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2721122e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5664991e-30], sum to 1.0000
[2019-03-27 01:41:06,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5141
[2019-03-27 01:41:06,841] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3071500491668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485706.0995626082, 485706.0995626082, 165969.880051621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2959200.0000, 
sim time next is 2959800.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3258918714676126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514351.9279243697, 514351.9279243691, 168095.8772717536], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18782153188868989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14287553553454713, 0.142875535534547, 0.2508893690623188], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.07207296], dtype=float32), 1.0388643]. 
=============================================
[2019-03-27 01:41:07,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6567487e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8744261e-32], sum to 1.0000
[2019-03-27 01:41:07,697] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2921
[2019-03-27 01:41:07,702] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3966616096263242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591877.1983858881, 591877.1983858881, 173790.0738845282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2667000.0000, 
sim time next is 2667600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3964040475273459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591492.9742238744, 591492.9742238751, 173754.7312613747], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2727759608763204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1643036039510762, 0.1643036039510764, 0.25933541979309654], 
reward next is 0.7407, 
noisyNet noise sample is [array([-1.4969], dtype=float32), -0.40418643]. 
=============================================
[2019-03-27 01:41:09,941] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4459041e-14 9.9999988e-01 1.4920214e-21 6.2796388e-19 8.8053490e-08], sum to 1.0000
[2019-03-27 01:41:09,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2607
[2019-03-27 01:41:09,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2341385.996347121 W.
[2019-03-27 01:41:09,964] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.73333333333333, 72.0, 1.0, 2.0, 0.8371668276780141, 1.0, 2.0, 0.8371668276780141, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2341385.996347121, 2341385.996347121, 438366.8813388195], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2562000.0000, 
sim time next is 2562600.0000, 
raw observation next is [29.66666666666667, 72.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.132508706145531, 6.9112, 168.9116170509246, 2452418.322558742, 2295415.554584287, 476178.743178099], 
processed observation next is [1.0, 0.6521739130434783, 0.6050552922590839, 0.725, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0221308706145531, 0.0, 0.8294333677929191, 0.6812273118218728, 0.6376154318289686, 0.7107145420568641], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.58731675], dtype=float32), -1.2024684]. 
=============================================
[2019-03-27 01:41:10,217] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1323944e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4891887e-30], sum to 1.0000
[2019-03-27 01:41:10,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5806
[2019-03-27 01:41:10,234] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4300009904569671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624140.126048276, 624140.1260482754, 176346.2269402324], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.428758649458126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622337.012059926, 622337.0120599266, 176170.4536607454], 
processed observation next is [0.0, 0.43478260869565216, 0.28909952606635075, 1.0, 1.0, 1.0, 0.311757408985694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17287139223886835, 0.17287139223886852, 0.26294097561305285], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.81047875], dtype=float32), -0.5309659]. 
=============================================
[2019-03-27 01:41:13,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2821319e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3372918e-27], sum to 1.0000
[2019-03-27 01:41:13,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3924
[2019-03-27 01:41:13,981] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 96.0, 1.0, 2.0, 0.337116271885331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520451.6342583667, 520451.6342583673, 168272.3744305947], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2776800.0000, 
sim time next is 2777400.0000, 
raw observation next is [21.5, 97.0, 1.0, 2.0, 0.3334609077491934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515505.4628424659, 515505.4628424659, 167904.5039443714], 
processed observation next is [1.0, 0.13043478260869565, 0.21800947867298584, 0.97, 1.0, 1.0, 0.19694085270987158, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14319596190068498, 0.14319596190068498, 0.2506037372304051], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.02505393], dtype=float32), -0.08456388]. 
=============================================
[2019-03-27 01:41:20,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2331029e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3839236e-26], sum to 1.0000
[2019-03-27 01:41:20,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4043
[2019-03-27 01:41:20,866] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4862582826163963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679464.2599147927, 679464.259914792, 181471.0371673381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3195600.0000, 
sim time next is 3196200.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4861444358929827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679305.1273804711, 679305.1273804705, 181453.6716423996], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.380896910714437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18869586871679753, 0.18869586871679736, 0.270826375585671], 
reward next is 0.7292, 
noisyNet noise sample is [array([-0.5567733], dtype=float32), 1.1225907]. 
=============================================
[2019-03-27 01:41:23,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0914688e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1208835e-29], sum to 1.0000
[2019-03-27 01:41:23,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0688
[2019-03-27 01:41:23,771] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.00000000000001, 1.0, 2.0, 0.5573041287044418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778775.2857123173, 778775.285712318, 193062.1271549221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3244800.0000, 
sim time next is 3245400.0000, 
raw observation next is [32.5, 63.0, 1.0, 2.0, 0.5598152607579185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782285.6270513296, 782285.6270513296, 193499.3783222623], 
processed observation next is [0.0, 0.5652173913043478, 0.7393364928909952, 0.63, 1.0, 1.0, 0.469656940672191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21730156306981377, 0.21730156306981377, 0.28880504227203324], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.07054213], dtype=float32), -0.20816624]. 
=============================================
[2019-03-27 01:41:25,302] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:41:25,303] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:41:25,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:41:25,305] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:41:25,305] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:41:25,306] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:41:25,307] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:41:25,307] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:41:25,308] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:41:25,308] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:41:25,312] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:41:25,332] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-27 01:41:25,354] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-27 01:41:25,355] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-27 01:41:25,356] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-27 01:41:25,374] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-27 01:41:46,644] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:41:46,645] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.9, 65.5, 1.0, 2.0, 0.239877786795414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397306.0539494473, 397306.0539494473, 159787.4285376885]
[2019-03-27 01:41:46,646] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:41:46,650] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.4772646e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1578837e-28], sampled 0.8910239686833922
[2019-03-27 01:41:49,390] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:41:49,392] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.63199921666667, 97.95226365833334, 1.0, 2.0, 0.3595760205881993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552955.5175224436, 552955.5175224436, 170868.072697408]
[2019-03-27 01:41:49,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:41:49,396] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5460795e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0536848e-27], sampled 0.09078521416925789
[2019-03-27 01:42:11,188] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:42:11,189] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.86666666666667, 54.5, 1.0, 2.0, 0.6880737538685587, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005974538962466, 6.9112, 168.9123157729517, 1858439.888104495, 1791203.823324146, 382080.2234827394]
[2019-03-27 01:42:11,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:42:11,195] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5807120e-19 1.0000000e+00 1.1063177e-28 8.6998755e-27 1.4782314e-11], sampled 0.544739195699444
[2019-03-27 01:42:11,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1858439.888104495 W.
[2019-03-27 01:42:38,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:42:38,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.181613675, 73.435913825, 1.0, 2.0, 0.480368784362377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681580.9365977101, 681580.9365977101, 181905.1646579539]
[2019-03-27 01:42:38,326] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:42:38,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8246219e-28 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0887292e-25], sampled 0.8046824999293221
[2019-03-27 01:42:57,336] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:42:57,338] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.01666666666667, 56.16666666666667, 1.0, 2.0, 0.576750368323801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805959.718113195, 805959.718113195, 196494.2807731653]
[2019-03-27 01:42:57,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:42:57,342] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.682328e-29 1.000000e+00 0.000000e+00 0.000000e+00 7.488216e-26], sampled 0.7093136292646385
[2019-03-27 01:43:01,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:43:01,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.35, 91.0, 1.0, 2.0, 0.4989144133893478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697154.8795929402, 697154.8795929402, 183425.3490759973]
[2019-03-27 01:43:01,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:43:01,381] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.7809226e-28 1.0000000e+00 6.7743465e-38 0.0000000e+00 3.2449018e-24], sampled 0.798098642620728
[2019-03-27 01:43:03,413] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:43:03,414] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.0, 89.66666666666667, 1.0, 2.0, 0.5817425217424251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 812938.4987035975, 812938.498703598, 197393.4397127215]
[2019-03-27 01:43:03,415] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:43:03,416] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.15116474e-29 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.57123797e-27], sampled 0.6960950923903224
[2019-03-27 01:43:04,815] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:43:04,816] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.2, 88.0, 1.0, 2.0, 0.4163090947651873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612018.797796959, 612018.7977969585, 175401.2052912403]
[2019-03-27 01:43:04,818] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:43:04,821] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9631295e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3420440e-29], sampled 0.2995147917672897
[2019-03-27 01:43:14,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.053436086]
[2019-03-27 01:43:14,428] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 70.5, 1.0, 2.0, 0.5300334283113481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740653.9966251006, 740653.9966251, 188435.6234481641]
[2019-03-27 01:43:14,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:43:14,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.1363167e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5532762e-27], sampled 0.8325426254102692
[2019-03-27 01:43:19,627] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.5888 3164241634.8982 1823.0000
[2019-03-27 01:43:19,718] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.0670 2927623579.9595 1354.0000
[2019-03-27 01:43:20,007] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.7333 3007854489.1893 1763.0000
[2019-03-27 01:43:20,088] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5351 2779617598.8667 934.0000
[2019-03-27 01:43:20,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.1590 2842598888.1602 1141.0000
[2019-03-27 01:43:21,168] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 925000, evaluation results [925000.0, 7873.58878064562, 3164241634.898238, 1823.0, 8250.06701150356, 2927623579.9594517, 1354.0, 8660.535148170085, 2779617598.866722, 934.0, 7998.733282494828, 3007854489.1892776, 1763.0, 8494.159010614003, 2842598888.1601534, 1141.0]
[2019-03-27 01:43:22,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1419954e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1036127e-26], sum to 1.0000
[2019-03-27 01:43:22,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1786
[2019-03-27 01:43:22,285] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4113506259202543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606088.3237429457, 606088.3237429451, 174881.5591191078], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2836800.0000, 
sim time next is 2837400.0000, 
raw observation next is [23.83333333333333, 89.83333333333334, 1.0, 2.0, 0.40969293241241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604490.5301258386, 604490.5301258386, 174756.7883453504], 
processed observation next is [1.0, 0.8695652173913043, 0.32859399684044216, 0.8983333333333334, 1.0, 1.0, 0.2887866655571205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1679140361460663, 0.1679140361460663, 0.26083102738111996], 
reward next is 0.7392, 
noisyNet noise sample is [array([2.2417958], dtype=float32), -0.5360361]. 
=============================================
[2019-03-27 01:43:23,673] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.28415820e-31 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.46740695e-27], sum to 1.0000
[2019-03-27 01:43:23,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0213
[2019-03-27 01:43:23,689] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 95.0, 1.0, 2.0, 0.3245970395610622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511336.1056912722, 511336.1056912722, 167844.7569561113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2919000.0000, 
sim time next is 2919600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3214245079321091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 507786.9718983622, 507786.9718983629, 167602.9986496499], 
processed observation next is [1.0, 0.8260869565217391, 0.19431279620853087, 0.94, 1.0, 1.0, 0.18243916618326395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14105193663843396, 0.14105193663843416, 0.2501537293278357], 
reward next is 0.7498, 
noisyNet noise sample is [array([0.06929668], dtype=float32), -2.1359112]. 
=============================================
[2019-03-27 01:43:24,845] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8235781e-28 1.0000000e+00 2.3536699e-38 0.0000000e+00 5.6202210e-24], sum to 1.0000
[2019-03-27 01:43:24,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9003
[2019-03-27 01:43:24,861] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3027017108702418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482035.3906956342, 482035.3906956336, 165767.8371366799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3023815467460085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481525.9147603776, 481525.9147603782, 165731.2519388977], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15949583945302226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13375719854454934, 0.1337571985445495, 0.24736007752074285], 
reward next is 0.7526, 
noisyNet noise sample is [array([-2.1714616], dtype=float32), -0.71289617]. 
=============================================
[2019-03-27 01:43:24,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.143654]
 [71.168015]
 [71.18982 ]
 [71.65876 ]
 [73.1155  ]], R is [[71.16577911]
 [71.20671082]
 [71.24713898]
 [71.28705597]
 [71.32644653]].
[2019-03-27 01:43:25,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3080695e-27 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8036896e-25], sum to 1.0000
[2019-03-27 01:43:25,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7982
[2019-03-27 01:43:25,040] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5967660794159431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 919100.3964346413, 919100.3964346407, 210456.2377780009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2883000.0000, 
sim time next is 2883600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5847745970982471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900618.5821089275, 900618.5821089275, 208019.128533037], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4997284302388519, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.250171828363591, 0.250171828363591, 0.3104763112433388], 
reward next is 0.6895, 
noisyNet noise sample is [array([-0.81542027], dtype=float32), -1.6781127]. 
=============================================
[2019-03-27 01:43:32,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6317342e-24 1.0000000e+00 1.8632397e-33 1.3860888e-35 6.8111556e-19], sum to 1.0000
[2019-03-27 01:43:32,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9256
[2019-03-27 01:43:32,180] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5611963101398872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887170.2215449043, 887170.2215449043, 205581.7588431406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2994000.0000, 
sim time next is 2994600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5458538625052386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862938.1936454087, 862938.1936454081, 202589.1821842346], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4528359789219742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2397050537903913, 0.23970505379039114, 0.30237191370781286], 
reward next is 0.6976, 
noisyNet noise sample is [array([-1.5772278], dtype=float32), 0.73077804]. 
=============================================
[2019-03-27 01:43:34,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3859148e-26 1.0000000e+00 2.5737667e-36 2.6488678e-38 9.2337994e-21], sum to 1.0000
[2019-03-27 01:43:34,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6614
[2019-03-27 01:43:34,197] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.489418320808139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683881.303761665, 683881.303761665, 181954.5824861134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3185400.0000, 
sim time next is 3186000.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4904074582857703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685263.9058920406, 685263.9058920412, 182106.5468497977], 
processed observation next is [1.0, 0.9130434782608695, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38603308227201244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19035108497001127, 0.19035108497001144, 0.2718008161937279], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.5167867], dtype=float32), -0.83555704]. 
=============================================
[2019-03-27 01:43:34,215] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.15165]
 [72.468  ]
 [72.6641 ]
 [72.57957]
 [72.99409]], R is [[72.22460938]
 [72.23078918]
 [72.23723602]
 [72.24384308]
 [72.25032806]].
[2019-03-27 01:43:34,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7495835e-27 1.0000000e+00 7.4430596e-38 0.0000000e+00 1.3314314e-23], sum to 1.0000
[2019-03-27 01:43:34,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7226
[2019-03-27 01:43:34,441] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3275383449927264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521563.783305758, 521563.783305758, 168714.6045830214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3032400.0000, 
sim time next is 3033000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3136293636407088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499421.3290167285, 499421.3290167285, 167037.5342275142], 
processed observation next is [1.0, 0.08695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.17304742607314313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13872814694909125, 0.13872814694909125, 0.2493097525783794], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.83605623], dtype=float32), -0.22009465]. 
=============================================
[2019-03-27 01:43:34,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.188965]
 [69.03596 ]
 [68.68592 ]
 [68.56591 ]
 [68.61898 ]], R is [[69.14361572]
 [69.20036316]
 [69.25151825]
 [69.31182098]
 [69.37155914]].
[2019-03-27 01:43:38,104] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.478403e-32 1.000000e+00 0.000000e+00 0.000000e+00 2.451281e-29], sum to 1.0000
[2019-03-27 01:43:38,116] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4384
[2019-03-27 01:43:38,121] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 98.0, 1.0, 2.0, 0.4229290631167505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618778.816211871, 618778.816211871, 175963.6500937533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3093600.0000, 
sim time next is 3094200.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.4176422193116753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613899.0158457832, 613899.0158457839, 175577.3353345396], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.97, 1.0, 1.0, 0.29836411965262083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17052750440160644, 0.17052750440160663, 0.2620557243799098], 
reward next is 0.7379, 
noisyNet noise sample is [array([1.4233412], dtype=float32), 0.23303747]. 
=============================================
[2019-03-27 01:43:41,821] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.64019669e-27 1.00000000e+00 1.75452146e-35 1.23938065e-36
 1.38540999e-23], sum to 1.0000
[2019-03-27 01:43:41,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1308
[2019-03-27 01:43:41,833] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.6908483189325079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965474.3956512511, 965474.3956512511, 218878.8560032139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3471000.0000, 
sim time next is 3471600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6439676557647749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899929.969421469, 899929.969421469, 209212.2430398599], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.79, 1.0, 1.0, 0.571045368391295, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24998054706151918, 0.24998054706151918, 0.31225707916397], 
reward next is 0.6877, 
noisyNet noise sample is [array([1.4544219], dtype=float32), 1.3711214]. 
=============================================
[2019-03-27 01:43:43,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5028810e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5519041e-32], sum to 1.0000
[2019-03-27 01:43:43,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3368
[2019-03-27 01:43:43,232] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 94.0, 1.0, 2.0, 0.5118501918551973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715236.6926207957, 715236.6926207957, 185474.0513091489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3189000.0000, 
sim time next is 3189600.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.5167947148369516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722148.2976788848, 722148.2976788848, 186269.8897154717], 
processed observation next is [1.0, 0.9565217391304348, 0.4312796208530806, 0.94, 1.0, 1.0, 0.4178249576348814, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2005967493552458, 0.2005967493552458, 0.27801476076936077], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.6217592], dtype=float32), 0.33132344]. 
=============================================
[2019-03-27 01:43:46,861] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3485312e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1366197e-36], sum to 1.0000
[2019-03-27 01:43:46,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1264
[2019-03-27 01:43:46,873] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.480609474195439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671568.5041160118, 671568.5041160111, 180613.7178263492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3310200.0000, 
sim time next is 3310800.0000, 
raw observation next is [27.33333333333333, 77.33333333333333, 1.0, 2.0, 0.4831848330666783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675168.2657133715, 675168.2657133715, 181003.4236145625], 
processed observation next is [0.0, 0.30434782608695654, 0.494470774091627, 0.7733333333333333, 1.0, 1.0, 0.3773311241767209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1875467404759365, 0.1875467404759365, 0.2701543636038246], 
reward next is 0.7298, 
noisyNet noise sample is [array([-1.0412161], dtype=float32), -0.3093204]. 
=============================================
[2019-03-27 01:43:48,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1197856e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8636404e-32], sum to 1.0000
[2019-03-27 01:43:48,378] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9276
[2019-03-27 01:43:48,384] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.5983974642588359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836221.6063354497, 836221.6063354504, 200448.1885635025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5975090512203506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834979.618799215, 834979.618799215, 200283.1510616608], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.5150711460486151, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23193878299978193, 0.23193878299978193, 0.298930076211434], 
reward next is 0.7011, 
noisyNet noise sample is [array([2.1868093], dtype=float32), 0.26834896]. 
=============================================
[2019-03-27 01:43:54,407] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.6785221e-22 1.0000000e+00 7.9784492e-30 2.6662460e-29 3.6725072e-19], sum to 1.0000
[2019-03-27 01:43:54,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5274
[2019-03-27 01:43:54,420] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.66666666666667, 1.0, 2.0, 0.9263271978219202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104049, 1294761.493988137, 1294761.493988137, 277281.5064062881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3379800.0000, 
sim time next is 3380400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.8665246254648504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1211125.569893053, 1211125.569893053, 260894.5689703742], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.94, 1.0, 1.0, 0.8391862957407836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33642376941473695, 0.33642376941473695, 0.38939487906026], 
reward next is 0.6106, 
noisyNet noise sample is [array([0.70925206], dtype=float32), -1.502723]. 
=============================================
[2019-03-27 01:43:57,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0659789e-22 1.0000000e+00 6.0911627e-30 1.3961970e-29 3.8367780e-19], sum to 1.0000
[2019-03-27 01:43:57,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1894
[2019-03-27 01:43:57,197] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.7646196907920382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068623.241328951, 1068623.241328951, 235408.5105545231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3564600.0000, 
sim time next is 3565200.0000, 
raw observation next is [27.33333333333334, 79.0, 1.0, 2.0, 0.7723456710232763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1079426.465963131, 1079426.465963132, 237233.2042361728], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.79, 1.0, 1.0, 0.7257176759316583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29984068498975863, 0.2998406849897589, 0.3540794093077206], 
reward next is 0.6459, 
noisyNet noise sample is [array([0.14245957], dtype=float32), 0.6166883]. 
=============================================
[2019-03-27 01:43:58,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4816309e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1261035e-29], sum to 1.0000
[2019-03-27 01:43:58,323] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1623
[2019-03-27 01:43:58,327] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 67.5, 1.0, 2.0, 0.5757454453705877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804554.8916714948, 804554.8916714948, 196314.2795485684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3870600.0000, 
sim time next is 3871200.0000, 
raw observation next is [31.33333333333334, 68.0, 1.0, 2.0, 0.5677755353998064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793413.4662478459, 793413.4662478459, 194896.0658034905], 
processed observation next is [0.0, 0.8260869565217391, 0.6840442338072673, 0.68, 1.0, 1.0, 0.47924763301181494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22039262951329053, 0.22039262951329053, 0.2908896504529709], 
reward next is 0.7091, 
noisyNet noise sample is [array([1.0143955], dtype=float32), -1.6308779]. 
=============================================
[2019-03-27 01:44:09,859] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2568170e-26 1.0000000e+00 1.4260131e-35 4.3874730e-35 4.0002928e-23], sum to 1.0000
[2019-03-27 01:44:09,870] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7946
[2019-03-27 01:44:09,878] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5019401521394582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701384.2721856466, 701384.2721856473, 183900.7382228879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3631200.0000, 
sim time next is 3631800.0000, 
raw observation next is [27.16666666666666, 82.33333333333333, 1.0, 2.0, 0.5040955941213173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704397.1699210979, 704397.1699210979, 184240.4475573107], 
processed observation next is [1.0, 0.0, 0.4865718799368086, 0.8233333333333333, 1.0, 1.0, 0.4025248121943582, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19566588053363831, 0.19566588053363831, 0.2749857426228518], 
reward next is 0.7250, 
noisyNet noise sample is [array([1.1976163], dtype=float32), 0.31436607]. 
=============================================
[2019-03-27 01:44:17,140] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-27 01:44:17,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:44:17,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:44:17,145] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:44:17,146] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:44:17,147] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:44:17,147] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:44:17,148] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:44:17,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:44:17,150] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:44:17,151] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:44:17,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-27 01:44:17,197] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-27 01:44:17,198] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-27 01:44:17,216] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-27 01:44:17,260] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-27 01:44:29,609] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:44:29,612] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.06405606333334, 86.14180276333335, 1.0, 2.0, 0.243111112659327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403025.4980777298, 403025.4980777298, 160059.9971032358]
[2019-03-27 01:44:29,613] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:44:29,618] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9454142e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.7494585e-37], sampled 0.9124012388046313
[2019-03-27 01:44:37,603] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:44:37,605] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.32264789, 86.47138281833332, 1.0, 2.0, 0.589161805179133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823310.3702798533, 823310.3702798533, 198737.6452279965]
[2019-03-27 01:44:37,605] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:44:37,607] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0009335e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.2466835e-35], sampled 0.9795976255159656
[2019-03-27 01:44:42,178] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:44:42,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.13333333333333, 96.66666666666666, 1.0, 2.0, 0.4162418191892954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610666.0307506204, 610666.030750621, 175237.0200970593]
[2019-03-27 01:44:42,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:44:42,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2450431e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.8948042e-36], sampled 0.6108743670932674
[2019-03-27 01:44:55,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:44:55,204] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.23358952, 88.23744587, 1.0, 2.0, 0.4452412468385704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649172.7132535055, 649172.7132535055, 178902.6502310641]
[2019-03-27 01:44:55,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:44:55,209] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.5524456e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0625825e-34], sampled 0.20106847350542967
[2019-03-27 01:45:01,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:45:01,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.2083401, 95.41905055, 1.0, 2.0, 0.3167284858895643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498254.7333830953, 498254.7333830959, 166837.6992692497]
[2019-03-27 01:45:01,165] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:45:01,170] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.566784e-36 1.000000e+00 0.000000e+00 0.000000e+00 8.948609e-37], sampled 0.9065929856569923
[2019-03-27 01:45:19,172] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:45:19,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.56666666666667, 63.66666666666667, 1.0, 2.0, 0.6538733393732858, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.991333626396493, 6.9112, 168.911739312972, 1810583.240249418, 1753734.097358651, 375524.0994044775]
[2019-03-27 01:45:19,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:45:19,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5734153e-27 1.0000000e+00 2.4224647e-36 7.9782950e-38 1.9070172e-26], sampled 0.7134753477907846
[2019-03-27 01:45:19,178] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1810583.240249418 W.
[2019-03-27 01:45:48,926] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:45:48,927] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.90616692, 87.11778645999999, 1.0, 2.0, 0.472363133454874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674598.670995042, 674598.670995042, 181243.1447197357]
[2019-03-27 01:45:48,928] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:45:48,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9635831e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5509119e-37], sampled 0.09726788352957194
[2019-03-27 01:46:10,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05436487]
[2019-03-27 01:46:10,492] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.86666666666667, 77.0, 1.0, 2.0, 0.5229475161067949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730748.9337854412, 730748.9337854405, 187270.6914510821]
[2019-03-27 01:46:10,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:46:10,497] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0855703e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5639281e-36], sampled 0.9318608329651663
[2019-03-27 01:46:11,509] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4033 2927329517.0614 1341.0000
[2019-03-27 01:46:11,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5749 3007726976.5989 1766.0000
[2019-03-27 01:46:11,607] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.7182 2779380068.5600 935.0000
[2019-03-27 01:46:11,725] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.0944 3164264916.5760 1780.0000
[2019-03-27 01:46:11,892] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1364 2842542380.3122 1131.0000
[2019-03-27 01:46:12,908] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 950000, evaluation results [950000.0, 7883.094390385518, 3164264916.5760145, 1780.0, 8254.403345880972, 2927329517.0613775, 1341.0, 8658.718210274206, 2779380068.559973, 935.0, 7997.574940102971, 3007726976.5988917, 1766.0, 8496.136434165788, 2842542380.3122225, 1131.0]
[2019-03-27 01:46:14,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2687535e-11 4.6424270e-01 8.0913056e-20 6.1897763e-13 5.3575730e-01], sum to 1.0000
[2019-03-27 01:46:14,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3793
[2019-03-27 01:46:14,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2748185.600730329 W.
[2019-03-27 01:46:14,394] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.6687652325659209, 1.0, 2.0, 0.6549726557972231, 1.0, 2.0, 1.03, 7.005095269452898, 6.9112, 170.5573041426782, 2748185.600730329, 2680924.599805855, 511644.5537408188], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3775200.0000, 
sim time next is 3775800.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9523507366088714, 1.0, 2.0, 0.9523507366088714, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2663875.448310253, 2663875.448310253, 500860.0359513643], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.63, 1.0, 1.0, 0.942591248926351, 1.0, 1.0, 0.942591248926351, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7399654023084037, 0.7399654023084037, 0.7475522924647229], 
reward next is 0.2524, 
noisyNet noise sample is [array([-0.03796685], dtype=float32), -0.14193448]. 
=============================================
[2019-03-27 01:46:19,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6959864e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4713262e-37], sum to 1.0000
[2019-03-27 01:46:19,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4243
[2019-03-27 01:46:19,097] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.66666666666667, 1.0, 2.0, 0.6045622374870508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844839.9029176588, 844839.9029176594, 201599.4660853402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5995248817101846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837797.7203946159, 837797.7203946159, 200657.9150921838], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5174998574821501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23272158899850443, 0.23272158899850443, 0.2994894255107221], 
reward next is 0.7005, 
noisyNet noise sample is [array([-2.600505], dtype=float32), 0.8395019]. 
=============================================
[2019-03-27 01:46:22,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9944923e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6608758e-34], sum to 1.0000
[2019-03-27 01:46:22,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1407
[2019-03-27 01:46:22,236] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5446465093164412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761081.2257639385, 761081.2257639385, 190886.9765032092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4051200.0000, 
sim time next is 4051800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5447695422033689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761253.2117634193, 761253.21176342, 190907.8748926287], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45152956891972157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2114592254898387, 0.2114592254898389, 0.28493712670541593], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.03873272], dtype=float32), 1.0437286]. 
=============================================
[2019-03-27 01:46:24,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8454765e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1996788e-35], sum to 1.0000
[2019-03-27 01:46:24,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8774
[2019-03-27 01:46:24,688] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 61.5, 1.0, 2.0, 0.6090855167490602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851163.4504296836, 851163.4504296836, 202451.2057352744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3951000.0000, 
sim time next is 3951600.0000, 
raw observation next is [34.0, 61.0, 1.0, 2.0, 0.6050224737384846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845483.3117925103, 845483.3117925103, 201685.7757938854], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.61, 1.0, 1.0, 0.5241234623355236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23485647549791955, 0.23485647549791955, 0.301023545961023], 
reward next is 0.6990, 
noisyNet noise sample is [array([-0.16362168], dtype=float32), 0.51742744]. 
=============================================
[2019-03-27 01:46:32,124] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9443184e-29 1.0000000e+00 4.9746667e-38 0.0000000e+00 2.9728838e-27], sum to 1.0000
[2019-03-27 01:46:32,132] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3399
[2019-03-27 01:46:32,139] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8433435168777597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1178707.775708078, 1178707.775708079, 254828.3455030523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4075800.0000, 
sim time next is 4076400.0000, 
raw observation next is [27.13333333333334, 88.33333333333334, 1.0, 2.0, 0.7576638047186189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1058896.932026893, 1058896.932026892, 233784.456891426], 
processed observation next is [1.0, 0.17391304347826086, 0.4849921011058455, 0.8833333333333334, 1.0, 1.0, 0.7080286803838782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2941380366741369, 0.2941380366741367, 0.3489320252110836], 
reward next is 0.6511, 
noisyNet noise sample is [array([-0.9423069], dtype=float32), -1.1847944]. 
=============================================
[2019-03-27 01:46:36,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2729005e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6782861e-28], sum to 1.0000
[2019-03-27 01:46:36,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9290
[2019-03-27 01:46:36,487] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5832246196768941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815010.4062550204, 815010.4062550204, 197662.1721715487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5833665816437785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815208.863036261, 815208.8630362618, 197687.9080803803], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49803202607684155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2264469063989614, 0.2264469063989616, 0.2950565792244482], 
reward next is 0.7049, 
noisyNet noise sample is [array([-1.7527542], dtype=float32), -0.44236237]. 
=============================================
[2019-03-27 01:46:36,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.40051 ]
 [65.41807 ]
 [65.447586]
 [65.48042 ]
 [65.62111 ]], R is [[65.4405899 ]
 [65.49116516]
 [65.54141998]
 [65.5912323 ]
 [65.64039612]].
[2019-03-27 01:46:46,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.00576345e-26 1.00000000e+00 5.57930428e-35 1.11585814e-36
 2.04401179e-26], sum to 1.0000
[2019-03-27 01:46:46,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2781
[2019-03-27 01:46:46,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6239946049151847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872006.63247808, 872006.6324780794, 205301.6594285692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4316400.0000, 
sim time next is 4317000.0000, 
raw observation next is [31.0, 79.00000000000001, 1.0, 2.0, 0.6264043020017939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875375.4721404426, 875375.4721404426, 205768.4096034746], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.7900000000000001, 1.0, 1.0, 0.5498847012069805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24315985337234516, 0.24315985337234516, 0.3071170292589173], 
reward next is 0.6929, 
noisyNet noise sample is [array([0.80512065], dtype=float32), -0.8280368]. 
=============================================
[2019-03-27 01:46:46,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[56.056328]
 [56.086346]
 [56.22477 ]
 [56.332554]
 [56.42819 ]], R is [[56.06998825]
 [56.20286942]
 [56.33472061]
 [56.46554565]
 [56.59526062]].
[2019-03-27 01:46:49,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5360224e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:46:49,081] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7098
[2019-03-27 01:46:49,086] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 58.0, 1.0, 2.0, 0.5295253535408311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739943.7797659085, 739943.7797659079, 188351.7016867764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4553400.0000, 
sim time next is 4554000.0000, 
raw observation next is [32.0, 59.0, 1.0, 2.0, 0.5268508633309894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736205.227550024, 736205.2275500234, 187910.214314848], 
processed observation next is [0.0, 0.7391304347826086, 0.7156398104265403, 0.59, 1.0, 1.0, 0.4299407991939631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2045014520972289, 0.2045014520972287, 0.28046300644007166], 
reward next is 0.7195, 
noisyNet noise sample is [array([1.5444741], dtype=float32), 0.40804112]. 
=============================================
[2019-03-27 01:46:49,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.065285]
 [74.98446 ]
 [74.908165]
 [74.83051 ]
 [74.72563 ]], R is [[75.14022064]
 [75.10769653]
 [75.07481384]
 [75.04176331]
 [75.00852203]].
[2019-03-27 01:46:51,612] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0095563e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4689892e-37], sum to 1.0000
[2019-03-27 01:46:51,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4896
[2019-03-27 01:46:51,625] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 68.5, 1.0, 2.0, 0.5535190075115133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773484.0417379951, 773484.0417379951, 192406.5548702447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4462200.0000, 
sim time next is 4462800.0000, 
raw observation next is [31.33333333333334, 69.33333333333333, 1.0, 2.0, 0.567211280111965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792624.6778014009, 792624.6778014009, 194797.0437605354], 
processed observation next is [0.0, 0.6521739130434783, 0.6840442338072673, 0.6933333333333332, 1.0, 1.0, 0.4785678073638132, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22017352161150025, 0.22017352161150025, 0.29074185635900807], 
reward next is 0.7093, 
noisyNet noise sample is [array([1.5541422], dtype=float32), -0.5520481]. 
=============================================
[2019-03-27 01:46:57,240] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.4776264e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1607273e-37], sum to 1.0000
[2019-03-27 01:46:57,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6947
[2019-03-27 01:46:57,250] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5105756537851628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713455.1117163413, 713455.111716342, 185269.0972298737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5087838850569756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710950.5380309998, 710950.5380310004, 184983.2191573847], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.408173355490332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19748626056416663, 0.1974862605641668, 0.27609435695132045], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.74040014], dtype=float32), -0.26201838]. 
=============================================
[2019-03-27 01:47:00,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.38551046e-11 5.79323053e-01 1.87481675e-19 2.51570054e-12
 4.20677006e-01], sum to 1.0000
[2019-03-27 01:47:00,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5432
[2019-03-27 01:47:00,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2921187.0563236 W.
[2019-03-27 01:47:00,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.00000000000001, 1.0, 2.0, 0.7511314959405531, 1.0, 2.0, 0.696155787484539, 1.0, 2.0, 1.03, 7.005101764173279, 6.9112, 170.5573041426782, 2921187.0563236, 2853921.402966728, 537966.2796015325], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4630800.0000, 
sim time next is 4631400.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.7607889126020554, 1.0, 2.0, 0.7009844958152903, 1.0, 2.0, 1.03, 7.005102525794832, 6.9112, 170.5573041426782, 2941472.988204304, 2874206.789266934, 541217.5498072833], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.711793870604886, 1.0, 1.0, 0.6397403564039642, 1.0, 1.0, 1.0365853658536586, 0.009390252579483161, 0.0, 0.8375144448122397, 0.8170758300567511, 0.7983907747963706, 0.8077873877720647], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94678754], dtype=float32), 1.4841776]. 
=============================================
[2019-03-27 01:47:04,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9809072e-13 9.9999928e-01 1.3827388e-21 5.9182204e-18 6.9863069e-07], sum to 1.0000
[2019-03-27 01:47:04,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0148
[2019-03-27 01:47:04,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2526457.531456721 W.
[2019-03-27 01:47:04,671] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.66666666666667, 64.33333333333333, 1.0, 2.0, 0.9032726643857252, 1.0, 1.0, 0.9032726643857252, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2526457.531456721, 2526457.531456721, 473290.2890098817], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.8930291221143348, 1.0, 2.0, 0.8930291221143348, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2497777.662495086, 2497777.662495086, 467713.4044825866], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.63, 1.0, 1.0, 0.8711194242341384, 1.0, 1.0, 0.8711194242341384, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6938271284708573, 0.6938271284708573, 0.698079708182965], 
reward next is 0.3019, 
noisyNet noise sample is [array([-1.2177777], dtype=float32), -0.5063252]. 
=============================================
[2019-03-27 01:47:04,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[50.577545]
 [50.73904 ]
 [51.331005]
 [52.09534 ]
 [51.1399  ]], R is [[51.04466248]
 [50.82781601]
 [50.31953812]
 [49.8163414 ]
 [49.66835785]].
[2019-03-27 01:47:08,975] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 01:47:08,978] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:47:08,979] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:47:08,979] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:47:08,981] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:47:08,982] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:47:08,981] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:47:08,984] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:47:08,984] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:47:08,983] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:47:08,987] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:47:09,009] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-27 01:47:09,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-27 01:47:09,010] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-27 01:47:09,072] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-27 01:47:09,072] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-27 01:47:40,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:47:40,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.82678863333334, 92.22237474666667, 1.0, 2.0, 0.4185880379091796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614126.6117944198, 614126.6117944198, 175566.6458712416]
[2019-03-27 01:47:40,372] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:47:40,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3257746e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3108715e-35], sampled 0.46359935578178124
[2019-03-27 01:47:41,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:47:41,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 94.0, 1.0, 2.0, 0.385141236795483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580562.65067678, 580562.6506767794, 172939.5211629773]
[2019-03-27 01:47:41,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:47:41,883] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.42573175e-33 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.73678912e-34], sampled 0.6877293317637175
[2019-03-27 01:47:56,687] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:47:56,688] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.93333333333333, 68.66666666666667, 1.0, 2.0, 0.5622866577346118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785740.4335815274, 785740.4335815281, 193930.0113634485]
[2019-03-27 01:47:56,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:47:56,697] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0260421e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1241680e-33], sampled 0.06167366318993728
[2019-03-27 01:47:56,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:47:56,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 93.66666666666667, 1.0, 2.0, 0.4849802903213595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677677.9107701418, 677677.9107701418, 181276.2185934498]
[2019-03-27 01:47:56,971] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:47:56,976] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1342296e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8107938e-30], sampled 0.34572267468434126
[2019-03-27 01:48:24,272] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:48:24,275] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.85, 84.0, 1.0, 2.0, 0.6269183578444356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876094.1413264525, 876094.1413264531, 205867.3167055651]
[2019-03-27 01:48:24,276] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:48:24,279] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3097958e-27 1.0000000e+00 2.4825890e-37 7.9992349e-37 1.3065149e-25], sampled 0.5593935428485511
[2019-03-27 01:48:42,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:48:42,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 68.0, 1.0, 2.0, 1.00514446676464, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.980317742906834, 6.9112, 168.9124894433693, 2302202.665775848, 2253168.295695627, 465805.8456561858]
[2019-03-27 01:48:42,843] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:48:42,845] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8794605e-13 9.9985886e-01 3.8338441e-23 2.9219571e-16 1.4110074e-04], sampled 0.453734216842686
[2019-03-27 01:48:42,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2302202.665775848 W.
[2019-03-27 01:48:45,375] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:48:45,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.2, 89.0, 1.0, 2.0, 0.4877212567801325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683139.9349258507, 683139.9349258501, 181904.0188615768]
[2019-03-27 01:48:45,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:48:45,381] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6280715e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0467854e-30], sampled 0.055994493230334674
[2019-03-27 01:48:54,783] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:48:54,784] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.85683427, 69.569237195, 1.0, 2.0, 0.4222274299614619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619669.4204588145, 619669.4204588139, 176101.7698827869]
[2019-03-27 01:48:54,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:48:54,791] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7533723e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6439990e-30], sampled 0.5550255642611184
[2019-03-27 01:48:55,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.05629654]
[2019-03-27 01:48:55,981] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.948305525, 91.196775555, 1.0, 2.0, 0.2787405624546376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 453040.5371639717, 453040.5371639717, 163767.2931956947]
[2019-03-27 01:48:55,984] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:48:55,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.7764814e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8925202e-33], sampled 0.09708263565700848
[2019-03-27 01:49:03,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.8340 3007791457.7712 1760.0000
[2019-03-27 01:49:03,470] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.7808 2842390213.1241 1152.0000
[2019-03-27 01:49:03,517] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3102 2779509054.0238 932.0000
[2019-03-27 01:49:03,572] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.2613 2927834182.8292 1358.0000
[2019-03-27 01:49:03,675] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.9804 3163860794.9768 1828.0000
[2019-03-27 01:49:04,692] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 975000, evaluation results [975000.0, 7889.980390401488, 3163860794.9767585, 1828.0, 8249.261306066897, 2927834182.8291807, 1358.0, 8661.31016560208, 2779509054.023785, 932.0, 8000.83396909005, 3007791457.771154, 1760.0, 8493.780847201144, 2842390213.124077, 1152.0]
[2019-03-27 01:49:06,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0058546e-28 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2738013e-29], sum to 1.0000
[2019-03-27 01:49:06,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3853
[2019-03-27 01:49:06,620] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5176365907795337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 186406.7668424318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4730400.0000, 
sim time next is 4731000.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5197230506336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726241.6359888734, 726241.635988874, 186744.9725686347], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.4213530730525808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20173378777468706, 0.20173378777468723, 0.2787238396546787], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.29342586], dtype=float32), 0.59314144]. 
=============================================
[2019-03-27 01:49:06,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.20952 ]
 [67.5448  ]
 [67.676636]
 [66.10199 ]
 [63.421696]], R is [[67.0574646 ]
 [67.1086731 ]
 [67.15975189]
 [67.21091461]
 [67.26313782]].
[2019-03-27 01:49:07,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6610755e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4527437e-27], sum to 1.0000
[2019-03-27 01:49:07,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9987
[2019-03-27 01:49:07,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5176365907795337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 186406.7668424318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4730400.0000, 
sim time next is 4731000.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5197230506336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726241.6359888734, 726241.635988874, 186744.9725686347], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.4213530730525808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20173378777468706, 0.20173378777468723, 0.2787238396546787], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.7494535], dtype=float32), -0.93154514]. 
=============================================
[2019-03-27 01:49:07,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2371298e-25 1.0000000e+00 8.5662267e-37 1.2103380e-36 5.3642876e-23], sum to 1.0000
[2019-03-27 01:49:07,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.02602]
 [70.85989]
 [71.48363]
 [70.99486]
 [69.6558 ]], R is [[69.67973328]
 [69.70471191]
 [69.72983551]
 [69.75530243]
 [69.7820816 ]].
[2019-03-27 01:49:07,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2601
[2019-03-27 01:49:07,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5173717296712005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722954.869546444, 722954.8695464433, 186363.1535348479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5163743409950602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721560.684549907, 721560.6845499076, 186201.8923903371], 
processed observation next is [1.0, 0.8260869565217391, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.4173184831265785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20043352348608529, 0.20043352348608542, 0.2779132722243837], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.4965821], dtype=float32), 1.2798914]. 
=============================================
[2019-03-27 01:49:10,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3097353e-12 4.8955698e-03 4.6250216e-22 7.3357325e-16 9.9510443e-01], sum to 1.0000
[2019-03-27 01:49:10,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3181
[2019-03-27 01:49:10,679] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.8, 63.0, 1.0, 2.0, 0.5299491740930026, 1.0, 2.0, 0.5299491740930026, 1.0, 2.0, 0.9086157724859879, 6.9112, 6.9112, 170.5573041426782, 2223135.614649716, 2223135.614649716, 434200.3070319249], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4978800.0000, 
sim time next is 4979400.0000, 
raw observation next is [30.83333333333334, 63.0, 1.0, 2.0, 0.4947342357720285, 1.0, 2.0, 0.4947342357720285, 1.0, 2.0, 0.8482165501993406, 6.9112, 6.9112, 170.5573041426782, 2075267.377542447, 2075267.377542447, 409553.4863982103], 
processed observation next is [1.0, 0.6521739130434783, 0.6603475513428123, 0.63, 1.0, 1.0, 0.3912460671952151, 1.0, 1.0, 0.3912460671952151, 1.0, 1.0, 0.8148982319504154, 0.0, 0.0, 0.8375144448122397, 0.5764631604284575, 0.5764631604284575, 0.6112738602958362], 
reward next is 0.3887, 
noisyNet noise sample is [array([0.37532747], dtype=float32), -1.2589517]. 
=============================================
[2019-03-27 01:49:18,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.02560355e-20 1.00000000e+00 6.86049889e-32 6.31945378e-28
 2.12484906e-14], sum to 1.0000
[2019-03-27 01:49:18,619] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3557
[2019-03-27 01:49:18,625] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5086982487952844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710830.8339312291, 710830.8339312284, 184970.2242233995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5091368395051649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711443.9050444425, 711443.9050444419, 185040.0746899869], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4085986018134516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976233069567896, 0.19762330695678942, 0.27617921595520434], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.33127716], dtype=float32), 0.47083402]. 
=============================================
[2019-03-27 01:49:19,824] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3813193e-19 1.0000000e+00 1.9639340e-30 4.4970427e-26 2.6175402e-14], sum to 1.0000
[2019-03-27 01:49:19,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8339
[2019-03-27 01:49:19,838] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 82.33333333333334, 1.0, 2.0, 0.5009439659678925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699991.7963676837, 699991.7963676844, 183744.1116567507], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5002800.0000, 
sim time next is 5003400.0000, 
raw observation next is [27.0, 83.16666666666666, 1.0, 2.0, 0.504204808114684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704549.8305291543, 704549.8305291549, 184257.5937168088], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8316666666666666, 1.0, 1.0, 0.4026563953188964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1957082862580984, 0.19570828625809858, 0.2750113339056848], 
reward next is 0.7250, 
noisyNet noise sample is [array([1.6445882], dtype=float32), -0.47951734]. 
=============================================
[2019-03-27 01:49:21,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0135083e-26 1.0000000e+00 7.1284916e-38 2.6649246e-36 2.1360974e-21], sum to 1.0000
[2019-03-27 01:49:21,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3180
[2019-03-27 01:49:21,779] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.5203540726090252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727123.7043904559, 727123.7043904553, 186847.3022153944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4991400.0000, 
sim time next is 4992000.0000, 
raw observation next is [29.33333333333334, 72.66666666666666, 1.0, 2.0, 0.5178451930211949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723616.6939220176, 723616.693922017, 186440.0421537166], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494474, 0.7266666666666666, 1.0, 1.0, 0.4190905940014396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20100463720056044, 0.20100463720056028, 0.27826871963241284], 
reward next is 0.7217, 
noisyNet noise sample is [array([-1.4309423], dtype=float32), -0.25592953]. 
=============================================
[2019-03-27 01:49:21,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.117546]
 [70.90865 ]
 [70.76436 ]
 [70.666756]
 [70.40214 ]], R is [[71.1964035 ]
 [71.20556641]
 [71.21456146]
 [71.22422028]
 [71.23516083]].
[2019-03-27 01:49:32,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1670152e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5771006e-31], sum to 1.0000
[2019-03-27 01:49:32,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7890
[2019-03-27 01:49:32,402] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 65.5, 1.0, 2.0, 0.5437880349774372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759881.1769350927, 759881.1769350927, 190741.2701560888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5158200.0000, 
sim time next is 5158800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5419611494005354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757327.4045932451, 757327.4045932451, 190431.7740785881], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4481459631331752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21036872349812363, 0.21036872349812363, 0.2842265284755046], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.09022016], dtype=float32), -1.7312226]. 
=============================================
[2019-03-27 01:49:32,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.141520e-29 1.000000e+00 0.000000e+00 0.000000e+00 5.676013e-28], sum to 1.0000
[2019-03-27 01:49:32,760] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4924
[2019-03-27 01:49:32,763] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 64.5, 1.0, 2.0, 0.5469941449358731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764362.9584741134, 764362.9584741141, 191286.8474243076], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5157000.0000, 
sim time next is 5157600.0000, 
raw observation next is [31.33333333333333, 65.0, 1.0, 2.0, 0.5454243160662059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762168.5117103031, 762168.5117103037, 191019.3302135518], 
processed observation next is [0.0, 0.6956521739130435, 0.6840442338072668, 0.65, 1.0, 1.0, 0.4523184530918143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2117134754750842, 0.21171347547508437, 0.2851034779306743], 
reward next is 0.7149, 
noisyNet noise sample is [array([0.59180194], dtype=float32), -0.76994467]. 
=============================================
[2019-03-27 01:49:46,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7451900e-16 1.0000000e+00 1.6696524e-22 1.7074576e-19 8.0636371e-14], sum to 1.0000
[2019-03-27 01:49:46,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9900
[2019-03-27 01:49:46,087] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333334, 84.83333333333334, 1.0, 2.0, 0.9271896082850597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1295967.652439482, 1295967.652439482, 277531.9163570435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5379000.0000, 
sim time next is 5379600.0000, 
raw observation next is [29.86666666666667, 83.66666666666667, 1.0, 2.0, 1.00421400093909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1403698.707146698, 1403698.707146698, 300218.0830243568], 
processed observation next is [1.0, 0.2608695652173913, 0.6145339652448659, 0.8366666666666667, 1.0, 1.0, 1.0050771095651687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3899163075407494, 0.3899163075407494, 0.4480866910811296], 
reward next is 0.5519, 
noisyNet noise sample is [array([0.20587733], dtype=float32), 0.5759168]. 
=============================================
[2019-03-27 01:49:48,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4739594e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4770707e-35], sum to 1.0000
[2019-03-27 01:49:48,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3509
[2019-03-27 01:49:48,772] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 79.66666666666667, 1.0, 2.0, 0.6116229153945442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854710.751960657, 854710.751960657, 202932.1652566262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5425800.0000, 
sim time next is 5426400.0000, 
raw observation next is [30.8, 80.33333333333334, 1.0, 2.0, 0.6167525996765194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861882.12361638, 861882.12361638, 203909.3314807037], 
processed observation next is [1.0, 0.8260869565217391, 0.6587677725118484, 0.8033333333333335, 1.0, 1.0, 0.5382561441885776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23941170100455, 0.23941170100455, 0.30434228579209505], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.5040413], dtype=float32), -0.45613608]. 
=============================================
[2019-03-27 01:49:53,053] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7939559e-09 8.0032155e-02 7.5701952e-15 6.8908200e-08 9.1996771e-01], sum to 1.0000
[2019-03-27 01:49:53,060] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1549
[2019-03-27 01:49:53,065] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.2, 48.0, 1.0, 2.0, 0.7189992059158452, 1.0, 2.0, 0.6800896424721852, 1.0, 2.0, 1.03, 7.005099230273505, 6.9112, 170.5573041426782, 2853693.795285965, 2786429.957064575, 527396.7565135933], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5497200.0000, 
sim time next is 5497800.0000, 
raw observation next is [36.08333333333334, 48.16666666666666, 1.0, 2.0, 0.7607575573462702, 1.0, 2.0, 0.7009688181873978, 1.0, 2.0, 1.03, 7.005102523321994, 6.9112, 170.5573041426782, 2941407.124241546, 2874140.92707557, 541205.2525332378], 
processed observation next is [1.0, 0.6521739130434783, 0.9091627172195897, 0.4816666666666666, 1.0, 1.0, 0.7117560931882773, 1.0, 1.0, 0.63972146769566, 1.0, 1.0, 1.0365853658536586, 0.009390252332199367, 0.0, 0.8375144448122397, 0.8170575345115405, 0.7983724797432139, 0.8077690336316982], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.01329721], dtype=float32), -1.1911682]. 
=============================================
[2019-03-27 01:49:55,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2723665e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9217023e-30], sum to 1.0000
[2019-03-27 01:49:55,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5968
[2019-03-27 01:49:55,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.98333333333333, 75.5, 1.0, 2.0, 0.5346733109947537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747139.920075564, 747139.9200755634, 189206.8853414786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5685000.0000, 
sim time next is 5685600.0000, 
raw observation next is [28.76666666666667, 77.0, 1.0, 2.0, 0.5349609614399293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747542.0174871245, 747542.0174871251, 189254.9854873544], 
processed observation next is [0.0, 0.8260869565217391, 0.5624012638230649, 0.77, 1.0, 1.0, 0.4397120017348546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20765056041309016, 0.20765056041309032, 0.2824701275930663], 
reward next is 0.7175, 
noisyNet noise sample is [array([-2.9843888], dtype=float32), -2.3943903]. 
=============================================
[2019-03-27 01:49:57,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2316718e-14 9.9998534e-01 3.2174237e-20 2.0554061e-14 1.4699580e-05], sum to 1.0000
[2019-03-27 01:49:57,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2498
[2019-03-27 01:49:57,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2474346.790000189 W.
[2019-03-27 01:49:57,308] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 62.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.179108646719601, 6.9112, 168.9115098795759, 2474346.790000189, 2284284.803085942, 475514.8126854888], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5569200.0000, 
sim time next is 5569800.0000, 
raw observation next is [32.6, 61.16666666666667, 1.0, 2.0, 0.6475418627237979, 1.0, 1.0, 0.6443609708761615, 1.0, 2.0, 1.03, 7.005093596245261, 6.9112, 170.5573041426782, 2703612.082428034, 2636352.280090252, 505270.0193392304], 
processed observation next is [1.0, 0.4782608695652174, 0.7440758293838864, 0.6116666666666667, 1.0, 1.0, 0.575351641835901, 1.0, 0.5, 0.5715192420194717, 1.0, 1.0, 1.0365853658536586, 0.009389359624526073, 0.0, 0.8375144448122397, 0.7510033562300095, 0.7323200778028478, 0.7541343572227319], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9271027], dtype=float32), 0.4536737]. 
=============================================
[2019-03-27 01:49:58,429] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3012830e-24 1.0000000e+00 2.8797153e-34 5.3128998e-33 2.2894699e-21], sum to 1.0000
[2019-03-27 01:49:58,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2539
[2019-03-27 01:49:58,455] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.01666666666667, 88.16666666666667, 1.0, 2.0, 0.5691164941061998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795288.0327493323, 795288.032749333, 195133.373759114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5602200.0000, 
sim time next is 5602800.0000, 
raw observation next is [27.93333333333334, 88.33333333333334, 1.0, 2.0, 0.5678499471285307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793517.4885544695, 793517.4885544688, 194909.1621618885], 
processed observation next is [1.0, 0.8695652173913043, 0.5229067930489735, 0.8833333333333334, 1.0, 1.0, 0.4793372856970249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22042152459846376, 0.22042152459846356, 0.29090919725655], 
reward next is 0.7091, 
noisyNet noise sample is [array([1.2423288], dtype=float32), -0.5660299]. 
=============================================
[2019-03-27 01:49:59,116] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1607466e-22 1.0000000e+00 5.2320647e-34 1.8057700e-32 6.1246288e-20], sum to 1.0000
[2019-03-27 01:49:59,125] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6114
[2019-03-27 01:49:59,131] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 89.66666666666667, 1.0, 2.0, 0.5543166012770465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774599.0007298482, 774599.0007298482, 192543.4819928273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5607600.0000, 
sim time next is 5608200.0000, 
raw observation next is [27.18333333333334, 89.83333333333333, 1.0, 2.0, 0.5521531483489314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771574.7053655775, 771574.7053655769, 192170.3739393374], 
processed observation next is [1.0, 0.9130434782608695, 0.4873617693522911, 0.8983333333333333, 1.0, 1.0, 0.4604254799384715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21432630704599376, 0.2143263070459936, 0.2868214536408021], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.31579357], dtype=float32), -0.34463215]. 
=============================================
[2019-03-27 01:50:00,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7808219e-27 1.0000000e+00 2.7565519e-38 1.2985428e-36 8.0341552e-23], sum to 1.0000
[2019-03-27 01:50:00,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3758
[2019-03-27 01:50:00,459] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 91.16666666666667, 1.0, 2.0, 0.5242929619366326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732629.6620206813, 732629.6620206819, 187489.5288003953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5616600.0000, 
sim time next is 5617200.0000, 
raw observation next is [26.2, 91.33333333333334, 1.0, 2.0, 0.5227985089389646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730540.6446770929, 730540.6446770923, 187244.9733231312], 
processed observation next is [0.0, 0.0, 0.44075829383886256, 0.9133333333333334, 1.0, 1.0, 0.42505844450477664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20292795685474802, 0.20292795685474788, 0.2794701094375093], 
reward next is 0.7205, 
noisyNet noise sample is [array([1.6596745], dtype=float32), 0.25845742]. 
=============================================
[2019-03-27 01:50:00,804] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:50:00,807] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:50:00,807] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:50:00,809] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:50:00,808] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:50:00,809] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:50:00,810] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:50:00,810] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:50:00,812] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:50:00,817] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:50:00,817] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:50:01,146] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-27 01:50:01,221] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-27 01:50:01,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-27 01:50:01,616] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-27 01:50:01,616] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-27 01:50:04,848] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.060812492]
[2019-03-27 01:50:04,849] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.64897642833333, 94.19887882833335, 1.0, 2.0, 0.4822884617199318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726504.8409322551, 726504.8409322544, 187337.2926549951]
[2019-03-27 01:50:04,850] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:50:04,854] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.168731e-31 1.000000e+00 0.000000e+00 0.000000e+00 6.984055e-31], sampled 0.7658052031794736
[2019-03-27 01:50:59,912] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.060812492]
[2019-03-27 01:50:59,914] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.61422871, 45.29626548333333, 1.0, 2.0, 0.5766572622442334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805829.5609100732, 805829.5609100732, 196477.1561055433]
[2019-03-27 01:50:59,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:50:59,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4662206e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5720638e-29], sampled 0.7128570321420423
[2019-03-27 01:51:20,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.060812492]
[2019-03-27 01:51:20,610] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.40237575499999, 65.002013105, 1.0, 2.0, 0.5900006538185988, 1.0, 2.0, 0.5900006538185988, 1.0, 2.0, 1.024636019485138, 6.911199999999999, 6.9112, 171.5212843490159, 2475287.153799669, 2475287.153799669, 483170.2685608686]
[2019-03-27 01:51:20,611] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:51:20,613] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2975283e-13 4.8881458e-05 1.3689734e-22 7.6401884e-13 9.9995112e-01], sampled 0.5306240905455344
[2019-03-27 01:51:26,596] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.060812492]
[2019-03-27 01:51:26,597] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.87240108, 82.968333255, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.979304260894852, 6.9112, 168.912244363554, 1502103.319400632, 1453788.016455055, 311355.6432161162]
[2019-03-27 01:51:26,598] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:51:26,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.09553526e-19 1.00000000e+00 2.59536645e-30 4.15938622e-26
 1.03812236e-13], sampled 0.8827844541680554
[2019-03-27 01:51:35,551] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.060812492]
[2019-03-27 01:51:35,552] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.5, 87.0, 1.0, 2.0, 0.5072361923811978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708787.1439663708, 708787.1439663714, 184737.6452244647]
[2019-03-27 01:51:35,552] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:51:35,557] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1251159e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5002290e-28], sampled 0.6056556523362119
[2019-03-27 01:51:54,491] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7914.8018 3162964680.0559 1785.0000
[2019-03-27 01:51:54,965] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8669.0880 2779383492.7178 912.0000
[2019-03-27 01:51:55,012] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.5783 2927933501.6252 1343.0000
[2019-03-27 01:51:55,542] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8507.2237 2841651442.6696 1115.0000
[2019-03-27 01:51:55,606] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8020.5646 3006769376.0826 1705.0000
[2019-03-27 01:51:56,624] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1000000, evaluation results [1000000.0, 7914.80180904899, 3162964680.0558996, 1785.0, 8258.57834422328, 2927933501.6252213, 1343.0, 8669.088001330043, 2779383492.7177873, 912.0, 8020.564566545213, 3006769376.0825543, 1705.0, 8507.223666236781, 2841651442.669574, 1115.0]
[2019-03-27 01:52:07,339] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2186292e-13 9.9999905e-01 1.9435501e-21 3.0268556e-14 9.7030954e-07], sum to 1.0000
[2019-03-27 01:52:07,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5027
[2019-03-27 01:52:07,352] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.01666666666667, 78.83333333333333, 1.0, 2.0, 0.9980611619884882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1395092.566235079, 1395092.566235079, 298337.040611556], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5817000.0000, 
sim time next is 5817600.0000, 
raw observation next is [29.2, 78.0, 1.0, 2.0, 0.9195888788130633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1285337.397135478, 1285337.397135478, 275384.3548494923], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.78, 1.0, 1.0, 0.9031191311000762, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3570381658709661, 0.3570381658709661, 0.411021425148496], 
reward next is 0.5890, 
noisyNet noise sample is [array([1.293297], dtype=float32), 0.655574]. 
=============================================
[2019-03-27 01:52:21,747] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8159666e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0543633e-29], sum to 1.0000
[2019-03-27 01:52:21,759] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4047
[2019-03-27 01:52:21,763] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 90.5, 1.0, 2.0, 0.5249050761389132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733485.3054963945, 733485.3054963939, 187590.4168267733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6237000.0000, 
sim time next is 6237600.0000, 
raw observation next is [26.56666666666667, 90.33333333333334, 1.0, 2.0, 0.5244757669673655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732885.1958526161, 732885.1958526167, 187520.0216786226], 
processed observation next is [0.0, 0.17391304347826086, 0.45813586097946307, 0.9033333333333334, 1.0, 1.0, 0.4270792373100789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20357922107017115, 0.2035792210701713, 0.2798806293710785], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.7253161], dtype=float32), -0.9136954]. 
=============================================
[2019-03-27 01:52:22,279] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.58405469e-22 1.00000000e+00 2.31324784e-32 1.20045154e-29
 2.63951530e-17], sum to 1.0000
[2019-03-27 01:52:22,291] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3376
[2019-03-27 01:52:22,295] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 85.0, 1.0, 2.0, 0.5310323353892278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742050.3291968158, 742050.3291968158, 188601.2464162806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6206400.0000, 
sim time next is 6207000.0000, 
raw observation next is [27.46666666666667, 85.16666666666667, 1.0, 2.0, 0.5308337023828295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741772.6678518901, 741772.6678518901, 188568.29408851], 
processed observation next is [1.0, 0.8695652173913043, 0.500789889415482, 0.8516666666666667, 1.0, 1.0, 0.43473940046124027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2060479632921917, 0.2060479632921917, 0.2814452150574776], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.49462715], dtype=float32), 0.6136176]. 
=============================================
[2019-03-27 01:52:22,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.74556 ]
 [64.845795]
 [64.76443 ]
 [64.86547 ]
 [64.908875]], R is [[64.77494049]
 [64.8456955 ]
 [64.91591644]
 [64.9855957 ]
 [65.05459595]].
[2019-03-27 01:52:23,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4761385e-15 2.3978166e-09 4.6822021e-25 6.9005890e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 01:52:23,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7237
[2019-03-27 01:52:23,613] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.46666666666667, 68.33333333333334, 1.0, 2.0, 0.442867135215822, 1.0, 2.0, 0.442867135215822, 1.0, 2.0, 0.7651800556402427, 6.9112, 6.9112, 170.5573041426782, 1857511.245457109, 1857511.245457109, 377268.4515225323], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6085200.0000, 
sim time next is 6085800.0000, 
raw observation next is [30.6, 67.5, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.4468229917203047, 1.0, 2.0, 0.7717789893972947, 6.911200000000001, 6.9112, 170.5573041426782, 1874117.748292412, 1874117.748292412, 379653.5834832476], 
processed observation next is [1.0, 0.43478260869565216, 0.6492890995260664, 0.675, 1.0, 1.0, 0.33352167677145145, 1.0, 1.0, 0.33352167677145145, 1.0, 1.0, 0.7216816943869446, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5205882634145589, 0.5205882634145589, 0.5666471395272352], 
reward next is 0.4334, 
noisyNet noise sample is [array([-0.7787591], dtype=float32), -0.5837551]. 
=============================================
[2019-03-27 01:52:45,006] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.0480347e-16 5.6653727e-12 3.8419640e-26 2.3031212e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 01:52:45,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0456
[2019-03-27 01:52:45,024] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.5375706199019178, 1.0, 2.0, 0.5375706199019178, 1.0, 2.0, 0.9237836276276001, 6.911199999999999, 6.9112, 170.5573041426782, 2255136.408568065, 2255136.408568065, 440172.9947556773], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6445800.0000, 
sim time next is 6446400.0000, 
raw observation next is [30.0, 68.0, 1.0, 2.0, 0.5312933422370448, 1.0, 2.0, 0.5312933422370448, 1.0, 2.0, 0.9129567689489965, 6.911199999999999, 6.9112, 170.5573041426782, 2228779.424929157, 2228779.424929158, 435573.7265865364], 
processed observation next is [1.0, 0.6086956521739131, 0.6208530805687204, 0.68, 1.0, 1.0, 0.4352931834181263, 1.0, 1.0, 0.4352931834181263, 1.0, 1.0, 0.8938497182304833, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6191053958136546, 0.619105395813655, 0.6501100396813976], 
reward next is 0.3499, 
noisyNet noise sample is [array([0.67100966], dtype=float32), -1.417901]. 
=============================================
[2019-03-27 01:52:51,213] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.6142172e-19 1.0000000e+00 3.3780929e-31 5.6014629e-27 7.4641378e-13], sum to 1.0000
[2019-03-27 01:52:51,223] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5460
[2019-03-27 01:52:51,231] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.33333333333334, 1.0, 2.0, 0.5202701149958331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727006.3449525684, 727006.3449525678, 186833.0928271207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6565200.0000, 
sim time next is 6565800.0000, 
raw observation next is [26.95, 86.66666666666666, 1.0, 2.0, 0.5201588571803704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726850.8241783377, 726850.8241783377, 186814.997434339], 
processed observation next is [1.0, 1.0, 0.476303317535545, 0.8666666666666666, 1.0, 1.0, 0.42187814118116906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20190300671620492, 0.20190300671620492, 0.27882835437961045], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.7597502], dtype=float32), -1.0588331]. 
=============================================
[2019-03-27 01:52:52,088] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 01:52:52,089] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:52:52,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:52:52,090] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:52:52,090] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:52:52,092] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:52:52,092] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:52:52,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:52:52,094] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:52:52,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:52:52,098] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:52:52,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-27 01:52:52,138] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-27 01:52:52,139] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-27 01:52:52,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-27 01:52:52,176] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-27 01:52:57,196] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.06297776]
[2019-03-27 01:52:57,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.69583235, 86.73085308, 1.0, 2.0, 0.3319908374588775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515755.6874758613, 515755.6874758613, 168003.3864904708]
[2019-03-27 01:52:57,199] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:52:57,201] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3631246e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8148798e-31], sampled 0.9209815172769619
[2019-03-27 01:53:26,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.06297776]
[2019-03-27 01:53:26,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 92.0, 1.0, 2.0, 0.3978475809323833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594702.4704726158, 594702.4704726164, 174081.4000166552]
[2019-03-27 01:53:26,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:53:26,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9649869e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6326383e-28], sampled 0.47931953498115554
[2019-03-27 01:53:31,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.06297776]
[2019-03-27 01:53:31,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.5, 86.5, 1.0, 2.0, 0.4660689571230514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655977.7309307858, 655977.7309307863, 179061.9022285829]
[2019-03-27 01:53:31,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:53:31,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.5587496e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7011052e-28], sampled 0.5616244818102332
[2019-03-27 01:54:44,184] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8375.6199 2994694600.2589 873.0000
[2019-03-27 01:54:44,435] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8715.6670 2836426984.5122 644.0000
[2019-03-27 01:54:44,451] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8168.3979 3153800209.4421 1180.0000
[2019-03-27 01:54:44,471] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8801.2169 2779926224.6383 623.0000
[2019-03-27 01:54:44,499] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8459.3144 2927641785.7118 938.0000
[2019-03-27 01:54:45,513] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1025000, evaluation results [1025000.0, 8168.397864316711, 3153800209.4420896, 1180.0, 8459.31443841306, 2927641785.711812, 938.0, 8801.21692325202, 2779926224.638302, 623.0, 8375.619863794722, 2994694600.2589397, 873.0, 8715.667026975152, 2836426984.5122004, 644.0]
[2019-03-27 01:54:49,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6357900e-21 1.0000000e+00 2.6006361e-32 4.0440835e-30 7.6975390e-16], sum to 1.0000
[2019-03-27 01:54:49,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2026
[2019-03-27 01:54:49,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 85.0, 1.0, 2.0, 0.5196470576909038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726135.4099857152, 726135.4099857152, 186731.8383390548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6634800.0000, 
sim time next is 6635400.0000, 
raw observation next is [27.16666666666667, 85.00000000000001, 1.0, 2.0, 0.5183179903372933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724277.5877025435, 724277.587702544, 186516.119772264], 
processed observation next is [1.0, 0.8260869565217391, 0.4865718799368091, 0.8500000000000001, 1.0, 1.0, 0.4196602293220401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2011882188062621, 0.20118821880626223, 0.27838226831681195], 
reward next is 0.7216, 
noisyNet noise sample is [array([1.4347649], dtype=float32), -0.95802224]. 
=============================================
[2019-03-27 01:54:51,644] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6370514e-20 1.0000000e+00 2.3772284e-32 3.5008561e-27 5.7449491e-16], sum to 1.0000
[2019-03-27 01:54:51,657] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7312
[2019-03-27 01:54:51,666] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.01666666666667, 94.16666666666667, 1.0, 2.0, 0.5159751547977751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721002.6885459501, 721002.6885459494, 186135.2218646462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6673800.0000, 
sim time next is 6674400.0000, 
raw observation next is [25.1, 94.0, 1.0, 2.0, 0.5193463287648424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725715.0390567803, 725715.0390567797, 186680.9908676649], 
processed observation next is [1.0, 0.2608695652173913, 0.38862559241706174, 0.94, 1.0, 1.0, 0.4208991912829427, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20158751084910564, 0.20158751084910548, 0.2786283445786043], 
reward next is 0.7214, 
noisyNet noise sample is [array([-1.3982942], dtype=float32), -2.5100183]. 
=============================================
[2019-03-27 01:55:03,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2874813e-28 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.5130906e-29], sum to 1.0000
[2019-03-27 01:55:03,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8796
[2019-03-27 01:55:03,041] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 69.66666666666667, 1.0, 2.0, 0.3745161622185243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568896.106408441, 568896.106408441, 172037.1032857276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6857400.0000, 
sim time next is 6858000.0000, 
raw observation next is [26.1, 68.0, 1.0, 2.0, 0.3715438503338529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565620.250718045, 565620.250718045, 171788.4464033191], 
processed observation next is [0.0, 0.391304347826087, 0.4360189573459717, 0.68, 1.0, 1.0, 0.24282391606488302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15711673631056808, 0.15711673631056808, 0.2564006662736106], 
reward next is 0.7436, 
noisyNet noise sample is [array([-1.1065385], dtype=float32), 0.010559925]. 
=============================================
[2019-03-27 01:55:03,053] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.776924]
 [72.79534 ]
 [72.818695]
 [72.83849 ]
 [72.86843 ]], R is [[72.84895325]
 [72.86369324]
 [72.87792969]
 [72.891716  ]
 [72.90516663]].
[2019-03-27 01:55:04,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8235407e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6530317e-32], sum to 1.0000
[2019-03-27 01:55:04,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1555
[2019-03-27 01:55:04,213] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 44.0, 1.0, 2.0, 0.3057959642267482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483978.4716310585, 483978.4716310579, 165853.4802347937], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6883800.0000, 
sim time next is 6884400.0000, 
raw observation next is [29.33333333333334, 45.0, 1.0, 2.0, 0.31181910228717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491540.4331209147, 491540.4331209154, 166362.4966810736], 
processed observation next is [0.0, 0.6956521739130435, 0.5892575039494474, 0.45, 1.0, 1.0, 0.1708663882977952, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13653900920025408, 0.13653900920025427, 0.24830223385234865], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.92125744], dtype=float32), 1.024535]. 
=============================================
[2019-03-27 01:55:10,981] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4548643e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.9249527e-32], sum to 1.0000
[2019-03-27 01:55:10,993] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2627
[2019-03-27 01:55:10,999] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.88333333333334, 63.16666666666667, 1.0, 2.0, 0.399229534664357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591997.600673984, 591997.6006739847, 173689.046321372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6988200.0000, 
sim time next is 6988800.0000, 
raw observation next is [27.76666666666667, 64.33333333333334, 1.0, 2.0, 0.4067242851163128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601176.8986169929, 601176.8986169929, 174480.027484207], 
processed observation next is [0.0, 0.9130434782608695, 0.515007898894155, 0.6433333333333334, 1.0, 1.0, 0.28520998206784676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669935829491647, 0.1669935829491647, 0.2604179514689657], 
reward next is 0.7396, 
noisyNet noise sample is [array([0.6695293], dtype=float32), 0.73347723]. 
=============================================
[2019-03-27 01:55:14,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9815505e-09 3.0255880e-02 1.3834867e-18 1.8288067e-10 9.6974415e-01], sum to 1.0000
[2019-03-27 01:55:14,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5172
[2019-03-27 01:55:14,619] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.55, 48.0, 1.0, 2.0, 0.4503660248533642, 1.0, 2.0, 0.4503660248533642, 1.0, 2.0, 0.7511122946616033, 6.9112, 6.9112, 170.5573041426782, 1895161.797013133, 1895161.797013133, 377884.2198095841], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7050600.0000, 
sim time next is 7051200.0000, 
raw observation next is [31.33333333333334, 49.33333333333333, 1.0, 2.0, 0.4531673381948091, 1.0, 2.0, 0.4531673381948091, 1.0, 2.0, 0.754207030391472, 6.911200000000001, 6.9112, 170.5573041426782, 1900751.55909607, 1900751.559096069, 378770.9839472981], 
processed observation next is [1.0, 0.6086956521739131, 0.6840442338072673, 0.4933333333333333, 1.0, 1.0, 0.3411654677045893, 1.0, 1.0, 0.3411654677045893, 1.0, 1.0, 0.7002524760871608, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5279865441933528, 0.5279865441933526, 0.565329826787012], 
reward next is 0.4347, 
noisyNet noise sample is [array([1.8376399], dtype=float32), 2.1450427]. 
=============================================
[2019-03-27 01:55:17,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8250558e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9283115e-34], sum to 1.0000
[2019-03-27 01:55:17,060] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8019
[2019-03-27 01:55:17,065] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 84.5, 1.0, 2.0, 0.4783634998231688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668429.1565326115, 668429.1565326108, 180275.4326957296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7151400.0000, 
sim time next is 7152000.0000, 
raw observation next is [26.1, 84.33333333333334, 1.0, 2.0, 0.4779966033264941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667916.3218073163, 667916.3218073157, 180220.2853804287], 
processed observation next is [1.0, 0.782608695652174, 0.4360189573459717, 0.8433333333333334, 1.0, 1.0, 0.3710802449716796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1855323116131434, 0.18553231161314324, 0.268985500567804], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.3543759], dtype=float32), -0.52146554]. 
=============================================
[2019-03-27 01:55:17,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.39787]
 [71.65245]
 [70.87282]
 [70.34885]
 [69.06273]], R is [[73.11000061]
 [73.10983276]
 [73.10957336]
 [73.10962677]
 [73.10962677]].
[2019-03-27 01:55:33,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1618023e-22 1.0000000e+00 3.6972122e-30 1.2627605e-28 8.7837499e-18], sum to 1.0000
[2019-03-27 01:55:33,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9763
[2019-03-27 01:55:33,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1864187.214571576 W.
[2019-03-27 01:55:33,831] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 75.0, 1.0, 2.0, 0.6921809337150275, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.983401298569357, 6.9112, 168.9120098340928, 1864187.214571576, 1812965.415464854, 383783.4511042393], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7812000.0000, 
sim time next is 7812600.0000, 
raw observation next is [29.1, 74.5, 1.0, 2.0, 0.4600317892524957, 1.0, 1.0, 0.4600317892524957, 1.0, 2.0, 0.7886334481479886, 6.911200000000001, 6.9112, 170.5573041426782, 1929569.534705759, 1929569.534705758, 386906.6464361104], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.745, 1.0, 1.0, 0.3494358906656575, 1.0, 0.5, 0.3494358906656575, 1.0, 1.0, 0.7422359123755959, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5359915374182664, 0.5359915374182661, 0.5774726066210603], 
reward next is 0.4225, 
noisyNet noise sample is [array([-1.1809727], dtype=float32), 0.23103155]. 
=============================================
[2019-03-27 01:55:35,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.929579e-32 1.000000e+00 0.000000e+00 0.000000e+00 8.318314e-34], sum to 1.0000
[2019-03-27 01:55:35,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3636
[2019-03-27 01:55:35,561] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157617010465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.9532688753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7414800.0000, 
sim time next is 7415400.0000, 
raw observation next is [21.63333333333333, 82.33333333333334, 1.0, 2.0, 0.2874782655347496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462706.1998459483, 462706.1998459483, 164457.4963176636], 
processed observation next is [1.0, 0.8260869565217391, 0.2243285939968403, 0.8233333333333335, 1.0, 1.0, 0.14154007895752962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12852949995720786, 0.12852949995720786, 0.24545894972785612], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.22127563], dtype=float32), -0.6202849]. 
=============================================
[2019-03-27 01:55:36,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.788282e-32 1.000000e+00 0.000000e+00 0.000000e+00 1.182567e-28], sum to 1.0000
[2019-03-27 01:55:36,990] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0983
[2019-03-27 01:55:36,995] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 88.0, 1.0, 2.0, 0.5054856309470599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706340.1814283559, 706340.1814283559, 184460.1662099836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7860600.0000, 
sim time next is 7861200.0000, 
raw observation next is [26.33333333333333, 88.33333333333333, 1.0, 2.0, 0.5067662071800055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708130.1906442683, 708130.190644269, 184663.1231249614], 
processed observation next is [1.0, 1.0, 0.44707740916271704, 0.8833333333333333, 1.0, 1.0, 0.40574241828916324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19670283073451897, 0.19670283073451916, 0.27561660167904684], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.31275493], dtype=float32), -0.09348669]. 
=============================================
[2019-03-27 01:55:41,459] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 01:55:41,462] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:55:41,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:55:41,467] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:55:41,467] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:55:41,467] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:55:41,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:55:41,469] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:55:41,468] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:55:41,471] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:55:41,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:55:41,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-27 01:55:41,505] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-27 01:55:41,525] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-27 01:55:41,545] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-27 01:55:41,561] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-27 01:56:23,755] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0644372]
[2019-03-27 01:56:23,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.26666666666667, 79.5, 1.0, 2.0, 0.5421940198371844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757652.9298828989, 757652.9298828983, 190470.2042116122]
[2019-03-27 01:56:23,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:56:23,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9441075e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4858103e-33], sampled 0.05510822299969442
[2019-03-27 01:57:31,458] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.0644372]
[2019-03-27 01:57:31,459] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.3, 59.0, 1.0, 2.0, 0.5886482452190097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564974519, 822592.4300359931, 822592.4300359931, 198651.3658598256]
[2019-03-27 01:57:31,459] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:57:31,463] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1215886e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1127378e-29], sampled 0.9832967700358394
[2019-03-27 01:57:35,322] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.0275 3164327584.4075 1807.0000
[2019-03-27 01:57:35,557] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.5031 2927437009.9503 1343.0000
[2019-03-27 01:57:35,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8287 2779335416.0074 932.0000
[2019-03-27 01:57:36,013] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.0926 3007737950.1533 1771.0000
[2019-03-27 01:57:36,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.8104 2842513915.2528 1137.0000
[2019-03-27 01:57:37,038] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1050000, evaluation results [1050000.0, 7878.027509370262, 3164327584.407488, 1807.0, 8252.503053453893, 2927437009.9503016, 1343.0, 8659.828675440927, 2779335416.0073533, 932.0, 7997.092604204602, 3007737950.1533012, 1771.0, 8494.810395387924, 2842513915.2527623, 1137.0]
[2019-03-27 01:57:37,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8001538e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7407400e-29], sum to 1.0000
[2019-03-27 01:57:37,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4366
[2019-03-27 01:57:37,895] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 94.33333333333334, 1.0, 2.0, 0.5504400963028612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770515.9300809628, 770515.9300809623, 192043.8561128107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7702800.0000, 
sim time next is 7703400.0000, 
raw observation next is [24.51666666666667, 94.16666666666667, 1.0, 2.0, 0.5476631611930595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767961.4259707705, 767961.4259707705, 191739.8831565204], 
processed observation next is [1.0, 0.13043478260869565, 0.36097946287519767, 0.9416666666666668, 1.0, 1.0, 0.4550158568591078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21332261832521404, 0.21332261832521404, 0.2861789300843588], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.5666534], dtype=float32), -0.19819956]. 
=============================================
[2019-03-27 01:57:37,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1540431e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:57:37,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5881
[2019-03-27 01:57:37,913] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333333, 90.16666666666666, 1.0, 2.0, 0.3708038476026096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561413.998717409, 561413.9987174096, 171329.538400087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7534200.0000, 
sim time next is 7534800.0000, 
raw observation next is [23.0, 90.0, 1.0, 2.0, 0.3685575550617078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 558984.8518724481, 558984.8518724474, 171150.8208961421], 
processed observation next is [0.0, 0.21739130434782608, 0.28909952606635075, 0.9, 1.0, 1.0, 0.2392259699538648, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15527356996456892, 0.15527356996456873, 0.2554489864121524], 
reward next is 0.7446, 
noisyNet noise sample is [array([0.20592865], dtype=float32), 1.6759937]. 
=============================================
[2019-03-27 01:57:37,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4241285e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 01:57:38,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5592
[2019-03-27 01:57:38,011] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 91.33333333333334, 1.0, 2.0, 0.3856503646686761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577294.8021213635, 577294.8021213642, 172522.9614048233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7530000.0000, 
sim time next is 7530600.0000, 
raw observation next is [23.21666666666667, 91.16666666666667, 1.0, 2.0, 0.3839516273713466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575495.7134693565, 575495.7134693565, 172385.6155568287], 
processed observation next is [0.0, 0.13043478260869565, 0.29936808846761465, 0.9116666666666667, 1.0, 1.0, 0.2577730450257188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15985992040815458, 0.15985992040815458, 0.25729196351765476], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.4594135], dtype=float32), 0.47487515]. 
=============================================
[2019-03-27 01:57:39,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:39,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:39,239] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-27 01:57:41,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4782305e-16 1.0000000e+00 3.9612343e-24 2.9554585e-20 5.0489934e-10], sum to 1.0000
[2019-03-27 01:57:41,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0630
[2019-03-27 01:57:41,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2062326.121273072 W.
[2019-03-27 01:57:41,361] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.73333333333333, 67.33333333333334, 1.0, 2.0, 0.7374780996092971, 1.0, 2.0, 0.7374780996092971, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2062326.121273072, 2062326.121273072, 390573.9709580236], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7659600.0000, 
sim time next is 7660200.0000, 
raw observation next is [29.61666666666667, 68.16666666666666, 1.0, 2.0, 0.8744108552965462, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.976130008330124, 6.9112, 168.9125706543992, 2119219.387154019, 2073155.909751477, 428091.9680077057], 
processed observation next is [1.0, 0.6521739130434783, 0.6026856240126385, 0.6816666666666665, 1.0, 1.0, 0.8486877774657182, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006493000833012364, 0.0, 0.8294380504225408, 0.5886720519872275, 0.5758766415976325, 0.6389432358323965], 
reward next is 0.0364, 
noisyNet noise sample is [array([-0.5400899], dtype=float32), 0.8513311]. 
=============================================
[2019-03-27 01:57:43,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4812589e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.7602624e-32], sum to 1.0000
[2019-03-27 01:57:43,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6437
[2019-03-27 01:57:43,858] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 89.33333333333334, 1.0, 2.0, 0.5538310938006488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780730.8434152335, 780730.843415233, 193333.0884085688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7632600.0000, 
sim time next is 7633200.0000, 
raw observation next is [25.33333333333334, 87.66666666666667, 1.0, 2.0, 0.7881605814381655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1109177.750850785, 1109177.750850785, 242093.9954172774], 
processed observation next is [1.0, 0.34782608695652173, 0.3996840442338076, 0.8766666666666667, 1.0, 1.0, 0.7447717848652596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3081049307918847, 0.3081049307918847, 0.36133432151832445], 
reward next is 0.6387, 
noisyNet noise sample is [array([0.8392969], dtype=float32), -1.0024395]. 
=============================================
[2019-03-27 01:57:47,210] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:47,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:47,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-27 01:57:48,885] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1055520: loss 1.3761
[2019-03-27 01:57:48,887] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1055520: learning rate 0.0001
[2019-03-27 01:57:49,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9249236e-16 1.0000000e+00 5.0015103e-25 2.8595065e-20 1.1977690e-10], sum to 1.0000
[2019-03-27 01:57:49,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1981
[2019-03-27 01:57:49,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2003619.961902207 W.
[2019-03-27 01:57:49,583] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.00000000000001, 59.33333333333334, 1.0, 2.0, 0.4776697780446216, 1.0, 1.0, 0.4776697780446216, 1.0, 2.0, 0.8224292988053319, 6.9112, 6.9112, 170.5573041426782, 2003619.961902207, 2003619.961902207, 398822.5774819896], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7741200.0000, 
sim time next is 7741800.0000, 
raw observation next is [31.9, 59.5, 1.0, 2.0, 0.7048409383560369, 1.0, 2.0, 0.7048409383560369, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1970973.730938143, 1970973.730938144, 376200.964470483], 
processed observation next is [1.0, 0.6086956521739131, 0.7109004739336492, 0.595, 1.0, 1.0, 0.6443866727181167, 1.0, 1.0, 0.6443866727181167, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.547492703038373, 0.5474927030383734, 0.5614939768216164], 
reward next is 0.4385, 
noisyNet noise sample is [array([-0.42224932], dtype=float32), 1.1635706]. 
=============================================
[2019-03-27 01:57:53,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:53,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:53,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-27 01:57:55,196] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:55,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:55,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-27 01:57:56,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5685108e-15 1.0000000e+00 8.4330217e-25 8.9278870e-20 5.0831954e-08], sum to 1.0000
[2019-03-27 01:57:56,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9122
[2019-03-27 01:57:56,081] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.11666666666667, 71.16666666666667, 1.0, 2.0, 0.7830654284616919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1208839.045687737, 1208839.045687737, 254667.3487820172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 36600.0000, 
sim time next is 37200.0000, 
raw observation next is [25.33333333333334, 70.33333333333334, 1.0, 2.0, 0.7384114937585701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1137441.175536041, 1137441.175536041, 242812.8783525481], 
processed observation next is [1.0, 0.43478260869565216, 0.3996840442338076, 0.7033333333333335, 1.0, 1.0, 0.6848331250103253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3159558820933447, 0.3159558820933447, 0.36240728112320614], 
reward next is 0.6376, 
noisyNet noise sample is [array([-0.07041474], dtype=float32), -0.11005236]. 
=============================================
[2019-03-27 01:57:56,377] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1059096: loss 4.8376
[2019-03-27 01:57:56,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1059097: learning rate 0.0001
[2019-03-27 01:57:58,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:58,519] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:58,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-27 01:57:59,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:57:59,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:57:59,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-27 01:58:00,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4795352e-27 1.0000000e+00 0.0000000e+00 2.9893923e-37 2.0153137e-23], sum to 1.0000
[2019-03-27 01:58:00,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3429
[2019-03-27 01:58:00,097] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.0, 1.0, 2.0, 0.5236008400448076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731662.1806736994, 731662.1806736994, 187376.9230011358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7934400.0000, 
sim time next is 7935000.0000, 
raw observation next is [27.9, 81.66666666666667, 1.0, 2.0, 0.5235415690042754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731579.3287968005, 731579.3287968005, 187367.2348529376], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.8166666666666668, 1.0, 1.0, 0.4259536975955125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20321648022133346, 0.20321648022133346, 0.2796525893327427], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.15744601], dtype=float32), 0.17356138]. 
=============================================
[2019-03-27 01:58:00,110] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.62882 ]
 [72.3547  ]
 [71.9056  ]
 [71.89018 ]
 [71.579895]], R is [[73.02659607]
 [73.0166626 ]
 [73.00682068]
 [72.99710083]
 [72.98776245]].
[2019-03-27 01:58:00,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2005447e-24 1.0000000e+00 1.7608352e-30 1.9433548e-33 1.6853211e-25], sum to 1.0000
[2019-03-27 01:58:00,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4307
[2019-03-27 01:58:00,522] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 84.66666666666667, 1.0, 2.0, 0.2663237023039255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 434981.5357246883, 434981.5357246888, 162532.3416770432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6000.0000, 
sim time next is 6600.0000, 
raw observation next is [20.53333333333333, 84.83333333333334, 1.0, 2.0, 0.2679605921680541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 437087.9319102297, 437087.9319102297, 162685.9713376667], 
processed observation next is [1.0, 0.043478260869565216, 0.17219589257503945, 0.8483333333333334, 1.0, 1.0, 0.11802480984102906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12141331441950824, 0.12141331441950824, 0.2428148825935324], 
reward next is 0.7572, 
noisyNet noise sample is [array([-0.41107816], dtype=float32), -1.185929]. 
=============================================
[2019-03-27 01:58:01,269] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1061553: loss 0.5223
[2019-03-27 01:58:01,271] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1061553: learning rate 0.0001
[2019-03-27 01:58:01,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,410] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-27 01:58:01,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,494] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-27 01:58:01,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-27 01:58:01,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-27 01:58:01,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-27 01:58:01,787] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:01,789] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:01,791] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-27 01:58:01,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-27 01:58:02,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:02,002] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:02,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-27 01:58:02,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:02,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:02,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-27 01:58:02,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 01:58:02,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:02,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-27 01:58:02,497] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1061970: loss 0.1673
[2019-03-27 01:58:02,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1061970: learning rate 0.0001
[2019-03-27 01:58:02,735] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1062038: loss 0.0181
[2019-03-27 01:58:02,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1062040: learning rate 0.0001
[2019-03-27 01:58:05,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1063298: loss 0.2069
[2019-03-27 01:58:05,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1063298: learning rate 0.0001
[2019-03-27 01:58:06,650] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1063916: loss 1.0444
[2019-03-27 01:58:06,651] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1063916: learning rate 0.0001
[2019-03-27 01:58:10,438] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1065609: loss 0.0562
[2019-03-27 01:58:10,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1065613: learning rate 0.0001
[2019-03-27 01:58:11,274] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1065984: loss 0.2435
[2019-03-27 01:58:11,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1065985: learning rate 0.0001
[2019-03-27 01:58:11,371] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1066026: loss 0.0382
[2019-03-27 01:58:11,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1066026: learning rate 0.0001
[2019-03-27 01:58:11,537] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1066100: loss 0.1179
[2019-03-27 01:58:11,542] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1066102: learning rate 0.0001
[2019-03-27 01:58:11,726] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066182: loss 0.0594
[2019-03-27 01:58:11,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066183: learning rate 0.0001
[2019-03-27 01:58:11,957] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1066285: loss 0.1204
[2019-03-27 01:58:11,960] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1066285: learning rate 0.0001
[2019-03-27 01:58:12,282] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1066429: loss 0.0284
[2019-03-27 01:58:12,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1066429: learning rate 0.0001
[2019-03-27 01:58:12,363] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1066463: loss 0.0779
[2019-03-27 01:58:12,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1066463: learning rate 0.0001
[2019-03-27 01:58:12,611] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1066571: loss 0.1491
[2019-03-27 01:58:12,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1066571: learning rate 0.0001
[2019-03-27 01:58:12,691] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1066607: loss 0.0120
[2019-03-27 01:58:12,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1066611: learning rate 0.0001
[2019-03-27 01:58:12,714] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1066618: loss 0.1011
[2019-03-27 01:58:12,715] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1066619: learning rate 0.0001
[2019-03-27 01:58:16,730] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1068412: loss 0.0548
[2019-03-27 01:58:16,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1068414: learning rate 0.0001
[2019-03-27 01:58:19,329] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1069581: loss 0.0329
[2019-03-27 01:58:19,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1069583: learning rate 0.0001
[2019-03-27 01:58:19,826] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1069802: loss 0.0879
[2019-03-27 01:58:19,828] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1069802: learning rate 0.0001
[2019-03-27 01:58:23,024] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1071225: loss 0.0066
[2019-03-27 01:58:23,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1071226: learning rate 0.0001
[2019-03-27 01:58:24,629] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1071944: loss 0.0022
[2019-03-27 01:58:24,632] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1071944: learning rate 0.0001
[2019-03-27 01:58:28,327] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1073596: loss 0.1825
[2019-03-27 01:58:28,335] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1073596: learning rate 0.0001
[2019-03-27 01:58:29,160] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1073967: loss 0.0584
[2019-03-27 01:58:29,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1073968: learning rate 0.0001
[2019-03-27 01:58:29,257] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1074011: loss 0.0402
[2019-03-27 01:58:29,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1074012: learning rate 0.0001
[2019-03-27 01:58:29,422] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1074084: loss 0.0008
[2019-03-27 01:58:29,425] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1074085: learning rate 0.0001
[2019-03-27 01:58:29,709] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1074211: loss 0.0010
[2019-03-27 01:58:29,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1074212: learning rate 0.0001
[2019-03-27 01:58:29,723] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074218: loss 0.0013
[2019-03-27 01:58:29,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074218: learning rate 0.0001
[2019-03-27 01:58:30,151] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1074413: loss 0.0030
[2019-03-27 01:58:30,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1074413: learning rate 0.0001
[2019-03-27 01:58:30,317] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1074485: loss 0.0026
[2019-03-27 01:58:30,319] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1074487: learning rate 0.0001
[2019-03-27 01:58:30,485] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1074562: loss 0.0044
[2019-03-27 01:58:30,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1074562: learning rate 0.0001
[2019-03-27 01:58:30,617] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1074619: loss 0.0090
[2019-03-27 01:58:30,619] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1074619: learning rate 0.0001
[2019-03-27 01:58:30,732] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1074673: loss 0.0009
[2019-03-27 01:58:30,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1074673: learning rate 0.0001
[2019-03-27 01:58:31,468] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 01:58:31,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 01:58:31,472] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:31,472] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 01:58:31,474] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 01:58:31,476] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 01:58:31,479] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:31,479] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:31,480] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:31,478] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 01:58:31,483] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 01:58:31,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-27 01:58:31,526] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-27 01:58:31,549] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-27 01:58:31,550] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-27 01:58:31,568] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-27 01:59:00,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:00,371] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.50970182833333, 91.16374566166668, 1.0, 2.0, 0.4228576450027613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627276.3205473637, 627276.3205473637, 177005.9573991395]
[2019-03-27 01:59:00,372] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 01:59:00,375] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.8845942e-24 1.0000000e+00 4.2544781e-36 5.8199877e-32 1.9268942e-17], sampled 0.34677007960000905
[2019-03-27 01:59:07,052] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:07,053] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.36666666666667, 72.5, 1.0, 2.0, 0.8230301068088046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1150301.149550287, 1150301.149550286, 249641.8106628332]
[2019-03-27 01:59:07,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:59:07,058] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0548872e-18 1.0000000e+00 2.1941050e-30 3.5785677e-24 1.4988878e-09], sampled 0.1558581873315802
[2019-03-27 01:59:11,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:11,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.7, 68.0, 1.0, 2.0, 0.5531822676211129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773013.312270568, 773013.312270568, 192347.5102186444]
[2019-03-27 01:59:11,094] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:59:11,097] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.67998232e-23 1.00000000e+00 1.00712186e-35 2.27498030e-31
 7.01759451e-17], sampled 0.9500555788989499
[2019-03-27 01:59:13,173] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:13,174] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754]
[2019-03-27 01:59:13,176] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:59:13,180] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4671763e-28 1.0000000e+00 0.0000000e+00 8.3884007e-38 3.7445744e-24], sampled 0.1860948653240858
[2019-03-27 01:59:25,010] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:25,012] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.94143034, 75.46994642666667, 1.0, 2.0, 0.5609826736823385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783917.5714018292, 783917.5714018292, 193702.5396902701]
[2019-03-27 01:59:25,012] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 01:59:25,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.7366093e-23 1.0000000e+00 4.6423400e-35 2.0037695e-30 8.2335928e-16], sampled 0.4198355204926947
[2019-03-27 01:59:27,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:27,240] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.63333333333333, 75.33333333333333, 1.0, 2.0, 0.5624462121678426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785963.4777372724, 785963.4777372724, 193957.6653991432]
[2019-03-27 01:59:27,240] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 01:59:27,244] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8799650e-21 1.0000000e+00 1.9845586e-33 3.6168919e-28 1.1604464e-13], sampled 0.7811319910073501
[2019-03-27 01:59:44,912] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:44,913] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 84.0, 1.0, 2.0, 0.4792585827153151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669680.2722406677, 669680.2722406677, 180409.6123663083]
[2019-03-27 01:59:44,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 01:59:44,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0204320e-23 1.0000000e+00 2.8634905e-35 6.0559203e-31 1.1449360e-16], sampled 0.16878613329229453
[2019-03-27 01:59:58,795] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 01:59:58,797] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 79.33333333333334, 1.0, 2.0, 0.3650338036101939, 1.0, 2.0, 0.3650338036101939, 1.0, 2.0, 0.6339429983473657, 6.911199999999999, 6.9112, 169.0403247858759, 1530832.662225645, 1530832.662225645, 334083.2163779663]
[2019-03-27 01:59:58,797] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 01:59:58,799] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0381994e-15 6.7389574e-06 4.0199450e-27 2.3051030e-16 9.9999321e-01], sampled 0.7938910867799518
[2019-03-27 02:00:00,696] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 02:00:00,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.59608553666667, 81.56003492333333, 1.0, 2.0, 0.5675927593417714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793157.958356005, 793157.9583560043, 194857.8822613218]
[2019-03-27 02:00:00,699] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:00:00,701] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.943406e-22 1.000000e+00 2.821669e-34 7.357810e-29 7.054147e-14], sampled 0.0305313312796216
[2019-03-27 02:00:12,024] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.07888929]
[2019-03-27 02:00:12,025] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.36666666666667, 95.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.2525875262042, 6.9112, 168.9105839836309, 1696109.461493865, 1453920.802547175, 311348.5398902586]
[2019-03-27 02:00:12,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:00:12,033] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7400674e-16 9.9999118e-01 5.7029622e-28 1.6677553e-20 8.7998924e-06], sampled 0.6855342216796096
[2019-03-27 02:00:12,035] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1696109.461493865 W.
[2019-03-27 02:00:25,637] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8530.8745 2928487726.6762 759.0000
[2019-03-27 02:00:25,906] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8764.6613 2836838843.7667 526.0000
[2019-03-27 02:00:25,907] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8851.1231 2781816225.4634 493.0000
[2019-03-27 02:00:26,018] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8467.9609 2992105144.3917 662.0000
[2019-03-27 02:00:26,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8233.8866 3151911067.7717 996.0000
[2019-03-27 02:00:27,095] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1075000, evaluation results [1075000.0, 8233.886599169866, 3151911067.7717133, 996.0, 8530.87449576463, 2928487726.6761627, 759.0, 8851.12308202148, 2781816225.4634285, 493.0, 8467.960928218332, 2992105144.3916683, 662.0, 8764.661285460126, 2836838843.766696, 526.0]
[2019-03-27 02:00:30,264] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1076428: loss 0.1065
[2019-03-27 02:00:30,266] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1076430: learning rate 0.0001
[2019-03-27 02:00:31,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7538733e-22 1.0000000e+00 9.2859113e-34 1.6115559e-29 2.5179251e-15], sum to 1.0000
[2019-03-27 02:00:31,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6109
[2019-03-27 02:00:31,711] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 70.5, 1.0, 2.0, 0.4084047101630429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671773.8923314483, 671773.8923314483, 180715.5563780493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 552600.0000, 
sim time next is 553200.0000, 
raw observation next is [22.0, 69.0, 1.0, 2.0, 0.4610482251419944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758572.5729980959, 758572.5729980959, 189053.0230014907], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.69, 1.0, 1.0, 0.3506605122192703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2107146036105822, 0.2107146036105822, 0.28216869104700104], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.671368], dtype=float32), -0.97237664]. 
=============================================
[2019-03-27 02:00:32,712] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1077522: loss 0.0206
[2019-03-27 02:00:32,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1077524: learning rate 0.0001
[2019-03-27 02:00:33,290] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1077786: loss 0.0182
[2019-03-27 02:00:33,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1077786: learning rate 0.0001
[2019-03-27 02:00:36,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.352331e-31 1.000000e+00 0.000000e+00 0.000000e+00 8.305783e-28], sum to 1.0000
[2019-03-27 02:00:36,267] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6939
[2019-03-27 02:00:36,273] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 65.0, 1.0, 2.0, 0.291832647351383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466766.9952571982, 466766.9952571976, 164717.0981647818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 814800.0000, 
sim time next is 815400.0000, 
raw observation next is [24.7, 64.0, 1.0, 2.0, 0.2917469936813316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 466636.1393664615, 466636.1393664609, 164708.0811510054], 
processed observation next is [0.0, 0.43478260869565216, 0.3696682464454976, 0.64, 1.0, 1.0, 0.14668312491726698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12962114982401707, 0.12962114982401693, 0.2458329569417991], 
reward next is 0.7542, 
noisyNet noise sample is [array([-1.8970221], dtype=float32), -2.5631752]. 
=============================================
[2019-03-27 02:00:36,646] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1079280: loss 0.0306
[2019-03-27 02:00:36,652] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1079283: learning rate 0.0001
[2019-03-27 02:00:38,291] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1080020: loss 0.0459
[2019-03-27 02:00:38,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1080021: learning rate 0.0001
[2019-03-27 02:00:38,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8187177e-22 1.0000000e+00 5.4956795e-34 1.2134589e-28 1.0864431e-14], sum to 1.0000
[2019-03-27 02:00:38,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0454
[2019-03-27 02:00:38,646] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.61666666666667, 59.0, 1.0, 2.0, 0.2432724582250208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 400281.1823143181, 400281.1823143174, 160248.1785644904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 669000.0000, 
sim time next is 669600.0000, 
raw observation next is [23.4, 60.0, 1.0, 2.0, 0.246842079243286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 406347.7311477184, 406347.7311477184, 160589.7111319035], 
processed observation next is [1.0, 0.782608695652174, 0.30805687203791465, 0.6, 1.0, 1.0, 0.0925808183654048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1128743697632551, 0.1128743697632551, 0.2396861360177664], 
reward next is 0.7603, 
noisyNet noise sample is [array([0.49779034], dtype=float32), -1.3408507]. 
=============================================
[2019-03-27 02:00:41,745] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1081562: loss 0.0148
[2019-03-27 02:00:41,746] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1081562: learning rate 0.0001
[2019-03-27 02:00:42,602] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1081943: loss 0.0049
[2019-03-27 02:00:42,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1081944: learning rate 0.0001
[2019-03-27 02:00:42,782] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1082023: loss 0.0027
[2019-03-27 02:00:42,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1082024: learning rate 0.0001
[2019-03-27 02:00:43,000] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1082118: loss 0.0068
[2019-03-27 02:00:43,002] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1082118: learning rate 0.0001
[2019-03-27 02:00:43,210] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1082213: loss 0.0054
[2019-03-27 02:00:43,212] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1082213: learning rate 0.0001
[2019-03-27 02:00:43,379] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082290: loss 0.0665
[2019-03-27 02:00:43,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082291: learning rate 0.0001
[2019-03-27 02:00:43,642] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1082408: loss 0.0413
[2019-03-27 02:00:43,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1082410: learning rate 0.0001
[2019-03-27 02:00:43,807] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1082481: loss 0.0054
[2019-03-27 02:00:43,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1082482: learning rate 0.0001
[2019-03-27 02:00:44,041] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1082587: loss 0.0373
[2019-03-27 02:00:44,043] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1082587: learning rate 0.0001
[2019-03-27 02:00:44,061] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1082594: loss 0.0393
[2019-03-27 02:00:44,065] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1082594: learning rate 0.0001
[2019-03-27 02:00:44,149] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1082630: loss 0.1145
[2019-03-27 02:00:44,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1082630: learning rate 0.0001
[2019-03-27 02:00:48,060] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1084379: loss 0.0113
[2019-03-27 02:00:48,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1084379: learning rate 0.0001
[2019-03-27 02:00:50,552] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1085495: loss 0.0008
[2019-03-27 02:00:50,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1085495: learning rate 0.0001
[2019-03-27 02:00:51,227] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1085802: loss 0.0085
[2019-03-27 02:00:51,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1085803: learning rate 0.0001
[2019-03-27 02:00:54,394] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1087214: loss 0.0464
[2019-03-27 02:00:54,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1087216: learning rate 0.0001
[2019-03-27 02:00:56,165] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1088000: loss 0.0203
[2019-03-27 02:00:56,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1088000: learning rate 0.0001
[2019-03-27 02:00:57,883] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6906888e-21 1.0000000e+00 3.3907678e-33 7.9233779e-28 3.1570070e-14], sum to 1.0000
[2019-03-27 02:00:57,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4094
[2019-03-27 02:00:57,898] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.46666666666667, 81.33333333333334, 1.0, 2.0, 0.3105481130154208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492629.4037627461, 492629.4037627461, 166507.4942497036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1113600.0000, 
sim time next is 1114200.0000, 
raw observation next is [22.35, 82.0, 1.0, 2.0, 0.3103949106922713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492663.1602369188, 492663.1602369188, 166514.7629283872], 
processed observation next is [1.0, 0.9130434782608695, 0.25829383886255936, 0.82, 1.0, 1.0, 0.16915049480996544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13685087784358854, 0.13685087784358854, 0.2485294969080406], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.3517329], dtype=float32), 0.73111475]. 
=============================================
[2019-03-27 02:00:59,563] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1089524: loss 0.0328
[2019-03-27 02:00:59,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1089524: learning rate 0.0001
[2019-03-27 02:01:00,134] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2263785e-22 1.0000000e+00 3.8550119e-35 1.3247746e-30 2.3266388e-16], sum to 1.0000
[2019-03-27 02:01:00,141] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5268
[2019-03-27 02:01:00,148] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 96.16666666666666, 1.0, 2.0, 0.3827724543523893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574503.0928234248, 574503.0928234248, 172321.7144258202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1043400.0000, 
sim time next is 1044000.0000, 
raw observation next is [22.6, 96.0, 1.0, 2.0, 0.3844018335886054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576693.127541197, 576693.1275411964, 172509.1392122945], 
processed observation next is [1.0, 0.08695652173913043, 0.27014218009478685, 0.96, 1.0, 1.0, 0.2583154621549463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16019253542811027, 0.16019253542811013, 0.2574763271825291], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.9469812], dtype=float32), -0.64095485]. 
=============================================
[2019-03-27 02:01:00,167] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.19411 ]
 [75.433655]
 [75.6828  ]
 [75.90862 ]
 [75.95181 ]], R is [[75.13733673]
 [75.12876892]
 [75.12046814]
 [75.11226654]
 [75.10414886]].
[2019-03-27 02:01:00,391] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1089886: loss 0.0490
[2019-03-27 02:01:00,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1089886: learning rate 0.0001
[2019-03-27 02:01:00,671] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1090008: loss 0.0485
[2019-03-27 02:01:00,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1090009: learning rate 0.0001
[2019-03-27 02:01:00,903] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1090112: loss 0.0101
[2019-03-27 02:01:00,904] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1090112: learning rate 0.0001
[2019-03-27 02:01:01,123] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1090209: loss 0.0026
[2019-03-27 02:01:01,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1090210: learning rate 0.0001
[2019-03-27 02:01:01,281] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1090279: loss 0.0055
[2019-03-27 02:01:01,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1090279: learning rate 0.0001
[2019-03-27 02:01:01,725] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1090478: loss 0.0171
[2019-03-27 02:01:01,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1090478: learning rate 0.0001
[2019-03-27 02:01:01,817] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1090518: loss 0.0098
[2019-03-27 02:01:01,819] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1090518: learning rate 0.0001
[2019-03-27 02:01:01,954] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1090578: loss 0.0233
[2019-03-27 02:01:01,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1090578: learning rate 0.0001
[2019-03-27 02:01:02,068] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1090627: loss 0.0151
[2019-03-27 02:01:02,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1090628: learning rate 0.0001
[2019-03-27 02:01:02,196] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1090686: loss 0.0257
[2019-03-27 02:01:02,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1090686: learning rate 0.0001
[2019-03-27 02:01:06,099] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1092405: loss 0.0093
[2019-03-27 02:01:06,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1092405: learning rate 0.0001
[2019-03-27 02:01:07,274] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7062321e-26 1.0000000e+00 0.0000000e+00 3.0520691e-34 9.0602813e-21], sum to 1.0000
[2019-03-27 02:01:07,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5588
[2019-03-27 02:01:07,287] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 95.0, 1.0, 2.0, 0.3185148446419488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503382.6962452093, 503382.6962452086, 167273.6079057441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1375200.0000, 
sim time next is 1375800.0000, 
raw observation next is [20.86666666666667, 95.16666666666667, 1.0, 2.0, 0.3180440035163916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502811.7905038574, 502811.7905038581, 167233.9838712227], 
processed observation next is [1.0, 0.9565217391304348, 0.18799368088467638, 0.9516666666666667, 1.0, 1.0, 0.17836626929685737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13966994180662706, 0.13966994180662726, 0.24960296100182494], 
reward next is 0.7504, 
noisyNet noise sample is [array([-1.3211219], dtype=float32), -0.7313686]. 
=============================================
[2019-03-27 02:01:07,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9282557e-18 1.0000000e+00 7.8528716e-29 3.9347471e-22 2.3039224e-09], sum to 1.0000
[2019-03-27 02:01:07,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7937
[2019-03-27 02:01:07,752] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 61.66666666666667, 1.0, 2.0, 0.8490237964735008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1287123.343506175, 1287123.343506175, 270109.6205624607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [27.5, 61.33333333333334, 1.0, 2.0, 0.8638685322429966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1308217.633318777, 1308217.633318777, 274173.3348086234], 
processed observation next is [1.0, 0.5652173913043478, 0.5023696682464456, 0.6133333333333334, 1.0, 1.0, 0.8359861834252972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3633937870329936, 0.3633937870329936, 0.40921393255018423], 
reward next is 0.5908, 
noisyNet noise sample is [array([-1.1748277], dtype=float32), 0.53946424]. 
=============================================
[2019-03-27 02:01:07,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.50389 ]
 [72.517815]
 [72.42767 ]
 [72.62341 ]
 [72.62887 ]], R is [[72.31372833]
 [72.18743896]
 [72.06086731]
 [71.96226501]
 [71.93524933]].
[2019-03-27 02:01:08,583] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1093513: loss 0.0799
[2019-03-27 02:01:08,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1093514: learning rate 0.0001
[2019-03-27 02:01:09,166] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1093778: loss 0.0042
[2019-03-27 02:01:09,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1093778: learning rate 0.0001
[2019-03-27 02:01:12,345] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1095203: loss 0.2725
[2019-03-27 02:01:12,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1095205: learning rate 0.0001
[2019-03-27 02:01:14,223] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1096033: loss 0.8604
[2019-03-27 02:01:14,226] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1096034: learning rate 0.0001
[2019-03-27 02:01:17,615] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1097551: loss 0.0318
[2019-03-27 02:01:17,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1097553: learning rate 0.0001
[2019-03-27 02:01:18,454] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1097930: loss 0.1168
[2019-03-27 02:01:18,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1097930: learning rate 0.0001
[2019-03-27 02:01:18,636] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1098008: loss 0.1086
[2019-03-27 02:01:18,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1098008: learning rate 0.0001
[2019-03-27 02:01:18,732] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1098052: loss 0.1983
[2019-03-27 02:01:18,734] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1098052: learning rate 0.0001
[2019-03-27 02:01:19,179] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1098240: loss 0.0885
[2019-03-27 02:01:19,182] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1098241: learning rate 0.0001
[2019-03-27 02:01:19,229] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.895071e-25 1.000000e+00 7.227124e-36 3.036650e-32 5.279717e-21], sum to 1.0000
[2019-03-27 02:01:19,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1409
[2019-03-27 02:01:19,243] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.6524538832873579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001912.57165928, 1001912.571659281, 222064.0386547018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [22.2, 92.0, 1.0, 2.0, 0.5936243180710554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914529.9528314535, 914529.9528314535, 209841.8331625114], 
processed observation next is [1.0, 0.5217391304347826, 0.2511848341232228, 0.92, 1.0, 1.0, 0.5103907446639222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2540360980087371, 0.2540360980087371, 0.31319676591419615], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.12666127], dtype=float32), 0.2317034]. 
=============================================
[2019-03-27 02:01:19,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.79788]
 [71.21392]
 [71.44547]
 [71.47187]
 [71.41609]], R is [[72.01861572]
 [71.96699524]
 [71.8390274 ]
 [71.74807739]
 [71.67801666]].
[2019-03-27 02:01:19,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098304: loss 0.1332
[2019-03-27 02:01:19,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098306: learning rate 0.0001
[2019-03-27 02:01:19,707] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1098476: loss 0.0850
[2019-03-27 02:01:19,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1098477: learning rate 0.0001
[2019-03-27 02:01:19,971] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1098589: loss 0.0550
[2019-03-27 02:01:19,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1098589: learning rate 0.0001
[2019-03-27 02:01:20,055] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1098627: loss 0.0513
[2019-03-27 02:01:20,057] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1098628: learning rate 0.0001
[2019-03-27 02:01:20,137] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1098662: loss 0.0643
[2019-03-27 02:01:20,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1098662: learning rate 0.0001
[2019-03-27 02:01:20,183] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1098685: loss 0.0422
[2019-03-27 02:01:20,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1098686: learning rate 0.0001
[2019-03-27 02:01:20,479] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3369838e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2646294e-29], sum to 1.0000
[2019-03-27 02:01:20,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0716
[2019-03-27 02:01:20,489] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 93.0, 1.0, 2.0, 0.3099032748595594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 492672.6424365093, 492672.6424365099, 166528.201158081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1360800.0000, 
sim time next is 1361400.0000, 
raw observation next is [20.93333333333333, 93.16666666666667, 1.0, 2.0, 0.3151404346517996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500459.3537640856, 500459.3537640856, 167096.4078086852], 
processed observation next is [1.0, 0.782608695652174, 0.19115323854660338, 0.9316666666666668, 1.0, 1.0, 0.17486799355638502, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13901648715669046, 0.13901648715669046, 0.24939762359505255], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.49031994], dtype=float32), -0.036005594]. 
=============================================
[2019-03-27 02:01:20,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.171741e-32 1.000000e+00 0.000000e+00 0.000000e+00 4.658264e-30], sum to 1.0000
[2019-03-27 02:01:20,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3114
[2019-03-27 02:01:20,609] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 93.5, 1.0, 2.0, 0.3197966538423226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506622.848828721, 506622.8488287205, 167539.9788568775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1362600.0000, 
sim time next is 1363200.0000, 
raw observation next is [21.03333333333333, 93.66666666666667, 1.0, 2.0, 0.3188438018254088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504488.8921073263, 504488.8921073263, 167368.041642011], 
processed observation next is [1.0, 0.782608695652174, 0.19589257503949445, 0.9366666666666668, 1.0, 1.0, 0.17932988171735997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14013580336314618, 0.14013580336314618, 0.2498030472268821], 
reward next is 0.7502, 
noisyNet noise sample is [array([-1.4948583], dtype=float32), 0.07023696]. 
=============================================
[2019-03-27 02:01:23,129] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 02:01:23,133] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:01:23,133] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:01:23,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:01:23,134] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:01:23,136] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:01:23,137] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:01:23,138] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:01:23,142] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:01:23,137] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:01:23,145] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:01:23,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-27 02:01:23,164] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-27 02:01:23,164] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-27 02:01:23,182] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-27 02:01:23,236] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-27 02:01:25,816] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:01:25,818] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.65, 71.5, 1.0, 2.0, 0.3582905148606409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558393.5533531292, 558393.5533531299, 171505.8044718865]
[2019-03-27 02:01:25,821] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:01:25,824] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.3986674e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2486439e-36], sampled 0.7251959757218315
[2019-03-27 02:01:33,409] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:01:33,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [14.06501092666667, 88.64624331666667, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 255317.5664223745, 255317.5664223745, 101552.4582970297]
[2019-03-27 02:01:33,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:01:33,414] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9516135e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.14539592989469574
[2019-03-27 02:01:42,765] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:01:42,766] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 94.00000000000001, 1.0, 2.0, 0.4584754386161387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650208.60854035, 650208.60854035, 178580.432649693]
[2019-03-27 02:01:42,768] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:01:42,771] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9802085e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.7862594e-30], sampled 0.7996556620976671
[2019-03-27 02:01:50,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:01:50,471] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.9, 82.0, 1.0, 2.0, 0.4464838072853984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634109.9684447253, 634109.9684447258, 176959.3209255862]
[2019-03-27 02:01:50,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:01:50,475] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9027150e-28 1.0000000e+00 0.0000000e+00 5.1043514e-36 1.5167985e-25], sampled 0.22617429372770514
[2019-03-27 02:02:02,642] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:02:02,644] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.16666666666667, 79.66666666666667, 1.0, 2.0, 0.5685421983591782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794485.2070491023, 794485.2070491023, 195031.300834659]
[2019-03-27 02:02:02,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:02:02,651] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9064358e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3180835e-33], sampled 0.12220313509425107
[2019-03-27 02:02:05,155] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:02:05,157] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.1, 53.5, 1.0, 2.0, 0.8670086370307445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1211802.450026001, 1211802.450026001, 261022.9887638931]
[2019-03-27 02:02:05,158] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:02:05,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2161240e-27 1.0000000e+00 0.0000000e+00 1.5480052e-35 3.4283570e-25], sampled 0.9559583666956366
[2019-03-27 02:02:46,199] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:02:46,200] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.5, 85.0, 1.0, 2.0, 0.6474231510511461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 904761.0013848625, 904761.0013848625, 209913.8744520256]
[2019-03-27 02:02:46,201] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:02:46,203] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.8733112e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4417138e-33], sampled 0.7722531270215848
[2019-03-27 02:03:13,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.078057]
[2019-03-27 02:03:13,287] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.1, 72.0, 1.0, 2.0, 0.8535211178566979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1192940.595817367, 1192940.595817368, 257473.476903864]
[2019-03-27 02:03:13,288] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:03:13,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4944038e-27 1.0000000e+00 2.1013023e-38 2.5373378e-35 3.4912440e-25], sampled 0.27341212787423386
[2019-03-27 02:03:16,845] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4701 2779455430.8154 933.0000
[2019-03-27 02:03:17,471] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2279 2927279696.3748 1342.0000
[2019-03-27 02:03:17,578] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.6483 3164154748.5017 1777.0000
[2019-03-27 02:03:17,626] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5749 3007726976.5989 1766.0000
[2019-03-27 02:03:17,681] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9330 2842541163.3094 1132.0000
[2019-03-27 02:03:18,697] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1100000, evaluation results [1100000.0, 7883.648251525301, 3164154748.5016727, 1777.0, 8254.227883025056, 2927279696.374846, 1342.0, 8658.47011481317, 2779455430.815364, 933.0, 7997.574940102971, 3007726976.5988917, 1766.0, 8495.932995999281, 2842541163.3094025, 1132.0]
[2019-03-27 02:03:19,363] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1100299: loss 0.0791
[2019-03-27 02:03:19,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1100299: learning rate 0.0001
[2019-03-27 02:03:19,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2900577e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:03:19,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9686
[2019-03-27 02:03:19,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.05, 53.66666666666666, 1.0, 2.0, 0.369761251020691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559216.1344637417, 559216.1344637417, 171119.5948696088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1522200.0000, 
sim time next is 1522800.0000, 
raw observation next is [28.9, 54.0, 1.0, 2.0, 0.367701528525152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557318.4125408985, 557318.4125408985, 170995.7174390102], 
processed observation next is [0.0, 0.6521739130434783, 0.5687203791469194, 0.54, 1.0, 1.0, 0.238194612680906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1548106701502496, 0.1548106701502496, 0.2552174887149406], 
reward next is 0.7448, 
noisyNet noise sample is [array([-0.9785754], dtype=float32), 1.540431]. 
=============================================
[2019-03-27 02:03:21,795] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1101396: loss 0.0030
[2019-03-27 02:03:21,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1101398: learning rate 0.0001
[2019-03-27 02:03:22,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0618804e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8703097e-33], sum to 1.0000
[2019-03-27 02:03:22,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0520
[2019-03-27 02:03:22,421] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
processed observation next is [1.0, 0.2608695652173913, 0.31674565560821466, 0.9816666666666667, 1.0, 1.0, 0.3702874765390186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1895836857474378, 0.18958368574743797, 0.2717977587738975], 
reward next is 0.7282, 
noisyNet noise sample is [array([-1.4715699], dtype=float32), -0.017640717]. 
=============================================
[2019-03-27 02:03:22,889] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1101880: loss -85.7966
[2019-03-27 02:03:22,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1101880: learning rate 0.0001
[2019-03-27 02:03:25,741] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1103144: loss 0.0146
[2019-03-27 02:03:25,742] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1103144: learning rate 0.0001
[2019-03-27 02:03:27,676] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1104012: loss 0.0259
[2019-03-27 02:03:27,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1104013: learning rate 0.0001
[2019-03-27 02:03:30,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2976074e-27 1.0000000e+00 0.0000000e+00 3.3233802e-35 2.9334375e-24], sum to 1.0000
[2019-03-27 02:03:30,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3712
[2019-03-27 02:03:30,041] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 85.0, 1.0, 2.0, 0.8052610820602031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206092.929919036, 1206092.929919036, 256171.913510364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1604400.0000, 
sim time next is 1605000.0000, 
raw observation next is [24.08333333333334, 85.0, 1.0, 2.0, 0.817020652302006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223013.416345706, 1223013.416345706, 259234.0670737856], 
processed observation next is [1.0, 0.5652173913043478, 0.34044233807267016, 0.85, 1.0, 1.0, 0.7795429545807302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3397259489849183, 0.3397259489849183, 0.38691651802057553], 
reward next is 0.6131, 
noisyNet noise sample is [array([-0.07984232], dtype=float32), -0.18721677]. 
=============================================
[2019-03-27 02:03:30,058] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.863205]
 [72.95079 ]
 [73.05704 ]
 [73.347946]
 [73.26997 ]], R is [[72.7075119 ]
 [72.59809113]
 [72.49572754]
 [72.40658569]
 [72.3556366 ]].
[2019-03-27 02:03:31,273] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1105629: loss -95.6923
[2019-03-27 02:03:31,277] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1105630: learning rate 0.0001
[2019-03-27 02:03:31,785] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1105853: loss 0.0681
[2019-03-27 02:03:31,787] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1105853: learning rate 0.0001
[2019-03-27 02:03:32,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1105982: loss 0.0610
[2019-03-27 02:03:32,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1105983: learning rate 0.0001
[2019-03-27 02:03:32,105] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1105991: loss 0.0637
[2019-03-27 02:03:32,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1105992: learning rate 0.0001
[2019-03-27 02:03:32,629] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1106229: loss 0.0877
[2019-03-27 02:03:32,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1106229: learning rate 0.0001
[2019-03-27 02:03:32,768] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1106291: loss 0.0981
[2019-03-27 02:03:32,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1106291: learning rate 0.0001
[2019-03-27 02:03:33,111] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1106444: loss 0.1230
[2019-03-27 02:03:33,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1106444: learning rate 0.0001
[2019-03-27 02:03:33,406] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1106576: loss 0.2644
[2019-03-27 02:03:33,409] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1106576: learning rate 0.0001
[2019-03-27 02:03:33,459] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1106599: loss 0.2321
[2019-03-27 02:03:33,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1106599: learning rate 0.0001
[2019-03-27 02:03:33,504] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1106616: loss 0.1484
[2019-03-27 02:03:33,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1106618: learning rate 0.0001
[2019-03-27 02:03:33,632] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1106675: loss 0.1166
[2019-03-27 02:03:33,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1106675: learning rate 0.0001
[2019-03-27 02:03:37,577] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1108413: loss 17.3159
[2019-03-27 02:03:37,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1108414: learning rate 0.0001
[2019-03-27 02:03:40,042] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1109522: loss -66.0173
[2019-03-27 02:03:40,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1109523: learning rate 0.0001
[2019-03-27 02:03:41,107] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1109995: loss 1.9618
[2019-03-27 02:03:41,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1109996: learning rate 0.0001
[2019-03-27 02:03:43,795] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1111198: loss -106.3578
[2019-03-27 02:03:43,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1111198: learning rate 0.0001
[2019-03-27 02:03:45,696] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1112047: loss -45.8991
[2019-03-27 02:03:45,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1112047: learning rate 0.0001
[2019-03-27 02:03:48,478] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.6078810e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3140852e-33], sum to 1.0000
[2019-03-27 02:03:48,489] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5591
[2019-03-27 02:03:48,494] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 95.33333333333333, 1.0, 2.0, 0.5244621872118427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747096.4184764257, 747096.4184764257, 189335.1929466597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1910400.0000, 
sim time next is 1911000.0000, 
raw observation next is [23.95, 95.16666666666667, 1.0, 2.0, 0.5101565507048049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728947.7083333338, 728947.7083333338, 187253.6777058527], 
processed observation next is [1.0, 0.08695652173913043, 0.3341232227488152, 0.9516666666666667, 1.0, 1.0, 0.40982716952386133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20248547453703716, 0.20248547453703716, 0.2794831010535115], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.09397352], dtype=float32), 0.46955657]. 
=============================================
[2019-03-27 02:03:48,507] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.46889]
 [68.36686]
 [68.27477]
 [68.05909]
 [68.67907]], R is [[68.53421783]
 [68.56628418]
 [68.58886719]
 [68.59760284]
 [68.56578827]].
[2019-03-27 02:03:49,322] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1113660: loss 2.1362
[2019-03-27 02:03:49,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1113661: learning rate 0.0001
[2019-03-27 02:03:49,699] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1113829: loss -43.1719
[2019-03-27 02:03:49,702] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1113830: learning rate 0.0001
[2019-03-27 02:03:50,048] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1113989: loss -0.8943
[2019-03-27 02:03:50,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1113989: learning rate 0.0001
[2019-03-27 02:03:50,159] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1114035: loss -98.2462
[2019-03-27 02:03:50,167] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1114037: learning rate 0.0001
[2019-03-27 02:03:50,607] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1114235: loss -71.7404
[2019-03-27 02:03:50,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1114236: learning rate 0.0001
[2019-03-27 02:03:50,887] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1114363: loss -166.4013
[2019-03-27 02:03:50,889] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1114363: learning rate 0.0001
[2019-03-27 02:03:51,168] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1114487: loss -118.5683
[2019-03-27 02:03:51,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1114487: learning rate 0.0001
[2019-03-27 02:03:51,180] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1114493: loss -93.0961
[2019-03-27 02:03:51,182] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1114493: learning rate 0.0001
[2019-03-27 02:03:51,437] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1114606: loss -1.1739
[2019-03-27 02:03:51,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1114606: learning rate 0.0001
[2019-03-27 02:03:51,509] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1114639: loss -103.6753
[2019-03-27 02:03:51,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1114639: learning rate 0.0001
[2019-03-27 02:03:51,653] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1114702: loss -102.8739
[2019-03-27 02:03:51,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1114704: learning rate 0.0001
[2019-03-27 02:03:55,436] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1116382: loss 2.8288
[2019-03-27 02:03:55,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1116382: learning rate 0.0001
[2019-03-27 02:03:57,931] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1117495: loss 2.4154
[2019-03-27 02:03:57,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1117496: learning rate 0.0001
[2019-03-27 02:03:58,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.54907110e-14 9.99999881e-01 1.43007466e-23 7.72015210e-16
 1.04004386e-07], sum to 1.0000
[2019-03-27 02:03:58,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6881
[2019-03-27 02:03:58,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2287596.661160362 W.
[2019-03-27 02:03:58,921] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.03333333333333, 65.66666666666667, 1.0, 2.0, 0.5453012929801376, 1.0, 2.0, 0.5453012929801376, 1.0, 2.0, 0.9470080120132471, 6.9112, 6.9112, 170.5573041426782, 2287596.661160362, 2287596.661160362, 447886.6662153274], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2212800.0000, 
sim time next is 2213400.0000, 
raw observation next is [32.01666666666667, 65.83333333333333, 1.0, 2.0, 0.8185308696900716, 1.0, 2.0, 0.8185308696900716, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2289217.259799727, 2289217.259799727, 428997.1834172534], 
processed observation next is [1.0, 0.6086956521739131, 0.7164296998420224, 0.6583333333333333, 1.0, 1.0, 0.7813624936024959, 1.0, 1.0, 0.7813624936024959, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6358936832777019, 0.6358936832777019, 0.6402943036078409], 
reward next is 0.3597, 
noisyNet noise sample is [array([1.1015947], dtype=float32), -1.4901017]. 
=============================================
[2019-03-27 02:03:59,908] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1118383: loss -44.9947
[2019-03-27 02:03:59,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1118385: learning rate 0.0001
[2019-03-27 02:04:00,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5264994e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:04:00,287] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6221
[2019-03-27 02:04:00,291] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.16666666666667, 1.0, 2.0, 0.5395752796280108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753992.2469766083, 753992.2469766083, 190029.6961108312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2110200.0000, 
sim time next is 2110800.0000, 
raw observation next is [29.2, 77.33333333333334, 1.0, 2.0, 0.5410830644166856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756099.9466820181, 756099.9466820174, 190284.0170862106], 
processed observation next is [0.0, 0.43478260869565216, 0.5829383886255924, 0.7733333333333334, 1.0, 1.0, 0.4470880294176935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21002776296722725, 0.21002776296722706, 0.2840059956510606], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.27535078], dtype=float32), -0.55131084]. 
=============================================
[2019-03-27 02:04:01,553] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1119116: loss 2.6322
[2019-03-27 02:04:01,557] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1119118: learning rate 0.0001
[2019-03-27 02:04:03,612] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1120035: loss 2.1141
[2019-03-27 02:04:03,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1120035: learning rate 0.0001
[2019-03-27 02:04:03,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4401808e-12 9.9998665e-01 2.2737046e-20 2.0315691e-12 1.3320913e-05], sum to 1.0000
[2019-03-27 02:04:03,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7779
[2019-03-27 02:04:03,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2382330.07022497 W.
[2019-03-27 02:04:03,647] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.84999999999999, 62.5, 1.0, 2.0, 0.8517925147295926, 1.0, 2.0, 0.8517925147295926, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2382330.07022497, 2382330.07022497, 445877.181138783], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2385000.0000, 
sim time next is 2385600.0000, 
raw observation next is [32.86666666666666, 62.33333333333333, 1.0, 2.0, 0.8846573777669906, 1.0, 2.0, 0.8846573777669906, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2474338.942066293, 2474338.942066292, 463192.9226784947], 
processed observation next is [1.0, 0.6086956521739131, 0.7567140600315952, 0.6233333333333333, 1.0, 1.0, 0.8610329852614345, 1.0, 1.0, 0.8610329852614345, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6873163727961925, 0.6873163727961922, 0.6913327204156637], 
reward next is 0.3087, 
noisyNet noise sample is [array([-1.0024463], dtype=float32), 0.26829404]. 
=============================================
[2019-03-27 02:04:04,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0481746e-16 1.0000000e+00 3.4422234e-24 6.0579274e-17 4.9071723e-12], sum to 1.0000
[2019-03-27 02:04:04,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4685
[2019-03-27 02:04:04,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1934378.392758628 W.
[2019-03-27 02:04:04,626] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.25, 69.5, 1.0, 2.0, 0.6917658642169919, 1.0, 2.0, 0.6917658642169919, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1934378.392758628, 1934378.392758628, 370618.6311974821], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2364600.0000, 
sim time next is 2365200.0000, 
raw observation next is [30.4, 69.0, 1.0, 2.0, 0.6853583617342853, 1.0, 2.0, 0.6853583617342853, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1916445.129002632, 1916445.129002632, 367919.1245382135], 
processed observation next is [1.0, 0.391304347826087, 0.6398104265402843, 0.69, 1.0, 1.0, 0.6209136888364883, 1.0, 1.0, 0.6209136888364883, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5323458691673978, 0.5323458691673978, 0.549133021698826], 
reward next is 0.4509, 
noisyNet noise sample is [array([1.004881], dtype=float32), -1.6403702]. 
=============================================
[2019-03-27 02:04:07,340] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1121701: loss 3.1355
[2019-03-27 02:04:07,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1121703: learning rate 0.0001
[2019-03-27 02:04:07,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1121826: loss 3.9157
[2019-03-27 02:04:07,629] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1121826: learning rate 0.0001
[2019-03-27 02:04:07,873] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1121938: loss 3.1109
[2019-03-27 02:04:07,875] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1121939: learning rate 0.0001
[2019-03-27 02:04:08,186] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1122066: loss -185.5438
[2019-03-27 02:04:08,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1122067: learning rate 0.0001
[2019-03-27 02:04:08,286] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1122110: loss 2.5990
[2019-03-27 02:04:08,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1122110: learning rate 0.0001
[2019-03-27 02:04:08,669] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1122279: loss 2.2925
[2019-03-27 02:04:08,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1122279: learning rate 0.0001
[2019-03-27 02:04:08,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8080501e-29 1.0000000e+00 7.7009187e-38 4.3397331e-35 3.3714535e-29], sum to 1.0000
[2019-03-27 02:04:08,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2270
[2019-03-27 02:04:08,807] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 85.33333333333334, 1.0, 2.0, 0.5162732218746887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721419.3367865313, 721419.3367865313, 186184.9187354792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2247600.0000, 
sim time next is 2248200.0000, 
raw observation next is [26.9, 85.5, 1.0, 2.0, 0.5158807715841106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720870.7565319655, 720870.7565319655, 186121.5643318131], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.855, 1.0, 1.0, 0.4167238211856754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20024187681443484, 0.20024187681443484, 0.27779337959972106], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.45339975], dtype=float32), 1.065611]. 
=============================================
[2019-03-27 02:04:08,809] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1122345: loss 2.1108
[2019-03-27 02:04:08,814] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1122346: learning rate 0.0001
[2019-03-27 02:04:08,948] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1122402: loss 2.0530
[2019-03-27 02:04:08,952] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1122403: learning rate 0.0001
[2019-03-27 02:04:09,040] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1122447: loss 1.9401
[2019-03-27 02:04:09,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1122448: learning rate 0.0001
[2019-03-27 02:04:09,089] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1122468: loss 1.7811
[2019-03-27 02:04:09,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1122468: learning rate 0.0001
[2019-03-27 02:04:09,234] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1122533: loss 1.9028
[2019-03-27 02:04:09,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1122534: learning rate 0.0001
[2019-03-27 02:04:09,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4880690e-24 1.0000000e+00 1.6611944e-32 7.1729290e-30 7.9431514e-25], sum to 1.0000
[2019-03-27 02:04:09,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9258
[2019-03-27 02:04:09,482] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.33333333333334, 1.0, 2.0, 0.6254545890678294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 874047.7394175718, 874047.7394175711, 205574.9690723456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2269200.0000, 
sim time next is 2269800.0000, 
raw observation next is [27.15, 80.5, 1.0, 2.0, 0.634765193559009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887064.3698499117, 887064.3698499117, 207391.9909037694], 
processed observation next is [1.0, 0.2608695652173913, 0.485781990521327, 0.805, 1.0, 1.0, 0.5599580645289264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24640676940275325, 0.24640676940275325, 0.3095402849309991], 
reward next is 0.6905, 
noisyNet noise sample is [array([-0.09151982], dtype=float32), 0.50962436]. 
=============================================
[2019-03-27 02:04:12,901] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0115408e-29 1.0000000e+00 0.0000000e+00 2.1679501e-35 5.8279024e-30], sum to 1.0000
[2019-03-27 02:04:12,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5943
[2019-03-27 02:04:12,916] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 81.0, 1.0, 2.0, 0.5466970916810645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763947.7106007746, 763947.7106007746, 191235.6963679117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2331600.0000, 
sim time next is 2332200.0000, 
raw observation next is [28.35, 81.0, 1.0, 2.0, 0.5446453350928243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761079.5843328472, 761079.5843328472, 190886.468196256], 
processed observation next is [1.0, 1.0, 0.5426540284360191, 0.81, 1.0, 1.0, 0.4513799217985835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2114109956480131, 0.2114109956480131, 0.2849051764123224], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.7167467], dtype=float32), 0.8442797]. 
=============================================
[2019-03-27 02:04:13,868] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1124603: loss -30.3877
[2019-03-27 02:04:13,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1124605: learning rate 0.0001
[2019-03-27 02:04:14,758] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 02:04:14,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:04:14,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:04:14,762] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:04:14,763] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:04:14,764] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:04:14,765] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:04:14,764] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:04:14,767] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:04:14,766] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:04:14,768] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:04:14,784] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-27 02:04:14,805] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-27 02:04:14,806] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-27 02:04:14,824] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-27 02:04:14,888] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-27 02:04:45,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.080052085]
[2019-03-27 02:04:45,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.08413380666667, 84.50113895000001, 1.0, 2.0, 0.4925031764276314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688193.2764941439, 688193.2764941439, 182428.9622466528]
[2019-03-27 02:04:45,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:04:45,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7726798722111109
[2019-03-27 02:05:20,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.080052085]
[2019-03-27 02:05:20,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 79.0, 1.0, 2.0, 0.6172868673671312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862629.0402382638, 862629.0402382644, 204011.4484585144]
[2019-03-27 02:05:20,233] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:05:20,237] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9150723061262164
[2019-03-27 02:05:23,414] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.080052085]
[2019-03-27 02:05:23,415] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.1, 72.0, 1.0, 2.0, 0.9546224740457024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1334335.692947347, 1334335.692947348, 285408.4072910756]
[2019-03-27 02:05:23,417] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:05:23,419] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.7859642e-29 1.0000000e+00 0.0000000e+00 2.0188959e-34 2.1001819e-29], sampled 0.3854212445456543
[2019-03-27 02:05:31,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.080052085]
[2019-03-27 02:05:31,417] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.8, 73.0, 1.0, 2.0, 0.625113895298861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873571.437721357, 873571.437721357, 205517.5909026735]
[2019-03-27 02:05:31,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:05:31,420] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5641818e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1342088e-36], sampled 0.8865574970736457
[2019-03-27 02:05:49,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.080052085]
[2019-03-27 02:05:49,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.3, 71.5, 1.0, 2.0, 0.4871206774760796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680669.6979250896, 680669.6979250903, 181602.5562100193]
[2019-03-27 02:05:49,848] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:05:49,852] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9895329856066256
[2019-03-27 02:06:09,184] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8312 2842479870.9176 1131.0000
[2019-03-27 02:06:09,475] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.8157 3164170182.1515 1789.0000
[2019-03-27 02:06:09,592] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5461 3007746331.4965 1766.0000
[2019-03-27 02:06:09,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.1462 2927399256.6943 1340.0000
[2019-03-27 02:06:09,731] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.1867 2779162961.9625 935.0000
[2019-03-27 02:06:10,747] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1125000, evaluation results [1125000.0, 7881.815740637718, 3164170182.151457, 1789.0, 8253.146239002337, 2927399256.6943064, 1340.0, 8660.18670008585, 2779162961.962491, 935.0, 7997.546052179389, 3007746331.496539, 1766.0, 8496.831184100816, 2842479870.917638, 1131.0]
[2019-03-27 02:06:12,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1789979e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3781245e-36], sum to 1.0000
[2019-03-27 02:06:12,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6147
[2019-03-27 02:06:12,047] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.8, 62.5, 1.0, 2.0, 0.5263077627060473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735446.0526841943, 735446.0526841943, 187824.5855354951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2395800.0000, 
sim time next is 2396400.0000, 
raw observation next is [32.66666666666666, 63.33333333333333, 1.0, 2.0, 0.5388995160467799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753047.6130571198, 753047.6130571192, 189918.3833170016], 
processed observation next is [1.0, 0.7391304347826086, 0.7472353870458132, 0.6333333333333333, 1.0, 1.0, 0.44445724824913235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091798925158666, 0.20917989251586644, 0.28346027360746506], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.4426378], dtype=float32), -0.2460462]. 
=============================================
[2019-03-27 02:06:12,166] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1125631: loss -110.6961
[2019-03-27 02:06:12,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1125631: learning rate 0.0001
[2019-03-27 02:06:13,106] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1126051: loss 0.0454
[2019-03-27 02:06:13,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1126052: learning rate 0.0001
[2019-03-27 02:06:13,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:06:13,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5902
[2019-03-27 02:06:13,780] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4749715298666664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663817.185979863, 663817.185979863, 179783.9465901326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4754971946229017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664551.669029083, 664551.6690290824, 179862.3850760613], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680689091842189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18459768584141195, 0.18459768584141178, 0.26845132100904673], 
reward next is 0.7315, 
noisyNet noise sample is [array([0.13616818], dtype=float32), 0.05662939]. 
=============================================
[2019-03-27 02:06:15,222] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9447663e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:06:15,230] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9818
[2019-03-27 02:06:15,237] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [22.66666666666667, 96.0, 1.0, 2.0, 0.3899526964879866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583306.3079049023, 583306.3079049016, 173051.4434765129], 
processed observation next is [0.0, 0.8260869565217391, 0.27330173775671435, 0.96, 1.0, 1.0, 0.26500324878070675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16202952997358397, 0.16202952997358377, 0.25828573653210884], 
reward next is 0.7417, 
noisyNet noise sample is [array([-0.3243594], dtype=float32), -1.0205637]. 
=============================================
[2019-03-27 02:06:15,261] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.176346]
 [76.18889 ]
 [76.13486 ]
 [76.16552 ]
 [76.19346 ]], R is [[76.15174866]
 [76.13186646]
 [76.11205292]
 [76.09222412]
 [76.07247925]].
[2019-03-27 02:06:15,815] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1127266: loss -7.3482
[2019-03-27 02:06:15,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1127267: learning rate 0.0001
[2019-03-27 02:06:17,631] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1128075: loss -14.6821
[2019-03-27 02:06:17,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1128075: learning rate 0.0001
[2019-03-27 02:06:19,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6001447e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:06:19,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6810
[2019-03-27 02:06:19,560] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.5507602230073979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852111.9518531389, 852111.9518531389, 201769.2983555433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2968200.0000, 
sim time next is 2968800.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.5597298578214112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864746.8019493389, 864746.8019493383, 203365.8018382049], 
processed observation next is [1.0, 0.34782608695652173, 0.22590837282780438, 0.96, 1.0, 1.0, 0.4695540455679652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24020744498592747, 0.2402074449859273, 0.30353104751970883], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.15611249], dtype=float32), 1.547009]. 
=============================================
[2019-03-27 02:06:21,424] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1129769: loss 0.1266
[2019-03-27 02:06:21,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1129770: learning rate 0.0001
[2019-03-27 02:06:21,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1129903: loss -45.6557
[2019-03-27 02:06:21,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1129903: learning rate 0.0001
[2019-03-27 02:06:21,745] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1129915: loss -203.4263
[2019-03-27 02:06:21,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1129915: learning rate 0.0001
[2019-03-27 02:06:22,179] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1130107: loss -133.6087
[2019-03-27 02:06:22,183] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1130107: learning rate 0.0001
[2019-03-27 02:06:22,539] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1130266: loss -128.8522
[2019-03-27 02:06:22,542] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1130267: learning rate 0.0001
[2019-03-27 02:06:22,770] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1130370: loss -96.2550
[2019-03-27 02:06:22,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1130370: learning rate 0.0001
[2019-03-27 02:06:23,023] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1130483: loss 15.7738
[2019-03-27 02:06:23,026] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1130483: learning rate 0.0001
[2019-03-27 02:06:23,047] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1130494: loss -71.2081
[2019-03-27 02:06:23,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1130495: learning rate 0.0001
[2019-03-27 02:06:23,296] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1130605: loss -107.3672
[2019-03-27 02:06:23,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1130606: learning rate 0.0001
[2019-03-27 02:06:23,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1130624: loss -16.4277
[2019-03-27 02:06:23,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1130625: learning rate 0.0001
[2019-03-27 02:06:23,474] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1130684: loss -110.1407
[2019-03-27 02:06:23,476] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1130684: learning rate 0.0001
[2019-03-27 02:06:26,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.829174e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:06:26,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8872
[2019-03-27 02:06:26,614] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 81.5, 1.0, 2.0, 0.49055919603352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685476.0028912062, 685476.0028912062, 182130.4274093183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.495270425329538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692061.3174084985, 692061.3174084985, 182858.6997948849], 
processed observation next is [0.0, 0.5217391304347826, 0.4786729857819906, 0.8233333333333335, 1.0, 1.0, 0.39189207871028675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.192239254835694, 0.192239254835694, 0.27292343252967893], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.33050883], dtype=float32), 0.07761348]. 
=============================================
[2019-03-27 02:06:26,971] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1132244: loss 0.4035
[2019-03-27 02:06:26,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1132246: learning rate 0.0001
[2019-03-27 02:06:29,551] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1133395: loss 0.1153
[2019-03-27 02:06:29,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1133395: learning rate 0.0001
[2019-03-27 02:06:30,509] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1133825: loss 7.8069
[2019-03-27 02:06:30,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1133827: learning rate 0.0001
[2019-03-27 02:06:33,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7712298e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:06:33,050] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8779
[2019-03-27 02:06:33,055] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3940499015925438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587981.5821450595, 587981.5821450589, 173432.7497484515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2739600.0000, 
sim time next is 2740200.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3939518268025217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587835.3044585264, 587835.3044585264, 173419.3759234499], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.26982147807532736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1632875845718129, 0.1632875845718129, 0.25883488943798494], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.8745674], dtype=float32), 0.0062587685]. 
=============================================
[2019-03-27 02:06:33,424] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1135131: loss 0.0466
[2019-03-27 02:06:33,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1135132: learning rate 0.0001
[2019-03-27 02:06:35,188] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1135918: loss 0.0083
[2019-03-27 02:06:35,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1135919: learning rate 0.0001
[2019-03-27 02:06:39,020] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1137629: loss 3.7468
[2019-03-27 02:06:39,024] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1137631: learning rate 0.0001
[2019-03-27 02:06:39,620] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1137901: loss 0.0480
[2019-03-27 02:06:39,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1137902: learning rate 0.0001
[2019-03-27 02:06:39,756] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1137961: loss 0.0226
[2019-03-27 02:06:39,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1137963: learning rate 0.0001
[2019-03-27 02:06:39,902] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1138025: loss 0.0092
[2019-03-27 02:06:39,904] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1138025: learning rate 0.0001
[2019-03-27 02:06:40,400] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1138247: loss 0.0243
[2019-03-27 02:06:40,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1138249: learning rate 0.0001
[2019-03-27 02:06:40,590] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1138332: loss 0.0399
[2019-03-27 02:06:40,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1138334: learning rate 0.0001
[2019-03-27 02:06:41,042] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1138538: loss 0.0368
[2019-03-27 02:06:41,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1138538: learning rate 0.0001
[2019-03-27 02:06:41,102] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1138568: loss 0.0248
[2019-03-27 02:06:41,106] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1138570: learning rate 0.0001
[2019-03-27 02:06:41,273] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1138644: loss 0.0028
[2019-03-27 02:06:41,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1138645: learning rate 0.0001
[2019-03-27 02:06:41,318] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1138665: loss 0.0018
[2019-03-27 02:06:41,321] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1138666: learning rate 0.0001
[2019-03-27 02:06:41,494] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1138745: loss 0.0153
[2019-03-27 02:06:41,497] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1138746: learning rate 0.0001
[2019-03-27 02:06:41,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7023149e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5564667e-36], sum to 1.0000
[2019-03-27 02:06:41,703] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1008
[2019-03-27 02:06:41,714] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.6208827562282309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 934995.076949588, 934995.0769495873, 213121.615267202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3059400.0000, 
sim time next is 3060000.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.594342783005197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 895015.1927884435, 895015.1927884435, 207700.950212236], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5112563650665024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2486153313301232, 0.2486153313301232, 0.3100014182272179], 
reward next is 0.6900, 
noisyNet noise sample is [array([0.22196963], dtype=float32), 1.2020004]. 
=============================================
[2019-03-27 02:06:41,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[67.136925]
 [67.18678 ]
 [67.28904 ]
 [67.273674]
 [67.18444 ]], R is [[67.33623505]
 [67.34477997]
 [67.35787964]
 [67.37956238]
 [67.40259552]].
[2019-03-27 02:06:43,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9005002e-34 1.0000000e+00 0.0000000e+00 4.1507856e-38 5.6236826e-36], sum to 1.0000
[2019-03-27 02:06:43,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7907
[2019-03-27 02:06:43,800] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 93.16666666666667, 1.0, 2.0, 0.5213168263441044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781545.8609867118, 781545.8609867118, 193553.6008072828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2897400.0000, 
sim time next is 2898000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.526962387468613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787175.6370659193, 787175.6370659193, 194221.3347785668], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.94, 1.0, 1.0, 0.43007516562483494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2186598991849776, 0.2186598991849776, 0.2898825892217415], 
reward next is 0.7101, 
noisyNet noise sample is [array([0.00388494], dtype=float32), 2.6377099]. 
=============================================
[2019-03-27 02:06:43,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.34345]
 [70.15539]
 [69.81162]
 [69.38419]
 [68.64184]], R is [[70.56556702]
 [70.57102203]
 [70.57717133]
 [70.57987213]
 [70.57339478]].
[2019-03-27 02:06:44,773] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1140202: loss 2.5593
[2019-03-27 02:06:44,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1140202: learning rate 0.0001
[2019-03-27 02:06:47,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.631594e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:06:47,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8692
[2019-03-27 02:06:47,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.5720936681797003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 882650.21200267, 882650.2120026695, 205658.5751015131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2969400.0000, 
sim time next is 2970000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5432639263750915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837069.2685101344, 837069.2685101344, 199991.506046124], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4497155739458933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23251924125281512, 0.23251924125281512, 0.29849478514346867], 
reward next is 0.7015, 
noisyNet noise sample is [array([1.1855707], dtype=float32), 0.25774008]. 
=============================================
[2019-03-27 02:06:47,408] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.57708 ]
 [77.0143  ]
 [77.65516 ]
 [78.653244]
 [79.65208 ]], R is [[76.51654816]
 [76.44442749]
 [76.37645721]
 [76.31154633]
 [76.26618958]].
[2019-03-27 02:06:47,434] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1141383: loss 3.9622
[2019-03-27 02:06:47,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1141384: learning rate 0.0001
[2019-03-27 02:06:48,802] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1141995: loss 4.8504
[2019-03-27 02:06:48,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1141996: learning rate 0.0001
[2019-03-27 02:06:51,281] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1143105: loss 6.0632
[2019-03-27 02:06:51,283] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1143105: learning rate 0.0001
[2019-03-27 02:06:53,026] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1143889: loss 6.2770
[2019-03-27 02:06:53,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1143889: learning rate 0.0001
[2019-03-27 02:06:54,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4249233e-30 1.0000000e+00 0.0000000e+00 6.9871321e-35 3.5331650e-33], sum to 1.0000
[2019-03-27 02:06:54,195] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6335
[2019-03-27 02:06:54,198] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4234433790790483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614557.5426876508, 614557.5426876508, 175415.638473793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3086400.0000, 
sim time next is 3087000.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4221929928478546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612799.3362681925, 612799.3362681925, 175248.3666270096], 
processed observation next is [1.0, 0.7391304347826086, 0.28909952606635075, 1.0, 1.0, 1.0, 0.30384697933476457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1702220378522757, 0.1702220378522757, 0.26156472630896954], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.08358092], dtype=float32), -0.028866101]. 
=============================================
[2019-03-27 02:06:54,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.95552 ]
 [67.60241 ]
 [65.334076]
 [64.9568  ]
 [64.93632 ]], R is [[69.64611053]
 [69.68783569]
 [69.71283722]
 [69.64083099]
 [69.56084442]].
[2019-03-27 02:06:57,312] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1145801: loss 3.9167
[2019-03-27 02:06:57,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1145803: learning rate 0.0001
[2019-03-27 02:06:57,472] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1145872: loss 6.6591
[2019-03-27 02:06:57,474] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1145872: learning rate 0.0001
[2019-03-27 02:06:57,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1145892: loss 7.0200
[2019-03-27 02:06:57,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1145892: learning rate 0.0001
[2019-03-27 02:06:57,673] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1145957: loss 6.9702
[2019-03-27 02:06:57,677] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1145959: learning rate 0.0001
[2019-03-27 02:06:58,326] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1146249: loss 7.4192
[2019-03-27 02:06:58,331] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1146250: learning rate 0.0001
[2019-03-27 02:06:58,480] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146316: loss 7.8555
[2019-03-27 02:06:58,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146316: learning rate 0.0001
[2019-03-27 02:06:58,841] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1146472: loss 8.1360
[2019-03-27 02:06:58,843] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1146472: learning rate 0.0001
[2019-03-27 02:06:58,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1146524: loss 8.6669
[2019-03-27 02:06:58,952] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1146525: learning rate 0.0001
[2019-03-27 02:06:59,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1146556: loss 8.7252
[2019-03-27 02:06:59,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1146556: learning rate 0.0001
[2019-03-27 02:06:59,215] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1146644: loss 8.0801
[2019-03-27 02:06:59,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1146644: learning rate 0.0001
[2019-03-27 02:06:59,265] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1146664: loss 8.2976
[2019-03-27 02:06:59,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1146665: learning rate 0.0001
[2019-03-27 02:07:02,910] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1148293: loss 8.5356
[2019-03-27 02:07:02,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1148293: learning rate 0.0001
[2019-03-27 02:07:05,458] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1149437: loss 9.3874
[2019-03-27 02:07:05,462] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1149437: learning rate 0.0001
[2019-03-27 02:07:06,729] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 02:07:06,731] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:07:06,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:07:06,732] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:07:06,733] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:07:06,735] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:07:06,737] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:07:06,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:07:06,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:07:06,741] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:07:06,745] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:07:06,756] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-27 02:07:06,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-27 02:07:06,806] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-27 02:07:06,827] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-27 02:07:06,846] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-27 02:07:32,072] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:07:32,073] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.85732241333334, 89.27080268333333, 1.0, 2.0, 0.3534356344609501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545012.7268023409, 545012.7268023409, 170247.2722717885]
[2019-03-27 02:07:32,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:07:32,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7685812e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.029072229278053086
[2019-03-27 02:07:42,082] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:07:42,084] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.35, 81.0, 1.0, 2.0, 0.5446453350928243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761079.5843328472, 761079.5843328472, 190886.468196256]
[2019-03-27 02:07:42,084] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:07:42,088] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1426605e-34 1.0000000e+00 0.0000000e+00 2.2850526e-38 1.4475339e-38], sampled 0.9400364588535342
[2019-03-27 02:07:51,664] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:07:51,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.83911238333333, 98.04223402666668, 1.0, 2.0, 0.338619744607792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531022.4793097235, 531022.4793097229, 169334.803819756]
[2019-03-27 02:07:51,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:07:51,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0934755e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.31015366545658607
[2019-03-27 02:08:19,641] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:08:19,642] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.75, 68.33333333333333, 1.0, 2.0, 0.5529711639588918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772718.2100819093, 772718.2100819086, 192311.42352804]
[2019-03-27 02:08:19,644] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:08:19,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7305488e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7576231390384808
[2019-03-27 02:08:34,743] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:08:34,744] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.96110496666667, 90.01819168666667, 1.0, 2.0, 0.5758656200512587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804722.8890987566, 804722.8890987572, 196330.6312113808]
[2019-03-27 02:08:34,745] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:08:34,748] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3069340e-32 1.0000000e+00 0.0000000e+00 1.1397605e-36 1.9509272e-37], sampled 0.19098835617544285
[2019-03-27 02:08:39,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:08:39,615] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.41666666666667, 82.66666666666666, 1.0, 2.0, 0.5321839862161647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743660.178473106, 743660.1784731053, 188791.2457624983]
[2019-03-27 02:08:39,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:08:39,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.717134e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.4496721410967899
[2019-03-27 02:08:48,941] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:08:48,943] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.527814735, 94.811273325, 1.0, 2.0, 0.4729534799191618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661974.4933352347, 661974.493335234, 179609.9921708585]
[2019-03-27 02:08:48,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:08:48,948] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2663347e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.07765583865285786
[2019-03-27 02:09:00,398] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.082589455]
[2019-03-27 02:09:00,399] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.06666666666667, 62.0, 1.0, 2.0, 0.310808234098318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497020.4418562265, 497020.4418562265, 166879.9828015846]
[2019-03-27 02:09:00,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:09:00,404] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4788584e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6854578084581472
[2019-03-27 02:09:01,272] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.2471 2842272267.2454 1140.0000
[2019-03-27 02:09:01,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.4108 2779501187.0930 939.0000
[2019-03-27 02:09:01,391] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.8280 3007600323.4498 1765.0000
[2019-03-27 02:09:01,523] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.8690 2927395491.3704 1347.0000
[2019-03-27 02:09:01,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.7025 3163772840.5795 1861.0000
[2019-03-27 02:09:02,686] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1150000, evaluation results [1150000.0, 7873.702485383267, 3163772840.5794926, 1861.0, 8252.868987070386, 2927395491.3703713, 1347.0, 8657.410767788562, 2779501187.092983, 939.0, 7999.828019221517, 3007600323.4497886, 1765.0, 8496.247104299173, 2842272267.2454267, 1140.0]
[2019-03-27 02:09:03,413] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1150334: loss 8.4276
[2019-03-27 02:09:03,417] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1150334: learning rate 0.0001
[2019-03-27 02:09:05,113] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1151094: loss 10.9166
[2019-03-27 02:09:05,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1151095: learning rate 0.0001
[2019-03-27 02:09:06,962] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1151935: loss 10.4345
[2019-03-27 02:09:06,967] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1151936: learning rate 0.0001
[2019-03-27 02:09:06,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2916157e-22 1.0000000e+00 1.7988065e-31 7.4401110e-22 4.8855410e-24], sum to 1.0000
[2019-03-27 02:09:06,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1395
[2019-03-27 02:09:06,984] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7506801962894162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1049131.948596385, 1049131.948596384, 232164.0864793828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7356349309633216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1028094.881958236, 1028094.881958236, 228724.3100700208], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.6814878686305079, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2855819116550656, 0.2855819116550656, 0.34137956726868773], 
reward next is 0.6586, 
noisyNet noise sample is [array([0.62778735], dtype=float32), 1.200489]. 
=============================================
[2019-03-27 02:09:06,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[53.535995]
 [53.811874]
 [53.902206]
 [53.905094]
 [54.285797]], R is [[53.76869965]
 [53.88450241]
 [53.99152374]
 [54.102314  ]
 [54.18316269]].
[2019-03-27 02:09:11,188] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1153790: loss 10.0386
[2019-03-27 02:09:11,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1153790: learning rate 0.0001
[2019-03-27 02:09:11,296] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1153837: loss 10.1948
[2019-03-27 02:09:11,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1153837: learning rate 0.0001
[2019-03-27 02:09:11,385] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1153875: loss 9.8568
[2019-03-27 02:09:11,388] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1153876: learning rate 0.0001
[2019-03-27 02:09:11,647] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1153993: loss -9.8186
[2019-03-27 02:09:11,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1153994: learning rate 0.0001
[2019-03-27 02:09:12,133] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154212: loss 9.4026
[2019-03-27 02:09:12,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154213: learning rate 0.0001
[2019-03-27 02:09:12,149] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1154219: loss 9.4625
[2019-03-27 02:09:12,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1154219: learning rate 0.0001
[2019-03-27 02:09:12,661] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1154451: loss 10.5641
[2019-03-27 02:09:12,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1154451: learning rate 0.0001
[2019-03-27 02:09:12,674] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1154454: loss 10.5543
[2019-03-27 02:09:12,677] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1154455: learning rate 0.0001
[2019-03-27 02:09:12,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1154466: loss 10.6077
[2019-03-27 02:09:12,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1154467: learning rate 0.0001
[2019-03-27 02:09:12,998] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1154601: loss 9.4494
[2019-03-27 02:09:13,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1154601: learning rate 0.0001
[2019-03-27 02:09:13,101] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1154640: loss 9.5164
[2019-03-27 02:09:13,102] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1154640: learning rate 0.0001
[2019-03-27 02:09:16,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2429990e-27 1.0000000e+00 3.7139902e-38 3.9492784e-27 1.4428744e-32], sum to 1.0000
[2019-03-27 02:09:16,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9677
[2019-03-27 02:09:16,654] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4918174229934308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687234.7373787031, 687234.7373787038, 182323.7222424673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3546600.0000, 
sim time next is 3547200.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4897845331431168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684393.1898831101, 684393.1898831107, 182010.8014198265], 
processed observation next is [1.0, 0.043478260869565216, 0.5102685624012641, 0.7566666666666667, 1.0, 1.0, 0.385282570051948, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19010921941197503, 0.1901092194119752, 0.2716579125669052], 
reward next is 0.7283, 
noisyNet noise sample is [array([-1.2413772], dtype=float32), 1.2362332]. 
=============================================
[2019-03-27 02:09:17,202] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1156477: loss -83.1223
[2019-03-27 02:09:17,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1156477: learning rate 0.0001
[2019-03-27 02:09:19,615] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1157551: loss -47.0492
[2019-03-27 02:09:19,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1157551: learning rate 0.0001
[2019-03-27 02:09:21,126] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1158215: loss 0.4349
[2019-03-27 02:09:21,128] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1158215: learning rate 0.0001
[2019-03-27 02:09:21,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:09:21,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2810
[2019-03-27 02:09:21,736] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.5913252310518421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826334.7751314177, 826334.7751314177, 199141.6164013741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3930600.0000, 
sim time next is 3931200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.5927436550983791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 828317.6943218128, 828317.6943218135, 199402.568686852], 
processed observation next is [0.0, 0.5217391304347826, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5093297049378062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2300882484227258, 0.230088248422726, 0.2976157741594806], 
reward next is 0.7024, 
noisyNet noise sample is [array([-0.44324043], dtype=float32), -0.36937863]. 
=============================================
[2019-03-27 02:09:23,151] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1159118: loss -49.6102
[2019-03-27 02:09:23,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1159118: learning rate 0.0001
[2019-03-27 02:09:24,901] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1159900: loss -6.6163
[2019-03-27 02:09:24,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1159902: learning rate 0.0001
[2019-03-27 02:09:28,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.404357e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:09:28,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8124
[2019-03-27 02:09:28,134] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4934420051251383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689505.5630190618, 689505.5630190624, 182574.681723728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3709200.0000, 
sim time next is 3709800.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.492888272639752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688731.5604081282, 688731.5604081289, 182489.0766849304], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38902201522861685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19131432233559117, 0.19131432233559137, 0.2723717562461648], 
reward next is 0.7276, 
noisyNet noise sample is [array([0.5889224], dtype=float32), -0.2593587]. 
=============================================
[2019-03-27 02:09:29,260] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1161837: loss 0.3083
[2019-03-27 02:09:29,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1161837: learning rate 0.0001
[2019-03-27 02:09:29,396] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1161895: loss -38.3725
[2019-03-27 02:09:29,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1161895: learning rate 0.0001
[2019-03-27 02:09:29,407] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1161901: loss -102.8714
[2019-03-27 02:09:29,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1161903: learning rate 0.0001
[2019-03-27 02:09:29,471] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1161929: loss 23.0272
[2019-03-27 02:09:29,474] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1161929: learning rate 0.0001
[2019-03-27 02:09:30,197] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1162253: loss -6.8556
[2019-03-27 02:09:30,199] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1162253: learning rate 0.0001
[2019-03-27 02:09:30,342] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1162318: loss -25.5521
[2019-03-27 02:09:30,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1162318: learning rate 0.0001
[2019-03-27 02:09:30,712] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1162483: loss -59.4535
[2019-03-27 02:09:30,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1162483: learning rate 0.0001
[2019-03-27 02:09:30,774] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1162506: loss -82.0460
[2019-03-27 02:09:30,776] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1162508: learning rate 0.0001
[2019-03-27 02:09:30,835] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1162535: loss 0.3642
[2019-03-27 02:09:30,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1162536: learning rate 0.0001
[2019-03-27 02:09:30,945] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1162584: loss -23.5655
[2019-03-27 02:09:30,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1162584: learning rate 0.0001
[2019-03-27 02:09:31,251] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1162721: loss -11.4948
[2019-03-27 02:09:31,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1162721: learning rate 0.0001
[2019-03-27 02:09:35,015] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:09:35,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6909
[2019-03-27 02:09:35,020] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.5633243643455713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787191.0640905906, 787191.0640905906, 194112.5849742868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3833400.0000, 
sim time next is 3834000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.563359603511998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787240.3256501538, 787240.3256501532, 194118.7997809594], 
processed observation next is [0.0, 0.391304347826087, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4739272331469855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21867786823615384, 0.21867786823615368, 0.2897295519118797], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.38193175], dtype=float32), -0.2930337]. 
=============================================
[2019-03-27 02:09:35,049] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1164414: loss 0.0463
[2019-03-27 02:09:35,050] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.11711 ]
 [71.08841 ]
 [71.05847 ]
 [71.032394]
 [71.02642 ]], R is [[71.2399826 ]
 [71.23786163]
 [71.23575592]
 [71.23364258]
 [71.23131561]].
[2019-03-27 02:09:35,051] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1164415: learning rate 0.0001
[2019-03-27 02:09:37,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.85239e-38 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sum to 1.0000
[2019-03-27 02:09:37,482] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3875
[2019-03-27 02:09:37,487] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 73.0, 1.0, 2.0, 0.5404450279479359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755208.0488084322, 755208.0488084329, 190175.6901563578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3878400.0000, 
sim time next is 3879000.0000, 
raw observation next is [29.5, 74.5, 1.0, 2.0, 0.5422619115277846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757747.8344441624, 757747.8344441624, 190482.7369363615], 
processed observation next is [0.0, 0.9130434782608695, 0.5971563981042655, 0.745, 1.0, 1.0, 0.4485083271419091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2104855095678229, 0.2104855095678229, 0.2843025924423306], 
reward next is 0.7157, 
noisyNet noise sample is [array([1.1864654], dtype=float32), 0.39796987]. 
=============================================
[2019-03-27 02:09:37,504] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.00316 ]
 [71.048874]
 [71.12111 ]
 [71.098305]
 [71.15252 ]], R is [[70.98199463]
 [70.98833466]
 [70.99403381]
 [71.00143433]
 [71.00865173]].
[2019-03-27 02:09:37,530] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1165523: loss 0.1460
[2019-03-27 02:09:37,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1165524: learning rate 0.0001
[2019-03-27 02:09:39,597] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1166441: loss 5.6542
[2019-03-27 02:09:39,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1166443: learning rate 0.0001
[2019-03-27 02:09:40,983] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1167033: loss 0.1147
[2019-03-27 02:09:40,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1167034: learning rate 0.0001
[2019-03-27 02:09:42,720] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1167813: loss 0.0923
[2019-03-27 02:09:42,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1167816: learning rate 0.0001
[2019-03-27 02:09:42,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3992333e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:09:42,907] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6813
[2019-03-27 02:09:42,915] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 60.5, 1.0, 2.0, 0.6015151681422277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840580.1191057914, 840580.1191057914, 201028.9345735524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.59795932202276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835609.0901588723, 835609.0901588729, 200366.724353474], 
processed observation next is [0.0, 0.782608695652174, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5156136409912772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2321136361552423, 0.23211363615524247, 0.29905481246787163], 
reward next is 0.7009, 
noisyNet noise sample is [array([2.6103358], dtype=float32), 0.06837595]. 
=============================================
[2019-03-27 02:09:45,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3858005e-29 1.0000000e+00 1.8905635e-38 4.8119780e-33 7.4023250e-35], sum to 1.0000
[2019-03-27 02:09:45,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5099
[2019-03-27 02:09:45,068] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.635418770645873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887978.1050642777, 887978.1050642777, 207530.7629131568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4220400.0000, 
sim time next is 4221000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6333670517561034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 885109.6965596054, 885109.696559606, 207127.5277510237], 
processed observation next is [1.0, 0.8695652173913043, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5582735563326547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2458638045998904, 0.24586380459989055, 0.3091455638074981], 
reward next is 0.6909, 
noisyNet noise sample is [array([-0.17953905], dtype=float32), -0.914722]. 
=============================================
[2019-03-27 02:09:45,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.78517 ]
 [53.682983]
 [54.43483 ]
 [54.403866]
 [54.783443]], R is [[53.81607056]
 [53.96816635]
 [54.11849976]
 [54.26855469]
 [54.41927719]].
[2019-03-27 02:09:47,137] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1169794: loss 0.1188
[2019-03-27 02:09:47,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1169794: learning rate 0.0001
[2019-03-27 02:09:47,305] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1169873: loss 0.0997
[2019-03-27 02:09:47,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1169873: learning rate 0.0001
[2019-03-27 02:09:47,327] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1169880: loss 0.0657
[2019-03-27 02:09:47,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1169880: learning rate 0.0001
[2019-03-27 02:09:47,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4477522e-11 9.9995041e-01 2.3522490e-18 4.6311525e-05 3.2137771e-06], sum to 1.0000
[2019-03-27 02:09:47,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6934
[2019-03-27 02:09:47,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2856111.941383308 W.
[2019-03-27 02:09:47,404] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 75.83333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.717365760740556, 6.9112, 168.9084694448361, 2856111.941383308, 2284205.397368127, 474272.5849082871], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4035000.0000, 
sim time next is 4035600.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5733584501253887, 1.0, 1.0, 0.5733584501253887, 1.0, 2.0, 0.9957340153308943, 6.911199999999999, 6.9112, 170.5573041426782, 2405412.685509244, 2405412.685509245, 469581.4426286069], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.48597403629564895, 1.0, 0.5, 0.48597403629564895, 1.0, 1.0, 0.9947975796718223, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6681701904192344, 0.6681701904192346, 0.7008678248188163], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.54342276], dtype=float32), -0.38445038]. 
=============================================
[2019-03-27 02:09:47,488] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1169952: loss 6.1296
[2019-03-27 02:09:47,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1169952: learning rate 0.0001
[2019-03-27 02:09:47,975] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1170171: loss 0.0415
[2019-03-27 02:09:47,983] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1170172: learning rate 0.0001
[2019-03-27 02:09:48,155] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1170250: loss 0.0278
[2019-03-27 02:09:48,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1170250: learning rate 0.0001
[2019-03-27 02:09:48,553] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1170427: loss 0.0110
[2019-03-27 02:09:48,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1170428: learning rate 0.0001
[2019-03-27 02:09:48,608] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1170452: loss 0.0040
[2019-03-27 02:09:48,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1170455: learning rate 0.0001
[2019-03-27 02:09:48,690] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1170487: loss 0.0172
[2019-03-27 02:09:48,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1170487: learning rate 0.0001
[2019-03-27 02:09:48,723] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1170502: loss 0.0041
[2019-03-27 02:09:48,732] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1170503: learning rate 0.0001
[2019-03-27 02:09:49,031] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1170641: loss 0.0087
[2019-03-27 02:09:49,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1170641: learning rate 0.0001
[2019-03-27 02:09:49,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0943872e-12 9.9999928e-01 2.6084795e-20 5.8159350e-07 1.3611411e-07], sum to 1.0000
[2019-03-27 02:09:49,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1457
[2019-03-27 02:09:49,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2533732.209319475 W.
[2019-03-27 02:09:49,515] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 53.66666666666667, 1.0, 2.0, 0.9058709118937972, 1.0, 2.0, 0.9058709118937972, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2533732.209319475, 2533732.209319476, 474714.0677886333], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4194600.0000, 
sim time next is 4195200.0000, 
raw observation next is [36.0, 54.33333333333334, 1.0, 2.0, 0.7995961941155979, 1.0, 2.0, 0.7203881365720615, 1.0, 1.0, 1.03, 7.005105586537755, 6.9112, 170.5573041426782, 3022993.127600608, 2955724.736128579, 554632.2047383895], 
processed observation next is [1.0, 0.5652173913043478, 0.9052132701421801, 0.5433333333333334, 1.0, 1.0, 0.7585496314645758, 1.0, 1.0, 0.663118236833809, 1.0, 0.5, 1.0365853658536586, 0.009390558653775471, 0.0, 0.8375144448122397, 0.839720313222391, 0.8210346489246052, 0.8278092608035663], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23655346], dtype=float32), 0.35146764]. 
=============================================
[2019-03-27 02:09:51,802] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8178071e-30 1.0000000e+00 0.0000000e+00 3.4766638e-33 1.0686621e-35], sum to 1.0000
[2019-03-27 02:09:51,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5377
[2019-03-27 02:09:51,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5995710417833949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837862.2515906361, 837862.2515906361, 200666.1458993476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4235400.0000, 
sim time next is 4236000.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.598794308316437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836776.3885920225, 836776.3885920225, 200521.6796239286], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5166196485740204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23243788572000623, 0.23243788572000623, 0.29928608899093817], 
reward next is 0.7007, 
noisyNet noise sample is [array([0.59173006], dtype=float32), -0.4575653]. 
=============================================
[2019-03-27 02:09:51,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.782314]
 [58.63187 ]
 [60.09105 ]
 [62.19128 ]
 [65.65157 ]], R is [[57.43812561]
 [57.56424332]
 [57.68867493]
 [57.81188965]
 [57.93407059]].
[2019-03-27 02:09:53,317] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1172546: loss 7.6949
[2019-03-27 02:09:53,320] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1172547: learning rate 0.0001
[2019-03-27 02:09:55,709] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1173608: loss 23.5804
[2019-03-27 02:09:55,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1173608: learning rate 0.0001
[2019-03-27 02:09:57,270] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1174298: loss 0.6486
[2019-03-27 02:09:57,278] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1174299: learning rate 0.0001
[2019-03-27 02:09:58,848] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 02:09:58,849] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:09:58,850] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:09:58,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:09:58,851] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:09:58,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:09:58,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:09:58,852] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:09:58,856] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:09:58,858] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:09:58,856] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:09:58,880] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-27 02:09:58,880] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-27 02:09:58,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-27 02:09:58,919] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-27 02:09:58,970] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-27 02:10:04,535] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:10:04,536] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.46666666666667, 45.33333333333334, 1.0, 2.0, 0.2179441235471362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 363408.1500635073, 363408.1500635073, 157310.8012338882]
[2019-03-27 02:10:04,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:10:04,541] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5288985320301637
[2019-03-27 02:10:20,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:10:20,472] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.06666666666667, 88.66666666666667, 1.0, 2.0, 0.2242693341027891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 373599.1926806182, 373599.1926806176, 157977.7903605663]
[2019-03-27 02:10:20,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:10:20,478] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31254506710336816
[2019-03-27 02:10:53,542] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:10:53,543] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.74990177333333, 56.03744836, 1.0, 2.0, 0.7356854859901114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1028165.5699086, 1028165.569908599, 228732.1175376549]
[2019-03-27 02:10:53,545] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:10:53,548] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.846068e-34 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.5627797576972219
[2019-03-27 02:10:56,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:10:56,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 56.0, 1.0, 2.0, 0.5985336799580335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836412.0338814144, 836412.0338814144, 200473.6040636892]
[2019-03-27 02:10:56,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:10:56,938] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.703504e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.3668780176193456
[2019-03-27 02:10:57,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:10:57,199] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.47540037, 62.77739141, 1.0, 2.0, 0.7097595977470635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 991915.6299942532, 991915.6299942532, 222978.5462990208]
[2019-03-27 02:10:57,201] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:10:57,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.31374e-37 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sampled 0.043623580767160774
[2019-03-27 02:11:33,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08281019]
[2019-03-27 02:11:33,251] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.783952635, 84.62882033, 1.0, 2.0, 0.4241323125755267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624195.5294350657, 624195.5294350657, 176584.1582572136]
[2019-03-27 02:11:33,251] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:11:33,254] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9149608015135656
[2019-03-27 02:11:53,151] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.0609 3164060861.7607 1783.0000
[2019-03-27 02:11:53,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 02:11:53,341] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1889 3007739130.6762 1766.0000
[2019-03-27 02:11:53,393] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.0959 2779207760.7447 934.0000
[2019-03-27 02:11:53,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0835 2842533343.5788 1134.0000
[2019-03-27 02:11:54,518] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1175000, evaluation results [1175000.0, 7881.06089606417, 3164060861.760722, 1783.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.095904163647, 2779207760.7446876, 934.0, 7998.1889210547915, 3007739130.6762238, 1766.0, 8496.083503226313, 2842533343.5788054, 1134.0]
[2019-03-27 02:11:54,791] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1175128: loss -0.2918
[2019-03-27 02:11:54,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1175128: learning rate 0.0001
[2019-03-27 02:11:56,623] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1175946: loss -6.9795
[2019-03-27 02:11:56,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1175947: learning rate 0.0001
[2019-03-27 02:11:57,972] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.0107258e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:11:57,981] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6677
[2019-03-27 02:11:57,985] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.83333333333334, 63.66666666666666, 1.0, 2.0, 0.6287773488097511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 878693.083891884, 878693.0838918834, 206229.6206891034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4306200.0000, 
sim time next is 4306800.0000, 
raw observation next is [33.66666666666667, 64.33333333333334, 1.0, 2.0, 0.6134271288844174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857233.0625851285, 857233.0625851285, 203274.8919988583], 
processed observation next is [1.0, 0.8695652173913043, 0.7946287519747238, 0.6433333333333334, 1.0, 1.0, 0.5342495528727921, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2381202951625357, 0.2381202951625357, 0.30339536119232585], 
reward next is 0.6966, 
noisyNet noise sample is [array([0.6480178], dtype=float32), 1.4013922]. 
=============================================
[2019-03-27 02:12:00,507] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1177688: loss 0.9151
[2019-03-27 02:12:00,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1177689: learning rate 0.0001
[2019-03-27 02:12:00,668] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1177758: loss 12.2788
[2019-03-27 02:12:00,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1177758: learning rate 0.0001
[2019-03-27 02:12:00,977] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1177896: loss 6.8704
[2019-03-27 02:12:00,979] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1177897: learning rate 0.0001
[2019-03-27 02:12:00,983] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1177900: loss 25.1902
[2019-03-27 02:12:00,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1177900: learning rate 0.0001
[2019-03-27 02:12:01,848] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1178278: loss -25.0019
[2019-03-27 02:12:01,849] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1178279: learning rate 0.0001
[2019-03-27 02:12:01,988] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1178346: loss -61.9059
[2019-03-27 02:12:01,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1178346: learning rate 0.0001
[2019-03-27 02:12:02,333] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1178494: loss -30.9594
[2019-03-27 02:12:02,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1178494: learning rate 0.0001
[2019-03-27 02:12:02,558] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1178595: loss -77.1911
[2019-03-27 02:12:02,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1178596: learning rate 0.0001
[2019-03-27 02:12:02,626] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1178620: loss -40.7654
[2019-03-27 02:12:02,627] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1178620: loss -126.6718
[2019-03-27 02:12:02,628] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1178620: learning rate 0.0001
[2019-03-27 02:12:02,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1178620: learning rate 0.0001
[2019-03-27 02:12:03,006] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1178789: loss 19.9268
[2019-03-27 02:12:03,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1178789: learning rate 0.0001
[2019-03-27 02:12:05,770] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2308633e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:12:05,780] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5748
[2019-03-27 02:12:05,790] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6172275254812768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862546.0791018306, 862546.0791018312, 203999.608627941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4406400.0000, 
sim time next is 4407000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6167573076257186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861888.7054186263, 861888.705418627, 203909.6718430076], 
processed observation next is [0.0, 0.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5382618164165284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23941352928295176, 0.23941352928295195, 0.30434279379553375], 
reward next is 0.6957, 
noisyNet noise sample is [array([0.22763099], dtype=float32), 0.17732465]. 
=============================================
[2019-03-27 02:12:05,807] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.88557 ]
 [67.66116 ]
 [67.79252 ]
 [67.911514]
 [68.01819 ]], R is [[68.84767914]
 [68.8547287 ]
 [68.86164093]
 [68.86849213]
 [68.87535095]].
[2019-03-27 02:12:06,467] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1180320: loss 0.0803
[2019-03-27 02:12:06,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1180321: learning rate 0.0001
[2019-03-27 02:12:08,997] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1181449: loss 0.2487
[2019-03-27 02:12:08,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1181449: learning rate 0.0001
[2019-03-27 02:12:10,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3353511e-26 1.0000000e+00 9.7070395e-35 5.0840620e-29 2.4902522e-30], sum to 1.0000
[2019-03-27 02:12:10,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9285
[2019-03-27 02:12:10,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.698444760184059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 976095.4535506312, 976095.4535506305, 220507.5525312316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4946400.0000, 
sim time next is 4947000.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.7237251118654191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011442.253608252, 1011442.253608251, 226047.3621503903], 
processed observation next is [1.0, 0.2608695652173913, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.6671386889944808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2809561815578478, 0.2809561815578475, 0.3373841226125229], 
reward next is 0.6626, 
noisyNet noise sample is [array([1.45103], dtype=float32), 0.1172733]. 
=============================================
[2019-03-27 02:12:10,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[42.886494]
 [41.695225]
 [43.28465 ]
 [45.401905]
 [45.613625]], R is [[43.22887421]
 [43.46746826]
 [43.64371872]
 [43.83493042]
 [44.07951355]].
[2019-03-27 02:12:11,052] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1182367: loss 230.4336
[2019-03-27 02:12:11,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1182368: learning rate 0.0001
[2019-03-27 02:12:12,312] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:12:12,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9837
[2019-03-27 02:12:12,330] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5096431343431829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712151.6148201709, 712151.6148201709, 185120.5779413317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5107364156530574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713679.8284751853, 713679.8284751853, 185295.2089790481], 
processed observation next is [0.0, 0.2608695652173913, 0.470774091627172, 0.8483333333333333, 1.0, 1.0, 0.41052580199163535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1982443967986626, 0.1982443967986626, 0.27656001340156433], 
reward next is 0.7234, 
noisyNet noise sample is [array([-2.7038999], dtype=float32), -1.1535143]. 
=============================================
[2019-03-27 02:12:12,435] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1182980: loss 0.4909
[2019-03-27 02:12:12,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1182981: learning rate 0.0001
[2019-03-27 02:12:14,357] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1183844: loss 0.1080
[2019-03-27 02:12:14,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1183844: learning rate 0.0001
[2019-03-27 02:12:18,305] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1185613: loss 0.2508
[2019-03-27 02:12:18,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1185613: learning rate 0.0001
[2019-03-27 02:12:18,731] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1185799: loss 118.6207
[2019-03-27 02:12:18,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1185799: learning rate 0.0001
[2019-03-27 02:12:18,747] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1185805: loss 0.3377
[2019-03-27 02:12:18,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1185807: learning rate 0.0001
[2019-03-27 02:12:18,833] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1185847: loss 0.2063
[2019-03-27 02:12:18,837] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1185847: learning rate 0.0001
[2019-03-27 02:12:19,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8143700e-08 4.5007216e-03 3.1008296e-16 1.0553689e-02 9.8494554e-01], sum to 1.0000
[2019-03-27 02:12:19,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8212
[2019-03-27 02:12:19,384] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.7377575492477914, 1.0, 2.0, 0.6894688141381583, 1.0, 1.0, 1.03, 7.005100709492496, 6.9112, 170.5573041426782, 2893094.893864238, 2825829.996018114, 533521.1202784328], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4629600.0000, 
sim time next is 4630200.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.7183680302974753, 1.0, 2.0, 0.6797740546630003, 1.0, 2.0, 1.03, 7.00509918050277, 6.9112, 170.5573041426782, 2852368.059483307, 2785104.256914719, 527194.6380566033], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.6606843738523799, 1.0, 1.0, 0.6141856080277113, 1.0, 1.0, 1.0365853658536586, 0.009389918050276957, 0.0, 0.8375144448122397, 0.7923244609675852, 0.7736400713651997, 0.7868576687411989], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0075078], dtype=float32), -0.2999269]. 
=============================================
[2019-03-27 02:12:19,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0700495e-09 7.0428022e-04 1.4721662e-15 7.1488512e-03 9.9214691e-01], sum to 1.0000
[2019-03-27 02:12:19,597] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3550
[2019-03-27 02:12:19,605] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.9023138907319616, 1.0, 2.0, 0.7717469848802434, 1.0, 1.0, 1.03, 7.005113689843539, 6.9112, 170.5573041426782, 3238791.744652032, 3171517.548452448, 592850.1585468209], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4633200.0000, 
sim time next is 4633800.0000, 
raw observation next is [34.66666666666667, 61.16666666666666, 1.0, 2.0, 0.7134752663560746, 1.0, 2.0, 0.6773276726922999, 1.0, 2.0, 1.03, 7.005098794692243, 6.9112, 170.5573041426782, 2842091.233163978, 2774827.706967163, 525620.2369738244], 
processed observation next is [1.0, 0.6521739130434783, 0.8420221169036337, 0.6116666666666666, 1.0, 1.0, 0.6547894775374393, 1.0, 1.0, 0.6112381598702408, 1.0, 1.0, 1.0365853658536586, 0.009389879469224293, 0.0, 0.8375144448122397, 0.7894697869899939, 0.7707854741575453, 0.7845078163788425], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0829053], dtype=float32), 0.14277169]. 
=============================================
[2019-03-27 02:12:19,683] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1186211: loss 0.0507
[2019-03-27 02:12:19,687] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1186212: learning rate 0.0001
[2019-03-27 02:12:19,889] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1186301: loss 0.1092
[2019-03-27 02:12:19,893] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1186302: learning rate 0.0001
[2019-03-27 02:12:20,252] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1186462: loss 0.1110
[2019-03-27 02:12:20,255] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1186464: learning rate 0.0001
[2019-03-27 02:12:20,469] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1186554: loss 0.0629
[2019-03-27 02:12:20,471] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1186554: learning rate 0.0001
[2019-03-27 02:12:20,634] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1186630: loss 0.0960
[2019-03-27 02:12:20,637] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1186630: learning rate 0.0001
[2019-03-27 02:12:20,767] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1186690: loss 0.2065
[2019-03-27 02:12:20,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1186690: learning rate 0.0001
[2019-03-27 02:12:20,953] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1186772: loss 0.0872
[2019-03-27 02:12:20,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1186772: learning rate 0.0001
[2019-03-27 02:12:24,607] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1188396: loss 75.2323
[2019-03-27 02:12:24,610] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1188397: learning rate 0.0001
[2019-03-27 02:12:27,122] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1189526: loss 59.1125
[2019-03-27 02:12:27,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1189527: learning rate 0.0001
[2019-03-27 02:12:28,882] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1190320: loss 0.0520
[2019-03-27 02:12:28,886] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1190320: learning rate 0.0001
[2019-03-27 02:12:28,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4293344e-27 1.0000000e+00 0.0000000e+00 4.3583087e-30 2.6529464e-28], sum to 1.0000
[2019-03-27 02:12:28,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5810
[2019-03-27 02:12:28,984] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 79.16666666666667, 1.0, 2.0, 0.5478126691939763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765507.1664619223, 765507.1664619229, 191426.2368811697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5256600.0000, 
sim time next is 5257200.0000, 
raw observation next is [28.73333333333333, 79.33333333333334, 1.0, 2.0, 0.5484807282593991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766441.0413705155, 766441.0413705155, 191540.3767640373], 
processed observation next is [1.0, 0.8695652173913043, 0.560821484992101, 0.7933333333333334, 1.0, 1.0, 0.45600087742096274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21290028926958762, 0.21290028926958762, 0.2858811593493094], 
reward next is 0.7141, 
noisyNet noise sample is [array([0.11546776], dtype=float32), 0.4957138]. 
=============================================
[2019-03-27 02:12:30,248] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1190926: loss 54.7957
[2019-03-27 02:12:30,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1190926: learning rate 0.0001
[2019-03-27 02:12:31,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5277721e-27 1.0000000e+00 2.4203850e-37 1.1390898e-31 1.3625125e-29], sum to 1.0000
[2019-03-27 02:12:31,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6463
[2019-03-27 02:12:31,509] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.7316039499709531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022458.62703462, 1022458.62703462, 227813.60955987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4867200.0000, 
sim time next is 4867800.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.8029879221334711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1122274.554489039, 1122274.554489039, 244644.3791342582], 
processed observation next is [1.0, 0.34782608695652173, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.7626360507632183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3117429318025108, 0.3117429318025108, 0.36514086437948984], 
reward next is 0.6349, 
noisyNet noise sample is [array([1.4843196], dtype=float32), -0.9345645]. 
=============================================
[2019-03-27 02:12:32,516] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1191940: loss 86.8591
[2019-03-27 02:12:32,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1191940: learning rate 0.0001
[2019-03-27 02:12:32,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0843266e-26 1.0000000e+00 7.9460469e-36 1.3064027e-29 4.7737806e-27], sum to 1.0000
[2019-03-27 02:12:32,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0882
[2019-03-27 02:12:32,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.66666666666667, 1.0, 2.0, 0.6381011400471721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 891728.2070959712, 891728.2070959718, 208050.6939751662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.6465221921074618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903501.3935972198, 903501.3935972198, 209724.8204450519], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.865, 1.0, 1.0, 0.5741231230210383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25097260933256105, 0.25097260933256105, 0.31302212006724167], 
reward next is 0.6870, 
noisyNet noise sample is [array([-0.6572978], dtype=float32), -0.9937083]. 
=============================================
[2019-03-27 02:12:34,552] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0259880e-17 1.0000000e+00 1.8381568e-25 2.0911541e-16 1.5364646e-10], sum to 1.0000
[2019-03-27 02:12:34,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2519
[2019-03-27 02:12:34,571] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 64.16666666666667, 1.0, 2.0, 0.2411518917110253, 1.0, 2.0, 0.2411518917110253, 1.0, 2.0, 0.413173528095124, 6.9112, 6.9112, 170.5573041426782, 1011060.882715614, 1011060.882715614, 281997.8955745593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [30.66666666666667, 65.33333333333334, 1.0, 2.0, 0.4967512454452823, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694131.2030626077, 694131.2030626077, 183090.6905307434], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.6533333333333334, 1.0, 1.0, 0.3936761993316655, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19281422307294657, 0.19281422307294657, 0.2732696873593185], 
reward next is 0.7267, 
noisyNet noise sample is [array([1.1574876], dtype=float32), -1.0195094]. 
=============================================
[2019-03-27 02:12:36,013] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1193489: loss 98.0912
[2019-03-27 02:12:36,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1193490: learning rate 0.0001
[2019-03-27 02:12:36,538] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0910176e-18 1.0000000e+00 1.0527176e-25 8.8318932e-19 1.0156669e-14], sum to 1.0000
[2019-03-27 02:12:36,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2645
[2019-03-27 02:12:36,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1932009.085511253 W.
[2019-03-27 02:12:36,567] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 67.33333333333334, 1.0, 2.0, 0.6909193238433284, 1.0, 2.0, 0.6909193238433284, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1932009.085511253, 1932009.085511253, 370256.4259230974], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4963200.0000, 
sim time next is 4963800.0000, 
raw observation next is [30.0, 66.66666666666666, 1.0, 2.0, 0.4534959682945399, 1.0, 2.0, 0.4534959682945399, 1.0, 1.0, 0.7752313731177373, 6.911199999999999, 6.9112, 170.5573041426782, 1902131.178945099, 1902131.178945099, 382464.7839198866], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6666666666666665, 1.0, 1.0, 0.34156140758378306, 1.0, 1.0, 0.34156140758378306, 1.0, 0.5, 0.7258919184362649, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5283697719291942, 0.5283697719291942, 0.5708429610744576], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32627994], dtype=float32), 1.2683333]. 
=============================================
[2019-03-27 02:12:36,660] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1193781: loss 0.1376
[2019-03-27 02:12:36,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1193781: learning rate 0.0001
[2019-03-27 02:12:36,752] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.710586e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:12:36,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7618
[2019-03-27 02:12:36,767] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 61.66666666666667, 1.0, 2.0, 0.5224315252581876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730027.658095248, 730027.6580952486, 187185.5399929009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [31.5, 61.0, 1.0, 2.0, 0.520904719009403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727893.4211147603, 727893.4211147596, 186936.6864803091], 
processed observation next is [0.0, 0.6086956521739131, 0.6919431279620853, 0.61, 1.0, 1.0, 0.422776769890847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20219261697632232, 0.20219261697632213, 0.27900997982135683], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.3421668], dtype=float32), 0.46917027]. 
=============================================
[2019-03-27 02:12:36,792] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1193842: loss 104.2419
[2019-03-27 02:12:36,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1193843: learning rate 0.0001
[2019-03-27 02:12:36,917] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1193894: loss 108.5734
[2019-03-27 02:12:36,919] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1193894: learning rate 0.0001
[2019-03-27 02:12:37,758] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1194268: loss 79.9575
[2019-03-27 02:12:37,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1194269: learning rate 0.0001
[2019-03-27 02:12:37,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1194341: loss 55.0299
[2019-03-27 02:12:37,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1194341: learning rate 0.0001
[2019-03-27 02:12:38,208] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1194476: loss 59.0145
[2019-03-27 02:12:38,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1194476: learning rate 0.0001
[2019-03-27 02:12:38,317] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1194520: loss 52.5290
[2019-03-27 02:12:38,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1194521: learning rate 0.0001
[2019-03-27 02:12:38,552] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1194626: loss 52.5734
[2019-03-27 02:12:38,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1194627: learning rate 0.0001
[2019-03-27 02:12:38,622] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1194654: loss 83.5140
[2019-03-27 02:12:38,626] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1194657: learning rate 0.0001
[2019-03-27 02:12:38,758] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1194716: loss 66.6446
[2019-03-27 02:12:38,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1194716: learning rate 0.0001
[2019-03-27 02:12:42,072] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1196198: loss 0.0324
[2019-03-27 02:12:42,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1196198: learning rate 0.0001
[2019-03-27 02:12:44,800] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1197415: loss 0.0395
[2019-03-27 02:12:44,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1197415: learning rate 0.0001
[2019-03-27 02:12:46,711] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1198266: loss 0.0722
[2019-03-27 02:12:46,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1198266: learning rate 0.0001
[2019-03-27 02:12:47,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2787897e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7661495e-37], sum to 1.0000
[2019-03-27 02:12:47,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3028
[2019-03-27 02:12:47,118] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5152269686468606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719956.849360887, 719956.8493608875, 186015.8832407366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5094000.0000, 
sim time next is 5094600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.516471439461945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721696.412124088, 721696.4121240875, 186216.6336728786], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41743546923125896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20047122559002445, 0.20047122559002428, 0.2779352741386248], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.01272377], dtype=float32), -0.12616552]. 
=============================================
[2019-03-27 02:12:48,191] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1198927: loss 0.2402
[2019-03-27 02:12:48,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1198927: learning rate 0.0001
[2019-03-27 02:12:50,530] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1199963: loss 0.0367
[2019-03-27 02:12:50,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1199964: learning rate 0.0001
[2019-03-27 02:12:50,614] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 02:12:50,618] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:12:50,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:12:50,621] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:12:50,622] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:12:50,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:12:50,624] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:12:50,622] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:12:50,625] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:12:50,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:12:50,629] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:12:50,649] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-27 02:12:50,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-27 02:12:50,687] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-27 02:12:50,707] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-27 02:12:50,708] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-27 02:13:31,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08886252]
[2019-03-27 02:13:31,373] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.93333333333333, 67.16666666666666, 1.0, 2.0, 0.5491507768235642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767377.6982096543, 767377.6982096549, 191655.2066249185]
[2019-03-27 02:13:31,374] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:13:31,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.8113343e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5345066739649534
[2019-03-27 02:13:52,081] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08886252]
[2019-03-27 02:13:52,084] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 55.66666666666667, 1.0, 2.0, 0.8830507769432467, 1.0, 2.0, 0.7621154279858859, 1.0, 2.0, 1.03, 7.005112169980768, 6.9112, 170.5573041426782, 3198319.213994929, 3131046.106534873, 585383.0232368185]
[2019-03-27 02:13:52,085] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:13:52,087] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4708258e-15 1.6758025e-12 3.8726343e-23 3.3830393e-08 1.0000000e+00], sampled 0.15308025516373214
[2019-03-27 02:14:05,255] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08886252]
[2019-03-27 02:14:05,256] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.3, 77.0, 1.0, 2.0, 0.5897717597224729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 824163.0676694204, 824163.0676694198, 198855.7347723629]
[2019-03-27 02:14:05,257] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:14:05,259] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3105773e-31 1.0000000e+00 0.0000000e+00 1.0599368e-36 4.9565895e-33], sampled 0.8189355156399397
[2019-03-27 02:14:14,592] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08886252]
[2019-03-27 02:14:14,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.75, 56.0, 1.0, 2.0, 0.6029227406181299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 842547.8944659401, 842547.8944659407, 201291.3311612885]
[2019-03-27 02:14:14,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:14:14,599] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.14301096e-32 1.00000000e+00 0.00000000e+00 0.00000000e+00
 2.22353825e-36], sampled 0.24618066777384418
[2019-03-27 02:14:26,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.08886252]
[2019-03-27 02:14:26,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.8, 95.0, 1.0, 2.0, 0.54289478418599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 758632.516046979, 758632.5160469783, 190585.7856484646]
[2019-03-27 02:14:26,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:14:26,928] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.2055805e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5215987e-37], sampled 0.9646383769645362
[2019-03-27 02:14:45,078] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8545.9213 2839529262.3970 1016.0000
[2019-03-27 02:14:45,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8119.3627 3001917095.5414 1447.0000
[2019-03-27 02:14:45,180] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7960.7686 3159973604.2303 1682.0000
[2019-03-27 02:14:45,190] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8681.0043 2779307183.4091 885.0000
[2019-03-27 02:14:45,254] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8286.1537 2926221555.8656 1265.0000
[2019-03-27 02:14:46,271] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1200000, evaluation results [1200000.0, 7960.768649846382, 3159973604.2302833, 1682.0, 8286.153719413722, 2926221555.8656235, 1265.0, 8681.004295148063, 2779307183.409067, 885.0, 8119.362724635065, 3001917095.5413857, 1447.0, 8545.921259533237, 2839529262.3970146, 1016.0]
[2019-03-27 02:14:47,951] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.35970088e-18 1.00000000e+00 1.21758685e-26 4.91052984e-21
 4.58119887e-16], sum to 1.0000
[2019-03-27 02:14:47,960] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2031
[2019-03-27 02:14:47,969] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.8712444868087609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217726.216471143, 1217726.216471143, 262147.7242955408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5197200.0000, 
sim time next is 5197800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.8397750199891579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173717.47252946, 1173717.47252946, 253905.8636983433], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.806957855408624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3260326312581833, 0.3260326312581833, 0.3789639756691691], 
reward next is 0.6210, 
noisyNet noise sample is [array([0.94179934], dtype=float32), -0.7275581]. 
=============================================
[2019-03-27 02:14:49,774] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1201574: loss 0.2529
[2019-03-27 02:14:49,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1201574: learning rate 0.0001
[2019-03-27 02:14:50,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3611615e-10 9.9539435e-01 1.3685277e-15 3.8274179e-09 4.6056490e-03], sum to 1.0000
[2019-03-27 02:14:50,150] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1201735: loss 0.7797
[2019-03-27 02:14:50,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1201735: learning rate 0.0001
[2019-03-27 02:14:50,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7494
[2019-03-27 02:14:50,168] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2325574.985274088 W.
[2019-03-27 02:14:50,173] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5543458847622855, 1.0, 2.0, 0.5543458847622855, 1.0, 2.0, 0.9627154768466291, 6.9112, 6.9112, 170.5573041426782, 2325574.985274088, 2325574.985274088, 454762.0577789375], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5229000.0000, 
sim time next is 5229600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.7790464831313264, 1.0, 2.0, 0.7790464831313264, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2178688.550619254, 2178688.550619254, 409805.2132262312], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.7337909435317186, 1.0, 1.0, 0.7337909435317186, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6051912640609038, 0.6051912640609038, 0.6116495719794496], 
reward next is 0.3884, 
noisyNet noise sample is [array([-0.49255082], dtype=float32), 0.1678541]. 
=============================================
[2019-03-27 02:14:50,482] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1201883: loss 0.3798
[2019-03-27 02:14:50,485] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1201883: learning rate 0.0001
[2019-03-27 02:14:50,559] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1201915: loss 0.3222
[2019-03-27 02:14:50,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1201917: learning rate 0.0001
[2019-03-27 02:14:51,497] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1202332: loss 0.1332
[2019-03-27 02:14:51,499] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1202333: learning rate 0.0001
[2019-03-27 02:14:51,752] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1202444: loss 0.0310
[2019-03-27 02:14:51,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1202444: learning rate 0.0001
[2019-03-27 02:14:52,079] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1202589: loss 0.6971
[2019-03-27 02:14:52,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1202590: learning rate 0.0001
[2019-03-27 02:14:52,137] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1202616: loss 0.3543
[2019-03-27 02:14:52,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1202616: learning rate 0.0001
[2019-03-27 02:14:52,424] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1202742: loss 0.1493
[2019-03-27 02:14:52,427] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1202743: learning rate 0.0001
[2019-03-27 02:14:52,516] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1202782: loss 0.2485
[2019-03-27 02:14:52,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1202784: learning rate 0.0001
[2019-03-27 02:14:52,624] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1202829: loss 0.4910
[2019-03-27 02:14:52,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1202830: learning rate 0.0001
[2019-03-27 02:14:55,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4551121e-09 1.5016522e-01 6.9828292e-14 1.8472898e-04 8.4965008e-01], sum to 1.0000
[2019-03-27 02:14:55,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8601
[2019-03-27 02:14:55,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.4, 60.0, 1.0, 2.0, 0.6469439197308786, 1.0, 2.0, 0.6440619993797018, 1.0, 2.0, 1.03, 7.005093549106371, 6.9112, 170.5573041426782, 2702356.300038926, 2635096.531468648, 505094.3511099438], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5306400.0000, 
sim time next is 5307000.0000, 
raw observation next is [34.73333333333333, 58.5, 1.0, 2.0, 0.6402949398473683, 1.0, 2.0, 0.6402949398473683, 1.0, 2.0, 1.03, 7.003364799399852, 6.9112, 170.5573041426782, 2686533.471638814, 2620512.076804357, 503052.034653267], 
processed observation next is [1.0, 0.43478260869565216, 0.8451816745655606, 0.585, 1.0, 1.0, 0.5666204094546605, 1.0, 1.0, 0.5666204094546605, 1.0, 1.0, 1.0365853658536586, 0.009216479939985155, 0.0, 0.8375144448122397, 0.7462592976774484, 0.7279200213345436, 0.750823932318309], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7260036], dtype=float32), 0.28499955]. 
=============================================
[2019-03-27 02:14:55,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[30.087288]
 [29.588764]
 [29.414127]
 [29.485773]
 [28.94566 ]], R is [[30.16895676]
 [29.86726761]
 [29.56859589]
 [29.27291107]
 [28.98018265]].
[2019-03-27 02:14:55,596] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1204155: loss 0.4583
[2019-03-27 02:14:55,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1204157: learning rate 0.0001
[2019-03-27 02:14:56,620] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5675415e-18 1.0000000e+00 4.0000180e-23 1.0970357e-18 6.1148103e-17], sum to 1.0000
[2019-03-27 02:14:56,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2095
[2019-03-27 02:14:56,631] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.044108480526523, 6.9112, 168.9117372627159, 1548108.672604601, 1453819.502765333, 311349.7603064726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5802000.0000, 
sim time next is 5802600.0000, 
raw observation next is [26.25, 92.66666666666666, 1.0, 2.0, 1.005661400366182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9128482513658, 1405723.234290371, 1405723.234290371, 300655.0206007176], 
processed observation next is [1.0, 0.13043478260869565, 0.4431279620853081, 0.9266666666666665, 1.0, 1.0, 1.0068209642966046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294394135507314, 0.39047867619176974, 0.39047867619176974, 0.44873883671748893], 
reward next is 0.5513, 
noisyNet noise sample is [array([-0.79762286], dtype=float32), -0.28655577]. 
=============================================
[2019-03-27 02:14:58,296] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1205360: loss 2.8518
[2019-03-27 02:14:58,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1205361: learning rate 0.0001
[2019-03-27 02:14:59,863] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1206062: loss 0.3062
[2019-03-27 02:14:59,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1206063: learning rate 0.0001
[2019-03-27 02:15:01,766] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1206912: loss 2.6919
[2019-03-27 02:15:01,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1206913: learning rate 0.0001
[2019-03-27 02:15:04,017] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1207904: loss 1.5577
[2019-03-27 02:15:04,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1207904: learning rate 0.0001
[2019-03-27 02:15:07,697] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1209551: loss 1.2956
[2019-03-27 02:15:07,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1209551: learning rate 0.0001
[2019-03-27 02:15:07,817] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1209602: loss 0.1827
[2019-03-27 02:15:07,821] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1209606: learning rate 0.0001
[2019-03-27 02:15:08,284] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1209812: loss 0.2789
[2019-03-27 02:15:08,286] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1209813: learning rate 0.0001
[2019-03-27 02:15:08,501] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1209909: loss 0.1424
[2019-03-27 02:15:08,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1209909: learning rate 0.0001
[2019-03-27 02:15:09,295] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1210258: loss 0.8425
[2019-03-27 02:15:09,298] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1210259: learning rate 0.0001
[2019-03-27 02:15:09,880] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1210523: loss 0.6159
[2019-03-27 02:15:09,883] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1210524: learning rate 0.0001
[2019-03-27 02:15:10,113] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1210626: loss 0.2879
[2019-03-27 02:15:10,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1210626: learning rate 0.0001
[2019-03-27 02:15:10,242] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1210684: loss 0.1900
[2019-03-27 02:15:10,244] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1210685: learning rate 0.0001
[2019-03-27 02:15:10,415] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1210761: loss 0.0785
[2019-03-27 02:15:10,422] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1210763: learning rate 0.0001
[2019-03-27 02:15:10,630] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1210859: loss 0.2801
[2019-03-27 02:15:10,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1210859: learning rate 0.0001
[2019-03-27 02:15:10,684] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1210875: loss 0.6559
[2019-03-27 02:15:10,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1210875: learning rate 0.0001
[2019-03-27 02:15:13,099] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1211960: loss 1.1116
[2019-03-27 02:15:13,101] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1211961: learning rate 0.0001
[2019-03-27 02:15:15,604] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.077760e-33 1.000000e+00 0.000000e+00 0.000000e+00 1.349472e-37], sum to 1.0000
[2019-03-27 02:15:15,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7296
[2019-03-27 02:15:15,618] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.93333333333334, 64.0, 1.0, 2.0, 0.5579368814429562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779659.8171915929, 779659.8171915929, 193171.5707137766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5660400.0000, 
sim time next is 5661000.0000, 
raw observation next is [32.0, 63.5, 1.0, 2.0, 0.5563401364978955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777427.7126325896, 777427.7126325902, 192894.3859055719], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.635, 1.0, 1.0, 0.46547004397336805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21595214239794155, 0.21595214239794172, 0.28790206851577893], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.38095275], dtype=float32), -0.7436565]. 
=============================================
[2019-03-27 02:15:15,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.92321 ]
 [71.9237  ]
 [71.92419 ]
 [71.833435]
 [71.81971 ]], R is [[71.91898346]
 [71.91147614]
 [71.90354156]
 [71.89609528]
 [71.88881683]].
[2019-03-27 02:15:16,130] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1213309: loss 0.8686
[2019-03-27 02:15:16,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1213310: learning rate 0.0001
[2019-03-27 02:15:16,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1490392e-12 9.9981779e-01 6.5450065e-20 3.0581637e-10 1.8227413e-04], sum to 1.0000
[2019-03-27 02:15:16,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1233
[2019-03-27 02:15:16,889] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333333, 87.16666666666667, 1.0, 2.0, 0.9108189792333158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1273072.091990388, 1273072.091990388, 272930.8928384309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811000.0000, 
sim time next is 5811600.0000, 
raw observation next is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
processed observation next is [1.0, 0.2608695652173913, 0.49605055292259104, 0.8633333333333334, 1.0, 1.0, 0.8555044593521333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34168523383518334, 0.34168523383518307, 0.394796870241407], 
reward next is 0.6052, 
noisyNet noise sample is [array([0.42736354], dtype=float32), 1.7211637]. 
=============================================
[2019-03-27 02:15:17,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8184260e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3641365e-37], sum to 1.0000
[2019-03-27 02:15:17,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2741
[2019-03-27 02:15:17,098] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.3, 60.0, 1.0, 2.0, 0.5416232201597805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756855.0197158619, 756855.0197158626, 190374.9048115258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5670000.0000, 
sim time next is 5670600.0000, 
raw observation next is [32.26666666666667, 60.0, 1.0, 2.0, 0.5599378463623756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782456.9912215542, 782456.9912215542, 193518.6832922029], 
processed observation next is [0.0, 0.6521739130434783, 0.7282780410742499, 0.6, 1.0, 1.0, 0.4698046341715369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21734916422820952, 0.21734916422820952, 0.28883385566000436], 
reward next is 0.7112, 
noisyNet noise sample is [array([0.31775078], dtype=float32), -0.26740742]. 
=============================================
[2019-03-27 02:15:18,130] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1214199: loss 0.2403
[2019-03-27 02:15:18,133] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1214199: learning rate 0.0001
[2019-03-27 02:15:19,646] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1214863: loss 0.0405
[2019-03-27 02:15:19,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1214864: learning rate 0.0001
[2019-03-27 02:15:21,953] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1215892: loss 0.2687
[2019-03-27 02:15:21,955] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1215892: learning rate 0.0001
[2019-03-27 02:15:25,767] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1217586: loss 3.3575
[2019-03-27 02:15:25,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1217589: learning rate 0.0001
[2019-03-27 02:15:26,104] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1217738: loss 0.8040
[2019-03-27 02:15:26,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1217739: learning rate 0.0001
[2019-03-27 02:15:26,324] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1217834: loss 2.0121
[2019-03-27 02:15:26,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1217836: learning rate 0.0001
[2019-03-27 02:15:26,622] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1217968: loss 2.2317
[2019-03-27 02:15:26,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1217968: learning rate 0.0001
[2019-03-27 02:15:27,397] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1218315: loss 2.1091
[2019-03-27 02:15:27,400] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1218316: learning rate 0.0001
[2019-03-27 02:15:27,909] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1218549: loss 0.4146
[2019-03-27 02:15:27,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1218549: learning rate 0.0001
[2019-03-27 02:15:27,985] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1218579: loss 0.1516
[2019-03-27 02:15:27,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1218580: learning rate 0.0001
[2019-03-27 02:15:28,024] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1218597: loss 0.4651
[2019-03-27 02:15:28,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1218597: learning rate 0.0001
[2019-03-27 02:15:28,299] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1218723: loss 0.2284
[2019-03-27 02:15:28,301] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1218723: learning rate 0.0001
[2019-03-27 02:15:28,454] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1218788: loss 0.0435
[2019-03-27 02:15:28,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1218788: learning rate 0.0001
[2019-03-27 02:15:28,555] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1218834: loss 0.0823
[2019-03-27 02:15:28,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1218834: learning rate 0.0001
[2019-03-27 02:15:31,290] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1220055: loss 1.8279
[2019-03-27 02:15:31,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1220056: learning rate 0.0001
[2019-03-27 02:15:34,378] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1221433: loss 0.1098
[2019-03-27 02:15:34,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1221433: learning rate 0.0001
[2019-03-27 02:15:35,877] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1222081: loss 31.8439
[2019-03-27 02:15:35,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1222083: learning rate 0.0001
[2019-03-27 02:15:36,363] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5088751e-21 1.0000000e+00 3.1831103e-32 6.1495928e-25 7.7116642e-19], sum to 1.0000
[2019-03-27 02:15:36,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4697
[2019-03-27 02:15:36,377] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 87.33333333333333, 1.0, 2.0, 0.5352399445195771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747931.999209393, 747931.9992093937, 189301.8224338974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6131400.0000, 
sim time next is 6132000.0000, 
raw observation next is [27.23333333333333, 87.66666666666667, 1.0, 2.0, 0.535146327582743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747801.1350051593, 747801.1350051587, 189286.2240246458], 
processed observation next is [1.0, 1.0, 0.4897314375987361, 0.8766666666666667, 1.0, 1.0, 0.4399353344370398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20772253750143313, 0.207722537501433, 0.28251675227559075], 
reward next is 0.7175, 
noisyNet noise sample is [array([-1.3354325], dtype=float32), -1.570833]. 
=============================================
[2019-03-27 02:15:36,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.82156 ]
 [69.79169 ]
 [69.64383 ]
 [69.639145]
 [69.626114]], R is [[69.89649963]
 [69.91499329]
 [69.9331665 ]
 [69.95089722]
 [69.96832275]].
[2019-03-27 02:15:37,802] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1222938: loss 0.2537
[2019-03-27 02:15:37,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1222938: learning rate 0.0001
[2019-03-27 02:15:38,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5605418e-11 4.6295052e-09 4.9875941e-19 8.2104765e-07 9.9999917e-01], sum to 1.0000
[2019-03-27 02:15:38,321] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2344
[2019-03-27 02:15:38,325] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 86.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 10.80234669686688, 6.9112, 168.8853601891817, 4215511.649550125, 1455449.261463032, 303345.9558527796], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6164400.0000, 
sim time next is 6165000.0000, 
raw observation next is [28.05, 85.5, 1.0, 2.0, 0.7086805885022064, 1.0, 1.0, 0.6749303337653659, 1.0, 1.0, 1.03, 7.005098416622283, 6.9112, 170.5573041426782, 2832020.510185041, 2764757.254815118, 524083.7984907231], 
processed observation next is [1.0, 0.34782608695652173, 0.528436018957346, 0.855, 1.0, 1.0, 0.6490127572315739, 1.0, 0.5, 0.6083497997173083, 1.0, 0.5, 1.0365853658536586, 0.009389841662228272, 0.0, 0.8375144448122397, 0.7866723639402892, 0.7679881263375328, 0.7822146246130195], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55727506], dtype=float32), -0.5404049]. 
=============================================
[2019-03-27 02:15:38,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[46.912605]
 [50.237206]
 [51.08499 ]
 [50.47425 ]
 [50.450333]], R is [[49.7012291 ]
 [49.204216  ]
 [48.71217346]
 [48.86784363]
 [48.95797729]].
[2019-03-27 02:15:40,016] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1223926: loss 0.2023
[2019-03-27 02:15:40,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1223926: learning rate 0.0001
[2019-03-27 02:15:42,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4986130e-11 2.9619244e-01 1.3566534e-18 3.2030744e-08 7.0380753e-01], sum to 1.0000
[2019-03-27 02:15:42,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8469
[2019-03-27 02:15:42,228] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.85, 75.0, 1.0, 2.0, 0.4367464421122426, 1.0, 2.0, 0.4367464421122426, 1.0, 2.0, 0.7497746897733966, 6.911199999999999, 6.9112, 170.5573041426782, 1831817.365791795, 1831817.365791795, 372807.3751455313], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6427800.0000, 
sim time next is 6428400.0000, 
raw observation next is [28.96666666666667, 74.33333333333333, 1.0, 2.0, 0.4792679407840142, 1.0, 2.0, 0.4792679407840142, 1.0, 2.0, 0.8232703206509813, 6.911199999999999, 6.9112, 170.5573041426782, 2010329.859253284, 2010329.859253284, 399538.6284485328], 
processed observation next is [1.0, 0.391304347826087, 0.5718799368088469, 0.7433333333333333, 1.0, 1.0, 0.3726119768482099, 1.0, 1.0, 0.3726119768482099, 1.0, 1.0, 0.7844760007938796, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.55842496090369, 0.55842496090369, 0.5963263111172131], 
reward next is 0.4037, 
noisyNet noise sample is [array([0.5173154], dtype=float32), -0.36192435]. 
=============================================
[2019-03-27 02:15:42,429] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:15:42,431] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:15:42,431] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:15:42,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:15:42,432] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:15:42,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:15:42,433] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:15:42,434] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:15:42,434] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:15:42,435] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:15:42,438] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:15:42,462] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-27 02:15:42,482] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-27 02:15:42,500] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-27 02:15:42,501] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-27 02:15:42,521] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-27 02:15:43,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:15:43,766] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.5, 80.0, 1.0, 2.0, 0.3833341416652205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579451.6744781063, 579451.6744781057, 172887.4795088473]
[2019-03-27 02:15:43,766] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:15:43,769] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.894633e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.2911233129738431
[2019-03-27 02:15:52,100] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:15:52,102] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669]
[2019-03-27 02:15:52,106] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:15:52,109] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.152703e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.2849440668692904
[2019-03-27 02:16:12,749] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:12,750] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.77134036333334, 89.83217757166668, 1.0, 2.0, 0.4109502102707555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611647.4006882858, 611647.4006882858, 175574.915839919]
[2019-03-27 02:16:12,752] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:16:12,756] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0641083e-31 1.0000000e+00 0.0000000e+00 1.8148802e-38 1.8297464e-34], sampled 0.3545985738174695
[2019-03-27 02:16:12,905] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:12,906] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.692154285, 87.8143075, 1.0, 2.0, 0.4811764602387963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672361.019641165, 672361.0196411656, 180699.2894469908]
[2019-03-27 02:16:12,907] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:16:12,911] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.951221e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.026999428096428857
[2019-03-27 02:16:13,656] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:13,659] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.5, 84.0, 1.0, 2.0, 0.4995513328288041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698045.1673368879, 698045.1673368879, 183525.323749321]
[2019-03-27 02:16:13,662] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:16:13,665] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.0650659e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5989417e-36], sampled 0.32880238166980114
[2019-03-27 02:16:14,583] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:14,588] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.26666666666667, 89.0, 1.0, 2.0, 0.4775230987860119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668845.8650506177, 668845.8650506177, 180354.5470897605]
[2019-03-27 02:16:14,589] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:16:14,593] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9530355e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9416803161108891
[2019-03-27 02:16:19,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:19,395] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.49205278, 75.50252961, 1.0, 2.0, 0.8282533520119465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1157605.353960862, 1157605.353960861, 250965.220454985]
[2019-03-27 02:16:19,396] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:16:19,399] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1918466e-27 1.0000000e+00 1.6088344e-37 2.6620242e-32 1.1218599e-27], sampled 0.8941260101754784
[2019-03-27 02:16:30,857] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:30,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5146523632729149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719153.6477883166, 719153.647788316, 185923.4973853219]
[2019-03-27 02:16:30,860] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:16:30,863] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6408032e-30 1.0000000e+00 0.0000000e+00 1.0583530e-36 2.0579790e-32], sampled 0.7319846303804488
[2019-03-27 02:16:42,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.092466794]
[2019-03-27 02:16:42,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.837877805, 94.76378003666666, 1.0, 2.0, 0.5090653931675156, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8668841780105321, 6.9112, 6.9112, 168.9127199547781, 1423164.746963196, 1423164.746963196, 309885.9327335608]
[2019-03-27 02:16:42,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:16:42,510] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0164360e-19 1.0000000e+00 2.3793846e-27 1.0017352e-19 1.5731212e-12], sampled 0.22320684599600327
[2019-03-27 02:17:33,641] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8675.7075 2779461859.0737 907.0000
[2019-03-27 02:17:33,684] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7941.1532 3161867835.3460 1720.0000
[2019-03-27 02:17:33,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8535.0307 2840195712.4197 1041.0000
[2019-03-27 02:17:33,849] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8272.1074 2927211692.9231 1304.0000
[2019-03-27 02:17:33,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8083.2651 3003819253.4573 1550.0000
[2019-03-27 02:17:34,908] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1225000, evaluation results [1225000.0, 7941.153159596842, 3161867835.3460183, 1720.0, 8272.107395891624, 2927211692.9230895, 1304.0, 8675.707482036538, 2779461859.073676, 907.0, 8083.265114915379, 3003819253.4573035, 1550.0, 8535.03072215379, 2840195712.419652, 1041.0]
[2019-03-27 02:17:36,172] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1225563: loss 0.0981
[2019-03-27 02:17:36,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1225567: learning rate 0.0001
[2019-03-27 02:17:36,395] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1225664: loss 2.2355
[2019-03-27 02:17:36,397] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1225664: learning rate 0.0001
[2019-03-27 02:17:36,772] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1225838: loss 0.0765
[2019-03-27 02:17:36,774] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1225838: learning rate 0.0001
[2019-03-27 02:17:36,986] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1225929: loss 0.0472
[2019-03-27 02:17:36,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1225930: learning rate 0.0001
[2019-03-27 02:17:37,955] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1226363: loss 0.0919
[2019-03-27 02:17:37,959] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1226363: learning rate 0.0001
[2019-03-27 02:17:38,385] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1226558: loss 0.0879
[2019-03-27 02:17:38,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1226558: learning rate 0.0001
[2019-03-27 02:17:38,388] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1226558: loss 0.7453
[2019-03-27 02:17:38,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1226558: learning rate 0.0001
[2019-03-27 02:17:38,535] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1226624: loss 0.0850
[2019-03-27 02:17:38,538] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1226625: learning rate 0.0001
[2019-03-27 02:17:38,711] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1226702: loss 0.0515
[2019-03-27 02:17:38,718] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1226703: learning rate 0.0001
[2019-03-27 02:17:38,937] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1226800: loss 0.0458
[2019-03-27 02:17:38,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1226801: learning rate 0.0001
[2019-03-27 02:17:38,985] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1226817: loss 0.0573
[2019-03-27 02:17:38,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1226818: learning rate 0.0001
[2019-03-27 02:17:41,804] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1228083: loss 952.1367
[2019-03-27 02:17:41,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1228083: learning rate 0.0001
[2019-03-27 02:17:44,868] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1229454: loss 700.4916
[2019-03-27 02:17:44,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1229454: learning rate 0.0001
[2019-03-27 02:17:46,353] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1230112: loss 0.5615
[2019-03-27 02:17:46,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1230112: learning rate 0.0001
[2019-03-27 02:17:48,262] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1230926: loss 784.1513
[2019-03-27 02:17:48,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1230927: learning rate 0.0001
[2019-03-27 02:17:50,361] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:17:50,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0415
[2019-03-27 02:17:50,373] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 63.0, 1.0, 2.0, 0.5339955561636565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746192.5087479838, 746192.5087479833, 189093.9941111217], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6352800.0000, 
sim time next is 6353400.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.529586121827304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740028.7252333068, 740028.7252333062, 188361.4660533321], 
processed observation next is [0.0, 0.5217391304347826, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4332362913581976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20556353478702966, 0.2055635347870295, 0.2811365164975106], 
reward next is 0.7189, 
noisyNet noise sample is [array([1.0865887], dtype=float32), 0.77212095]. 
=============================================
[2019-03-27 02:17:50,689] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1232008: loss 729.3864
[2019-03-27 02:17:50,692] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1232008: learning rate 0.0001
[2019-03-27 02:17:53,943] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1233465: loss 1126.6504
[2019-03-27 02:17:53,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1233465: learning rate 0.0001
[2019-03-27 02:17:54,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2537315e-12 9.9993217e-01 4.3440455e-19 1.5480617e-10 6.7832450e-05], sum to 1.0000
[2019-03-27 02:17:54,330] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3367
[2019-03-27 02:17:54,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2037874.944926236 W.
[2019-03-27 02:17:54,349] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.4, 77.5, 1.0, 2.0, 0.4858285218898289, 1.0, 2.0, 0.4858285218898289, 1.0, 1.0, 0.8339493093001245, 6.911200000000001, 6.9112, 170.5573041426782, 2037874.944926236, 2037874.944926235, 403766.8828942544], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6425400.0000, 
sim time next is 6426000.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.7944043631939868, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98371098172421, 6.9112, 168.9125244512093, 2007242.136947958, 1955800.482459377, 407265.9804984309], 
processed observation next is [1.0, 0.391304347826087, 0.5497630331753555, 0.77, 1.0, 1.0, 0.7522944134867311, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007251098172420978, 0.0, 0.8294378235437265, 0.5575672602633217, 0.5432779117942714, 0.6078596723857178], 
reward next is 0.0296, 
noisyNet noise sample is [array([-0.5693509], dtype=float32), -1.2494726]. 
=============================================
[2019-03-27 02:17:54,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[43.133514]
 [43.26983 ]
 [43.07265 ]
 [42.507015]
 [47.089825]], R is [[42.4600296 ]
 [42.43279266]
 [42.42742538]
 [42.35525513]
 [41.93170166]].
[2019-03-27 02:17:54,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:17:54,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0613
[2019-03-27 02:17:54,478] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 61.5, 1.0, 2.0, 0.3668932981249972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559866.197849532, 559866.1978495327, 171331.4610088502], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6895800.0000, 
sim time next is 6896400.0000, 
raw observation next is [27.06666666666667, 62.33333333333333, 1.0, 2.0, 0.3680216301841056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561263.2853430847, 561263.2853430847, 171441.8689189892], 
processed observation next is [0.0, 0.8260869565217391, 0.48183254344391807, 0.6233333333333333, 1.0, 1.0, 0.23858027733024773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15590646815085688, 0.15590646815085688, 0.2558833864462525], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.05069677], dtype=float32), 0.75951725]. 
=============================================
[2019-03-27 02:17:54,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0496910e-31 1.0000000e+00 0.0000000e+00 1.4963021e-36 6.9884948e-34], sum to 1.0000
[2019-03-27 02:17:54,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8550
[2019-03-27 02:17:54,523] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 91.0, 1.0, 2.0, 0.4961977556352681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693357.5362922495, 693357.5362922495, 183002.5576879114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6655200.0000, 
sim time next is 6655800.0000, 
raw observation next is [25.58333333333333, 91.5, 1.0, 2.0, 0.4956290488645314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692562.5998764716, 692562.5998764716, 182914.139154077], 
processed observation next is [1.0, 0.0, 0.41153238546603454, 0.915, 1.0, 1.0, 0.39232415525847153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19237849996568654, 0.19237849996568654, 0.273006177841906], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.6685259], dtype=float32), 0.47872826]. 
=============================================
[2019-03-27 02:17:54,765] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1233830: loss 715.5464
[2019-03-27 02:17:54,767] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1233830: learning rate 0.0001
[2019-03-27 02:17:54,925] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1233900: loss 614.1067
[2019-03-27 02:17:54,926] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1233900: learning rate 0.0001
[2019-03-27 02:17:55,031] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1233945: loss 0.2165
[2019-03-27 02:17:55,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1233946: learning rate 0.0001
[2019-03-27 02:17:55,810] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4096803e-26 1.0000000e+00 3.1459130e-35 6.3451181e-32 2.6884201e-28], sum to 1.0000
[2019-03-27 02:17:55,818] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1234295: loss 221.0365
[2019-03-27 02:17:55,820] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1234295: learning rate 0.0001
[2019-03-27 02:17:55,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6734
[2019-03-27 02:17:55,827] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.78333333333333, 91.66666666666667, 1.0, 2.0, 0.6790655561150161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 949000.3908459352, 949000.3908459352, 216389.4681324883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6678600.0000, 
sim time next is 6679200.0000, 
raw observation next is [25.86666666666667, 91.33333333333334, 1.0, 2.0, 0.7108302742604002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 993412.6403899976, 993412.6403899969, 223198.3808237764], 
processed observation next is [1.0, 0.30434782608695654, 0.42496050552922615, 0.9133333333333334, 1.0, 1.0, 0.6516027400727713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27594795566388824, 0.275947955663888, 0.3331319116772782], 
reward next is 0.6669, 
noisyNet noise sample is [array([0.8690962], dtype=float32), 0.37715355]. 
=============================================
[2019-03-27 02:17:56,330] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1234524: loss 327.6668
[2019-03-27 02:17:56,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1234524: learning rate 0.0001
[2019-03-27 02:17:56,394] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1234554: loss 566.7545
[2019-03-27 02:17:56,398] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1234556: learning rate 0.0001
[2019-03-27 02:17:56,625] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1234659: loss 802.2097
[2019-03-27 02:17:56,628] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1234659: learning rate 0.0001
[2019-03-27 02:17:56,630] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1234659: loss 309.1770
[2019-03-27 02:17:56,634] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1234659: learning rate 0.0001
[2019-03-27 02:17:56,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4317659e-31 1.0000000e+00 0.0000000e+00 4.3979057e-38 1.4583606e-35], sum to 1.0000
[2019-03-27 02:17:56,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8320
[2019-03-27 02:17:56,751] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 78.33333333333334, 1.0, 2.0, 0.4979545386699867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695813.1654802216, 695813.1654802216, 183276.4264311119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6556800.0000, 
sim time next is 6557400.0000, 
raw observation next is [27.6, 79.0, 1.0, 2.0, 0.5000774113909455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698780.5219603968, 698780.5219603961, 183608.4527207948], 
processed observation next is [1.0, 0.9130434782608695, 0.5071090047393366, 0.79, 1.0, 1.0, 0.39768362818186204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19410570054455467, 0.19410570054455448, 0.2740424667474549], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.5071929], dtype=float32), 0.000980566]. 
=============================================
[2019-03-27 02:17:56,923] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1234785: loss 812.2961
[2019-03-27 02:17:56,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1234786: learning rate 0.0001
[2019-03-27 02:17:57,012] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1234824: loss 793.6400
[2019-03-27 02:17:57,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1234825: learning rate 0.0001
[2019-03-27 02:17:59,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0613429e-17 1.0000000e+00 9.8263098e-25 9.2818693e-20 4.4215963e-13], sum to 1.0000
[2019-03-27 02:17:59,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8197
[2019-03-27 02:17:59,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2165778.335991719 W.
[2019-03-27 02:17:59,705] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.4, 58.0, 1.0, 2.0, 0.5162898426827907, 1.0, 2.0, 0.5162898426827907, 1.0, 2.0, 0.8801001787067808, 6.911200000000001, 6.9112, 170.5573041426782, 2165778.335991719, 2165778.335991718, 423465.9878404379], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6534000.0000, 
sim time next is 6534600.0000, 
raw observation next is [31.26666666666667, 58.5, 1.0, 2.0, 0.7490888181572619, 1.0, 2.0, 0.7490888181572619, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2094826.748400018, 2094826.748400018, 395838.079617347], 
processed observation next is [1.0, 0.6521739130434783, 0.6808846761453398, 0.585, 1.0, 1.0, 0.6976973712738095, 1.0, 1.0, 0.6976973712738095, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.581896319000005, 0.581896319000005, 0.590803103906488], 
reward next is 0.4092, 
noisyNet noise sample is [array([-1.0973793], dtype=float32), 1.4000995]. 
=============================================
[2019-03-27 02:18:00,440] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1236359: loss 0.5465
[2019-03-27 02:18:00,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1236360: learning rate 0.0001
[2019-03-27 02:18:03,194] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1237583: loss 0.1665
[2019-03-27 02:18:03,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1237583: learning rate 0.0001
[2019-03-27 02:18:03,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9314112e-12 2.4409583e-02 4.9345259e-20 5.6392618e-10 9.7559035e-01], sum to 1.0000
[2019-03-27 02:18:03,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4257
[2019-03-27 02:18:03,345] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.76666666666667, 46.66666666666667, 1.0, 2.0, 0.4395640031829141, 1.0, 2.0, 0.4395640031829141, 1.0, 2.0, 0.7353312967968394, 6.9112, 6.9112, 170.5573041426782, 1857689.932711337, 1857689.932711337, 372535.8957869422], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7050000.0000, 
sim time next is 7050600.0000, 
raw observation next is [31.55, 48.0, 1.0, 2.0, 0.4503655846680591, 1.0, 2.0, 0.4503655846680591, 1.0, 2.0, 0.7511120780295011, 6.911200000000001, 6.9112, 170.5573041426782, 1895161.796963154, 1895161.796963153, 377884.2016656983], 
processed observation next is [1.0, 0.6086956521739131, 0.6943127962085308, 0.48, 1.0, 1.0, 0.3377898610458543, 1.0, 1.0, 0.3377898610458543, 1.0, 1.0, 0.696478143938416, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.526433832489765, 0.5264338324897647, 0.5640062711428333], 
reward next is 0.4360, 
noisyNet noise sample is [array([0.8589604], dtype=float32), 0.96467155]. 
=============================================
[2019-03-27 02:18:03,701] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1237800: loss 26.9872
[2019-03-27 02:18:03,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1237800: learning rate 0.0001
[2019-03-27 02:18:04,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9678952e-21 1.0000000e+00 1.0218226e-29 8.2618399e-25 2.3027773e-19], sum to 1.0000
[2019-03-27 02:18:04,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3257
[2019-03-27 02:18:04,636] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 86.5, 1.0, 2.0, 0.663437006082933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 927149.8141302812, 927149.8141302812, 213149.8031518069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6593400.0000, 
sim time next is 6594000.0000, 
raw observation next is [26.86666666666667, 86.0, 1.0, 2.0, 0.6336157989283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885457.457484027, 885457.4574840264, 207167.517779636], 
processed observation next is [1.0, 0.30434782608695654, 0.4723538704581361, 0.86, 1.0, 1.0, 0.5585732517209296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24596040485667417, 0.245960404856674, 0.30920525041736713], 
reward next is 0.6908, 
noisyNet noise sample is [array([-0.04762898], dtype=float32), -0.06529134]. 
=============================================
[2019-03-27 02:18:04,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[52.530212]
 [52.355183]
 [52.277798]
 [51.92902 ]
 [51.64105 ]], R is [[52.9613533 ]
 [53.1136055 ]
 [53.26662445]
 [53.41733932]
 [53.55820084]].
[2019-03-27 02:18:06,615] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1239097: loss 0.8583
[2019-03-27 02:18:06,619] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1239099: learning rate 0.0001
[2019-03-27 02:18:06,786] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.93288470e-27 1.00000000e+00 7.25715031e-37 3.60188844e-33
 1.24314664e-26], sum to 1.0000
[2019-03-27 02:18:06,795] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4201
[2019-03-27 02:18:06,800] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 83.33333333333334, 1.0, 2.0, 0.5266954191485647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749764.8927525079, 749764.8927525079, 189644.7769285798], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7108800.0000, 
sim time next is 7109400.0000, 
raw observation next is [25.83333333333334, 82.16666666666666, 1.0, 2.0, 0.5359684677292955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763047.9393231751, 763047.9393231751, 191225.3700392197], 
processed observation next is [1.0, 0.2608695652173913, 0.42338072669826254, 0.8216666666666665, 1.0, 1.0, 0.44092586473409096, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2119577609231042, 0.2119577609231042, 0.28541100005853687], 
reward next is 0.7146, 
noisyNet noise sample is [array([-1.2406414], dtype=float32), 0.769611]. 
=============================================
[2019-03-27 02:18:08,730] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1240043: loss 0.5864
[2019-03-27 02:18:08,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1240044: learning rate 0.0001
[2019-03-27 02:18:11,395] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0809304e-18 1.0000000e+00 2.2000287e-24 1.0296221e-19 1.1522869e-11], sum to 1.0000
[2019-03-27 02:18:11,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7227
[2019-03-27 02:18:11,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2242320.977778457 W.
[2019-03-27 02:18:11,417] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.56666666666667, 67.33333333333334, 1.0, 2.0, 0.801777698363693, 1.0, 1.0, 0.801777698363693, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2242320.977778457, 2242320.977778456, 420727.8771090712], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6696600.0000, 
sim time next is 6697200.0000, 
raw observation next is [29.63333333333333, 66.66666666666667, 1.0, 2.0, 0.6740514895667146, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.972310106854748, 6.9112, 168.9125911287252, 1838818.264638125, 1795464.745367573, 380267.5331497805], 
processed observation next is [1.0, 0.5217391304347826, 0.6034755134281199, 0.6666666666666667, 1.0, 1.0, 0.6072909512851983, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006111010685474838, 0.0, 0.8294381509608548, 0.5107828512883681, 0.4987402070465481, 0.5675634823131052], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38166052], dtype=float32), 0.0038272776]. 
=============================================
[2019-03-27 02:18:11,761] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1241398: loss 0.6187
[2019-03-27 02:18:11,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1241398: learning rate 0.0001
[2019-03-27 02:18:12,385] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1241678: loss 30.3809
[2019-03-27 02:18:12,389] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1241678: learning rate 0.0001
[2019-03-27 02:18:12,710] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1241822: loss 0.8534
[2019-03-27 02:18:12,715] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1241823: learning rate 0.0001
[2019-03-27 02:18:12,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:18:12,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8640
[2019-03-27 02:18:12,744] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 84.33333333333334, 1.0, 2.0, 0.3452182871015645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548351.6535439709, 548351.6535439703, 170826.0941097662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6751200.0000, 
sim time next is 6751800.0000, 
raw observation next is [21.95, 84.5, 1.0, 2.0, 0.3469345609765446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 551418.8007037522, 551418.8007037516, 171077.5467888869], 
processed observation next is [1.0, 0.13043478260869565, 0.2393364928909953, 0.845, 1.0, 1.0, 0.2131741698512586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15317188908437562, 0.15317188908437546, 0.2553396220729655], 
reward next is 0.7447, 
noisyNet noise sample is [array([0.6440499], dtype=float32), 0.9325948]. 
=============================================
[2019-03-27 02:18:12,998] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1241952: loss 0.6634
[2019-03-27 02:18:12,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1241953: learning rate 0.0001
[2019-03-27 02:18:13,461] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1923334e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:18:13,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1017
[2019-03-27 02:18:13,477] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 70.66666666666667, 1.0, 2.0, 0.3646489682755514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 558275.8642813705, 558275.86428137, 171249.0850692877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6733200.0000, 
sim time next is 6733800.0000, 
raw observation next is [25.35, 71.0, 1.0, 2.0, 0.3628508173431224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556491.2114429169, 556491.2114429175, 171124.9044373804], 
processed observation next is [1.0, 0.9565217391304348, 0.4004739336492892, 0.71, 1.0, 1.0, 0.23235038234111133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1545808920674769, 0.15458089206747708, 0.2554103051304185], 
reward next is 0.7446, 
noisyNet noise sample is [array([-0.2878891], dtype=float32), -1.3107413]. 
=============================================
[2019-03-27 02:18:13,833] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1242318: loss 0.4902
[2019-03-27 02:18:13,836] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1242318: learning rate 0.0001
[2019-03-27 02:18:14,460] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1242597: loss 0.2545
[2019-03-27 02:18:14,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1242597: learning rate 0.0001
[2019-03-27 02:18:14,617] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1242669: loss 0.1847
[2019-03-27 02:18:14,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1242670: learning rate 0.0001
[2019-03-27 02:18:14,759] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1242731: loss 0.2952
[2019-03-27 02:18:14,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1242732: learning rate 0.0001
[2019-03-27 02:18:14,792] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1242746: loss 0.1969
[2019-03-27 02:18:14,796] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1242747: learning rate 0.0001
[2019-03-27 02:18:15,032] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1242856: loss 0.2329
[2019-03-27 02:18:15,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1242858: learning rate 0.0001
[2019-03-27 02:18:15,076] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1242876: loss 0.1559
[2019-03-27 02:18:15,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1242876: learning rate 0.0001
[2019-03-27 02:18:18,168] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1244250: loss -66.8720
[2019-03-27 02:18:18,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1244250: learning rate 0.0001
[2019-03-27 02:18:20,952] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1245498: loss -21.4100
[2019-03-27 02:18:20,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1245498: learning rate 0.0001
[2019-03-27 02:18:21,657] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1245815: loss 0.1104
[2019-03-27 02:18:21,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1245816: learning rate 0.0001
[2019-03-27 02:18:24,528] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1247101: loss 23.3439
[2019-03-27 02:18:24,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1247101: learning rate 0.0001
[2019-03-27 02:18:26,573] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1248016: loss 9.1724
[2019-03-27 02:18:26,574] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1248016: learning rate 0.0001
[2019-03-27 02:18:29,129] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:18:29,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9230
[2019-03-27 02:18:29,144] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 78.33333333333333, 1.0, 2.0, 0.4134173336485709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604483.2407646085, 604483.2407646085, 174592.2971846314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7483800.0000, 
sim time next is 7484400.0000, 
raw observation next is [25.8, 78.0, 1.0, 2.0, 0.4147308461261768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606051.9174150872, 606051.9174150872, 174729.5287385545], 
processed observation next is [0.0, 0.6521739130434783, 0.42180094786729866, 0.78, 1.0, 1.0, 0.2948564411158756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16834775483752423, 0.16834775483752423, 0.26079034140082763], 
reward next is 0.7392, 
noisyNet noise sample is [array([0.758378], dtype=float32), 0.54572034]. 
=============================================
[2019-03-27 02:18:29,750] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1249439: loss -7.4008
[2019-03-27 02:18:29,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1249439: learning rate 0.0001
[2019-03-27 02:18:30,448] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1249750: loss 0.0121
[2019-03-27 02:18:30,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1249750: learning rate 0.0001
[2019-03-27 02:18:30,559] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1249802: loss 35.2907
[2019-03-27 02:18:30,561] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1249803: learning rate 0.0001
[2019-03-27 02:18:30,968] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1249986: loss 123.6648
[2019-03-27 02:18:30,972] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1249986: learning rate 0.0001
[2019-03-27 02:18:31,007] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 02:18:31,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:18:31,009] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:18:31,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:18:31,010] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:18:31,013] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:18:31,014] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:18:31,014] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:18:31,015] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:18:31,016] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:18:31,017] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:18:31,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-27 02:18:31,068] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-27 02:18:31,069] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-27 02:18:31,086] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-27 02:18:31,107] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-27 02:18:47,022] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:18:47,024] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.02440970666667, 94.56873660333335, 1.0, 2.0, 0.2625417581393752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 432476.1816477088, 432476.1816477095, 162156.5156376591]
[2019-03-27 02:18:47,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:18:47,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4876237600123776
[2019-03-27 02:18:58,375] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:18:58,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.68684549, 90.93316066666668, 1.0, 2.0, 0.5960068448504305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 910898.2445226571, 910898.2445226571, 209542.125981264]
[2019-03-27 02:18:58,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:18:58,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.217249e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7820416812871964
[2019-03-27 02:19:29,734] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:19:29,734] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.18333333333333, 64.66666666666667, 1.0, 2.0, 0.5730266532908891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800754.1807497763, 800754.1807497763, 195828.4982654164]
[2019-03-27 02:19:29,735] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:19:29,738] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.778652e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6753453202472822
[2019-03-27 02:19:36,736] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:19:36,737] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.615853115, 79.27954700166667, 1.0, 2.0, 0.4990878232244309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701509.8323892835, 701509.8323892835, 183981.3239383912]
[2019-03-27 02:19:36,738] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:19:36,742] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.35700425e-36 1.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00], sampled 0.6402920013861783
[2019-03-27 02:19:42,413] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:19:42,414] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.9836065, 65.0838233, 1.0, 2.0, 0.9179391509212663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1283030.128881213, 1283030.128881212, 274924.9950862636]
[2019-03-27 02:19:42,417] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:19:42,420] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.153411e-32 1.000000e+00 0.000000e+00 0.000000e+00 2.804240e-36], sampled 0.7275450050389123
[2019-03-27 02:20:04,795] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:20:04,796] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.06666666666666, 81.33333333333334, 1.0, 2.0, 0.8058705464351424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1126305.513811746, 1126305.513811746, 245356.7790915899]
[2019-03-27 02:20:04,797] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:20:04,798] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.0978673e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6463323058187433
[2019-03-27 02:20:15,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:20:15,441] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.1, 68.0, 1.0, 2.0, 0.5894892426514151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823768.1175480718, 823768.1175480718, 198804.4405053689]
[2019-03-27 02:20:15,442] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:20:15,445] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.383063e-34 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.44365747385659093
[2019-03-27 02:20:24,616] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1702 3164110757.8259 1777.0000
[2019-03-27 02:20:24,827] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.090226404]
[2019-03-27 02:20:24,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.198389685, 67.52862945, 1.0, 2.0, 0.9628839571739558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1345890.605766087, 1345890.605766088, 287825.3611494277]
[2019-03-27 02:20:24,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:20:24,832] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1706174e-31 1.0000000e+00 0.0000000e+00 3.1902482e-38 2.7177048e-35], sampled 0.062060740895092614
[2019-03-27 02:20:25,328] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 02:20:25,336] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 02:20:25,459] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6857 2927400807.7850 1338.0000
[2019-03-27 02:20:25,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.1326 2842568365.7369 1131.0000
[2019-03-27 02:20:26,506] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1250000, evaluation results [1250000.0, 7884.170154442996, 3164110757.8258896, 1777.0, 8253.68573338827, 2927400807.784972, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8496.13258090819, 2842568365.7369456, 1131.0]
[2019-03-27 02:20:27,248] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1250338: loss -5.5019
[2019-03-27 02:20:27,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1250339: learning rate 0.0001
[2019-03-27 02:20:27,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1250583: loss 37.6086
[2019-03-27 02:20:27,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1250583: learning rate 0.0001
[2019-03-27 02:20:27,851] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1250603: loss -11.7462
[2019-03-27 02:20:27,852] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1250603: learning rate 0.0001
[2019-03-27 02:20:27,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1250618: loss -94.7244
[2019-03-27 02:20:27,894] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1250619: learning rate 0.0001
[2019-03-27 02:20:28,099] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1250713: loss 97.8362
[2019-03-27 02:20:28,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1250713: learning rate 0.0001
[2019-03-27 02:20:28,266] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1250788: loss 79.6710
[2019-03-27 02:20:28,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1250788: learning rate 0.0001
[2019-03-27 02:20:28,315] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1250809: loss 47.9006
[2019-03-27 02:20:28,319] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1250810: learning rate 0.0001
[2019-03-27 02:20:31,438] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1252205: loss 0.0416
[2019-03-27 02:20:31,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1252206: learning rate 0.0001
[2019-03-27 02:20:34,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2678109e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:20:34,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9650
[2019-03-27 02:20:34,216] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 72.0, 1.0, 2.0, 0.7655216118082576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1181190.933044879, 1181190.933044879, 249976.0426289992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7293600.0000, 
sim time next is 7294200.0000, 
raw observation next is [25.23333333333333, 71.16666666666667, 1.0, 2.0, 0.8594900294942697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1323079.873162639, 1323079.873162639, 275564.7865311849], 
processed observation next is [1.0, 0.43478260869565216, 0.39494470774091617, 0.7116666666666667, 1.0, 1.0, 0.8307108789087586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36752218698962197, 0.36752218698962197, 0.41129072616594764], 
reward next is 0.5887, 
noisyNet noise sample is [array([1.4782156], dtype=float32), 1.1493655]. 
=============================================
[2019-03-27 02:20:34,243] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1253461: loss 0.0782
[2019-03-27 02:20:34,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1253462: learning rate 0.0001
[2019-03-27 02:20:34,580] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1253616: loss -5.6389
[2019-03-27 02:20:34,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1253616: learning rate 0.0001
[2019-03-27 02:20:35,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4701566e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:20:35,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7019
[2019-03-27 02:20:35,111] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 86.66666666666667, 1.0, 2.0, 0.4781715024161082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668160.789432546, 668160.7894325453, 180246.6167092501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7177200.0000, 
sim time next is 7177800.0000, 
raw observation next is [25.8, 86.83333333333333, 1.0, 2.0, 0.4788232892238842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669071.8339672287, 669071.833967228, 180344.6030678092], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.8683333333333333, 1.0, 1.0, 0.37207625207696887, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18585328721311908, 0.1858532872131189, 0.2691710493549391], 
reward next is 0.7308, 
noisyNet noise sample is [array([0.9825454], dtype=float32), 0.009307456]. 
=============================================
[2019-03-27 02:20:37,805] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1255054: loss 0.2310
[2019-03-27 02:20:37,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1255054: learning rate 0.0001
[2019-03-27 02:20:39,744] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1255927: loss 0.2858
[2019-03-27 02:20:39,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1255929: learning rate 0.0001
[2019-03-27 02:20:41,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.170122e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:20:41,148] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5351
[2019-03-27 02:20:41,153] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.7, 92.5, 1.0, 2.0, 0.5993035104420347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 958112.2415123684, 958112.2415123684, 214230.8905918325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7378200.0000, 
sim time next is 7378800.0000, 
raw observation next is [20.76666666666667, 92.66666666666667, 1.0, 2.0, 0.5826960461003439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 929857.5751164259, 929857.5751164252, 210624.8334735184], 
processed observation next is [1.0, 0.391304347826087, 0.18325434439178534, 0.9266666666666667, 1.0, 1.0, 0.49722415192812514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25829377086567384, 0.2582937708656737, 0.3143654230948036], 
reward next is 0.6856, 
noisyNet noise sample is [array([1.0102384], dtype=float32), -1.2932628]. 
=============================================
[2019-03-27 02:20:42,993] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1257385: loss 0.1517
[2019-03-27 02:20:42,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1257385: learning rate 0.0001
[2019-03-27 02:20:43,360] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1257547: loss -65.7558
[2019-03-27 02:20:43,364] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1257548: learning rate 0.0001
[2019-03-27 02:20:43,875] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1257777: loss 0.1981
[2019-03-27 02:20:43,876] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1257777: learning rate 0.0001
[2019-03-27 02:20:44,438] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1258029: loss 0.0823
[2019-03-27 02:20:44,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1258029: learning rate 0.0001
[2019-03-27 02:20:45,251] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1258397: loss 0.1341
[2019-03-27 02:20:45,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1258397: learning rate 0.0001
[2019-03-27 02:20:45,797] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1258640: loss 0.1166
[2019-03-27 02:20:45,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1258643: loss 0.1033
[2019-03-27 02:20:45,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1258643: learning rate 0.0001
[2019-03-27 02:20:45,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1258643: learning rate 0.0001
[2019-03-27 02:20:45,866] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1258667: loss 0.1194
[2019-03-27 02:20:45,868] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1258668: learning rate 0.0001
[2019-03-27 02:20:46,032] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1258743: loss 0.1531
[2019-03-27 02:20:46,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1258743: learning rate 0.0001
[2019-03-27 02:20:46,209] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1258823: loss 0.0646
[2019-03-27 02:20:46,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1258824: learning rate 0.0001
[2019-03-27 02:20:46,419] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1258913: loss 0.0551
[2019-03-27 02:20:46,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1258914: learning rate 0.0001
[2019-03-27 02:20:49,097] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1260100: loss -163.2935
[2019-03-27 02:20:49,101] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1260102: learning rate 0.0001
[2019-03-27 02:20:49,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.858543e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:20:49,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4955
[2019-03-27 02:20:49,386] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 95.0, 1.0, 2.0, 0.4901608647690107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708669.5828691515, 708669.5828691515, 185082.6332041952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7614000.0000, 
sim time next is 7614600.0000, 
raw observation next is [23.65, 95.0, 1.0, 2.0, 0.523412068540784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758117.3739358892, 758117.3739358892, 190729.2154654737], 
processed observation next is [1.0, 0.13043478260869565, 0.31990521327014215, 0.95, 1.0, 1.0, 0.42579767294070364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2105881594266359, 0.2105881594266359, 0.2846704708439906], 
reward next is 0.7153, 
noisyNet noise sample is [array([0.40459695], dtype=float32), 1.1436752]. 
=============================================
[2019-03-27 02:20:51,949] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1261382: loss -3.0460
[2019-03-27 02:20:51,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1261382: learning rate 0.0001
[2019-03-27 02:20:52,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:20:52,121] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3187
[2019-03-27 02:20:52,129] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 79.0, 1.0, 2.0, 0.4158893333616966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606903.4914427524, 606903.4914427524, 174784.5962774617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7496400.0000, 
sim time next is 7497000.0000, 
raw observation next is [25.55, 79.5, 1.0, 2.0, 0.4140619188921128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605155.6039935349, 605155.6039935349, 174647.3863554103], 
processed observation next is [0.0, 0.782608695652174, 0.40995260663507116, 0.795, 1.0, 1.0, 0.29405050468929256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16809877888709304, 0.16809877888709304, 0.2606677408289706], 
reward next is 0.7393, 
noisyNet noise sample is [array([-0.9734276], dtype=float32), -1.1045183]. 
=============================================
[2019-03-27 02:20:52,144] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.23097]
 [75.17412]
 [75.12804]
 [75.00061]
 [74.95844]], R is [[75.26302338]
 [75.24951935]
 [75.23593903]
 [75.2223587 ]
 [75.20892334]].
[2019-03-27 02:20:53,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:20:53,399] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7530
[2019-03-27 02:20:53,402] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 78.33333333333333, 1.0, 2.0, 0.4134173336485709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604483.2407646085, 604483.2407646085, 174592.2971846314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7483800.0000, 
sim time next is 7484400.0000, 
raw observation next is [25.8, 78.0, 1.0, 2.0, 0.4147308461261768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606051.9174150872, 606051.9174150872, 174729.5287385545], 
processed observation next is [0.0, 0.6521739130434783, 0.42180094786729866, 0.78, 1.0, 1.0, 0.2948564411158756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16834775483752423, 0.16834775483752423, 0.26079034140082763], 
reward next is 0.7392, 
noisyNet noise sample is [array([1.4900566], dtype=float32), 0.7624422]. 
=============================================
[2019-03-27 02:20:53,921] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:20:53,923] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:20:53,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-27 02:20:55,214] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1262974: loss 39.3006
[2019-03-27 02:20:55,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1262974: learning rate 0.0001
[2019-03-27 02:20:57,042] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1263786: loss -28.1609
[2019-03-27 02:20:57,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1263786: learning rate 0.0001
[2019-03-27 02:20:59,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3836145e-15 1.0000000e+00 5.0273004e-23 1.0970462e-14 1.7459309e-10], sum to 1.0000
[2019-03-27 02:20:59,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7810
[2019-03-27 02:20:59,781] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.23333333333333, 61.33333333333334, 1.0, 2.0, 0.3417541370303987, 1.0, 2.0, 0.3417541370303987, 1.0, 1.0, 0.5771501289585819, 6.9112, 6.9112, 170.5573041426782, 1433130.883961654, 1433130.883961654, 321054.4271526613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7647600.0000, 
sim time next is 7648200.0000, 
raw observation next is [30.3, 61.0, 1.0, 2.0, 0.9738046232458037, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1361164.974416144, 1361164.974416144, 291042.8386105252], 
processed observation next is [1.0, 0.5217391304347826, 0.6350710900473934, 0.61, 1.0, 1.0, 0.9684393051154262, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3781013817822622, 0.3781013817822622, 0.43439229643361965], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.50691557], dtype=float32), -0.7986546]. 
=============================================
[2019-03-27 02:21:00,299] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1265250: loss 74.0622
[2019-03-27 02:21:00,304] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1265250: learning rate 0.0001
[2019-03-27 02:21:01,093] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1265605: loss -5.2610
[2019-03-27 02:21:01,098] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1265605: learning rate 0.0001
[2019-03-27 02:21:01,721] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1265885: loss -113.4611
[2019-03-27 02:21:01,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1265885: learning rate 0.0001
[2019-03-27 02:21:02,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:02,128] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:02,206] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-27 02:21:02,376] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1266172: loss 23.8716
[2019-03-27 02:21:02,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1266172: learning rate 0.0001
[2019-03-27 02:21:02,796] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1266410: loss 45.5133
[2019-03-27 02:21:02,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1266412: learning rate 0.0001
[2019-03-27 02:21:02,808] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1266416: loss 0.3757
[2019-03-27 02:21:02,812] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1266418: learning rate 0.0001
[2019-03-27 02:21:02,825] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1266425: loss 25.8508
[2019-03-27 02:21:02,827] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1266425: learning rate 0.0001
[2019-03-27 02:21:02,927] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1266481: loss 16.4758
[2019-03-27 02:21:02,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1266483: learning rate 0.0001
[2019-03-27 02:21:03,034] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1266520: loss 38.1186
[2019-03-27 02:21:03,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1266520: learning rate 0.0001
[2019-03-27 02:21:03,305] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1266681: loss 67.4186
[2019-03-27 02:21:03,308] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1266682: learning rate 0.0001
[2019-03-27 02:21:05,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4715072e-31 1.0000000e+00 0.0000000e+00 5.4243944e-38 6.5854004e-34], sum to 1.0000
[2019-03-27 02:21:05,762] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2724
[2019-03-27 02:21:05,768] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 83.66666666666667, 1.0, 2.0, 0.5265198497206027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735742.5191000969, 735742.5191000969, 187855.7836884258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7936800.0000, 
sim time next is 7937400.0000, 
raw observation next is [27.5, 84.33333333333334, 1.0, 2.0, 0.5279972855622547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737807.7591457578, 737807.7591457578, 188099.1065370432], 
processed observation next is [1.0, 0.8695652173913043, 0.5023696682464456, 0.8433333333333334, 1.0, 1.0, 0.43132203079789716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20494659976271049, 0.20494659976271049, 0.2807449351299152], 
reward next is 0.7193, 
noisyNet noise sample is [array([-1.0652101], dtype=float32), -1.3036948]. 
=============================================
[2019-03-27 02:21:07,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:07,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:07,558] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-27 02:21:09,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:09,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:09,796] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-27 02:21:10,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4015775e-30 1.0000000e+00 0.0000000e+00 3.5975573e-36 3.2578747e-32], sum to 1.0000
[2019-03-27 02:21:10,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6164
[2019-03-27 02:21:10,713] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 84.0, 1.0, 2.0, 0.6687241109450456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 934541.766919579, 934541.766919579, 214237.9198633613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7802400.0000, 
sim time next is 7803000.0000, 
raw observation next is [27.3, 83.5, 1.0, 2.0, 0.7140558042943365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 997922.5606028958, 997922.5606028958, 223907.2392876085], 
processed observation next is [1.0, 0.30434782608695654, 0.4928909952606636, 0.835, 1.0, 1.0, 0.65548892083655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27720071127858215, 0.27720071127858215, 0.33418990938449034], 
reward next is 0.6658, 
noisyNet noise sample is [array([0.45745414], dtype=float32), 0.5276604]. 
=============================================
[2019-03-27 02:21:10,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[59.641457]
 [59.30712 ]
 [60.432358]
 [60.11932 ]
 [60.25096 ]], R is [[59.42004776]
 [59.50608826]
 [59.56249619]
 [59.66262817]
 [59.76462936]].
[2019-03-27 02:21:12,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:12,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:12,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-27 02:21:14,415] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:14,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:14,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-27 02:21:16,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:16,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:16,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-27 02:21:17,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:17,382] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:17,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-27 02:21:17,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:17,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-27 02:21:18,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-27 02:21:18,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5971033e-14 9.9995232e-01 9.4401273e-24 2.3078427e-15 4.7646463e-05], sum to 1.0000
[2019-03-27 02:21:18,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5560
[2019-03-27 02:21:18,763] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.66666666666667, 1.0, 2.0, 0.9088440893745711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1355578.060794108, 1355578.060794109, 284677.1901405191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 128400.0000, 
sim time next is 129000.0000, 
raw observation next is [22.8, 95.83333333333333, 1.0, 2.0, 0.8993225992315843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1340447.958229606, 1340447.958229607, 281729.6384257205], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9583333333333333, 1.0, 1.0, 0.8787019267850413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37234665506377945, 0.37234665506377973, 0.42049199765032913], 
reward next is 0.5795, 
noisyNet noise sample is [array([-1.323051], dtype=float32), -0.4385004]. 
=============================================
[2019-03-27 02:21:18,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.17772 ]
 [70.910385]
 [70.82041 ]
 [70.7345  ]
 [71.031685]], R is [[71.18823242]
 [71.05146027]
 [70.89469147]
 [70.745224  ]
 [70.61087036]].
[2019-03-27 02:21:18,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-27 02:21:18,879] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,880] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,924] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:18,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-27 02:21:18,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-27 02:21:18,990] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:18,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-27 02:21:19,053] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-27 02:21:19,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.344015e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:21:19,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6089
[2019-03-27 02:21:19,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:21:19,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,108] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666666, 94.16666666666667, 1.0, 2.0, 0.2890798779014609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465186.8719295451, 465186.8719295457, 164627.5537880551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 197400.0000, 
sim time next is 198000.0000, 
raw observation next is [20.2, 94.0, 1.0, 2.0, 0.2888478642626525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 464688.8910107907, 464688.8910107907, 164593.0614477661], 
processed observation next is [0.0, 0.30434782608695654, 0.15639810426540288, 0.94, 1.0, 1.0, 0.14319019790681026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12908024750299743, 0.12908024750299743, 0.24566128574293444], 
reward next is 0.7543, 
noisyNet noise sample is [array([-0.93477273], dtype=float32), 0.2689871]. 
=============================================
[2019-03-27 02:21:19,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.97698 ]
 [77.96676 ]
 [77.96194 ]
 [77.96513 ]
 [77.985794]], R is [[78.03382874]
 [78.00778198]
 [77.98199463]
 [77.95656586]
 [77.93157959]].
[2019-03-27 02:21:19,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-27 02:21:19,992] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 02:21:19,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:21:19,994] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:21:19,995] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,995] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,995] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:21:19,997] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:21:19,997] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,997] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:19,997] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:21:19,999] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:21:20,009] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-27 02:21:20,032] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-27 02:21:20,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-27 02:21:20,034] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-27 02:21:20,048] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-27 02:21:40,616] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:21:40,617] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.48333333333333, 52.5, 1.0, 2.0, 0.2760691597160399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 450659.1544961809, 450659.1544961816, 163554.3657933394]
[2019-03-27 02:21:40,618] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:21:40,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8399496e-30 1.0000000e+00 0.0000000e+00 2.1171136e-37 4.8199402e-30], sampled 0.22522756615728656
[2019-03-27 02:21:54,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:21:54,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 71.0, 1.0, 2.0, 0.7822345040312195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1137638.169215022, 1137638.169215021, 245698.3375146263]
[2019-03-27 02:21:54,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:21:54,429] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4705595e-23 1.0000000e+00 2.2566615e-34 2.1362121e-28 8.5636548e-20], sampled 0.16194853631016382
[2019-03-27 02:22:32,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:22:32,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.78333333333333, 78.16666666666667, 1.0, 2.0, 0.8548028925953215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1194733.101436905, 1194733.101436905, 257810.5537583136]
[2019-03-27 02:22:32,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:22:32,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.6025431e-22 1.0000000e+00 8.8604459e-33 3.2919523e-26 1.3105035e-17], sampled 0.5136689837796148
[2019-03-27 02:22:32,909] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:22:32,910] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.218214745, 75.07206311, 1.0, 2.0, 0.5266980277603946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735991.5858264344, 735991.5858264351, 187883.2942604733]
[2019-03-27 02:22:32,912] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:22:32,915] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4461581e-25 1.0000000e+00 5.2241788e-37 1.9843640e-31 7.9894116e-23], sampled 0.12356689648169694
[2019-03-27 02:22:43,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:22:43,487] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.0, 83.0, 1.0, 2.0, 0.6549006453309991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 915215.1536344603, 915215.1536344603, 211420.208859723]
[2019-03-27 02:22:43,488] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:22:43,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4788241e-29 1.0000000e+00 0.0000000e+00 5.2774187e-37 1.3656452e-29], sampled 0.754610473125141
[2019-03-27 02:22:54,562] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10195517]
[2019-03-27 02:22:54,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.03333333333333, 88.5, 1.0, 2.0, 0.6525244820623772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 911893.0707203548, 911893.0707203554, 210931.542790274]
[2019-03-27 02:22:54,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:22:54,566] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.11195395e-27 1.00000000e+00 0.00000000e+00 1.42006973e-34
 7.48766973e-27], sampled 0.907178990849582
[2019-03-27 02:23:13,825] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8339.7559 2925381253.9630 1154.0000
[2019-03-27 02:23:13,967] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8052.0270 3156004625.1307 1448.0000
[2019-03-27 02:23:14,114] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8704.9907 2778602238.4693 818.0000
[2019-03-27 02:23:14,425] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8233.1570 2997267027.4393 1166.0000
[2019-03-27 02:23:14,497] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8597.2911 2837574858.3751 892.0000
[2019-03-27 02:23:15,512] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1275000, evaluation results [1275000.0, 8052.027042093053, 3156004625.1306553, 1448.0, 8339.755870987501, 2925381253.9629803, 1154.0, 8704.99070893475, 2778602238.4693217, 818.0, 8233.156996457872, 2997267027.439302, 1166.0, 8597.291074734458, 2837574858.3751063, 892.0]
[2019-03-27 02:23:21,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.336032e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:23:21,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1135
[2019-03-27 02:23:21,758] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 95.5, 1.0, 2.0, 0.2650526154486161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431053.1406491161, 431053.1406491161, 162334.7419220064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 275400.0000, 
sim time next is 276000.0000, 
raw observation next is [19.33333333333333, 95.66666666666667, 1.0, 2.0, 0.26342732567264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428744.169440457, 428744.1694404576, 162181.6441650866], 
processed observation next is [0.0, 0.17391304347826086, 0.11532385466034739, 0.9566666666666667, 1.0, 1.0, 0.11256304297908432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11909560262234917, 0.11909560262234933, 0.24206215547027848], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.80144453], dtype=float32), 0.71066874]. 
=============================================
[2019-03-27 02:23:21,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.78451 ]
 [72.85671 ]
 [72.856514]
 [73.01583 ]
 [72.86641 ]], R is [[72.80447388]
 [72.83413696]
 [72.86325073]
 [72.89180756]
 [72.91983795]].
[2019-03-27 02:23:22,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.947541e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:23:22,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0690
[2019-03-27 02:23:22,171] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 95.5, 1.0, 2.0, 0.2835464524042191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457260.8337850788, 457260.8337850795, 164087.344172261], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 192600.0000, 
sim time next is 193200.0000, 
raw observation next is [19.93333333333333, 95.33333333333333, 1.0, 2.0, 0.2837356703576741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457444.3041101692, 457444.3041101685, 164099.8781361738], 
processed observation next is [0.0, 0.21739130434782608, 0.14375987361769343, 0.9533333333333333, 1.0, 1.0, 0.13703092814177603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12706786225282476, 0.12706786225282457, 0.2449251912480206], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.62958544], dtype=float32), -0.022008715]. 
=============================================
[2019-03-27 02:23:23,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2876393e-16 1.0000000e+00 2.3966826e-28 1.8924073e-19 5.8807952e-08], sum to 1.0000
[2019-03-27 02:23:23,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9475
[2019-03-27 02:23:23,475] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 96.0, 1.0, 2.0, 0.5326670631901701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793309.4376163457, 793309.4376163457, 194955.4486916788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 131400.0000, 
sim time next is 132000.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.5460743147560411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 813333.7370825892, 813333.7370825885, 197379.0953872523], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.45310158404342293, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22592603807849698, 0.2259260380784968, 0.294595664757093], 
reward next is 0.7054, 
noisyNet noise sample is [array([0.23710641], dtype=float32), 0.95521504]. 
=============================================
[2019-03-27 02:23:23,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.544464]
 [77.4992  ]
 [76.98275 ]
 [76.71587 ]
 [76.60288 ]], R is [[77.31633759]
 [77.25219727]
 [77.1727829 ]
 [77.01995087]
 [76.82472992]].
[2019-03-27 02:23:25,001] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1478599e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6152124e-37], sum to 1.0000
[2019-03-27 02:23:25,013] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5280
[2019-03-27 02:23:25,021] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.21666666666667, 90.33333333333333, 1.0, 2.0, 0.3056395780180961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485981.2259200464, 485981.225920047, 166041.3180243992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 215400.0000, 
sim time next is 216000.0000, 
raw observation next is [21.3, 90.0, 1.0, 2.0, 0.3080347734248821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489334.1027988617, 489334.102798861, 166277.8622031632], 
processed observation next is [0.0, 0.5217391304347826, 0.2085308056872039, 0.9, 1.0, 1.0, 0.1663069559335929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13592613966635048, 0.13592613966635028, 0.24817591373606449], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.0121511], dtype=float32), 0.26193666]. 
=============================================
[2019-03-27 02:23:25,035] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.76076 ]
 [75.77791 ]
 [75.794975]
 [75.81684 ]
 [75.84299 ]], R is [[75.80329132]
 [75.79743195]
 [75.79180908]
 [75.7861557 ]
 [75.78062439]].
[2019-03-27 02:23:25,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0797610e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0683673e-30], sum to 1.0000
[2019-03-27 02:23:25,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7224
[2019-03-27 02:23:25,269] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 96.0, 1.0, 2.0, 0.357945376625525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551781.4168478603, 551781.4168478603, 170805.9791371891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.3543341171355828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 547783.4350458741, 547783.4350458735, 170514.375543383], 
processed observation next is [1.0, 0.9130434782608695, 0.22274881516587688, 0.96, 1.0, 1.0, 0.22208929775371425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1521620652905206, 0.15216206529052043, 0.2544990679751985], 
reward next is 0.7455, 
noisyNet noise sample is [array([-1.0470475], dtype=float32), -0.33667406]. 
=============================================
[2019-03-27 02:23:33,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5928990e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5024056e-35], sum to 1.0000
[2019-03-27 02:23:33,064] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3406
[2019-03-27 02:23:33,071] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 80.5, 1.0, 2.0, 0.3010275442479547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478008.425114664, 478008.4251146634, 165455.8211644339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 293400.0000, 
sim time next is 294000.0000, 
raw observation next is [22.63333333333333, 80.0, 1.0, 2.0, 0.3022909784836246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 479864.490612335, 479864.4906123343, 165585.9281784119], 
processed observation next is [0.0, 0.391304347826087, 0.27172195892575024, 0.8, 1.0, 1.0, 0.1593867210646079, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1332956918367597, 0.13329569183675952, 0.24714317638568942], 
reward next is 0.7529, 
noisyNet noise sample is [array([-1.0460141], dtype=float32), 0.6029702]. 
=============================================
[2019-03-27 02:23:33,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.65808]
 [76.66093]
 [76.66757]
 [76.67219]
 [76.59975]], R is [[76.64051056]
 [76.62715912]
 [76.61405945]
 [76.60109711]
 [76.58834839]].
[2019-03-27 02:23:44,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7632314e-19 1.0000000e+00 8.1580888e-31 4.5617904e-23 4.4383408e-14], sum to 1.0000
[2019-03-27 02:23:44,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7357
[2019-03-27 02:23:44,659] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.98333333333333, 58.5, 1.0, 2.0, 0.6424025593383613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053425.796239427, 1053425.796239427, 225013.9144379167], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [23.9, 59.0, 1.0, 2.0, 0.6052822334205809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 992495.5967563349, 992495.5967563356, 216736.5010347012], 
processed observation next is [1.0, 0.6086956521739131, 0.33175355450236965, 0.59, 1.0, 1.0, 0.5244364258079287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2756932213212041, 0.27569322132120433, 0.323487314977166], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.61537254], dtype=float32), -3.009185]. 
=============================================
[2019-03-27 02:23:47,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7803278e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5437891e-30], sum to 1.0000
[2019-03-27 02:23:47,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3209
[2019-03-27 02:23:47,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.11666666666667, 91.83333333333333, 1.0, 2.0, 0.206211874541012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 344595.6360731431, 344595.6360731431, 155942.4043292709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 615000.0000, 
sim time next is 615600.0000, 
raw observation next is [17.1, 92.0, 1.0, 2.0, 0.2060497328044147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 344322.7328201954, 344322.7328201961, 155930.3209037432], 
processed observation next is [1.0, 0.13043478260869565, 0.009478672985782125, 0.92, 1.0, 1.0, 0.04343341301736708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09564520356116538, 0.09564520356116557, 0.23273182224439284], 
reward next is 0.7673, 
noisyNet noise sample is [array([0.7041319], dtype=float32), -1.9499077]. 
=============================================
[2019-03-27 02:23:48,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1283054e-21 1.0000000e+00 4.2794702e-33 7.0084860e-28 4.4450732e-19], sum to 1.0000
[2019-03-27 02:23:48,727] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7779
[2019-03-27 02:23:48,733] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.91666666666666, 56.5, 1.0, 2.0, 0.6107905061670916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1006836.955491657, 1006836.955491657, 217927.6554883869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 558600.0000, 
sim time next is 559200.0000, 
raw observation next is [24.13333333333333, 56.00000000000001, 1.0, 2.0, 0.6120043508527482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1007447.090773712, 1007447.090773713, 218210.8620675619], 
processed observation next is [1.0, 0.4782608695652174, 0.3428120063191152, 0.56, 1.0, 1.0, 0.5325353624731906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2798464141038089, 0.27984641410380917, 0.3256878538321819], 
reward next is 0.6743, 
noisyNet noise sample is [array([1.1632662], dtype=float32), 1.765148]. 
=============================================
[2019-03-27 02:23:49,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.164761e-19 1.000000e+00 5.126043e-31 9.394599e-23 3.599949e-12], sum to 1.0000
[2019-03-27 02:23:49,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3581
[2019-03-27 02:23:49,236] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098600.0000, 
sim time next is 1099200.0000, 
raw observation next is [25.53333333333333, 68.0, 1.0, 2.0, 0.3327323270616147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514524.5463198539, 514524.5463198539, 167832.2459973225], 
processed observation next is [1.0, 0.7391304347826086, 0.4091627172195892, 0.68, 1.0, 1.0, 0.19606304465254784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1429234850888483, 0.1429234850888483, 0.25049588954824253], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.9431757], dtype=float32), -0.37966588]. 
=============================================
[2019-03-27 02:23:50,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0655188e-27 1.0000000e+00 1.5356992e-37 1.4641321e-32 6.5902806e-25], sum to 1.0000
[2019-03-27 02:23:50,082] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0437
[2019-03-27 02:23:50,087] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.0, 82.5, 1.0, 2.0, 0.2264758970707983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 376717.4235899877, 376717.4235899884, 158308.8500992186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [18.86666666666667, 83.0, 1.0, 2.0, 0.2241016475739114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 372957.7030028827, 372957.7030028827, 158053.9721315993], 
processed observation next is [1.0, 0.9565217391304348, 0.09320695102685649, 0.83, 1.0, 1.0, 0.06518270792037517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1035993619452452, 0.1035993619452452, 0.23590145094268553], 
reward next is 0.7641, 
noisyNet noise sample is [array([-1.0250838], dtype=float32), 1.4996477]. 
=============================================
[2019-03-27 02:23:50,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.25115 ]
 [78.2116  ]
 [78.178246]
 [78.15061 ]
 [78.09415 ]], R is [[78.27344513]
 [78.25443268]
 [78.23529053]
 [78.21620941]
 [78.19724274]].
[2019-03-27 02:23:59,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0944124e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0239606e-32], sum to 1.0000
[2019-03-27 02:23:59,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1163
[2019-03-27 02:23:59,645] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 61.5, 1.0, 2.0, 0.2897228075243004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463660.3177618643, 463660.3177618649, 164505.1470210628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 819000.0000, 
sim time next is 819600.0000, 
raw observation next is [25.06666666666666, 61.66666666666666, 1.0, 2.0, 0.2895305742930316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463387.3086991419, 463387.3086991413, 164486.7115358807], 
processed observation next is [0.0, 0.4782608695652174, 0.38704581358609763, 0.6166666666666666, 1.0, 1.0, 0.14401274011208623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12871869686087276, 0.1287186968608726, 0.2455025545311652], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.44518045], dtype=float32), 1.5534009]. 
=============================================
[2019-03-27 02:24:01,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9226443e-23 1.0000000e+00 3.9496668e-36 4.7436007e-31 4.1550514e-21], sum to 1.0000
[2019-03-27 02:24:01,032] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5833
[2019-03-27 02:24:01,038] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 71.66666666666667, 1.0, 2.0, 0.2502482864269294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 411660.5502184638, 411660.5502184638, 160928.9597454255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 760800.0000, 
sim time next is 761400.0000, 
raw observation next is [21.45, 73.5, 1.0, 2.0, 0.2509682782545777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412607.3434961428, 412607.3434961434, 161003.1870389815], 
processed observation next is [1.0, 0.8260869565217391, 0.2156398104265403, 0.735, 1.0, 1.0, 0.09755214247539479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11461315097115078, 0.11461315097115095, 0.24030326423728582], 
reward next is 0.7597, 
noisyNet noise sample is [array([-0.12102187], dtype=float32), -0.55376774]. 
=============================================
[2019-03-27 02:24:03,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6607539e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6874527e-34], sum to 1.0000
[2019-03-27 02:24:03,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0597
[2019-03-27 02:24:03,284] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 67.0, 1.0, 2.0, 0.2960799492551895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 472020.0737030072, 472020.0737030066, 165063.6170354878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 831600.0000, 
sim time next is 832200.0000, 
raw observation next is [24.35, 67.5, 1.0, 2.0, 0.2984486034324834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 475367.6815119153, 475367.681511916, 165293.4735364873], 
processed observation next is [0.0, 0.6521739130434783, 0.35308056872037924, 0.675, 1.0, 1.0, 0.15475735353311254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13204657819775426, 0.13204657819775445, 0.2467066769201303], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.41316605], dtype=float32), 1.221625]. 
=============================================
[2019-03-27 02:24:05,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.8786613e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2862878e-34], sum to 1.0000
[2019-03-27 02:24:05,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5379
[2019-03-27 02:24:05,816] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 89.0, 1.0, 2.0, 0.2899520473730447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464383.0859698807, 464383.0859698813, 164558.8641335424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 874200.0000, 
sim time next is 874800.0000, 
raw observation next is [21.0, 89.0, 1.0, 2.0, 0.2893319553539936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463545.5184226623, 463545.5184226629, 164502.6423600968], 
processed observation next is [0.0, 0.13043478260869565, 0.19431279620853087, 0.89, 1.0, 1.0, 0.14377344018553442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12876264400629508, 0.12876264400629525, 0.2455263318807415], 
reward next is 0.7545, 
noisyNet noise sample is [array([2.4836943], dtype=float32), 1.3806977]. 
=============================================
[2019-03-27 02:24:05,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3442278e-31 1.0000000e+00 0.0000000e+00 1.9192369e-38 4.8205815e-31], sum to 1.0000
[2019-03-27 02:24:05,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9688
[2019-03-27 02:24:05,857] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 76.33333333333334, 1.0, 2.0, 0.3101469496316649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490613.1999019503, 490613.1999019509, 166332.5888356419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [23.15, 77.16666666666666, 1.0, 2.0, 0.3103404121180225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491231.2380670637, 491231.2380670637, 166384.4380141593], 
processed observation next is [0.0, 0.7391304347826086, 0.2962085308056872, 0.7716666666666666, 1.0, 1.0, 0.16908483387713552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645312168529547, 0.13645312168529547, 0.24833498211068553], 
reward next is 0.7517, 
noisyNet noise sample is [array([-0.2261123], dtype=float32), -0.19642429]. 
=============================================
[2019-03-27 02:24:06,082] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8486731e-29 1.0000000e+00 0.0000000e+00 2.1080002e-36 1.7847971e-28], sum to 1.0000
[2019-03-27 02:24:06,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3922
[2019-03-27 02:24:06,104] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.31666666666667, 89.0, 1.0, 2.0, 0.3008122386717805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478762.3326609325, 478762.3326609325, 165529.1557043206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 867000.0000, 
sim time next is 867600.0000, 
raw observation next is [21.3, 89.0, 1.0, 2.0, 0.2999799249130229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 477609.8224887764, 477609.8224887764, 165449.7052868117], 
processed observation next is [0.0, 0.043478260869565216, 0.2085308056872039, 0.89, 1.0, 1.0, 0.15660231917231673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13266939513577122, 0.13266939513577122, 0.24693985863703238], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.23098063], dtype=float32), 0.37764975]. 
=============================================
[2019-03-27 02:24:11,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0719546e-17 1.0000000e+00 2.6273539e-29 4.0284102e-22 9.9671107e-13], sum to 1.0000
[2019-03-27 02:24:11,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9144
[2019-03-27 02:24:11,216] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666667, 65.5, 1.0, 2.0, 0.7063718803452317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1079570.602610625, 1079570.602610624, 233967.8584736896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1167000.0000, 
sim time next is 1167600.0000, 
raw observation next is [26.53333333333333, 65.0, 1.0, 2.0, 0.6571980537275418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1003648.483374566, 1003648.483374566, 222509.327096323], 
processed observation next is [1.0, 0.5217391304347826, 0.45655608214849913, 0.65, 1.0, 1.0, 0.5869856069006527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27879124538182387, 0.27879124538182387, 0.33210347327809403], 
reward next is 0.6679, 
noisyNet noise sample is [array([-0.5439624], dtype=float32), 1.6863157]. 
=============================================
[2019-03-27 02:24:11,454] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 02:24:11,456] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:24:11,457] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:24:11,458] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:24:11,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:24:11,459] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:24:11,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:24:11,460] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:24:11,463] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:24:11,463] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:24:11,464] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:24:11,482] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-27 02:24:11,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-27 02:24:11,535] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-27 02:24:11,557] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-27 02:24:11,557] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-27 02:24:39,731] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:24:39,732] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.152305130557446, 6.9112, 168.9115474263927, 1624918.922166852, 1453872.071248269, 311348.3894367212]
[2019-03-27 02:24:39,734] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:24:39,737] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8207417e-21 1.0000000e+00 3.5876271e-33 4.1792190e-26 9.8965450e-17], sampled 0.0745693263912558
[2019-03-27 02:24:40,430] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:24:40,432] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.94086054333333, 99.53873220333332, 1.0, 2.0, 0.4525512498306878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653563.6328373454, 653563.632837346, 179206.3657788415]
[2019-03-27 02:24:40,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:24:40,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4620637e-29 1.0000000e+00 0.0000000e+00 6.1409522e-37 3.4433045e-29], sampled 0.8260989073262761
[2019-03-27 02:25:17,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:25:17,244] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.06666666666667, 49.0, 1.0, 2.0, 0.7423294431705827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1037455.443044864, 1037455.443044864, 230253.2240697509]
[2019-03-27 02:25:17,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:25:17,249] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0612160e-27 1.0000000e+00 0.0000000e+00 2.1205197e-34 4.2249997e-26], sampled 0.06266263454768717
[2019-03-27 02:25:33,607] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:25:33,609] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.4, 94.0, 1.0, 2.0, 0.6251372233940712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 873604.0512001148, 873604.0512001141, 205521.9804673836]
[2019-03-27 02:25:33,611] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:25:33,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3005352e-29 1.0000000e+00 0.0000000e+00 4.2835116e-37 2.2841306e-29], sampled 0.4465751571266915
[2019-03-27 02:25:35,350] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:25:35,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.50057931, 61.373417985, 1.0, 2.0, 0.5237307440324181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731843.7666629656, 731843.7666629656, 187398.1950207081]
[2019-03-27 02:25:35,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:25:35,354] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.3724568e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6175284e-31], sampled 0.670314404096851
[2019-03-27 02:25:52,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:25:52,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.34896434333334, 58.96211854333334, 1.0, 2.0, 0.4942876757559094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 690687.635402616, 690687.635402616, 182704.7056175029]
[2019-03-27 02:25:52,856] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:25:52,858] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5359684e-30 1.0000000e+00 0.0000000e+00 6.0230683e-38 3.7715140e-30], sampled 0.8720571323085119
[2019-03-27 02:26:02,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.10996215]
[2019-03-27 02:26:02,669] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.80046174, 76.43699755, 1.0, 2.0, 0.5447809524901476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799941.0791716947, 799941.0791716947, 195770.807380333]
[2019-03-27 02:26:02,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:26:02,672] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2919326e-30 1.0000000e+00 0.0000000e+00 9.9114378e-38 2.8457372e-30], sampled 0.6030475158198837
[2019-03-27 02:26:05,951] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.8995 3007517287.3333 1760.0000
[2019-03-27 02:26:05,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.9104 2842541425.6700 1162.0000
[2019-03-27 02:26:06,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.8988 2927348875.6851 1331.0000
[2019-03-27 02:26:06,095] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.6106 3163785850.3650 1852.0000
[2019-03-27 02:26:06,126] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8172 2779323821.3833 931.0000
[2019-03-27 02:26:07,140] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1300000, evaluation results [1300000.0, 7884.610596682498, 3163785850.3649955, 1852.0, 8260.89875417998, 2927348875.6850595, 1331.0, 8659.817243421787, 2779323821.3833375, 931.0, 8001.899523114949, 3007517287.3333435, 1760.0, 8492.910427444516, 2842541425.6699595, 1162.0]
[2019-03-27 02:26:07,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9154549e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.1168295e-32], sum to 1.0000
[2019-03-27 02:26:07,382] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4946
[2019-03-27 02:26:07,390] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 88.0, 1.0, 2.0, 0.3387289176552802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524766.1754814181, 524766.1754814174, 168672.0485317231], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 937800.0000, 
sim time next is 938400.0000, 
raw observation next is [22.5, 88.33333333333333, 1.0, 2.0, 0.3383860009893634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524331.8892428037, 524331.8892428043, 168640.3182976949], 
processed observation next is [0.0, 0.8695652173913043, 0.2654028436018958, 0.8833333333333333, 1.0, 1.0, 0.2028746999871848, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1456477470118899, 0.14564774701189007, 0.25170196760849983], 
reward next is 0.7483, 
noisyNet noise sample is [array([1.093029], dtype=float32), 1.0097954]. 
=============================================
[2019-03-27 02:26:09,242] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.5583875e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5311982e-33], sum to 1.0000
[2019-03-27 02:26:09,251] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9475
[2019-03-27 02:26:09,258] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3282851924684677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508893.8120397911, 508893.8120397917, 167434.2684789953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 973200.0000, 
sim time next is 973800.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3321974131769301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514957.990941065, 514957.990941065, 167906.5624564621], 
processed observation next is [1.0, 0.2608695652173913, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19541857009268684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14304388637251805, 0.14304388637251805, 0.2506068096365106], 
reward next is 0.7494, 
noisyNet noise sample is [array([-1.865941], dtype=float32), 0.08380915]. 
=============================================
[2019-03-27 02:26:21,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0266430e-19 1.0000000e+00 5.2916904e-30 1.7088624e-22 2.4198213e-15], sum to 1.0000
[2019-03-27 02:26:21,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1201
[2019-03-27 02:26:21,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 59.66666666666666, 1.0, 2.0, 0.9932054985899973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1511620.455253242, 1511620.455253242, 315129.8249189805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1176000.0000, 
sim time next is 1176600.0000, 
raw observation next is [27.6, 59.33333333333334, 1.0, 2.0, 0.9917971786540183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1512080.254822638, 1512080.254822638, 315012.3284870182], 
processed observation next is [1.0, 0.6086956521739131, 0.5071090047393366, 0.5933333333333334, 1.0, 1.0, 0.9901170827156847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4200222930062883, 0.4200222930062883, 0.4701676544582361], 
reward next is 0.5298, 
noisyNet noise sample is [array([0.6611426], dtype=float32), 0.13732678]. 
=============================================
[2019-03-27 02:26:23,428] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7758902e-30 1.0000000e+00 0.0000000e+00 1.4078292e-37 5.3337169e-32], sum to 1.0000
[2019-03-27 02:26:23,441] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6034
[2019-03-27 02:26:23,451] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 85.0, 1.0, 2.0, 0.6640628917258884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1012614.973353868, 1012614.973353868, 223877.5349002591], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [23.51666666666667, 85.00000000000001, 1.0, 2.0, 0.7677448542104809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1170121.890334053, 1170121.890334052, 248882.1399740402], 
processed observation next is [1.0, 0.391304347826087, 0.31358609794628767, 0.8500000000000001, 1.0, 1.0, 0.7201745231451577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3250338584261258, 0.3250338584261256, 0.37146588055826896], 
reward next is 0.6285, 
noisyNet noise sample is [array([-0.1250263], dtype=float32), 0.55678654]. 
=============================================
[2019-03-27 02:26:27,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7254084e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5617578e-38], sum to 1.0000
[2019-03-27 02:26:27,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6876
[2019-03-27 02:26:27,928] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 92.0, 1.0, 2.0, 0.4428933322020195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633474.8924364913, 633474.8924364913, 177016.9548026978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [24.26666666666667, 92.33333333333334, 1.0, 2.0, 0.4391945585639844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629226.1205875443, 629226.1205875437, 176621.3329282753], 
processed observation next is [1.0, 0.21739130434782608, 0.34913112164297017, 0.9233333333333335, 1.0, 1.0, 0.3243307934505837, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17478503349654007, 0.17478503349653993, 0.2636139297436945], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.9252953], dtype=float32), -0.30110964]. 
=============================================
[2019-03-27 02:26:32,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7288544e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3778429e-34], sum to 1.0000
[2019-03-27 02:26:32,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6683
[2019-03-27 02:26:32,706] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.11666666666667, 94.00000000000001, 1.0, 2.0, 0.3215166228207157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 507093.7928781672, 507093.7928781665, 167533.7984119957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1365000.0000, 
sim time next is 1365600.0000, 
raw observation next is [21.13333333333334, 94.0, 1.0, 2.0, 0.3217289673402233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507216.5041706824, 507216.5041706824, 167538.7064395515], 
processed observation next is [1.0, 0.8260869565217391, 0.20063191153238583, 0.94, 1.0, 1.0, 0.182805984747257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14089347338074512, 0.14089347338074512, 0.2500577708053007], 
reward next is 0.7499, 
noisyNet noise sample is [array([0.1149676], dtype=float32), 0.52753365]. 
=============================================
[2019-03-27 02:26:38,113] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4004793e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8981884e-37], sum to 1.0000
[2019-03-27 02:26:38,119] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2751
[2019-03-27 02:26:38,127] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 93.33333333333334, 1.0, 2.0, 0.3817276113525269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575046.238524547, 575046.2385245475, 172435.9016830741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [22.75, 93.66666666666667, 1.0, 2.0, 0.3812725536995554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574517.8006273448, 574517.800627344, 172393.7785899106], 
processed observation next is [0.0, 0.8695652173913043, 0.27725118483412325, 0.9366666666666668, 1.0, 1.0, 0.2545452454211511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595882779520402, 0.15958827795204, 0.2573041471491203], 
reward next is 0.7427, 
noisyNet noise sample is [array([0.6096369], dtype=float32), 0.07043429]. 
=============================================
[2019-03-27 02:26:39,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.32677e-37 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sum to 1.0000
[2019-03-27 02:26:39,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0457
[2019-03-27 02:26:39,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 94.16666666666667, 1.0, 2.0, 0.3344629297533469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519782.143610795, 519782.1436107944, 168324.9827685973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1491000.0000, 
sim time next is 1491600.0000, 
raw observation next is [21.9, 93.33333333333334, 1.0, 2.0, 0.3382249340770461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524194.9318888809, 524194.9318888815, 168632.8292670927], 
processed observation next is [0.0, 0.2608695652173913, 0.23696682464454974, 0.9333333333333335, 1.0, 1.0, 0.20268064346632061, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14560970330246692, 0.1456097033024671, 0.2516907899508846], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.3389417], dtype=float32), 0.97021264]. 
=============================================
[2019-03-27 02:26:40,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.367434e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:26:40,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6706
[2019-03-27 02:26:40,357] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 97.66666666666669, 1.0, 2.0, 0.3132529939445962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495157.7733525257, 495157.7733525263, 166660.6729363645], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1488000.0000, 
sim time next is 1488600.0000, 
raw observation next is [20.85, 97.0, 1.0, 2.0, 0.317154963496824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499638.4752906411, 499638.4752906417, 166958.1857657255], 
processed observation next is [0.0, 0.21739130434782608, 0.18720379146919444, 0.97, 1.0, 1.0, 0.1772951367431614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13878846535851141, 0.13878846535851158, 0.24919132203839628], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.40827727], dtype=float32), -0.24259505]. 
=============================================
[2019-03-27 02:26:44,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9832675e-30 1.0000000e+00 0.0000000e+00 3.4427915e-37 3.5822426e-31], sum to 1.0000
[2019-03-27 02:26:44,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4619
[2019-03-27 02:26:44,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3260588268832261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511414.5032452514, 511414.5032452508, 167800.4732867103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1560600.0000, 
sim time next is 1561200.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3261569680534232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511568.5270110629, 511568.5270110629, 167812.3356408702], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18814092536557012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14210236861418413, 0.14210236861418413, 0.2504661725983137], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.54210407], dtype=float32), -2.072483]. 
=============================================
[2019-03-27 02:26:50,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1617151e-28 1.0000000e+00 1.6213837e-38 3.5609811e-35 2.8659908e-29], sum to 1.0000
[2019-03-27 02:26:50,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9247
[2019-03-27 02:26:50,463] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.46666666666667, 94.00000000000001, 1.0, 2.0, 0.5072102117221868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713654.1920733452, 713654.1920733452, 185362.3277467391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1739400.0000, 
sim time next is 1740000.0000, 
raw observation next is [24.43333333333334, 94.0, 1.0, 2.0, 0.4719187475649988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664982.1040135298, 664982.1040135304, 180030.1480528139], 
processed observation next is [1.0, 0.13043478260869565, 0.3570300157977887, 0.94, 1.0, 1.0, 0.3637575271867456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1847172511148694, 0.18471725111486956, 0.2687017135116625], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.4442018], dtype=float32), 0.6296799]. 
=============================================
[2019-03-27 02:26:50,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.93317]
 [66.884  ]
 [66.94685]
 [67.13353]
 [67.22113]], R is [[66.84705353]
 [66.90192413]
 [66.95485687]
 [67.00660706]
 [67.05527496]].
[2019-03-27 02:26:57,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.418006e-28 1.000000e+00 0.000000e+00 6.186081e-34 1.981432e-28], sum to 1.0000
[2019-03-27 02:26:57,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4951
[2019-03-27 02:26:57,831] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3456426327659531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535458.5648562041, 535458.5648562048, 169532.0914627482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1818000.0000, 
sim time next is 1818600.0000, 
raw observation next is [21.83333333333334, 93.83333333333334, 1.0, 2.0, 0.3459735753827479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535856.5860672527, 535856.5860672527, 169561.2748346665], 
processed observation next is [1.0, 0.043478260869565216, 0.23380726698262277, 0.9383333333333335, 1.0, 1.0, 0.21201635588282883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14884905168534796, 0.14884905168534796, 0.25307652960397986], 
reward next is 0.7469, 
noisyNet noise sample is [array([0.8515496], dtype=float32), 1.8218648]. 
=============================================
[2019-03-27 02:26:58,367] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.804901e-27 1.000000e+00 0.000000e+00 2.830056e-34 5.232617e-29], sum to 1.0000
[2019-03-27 02:26:58,374] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7876
[2019-03-27 02:26:58,380] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.78333333333333, 94.16666666666667, 1.0, 2.0, 0.3452024229164352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534722.3565779442, 534722.356577945, 169470.8104871143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3456426327659531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535458.5648562041, 535458.5648562048, 169532.0914627482], 
processed observation next is [1.0, 0.043478260869565216, 0.23222748815165886, 0.94, 1.0, 1.0, 0.2116176298384977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14873849023783448, 0.14873849023783467, 0.25303297233246], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.03688713], dtype=float32), -1.2739667]. 
=============================================
[2019-03-27 02:26:58,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.373146]
 [73.65721 ]
 [74.085266]
 [74.69979 ]
 [75.1572  ]], R is [[73.27830505]
 [73.29257965]
 [73.30675507]
 [73.320755  ]
 [73.33457947]].
[2019-03-27 02:27:03,145] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 02:27:03,146] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:27:03,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:27:03,150] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:27:03,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:27:03,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:27:03,152] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:27:03,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:27:03,153] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:27:03,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:27:03,157] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:27:03,187] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-27 02:27:03,209] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-27 02:27:03,227] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-27 02:27:03,248] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-27 02:27:03,249] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-27 02:27:12,072] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:27:12,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.94762266, 62.75625101333333, 1.0, 2.0, 0.3260462934304502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520388.6615599389, 520388.6615599389, 168632.1828045047]
[2019-03-27 02:27:12,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:27:12,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0358313e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5351108e-38], sampled 0.7961019595586996
[2019-03-27 02:28:01,304] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:01,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.4, 45.16666666666666, 1.0, 2.0, 0.6365584448194274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 889571.4315062357, 889571.4315062352, 207751.7380808285]
[2019-03-27 02:28:01,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:01,309] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.1083861e-31 1.0000000e+00 0.0000000e+00 3.9709340e-38 5.1044564e-33], sampled 0.21602019681892426
[2019-03-27 02:28:05,896] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:05,897] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.0, 61.0, 1.0, 2.0, 0.5456206856884069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762443.0144289293, 762443.0144289287, 191052.3971866705]
[2019-03-27 02:28:05,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:28:05,901] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2362803e-29 1.0000000e+00 0.0000000e+00 3.0869828e-36 4.8876486e-31], sampled 0.87318957258673
[2019-03-27 02:28:15,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:15,200] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.0, 1.0, 2.0, 0.9091592265530285, 1.0, 1.0, 0.9091592265530285, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2542939.025665193, 2542939.025665193, 476510.8530655407]
[2019-03-27 02:28:15,201] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:15,203] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.6495543e-16 1.0000000e+00 3.6806986e-26 1.3372950e-16 3.6827319e-09], sampled 0.12322099114186846
[2019-03-27 02:28:15,204] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2542939.025665193 W.
[2019-03-27 02:28:26,293] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:26,294] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.32171297000001, 70.42732115333334, 1.0, 2.0, 0.6232974006576744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871031.9188654238, 871031.9188654238, 205166.4166950089]
[2019-03-27 02:28:26,295] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:28:26,298] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.2588659e-31 1.0000000e+00 0.0000000e+00 4.8401880e-38 8.8252296e-33], sampled 0.0037513428574458274
[2019-03-27 02:28:39,326] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:39,328] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.91666666666667, 74.83333333333334, 1.0, 2.0, 0.8156757905608464, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981759862768937, 6.9112, 168.91247999851, 2037012.517504252, 1986955.063587898, 412617.207484535]
[2019-03-27 02:28:39,331] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:39,334] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6770076e-20 1.0000000e+00 8.1350924e-31 3.6649982e-23 1.3320661e-16], sampled 0.48902660425102107
[2019-03-27 02:28:39,335] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2037012.517504252 W.
[2019-03-27 02:28:47,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:47,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.1, 71.0, 1.0, 2.0, 0.5836044939511805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815541.4543886071, 815541.4543886071, 197730.8416206079]
[2019-03-27 02:28:47,133] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:47,136] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.4490226e-31 1.0000000e+00 0.0000000e+00 7.0984792e-38 2.1300560e-32], sampled 0.7544223446277698
[2019-03-27 02:28:47,172] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:47,173] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.466920975, 77.447682855, 1.0, 2.0, 0.5411374589779672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773923.9422861103, 773923.9422861097, 192557.3672472984]
[2019-03-27 02:28:47,175] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:28:47,178] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.3717249e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4061194e-36], sampled 0.41850687984741464
[2019-03-27 02:28:47,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:47,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.91601996666667, 55.14084800333333, 1.0, 2.0, 0.6141208419647083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924554.62364284, 924554.6236428393, 211689.5323535734]
[2019-03-27 02:28:47,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:28:47,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.5396065e-30 1.0000000e+00 0.0000000e+00 1.9117628e-37 2.0973846e-32], sampled 0.43438755027275144
[2019-03-27 02:28:48,584] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:48,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.65, 89.33333333333333, 1.0, 2.0, 0.3221267845688506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508478.1178813502, 508478.1178813502, 167647.5761161949]
[2019-03-27 02:28:48,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:48,590] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1570965e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9329414844578963
[2019-03-27 02:28:57,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-03-27 02:28:57,089] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.109319136]
[2019-03-27 02:28:57,090] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.45, 87.83333333333334, 1.0, 2.0, 0.5415887712110053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756806.8641954357, 756806.8641954362, 190369.0900165024]
[2019-03-27 02:28:57,091] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:28:57,092] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7846319e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4506814e-35], sampled 0.4025689288655846
[2019-03-27 02:28:57,247] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 02:28:57,599] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:28:57,688] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-27 02:28:57,768] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.0108 3164047374.5595 1777.0000
[2019-03-27 02:28:58,786] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1325000, evaluation results [1325000.0, 7885.010848014406, 3164047374.559538, 1777.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:29:10,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0112745e-29 1.0000000e+00 0.0000000e+00 9.5096334e-35 1.0070294e-28], sum to 1.0000
[2019-03-27 02:29:10,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5613
[2019-03-27 02:29:10,415] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 84.66666666666667, 1.0, 2.0, 0.5300039202080183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740612.7484461255, 740612.7484461262, 188430.6489737317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [27.38333333333333, 84.83333333333334, 1.0, 2.0, 0.5291336405919979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739396.2206433906, 739396.2206433906, 188286.5699530148], 
processed observation next is [1.0, 0.9565217391304348, 0.4968404423380725, 0.8483333333333334, 1.0, 1.0, 0.4326911332433709, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2053878390676085, 0.2053878390676085, 0.2810247312731564], 
reward next is 0.7190, 
noisyNet noise sample is [array([0.16938871], dtype=float32), 2.2047276]. 
=============================================
[2019-03-27 02:29:19,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0876475e-09 9.4254416e-01 1.5533042e-16 3.5578737e-06 5.7452235e-02], sum to 1.0000
[2019-03-27 02:29:19,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6920
[2019-03-27 02:29:19,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2262173.484549613 W.
[2019-03-27 02:29:19,605] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.1, 64.5, 1.0, 2.0, 0.5392465738095493, 1.0, 2.0, 0.5392465738095493, 1.0, 1.0, 0.9364929671401623, 6.9112, 6.9112, 170.5573041426782, 2262173.484549613, 2262173.484549613, 443346.9790869851], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2302200.0000, 
sim time next is 2302800.0000, 
raw observation next is [32.13333333333334, 64.33333333333333, 1.0, 2.0, 0.8300988998031312, 1.0, 2.0, 0.8300988998031312, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2321600.072552725, 2321600.072552725, 434792.6005413902], 
processed observation next is [1.0, 0.6521739130434783, 0.7219589257503953, 0.6433333333333333, 1.0, 1.0, 0.795299879280881, 1.0, 1.0, 0.795299879280881, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6448889090424237, 0.6448889090424237, 0.6489441799125226], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.517773], dtype=float32), 0.27720827]. 
=============================================
[2019-03-27 02:29:20,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.55374350e-28 1.00000000e+00 4.09397171e-37 1.18149525e-33
 1.76025765e-30], sum to 1.0000
[2019-03-27 02:29:20,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5182
[2019-03-27 02:29:20,183] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.81666666666667, 67.33333333333334, 1.0, 2.0, 0.5449708193366625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761534.5743908766, 761534.574390876, 190944.5480120095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [31.7, 68.0, 1.0, 2.0, 0.5517458483856501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771005.3408598445, 771005.340859845, 192102.7081934328], 
processed observation next is [1.0, 0.782608695652174, 0.7014218009478673, 0.68, 1.0, 1.0, 0.4599347570911447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2141681502388457, 0.21416815023884583, 0.2867204599901982], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.02181186], dtype=float32), 0.6768291]. 
=============================================
[2019-03-27 02:29:23,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8174196e-09 4.1352984e-02 1.1599173e-15 5.0272169e-03 9.5361978e-01], sum to 1.0000
[2019-03-27 02:29:23,578] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0302
[2019-03-27 02:29:23,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2334702.743667849 W.
[2019-03-27 02:29:23,594] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.45, 65.66666666666667, 1.0, 2.0, 0.5565196324078403, 1.0, 2.0, 0.5565196324078403, 1.0, 2.0, 0.9664905576376265, 6.9112, 6.9112, 170.5573041426782, 2334702.743667849, 2334702.743667849, 456430.3414981039], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2369400.0000, 
sim time next is 2370000.0000, 
raw observation next is [31.6, 65.33333333333334, 1.0, 2.0, 0.7366685964883136, 1.0, 2.0, 0.7366685964883136, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2060060.203615905, 2060060.203615904, 390219.2667603217], 
processed observation next is [1.0, 0.43478260869565216, 0.6966824644549764, 0.6533333333333334, 1.0, 1.0, 0.6827332487811008, 1.0, 1.0, 0.6827332487811008, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5722389454488626, 0.5722389454488622, 0.5824168160601816], 
reward next is 0.4176, 
noisyNet noise sample is [array([0.30258244], dtype=float32), -1.1707824]. 
=============================================
[2019-03-27 02:29:23,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[41.03637 ]
 [40.61909 ]
 [39.933456]
 [39.641514]
 [39.964   ]], R is [[41.49486923]
 [41.39868164]
 [41.3548851 ]
 [41.30635071]
 [41.26335144]].
[2019-03-27 02:29:29,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2015590e-11 9.9230123e-01 2.5137548e-19 3.9090482e-09 7.6987757e-03], sum to 1.0000
[2019-03-27 02:29:29,714] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1159
[2019-03-27 02:29:29,718] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 89.0, 1.0, 2.0, 0.3474350318314943, 1.0, 2.0, 0.3474350318314943, 1.0, 1.0, 0.5910299217090563, 6.911200000000001, 6.9112, 170.5573041426782, 1456969.656241066, 1456969.656241065, 324222.6431633132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2466000.0000, 
sim time next is 2466600.0000, 
raw observation next is [26.23333333333333, 89.0, 1.0, 2.0, 0.9397550503528604, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103778, 1313541.706608114, 1313541.706608114, 281105.4066723536], 
processed observation next is [1.0, 0.5652173913043478, 0.44233807266982617, 0.89, 1.0, 1.0, 0.9274157233166993, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451520414, 0.36487269628003166, 0.36487269628003166, 0.41956030846619946], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.511209], dtype=float32), 0.884495]. 
=============================================
[2019-03-27 02:29:32,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5688911e-30 1.0000000e+00 0.0000000e+00 1.4235395e-36 3.2231391e-32], sum to 1.0000
[2019-03-27 02:29:32,613] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4266
[2019-03-27 02:29:32,621] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 83.66666666666667, 1.0, 2.0, 0.5401608258184066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754810.768819606, 754810.7688196066, 190127.7871818624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [27.85, 84.0, 1.0, 2.0, 0.5411116308934741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756139.8791816902, 756139.8791816895, 190288.0578679675], 
processed observation next is [1.0, 0.8260869565217391, 0.5189573459715641, 0.84, 1.0, 1.0, 0.4471224468596074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21003885532824726, 0.21003885532824706, 0.2840120266686082], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.1066174], dtype=float32), 0.1652111]. 
=============================================
[2019-03-27 02:29:34,192] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.12784336e-28 1.00000000e+00 0.00000000e+00 1.83203722e-34
 1.10646734e-32], sum to 1.0000
[2019-03-27 02:29:34,199] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7428
[2019-03-27 02:29:34,203] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5458538625052386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862938.1936454087, 862938.1936454081, 202589.1821842346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5993207983065164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947479.7354895928, 947479.7354895921, 213367.7240488339], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.5172539738632728, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26318881541377576, 0.2631888154137756, 0.3184592896251252], 
reward next is 0.6815, 
noisyNet noise sample is [array([1.5643884], dtype=float32), 2.4593637]. 
=============================================
[2019-03-27 02:29:34,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4350640e-31 1.0000000e+00 0.0000000e+00 9.4418127e-37 2.1206005e-34], sum to 1.0000
[2019-03-27 02:29:34,882] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7011
[2019-03-27 02:29:34,886] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.78333333333333, 94.00000000000001, 1.0, 2.0, 0.5526303491747118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772241.7846558696, 772241.7846558696, 192252.9926389841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2502600.0000, 
sim time next is 2503200.0000, 
raw observation next is [26.76666666666667, 94.0, 1.0, 2.0, 0.552070865847513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771459.6826550601, 771459.6826550601, 192156.6470472417], 
processed observation next is [1.0, 1.0, 0.46761453396524505, 0.94, 1.0, 1.0, 0.4603263443945939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21429435629307225, 0.21429435629307225, 0.2868009657421518], 
reward next is 0.7132, 
noisyNet noise sample is [array([1.7694126], dtype=float32), -0.17530364]. 
=============================================
[2019-03-27 02:29:35,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3823063e-30 1.0000000e+00 0.0000000e+00 3.1221903e-37 3.5464756e-35], sum to 1.0000
[2019-03-27 02:29:35,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8488
[2019-03-27 02:29:35,870] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 93.0, 1.0, 2.0, 0.5542783131522021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774545.4776053568, 774545.4776053568, 192537.2200161644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2493600.0000, 
sim time next is 2494200.0000, 
raw observation next is [26.91666666666667, 93.0, 1.0, 2.0, 0.5527946400132785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772471.4470041594, 772471.4470041594, 192281.2788737733], 
processed observation next is [1.0, 0.8695652173913043, 0.4747235387045816, 0.93, 1.0, 1.0, 0.4611983614617813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21457540194559985, 0.21457540194559985, 0.2869869833936915], 
reward next is 0.7130, 
noisyNet noise sample is [array([-2.1710281], dtype=float32), 1.0224388]. 
=============================================
[2019-03-27 02:29:44,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7782354e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2723270e-37], sum to 1.0000
[2019-03-27 02:29:44,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3790
[2019-03-27 02:29:44,358] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3958583586890215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590679.0946986069, 590679.0946986069, 173679.9431152444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2669400.0000, 
sim time next is 2670000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3957938915681671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590582.9688820627, 590582.9688820634, 173671.1171624665], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2720408332146591, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16405082468946186, 0.16405082468946205, 0.259210622630547], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.22770886], dtype=float32), 0.32192692]. 
=============================================
[2019-03-27 02:29:44,374] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.77815 ]
 [76.76543 ]
 [76.752014]
 [76.73695 ]
 [76.648895]], R is [[76.76321411]
 [76.73635864]
 [76.70974731]
 [76.68336487]
 [76.65719604]].
[2019-03-27 02:29:51,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9432698e-29 1.0000000e+00 0.0000000e+00 7.5308361e-35 1.4451169e-31], sum to 1.0000
[2019-03-27 02:29:51,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3400
[2019-03-27 02:29:51,793] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5695950113844704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900436.0167523412, 900436.0167523412, 207253.4065095986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2992800.0000, 
sim time next is 2993400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.6067918806907895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959263.9464168317, 959263.9464168324, 214947.5562249226], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.5262552779407103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2664622073380088, 0.266462207338009, 0.3208172480968994], 
reward next is 0.6792, 
noisyNet noise sample is [array([-0.3189368], dtype=float32), -0.35569206]. 
=============================================
[2019-03-27 02:29:54,746] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 02:29:54,750] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:29:54,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:29:54,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:29:54,752] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:29:54,753] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:29:54,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:29:54,754] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:29:54,755] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:29:54,758] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:29:54,757] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:29:54,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-27 02:29:54,801] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-27 02:29:54,832] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-27 02:29:54,834] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-27 02:29:54,835] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-27 02:30:31,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:30:31,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.18019302, 77.80221898, 1.0, 2.0, 0.7691201768445993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1074916.249734566, 1074916.249734567, 236489.8307449518]
[2019-03-27 02:30:31,198] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:30:31,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8747202e-31 1.0000000e+00 0.0000000e+00 4.9569577e-37 5.9009082e-34], sampled 0.4127478760616905
[2019-03-27 02:30:39,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:30:39,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.523489005, 87.445027585, 1.0, 2.0, 0.2398872087118128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 400663.1205745942, 400663.1205745942, 159041.073043711]
[2019-03-27 02:30:39,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:30:39,156] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.962871e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.16812881179579786
[2019-03-27 02:30:41,116] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:30:41,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.46854128333333, 99.02662297, 1.0, 2.0, 0.3952657995527875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 588138.7267592623, 588138.7267592623, 173396.3442433239]
[2019-03-27 02:30:41,119] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:30:41,122] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7530582e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.43771038182330724
[2019-03-27 02:30:43,438] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:30:43,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.90000000000001, 63.0, 1.0, 2.0, 0.5906023275893998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 825324.1766915764, 825324.1766915771, 199007.9544383061]
[2019-03-27 02:30:43,441] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:30:43,445] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7612394e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7606162752189941
[2019-03-27 02:30:48,437] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:30:48,438] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.47097339833333, 67.9065482, 1.0, 2.0, 0.5008293044265045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699831.5218010428, 699831.5218010434, 183724.1719031576]
[2019-03-27 02:30:48,441] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:30:48,443] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2017687e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2451510e-38], sampled 0.5527569540254305
[2019-03-27 02:31:33,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:31:33,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.4, 88.5, 1.0, 2.0, 0.4911506597302738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686302.7433588683, 686302.7433588676, 182220.1406071647]
[2019-03-27 02:31:33,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:31:33,250] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.021936e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.6288007169890698
[2019-03-27 02:31:33,929] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113530025]
[2019-03-27 02:31:33,929] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.508893275, 64.96264298166668, 1.0, 2.0, 0.4315099161231325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630770.8971663492, 630770.8971663492, 177112.1295044941]
[2019-03-27 02:31:33,930] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:31:33,933] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.323958e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7410014020918012
[2019-03-27 02:31:48,723] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-03-27 02:31:48,856] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.5787 3164143480.2955 1781.0000
[2019-03-27 02:31:48,990] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6191 2842467191.2929 1132.0000
[2019-03-27 02:31:49,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6857 2927400807.7850 1338.0000
[2019-03-27 02:31:49,281] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2841 3007675385.2347 1766.0000
[2019-03-27 02:31:50,299] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1350000, evaluation results [1350000.0, 7883.578665216324, 3164143480.2955017, 1781.0, 8253.68573338827, 2927400807.784972, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7998.284063504842, 3007675385.2346897, 1766.0, 8496.61908412816, 2842467191.292949, 1132.0]
[2019-03-27 02:31:52,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2281947e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8875593e-37], sum to 1.0000
[2019-03-27 02:31:52,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3562
[2019-03-27 02:31:52,429] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.352172829773883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542171.901748241, 542171.9017482405, 169985.6101424381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2851200.0000, 
sim time next is 2851800.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.3512009365583582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540994.8831060061, 540994.8831060055, 169897.9269692278], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.2183143813956123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15027635641833503, 0.15027635641833487, 0.2535789954764594], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.5983949], dtype=float32), -0.7713808]. 
=============================================
[2019-03-27 02:32:02,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1378974e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:32:02,633] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7813
[2019-03-27 02:32:02,640] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3039788098850856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484069.1000402491, 484069.1000402485, 165914.2516932636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3025200.0000, 
sim time next is 3025800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3036599564343593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483561.1139139077, 483561.1139139077, 165877.623578947], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1610360920895895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13432253164275215, 0.13432253164275215, 0.24757854265514478], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.41639605], dtype=float32), -1.2641183]. 
=============================================
[2019-03-27 02:32:02,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:32:02,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7651
[2019-03-27 02:32:02,894] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 83.5, 1.0, 2.0, 0.4553574347421961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650414.1350752919, 650414.1350752924, 178714.4579361897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3295800.0000, 
sim time next is 3296400.0000, 
raw observation next is [25.33333333333333, 83.33333333333333, 1.0, 2.0, 0.4455018479294462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641259.7541544046, 641259.7541544046, 177905.087590808], 
processed observation next is [0.0, 0.13043478260869565, 0.3996840442338071, 0.8333333333333333, 1.0, 1.0, 0.331929937264393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1781277094873346, 0.1781277094873346, 0.2655299814788179], 
reward next is 0.7345, 
noisyNet noise sample is [array([1.5091407], dtype=float32), 0.27614793]. 
=============================================
[2019-03-27 02:32:04,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:32:04,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6277
[2019-03-27 02:32:04,082] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.331634081581932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515089.818378539, 515089.818378539, 167947.7659570396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3043800.0000, 
sim time next is 3044400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3314771753901596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514847.1167386528, 514847.1167386528, 167928.8227189743], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19455081372308383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14301308798295911, 0.14301308798295911, 0.25064003390891687], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.08042897], dtype=float32), 0.74643236]. 
=============================================
[2019-03-27 02:32:10,023] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4596441e-12 8.7187271e-09 6.4075023e-19 7.5152361e-06 9.9999249e-01], sum to 1.0000
[2019-03-27 02:32:10,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2974
[2019-03-27 02:32:10,033] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.7707462562697565, 1.0, 2.0, 0.7059631676491408, 1.0, 2.0, 1.03, 7.005103311095686, 6.9112, 170.5573041426782, 2962389.278136724, 2895122.516656408, 544605.6161873388], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3413400.0000, 
sim time next is 3414000.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.6137205816619804, 1.0, 2.0, 0.6137205816619804, 1.0, 2.0, 1.03, 6.951478924194703, 6.9112, 170.5573041426782, 2574918.547273557, 2546065.11543101, 493008.265575125], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.534603110436121, 1.0, 1.0, 0.534603110436121, 1.0, 1.0, 1.0365853658536586, 0.0040278924194702805, 0.0, 0.8375144448122397, 0.7152551520204325, 0.7072403098419472, 0.7358332322016791], 
reward next is 0.0628, 
noisyNet noise sample is [array([-0.81074727], dtype=float32), 0.9574808]. 
=============================================
[2019-03-27 02:32:10,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[40.689835]
 [41.154175]
 [40.88564 ]
 [40.518456]
 [40.50109 ]], R is [[41.18624496]
 [40.77438354]
 [40.366642  ]
 [39.96297455]
 [39.56334686]].
[2019-03-27 02:32:13,063] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:32:13,070] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2566
[2019-03-27 02:32:13,074] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4557218816054515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645251.6411246454, 645251.6411246454, 178042.8757508932], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.34424323084994163, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17923656697906815, 0.17923656697906815, 0.26573563544909434], 
reward next is 0.7343, 
noisyNet noise sample is [array([-0.9252046], dtype=float32), -0.8216233]. 
=============================================
[2019-03-27 02:32:38,101] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.625538e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:32:38,111] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3829
[2019-03-27 02:32:38,116] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 61.0, 1.0, 2.0, 0.616858181621116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862029.7291866334, 862029.7291866334, 203929.5574139173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3848400.0000, 
sim time next is 3849000.0000, 
raw observation next is [34.58333333333334, 60.83333333333334, 1.0, 2.0, 0.6432020043697867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898859.5364505397, 898859.5364505397, 209070.0666631784], 
processed observation next is [0.0, 0.5652173913043478, 0.8380726698262247, 0.6083333333333334, 1.0, 1.0, 0.5701228968310683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24968320456959436, 0.24968320456959436, 0.3120448756166842], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.6183726], dtype=float32), 0.5376936]. 
=============================================
[2019-03-27 02:32:38,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.28305]
 [73.16137]
 [73.13261]
 [73.1017 ]
 [73.06268]], R is [[73.14758301]
 [73.11173248]
 [73.07627869]
 [73.04122925]
 [73.00653839]].
[2019-03-27 02:32:40,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.11813815e-10 9.99992847e-01 1.64572225e-18 2.04409005e-08
 7.10619770e-06], sum to 1.0000
[2019-03-27 02:32:40,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8872
[2019-03-27 02:32:40,374] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2011384.203966908 W.
[2019-03-27 02:32:40,383] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 72.66666666666666, 1.0, 2.0, 0.4795190636017868, 1.0, 2.0, 0.4795190636017868, 1.0, 1.0, 0.8210013611407189, 6.9112, 6.9112, 170.5573041426782, 2011384.203966908, 2011384.203966908, 399230.2737823962], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3660000.0000, 
sim time next is 3660600.0000, 
raw observation next is [29.0, 73.33333333333334, 1.0, 2.0, 0.7770063161070037, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.981488127095602, 6.9112, 168.9125378622753, 1982893.358751341, 1933028.665848263, 403114.1889170818], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7333333333333334, 1.0, 1.0, 0.7313329109722936, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007028812709560217, 0.0, 0.8294378893982002, 0.5508037107642614, 0.5369524071800731, 0.6016629685329579], 
reward next is 0.0469, 
noisyNet noise sample is [array([-0.44003853], dtype=float32), -2.0357695]. 
=============================================
[2019-03-27 02:32:43,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4425681e-28 1.0000000e+00 0.0000000e+00 4.4437216e-32 1.9986568e-28], sum to 1.0000
[2019-03-27 02:32:43,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6650
[2019-03-27 02:32:43,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 75.66666666666666, 1.0, 2.0, 0.5359869445913245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 748976.2079533341, 748976.2079533335, 189426.4703816694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3699600.0000, 
sim time next is 3700200.0000, 
raw observation next is [29.0, 74.83333333333334, 1.0, 2.0, 0.5307761881085955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741692.2709010799, 741692.2709010799, 188558.4893703967], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7483333333333334, 1.0, 1.0, 0.4346701061549343, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2060256308058555, 0.2060256308058555, 0.2814305811498458], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.689176], dtype=float32), -0.2938734]. 
=============================================
[2019-03-27 02:32:44,479] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0421232e-24 1.0000000e+00 4.5676529e-33 8.4428624e-27 6.5028260e-25], sum to 1.0000
[2019-03-27 02:32:44,487] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7778
[2019-03-27 02:32:44,491] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.8747429575698875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1222618.795375424, 1222618.795375424, 263081.4571804818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3740400.0000, 
sim time next is 3741000.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.7772946865419238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1086346.723451495, 1086346.723451494, 238410.9091742611], 
processed observation next is [1.0, 0.30434782608695654, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.7316803452312335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30176297873652635, 0.30176297873652613, 0.35583717787203145], 
reward next is 0.6442, 
noisyNet noise sample is [array([0.81098384], dtype=float32), -1.7323995]. 
=============================================
[2019-03-27 02:32:44,506] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[56.553272]
 [57.392345]
 [58.08649 ]
 [58.941593]
 [59.6448  ]], R is [[57.23576355]
 [57.27074814]
 [57.3378067 ]
 [57.40830994]
 [57.48322296]].
[2019-03-27 02:32:46,248] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 02:32:46,249] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:32:46,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:32:46,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:32:46,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:32:46,253] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:32:46,251] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:32:46,255] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:32:46,258] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:32:46,260] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:32:46,257] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:32:46,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-27 02:32:46,282] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-27 02:32:46,325] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-27 02:32:46,354] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-27 02:32:46,354] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-27 02:34:02,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.111645706]
[2019-03-27 02:34:02,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.51666666666667, 84.66666666666666, 1.0, 2.0, 0.9259737756808779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1294267.201793007, 1294267.201793007, 277187.3852235004]
[2019-03-27 02:34:02,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:34:02,849] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6082520e-23 1.0000000e+00 6.2321052e-33 9.8927632e-26 1.8792817e-23], sampled 0.7753587747258718
[2019-03-27 02:34:07,397] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.111645706]
[2019-03-27 02:34:07,398] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.15243416, 80.66271835333333, 1.0, 2.0, 0.5109779325684054, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8873996858868084, 6.9112, 6.9112, 160.1138559829176, 1428567.944973163, 1428567.944973163, 312839.7546469936]
[2019-03-27 02:34:07,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:34:07,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9046626e-14 9.9999976e-01 8.5026388e-23 8.3074173e-12 2.3962551e-07], sampled 0.682406012526414
[2019-03-27 02:34:21,098] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.111645706]
[2019-03-27 02:34:21,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.62922882, 54.10069752, 1.0, 2.0, 0.8774819599011908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1226449.280461338, 1226449.280461338, 263815.0789610773]
[2019-03-27 02:34:21,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:34:21,107] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9341158e-27 1.0000000e+00 8.5251804e-38 5.8813180e-32 1.6135858e-29], sampled 0.7562184544303959
[2019-03-27 02:34:39,650] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.111645706]
[2019-03-27 02:34:39,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.08333333333334, 61.5, 1.0, 2.0, 0.3088884193182381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494597.3073083721, 494597.3073083721, 166706.1155368728]
[2019-03-27 02:34:39,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:34:39,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.745684e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.5781711633943999
[2019-03-27 02:34:39,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7949.8535 3161060395.0996 1742.0000
[2019-03-27 02:34:39,833] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8071.8449 3003868904.7864 1571.0000
[2019-03-27 02:34:40,345] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.1183 2779238904.8038 922.0000
[2019-03-27 02:34:40,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8277.7673 2926370620.7329 1282.0000
[2019-03-27 02:34:40,573] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8519.1277 2841007871.5147 1098.0000
[2019-03-27 02:34:41,591] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1375000, evaluation results [1375000.0, 7949.853527037266, 3161060395.099615, 1742.0, 8277.767319316561, 2926370620.7328525, 1282.0, 8665.118318246641, 2779238904.803791, 922.0, 8071.844908254263, 3003868904.7864013, 1571.0, 8519.127704028338, 2841007871.5147476, 1098.0]
[2019-03-27 02:34:42,717] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.847944e-32 1.000000e+00 0.000000e+00 0.000000e+00 3.047190e-37], sum to 1.0000
[2019-03-27 02:34:42,727] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0201
[2019-03-27 02:34:42,733] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.63709169833886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890316.9497324243, 890316.9497324243, 207857.816123641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3942600.0000, 
sim time next is 3943200.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.597917216978418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835550.227966414, 835550.2279664146, 200359.1218583597], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5155629120221904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2320972855462261, 0.2320972855462263, 0.29904346546023836], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.1021527], dtype=float32), 0.16300495]. 
=============================================
[2019-03-27 02:34:47,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5786359e-13 6.5880972e-09 1.1093094e-21 3.9832077e-08 1.0000000e+00], sum to 1.0000
[2019-03-27 02:34:47,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0613
[2019-03-27 02:34:47,186] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.66666666666667, 66.0, 1.0, 2.0, 0.7259541748852942, 1.0, 2.0, 0.6835671269569098, 1.0, 2.0, 1.03, 7.005099778707764, 6.9112, 170.5573041426782, 2868302.265430878, 2801038.034343719, 529654.1771830407], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4112400.0000, 
sim time next is 4113000.0000, 
raw observation next is [35.0, 65.5, 1.0, 2.0, 0.7257896945160153, 1.0, 2.0, 0.6834848867722703, 1.0, 2.0, 1.03, 7.005099765737511, 6.9112, 170.5573041426782, 2867956.782899982, 2800692.561103943, 529600.8746301879], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.655, 1.0, 1.0, 0.6696261379711027, 1.0, 1.0, 0.6186564900870726, 1.0, 1.0, 1.0365853658536586, 0.009389976573751114, 0.0, 0.8375144448122397, 0.7966546619166616, 0.7779701558622063, 0.7904490666122207], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6880894], dtype=float32), -1.3113095]. 
=============================================
[2019-03-27 02:34:47,201] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[50.796165]
 [50.497437]
 [49.907837]
 [49.41535 ]
 [48.728493]], R is [[50.63705444]
 [50.1306839 ]
 [49.62937927]
 [49.13308716]
 [48.64175797]].
[2019-03-27 02:34:48,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0091493e-31 1.0000000e+00 0.0000000e+00 2.3906438e-38 1.7975895e-34], sum to 1.0000
[2019-03-27 02:34:48,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1240
[2019-03-27 02:34:48,046] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 71.0, 1.0, 2.0, 0.6219712885236781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869177.9746105764, 869177.9746105757, 204912.5457718897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4125600.0000, 
sim time next is 4126200.0000, 
raw observation next is [32.66666666666666, 72.33333333333334, 1.0, 2.0, 0.6269026577243836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876072.1919675248, 876072.1919675248, 205866.4112757135], 
processed observation next is [1.0, 0.782608695652174, 0.7472353870458132, 0.7233333333333334, 1.0, 1.0, 0.5504851297884139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24335338665764578, 0.24335338665764578, 0.3072633004115127], 
reward next is 0.6927, 
noisyNet noise sample is [array([-0.43573052], dtype=float32), -0.0959055]. 
=============================================
[2019-03-27 02:34:51,985] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:34:51,993] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1751
[2019-03-27 02:34:51,999] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.5, 61.5, 1.0, 2.0, 0.5890273339509561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823122.3837973902, 823122.3837973902, 198720.0677096287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3929400.0000, 
sim time next is 3930000.0000, 
raw observation next is [33.66666666666666, 61.0, 1.0, 2.0, 0.5900944955854187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824614.242596752, 824614.242596752, 198915.6553696985], 
processed observation next is [0.0, 0.4782608695652174, 0.7946287519747232, 0.61, 1.0, 1.0, 0.5061379464884562, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22905951183243112, 0.22905951183243112, 0.29688903786522164], 
reward next is 0.7031, 
noisyNet noise sample is [array([-0.92839503], dtype=float32), 1.011491]. 
=============================================
[2019-03-27 02:34:52,016] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.46543]
 [72.46204]
 [72.40368]
 [72.52471]
 [72.45611]], R is [[72.45366669]
 [72.43253326]
 [72.41130066]
 [72.38502502]
 [72.36555481]].
[2019-03-27 02:34:57,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5222289e-12 2.3800812e-06 4.1338595e-18 7.0568558e-07 9.9999690e-01], sum to 1.0000
[2019-03-27 02:34:57,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0470
[2019-03-27 02:34:57,388] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.6061944752651448, 1.0, 2.0, 0.6061944752651448, 1.0, 2.0, 1.03, 6.936785201182707, 6.9112, 170.5573041426782, 2543309.970129253, 2524982.249782637, 490236.0449049943], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4020600.0000, 
sim time next is 4021200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.6050652308811718, 1.0, 2.0, 0.6050652308811718, 1.0, 2.0, 1.03, 6.93458053356473, 6.9112, 170.5573041426782, 2538567.373211158, 2521818.945959444, 489823.0077536169], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5241749769652673, 1.0, 1.0, 0.5241749769652673, 1.0, 1.0, 1.0365853658536586, 0.002338053356472969, 0.0, 0.8375144448122397, 0.7051576036697661, 0.7005052627665123, 0.7310791160501745], 
reward next is 0.1520, 
noisyNet noise sample is [array([-0.6751441], dtype=float32), 0.8516343]. 
=============================================
[2019-03-27 02:35:04,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.700579e-33 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:35:04,418] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6562
[2019-03-27 02:35:04,426] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5814238993722727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812493.0792092942, 812493.0792092936, 197336.242391479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4139400.0000, 
sim time next is 4140000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5826754100228536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814242.634907639, 814242.634907639, 197562.6636269549], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49719928918416095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2261785096965664, 0.2261785096965664, 0.2948696472044103], 
reward next is 0.7051, 
noisyNet noise sample is [array([-1.090825], dtype=float32), 1.4752436]. 
=============================================
[2019-03-27 02:35:04,442] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.2146 ]
 [67.25451]
 [67.27984]
 [67.66084]
 [67.42538]], R is [[67.01411438]
 [67.04943848]
 [67.08468628]
 [67.11972046]
 [67.15446472]].
[2019-03-27 02:35:05,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9440795e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:35:05,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6597
[2019-03-27 02:35:05,655] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4910199902297619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686120.0951298968, 686120.0951298961, 182200.797383525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4664400.0000, 
sim time next is 4665000.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.491204384838853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686377.8396673821, 686377.8396673815, 182229.1919985354], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.386993234745606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19066051101871725, 0.1906605110187171, 0.27198386865453045], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.5643263], dtype=float32), 0.54436046]. 
=============================================
[2019-03-27 02:35:05,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.98122 ]
 [70.01298 ]
 [70.05264 ]
 [70.08294 ]
 [70.133705]], R is [[70.0067215 ]
 [70.03471375]
 [70.0623703 ]
 [70.08963776]
 [70.11653137]].
[2019-03-27 02:35:08,658] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.5430446e-32 1.0000000e+00 0.0000000e+00 2.5411600e-38 5.7941712e-35], sum to 1.0000
[2019-03-27 02:35:08,668] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3041
[2019-03-27 02:35:08,671] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.66666666666667, 52.0, 1.0, 2.0, 0.5574657673781949, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564928207, 779001.2419859071, 779001.2419859064, 193092.1862963553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4209600.0000, 
sim time next is 4210200.0000, 
raw observation next is [35.5, 53.0, 1.0, 2.0, 0.552596270508363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104266, 772194.1460560119, 772194.1460560126, 192250.3639891683], 
processed observation next is [1.0, 0.7391304347826086, 0.8815165876777251, 0.53, 1.0, 1.0, 0.46095936205826865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439945152281, 0.21449837390444776, 0.21449837390444795, 0.28694084177487805], 
reward next is 0.7131, 
noisyNet noise sample is [array([-0.83594173], dtype=float32), 0.0991649]. 
=============================================
[2019-03-27 02:35:11,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7619658e-13 2.3578645e-10 1.5594867e-19 1.7989098e-08 1.0000000e+00], sum to 1.0000
[2019-03-27 02:35:11,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0532
[2019-03-27 02:35:11,555] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.0, 1.0, 2.0, 0.9722520692358061, 1.0, 2.0, 0.8067160741321655, 1.0, 2.0, 1.03, 7.005119208804803, 6.9112, 170.5573041426782, 3385745.530764027, 3318467.381107992, 621127.3936475036], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4287600.0000, 
sim time next is 4288200.0000, 
raw observation next is [38.0, 50.0, 1.0, 2.0, 0.8889057652913162, 1.0, 2.0, 0.7650429221599205, 1.0, 2.0, 1.03, 7.005112631929873, 6.9112, 170.5573041426782, 3210620.627215636, 3143347.188842644, 587638.5156567066], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.5, 1.0, 1.0, 0.8661515244473689, 1.0, 1.0, 0.7169191833252054, 1.0, 1.0, 1.0365853658536586, 0.009391263192987331, 0.0, 0.8375144448122397, 0.8918390631154545, 0.8731519969007344, 0.8770724114279204], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1401783], dtype=float32), -1.8496559]. 
=============================================
[2019-03-27 02:35:13,448] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.5618532e-13 5.5862765e-06 3.8159566e-19 6.5697370e-10 9.9999440e-01], sum to 1.0000
[2019-03-27 02:35:13,455] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0245
[2019-03-27 02:35:13,459] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.16666666666666, 1.0, 2.0, 0.9781044922728563, 1.0, 2.0, 0.8096422856506906, 1.0, 2.0, 1.03, 7.00511967069005, 6.9112, 170.5573041426782, 3398043.399922578, 3330764.919399354, 623576.5459053679], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4287000.0000, 
sim time next is 4287600.0000, 
raw observation next is [38.0, 51.0, 1.0, 2.0, 0.9722520692358061, 1.0, 2.0, 0.8067160741321655, 1.0, 2.0, 1.03, 7.005119208804803, 6.9112, 170.5573041426782, 3385745.530764027, 3318467.381107992, 621127.3936474669], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.51, 1.0, 1.0, 0.966568758115429, 1.0, 1.0, 0.7671278001592355, 1.0, 1.0, 1.0365853658536586, 0.00939192088048033, 0.0, 0.8375144448122397, 0.9404848696566742, 0.92179649475222, 0.9270558114141297], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28534815], dtype=float32), 0.6350807]. 
=============================================
[2019-03-27 02:35:18,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3150490e-13 1.7191453e-06 8.8543313e-22 1.2499023e-10 9.9999833e-01], sum to 1.0000
[2019-03-27 02:35:18,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5948
[2019-03-27 02:35:18,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666666, 68.0, 1.0, 2.0, 0.8616058489835706, 1.0, 2.0, 0.751392964006048, 1.0, 2.0, 1.03, 7.00511047808831, 6.9112, 170.5573041426782, 3153264.152106494, 3085992.256617826, 577233.5052407497], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [32.83333333333334, 71.5, 1.0, 2.0, 0.8281773651325262, 1.0, 2.0, 0.7346787220805256, 1.0, 2.0, 1.03, 7.005107840999369, 6.9112, 170.5573041426782, 3083035.289709883, 3015765.283275294, 564871.5385371628], 
processed observation next is [1.0, 0.5652173913043478, 0.7551342812006324, 0.715, 1.0, 1.0, 0.7929847772681038, 1.0, 1.0, 0.6803358097355731, 1.0, 1.0, 1.0365853658536586, 0.009390784099936855, 0.0, 0.8375144448122397, 0.8563986915860785, 0.8377125786875818, 0.8430918485629296], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.15274572], dtype=float32), -0.18912171]. 
=============================================
[2019-03-27 02:35:18,513] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3144672e-27 1.0000000e+00 4.2406685e-38 2.3756123e-34 4.9789993e-29], sum to 1.0000
[2019-03-27 02:35:18,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0963
[2019-03-27 02:35:18,524] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5161334735872127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721223.9918837352, 721223.9918837352, 186162.8632412496], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4735800.0000, 
sim time next is 4736400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5168451136438941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722218.7469001882, 722218.7469001882, 186277.7166453443], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.41788567908902896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2006163185833856, 0.2006163185833856, 0.2780264427542452], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.74274623], dtype=float32), -0.030046022]. 
=============================================
[2019-03-27 02:35:19,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2665461e-32 1.0000000e+00 0.0000000e+00 1.4702620e-38 6.6451505e-36], sum to 1.0000
[2019-03-27 02:35:19,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6172
[2019-03-27 02:35:19,280] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6170007420431912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862229.0314055202, 862229.0314055202, 203956.2245229629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4402800.0000, 
sim time next is 4403400.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6180674161259035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863720.2635273634, 863720.263527364, 204160.4142611974], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5398402603926549, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23992229542426763, 0.23992229542426777, 0.3047170362107424], 
reward next is 0.6953, 
noisyNet noise sample is [array([-1.6091597], dtype=float32), 0.4929605]. 
=============================================
[2019-03-27 02:35:19,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:35:19,616] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9968
[2019-03-27 02:35:19,621] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5856808067897477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818444.0564191804, 818444.0564191804, 198108.2894874195], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4421400.0000, 
sim time next is 4422000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5853686592339303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818007.6861553146, 818007.6861553151, 198051.4949600791], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5004441677517233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22722435726536516, 0.22722435726536533, 0.29559924620907324], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.6424467], dtype=float32), 0.41196957]. 
=============================================
[2019-03-27 02:35:19,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.20287 ]
 [75.334274]
 [75.541695]
 [75.54043 ]
 [75.474594]], R is [[74.70301056]
 [74.66029358]
 [74.61784363]
 [74.57461548]
 [74.53063202]].
[2019-03-27 02:35:29,148] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1261244e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:35:29,157] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7624
[2019-03-27 02:35:29,162] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.5315886663888735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742828.0030241485, 742828.0030241485, 188693.4436522572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4570200.0000, 
sim time next is 4570800.0000, 
raw observation next is [28.0, 82.33333333333334, 1.0, 2.0, 0.5351957552558767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747870.2284100746, 747870.2284100752, 189294.3949329485], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.8233333333333335, 1.0, 1.0, 0.4399948858504538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20774173011390962, 0.20774173011390978, 0.28252894766111714], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.30317923], dtype=float32), 0.11342453]. 
=============================================
[2019-03-27 02:35:35,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.8999710e-23 1.0000000e+00 2.0956320e-33 4.7489696e-27 8.1833770e-21], sum to 1.0000
[2019-03-27 02:35:35,997] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5059
[2019-03-27 02:35:36,001] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5023556926498313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701965.1180817671, 701965.1180817671, 183965.8295202671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5034974661537365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703561.099982467, 703561.0999824664, 184145.8698682397], 
processed observation next is [1.0, 0.0, 0.44707740916271754, 0.8733333333333334, 1.0, 1.0, 0.4018041760888391, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19543363888401863, 0.19543363888401846, 0.27484458189289507], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.17640321], dtype=float32), 0.49825457]. 
=============================================
[2019-03-27 02:35:36,022] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[56.48973 ]
 [58.047016]
 [60.202007]
 [63.64565 ]
 [68.51555 ]], R is [[55.62193298]
 [55.79114151]
 [55.95910263]
 [56.12632751]
 [56.29282379]].
[2019-03-27 02:35:36,066] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7306342e-16 3.9061039e-09 3.6577233e-22 5.3178633e-12 1.0000000e+00], sum to 1.0000
[2019-03-27 02:35:36,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4290
[2019-03-27 02:35:36,079] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 65.0, 1.0, 2.0, 0.5541630145441088, 1.0, 2.0, 0.5541630145441088, 1.0, 2.0, 0.9623978917537613, 6.911200000000001, 6.9112, 170.5573041426782, 2324807.100113358, 2324807.100113357, 454621.0360776267], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4892400.0000, 
sim time next is 4893000.0000, 
raw observation next is [31.41666666666666, 65.16666666666667, 1.0, 2.0, 0.641495258217049, 1.0, 2.0, 0.641337668622787, 1.0, 2.0, 1.03, 7.005093119565008, 6.9112, 170.5573041426782, 2690913.238610848, 2623653.777738521, 503484.2512596037], 
processed observation next is [1.0, 0.6521739130434783, 0.6879936808846759, 0.6516666666666667, 1.0, 1.0, 0.5680665761651192, 1.0, 1.0, 0.5678767091840806, 1.0, 1.0, 1.0365853658536586, 0.009389311956500812, 0.0, 0.8375144448122397, 0.7474758996141244, 0.7287927160384781, 0.7514690317307519], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1425858], dtype=float32), 1.1732126]. 
=============================================
[2019-03-27 02:35:36,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.897755]
 [62.469315]
 [62.17778 ]
 [61.93072 ]
 [61.69669 ]], R is [[62.35512924]
 [62.05303955]
 [61.75877762]
 [61.47667694]
 [61.22482681]].
[2019-03-27 02:35:37,644] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 02:35:37,647] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:35:37,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:35:37,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:35:37,648] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:35:37,649] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:35:37,650] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:35:37,652] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:35:37,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:35:37,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:35:37,650] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:35:37,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-27 02:35:37,683] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-27 02:35:37,721] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-27 02:35:37,722] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-27 02:35:37,765] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-27 02:36:33,870] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113239]
[2019-03-27 02:36:33,872] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.78333333333333, 80.66666666666667, 1.0, 2.0, 0.5687581582467229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794787.1039514483, 794787.1039514483, 195068.9332086187]
[2019-03-27 02:36:33,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:36:33,879] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1610839e-28 1.0000000e+00 0.0000000e+00 1.4144842e-35 1.4342992e-29], sampled 0.6230696230505087
[2019-03-27 02:36:56,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113239]
[2019-03-27 02:36:56,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.4, 53.0, 1.0, 2.0, 0.9723411191365198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1359118.007646649, 1359118.007646649, 290616.1650388284]
[2019-03-27 02:36:56,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:36:56,660] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.8190833e-20 1.0000000e+00 1.1411910e-30 5.8436816e-24 6.7217769e-17], sampled 0.15706626203822027
[2019-03-27 02:37:16,597] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113239]
[2019-03-27 02:37:16,600] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.35, 59.5, 1.0, 2.0, 0.4890405098527899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683353.2055207441, 683353.2055207447, 181896.0843139511]
[2019-03-27 02:37:16,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:37:16,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2835497e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3655203e-34], sampled 0.6214322594370747
[2019-03-27 02:37:21,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([9.2245773e-07], dtype=float32), -0.113239]
[2019-03-27 02:37:21,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 83.83333333333333, 1.0, 2.0, 0.4762583401505951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 665486.6434327345, 665486.643432734, 179959.6477878545]
[2019-03-27 02:37:21,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:37:21,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1823395e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2101667e-33], sampled 0.6436343839043139
[2019-03-27 02:37:31,844] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8239.8968 2995915049.5350 1152.0000
[2019-03-27 02:37:32,276] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8358.8680 2924577881.3575 1115.0000
[2019-03-27 02:37:32,298] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8588.6973 2837578794.0138 914.0000
[2019-03-27 02:37:32,319] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8076.8491 3154714399.6438 1390.0000
[2019-03-27 02:37:32,338] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8703.3538 2778748168.3183 836.0000
[2019-03-27 02:37:33,353] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1400000, evaluation results [1400000.0, 8076.849139023897, 3154714399.6438284, 1390.0, 8358.86796393949, 2924577881.357517, 1115.0, 8703.353805804772, 2778748168.318335, 836.0, 8239.896755337903, 2995915049.5350456, 1152.0, 8588.69728066117, 2837578794.0138254, 914.0]
[2019-03-27 02:37:34,942] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4673792e-18 2.9033953e-13 1.3184508e-25 1.8097992e-15 1.0000000e+00], sum to 1.0000
[2019-03-27 02:37:34,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3507
[2019-03-27 02:37:34,961] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.9833083767825556, 6.9112, 6.9112, 170.5573041426782, 2375367.316592513, 2375367.316592513, 463945.6516273014], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726800.0000, 
sim time next is 4727400.0000, 
raw observation next is [30.83333333333334, 70.0, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.4449123142853757, 6.9112, 6.9112, 170.5573041426782, 1074129.641498495, 1074129.641498495, 287578.1815063162], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.32306379790899475, 0.0, 0.0, 0.8375144448122397, 0.29836934486069305, 0.29836934486069305, 0.4292211664273376], 
reward next is 0.5708, 
noisyNet noise sample is [array([0.1767905], dtype=float32), -0.38470325]. 
=============================================
[2019-03-27 02:37:41,333] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2000979e-16 1.0000000e+00 5.0567834e-24 3.7680426e-18 1.9292676e-08], sum to 1.0000
[2019-03-27 02:37:41,336] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1250
[2019-03-27 02:37:41,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1959221.881469233 W.
[2019-03-27 02:37:41,343] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.4670947950546355, 1.0, 2.0, 0.4670947950546355, 1.0, 2.0, 0.8020780029922162, 6.9112, 6.9112, 170.5573041426782, 1959221.881469233, 1959221.881469233, 391606.5158789285], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4985400.0000, 
sim time next is 4986000.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.793106164918104, 1.0, 2.0, 0.793106164918104, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2218047.895903542, 2218047.895903542, 416523.1363848734], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.63, 1.0, 1.0, 0.7507303191784386, 1.0, 1.0, 0.7507303191784386, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6161244155287617, 0.6161244155287617, 0.6216763229624976], 
reward next is 0.3783, 
noisyNet noise sample is [array([-0.69769204], dtype=float32), 0.20352577]. 
=============================================
[2019-03-27 02:37:41,378] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.14862 ]
 [61.320473]
 [60.780277]
 [60.195545]
 [59.278446]], R is [[61.58514786]
 [61.38480759]
 [61.16742325]
 [60.8922348 ]
 [60.63546371]].
[2019-03-27 02:37:50,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.49633414e-24 1.00000000e+00 1.04012244e-35 5.78062795e-31
 2.02674813e-22], sum to 1.0000
[2019-03-27 02:37:50,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8964
[2019-03-27 02:37:50,730] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.41666666666666, 78.16666666666667, 1.0, 2.0, 0.630550140738688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 881171.5227346177, 881171.5227346184, 206576.2042180133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5343000.0000, 
sim time next is 5343600.0000, 
raw observation next is [31.33333333333334, 78.33333333333334, 1.0, 2.0, 0.6295074414243633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879713.7838530749, 879713.7838530749, 206372.6021989454], 
processed observation next is [1.0, 0.8695652173913043, 0.6840442338072673, 0.7833333333333334, 1.0, 1.0, 0.5536234234028473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24436493995918745, 0.24436493995918745, 0.3080188092521573], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.72167295], dtype=float32), 0.7367642]. 
=============================================
[2019-03-27 02:37:52,971] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6694305e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:37:52,982] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0308
[2019-03-27 02:37:52,991] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 81.5, 1.0, 2.0, 0.5199351973651816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726538.1833107037, 726538.1833107043, 186778.0769745961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5088600.0000, 
sim time next is 5089200.0000, 
raw observation next is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5188716237815977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725051.4777706177, 725051.4777706177, 186605.2996532201], 
processed observation next is [0.0, 0.9130434782608695, 0.4944707740916275, 0.8233333333333335, 1.0, 1.0, 0.42032725756818995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20140318826961603, 0.20140318826961603, 0.2785153726167464], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.480108], dtype=float32), 0.8223408]. 
=============================================
[2019-03-27 02:38:01,207] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5444305e-28 1.0000000e+00 1.4937114e-38 1.1281763e-36 8.7378357e-32], sum to 1.0000
[2019-03-27 02:38:01,217] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3960
[2019-03-27 02:38:01,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666667, 80.0, 1.0, 2.0, 0.5794609501533077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 809748.9701397619, 809748.9701397613, 196981.8790276434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5517600.0000, 
sim time next is 5518200.0000, 
raw observation next is [29.43333333333333, 80.5, 1.0, 2.0, 0.5779775225271246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807675.214012238, 807675.214012238, 196714.8250113312], 
processed observation next is [1.0, 0.8695652173913043, 0.5939968404423379, 0.805, 1.0, 1.0, 0.49153918376762, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22435422611451056, 0.22435422611451056, 0.2936042164348227], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.16082819], dtype=float32), -0.8377141]. 
=============================================
[2019-03-27 02:38:05,740] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6591601e-30 1.0000000e+00 0.0000000e+00 1.6498289e-37 1.3002393e-34], sum to 1.0000
[2019-03-27 02:38:05,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4815
[2019-03-27 02:38:05,754] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 80.0, 1.0, 2.0, 0.5469442937905147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764293.2720285837, 764293.2720285837, 191277.961218172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5259600.0000, 
sim time next is 5260200.0000, 
raw observation next is [28.58333333333334, 80.16666666666667, 1.0, 2.0, 0.5460183157576335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762998.8569082768, 762998.8569082768, 191120.2139459411], 
processed observation next is [1.0, 0.9130434782608695, 0.5537124802527649, 0.8016666666666667, 1.0, 1.0, 0.45303411537064275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21194412691896578, 0.21194412691896578, 0.28525405066558374], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.82044196], dtype=float32), -0.32081333]. 
=============================================
[2019-03-27 02:38:13,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0850537e-30 1.0000000e+00 0.0000000e+00 2.1906098e-37 9.5589296e-32], sum to 1.0000
[2019-03-27 02:38:13,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9858
[2019-03-27 02:38:13,713] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.11666666666667, 66.66666666666667, 1.0, 2.0, 0.5294437350616796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739829.6886895798, 739829.6886895798, 188339.5782875431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5595000.0000, 
sim time next is 5595600.0000, 
raw observation next is [30.83333333333333, 68.33333333333334, 1.0, 2.0, 0.5362148964518305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749294.8551498201, 749294.8551498195, 189465.9900256328], 
processed observation next is [1.0, 0.782608695652174, 0.6603475513428118, 0.6833333333333335, 1.0, 1.0, 0.44122276680943434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2081374597638389, 0.20813745976383877, 0.2827850597397505], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.1506801], dtype=float32), -0.078298815]. 
=============================================
[2019-03-27 02:38:29,380] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 02:38:29,382] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:38:29,383] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:38:29,384] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:38:29,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:38:29,388] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:38:29,388] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:38:29,388] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:38:29,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:38:29,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:38:29,393] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:38:29,420] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-27 02:38:29,442] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-27 02:38:29,464] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-27 02:38:29,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-27 02:38:29,503] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-27 02:39:05,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11577311]
[2019-03-27 02:39:05,029] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.24798191, 67.16666310666668, 1.0, 2.0, 0.6677197180690161, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.999851454665725, 6.9112, 168.9123514312293, 1829958.21764558, 1767066.049850956, 378041.4172516554]
[2019-03-27 02:39:05,030] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:39:05,034] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6858986e-14 9.9999487e-01 2.2856162e-23 8.5821429e-15 5.0846970e-06], sampled 0.8482738694811751
[2019-03-27 02:39:05,034] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1829958.21764558 W.
[2019-03-27 02:39:21,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11577311]
[2019-03-27 02:39:21,104] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.5, 84.0, 1.0, 2.0, 0.7802758105508892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090515.280487867, 1090515.280487866, 239123.9517923286]
[2019-03-27 02:39:21,105] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:39:21,110] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5586097e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7333208e-35], sampled 0.68101166136351
[2019-03-27 02:39:41,168] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11577311]
[2019-03-27 02:39:41,171] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.96666666666667, 46.0, 1.0, 2.0, 0.5413117220095868, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103833, 756419.5825448271, 756419.5825448264, 190322.923028647]
[2019-03-27 02:39:41,171] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:39:41,173] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8674360e-26 1.0000000e+00 4.2337365e-37 9.5855128e-32 1.2478135e-24], sampled 0.6130449868299468
[2019-03-27 02:40:23,238] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7894.9919 3163486088.9644 1825.0000
[2019-03-27 02:40:23,383] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8500.6960 2841860848.7137 1130.0000
[2019-03-27 02:40:23,578] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8266.2719 2926881484.7155 1309.0000
[2019-03-27 02:40:23,610] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3263 2779208151.5814 930.0000
[2019-03-27 02:40:23,629] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8013.8241 3007033839.1591 1726.0000
[2019-03-27 02:40:24,646] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1425000, evaluation results [1425000.0, 7894.991851488592, 3163486088.9644146, 1825.0, 8266.271866145145, 2926881484.715513, 1309.0, 8661.32632355176, 2779208151.5813584, 930.0, 8013.824079756419, 3007033839.1591415, 1726.0, 8500.696034914768, 2841860848.713694, 1130.0]
[2019-03-27 02:40:25,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1780365e-22 1.0000000e+00 2.1725648e-32 3.6888072e-27 1.6672864e-20], sum to 1.0000
[2019-03-27 02:40:25,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7768
[2019-03-27 02:40:25,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 91.0, 1.0, 2.0, 0.538071823698464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751890.6026538422, 751890.6026538422, 189776.2377056621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6139800.0000, 
sim time next is 6140400.0000, 
raw observation next is [26.76666666666667, 91.33333333333333, 1.0, 2.0, 0.5392792475745962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753578.4306708007, 753578.4306708013, 189979.2090079504], 
processed observation next is [1.0, 0.043478260869565216, 0.46761453396524505, 0.9133333333333333, 1.0, 1.0, 0.4449147561139713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2093273418530002, 0.20932734185300036, 0.2835510582208215], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.38334677], dtype=float32), 1.8354589]. 
=============================================
[2019-03-27 02:40:41,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9748553e-17 1.0000000e+00 2.2818100e-25 3.4985856e-20 3.2706663e-15], sum to 1.0000
[2019-03-27 02:40:41,471] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8449
[2019-03-27 02:40:41,481] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 81.0, 1.0, 2.0, 1.016429527902393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103974, 1420785.099200734, 1420785.099200735, 303974.48086177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5904000.0000, 
sim time next is 5904600.0000, 
raw observation next is [29.36666666666667, 80.5, 1.0, 2.0, 0.9795041303634713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1369136.770771229, 1369136.77077123, 292746.1710308382], 
processed observation next is [1.0, 0.34782608695652173, 0.5908372827804109, 0.805, 1.0, 1.0, 0.9753061811608088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3803157696586747, 0.38031576965867503, 0.43693458362811677], 
reward next is 0.5631, 
noisyNet noise sample is [array([-1.7646163], dtype=float32), -0.13722043]. 
=============================================
[2019-03-27 02:40:41,561] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2266150e-17 1.0000000e+00 4.9549675e-24 1.2534421e-18 2.2239143e-11], sum to 1.0000
[2019-03-27 02:40:41,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8545
[2019-03-27 02:40:41,579] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2065309.090446433 W.
[2019-03-27 02:40:41,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.13333333333333, 78.0, 1.0, 2.0, 0.7385437680514902, 1.0, 2.0, 0.7385437680514902, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2065309.090446433, 2065309.090446433, 391062.1850821017], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6172800.0000, 
sim time next is 6173400.0000, 
raw observation next is [29.21666666666667, 77.5, 1.0, 2.0, 0.8946933484949704, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.996336989685306, 6.9112, 168.9124499969606, 2147608.68593567, 2087209.756380604, 432952.7860284569], 
processed observation next is [1.0, 0.43478260869565216, 0.5837282780410744, 0.775, 1.0, 1.0, 0.8731245162590004, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008513698968530558, 0.0, 0.8294374579392801, 0.5965579683154639, 0.5797804878835011, 0.6461981881021744], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0580935], dtype=float32), -0.02261284]. 
=============================================
[2019-03-27 02:40:45,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0886068e-12 9.9733305e-01 8.2637230e-19 6.0363466e-13 2.6669595e-03], sum to 1.0000
[2019-03-27 02:40:45,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2442
[2019-03-27 02:40:45,280] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2682627.465905166 W.
[2019-03-27 02:40:45,285] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.3, 69.0, 1.0, 2.0, 0.6393650003990913, 1.0, 2.0, 0.6393650003990913, 1.0, 2.0, 1.03, 7.001549034003704, 6.9112, 170.5573041426782, 2682627.465905166, 2617906.777684586, 502693.536367173], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6003000.0000, 
sim time next is 6003600.0000, 
raw observation next is [32.53333333333333, 68.33333333333333, 1.0, 2.0, 0.9120866502422366, 1.0, 2.0, 0.9120866502422366, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2551135.455377101, 2551135.455377101, 478136.2972636286], 
processed observation next is [1.0, 0.4782608695652174, 0.7409162717219588, 0.6833333333333332, 1.0, 1.0, 0.8940803014966705, 1.0, 1.0, 0.8940803014966705, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7086487376047502, 0.7086487376047502, 0.71363626457258], 
reward next is 0.2864, 
noisyNet noise sample is [array([-1.031052], dtype=float32), 0.23165287]. 
=============================================
[2019-03-27 02:40:53,200] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.5231054e-12 9.9736625e-01 1.1613060e-17 2.2788516e-12 2.6337926e-03], sum to 1.0000
[2019-03-27 02:40:53,214] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2002
[2019-03-27 02:40:53,221] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1836320.949769333 W.
[2019-03-27 02:40:53,228] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.96666666666667, 73.0, 1.0, 2.0, 0.6567289176869482, 1.0, 1.0, 0.6567289176869482, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1836320.949769333, 1836320.949769333, 356152.5419079608], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6178800.0000, 
sim time next is 6179400.0000, 
raw observation next is [30.05, 72.5, 1.0, 2.0, 0.6184238275560708, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.950904093996643, 6.9112, 168.9126851400124, 1729140.846889528, 1700973.441351826, 368414.4332535015], 
processed observation next is [1.0, 0.5217391304347826, 0.6232227488151659, 0.725, 1.0, 1.0, 0.5402696717543022, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.003970409399664287, 0.0, 0.8294386125993134, 0.48031690191375775, 0.47249262259772945, 0.5498722884380619], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.95630157], dtype=float32), 0.59481496]. 
=============================================
[2019-03-27 02:40:53,254] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:40:53,263] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4401
[2019-03-27 02:40:53,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 62.33333333333334, 1.0, 2.0, 0.5063617427186132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707564.8236539173, 707564.8236539173, 184598.9760853695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6276000.0000, 
sim time next is 6276600.0000, 
raw observation next is [30.65, 62.5, 1.0, 2.0, 0.5083801552382119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710386.1964374154, 710386.1964374154, 184919.5407924289], 
processed observation next is [0.0, 0.6521739130434783, 0.6516587677725119, 0.625, 1.0, 1.0, 0.407686934021942, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19732949901039318, 0.19732949901039318, 0.2759993146155655], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.6792166], dtype=float32), -0.20230502]. 
=============================================
[2019-03-27 02:40:53,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5181572e-19 1.0000000e+00 1.4465492e-26 5.5208858e-22 1.1147973e-16], sum to 1.0000
[2019-03-27 02:40:53,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9658
[2019-03-27 02:40:53,622] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1800415.887093655 W.
[2019-03-27 02:40:53,629] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 92.0, 1.0, 2.0, 0.6438988624880327, 1.0, 1.0, 0.6438988624880327, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1800415.887093655, 1800415.887093656, 351032.0641389362], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6142800.0000, 
sim time next is 6143400.0000, 
raw observation next is [26.65, 92.0, 1.0, 2.0, 0.501212984407451, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8579971479779009, 6.911200000000001, 6.9112, 168.9129561858748, 1401197.7314545, 1401197.731454499, 306537.3999890592], 
processed observation next is [1.0, 0.08695652173913043, 0.462085308056872, 0.92, 1.0, 1.0, 0.399051788442712, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.8268257902169523, 8.881784197001253e-17, 0.0, 0.8294399435585832, 0.38922159207069446, 0.3892215920706942, 0.457518507446357], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7433768], dtype=float32), 0.9462166]. 
=============================================
[2019-03-27 02:40:55,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4680392e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:40:55,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 02:40:55,044] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333333, 75.66666666666667, 1.0, 2.0, 0.5180286488138364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723873.1352450701, 723873.1352450701, 186468.7692289273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6372600.0000, 
sim time next is 6373200.0000, 
raw observation next is [28.36666666666667, 76.33333333333334, 1.0, 2.0, 0.5198048218945126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726355.9391702501, 726355.9391702507, 186756.9989314036], 
processed observation next is [0.0, 0.782608695652174, 0.543443917851501, 0.7633333333333334, 1.0, 1.0, 0.42145159264399107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017655386584028, 0.20176553865840297, 0.2787417894498561], 
reward next is 0.7213, 
noisyNet noise sample is [array([1.8669072], dtype=float32), -1.0317339]. 
=============================================
[2019-03-27 02:40:56,961] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:40:56,969] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0068
[2019-03-27 02:40:56,977] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 89.16666666666667, 1.0, 2.0, 0.5286747282065754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738754.7266024591, 738754.7266024585, 188211.2551154973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6241800.0000, 
sim time next is 6242400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5298350651994064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740376.7129195761, 740376.7129195761, 188403.1556072866], 
processed observation next is [0.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4335362231318149, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20566019803321559, 0.20566019803321559, 0.2811987397123681], 
reward next is 0.7188, 
noisyNet noise sample is [array([-1.64733], dtype=float32), 0.13771488]. 
=============================================
[2019-03-27 02:40:58,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5853795e-15 1.0115163e-05 8.2102390e-23 6.6916768e-16 9.9998987e-01], sum to 1.0000
[2019-03-27 02:40:58,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2599
[2019-03-27 02:40:58,424] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.55, 69.5, 1.0, 2.0, 0.4861515318085155, 1.0, 2.0, 0.4861515318085155, 1.0, 2.0, 0.84404377603027, 6.9112, 6.9112, 170.5573041426782, 2039231.144286892, 2039231.144286892, 405659.0848871521], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6183000.0000, 
sim time next is 6183600.0000, 
raw observation next is [30.63333333333333, 69.0, 1.0, 2.0, 0.5210215467028573, 1.0, 2.0, 0.5210215467028573, 1.0, 2.0, 0.904842122164402, 6.911199999999999, 6.9112, 170.5573041426782, 2185647.557686732, 2185647.557686733, 429986.7947238454], 
processed observation next is [1.0, 0.5652173913043478, 0.6508688783570299, 0.69, 1.0, 1.0, 0.4229175261480208, 1.0, 1.0, 0.4229175261480208, 1.0, 1.0, 0.8839538075175633, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6071243215796478, 0.6071243215796481, 0.6417713354087244], 
reward next is 0.3582, 
noisyNet noise sample is [array([-0.5037106], dtype=float32), 0.6734458]. 
=============================================
[2019-03-27 02:40:58,866] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8861983e-15 4.2092950e-05 4.6765120e-23 4.1812672e-15 9.9995792e-01], sum to 1.0000
[2019-03-27 02:40:58,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6445
[2019-03-27 02:40:58,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.8, 72.33333333333333, 1.0, 2.0, 0.5273050736394174, 1.0, 2.0, 0.5273050736394174, 1.0, 2.0, 0.9135525074943883, 6.911200000000001, 6.9112, 170.5573041426782, 2212033.714756694, 2212033.714756694, 434120.3624294887], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6190800.0000, 
sim time next is 6191400.0000, 
raw observation next is [29.7, 72.66666666666667, 1.0, 2.0, 0.5155173322136273, 1.0, 2.0, 0.5155173322136273, 1.0, 2.0, 0.8923070127270656, 6.9112, 6.9112, 170.5573041426782, 2162534.469488753, 2162534.469488753, 425487.0017752475], 
processed observation next is [1.0, 0.6521739130434783, 0.6066350710900474, 0.7266666666666667, 1.0, 1.0, 0.41628594242605693, 1.0, 1.0, 0.41628594242605693, 1.0, 1.0, 0.8686670886915434, 0.0, 0.0, 0.8375144448122397, 0.6007040193024313, 0.6007040193024313, 0.6350552265302202], 
reward next is 0.3649, 
noisyNet noise sample is [array([0.02282688], dtype=float32), -1.006943]. 
=============================================
[2019-03-27 02:41:00,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3682911e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:41:00,752] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8951
[2019-03-27 02:41:00,761] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 90.66666666666667, 1.0, 2.0, 0.5251425489204847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733817.2568692582, 733817.2568692588, 187629.3871789379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6236400.0000, 
sim time next is 6237000.0000, 
raw observation next is [26.55, 90.5, 1.0, 2.0, 0.5249050761389132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733485.3054963945, 733485.3054963939, 187590.4168267733], 
processed observation next is [0.0, 0.17391304347826086, 0.4573459715639811, 0.905, 1.0, 1.0, 0.4275964772757991, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20374591819344293, 0.20374591819344276, 0.279985696756378], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.9914514], dtype=float32), 1.5580819]. 
=============================================
[2019-03-27 02:41:00,771] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.03081 ]
 [72.21075 ]
 [72.281105]
 [72.272484]
 [72.368126]], R is [[72.15914154]
 [72.15750885]
 [72.15589905]
 [72.15429688]
 [72.15270996]].
[2019-03-27 02:41:01,109] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.727864e-31 1.000000e+00 0.000000e+00 0.000000e+00 3.800300e-36], sum to 1.0000
[2019-03-27 02:41:01,119] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9110
[2019-03-27 02:41:01,124] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.5333533618985864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745294.80715311, 745294.80715311, 188986.7504798246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6311400.0000, 
sim time next is 6312000.0000, 
raw observation next is [27.3, 86.0, 1.0, 2.0, 0.5329319024052557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744705.6636257971, 744705.6636257964, 188916.6048362001], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 1.0, 1.0, 0.43726735229548874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2068626843404992, 0.206862684340499, 0.2819650818450748], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.60748494], dtype=float32), -0.2900249]. 
=============================================
[2019-03-27 02:41:01,137] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[70.70791 ]
 [70.52999 ]
 [70.11488 ]
 [69.84632 ]
 [69.812004]], R is [[70.83750153]
 [70.84706116]
 [70.85652924]
 [70.86619568]
 [70.87602234]].
[2019-03-27 02:41:09,627] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.1035410e-24 1.0000000e+00 9.1387480e-32 7.6872140e-30 1.3895703e-23], sum to 1.0000
[2019-03-27 02:41:09,634] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5165
[2019-03-27 02:41:09,639] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 83.66666666666667, 1.0, 2.0, 0.5911130904518817, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9974395616459774, 6.911199999999999, 6.9112, 168.9126680224911, 1652719.338074725, 1652719.338074726, 355538.461799066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6402000.0000, 
sim time next is 6402600.0000, 
raw observation next is [26.95, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.048894586348931, 6.9112, 168.9122731685106, 1551506.70830604, 1453821.824856618, 311348.5100121339], 
processed observation next is [1.0, 0.08695652173913043, 0.476303317535545, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.013769458634893095, 0.0, 0.8294365896306383, 0.43097408564056666, 0.403839395793505, 0.46469926867482675], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.49467], dtype=float32), -0.77081066]. 
=============================================
[2019-03-27 02:41:12,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6309093e-28 1.0000000e+00 6.1770604e-38 3.1499927e-37 7.1200880e-32], sum to 1.0000
[2019-03-27 02:41:12,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8538
[2019-03-27 02:41:12,109] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 88.33333333333334, 1.0, 2.0, 0.6823152980453945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 953543.9740699849, 953543.9740699856, 217072.957834213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6591000.0000, 
sim time next is 6591600.0000, 
raw observation next is [26.6, 88.0, 1.0, 2.0, 0.6854201674262496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957885.0238343929, 957885.0238343923, 217728.3920143394], 
processed observation next is [1.0, 0.30434782608695654, 0.4597156398104266, 0.88, 1.0, 1.0, 0.6209881535256019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2660791732873314, 0.2660791732873312, 0.32496774927513344], 
reward next is 0.6750, 
noisyNet noise sample is [array([1.6956507], dtype=float32), -0.24599262]. 
=============================================
[2019-03-27 02:41:13,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6471815e-22 1.0000000e+00 2.6391442e-31 6.4635677e-28 3.4997967e-22], sum to 1.0000
[2019-03-27 02:41:13,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6187
[2019-03-27 02:41:13,793] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 84.0, 1.0, 2.0, 1.030942770943637, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9126505971881, 1441085.766527247, 1441085.766527247, 308494.2063713447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6685200.0000, 
sim time next is 6685800.0000, 
raw observation next is [27.36666666666667, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.430659510912266, 6.9112, 168.9104082209386, 1822524.229758985, 1454007.333980737, 311349.2614706486], 
processed observation next is [1.0, 0.391304347826087, 0.49605055292259104, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.05194595109122657, 0.0, 0.829427431884283, 0.506256730488607, 0.4038909261057603, 0.4647003902546994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49978462], dtype=float32), -0.7986406]. 
=============================================
[2019-03-27 02:41:20,622] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 02:41:20,624] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:41:20,626] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:41:20,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:41:20,629] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:41:20,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:41:20,630] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:41:20,633] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:41:20,631] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:41:20,636] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:41:20,635] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:41:20,660] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-27 02:41:20,683] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-27 02:41:20,706] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-27 02:41:20,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-27 02:41:20,726] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-27 02:41:33,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1153012]
[2019-03-27 02:41:33,039] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.20229797, 90.06329745666667, 1.0, 2.0, 0.3193425070408107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507056.2613179737, 507056.2613179737, 167590.0537639621]
[2019-03-27 02:41:33,040] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:41:33,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.936895e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.1499276643562466
[2019-03-27 02:42:23,026] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1153012]
[2019-03-27 02:42:23,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 50.0, 1.0, 2.0, 0.9836026564267562, 1.0, 2.0, 0.9836026564267562, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2751388.264747042, 2751388.264747043, 519156.8804360755]
[2019-03-27 02:42:23,028] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:42:23,031] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5502931e-19 1.0000000e+00 1.2510043e-28 6.7770537e-25 1.1222954e-14], sampled 0.5730931865343832
[2019-03-27 02:42:23,035] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2751388.264747042 W.
[2019-03-27 02:42:25,002] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1153012]
[2019-03-27 02:42:25,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.23988298333333, 87.21202153333334, 1.0, 2.0, 0.9446242586218814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565078475, 1320351.869922064, 1320351.869922064, 282512.9295112622]
[2019-03-27 02:42:25,004] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:42:25,007] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.0298252e-26 1.0000000e+00 5.8091070e-36 1.2423748e-32 7.6628039e-28], sampled 0.4779321048949847
[2019-03-27 02:43:11,964] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 02:43:11,974] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 02:43:11,993] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.4076 2842465965.6767 1133.0000
[2019-03-27 02:43:12,018] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 02:43:12,041] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9561 3164180846.6292 1789.0000
[2019-03-27 02:43:13,056] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1450000, evaluation results [1450000.0, 7881.956122662321, 3164180846.6292114, 1789.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8496.407631351267, 2842465965.6766696, 1133.0]
[2019-03-27 02:43:21,218] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:43:21,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5881
[2019-03-27 02:43:21,233] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 56.33333333333334, 1.0, 2.0, 0.3965311551739365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591149.2877517611, 591149.2877517617, 173707.0568044419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6981600.0000, 
sim time next is 6982200.0000, 
raw observation next is [28.9, 56.0, 1.0, 2.0, 0.3939389622078006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589670.6571792588, 589670.6571792588, 173642.7863888592], 
processed observation next is [0.0, 0.8260869565217391, 0.5687203791469194, 0.56, 1.0, 1.0, 0.2698059785636152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16379740477201632, 0.16379740477201632, 0.2591683378938197], 
reward next is 0.7408, 
noisyNet noise sample is [array([1.3916621], dtype=float32), -1.3691895]. 
=============================================
[2019-03-27 02:43:22,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5509556e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:43:22,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0104
[2019-03-27 02:43:22,313] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 79.0, 1.0, 2.0, 0.3309207002968157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518690.2853947809, 518690.2853947815, 168356.120219637], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6742800.0000, 
sim time next is 6743400.0000, 
raw observation next is [23.15, 79.66666666666667, 1.0, 2.0, 0.3288233166778067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516118.5951951673, 516118.5951951667, 168172.7069545603], 
processed observation next is [1.0, 0.043478260869565216, 0.2962085308056872, 0.7966666666666667, 1.0, 1.0, 0.19135339358771888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14336627644310201, 0.14336627644310188, 0.251004040230687], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.0167655], dtype=float32), -1.0150292]. 
=============================================
[2019-03-27 02:43:23,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9923686e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:43:23,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2435
[2019-03-27 02:43:23,874] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 83.0, 1.0, 2.0, 0.320177754526889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505965.5815278277, 505965.5815278277, 167467.7537844985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6746400.0000, 
sim time next is 6747000.0000, 
raw observation next is [22.35, 83.16666666666667, 1.0, 2.0, 0.4970624656718193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786147.8112437947, 786147.8112437954, 193619.7682607958], 
processed observation next is [1.0, 0.08695652173913043, 0.25829383886255936, 0.8316666666666667, 1.0, 1.0, 0.3940511634600233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2183743920121652, 0.2183743920121654, 0.2889847287474564], 
reward next is 0.7110, 
noisyNet noise sample is [array([-1.6161431], dtype=float32), 0.007337788]. 
=============================================
[2019-03-27 02:43:23,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.28229]
 [73.91408]
 [74.00168]
 [73.95924]
 [73.84465]], R is [[74.21961212]
 [74.22746277]
 [74.23493195]
 [74.24211121]
 [74.24910736]].
[2019-03-27 02:43:31,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:43:31,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8893
[2019-03-27 02:43:31,653] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.81666666666667, 31.16666666666666, 1.0, 2.0, 0.2536829169178785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417426.2472565491, 417426.2472565497, 161267.5597546505], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6875400.0000, 
sim time next is 6876000.0000, 
raw observation next is [29.9, 30.0, 1.0, 2.0, 0.2544687688644385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419707.3587315264, 419707.3587315264, 161319.5096549368], 
processed observation next is [0.0, 0.6086956521739131, 0.6161137440758293, 0.3, 1.0, 1.0, 0.10176960104149213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.116585377425424, 0.116585377425424, 0.24077538754468178], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.95455456], dtype=float32), 1.3348525]. 
=============================================
[2019-03-27 02:43:31,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.51522 ]
 [80.49659 ]
 [80.45625 ]
 [80.430374]
 [80.38721 ]], R is [[80.57517242]
 [80.52872467]
 [80.48257446]
 [80.43597412]
 [80.38925934]].
[2019-03-27 02:43:35,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0435185e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3170838e-37], sum to 1.0000
[2019-03-27 02:43:35,868] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7303
[2019-03-27 02:43:35,872] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 76.0, 1.0, 2.0, 0.5917320944556285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846554.1892061256, 846554.1892061256, 201742.0874032477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7023000.0000, 
sim time next is 7023600.0000, 
raw observation next is [26.8, 75.0, 1.0, 2.0, 0.6364402611088918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910828.0310405577, 910828.031040557, 210518.0316832197], 
processed observation next is [1.0, 0.30434782608695654, 0.4691943127962086, 0.75, 1.0, 1.0, 0.5619762182034841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2530077864001549, 0.2530077864001547, 0.31420601743764137], 
reward next is 0.6858, 
noisyNet noise sample is [array([0.14653423], dtype=float32), -0.39119992]. 
=============================================
[2019-03-27 02:43:44,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1723864e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1611640e-35], sum to 1.0000
[2019-03-27 02:43:44,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5976
[2019-03-27 02:43:44,125] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 68.0, 1.0, 2.0, 0.390849191565308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584974.975979596, 584974.9759795954, 173212.8141027009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7323600.0000, 
sim time next is 7324200.0000, 
raw observation next is [26.6, 68.5, 1.0, 2.0, 0.3917519939357751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586588.1526171557, 586588.1526171563, 173367.3444991322], 
processed observation next is [1.0, 0.782608695652174, 0.4597156398104266, 0.685, 1.0, 1.0, 0.2671710770310543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16294115350476546, 0.16294115350476562, 0.2587572305957197], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.9611495], dtype=float32), 0.5681103]. 
=============================================
[2019-03-27 02:43:45,535] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.805296e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 02:43:45,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8500
[2019-03-27 02:43:45,547] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 87.33333333333333, 1.0, 2.0, 0.7933171210571252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1108751.36192079, 1108751.361920791, 242273.5931190593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7179000.0000, 
sim time next is 7179600.0000, 
raw observation next is [25.8, 87.66666666666667, 1.0, 2.0, 0.7570751665526095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1058073.852354415, 1058073.852354415, 233643.9035229445], 
processed observation next is [1.0, 0.08695652173913043, 0.42180094786729866, 0.8766666666666667, 1.0, 1.0, 0.7073194777742283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29390940343178196, 0.29390940343178196, 0.3487222440640963], 
reward next is 0.6513, 
noisyNet noise sample is [array([-1.5009129], dtype=float32), -0.64569604]. 
=============================================
[2019-03-27 02:43:47,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9067418e-18 1.0000000e+00 9.1651994e-27 6.7849015e-22 1.5507748e-12], sum to 1.0000
[2019-03-27 02:43:47,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8503
[2019-03-27 02:43:47,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1691087.871869041 W.
[2019-03-27 02:43:47,872] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.11666666666667, 85.66666666666667, 1.0, 2.0, 0.6048297333147713, 1.0, 2.0, 0.6048297333147713, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1691087.871869041, 1691087.871869041, 336032.7777433522], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7145400.0000, 
sim time next is 7146000.0000, 
raw observation next is [26.1, 86.0, 1.0, 2.0, 0.5352324899455325, 1.0, 2.0, 0.5352324899455325, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1496359.98091021, 1496359.980910211, 311552.1853176017], 
processed observation next is [1.0, 0.7391304347826086, 0.4360189573459717, 0.86, 1.0, 1.0, 0.4400391445126898, 1.0, 1.0, 0.4400391445126898, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.41565555025283607, 0.4156555502528364, 0.4650032616680623], 
reward next is 0.5350, 
noisyNet noise sample is [array([-0.43026537], dtype=float32), 1.6311535]. 
=============================================
[2019-03-27 02:43:47,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.52978 ]
 [68.37679 ]
 [67.41578 ]
 [67.83097 ]
 [66.487206]], R is [[69.38954163]
 [69.19410706]
 [68.50216675]
 [68.26782227]
 [67.58514404]].
[2019-03-27 02:43:49,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.7633314e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:43:49,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5093
[2019-03-27 02:43:49,084] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 83.0, 1.0, 2.0, 0.4736776459209657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 663084.7484409822, 663084.7484409829, 179730.2314350751], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7156800.0000, 
sim time next is 7157400.0000, 
raw observation next is [26.08333333333334, 83.16666666666667, 1.0, 2.0, 0.4741852149588767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663868.1781886118, 663868.1781886118, 179815.3748676226], 
processed observation next is [1.0, 0.8695652173913043, 0.43522906793049004, 0.8316666666666667, 1.0, 1.0, 0.36648821079382743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18440782727461438, 0.18440782727461438, 0.2683811565188397], 
reward next is 0.7316, 
noisyNet noise sample is [array([-1.3882006], dtype=float32), -0.65587115]. 
=============================================
[2019-03-27 02:43:53,382] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0249068e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:43:53,384] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4440964e-25 1.0000000e+00 1.0327685e-35 4.8298903e-33 1.0881049e-24], sum to 1.0000
[2019-03-27 02:43:53,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8074
[2019-03-27 02:43:53,394] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1113
[2019-03-27 02:43:53,398] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 93.0, 1.0, 2.0, 0.4081216371613072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599604.4694083417, 599604.469408341, 174223.9876948501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7518600.0000, 
sim time next is 7519200.0000, 
raw observation next is [23.53333333333333, 93.0, 1.0, 2.0, 0.4079176848427963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599679.8845819202, 599679.8845819196, 174242.5415096132], 
processed observation next is [0.0, 0.0, 0.3143759873617693, 0.93, 1.0, 1.0, 0.28664781306361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16657774571720008, 0.1665777457171999, 0.2600634947904675], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.53377604], dtype=float32), 1.0073406]. 
=============================================
[2019-03-27 02:43:53,400] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 60.33333333333333, 1.0, 2.0, 0.8102575673907269, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564516758, 1215834.407952453, 1215834.407952453, 257793.2495894495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7312200.0000, 
sim time next is 7312800.0000, 
raw observation next is [27.73333333333333, 60.66666666666667, 1.0, 2.0, 0.9443295410775713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104163, 1425410.460267707, 1425410.460267708, 297759.696276618], 
processed observation next is [1.0, 0.6521739130434783, 0.513428120063191, 0.6066666666666667, 1.0, 1.0, 0.9329271579247846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522305, 0.39594735007436305, 0.3959473500743633, 0.4444174571292806], 
reward next is 0.5556, 
noisyNet noise sample is [array([-1.0454667], dtype=float32), 0.47245234]. 
=============================================
[2019-03-27 02:44:02,027] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.3685597e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:44:02,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6224
[2019-03-27 02:44:02,042] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 89.66666666666667, 1.0, 2.0, 0.5466142941974074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 881804.8437253291, 881804.8437253286, 204049.5315351732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7402800.0000, 
sim time next is 7403400.0000, 
raw observation next is [20.5, 89.5, 1.0, 2.0, 0.5140196600526521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830045.3471967152, 830045.3471967159, 197907.1468202413], 
processed observation next is [1.0, 0.6956521739130435, 0.1706161137440759, 0.895, 1.0, 1.0, 0.41448151813572537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23056815199908756, 0.23056815199908776, 0.29538380122424074], 
reward next is 0.7046, 
noisyNet noise sample is [array([-0.3869854], dtype=float32), -0.20922372]. 
=============================================
[2019-03-27 02:44:02,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5400487e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:44:02,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9724
[2019-03-27 02:44:02,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.98333333333333, 92.0, 1.0, 2.0, 0.6234993951955499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991406.5545155546, 991406.5545155553, 219040.6241461265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7391400.0000, 
sim time next is 7392000.0000, 
raw observation next is [20.96666666666667, 92.0, 1.0, 2.0, 0.6419716570852705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021055.069585225, 1021055.069585225, 223176.3890289467], 
processed observation next is [1.0, 0.5652173913043478, 0.1927330173775673, 0.92, 1.0, 1.0, 0.5686405507051452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.283626408218118, 0.283626408218118, 0.3330990881029055], 
reward next is 0.6669, 
noisyNet noise sample is [array([0.9631063], dtype=float32), -0.09311325]. 
=============================================
[2019-03-27 02:44:02,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.062195]
 [74.52932 ]
 [74.63907 ]
 [74.58616 ]
 [74.50916 ]], R is [[73.77565002]
 [73.71096802]
 [73.6879425 ]
 [73.68222046]
 [73.67697144]].
[2019-03-27 02:44:03,202] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:44:03,211] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5084
[2019-03-27 02:44:03,218] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 88.66666666666667, 1.0, 2.0, 0.3071877475816929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489729.4030130697, 489729.4030130691, 166331.8605976935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7422000.0000, 
sim time next is 7422600.0000, 
raw observation next is [21.25, 89.33333333333333, 1.0, 2.0, 0.3078959167489407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490507.2633486398, 490507.2633486398, 166384.1532738123], 
processed observation next is [1.0, 0.9130434782608695, 0.20616113744075834, 0.8933333333333333, 1.0, 1.0, 0.16613965873366346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13625201759684438, 0.13625201759684438, 0.24833455712509298], 
reward next is 0.7517, 
noisyNet noise sample is [array([-1.1842972], dtype=float32), -0.8379512]. 
=============================================
[2019-03-27 02:44:06,714] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:44:06,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:06,782] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-27 02:44:08,730] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 02:44:08,732] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:44:08,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:08,733] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:44:08,733] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:44:08,734] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:08,735] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:08,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:44:08,737] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:44:08,738] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:08,738] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:44:08,766] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-27 02:44:08,767] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-27 02:44:08,809] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-27 02:44:08,825] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-27 02:44:08,852] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-27 02:44:18,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11885937]
[2019-03-27 02:44:18,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.94762266, 62.75625101333333, 1.0, 2.0, 0.3260462934304502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520388.6615599389, 520388.6615599389, 168632.1828045047]
[2019-03-27 02:44:18,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:44:18,102] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4603299593864333
[2019-03-27 02:45:10,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11885937]
[2019-03-27 02:45:10,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.71356591333334, 67.49166656166666, 1.0, 2.0, 0.9159773859950227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1280286.459945113, 1280286.459945114, 274376.036558382]
[2019-03-27 02:45:10,120] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:45:10,125] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.979208e-31 1.000000e+00 0.000000e+00 0.000000e+00 2.760493e-34], sampled 0.13672727310821486
[2019-03-27 02:45:36,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.11885937]
[2019-03-27 02:45:36,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.98794766666666, 82.697313135, 1.0, 2.0, 0.55373743010051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773789.3752891549, 773789.3752891549, 192442.6883184224]
[2019-03-27 02:45:36,211] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:45:36,213] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6656017e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7144171439238349
[2019-03-27 02:46:03,482] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.0407 3164046730.3090 1783.0000
[2019-03-27 02:46:03,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 02:46:03,661] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2841 3007675385.2347 1766.0000
[2019-03-27 02:46:03,737] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 02:46:03,746] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3314 2842435418.4093 1132.0000
[2019-03-27 02:46:04,765] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1475000, evaluation results [1475000.0, 7882.04072602001, 3164046730.3090086, 1783.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7998.284063504842, 3007675385.2346897, 1766.0, 8497.331445041236, 2842435418.4092717, 1132.0]
[2019-03-27 02:46:05,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.108108e-33 1.000000e+00 0.000000e+00 0.000000e+00 7.688238e-38], sum to 1.0000
[2019-03-27 02:46:05,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3185
[2019-03-27 02:46:05,192] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 88.0, 1.0, 2.0, 0.4802845801489442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671114.3775500917, 671114.3775500911, 180564.3190882282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7686000.0000, 
sim time next is 7686600.0000, 
raw observation next is [25.45, 88.33333333333334, 1.0, 2.0, 0.4791598225705687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669542.2286759431, 669542.2286759437, 180394.9043858152], 
processed observation next is [1.0, 1.0, 0.4052132701421801, 0.8833333333333334, 1.0, 1.0, 0.37248171394044427, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1859839524099842, 0.18598395240998436, 0.2692461259489779], 
reward next is 0.7308, 
noisyNet noise sample is [array([0.07357104], dtype=float32), 1.0506295]. 
=============================================
[2019-03-27 02:46:10,298] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:10,298] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:10,369] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-27 02:46:11,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.0219478e-17 1.0000000e+00 1.0903196e-24 4.1877719e-19 4.8751454e-12], sum to 1.0000
[2019-03-27 02:46:11,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8633
[2019-03-27 02:46:11,165] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 75.0, 1.0, 2.0, 0.5826045908107487, 1.0, 1.0, 0.5826045908107487, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1628899.737023019, 1628899.737023019, 327904.5281540327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7638000.0000, 
sim time next is 7638600.0000, 
raw observation next is [27.73333333333333, 73.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.705338924730809, 6.9112, 168.9086037850244, 2017515.806963858, 1454140.839615848, 311346.7364848946], 
processed observation next is [1.0, 0.391304347826087, 0.513428120063191, 0.735, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.07941389247308087, 0.0, 0.82941857127787, 0.5604210574899605, 0.4039280110044022, 0.46469662161924563], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.393234], dtype=float32), 0.6211629]. 
=============================================
[2019-03-27 02:46:15,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:15,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:15,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-27 02:46:17,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:17,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:17,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-27 02:46:20,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:20,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:20,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-27 02:46:22,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:22,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:22,571] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-27 02:46:25,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8633738e-25 1.0000000e+00 2.4423423e-34 4.6203008e-31 3.0256211e-23], sum to 1.0000
[2019-03-27 02:46:25,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2347
[2019-03-27 02:46:25,203] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 89.83333333333333, 1.0, 2.0, 0.6508600437936357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909566.0474464631, 909566.0474464631, 210594.4513989143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7877400.0000, 
sim time next is 7878000.0000, 
raw observation next is [26.23333333333333, 89.66666666666667, 1.0, 2.0, 0.6033241219465292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843109.0231053925, 843109.0231053925, 201360.2444601352], 
processed observation next is [1.0, 0.17391304347826086, 0.44233807266982617, 0.8966666666666667, 1.0, 1.0, 0.522077255357264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23419695086260903, 0.23419695086260903, 0.30053767829870925], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.0847084], dtype=float32), 0.073497444]. 
=============================================
[2019-03-27 02:46:25,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[59.89485]
 [59.46586]
 [59.85426]
 [59.95926]
 [60.32599]], R is [[59.83177185]
 [59.91913605]
 [59.97779846]
 [60.0304718 ]
 [60.07870865]].
[2019-03-27 02:46:25,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:25,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:25,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-27 02:46:25,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:25,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:26,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-27 02:46:28,049] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:28,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:28,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-27 02:46:28,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:28,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:28,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-27 02:46:28,362] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1526367e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1046900e-33], sum to 1.0000
[2019-03-27 02:46:28,365] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2889
[2019-03-27 02:46:28,370] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 82.0, 1.0, 2.0, 0.542318566617543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 854020.5062737424, 854020.5062737431, 201616.6798820528], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 30000.0000, 
sim time next is 30600.0000, 
raw observation next is [22.95, 81.0, 1.0, 2.0, 0.6087727765698281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 956732.6883268057, 956732.6883268063, 214858.5586430603], 
processed observation next is [1.0, 0.34782608695652173, 0.28672985781990523, 0.81, 1.0, 1.0, 0.5286418994817206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26575908009077936, 0.26575908009077953, 0.3206844158851646], 
reward next is 0.6793, 
noisyNet noise sample is [array([0.1591499], dtype=float32), -0.97387064]. 
=============================================
[2019-03-27 02:46:29,003] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,004] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,060] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-27 02:46:29,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,112] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-27 02:46:29,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-27 02:46:29,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-27 02:46:29,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,515] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-27 02:46:29,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 02:46:29,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:29,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-27 02:46:31,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:46:31,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6499
[2019-03-27 02:46:31,282] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.85, 96.0, 1.0, 2.0, 0.2850920325991209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 459598.5327173639, 459598.5327173633, 164245.8599323117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [19.83333333333333, 96.0, 1.0, 2.0, 0.2844410003306096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458693.0151236086, 458693.0151236086, 164184.2491094271], 
processed observation next is [0.0, 0.17391304347826086, 0.13902053712480236, 0.96, 1.0, 1.0, 0.13788072328989107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1274147264232246, 0.1274147264232246, 0.24505111807377178], 
reward next is 0.7549, 
noisyNet noise sample is [array([-1.0711609], dtype=float32), 2.958153]. 
=============================================
[2019-03-27 02:46:33,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3358645e-15 9.9987507e-01 3.3384830e-25 2.2274586e-17 1.2485702e-04], sum to 1.0000
[2019-03-27 02:46:33,052] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6944
[2019-03-27 02:46:33,058] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 62.16666666666667, 1.0, 2.0, 0.9338398950633804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1391413.295148267, 1391413.295148266, 292020.8634988565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 49800.0000, 
sim time next is 50400.0000, 
raw observation next is [28.0, 62.0, 1.0, 2.0, 0.9934345129241986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1477009.605252179, 1477009.60525218, 310327.2248383393], 
processed observation next is [1.0, 0.6086956521739131, 0.5260663507109005, 0.62, 1.0, 1.0, 0.9920897746074682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4102804459033831, 0.41028044590338336, 0.46317496244528256], 
reward next is 0.5368, 
noisyNet noise sample is [array([-0.843892], dtype=float32), -0.7677632]. 
=============================================
[2019-03-27 02:46:34,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9432066e-16 9.5708601e-06 4.2112846e-25 2.2841326e-16 9.9999046e-01], sum to 1.0000
[2019-03-27 02:46:34,366] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4417553e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0420265e-34], sum to 1.0000
[2019-03-27 02:46:34,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1313
[2019-03-27 02:46:34,376] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3528
[2019-03-27 02:46:34,378] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.6, 71.0, 1.0, 2.0, 0.3692107450890336, 1.0, 2.0, 0.3692107450890336, 1.0, 2.0, 0.6235747205077288, 6.9112, 6.9112, 170.5573041426782, 1581662.856251037, 1581662.856251037, 337108.436597245], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 61200.0000, 
sim time next is 61800.0000, 
raw observation next is [26.43333333333333, 71.66666666666667, 1.0, 2.0, 0.1788002435635671, 1.0, 2.0, 0.1788002435635671, 1.0, 2.0, 0.3042836472701003, 6.911200000000001, 6.9112, 170.5573041426782, 774128.1426121544, 774128.1426121538, 267173.0207582854], 
processed observation next is [1.0, 0.7391304347826086, 0.4518167456556081, 0.7166666666666667, 1.0, 1.0, 0.010602703088635054, 1.0, 1.0, 0.010602703088635054, 1.0, 1.0, 0.15156542350012234, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.21503559517004286, 0.21503559517004273, 0.3987657026243066], 
reward next is 0.6012, 
noisyNet noise sample is [array([-0.6117939], dtype=float32), -0.36886206]. 
=============================================
[2019-03-27 02:46:34,384] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 96.0, 1.0, 2.0, 0.3012043923353655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481151.2677636385, 481151.2677636392, 165724.1129445849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 174600.0000, 
sim time next is 175200.0000, 
raw observation next is [20.26666666666667, 96.0, 1.0, 2.0, 0.2998768415748062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479368.807101741, 479368.807101741, 165600.5459385222], 
processed observation next is [0.0, 0.0, 0.15955766192733034, 0.96, 1.0, 1.0, 0.15647812237928457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13315800197270583, 0.13315800197270583, 0.24716499393809285], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.68490773], dtype=float32), 0.7396909]. 
=============================================
[2019-03-27 02:46:44,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0603954e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0562034e-34], sum to 1.0000
[2019-03-27 02:46:44,096] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0315
[2019-03-27 02:46:44,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 88.0, 1.0, 2.0, 0.3012355422874743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479912.221147465, 479912.2211474656, 165618.7734097744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [21.38333333333333, 88.16666666666667, 1.0, 2.0, 0.3010137077222932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479568.2306969896, 479568.2306969896, 165594.3266130765], 
processed observation next is [0.0, 0.782608695652174, 0.21248025276461283, 0.8816666666666667, 1.0, 1.0, 0.1578478406292689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13321339741583044, 0.13321339741583044, 0.24715571136280073], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.07697419], dtype=float32), -1.3337495]. 
=============================================
[2019-03-27 02:46:44,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9715399e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.4590466e-31], sum to 1.0000
[2019-03-27 02:46:44,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1852
[2019-03-27 02:46:44,317] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 86.0, 1.0, 2.0, 0.2732702650661859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 441973.9626977112, 441973.9626977112, 163067.3278540964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 339000.0000, 
sim time next is 339600.0000, 
raw observation next is [20.83333333333333, 86.0, 1.0, 2.0, 0.2764787152356477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 447421.9574306399, 447421.9574306393, 163423.2076608312], 
processed observation next is [0.0, 0.9565217391304348, 0.1864139020537123, 0.86, 1.0, 1.0, 0.12828760871764783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12428387706406664, 0.12428387706406648, 0.2439152353146734], 
reward next is 0.7561, 
noisyNet noise sample is [array([-0.539696], dtype=float32), -0.5500345]. 
=============================================
[2019-03-27 02:46:47,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9473596e-28 1.0000000e+00 0.0000000e+00 3.6445336e-37 9.2154731e-27], sum to 1.0000
[2019-03-27 02:46:47,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1634
[2019-03-27 02:46:47,516] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 81.0, 1.0, 2.0, 0.2292047561545382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380660.749794824, 380660.749794824, 158663.3649295302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 597600.0000, 
sim time next is 598200.0000, 
raw observation next is [19.26666666666667, 81.5, 1.0, 2.0, 0.2289954706381522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380504.1474334769, 380504.1474334762, 158613.0745081647], 
processed observation next is [1.0, 0.9565217391304348, 0.1121642969984204, 0.815, 1.0, 1.0, 0.07107888028693035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10569559650929913, 0.10569559650929894, 0.23673593210173835], 
reward next is 0.7633, 
noisyNet noise sample is [array([-1.2420847], dtype=float32), -0.7192502]. 
=============================================
[2019-03-27 02:46:47,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3370384e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.4616025e-36], sum to 1.0000
[2019-03-27 02:46:47,570] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1881
[2019-03-27 02:46:47,574] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 77.66666666666666, 1.0, 2.0, 0.3048790734057678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 482884.8895118933, 482884.889511894, 165781.7774566349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [23.13333333333333, 77.33333333333334, 1.0, 2.0, 0.3051059986316448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483043.7674686298, 483043.7674686298, 165789.0089884317], 
processed observation next is [0.0, 0.43478260869565216, 0.29541864139020524, 0.7733333333333334, 1.0, 1.0, 0.1627783116043913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1341788242968416, 0.1341788242968416, 0.24744628207228614], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.8266449], dtype=float32), -2.2469962]. 
=============================================
[2019-03-27 02:46:57,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0260396e-27 1.0000000e+00 0.0000000e+00 4.8861425e-37 9.5392888e-26], sum to 1.0000
[2019-03-27 02:46:57,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8861
[2019-03-27 02:46:57,940] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333333, 67.66666666666667, 1.0, 2.0, 0.4671205029577186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766549.5944158662, 766549.5944158655, 190072.9993363132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 470400.0000, 
sim time next is 471000.0000, 
raw observation next is [22.56666666666667, 66.83333333333333, 1.0, 2.0, 0.4749960563640614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779425.2878549513, 779425.2878549506, 191417.6931008899], 
processed observation next is [1.0, 0.43478260869565216, 0.26856240126382325, 0.6683333333333333, 1.0, 1.0, 0.3674651281494715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21650702440415315, 0.21650702440415295, 0.2856980494043133], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.6888213], dtype=float32), -0.85838825]. 
=============================================
[2019-03-27 02:46:57,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.43822]
 [75.48236]
 [75.44685]
 [75.50686]
 [75.43749]], R is [[75.37309265]
 [75.33567047]
 [75.30028534]
 [75.26101685]
 [75.22909546]].
[2019-03-27 02:46:58,221] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 02:46:58,225] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:46:58,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:58,233] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:46:58,235] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:58,237] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:46:58,239] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:46:58,239] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:58,239] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:58,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:46:58,270] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:46:59,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-27 02:46:59,291] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-27 02:46:59,292] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-27 02:46:59,327] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-27 02:46:59,327] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-27 02:47:00,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13033384]
[2019-03-27 02:47:00,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.98333333333333, 67.83333333333333, 1.0, 2.0, 0.8706346101013434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332474.388643566, 1332474.388643566, 277886.5639423959]
[2019-03-27 02:47:00,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:47:00,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8845645e-26 1.0000000e+00 1.5820922e-37 6.8619056e-34 1.7381045e-23], sampled 0.7073344893259047
[2019-03-27 02:47:54,537] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13033384]
[2019-03-27 02:47:54,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.62606196666667, 65.71140327333333, 1.0, 2.0, 0.6758734506492331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944537.4101985864, 944537.4101985864, 215723.3814990107]
[2019-03-27 02:47:54,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:47:54,541] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2677388e-28 1.0000000e+00 0.0000000e+00 2.5405619e-36 1.7631913e-26], sampled 0.019977924596123242
[2019-03-27 02:48:41,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13033384]
[2019-03-27 02:48:41,353] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.95460383333334, 69.18121404, 1.0, 2.0, 0.8740834721359388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1244705.641700906, 1244705.641700906, 266263.7923512125]
[2019-03-27 02:48:41,354] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:48:41,356] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3648986e-24 1.0000000e+00 9.8428740e-36 1.2157597e-31 3.9694540e-21], sampled 0.920647200444376
[2019-03-27 02:48:52,279] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7905.1966 3162743661.1043 1825.0000
[2019-03-27 02:48:53,592] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8040.0294 3005517022.7227 1653.0000
[2019-03-27 02:48:53,733] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.8502 2779160678.3583 922.0000
[2019-03-27 02:48:53,741] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8270.4605 2926638743.1914 1305.0000
[2019-03-27 02:48:53,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8507.0973 2841563884.1558 1127.0000
[2019-03-27 02:48:54,904] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1500000, evaluation results [1500000.0, 7905.19664246587, 3162743661.1042676, 1825.0, 8270.46052656632, 2926638743.1913896, 1305.0, 8663.850245679792, 2779160678.358265, 922.0, 8040.029413748112, 3005517022.722703, 1653.0, 8507.097349153179, 2841563884.1557527, 1127.0]
[2019-03-27 02:48:55,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9215133e-25 1.0000000e+00 3.6976116e-35 2.0529696e-31 1.0650975e-19], sum to 1.0000
[2019-03-27 02:48:55,215] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6570
[2019-03-27 02:48:55,221] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 49.33333333333334, 1.0, 2.0, 0.5581621855736757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918740.3667638666, 918740.3667638672, 206917.3151846794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 746400.0000, 
sim time next is 747000.0000, 
raw observation next is [25.3, 49.5, 1.0, 2.0, 0.5500315423677762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 905698.99976021, 905698.9997602093, 205294.7593062091], 
processed observation next is [1.0, 0.6521739130434783, 0.39810426540284366, 0.495, 1.0, 1.0, 0.4578693281539472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2515830554889472, 0.25158305548894705, 0.30641008851673], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.9223083], dtype=float32), -0.6101019]. 
=============================================
[2019-03-27 02:48:55,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.04885]
 [80.90611]
 [80.72388]
 [80.54766]
 [80.4469 ]], R is [[80.91677856]
 [80.79878235]
 [80.66090393]
 [80.5318985 ]
 [80.41397095]].
[2019-03-27 02:48:58,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5947904e-29 1.0000000e+00 0.0000000e+00 4.3115922e-38 9.4241511e-28], sum to 1.0000
[2019-03-27 02:48:58,307] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2098
[2019-03-27 02:48:58,311] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [18.8, 87.0, 1.0, 2.0, 0.2299358285746499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381481.9020643638, 381481.9020643638, 158786.4280343929], 
processed observation next is [1.0, 0.0, 0.09004739336492901, 0.87, 1.0, 1.0, 0.07221184165620467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10596719501787884, 0.10596719501787884, 0.2369946687080491], 
reward next is 0.7630, 
noisyNet noise sample is [array([1.2861305], dtype=float32), -2.1019962]. 
=============================================
[2019-03-27 02:49:07,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4259137e-24 1.0000000e+00 9.8866013e-36 8.9250743e-32 6.5476662e-22], sum to 1.0000
[2019-03-27 02:49:07,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7693
[2019-03-27 02:49:07,264] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 93.5, 1.0, 2.0, 0.5693733268232177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 880004.0057918355, 880004.0057918349, 205284.1763980274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 984600.0000, 
sim time next is 985200.0000, 
raw observation next is [21.96666666666667, 93.66666666666667, 1.0, 2.0, 0.5792133819971944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894249.3052030515, 894249.3052030515, 207135.8827228688], 
processed observation next is [1.0, 0.391304347826087, 0.24012638230647723, 0.9366666666666668, 1.0, 1.0, 0.4930281710809571, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24840258477862542, 0.24840258477862542, 0.3091580339147295], 
reward next is 0.6908, 
noisyNet noise sample is [array([-1.6863661], dtype=float32), 0.09148585]. 
=============================================
[2019-03-27 02:49:09,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3589873e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4924927e-32], sum to 1.0000
[2019-03-27 02:49:09,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6128
[2019-03-27 02:49:09,686] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.53333333333333, 92.0, 1.0, 2.0, 0.2172313405602214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 362233.1496192599, 362233.1496192593, 157244.0798844782], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 708000.0000, 
sim time next is 708600.0000, 
raw observation next is [17.51666666666667, 92.0, 1.0, 2.0, 0.2141490871248178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357128.8032411602, 357128.8032411602, 156965.2098925418], 
processed observation next is [1.0, 0.17391304347826086, 0.029225908372827993, 0.92, 1.0, 1.0, 0.05319167123472024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09920244534476672, 0.09920244534476672, 0.23427643267543555], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.5628987], dtype=float32), -0.06606718]. 
=============================================
[2019-03-27 02:49:13,654] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1822193e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:49:13,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9659
[2019-03-27 02:49:13,669] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 62.16666666666667, 1.0, 2.0, 0.28930423503913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463132.4665881591, 463132.4665881597, 164470.3522888233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 821400.0000, 
sim time next is 822000.0000, 
raw observation next is [24.93333333333333, 62.33333333333334, 1.0, 2.0, 0.2889146941406687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462546.0055829607, 462546.0055829613, 164430.3787668172], 
processed observation next is [0.0, 0.5217391304347826, 0.38072669826224315, 0.6233333333333334, 1.0, 1.0, 0.143270715832131, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1284850015508224, 0.1284850015508226, 0.24541847577136894], 
reward next is 0.7546, 
noisyNet noise sample is [array([-2.496736], dtype=float32), 1.8842303]. 
=============================================
[2019-03-27 02:49:13,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.25851 ]
 [76.21918 ]
 [76.11027 ]
 [76.08636 ]
 [76.055305]], R is [[76.27432251]
 [76.26610565]
 [76.25793457]
 [76.24985504]
 [76.24185181]].
[2019-03-27 02:49:23,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8521036e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3944669e-37], sum to 1.0000
[2019-03-27 02:49:23,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6950
[2019-03-27 02:49:23,507] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 64.66666666666667, 1.0, 2.0, 0.3609115328024661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549785.6662363309, 549785.6662363316, 170443.7135454415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [26.93333333333333, 62.83333333333333, 1.0, 2.0, 0.3593082526810152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548245.3583190968, 548245.3583190962, 170342.747135036], 
processed observation next is [0.0, 0.391304347826087, 0.4755134281200631, 0.6283333333333333, 1.0, 1.0, 0.22808223214580142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15229037731086023, 0.15229037731086004, 0.25424290617169554], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.59063685], dtype=float32), -2.525916]. 
=============================================
[2019-03-27 02:49:27,632] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8035576e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3451592e-38], sum to 1.0000
[2019-03-27 02:49:27,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8763
[2019-03-27 02:49:27,646] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 69.33333333333334, 1.0, 2.0, 0.4339430502299369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622305.1670256971, 622305.1670256971, 175954.0529619521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1437600.0000, 
sim time next is 1438200.0000, 
raw observation next is [27.8, 69.5, 1.0, 2.0, 0.4380729333670599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625911.7252979735, 625911.7252979735, 176244.3485751881], 
processed observation next is [0.0, 0.6521739130434783, 0.5165876777251186, 0.695, 1.0, 1.0, 0.3229794377916384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17386436813832598, 0.17386436813832598, 0.2630512665301315], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.6098094], dtype=float32), -1.202869]. 
=============================================
[2019-03-27 02:49:38,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6346368e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2125442e-33], sum to 1.0000
[2019-03-27 02:49:38,455] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4012
[2019-03-27 02:49:38,464] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 94.0, 1.0, 2.0, 0.4580692900350514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650440.6927679601, 650440.6927679601, 178624.1862881741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [24.25, 94.0, 1.0, 2.0, 0.4570678034078495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649487.8429542875, 649487.8429542875, 178537.2880311009], 
processed observation next is [1.0, 0.043478260869565216, 0.3483412322274882, 0.94, 1.0, 1.0, 0.34586482338295127, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1804132897095243, 0.1804132897095243, 0.26647356422552376], 
reward next is 0.7335, 
noisyNet noise sample is [array([-1.2721311], dtype=float32), 0.34304556]. 
=============================================
[2019-03-27 02:49:40,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9859595e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:49:40,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9524
[2019-03-27 02:49:40,769] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.36666666666667, 99.0, 1.0, 2.0, 0.4450125520757766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639576.0243611212, 639576.0243611212, 177710.6118858715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1658400.0000, 
sim time next is 1659000.0000, 
raw observation next is [23.38333333333333, 99.0, 1.0, 2.0, 0.4489660344023746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644802.908381624, 644802.9083816233, 178227.8249370781], 
processed observation next is [1.0, 0.17391304347826086, 0.30726698262243274, 0.99, 1.0, 1.0, 0.3361036559064754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17911191899489556, 0.17911191899489537, 0.26601167901056433], 
reward next is 0.7340, 
noisyNet noise sample is [array([-1.0955219], dtype=float32), 0.7038958]. 
=============================================
[2019-03-27 02:49:40,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.211426]
 [75.32686 ]
 [75.3005  ]
 [75.382385]
 [75.411194]], R is [[75.18467712]
 [75.16758728]
 [75.15265656]
 [75.1388855 ]
 [75.12167358]].
[2019-03-27 02:49:45,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6421922e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1802676e-37], sum to 1.0000
[2019-03-27 02:49:45,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2883
[2019-03-27 02:49:45,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.5, 1.0, 2.0, 0.4159657037320666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 640971.4273634896, 640971.4273634889, 178767.7052098467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1582200.0000, 
sim time next is 1582800.0000, 
raw observation next is [23.2, 85.33333333333334, 1.0, 2.0, 0.3917076470776573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602260.0155573708, 602260.0155573714, 175180.5040984568], 
processed observation next is [1.0, 0.30434782608695654, 0.29857819905213273, 0.8533333333333334, 1.0, 1.0, 0.26711764708151486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16729444876593635, 0.16729444876593652, 0.26146343895292057], 
reward next is 0.7385, 
noisyNet noise sample is [array([0.01701799], dtype=float32), 0.81849205]. 
=============================================
[2019-03-27 02:49:47,450] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.8647371e-29 1.0000000e+00 0.0000000e+00 1.2904486e-37 3.3740526e-29], sum to 1.0000
[2019-03-27 02:49:47,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2004
[2019-03-27 02:49:47,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.6524538832873579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001912.57165928, 1001912.571659281, 222064.0386547018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [22.2, 92.0, 1.0, 2.0, 0.5936243180710554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914529.9528314535, 914529.9528314535, 209841.8331625114], 
processed observation next is [1.0, 0.5217391304347826, 0.2511848341232228, 0.92, 1.0, 1.0, 0.5103907446639222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2540360980087371, 0.2540360980087371, 0.31319676591419615], 
reward next is 0.6868, 
noisyNet noise sample is [array([-0.04208731], dtype=float32), -1.4080083]. 
=============================================
[2019-03-27 02:49:47,475] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.29923]
 [78.60261]
 [78.72474]
 [78.61557]
 [78.42062]], R is [[79.47685242]
 [79.35064697]
 [79.14884186]
 [78.98479462]
 [78.84236908]].
[2019-03-27 02:49:50,900] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 02:49:50,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:49:50,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:49:50,902] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:49:50,903] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:49:50,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:49:50,905] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:49:50,905] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:49:50,906] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:49:50,907] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:49:50,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:49:50,942] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-27 02:49:50,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-27 02:49:50,987] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-27 02:49:51,006] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-27 02:49:51,007] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-27 02:50:32,414] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:50:32,416] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.40564607, 96.51065828666665, 1.0, 2.0, 0.3687371757574615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559374.4181033896, 559374.4181033901, 171188.5924020997]
[2019-03-27 02:50:32,416] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:50:32,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.186461e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8560678290834869
[2019-03-27 02:50:41,440] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:50:41,442] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.13008144333334, 93.13228685666667, 1.0, 2.0, 0.540143591946461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754786.6779688193, 754786.6779688193, 190123.6891455374]
[2019-03-27 02:50:41,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:50:41,447] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1525307e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5437914565477987
[2019-03-27 02:50:50,165] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:50:50,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5909416752724208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825798.5749622694, 825798.5749622694, 199069.8323283244]
[2019-03-27 02:50:50,168] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:50:50,169] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.6815058e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.5582124e-37], sampled 0.618694609800667
[2019-03-27 02:50:57,550] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:50:57,552] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.90000000000001, 46.33333333333334, 1.0, 2.0, 0.5882238956555137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821999.2034125515, 821999.2034125515, 198572.449495239]
[2019-03-27 02:50:57,554] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:50:57,557] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.82742e-36 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sampled 0.852640045699898
[2019-03-27 02:51:13,294] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:51:13,297] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.26666666666667, 92.33333333333333, 1.0, 2.0, 0.5708127086410628, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9861390252757317, 6.9112, 6.9112, 168.9121922093981, 1595917.928991621, 1595917.928991621, 348149.266959767]
[2019-03-27 02:51:13,298] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:51:13,304] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1281902e-30 1.0000000e+00 0.0000000e+00 2.8330096e-38 1.1864297e-29], sampled 0.30441977132288567
[2019-03-27 02:51:18,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:51:18,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.36934776, 90.35181651, 1.0, 2.0, 0.5789840769806014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 809082.325335836, 809082.3253358367, 196894.3295736987]
[2019-03-27 02:51:18,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:51:18,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2870073e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4734765e-37], sampled 0.478938782795164
[2019-03-27 02:51:41,667] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:51:41,668] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.3, 61.0, 1.0, 2.0, 0.9753208419627818, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564997423, 1363285.675958091, 1363285.67595809, 291493.3947095395]
[2019-03-27 02:51:41,668] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:51:41,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.00843535e-30 1.00000000e+00 0.00000000e+00 2.85298650e-38
 1.08209126e-30], sampled 0.2901323157560296
[2019-03-27 02:51:43,799] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1326343]
[2019-03-27 02:51:43,799] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.53333333333333, 62.33333333333334, 1.0, 2.0, 0.5635164618726181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 870642.1117192896, 870642.1117192901, 204105.463384183]
[2019-03-27 02:51:43,801] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:51:43,802] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7309422e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7564307e-38], sampled 0.20137952293781225
[2019-03-27 02:51:44,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 02:51:45,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 02:51:45,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 02:51:45,358] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 02:51:45,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 02:51:46,397] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1525000, evaluation results [1525000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 02:51:50,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3757602e-30 1.0000000e+00 0.0000000e+00 2.1728991e-37 5.4048534e-31], sum to 1.0000
[2019-03-27 02:51:50,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9831
[2019-03-27 02:51:50,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.0, 1.0, 2.0, 0.6004387425506822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 953896.1516256345, 953896.1516256345, 213998.0517939191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1782000.0000, 
sim time next is 1782600.0000, 
raw observation next is [21.0, 92.33333333333334, 1.0, 2.0, 0.611183759918883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970856.4212880665, 970856.4212880665, 216282.768564925], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9233333333333335, 1.0, 1.0, 0.5315466986974493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26968233924668517, 0.26968233924668517, 0.32281010233570895], 
reward next is 0.6772, 
noisyNet noise sample is [array([-0.2942195], dtype=float32), -0.308271]. 
=============================================
[2019-03-27 02:51:53,467] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.3588407e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1832462e-38], sum to 1.0000
[2019-03-27 02:51:53,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7057
[2019-03-27 02:51:53,480] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3257056396358542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 510860.6854647928, 510860.6854647922, 167757.8591853594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [21.7, 90.83333333333334, 1.0, 2.0, 0.4555719556109278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714791.7576813748, 714791.7576813741, 186076.7926937129], 
processed observation next is [1.0, 0.08695652173913043, 0.2274881516587678, 0.9083333333333334, 1.0, 1.0, 0.3440625971215997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19855326602260412, 0.19855326602260392, 0.27772655625927295], 
reward next is 0.7223, 
noisyNet noise sample is [array([-1.1710957], dtype=float32), 0.048965335]. 
=============================================
[2019-03-27 02:51:53,497] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[69.84038]
 [69.83119]
 [69.8791 ]
 [69.94801]
 [69.94884]], R is [[70.55732727]
 [70.60137177]
 [70.64491272]
 [70.68799591]
 [70.73066711]].
[2019-03-27 02:51:59,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4108231e-28 1.0000000e+00 0.0000000e+00 5.7498704e-35 1.0069382e-26], sum to 1.0000
[2019-03-27 02:51:59,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9826
[2019-03-27 02:51:59,674] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 90.0, 1.0, 2.0, 0.9086910339732761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1270096.036179465, 1270096.036179465, 272336.1166628831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1678800.0000, 
sim time next is 1679400.0000, 
raw observation next is [25.55, 89.5, 1.0, 2.0, 0.9582083846393582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1339351.106769004, 1339351.106769004, 286447.7388908998], 
processed observation next is [1.0, 0.43478260869565216, 0.40995260663507116, 0.895, 1.0, 1.0, 0.9496486561919978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37204197410250106, 0.37204197410250106, 0.42753393864313405], 
reward next is 0.5725, 
noisyNet noise sample is [array([-1.2923237], dtype=float32), -0.29941323]. 
=============================================
[2019-03-27 02:52:07,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5910995e-24 1.0000000e+00 7.0294258e-35 9.7760716e-30 3.7472912e-23], sum to 1.0000
[2019-03-27 02:52:07,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8812
[2019-03-27 02:52:07,931] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 88.5, 1.0, 2.0, 0.7681212971222121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1111738.9730765, 1111738.973076499, 241469.6755546128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [24.43333333333333, 88.33333333333334, 1.0, 2.0, 0.8651847732561314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1256650.847859527, 1256650.847859528, 267346.7810243415], 
processed observation next is [1.0, 0.391304347826087, 0.3570300157977882, 0.8833333333333334, 1.0, 1.0, 0.8375720159712426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3490696799609797, 0.34906967996098, 0.39902504630498725], 
reward next is 0.6010, 
noisyNet noise sample is [array([-0.6097878], dtype=float32), -1.6725358]. 
=============================================
[2019-03-27 02:52:11,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4427657e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7677817e-33], sum to 1.0000
[2019-03-27 02:52:11,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7850
[2019-03-27 02:52:11,054] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 96.33333333333333, 1.0, 2.0, 0.3623963527946852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555663.2742153206, 555663.2742153206, 171050.950247655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1829400.0000, 
sim time next is 1830000.0000, 
raw observation next is [21.86666666666667, 96.66666666666666, 1.0, 2.0, 0.350525081400969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537080.5314178695, 537080.5314178695, 169486.7710504025], 
processed observation next is [1.0, 0.17391304347826086, 0.23538704581358633, 0.9666666666666666, 1.0, 1.0, 0.2175000980734566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14918903650496376, 0.14918903650496376, 0.25296532992597387], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.17682496], dtype=float32), -1.0311548]. 
=============================================
[2019-03-27 02:52:11,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.996895]
 [72.94985 ]
 [72.95849 ]
 [73.04196 ]
 [73.186165]], R is [[72.94844055]
 [72.96365356]
 [72.98114777]
 [72.99855804]
 [73.01591492]].
[2019-03-27 02:52:11,468] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0032838e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8963376e-35], sum to 1.0000
[2019-03-27 02:52:11,477] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5540
[2019-03-27 02:52:11,484] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.38333333333333, 83.66666666666667, 1.0, 2.0, 0.4781189451026186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687416.3430143271, 687416.3430143271, 182702.9621575311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1929000.0000, 
sim time next is 1929600.0000, 
raw observation next is [25.5, 83.0, 1.0, 2.0, 0.4865390156703512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698938.4954897553, 698938.4954897553, 183945.1048319371], 
processed observation next is [1.0, 0.34782608695652173, 0.40758293838862564, 0.83, 1.0, 1.0, 0.3813723080365677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19414958208048758, 0.19414958208048758, 0.27454493258498075], 
reward next is 0.7255, 
noisyNet noise sample is [array([1.2726343], dtype=float32), -0.22878364]. 
=============================================
[2019-03-27 02:52:16,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9461772e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.0499541e-38], sum to 1.0000
[2019-03-27 02:52:16,336] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4173
[2019-03-27 02:52:16,341] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.5, 1.0, 2.0, 0.5597742409293968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782228.2848523678, 782228.2848523678, 193491.8349713187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2118600.0000, 
sim time next is 2119200.0000, 
raw observation next is [30.0, 75.66666666666667, 1.0, 2.0, 0.5612681575784264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784316.6540803378, 784316.6540803378, 193752.661080726], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.7566666666666667, 1.0, 1.0, 0.4714074187691884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21786573724453828, 0.21786573724453828, 0.28918307623988954], 
reward next is 0.7108, 
noisyNet noise sample is [array([0.68976533], dtype=float32), 1.3021905]. 
=============================================
[2019-03-27 02:52:24,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.68398e-36 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00], sum to 1.0000
[2019-03-27 02:52:24,164] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4215
[2019-03-27 02:52:24,172] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 84.66666666666667, 1.0, 2.0, 0.5084988919999548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710552.1690202173, 710552.1690202173, 184938.6034889804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [26.9, 85.0, 1.0, 2.0, 0.5087270310955853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710871.0664116039, 710871.0664116046, 184974.9145333862], 
processed observation next is [0.0, 0.6956521739130435, 0.4739336492890995, 0.85, 1.0, 1.0, 0.40810485674166896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19746418511433442, 0.1974641851143346, 0.2760819619901287], 
reward next is 0.7239, 
noisyNet noise sample is [array([-1.4815553], dtype=float32), -0.17639677]. 
=============================================
[2019-03-27 02:52:37,187] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3319162e-24 1.0000000e+00 4.9051428e-34 5.2633270e-29 3.0707967e-22], sum to 1.0000
[2019-03-27 02:52:37,198] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6412
[2019-03-27 02:52:37,203] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 76.0, 1.0, 2.0, 0.6760677007443725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944808.9966840319, 944808.9966840319, 215762.2623978976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2273400.0000, 
sim time next is 2274000.0000, 
raw observation next is [28.26666666666667, 75.33333333333333, 1.0, 2.0, 0.6982862673080539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975873.8536989326, 975873.8536989326, 220473.8462968949], 
processed observation next is [1.0, 0.30434782608695654, 0.53870458135861, 0.7533333333333333, 1.0, 1.0, 0.6364894786844022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2710760704719257, 0.2710760704719257, 0.3290654422341715], 
reward next is 0.6709, 
noisyNet noise sample is [array([0.268322], dtype=float32), 0.14297752]. 
=============================================
[2019-03-27 02:52:37,217] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[57.60906 ]
 [57.654446]
 [57.64273 ]
 [57.450108]
 [57.050705]], R is [[57.58041   ]
 [57.68257523]
 [57.78828812]
 [57.89731598]
 [58.00734329]].
[2019-03-27 02:52:39,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6110059e-25 1.0000000e+00 2.8358144e-35 4.5627815e-31 6.6012070e-23], sum to 1.0000
[2019-03-27 02:52:39,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6759
[2019-03-27 02:52:39,875] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 77.0, 1.0, 2.0, 0.7166497822587589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001549.461716232, 1001549.461716232, 224478.8678559963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2356200.0000, 
sim time next is 2356800.0000, 
raw observation next is [28.43333333333333, 76.33333333333334, 1.0, 2.0, 0.7052609130727916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985625.6320818511, 985625.6320818511, 221983.717802521], 
processed observation next is [1.0, 0.2608695652173913, 0.546603475513428, 0.7633333333333334, 1.0, 1.0, 0.644892666352761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2737848978005142, 0.2737848978005142, 0.33131898179480745], 
reward next is 0.6687, 
noisyNet noise sample is [array([-1.0610836], dtype=float32), 0.16575597]. 
=============================================
[2019-03-27 02:52:42,418] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-27 02:52:42,421] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:52:42,422] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:52:42,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:52:42,424] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:52:42,425] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:52:42,425] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:52:42,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:52:42,427] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:52:42,428] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:52:42,424] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:52:42,453] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-27 02:52:42,477] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-27 02:52:42,499] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-27 02:52:42,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-27 02:52:42,539] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-27 02:52:46,522] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:52:46,523] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.48397663, 81.71200375500001, 1.0, 2.0, 0.2730093612819837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446910.9145294954, 446910.9145294954, 163256.5468216284]
[2019-03-27 02:52:46,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:52:46,528] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19381442242200153
[2019-03-27 02:53:38,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:53:38,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.35, 83.0, 1.0, 2.0, 0.5519287431790777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771261.0094031098, 771261.0094031098, 192132.1821833044]
[2019-03-27 02:53:38,597] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:53:38,598] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.7900614e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8148798e-31], sampled 0.03786494699460974
[2019-03-27 02:53:39,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:53:39,505] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.38906863666667, 91.84734178666668, 1.0, 2.0, 0.5546239374614502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775028.6273220747, 775028.6273220741, 192597.7274590018]
[2019-03-27 02:53:39,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:53:39,508] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.925158e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.35695183777679096
[2019-03-27 02:53:47,064] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:53:47,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.51468596, 74.48606137333334, 1.0, 2.0, 0.6657130703297489, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973142864387, 6.9112, 168.9123157702096, 1827150.330790939, 1759915.256447952, 377521.9427540944]
[2019-03-27 02:53:47,067] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 02:53:47,070] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6443804e-19 1.0000000e+00 3.8154929e-29 2.0492928e-23 2.0081937e-14], sampled 0.7131879984263024
[2019-03-27 02:53:47,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1827150.330790939 W.
[2019-03-27 02:53:47,792] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:53:47,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.72059660666666, 57.81426419166667, 1.0, 2.0, 0.5416075162650872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 756833.0675443773, 756833.0675443766, 190372.0977092239]
[2019-03-27 02:53:47,793] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:53:47,797] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2306685e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9192638e-33], sampled 0.8635040710928596
[2019-03-27 02:54:19,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:54:19,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.116016075, 72.742136625, 1.0, 2.0, 0.9730589525137833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1360122.022830612, 1360122.022830612, 290826.2128287418]
[2019-03-27 02:54:19,170] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:54:19,174] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.6664161e-28 1.0000000e+00 0.0000000e+00 8.8743062e-35 1.1708474e-27], sampled 0.05702647992155496
[2019-03-27 02:54:19,444] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13265222]
[2019-03-27 02:54:19,445] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.2, 64.0, 1.0, 2.0, 0.8344485681396929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1166268.83106246, 1166268.83106246, 252542.469887179]
[2019-03-27 02:54:19,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:54:19,448] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0415941e-30 1.0000000e+00 0.0000000e+00 8.0607704e-38 4.7600238e-31], sampled 0.9678188085925749
[2019-03-27 02:54:36,677] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.3802 2927080149.2749 1327.0000
[2019-03-27 02:54:36,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.2594 3007495821.2538 1764.0000
[2019-03-27 02:54:37,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7872.7913 3164168961.1847 1882.0000
[2019-03-27 02:54:37,215] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.4049 2842567599.3516 1155.0000
[2019-03-27 02:54:37,244] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2415 2779220970.4527 935.0000
[2019-03-27 02:54:38,258] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1550000, evaluation results [1550000.0, 7872.79132198878, 3164168961.1846523, 1882.0, 8261.380247329022, 2927080149.2748756, 1327.0, 8660.241500758022, 2779220970.4526916, 935.0, 8002.259384499581, 3007495821.253809, 1764.0, 8493.404862181571, 2842567599.351618, 1155.0]
[2019-03-27 02:54:39,547] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8381456e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9155398e-34], sum to 1.0000
[2019-03-27 02:54:39,556] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9005
[2019-03-27 02:54:39,562] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412600.0000, 
sim time next is 2413200.0000, 
raw observation next is [29.63333333333334, 78.66666666666667, 1.0, 2.0, 0.5723458124047743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799802.4074891193, 799802.4074891187, 195707.1499520162], 
processed observation next is [1.0, 0.9565217391304348, 0.6034755134281204, 0.7866666666666667, 1.0, 1.0, 0.4847539908491257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22216733541364425, 0.22216733541364408, 0.2921002238089794], 
reward next is 0.7079, 
noisyNet noise sample is [array([1.3131973], dtype=float32), -0.13093223]. 
=============================================
[2019-03-27 02:54:39,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2448499e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:54:39,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1844
[2019-03-27 02:54:39,984] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3125757397116858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496358.7754493747, 496358.7754493747, 166791.2369796498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2922000.0000, 
sim time next is 2922600.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3101219421506055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493050.0560429331, 493050.0560429325, 166556.303467885], 
processed observation next is [1.0, 0.8260869565217391, 0.15481832543443946, 0.99, 1.0, 1.0, 0.1688216170489223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13695834890081474, 0.13695834890081457, 0.2485914977132612], 
reward next is 0.7514, 
noisyNet noise sample is [array([-1.3930818], dtype=float32), 0.7398625]. 
=============================================
[2019-03-27 02:54:48,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:48,119] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3489
[2019-03-27 02:54:48,127] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 92.0, 1.0, 2.0, 0.4303863808367035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625771.8717359477, 625771.8717359477, 176534.932103304], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2608200.0000, 
sim time next is 2608800.0000, 
raw observation next is [23.93333333333333, 92.0, 1.0, 2.0, 0.4307682945254191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626735.3034365972, 626735.3034365979, 176640.1461557788], 
processed observation next is [0.0, 0.17391304347826086, 0.3333333333333332, 0.92, 1.0, 1.0, 0.31417866810291456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17409313984349922, 0.17409313984349942, 0.26364200918772956], 
reward next is 0.7364, 
noisyNet noise sample is [array([-0.21749216], dtype=float32), 0.9152332]. 
=============================================
[2019-03-27 02:54:49,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9539410e-31 1.0000000e+00 0.0000000e+00 1.7237431e-38 3.4522900e-35], sum to 1.0000
[2019-03-27 02:54:49,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7412
[2019-03-27 02:54:49,404] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.33333333333333, 1.0, 2.0, 0.7834872276045254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095005.878563655, 1095005.878563655, 239898.1733106222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2535000.0000, 
sim time next is 2535600.0000, 
raw observation next is [26.7, 91.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.976737658534601, 6.9112, 168.9073340352205, 2210174.842893007, 1454270.916980723, 311350.9248962943], 
processed observation next is [1.0, 0.34782608695652173, 0.46445497630331756, 0.9166666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.1065537658534601, 0.0, 0.8294123362250387, 0.6139374563591686, 0.4039641436057564, 0.46470287297954377], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3415822], dtype=float32), 1.2382876]. 
=============================================
[2019-03-27 02:54:50,167] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:54:50,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9877
[2019-03-27 02:54:50,182] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3659199661941329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556163.915749069, 556163.9157490697, 170946.395624512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2756400.0000, 
sim time next is 2757000.0000, 
raw observation next is [22.0, 99.0, 1.0, 2.0, 0.370312339705314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560688.3996660832, 560688.3996660825, 171267.4939061284], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.99, 1.0, 1.0, 0.24134016831965543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1557467776850231, 0.1557467776850229, 0.25562312523302744], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.60494274], dtype=float32), 0.21273647]. 
=============================================
[2019-03-27 02:54:50,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.66733 ]
 [80.60719 ]
 [80.5597  ]
 [80.509995]
 [80.456024]], R is [[80.65050507]
 [80.58885956]
 [80.52825928]
 [80.46865845]
 [80.41000366]].
[2019-03-27 02:54:51,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.25301765e-11 9.38645780e-01 5.30044089e-20 8.52265758e-11
 6.13541976e-02], sum to 1.0000
[2019-03-27 02:54:51,607] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2522
[2019-03-27 02:54:51,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2114780.897937914 W.
[2019-03-27 02:54:51,620] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.08333333333333, 69.66666666666667, 1.0, 2.0, 0.7562171834317362, 1.0, 1.0, 0.7562171834317362, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2114780.897937914, 2114780.897937915, 399116.5096651568], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2559000.0000, 
sim time next is 2559600.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5099592669127989, 1.0, 2.0, 0.5099592669127989, 1.0, 1.0, 0.8795954500034355, 6.9112, 6.9112, 170.5573041426782, 2139195.759071261, 2139195.759071261, 420985.9490316624], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4095894782081914, 1.0, 1.0, 0.4095894782081914, 1.0, 0.5, 0.853165182931019, 0.0, 0.0, 0.8375144448122397, 0.5942210441864614, 0.5942210441864614, 0.6283372373606901], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7930418], dtype=float32), 0.7888068]. 
=============================================
[2019-03-27 02:54:53,779] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.9384281e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:54:53,790] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8489
[2019-03-27 02:54:53,797] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4787173979121028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668923.8227857114, 668923.8227857107, 180328.5257766519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2704800.0000, 
sim time next is 2705400.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4792667362554635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669691.6689820278, 669691.6689820278, 180411.1458100419], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.3726105256089922, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18602546360611882, 0.18602546360611882, 0.26927036688065953], 
reward next is 0.7307, 
noisyNet noise sample is [array([-1.447285], dtype=float32), 0.3799367]. 
=============================================
[2019-03-27 02:55:11,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5409181e-27 1.0000000e+00 2.1928911e-38 1.4278810e-34 1.8513992e-27], sum to 1.0000
[2019-03-27 02:55:11,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6350
[2019-03-27 02:55:11,286] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.6692399577254918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027449.271806899, 1027449.271806899, 225822.4052598489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.6907842199617018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1058590.6258932, 1058590.625893201, 230597.951246472], 
processed observation next is [1.0, 0.6086956521739131, 0.2654028436018958, 0.915, 1.0, 1.0, 0.6274508674237371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.294052951637, 0.2940529516370003, 0.34417604663652535], 
reward next is 0.6558, 
noisyNet noise sample is [array([-0.28529605], dtype=float32), 2.0681539]. 
=============================================
[2019-03-27 02:55:12,276] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.168957e-35 1.000000e+00 0.000000e+00 0.000000e+00 4.297063e-37], sum to 1.0000
[2019-03-27 02:55:12,283] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5173
[2019-03-27 02:55:12,288] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3046364944211646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485115.6906017719, 485115.6906017712, 165989.8088759029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3045042132623334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484905.0750497521, 484905.0750497521, 165974.590092472], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16205326899076314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13469585418048668, 0.13469585418048668, 0.24772326879473433], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.37612614], dtype=float32), -1.234698]. 
=============================================
[2019-03-27 02:55:23,438] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7052991e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8373563e-34], sum to 1.0000
[2019-03-27 02:55:23,445] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0686
[2019-03-27 02:55:23,450] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 89.0, 1.0, 2.0, 0.7330855957331389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1094865.145498275, 1094865.145498275, 237409.9708477152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3145200.0000, 
sim time next is 3145800.0000, 
raw observation next is [23.83333333333333, 89.0, 1.0, 2.0, 0.7652901427489769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1136342.606158289, 1136342.60615829, 244539.8227294159], 
processed observation next is [1.0, 0.391304347826087, 0.32859399684044216, 0.89, 1.0, 1.0, 0.7172170394565986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31565072393285803, 0.31565072393285837, 0.3649848100439043], 
reward next is 0.6350, 
noisyNet noise sample is [array([-0.25846937], dtype=float32), -1.0291194]. 
=============================================
[2019-03-27 02:55:33,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5418524e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 02:55:33,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6371
[2019-03-27 02:55:33,458] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.578124993567868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807881.3708210465, 807881.3708210465, 196741.2898296856], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.77, 1.0, 1.0, 0.4917168597203228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22441149189473514, 0.22441149189473514, 0.2936437161637098], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.55443335], dtype=float32), -0.9657438]. 
=============================================
[2019-03-27 02:55:34,340] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-27 02:55:34,342] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:55:34,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:55:34,342] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:55:34,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:55:34,344] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:55:34,345] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:55:34,347] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:55:34,346] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:55:34,348] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:55:34,349] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:55:34,376] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-27 02:55:34,399] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-27 02:55:34,419] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-27 02:55:34,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-27 02:55:34,460] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-27 02:55:44,636] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:55:44,638] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.83333333333334, 79.0, 1.0, 2.0, 0.2216664275007999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367760.7904005787, 367760.7904005787, 158041.4913121994]
[2019-03-27 02:55:44,638] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:55:44,641] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39664602623367284
[2019-03-27 02:56:02,744] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:02,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.9, 83.66666666666666, 1.0, 2.0, 1.0333137698404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1444402.279950556, 1444402.279950556, 309238.430981823]
[2019-03-27 02:56:02,747] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:56:02,750] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.9801699e-26 1.0000000e+00 1.0258378e-36 3.5599832e-31 1.9226800e-24], sampled 0.2821108493705732
[2019-03-27 02:56:23,355] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:23,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.70424616666666, 56.57591704, 1.0, 2.0, 0.5994250579717251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837658.1680896241, 837658.1680896241, 200638.9568505537]
[2019-03-27 02:56:23,358] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:56:23,361] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.728095e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.49721917654652226
[2019-03-27 02:56:37,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:37,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 57.33333333333333, 1.0, 2.0, 0.5380066837759849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751799.545241202, 751799.545241202, 189764.923392691]
[2019-03-27 02:56:37,069] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 02:56:37,072] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.8530219e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5631042e-37], sampled 0.5540815784297417
[2019-03-27 02:56:39,534] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:39,536] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.31666666666666, 69.5, 1.0, 2.0, 0.5493892778557266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767711.0975607134, 767711.0975607134, 191695.5992682042]
[2019-03-27 02:56:39,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:56:39,540] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0638869e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9714342768802791
[2019-03-27 02:56:51,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:51,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.02403813666667, 66.09101955333334, 1.0, 2.0, 0.5827496771926398, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9844674986749079, 6.911199999999999, 6.9112, 168.9129564154384, 1629317.748446024, 1629317.748446025, 350660.3882268223]
[2019-03-27 02:56:51,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:56:51,668] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2026875e-20 1.0000000e+00 2.0859128e-30 8.1810278e-23 3.1187287e-13], sampled 0.7242481908013508
[2019-03-27 02:56:54,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13249972]
[2019-03-27 02:56:54,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.9, 81.0, 1.0, 2.0, 0.5521003634488357, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9588157489304128, 6.911200000000001, 6.9112, 168.9125592977415, 1543562.606507262, 1543562.606507262, 337859.8356269777]
[2019-03-27 02:56:54,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:56:54,839] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.0679805e-21 1.0000000e+00 9.9959573e-31 3.7264768e-24 8.8853243e-18], sampled 0.5907895516788095
[2019-03-27 02:57:28,740] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8264.9784 2927000174.4944 1315.0000
[2019-03-27 02:57:28,863] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8019.5330 3006573632.4783 1714.0000
[2019-03-27 02:57:29,023] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5140 2779232105.8179 932.0000
[2019-03-27 02:57:29,060] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8611 3164106536.6676 1924.0000
[2019-03-27 02:57:29,070] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.1880 2842345072.3508 1163.0000
[2019-03-27 02:57:30,088] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1575000, evaluation results [1575000.0, 7875.861081315169, 3164106536.667563, 1924.0, 8264.978425823625, 2927000174.4943995, 1315.0, 8660.514022167306, 2779232105.8178644, 932.0, 8019.533044459543, 3006573632.478348, 1714.0, 8492.187994395063, 2842345072.3508034, 1163.0]
[2019-03-27 02:57:37,362] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2942286e-17 6.1097195e-12 3.3891634e-26 1.0957785e-15 1.0000000e+00], sum to 1.0000
[2019-03-27 02:57:37,378] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1924
[2019-03-27 02:57:37,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.5888544740733571, 1.0, 2.0, 0.5888544740733571, 1.0, 2.0, 1.022645484315087, 6.911199999999999, 6.9112, 170.5573041426782, 2470487.47303752, 2470487.473037521, 482027.0735182371], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3516000.0000, 
sim time next is 3516600.0000, 
raw observation next is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 0.5861297500241115, 1.0, 2.0, 0.5861297500241115, 1.0, 2.0, 1.01791353972156, 6.9112, 6.9112, 170.5573041426782, 2459044.883353442, 2459044.883353442, 479814.8160799738], 
processed observation next is [1.0, 0.6956521739130435, 0.7235387045813582, 0.6566666666666667, 1.0, 1.0, 0.5013611446073633, 1.0, 1.0, 0.5013611446073633, 1.0, 1.0, 1.021845780148244, 0.0, 0.0, 0.8375144448122397, 0.6830680231537338, 0.6830680231537338, 0.7161415165372743], 
reward next is 0.2839, 
noisyNet noise sample is [array([-0.65638906], dtype=float32), 1.1945374]. 
=============================================
[2019-03-27 02:57:40,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7385204e-24 1.0000000e+00 3.9118845e-34 9.1833486e-30 3.5695028e-23], sum to 1.0000
[2019-03-27 02:57:40,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5386
[2019-03-27 02:57:40,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4536208204286624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648448.9884720983, 648448.9884720977, 178525.3691847958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3720000.0000, 
sim time next is 3720600.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.4525832250070201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646969.9019658825, 646969.9019658825, 178374.034024248], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.74, 1.0, 1.0, 0.3404617168759278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17971386165718958, 0.17971386165718958, 0.26622990152872833], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.1718027], dtype=float32), 0.6243565]. 
=============================================
[2019-03-27 02:57:49,283] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 02:57:49,292] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1216
[2019-03-27 02:57:49,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5363314133705559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749457.7307579762, 749457.7307579762, 189484.2553356457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5360902168233459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749120.569155017, 749120.5691550176, 189443.8925984801], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4410725503895733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20808904698750474, 0.20808904698750488, 0.2827520785051942], 
reward next is 0.7172, 
noisyNet noise sample is [array([-0.5427185], dtype=float32), -0.046204694]. 
=============================================
[2019-03-27 02:57:57,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0900168e-13 9.9862671e-01 6.4705875e-22 1.6851852e-13 1.3733415e-03], sum to 1.0000
[2019-03-27 02:57:57,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1778
[2019-03-27 02:57:57,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2746462.299478929 W.
[2019-03-27 02:57:57,061] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.5, 55.5, 1.0, 2.0, 0.6679447087670655, 1.0, 2.0, 0.6545623938977954, 1.0, 2.0, 1.03, 7.00509520476224, 6.9112, 170.5573041426782, 2746462.299478929, 2679201.344895005, 511397.7752009724], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [37.66666666666666, 55.0, 1.0, 2.0, 1.028338848901594, 1.0, 2.0, 1.028338848901594, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2876670.924905259, 2876670.924905259, 546364.3871360451], 
processed observation next is [1.0, 0.5217391304347826, 0.9842022116903629, 0.55, 1.0, 1.0, 1.0341431914477037, 1.0, 1.0, 1.0341431914477037, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7990752569181275, 0.7990752569181275, 0.8154692345314106], 
reward next is 0.1845, 
noisyNet noise sample is [array([-1.1659564], dtype=float32), 1.462056]. 
=============================================
[2019-03-27 02:57:58,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7634982e-28 1.0000000e+00 1.8454948e-37 1.1663960e-31 1.2588072e-28], sum to 1.0000
[2019-03-27 02:57:58,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9912
[2019-03-27 02:57:58,929] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 85.33333333333334, 1.0, 2.0, 0.5405564260906415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755363.7698949806, 755363.76989498, 190194.6108314481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4065600.0000, 
sim time next is 4066200.0000, 
raw observation next is [27.7, 85.5, 1.0, 2.0, 0.5400614900624227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754671.9095572466, 754671.909557246, 190111.1631032673], 
processed observation next is [1.0, 0.043478260869565216, 0.5118483412322274, 0.855, 1.0, 1.0, 0.4458572169426779, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20963108598812405, 0.20963108598812388, 0.2837480046317422], 
reward next is 0.7163, 
noisyNet noise sample is [array([0.62494063], dtype=float32), 1.9901899]. 
=============================================
[2019-03-27 02:58:26,091] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 02:58:26,093] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 02:58:26,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:58:26,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 02:58:26,096] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 02:58:26,099] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:58:26,099] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 02:58:26,100] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:58:26,101] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 02:58:26,101] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:58:26,102] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 02:58:26,138] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-27 02:58:26,158] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-27 02:58:26,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-27 02:58:26,214] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-27 02:58:26,246] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-27 02:58:34,308] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.12918091]
[2019-03-27 02:58:34,309] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.86666666666667, 72.83333333333333, 1.0, 2.0, 0.5812225832593766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869303.1758718659, 869303.1758718659, 204413.1685832147]
[2019-03-27 02:58:34,309] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 02:58:34,314] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.189367e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.5531411379646674
[2019-03-27 02:58:55,366] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.12918091]
[2019-03-27 02:58:55,367] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.45, 88.5, 1.0, 2.0, 0.4201363266885031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610519.9239026884, 610519.9239026884, 175051.2595800947]
[2019-03-27 02:58:55,368] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 02:58:55,375] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22359060910128992
[2019-03-27 02:59:03,213] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.12918091]
[2019-03-27 02:59:03,213] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.55884739333333, 80.62687243333335, 1.0, 2.0, 0.674059726668001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 942001.595375844, 942001.5953758435, 215344.1092854651]
[2019-03-27 02:59:03,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 02:59:03,217] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.2774014e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.23919810374335593
[2019-03-27 03:00:05,910] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.12918091]
[2019-03-27 03:00:05,910] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.37704577333334, 79.12529703666667, 1.0, 2.0, 0.3809027050748707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580249.262825588, 580249.262825588, 173083.4501195359]
[2019-03-27 03:00:05,911] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:00:05,916] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1564501e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.40451536701242874
[2019-03-27 03:00:19,903] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:00:20,591] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4238 2927327234.8387 1338.0000
[2019-03-27 03:00:20,708] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 03:00:20,798] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.5966 3164066227.5980 1779.0000
[2019-03-27 03:00:20,798] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.5669 2842424969.2684 1131.0000
[2019-03-27 03:00:21,815] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1600000, evaluation results [1600000.0, 7884.596601095898, 3164066227.5979652, 1779.0, 8254.423757959617, 2927327234.8387394, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8497.5669135121, 2842424969.2684097, 1131.0]
[2019-03-27 03:00:25,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1032166e-11 9.9999189e-01 3.7252037e-20 2.0570377e-08 8.0667505e-06], sum to 1.0000
[2019-03-27 03:00:25,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6520
[2019-03-27 03:00:25,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2047130.096565814 W.
[2019-03-27 03:00:25,063] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.83333333333334, 83.16666666666666, 1.0, 2.0, 0.7320492599396323, 1.0, 1.0, 0.7320492599396323, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2047130.096565814, 2047130.096565814, 388164.7960189444], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4261800.0000, 
sim time next is 4262400.0000, 
raw observation next is [31.0, 84.0, 1.0, 2.0, 0.7208129987716214, 1.0, 2.0, 0.7208129987716214, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2015679.032380346, 2015679.032380346, 383182.3662946142], 
processed observation next is [1.0, 0.34782608695652173, 0.6682464454976303, 0.84, 1.0, 1.0, 0.6636301190019535, 1.0, 1.0, 0.6636301190019535, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5599108423278739, 0.5599108423278739, 0.5719139795442003], 
reward next is 0.4281, 
noisyNet noise sample is [array([-0.87357736], dtype=float32), -2.247448]. 
=============================================
[2019-03-27 03:00:32,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6937758e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:00:32,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8089
[2019-03-27 03:00:32,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0633946e-27 1.0000000e+00 3.4296945e-38 1.9118416e-31 7.1155340e-30], sum to 1.0000
[2019-03-27 03:00:32,742] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.4954519769395319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692315.0894553369, 692315.0894553375, 182886.5316965814], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4822200.0000, 
sim time next is 4822800.0000, 
raw observation next is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.492663254023711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688417.0314402857, 688417.0314402852, 182454.5536855476], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7266666666666666, 1.0, 1.0, 0.3887509084623024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19122695317785715, 0.19122695317785698, 0.27232022938141437], 
reward next is 0.7277, 
noisyNet noise sample is [array([-0.5825225], dtype=float32), 0.51574177]. 
=============================================
[2019-03-27 03:00:32,745] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5495
[2019-03-27 03:00:32,751] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 91.5, 1.0, 2.0, 0.9047999428778082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129226059842, 1264654.139598119, 1264654.139598119, 271262.4333311477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4674600.0000, 
sim time next is 4675200.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.8683405035299878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956501965, 1213665.039919972, 1213665.039919971, 261378.6570878887], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.8413741006385396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451107307, 0.33712917775554774, 0.3371291777555475, 0.39011739863863987], 
reward next is 0.6099, 
noisyNet noise sample is [array([-2.7245157], dtype=float32), -0.821919]. 
=============================================
[2019-03-27 03:00:44,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6847511e-13 7.2873779e-02 5.0279550e-20 1.9954199e-10 9.2712617e-01], sum to 1.0000
[2019-03-27 03:00:44,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1127
[2019-03-27 03:00:44,972] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.4989835318860064, 1.0, 2.0, 0.4989835318860064, 1.0, 2.0, 0.8630286869827752, 6.9112, 6.9112, 170.5573041426782, 2093109.362706628, 2093109.362706628, 413806.351115196], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4876800.0000, 
sim time next is 4877400.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.5009672261858708, 1.0, 2.0, 0.5009672261858708, 1.0, 2.0, 0.8669434873613071, 6.9112, 6.9112, 170.5573041426782, 2101438.619051258, 2101438.619051258, 415260.3436521877], 
processed observation next is [1.0, 0.43478260869565216, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.39875569419984425, 1.0, 1.0, 0.39875569419984425, 1.0, 1.0, 0.837735960196716, 0.0, 0.0, 0.8375144448122397, 0.5837329497364606, 0.5837329497364606, 0.6197915576898323], 
reward next is 0.3802, 
noisyNet noise sample is [array([2.140402], dtype=float32), -0.50477564]. 
=============================================
[2019-03-27 03:00:46,906] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3278002e-15 1.0000000e+00 1.6347043e-24 9.9079953e-16 1.1358872e-09], sum to 1.0000
[2019-03-27 03:00:46,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6412
[2019-03-27 03:00:46,922] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 87.33333333333334, 1.0, 2.0, 0.9252187122812912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1293211.179463326, 1293211.179463327, 276969.1998545301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4688400.0000, 
sim time next is 4689000.0000, 
raw observation next is [27.5, 86.5, 1.0, 2.0, 0.9592886103637915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340861.963088486, 1340861.963088486, 286767.4255070675], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.865, 1.0, 1.0, 0.9509501329684236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37246165641346834, 0.37246165641346834, 0.42801108284636946], 
reward next is 0.5720, 
noisyNet noise sample is [array([-0.42300406], dtype=float32), 0.59047365]. 
=============================================
[2019-03-27 03:00:46,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.73771 ]
 [48.78873 ]
 [49.559265]
 [49.760315]
 [49.938877]], R is [[48.8188324 ]
 [48.9172554 ]
 [49.03127289]
 [49.22567368]
 [49.42049408]].
[2019-03-27 03:00:47,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3868937e-26 1.0000000e+00 1.2332201e-38 4.0028062e-33 7.3750152e-27], sum to 1.0000
[2019-03-27 03:00:47,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2835
[2019-03-27 03:00:47,098] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 74.83333333333334, 1.0, 2.0, 0.4895930152108962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684125.4889527609, 684125.4889527604, 181981.4572746074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4901598713567438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684917.8322771245, 684917.8322771239, 182068.4723652152], 
processed observation next is [1.0, 0.8695652173913043, 0.5102685624012641, 0.7566666666666667, 1.0, 1.0, 0.3857347847671613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19025495341031237, 0.1902549534103122, 0.2717439886047988], 
reward next is 0.7283, 
noisyNet noise sample is [array([0.43953943], dtype=float32), -1.1628003]. 
=============================================
[2019-03-27 03:00:56,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2571586e-21 1.0000000e+00 1.6765751e-32 4.4538112e-28 1.5751220e-18], sum to 1.0000
[2019-03-27 03:00:56,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2008
[2019-03-27 03:00:56,976] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 71.33333333333334, 1.0, 2.0, 0.4976041737908742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695323.4254254154, 695323.4254254154, 183221.5086271375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [28.5, 72.0, 1.0, 2.0, 0.4954519769395319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692315.0894553369, 692315.0894553375, 182886.5316965814], 
processed observation next is [1.0, 0.8260869565217391, 0.5497630331753555, 0.72, 1.0, 1.0, 0.39211081558979743, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19230974707092693, 0.19230974707092707, 0.27296497268146475], 
reward next is 0.7270, 
noisyNet noise sample is [array([-2.0692031], dtype=float32), -0.22448792]. 
=============================================
[2019-03-27 03:00:58,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5178932e-24 1.0000000e+00 2.5652306e-35 7.8955172e-31 1.8127067e-24], sum to 1.0000
[2019-03-27 03:00:58,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3239
[2019-03-27 03:00:58,858] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 79.5, 1.0, 2.0, 0.5479810284523111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765742.5145672448, 765742.5145672441, 191454.9592761847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5257800.0000, 
sim time next is 5258400.0000, 
raw observation next is [28.66666666666667, 79.66666666666666, 1.0, 2.0, 0.5469552025755323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764308.521315797, 764308.5213157964, 191279.8565869479], 
processed observation next is [1.0, 0.8695652173913043, 0.5576619273301741, 0.7966666666666665, 1.0, 1.0, 0.45416289466931603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21230792258772138, 0.21230792258772122, 0.28549232326410134], 
reward next is 0.7145, 
noisyNet noise sample is [array([-1.1428126], dtype=float32), -2.0841525]. 
=============================================
[2019-03-27 03:00:59,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1069885e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6854570e-36], sum to 1.0000
[2019-03-27 03:00:59,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9790
[2019-03-27 03:00:59,036] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 65.66666666666667, 1.0, 2.0, 0.5698275538916122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796282.0459488718, 796282.0459488718, 195259.7981477057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5142000.0000, 
sim time next is 5142600.0000, 
raw observation next is [32.0, 65.0, 1.0, 2.0, 0.5646492078273323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789043.0917657722, 789043.0917657722, 194345.3254130156], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.65, 1.0, 1.0, 0.4754809732859425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2191786366016034, 0.2191786366016034, 0.2900676498701726], 
reward next is 0.7099, 
noisyNet noise sample is [array([0.28812423], dtype=float32), 0.8035224]. 
=============================================
[2019-03-27 03:01:00,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9709214e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9695615e-37], sum to 1.0000
[2019-03-27 03:01:00,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9302
[2019-03-27 03:01:00,365] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5068745005311205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708281.5648995986, 708281.564899598, 184680.3678584687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5055298757377807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706402.0274291529, 706402.0274291529, 184467.3025625779], 
processed observation next is [0.0, 0.43478260869565216, 0.6208530805687204, 0.66, 1.0, 1.0, 0.40425286233467556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1962227853969869, 0.1962227853969869, 0.2753243321829521], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.61666465], dtype=float32), -0.39868736]. 
=============================================
[2019-03-27 03:01:02,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7667619e-11 3.3359766e-01 1.1932067e-19 4.6881995e-12 6.6640240e-01], sum to 1.0000
[2019-03-27 03:01:02,141] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1011
[2019-03-27 03:01:02,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2101438.619051258 W.
[2019-03-27 03:01:02,157] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.5009672261858708, 1.0, 2.0, 0.5009672261858708, 1.0, 1.0, 0.8664135485713671, 6.9112, 6.9112, 170.5573041426782, 2101438.619051258, 2101438.619051258, 415164.3636424851], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4877400.0000, 
sim time next is 4878000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8209248802617143, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992784572157149, 6.9112, 168.9124705654728, 2044358.998233168, 1986480.261332185, 413569.0354553328], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7842468436888124, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008158457215714865, 0.0, 0.8294375589400914, 0.5678774995092133, 0.5518000725922736, 0.6172672170975116], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21185471], dtype=float32), 1.2356805]. 
=============================================
[2019-03-27 03:01:02,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[54.728554]
 [54.241722]
 [53.81116 ]
 [53.99121 ]
 [53.836437]], R is [[54.37458038]
 [54.21118546]
 [54.07867813]
 [53.53789139]
 [53.00251389]].
[2019-03-27 03:01:11,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4799268e-24 1.0000000e+00 1.9684988e-33 5.7819816e-28 6.4147487e-24], sum to 1.0000
[2019-03-27 03:01:11,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6162
[2019-03-27 03:01:11,955] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666666, 84.66666666666667, 1.0, 2.0, 0.6033677270109657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 843169.9827363336, 843169.9827363342, 201375.3104054302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5359200.0000, 
sim time next is 5359800.0000, 
raw observation next is [29.53333333333333, 84.83333333333334, 1.0, 2.0, 0.6028952230599088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842509.425098895, 842509.4250988944, 201286.8559738267], 
processed observation next is [1.0, 0.0, 0.598736176935229, 0.8483333333333334, 1.0, 1.0, 0.5215605097107335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23403039586080415, 0.23403039586080399, 0.30042814324451744], 
reward next is 0.6996, 
noisyNet noise sample is [array([-0.96594185], dtype=float32), 1.1420257]. 
=============================================
[2019-03-27 03:01:17,732] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 03:01:17,735] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:01:17,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:01:17,736] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:01:17,740] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:01:17,740] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:01:17,741] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:01:17,737] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:01:17,743] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:01:17,745] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:01:17,742] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:01:17,771] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-27 03:01:17,793] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-27 03:01:17,794] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-27 03:01:17,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-27 03:01:17,868] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-27 03:01:27,278] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13034938]
[2019-03-27 03:01:27,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.36666666666667, 64.5, 1.0, 2.0, 0.3307820616167551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539556.480864077, 539556.4808640777, 169936.0921577853]
[2019-03-27 03:01:27,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:01:27,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7823278e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.06787033484073768
[2019-03-27 03:01:44,607] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13034938]
[2019-03-27 03:01:44,610] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.43333333333333, 92.66666666666666, 1.0, 2.0, 0.404684953429479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598239.8948822488, 598239.8948822494, 174210.4523949157]
[2019-03-27 03:01:44,611] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:01:44,614] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8326612996375572
[2019-03-27 03:02:20,206] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13034938]
[2019-03-27 03:02:20,206] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.01666666666667, 55.33333333333334, 1.0, 2.0, 0.6964491829027027, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975061815287, 6.9112, 168.9123159370428, 1870159.958174031, 1802923.522399986, 383881.9879439936]
[2019-03-27 03:02:20,207] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:02:20,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2778884e-22 1.0000000e+00 4.0340642e-32 1.2441049e-26 7.0578731e-22], sampled 0.14221519703574415
[2019-03-27 03:02:20,213] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1870159.958174031 W.
[2019-03-27 03:02:40,748] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13034938]
[2019-03-27 03:02:40,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.26666666666667, 87.66666666666666, 1.0, 2.0, 0.5021355239562239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701657.3644563508, 701657.3644563508, 183931.39000548]
[2019-03-27 03:02:40,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:02:40,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2331890263119034
[2019-03-27 03:02:59,470] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13034938]
[2019-03-27 03:02:59,472] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.11129635333334, 78.93907732666668, 1.0, 2.0, 0.5619010055509484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805306.1761657817, 805306.1761657817, 196431.704058636]
[2019-03-27 03:02:59,472] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:02:59,476] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.02312073923159874
[2019-03-27 03:03:12,233] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.2477 2842437882.3074 1137.0000
[2019-03-27 03:03:12,711] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7281 2779199535.5119 933.0000
[2019-03-27 03:03:12,729] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:03:12,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.1842 3164401439.4729 1842.0000
[2019-03-27 03:03:12,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2646 3007609417.0562 1775.0000
[2019-03-27 03:03:13,776] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1625000, evaluation results [1625000.0, 7875.184244343617, 3164401439.472924, 1842.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.728066421085, 2779199535.5119066, 933.0, 7998.26455629416, 3007609417.056184, 1775.0, 8496.247723344004, 2842437882.307381, 1137.0]
[2019-03-27 03:03:20,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1128821e-18 1.0000000e+00 3.3606711e-27 1.0285891e-20 2.5226335e-17], sum to 1.0000
[2019-03-27 03:03:20,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1681
[2019-03-27 03:03:20,117] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 82.0, 1.0, 2.0, 1.007630450325946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1408477.416688641, 1408477.416688641, 301261.517850252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5558400.0000, 
sim time next is 5559000.0000, 
raw observation next is [29.0, 80.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.981440663791823, 6.9112, 168.9124306376941, 1503620.042500017, 1453789.053247153, 311352.4929811025], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.8083333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.007024066379182336, 0.0, 0.8294373628764189, 0.4176722340277825, 0.4038302925686536, 0.4647052134046306], 
reward next is 0.1841, 
noisyNet noise sample is [array([-0.38514185], dtype=float32), -0.07099236]. 
=============================================
[2019-03-27 03:03:20,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[35.982235]
 [36.3485  ]
 [36.18104 ]
 [35.70837 ]
 [35.842834]], R is [[35.6390152 ]
 [35.83298492]
 [36.0857048 ]
 [36.33190918]
 [36.5434761 ]].
[2019-03-27 03:03:21,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1583513e-17 1.0000000e+00 1.9086185e-26 5.6765121e-20 2.3973304e-14], sum to 1.0000
[2019-03-27 03:03:21,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6571
[2019-03-27 03:03:21,758] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 80.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.981440663791823, 6.9112, 168.9124306376941, 1503620.042500017, 1453789.053247153, 311352.4929811025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5559000.0000, 
sim time next is 5559600.0000, 
raw observation next is [29.2, 79.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.345313430236915, 6.9112, 168.9047713947167, 2471784.636362526, 1454424.511791837, 310804.4880376042], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.7966666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.14341134302369155, 0.0, 0.8293997524869833, 0.686606843434035, 0.4040068088310658, 0.4638872955785137], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.28567436], dtype=float32), -0.40344647]. 
=============================================
[2019-03-27 03:03:22,696] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.3362403e-13 4.0245354e-09 1.1553274e-20 1.3937149e-10 1.0000000e+00], sum to 1.0000
[2019-03-27 03:03:22,705] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2464
[2019-03-27 03:03:22,712] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.11666666666667, 52.83333333333334, 1.0, 2.0, 0.788882497011246, 1.0, 2.0, 0.7150312880198856, 1.0, 2.0, 1.03, 7.005104741505076, 6.9112, 170.5573041426782, 3000486.972042491, 2933219.185901743, 550872.4146262114], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5320200.0000, 
sim time next is 5320800.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 0.7131634214447364, 1.0, 2.0, 0.6771717502366308, 1.0, 2.0, 1.03, 7.005098770102461, 6.9112, 170.5573041426782, 2841436.232839439, 2774172.724257286, 525519.7452838859], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 0.6544137607767909, 1.0, 1.0, 0.6110503014899166, 1.0, 1.0, 1.0365853658536586, 0.009389877010246118, 0.0, 0.8375144448122397, 0.7892878424553997, 0.7706035345159128, 0.7843578287819192], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13141297], dtype=float32), 1.8554648]. 
=============================================
[2019-03-27 03:03:26,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.148423e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:03:26,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0246
[2019-03-27 03:03:26,769] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 87.5, 1.0, 2.0, 0.5386528356464708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752702.7845720698, 752702.7845720698, 189873.4473849325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5787000.0000, 
sim time next is 5787600.0000, 
raw observation next is [27.13333333333333, 87.66666666666667, 1.0, 2.0, 0.5386799251324969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752740.6522951062, 752740.6522951062, 189878.0200447449], 
processed observation next is [0.0, 1.0, 0.484992101105845, 0.8766666666666667, 1.0, 1.0, 0.4441926808825263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2090946256375295, 0.2090946256375295, 0.28340002991752966], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.0859045], dtype=float32), -0.7105487]. 
=============================================
[2019-03-27 03:03:30,095] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1698230e-23 1.0000000e+00 3.2441798e-34 2.6989833e-28 2.6573998e-23], sum to 1.0000
[2019-03-27 03:03:30,104] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9169
[2019-03-27 03:03:30,108] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 90.66666666666667, 1.0, 2.0, 0.5475336966681766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765117.1930401626, 765117.193040162, 191378.6003513641], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5530800.0000, 
sim time next is 5531400.0000, 
raw observation next is [26.95, 91.0, 1.0, 2.0, 0.5467395098928274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764007.0065990146, 764007.006599014, 191243.0294276118], 
processed observation next is [1.0, 0.0, 0.476303317535545, 0.91, 1.0, 1.0, 0.4539030239672619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21222416849972628, 0.2122241684997261, 0.2854373573546445], 
reward next is 0.7146, 
noisyNet noise sample is [array([-1.3745329], dtype=float32), -1.4320403]. 
=============================================
[2019-03-27 03:03:38,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5818393e-14 1.5090890e-05 2.9362719e-23 4.5965913e-14 9.9998486e-01], sum to 1.0000
[2019-03-27 03:03:38,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8394
[2019-03-27 03:03:38,883] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 62.33333333333334, 1.0, 2.0, 0.5641246371384144, 1.0, 2.0, 0.5641246371384144, 1.0, 2.0, 0.9796979358411376, 6.911199999999999, 6.9112, 170.5573041426782, 2366637.355017963, 2366637.355017964, 462321.5034632798], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5844000.0000, 
sim time next is 5844600.0000, 
raw observation next is [32.45, 62.5, 1.0, 2.0, 0.5581596910885169, 1.0, 2.0, 0.5581596910885169, 1.0, 2.0, 0.969338797190268, 6.911200000000001, 6.9112, 170.5573041426782, 2341589.53907837, 2341589.539078369, 457694.3854347192], 
processed observation next is [1.0, 0.6521739130434783, 0.7369668246445499, 0.625, 1.0, 1.0, 0.4676622784198999, 1.0, 1.0, 0.4676622784198999, 1.0, 1.0, 0.9626082892564245, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6504415386328806, 0.6504415386328802, 0.6831259484100287], 
reward next is 0.3169, 
noisyNet noise sample is [array([0.86970997], dtype=float32), 1.6726652]. 
=============================================
[2019-03-27 03:03:40,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4812037e-18 1.0000000e+00 6.0100628e-28 1.2193610e-21 4.7619328e-15], sum to 1.0000
[2019-03-27 03:03:40,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7279
[2019-03-27 03:03:40,066] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 91.0, 1.0, 2.0, 0.680065376765337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 950398.2746481972, 950398.2746481972, 216601.6165847068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5896800.0000, 
sim time next is 5897400.0000, 
raw observation next is [27.0, 90.16666666666667, 1.0, 2.0, 0.6862671686628288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959069.2553212187, 959069.2553212187, 217909.5682786445], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9016666666666667, 1.0, 1.0, 0.6220086369431672, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2664081264781163, 0.2664081264781163, 0.32523816160991714], 
reward next is 0.6748, 
noisyNet noise sample is [array([1.3375072], dtype=float32), 0.5868501]. 
=============================================
[2019-03-27 03:03:53,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0704686e-16 1.0000000e+00 2.0566688e-24 1.3549425e-18 5.2036453e-10], sum to 1.0000
[2019-03-27 03:03:53,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6116
[2019-03-27 03:03:53,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1691581.397378322 W.
[2019-03-27 03:03:53,598] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.45, 91.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.246208015435682, 6.9112, 168.911166809926, 1691581.397378322, 1453917.699311063, 311349.9358253386], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5800200.0000, 
sim time next is 5800800.0000, 
raw observation next is [26.4, 91.66666666666667, 1.0, 2.0, 0.5554703729525513, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9452052355189744, 6.9112, 6.9112, 168.9126837324301, 1552991.373466539, 1552991.373466539, 335848.4902254333], 
processed observation next is [1.0, 0.13043478260869565, 0.45023696682464454, 0.9166666666666667, 1.0, 1.0, 0.4644221360874112, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9331771164865541, 0.0, 0.0, 0.82943860568744, 0.43138649262959416, 0.43138649262959416, 0.5012664033215423], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.66997224], dtype=float32), 0.36739838]. 
=============================================
[2019-03-27 03:04:10,080] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 03:04:10,083] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:04:10,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:04:10,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:04:10,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:04:10,089] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:04:10,090] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:04:10,090] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:04:10,091] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:04:10,092] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:04:10,094] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:04:10,123] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-27 03:04:10,143] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-27 03:04:10,162] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-27 03:04:10,184] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-27 03:04:10,202] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-27 03:04:24,371] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:04:24,372] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.52966115, 73.58486394, 1.0, 2.0, 0.2386322913161334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395463.8784204829, 395463.8784204835, 159647.1351563344]
[2019-03-27 03:04:24,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:04:24,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12644411055266125
[2019-03-27 03:04:45,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:04:45,855] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.2709564, 76.91279052333333, 1.0, 2.0, 1.004591953639554, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564119162, 1404227.362003821, 1404227.362003821, 300324.4998495929]
[2019-03-27 03:04:45,856] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:04:45,858] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.02572467e-25 1.00000000e+00 1.27369037e-34 1.62669881e-31
 1.30107515e-23], sampled 0.559610085795242
[2019-03-27 03:04:51,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:04:51,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.33333333333334, 98.0, 1.0, 2.0, 0.385555475527079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578518.6681064686, 578518.6681064686, 172675.5154239802]
[2019-03-27 03:04:51,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:04:51,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.6483863e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8169056977205063
[2019-03-27 03:04:55,655] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:04:55,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.80554950666667, 98.80392999666667, 1.0, 2.0, 0.2985021409872889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478357.2561381347, 478357.2561381347, 165538.8237053787]
[2019-03-27 03:04:55,657] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:04:55,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4566990e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7380581e-38], sampled 0.9295604146861134
[2019-03-27 03:04:59,142] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:04:59,142] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.60087160666667, 49.85561973833333, 1.0, 2.0, 0.5423219035067058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757831.6961489989, 757831.6961489995, 190492.8602618537]
[2019-03-27 03:04:59,144] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:04:59,148] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0125820e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.8614523e-37], sampled 0.37366675594727894
[2019-03-27 03:05:15,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:05:15,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.0, 60.0, 1.0, 2.0, 0.6560437914609386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916813.3750503162, 916813.3750503155, 211648.5110980491]
[2019-03-27 03:05:15,222] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:05:15,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0955433e-29 1.0000000e+00 0.0000000e+00 2.9333845e-37 2.1580295e-30], sampled 0.2934407117134271
[2019-03-27 03:05:22,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:05:22,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.7, 41.66666666666667, 1.0, 2.0, 0.5771121499623458, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9996619748182095, 6.911199999999999, 6.9112, 168.9126994025565, 1613543.729886544, 1613543.729886545, 352607.3942962335]
[2019-03-27 03:05:22,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:05:22,303] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6202848e-16 1.0000000e+00 7.7610392e-26 2.9049169e-18 2.8393252e-10], sampled 0.16030973651960378
[2019-03-27 03:05:50,035] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:05:50,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 70.33333333333334, 1.0, 2.0, 0.5218430860897018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729205.1114161616, 729205.1114161616, 187088.3936533613]
[2019-03-27 03:05:50,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:05:50,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.801194e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.3187790129190853
[2019-03-27 03:05:52,419] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:05:52,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.6, 80.66666666666667, 1.0, 2.0, 0.5924255222412755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827872.9527046868, 827872.9527046868, 199342.7228522658]
[2019-03-27 03:05:52,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:05:52,424] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5376516e-29 1.0000000e+00 0.0000000e+00 2.8115114e-37 6.8342219e-32], sampled 0.7976671963186217
[2019-03-27 03:05:55,905] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:05:55,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 93.66666666666667, 1.0, 2.0, 0.3452236390077541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532743.9537262849, 532743.9537262843, 169251.9696557469]
[2019-03-27 03:05:55,907] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:05:55,909] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7242915092823612
[2019-03-27 03:06:01,012] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13406473]
[2019-03-27 03:06:01,013] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.2, 58.0, 1.0, 2.0, 0.3489873638725435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553383.23382466, 553383.2338246593, 171229.6449429682]
[2019-03-27 03:06:01,014] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:06:01,015] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7982599e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.42368425625001427
[2019-03-27 03:06:01,207] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.0889 3163907142.8838 1855.0000
[2019-03-27 03:06:01,325] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.7581 2842400587.5795 1155.0000
[2019-03-27 03:06:01,334] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.4962 2927199261.7645 1324.0000
[2019-03-27 03:06:01,428] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2790 2779076634.3977 931.0000
[2019-03-27 03:06:01,492] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8007.7536 3007377665.3249 1749.0000
[2019-03-27 03:06:02,509] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1650000, evaluation results [1650000.0, 7882.088912323633, 3163907142.88383, 1855.0, 8262.496239142049, 2927199261.7645364, 1324.0, 8661.279023181141, 2779076634.397678, 931.0, 8007.753583299829, 3007377665.324934, 1749.0, 8494.758102713646, 2842400587.579516, 1155.0]
[2019-03-27 03:06:04,793] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2365769e-27 1.0000000e+00 1.0475881e-36 8.8424066e-35 1.2837231e-28], sum to 1.0000
[2019-03-27 03:06:04,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0746
[2019-03-27 03:06:04,806] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333334, 94.33333333333334, 1.0, 2.0, 0.5131583374818991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717065.2541081898, 717065.2541081905, 185681.7564081294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6673200.0000, 
sim time next is 6673800.0000, 
raw observation next is [25.01666666666667, 94.16666666666667, 1.0, 2.0, 0.5159751547977751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721002.6885459501, 721002.6885459494, 186135.2218646462], 
processed observation next is [1.0, 0.21739130434782608, 0.3846761453396526, 0.9416666666666668, 1.0, 1.0, 0.41683753590093386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20027852459609724, 0.20027852459609705, 0.2778137639770839], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.9165548], dtype=float32), 0.54947746]. 
=============================================
[2019-03-27 03:06:09,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.665554e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:06:09,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3781
[2019-03-27 03:06:09,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666666, 90.83333333333334, 1.0, 2.0, 0.5222044214293757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729710.2020953537, 729710.202095353, 187148.3660790444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6227400.0000, 
sim time next is 6228000.0000, 
raw observation next is [26.4, 91.0, 1.0, 2.0, 0.5218648377209403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729235.516811192, 729235.5168111913, 187092.9816656346], 
processed observation next is [0.0, 0.08695652173913043, 0.45023696682464454, 0.91, 1.0, 1.0, 0.4239335394228196, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20256542133644223, 0.20256542133644204, 0.27924325621736507], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.6658274], dtype=float32), 0.2176134]. 
=============================================
[2019-03-27 03:06:09,908] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.570076]
 [70.712685]
 [70.81699 ]
 [70.990616]
 [71.08154 ]], R is [[70.7020874 ]
 [70.71573639]
 [70.72931671]
 [70.74303436]
 [70.75682068]].
[2019-03-27 03:06:17,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:06:17,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4363
[2019-03-27 03:06:17,449] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 76.5, 1.0, 2.0, 0.3844108377844545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603003.0325276207, 603003.0325276207, 175421.6376671486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6767400.0000, 
sim time next is 6768000.0000, 
raw observation next is [23.8, 76.0, 1.0, 2.0, 0.3695941449649225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 578925.4032060098, 578925.4032060092, 173304.9593922844], 
processed observation next is [1.0, 0.34782608695652173, 0.3270142180094788, 0.76, 1.0, 1.0, 0.24047487345171384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16081261200166938, 0.16081261200166921, 0.25866411849594684], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.73837835], dtype=float32), -1.1138119]. 
=============================================
[2019-03-27 03:06:17,466] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.10994 ]
 [66.376396]
 [66.582565]
 [66.61445 ]
 [66.49892 ]], R is [[66.4307785 ]
 [66.5046463 ]
 [66.57963562]
 [66.65646362]
 [66.73487854]].
[2019-03-27 03:06:18,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6894415e-24 1.0000000e+00 1.3031094e-34 2.8140738e-31 4.2441599e-25], sum to 1.0000
[2019-03-27 03:06:18,105] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4294
[2019-03-27 03:06:18,112] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 56.0, 1.0, 2.0, 0.8572734533926873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1331977.314721376, 1331977.314721376, 276348.2852844648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6777600.0000, 
sim time next is 6778200.0000, 
raw observation next is [27.76666666666667, 55.0, 1.0, 2.0, 0.911725480399462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1415360.450796674, 1415360.450796674, 292633.9081865932], 
processed observation next is [1.0, 0.43478260869565216, 0.515007898894155, 0.55, 1.0, 1.0, 0.8936451571077856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.39315568077685387, 0.39315568077685387, 0.436767027144169], 
reward next is 0.5632, 
noisyNet noise sample is [array([1.6547772], dtype=float32), -0.52546394]. 
=============================================
[2019-03-27 03:06:26,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8273744e-18 1.0000000e+00 4.5310491e-27 1.2416001e-21 8.9995502e-14], sum to 1.0000
[2019-03-27 03:06:26,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1341
[2019-03-27 03:06:26,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.98333333333333, 47.5, 1.0, 2.0, 0.6511521521650555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1019423.675422215, 1019423.675422215, 223797.3876104732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6786600.0000, 
sim time next is 6787200.0000, 
raw observation next is [29.06666666666667, 47.0, 1.0, 2.0, 0.8560152899144252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1341146.278708486, 1341146.278708485, 277200.9394247295], 
processed observation next is [1.0, 0.5652173913043478, 0.5766192733017379, 0.47, 1.0, 1.0, 0.8265244456800304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3725406329745794, 0.3725406329745792, 0.413732745410044], 
reward next is 0.5863, 
noisyNet noise sample is [array([-0.7472134], dtype=float32), 0.3657751]. 
=============================================
[2019-03-27 03:06:28,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4453406e-22 1.0000000e+00 1.8628702e-33 8.6958144e-30 5.3618257e-18], sum to 1.0000
[2019-03-27 03:06:28,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8955
[2019-03-27 03:06:28,838] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333334, 85.0, 1.0, 2.0, 0.517155402362409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722652.4794647552, 722652.4794647545, 186327.840041978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6636000.0000, 
sim time next is 6636600.0000, 
raw observation next is [27.1, 85.0, 1.0, 2.0, 0.5160589875767615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721119.8728230798, 721119.8728230791, 186150.6386590498], 
processed observation next is [1.0, 0.8260869565217391, 0.4834123222748816, 0.85, 1.0, 1.0, 0.41693853924911023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20031107578418883, 0.20031107578418864, 0.27783677411798474], 
reward next is 0.7222, 
noisyNet noise sample is [array([-1.2341995], dtype=float32), 0.6043208]. 
=============================================
[2019-03-27 03:06:31,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:06:31,143] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7294
[2019-03-27 03:06:31,147] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 84.5, 1.0, 2.0, 0.3469345609765446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 551418.8007037522, 551418.8007037516, 171077.5467888869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6751800.0000, 
sim time next is 6752400.0000, 
raw observation next is [21.9, 84.66666666666667, 1.0, 2.0, 0.3306257402689252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525818.6063668791, 525818.6063668791, 169039.1116392021], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.8466666666666667, 1.0, 1.0, 0.19352498827581352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14606072399079975, 0.14606072399079975, 0.2522971815510479], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.29665363], dtype=float32), 0.963823]. 
=============================================
[2019-03-27 03:06:45,270] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2572238e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:06:45,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1643
[2019-03-27 03:06:45,285] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 81.0, 1.0, 2.0, 0.537309990994946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765116.9800715566, 765116.9800715559, 191474.8870578634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7110000.0000, 
sim time next is 7110600.0000, 
raw observation next is [26.16666666666667, 80.16666666666667, 1.0, 2.0, 0.50004453226819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711263.0992687366, 711263.0992687372, 185205.4327044014], 
processed observation next is [1.0, 0.30434782608695654, 0.4391785150078992, 0.8016666666666667, 1.0, 1.0, 0.3976440147809518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1975730831302046, 0.1975730831302048, 0.27642601896179314], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.06873824], dtype=float32), -0.94580865]. 
=============================================
[2019-03-27 03:06:58,520] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 03:06:58,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:06:58,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:58,525] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:06:58,526] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:06:58,527] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:58,528] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:58,528] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:06:58,530] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:06:58,532] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:58,532] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:06:58,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-27 03:06:58,583] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-27 03:06:58,606] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-27 03:06:58,607] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-27 03:06:58,607] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-27 03:07:40,263] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:07:40,265] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.45535284, 100.0, 1.0, 2.0, 0.2920057100804507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470167.9025119431, 470167.9025119431, 164971.9369485484]
[2019-03-27 03:07:40,267] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:07:40,270] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.330688341062767
[2019-03-27 03:07:41,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:07:41,310] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.350244189542749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539528.9942654476, 539528.9942654483, 169777.4929953436]
[2019-03-27 03:07:41,311] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:07:41,313] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38787949300151037
[2019-03-27 03:07:47,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:07:47,497] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.13056492666666, 59.56577061333333, 1.0, 2.0, 0.5847222837581806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817104.0786987327, 817104.078698732, 197935.5625539802]
[2019-03-27 03:07:47,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:07:47,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9029086e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.07127427019945676
[2019-03-27 03:08:00,342] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:08:00,344] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.20945584, 76.20860677, 1.0, 2.0, 0.6883841724682689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962029.141738455, 962029.141738455, 218369.1271548247]
[2019-03-27 03:08:00,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:08:00,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.5415387e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8174377e-37], sampled 0.7432808230099347
[2019-03-27 03:08:08,109] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:08:08,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.259992635, 88.95697168000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.469644490660508, 6.9112, 168.9097725617251, 1850198.557053267, 1454026.283048845, 311348.6242355703]
[2019-03-27 03:08:08,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:08:08,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4576676e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6643292808713761
[2019-03-27 03:08:08,114] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1850198.557053267 W.
[2019-03-27 03:08:49,314] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.13803409]
[2019-03-27 03:08:49,317] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.85, 69.0, 1.0, 2.0, 0.88916076754004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1242782.206781854, 1242782.206781854, 266975.1145751813]
[2019-03-27 03:08:49,319] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:08:49,322] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7240773e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5544842e-36], sampled 0.8676995809293054
[2019-03-27 03:08:53,511] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.2668 3163933218.1750 1781.0000
[2019-03-27 03:08:53,699] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:08:53,759] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-27 03:08:53,829] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:08:53,905] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.1592 2842458080.1189 1133.0000
[2019-03-27 03:08:54,922] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1675000, evaluation results [1675000.0, 7885.266798794063, 3163933218.1749697, 1781.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8497.159200217699, 2842458080.1189265, 1133.0]
[2019-03-27 03:08:59,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4897161e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7561346e-33], sum to 1.0000
[2019-03-27 03:08:59,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0452
[2019-03-27 03:08:59,926] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 73.83333333333334, 1.0, 2.0, 0.3854389143278537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581655.8441599511, 581655.8441599518, 173056.399844698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7332600.0000, 
sim time next is 7333200.0000, 
raw observation next is [25.4, 74.0, 1.0, 2.0, 0.3807836591491494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575343.7028946707, 575343.7028946707, 172514.4012383298], 
processed observation next is [1.0, 0.9130434782608695, 0.4028436018957346, 0.74, 1.0, 1.0, 0.2539562158423487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15981769524851963, 0.15981769524851963, 0.257484180952731], 
reward next is 0.7425, 
noisyNet noise sample is [array([0.5681886], dtype=float32), -0.36667222]. 
=============================================
[2019-03-27 03:09:01,576] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8401325e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:09:01,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2209
[2019-03-27 03:09:01,594] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 89.66666666666666, 1.0, 2.0, 0.5690731935054382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795227.5014856862, 795227.5014856867, 195121.7565226524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7191600.0000, 
sim time next is 7192200.0000, 
raw observation next is [26.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5704420546150114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797141.0768989144, 797141.0768989144, 195364.6506978352], 
processed observation next is [1.0, 0.21739130434782608, 0.45339652448657203, 0.8933333333333333, 1.0, 1.0, 0.48246030676507395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22142807691636512, 0.22142807691636512, 0.2915890308922913], 
reward next is 0.7084, 
noisyNet noise sample is [array([-1.7407192], dtype=float32), 0.98281825]. 
=============================================
[2019-03-27 03:09:01,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4349009e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2231883e-32], sum to 1.0000
[2019-03-27 03:09:01,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8969
[2019-03-27 03:09:01,686] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.95, 91.66666666666667, 1.0, 2.0, 0.47725436230152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666878.844759051, 666878.8447590516, 180108.6580262962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7692600.0000, 
sim time next is 7693200.0000, 
raw observation next is [24.9, 92.0, 1.0, 2.0, 0.4764218507735118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665715.192146436, 665715.192146436, 179983.9668782419], 
processed observation next is [1.0, 0.043478260869565216, 0.3791469194312796, 0.92, 1.0, 1.0, 0.36918295273917084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18492088670734333, 0.18492088670734333, 0.26863278638543564], 
reward next is 0.7314, 
noisyNet noise sample is [array([-0.565639], dtype=float32), -1.5531964]. 
=============================================
[2019-03-27 03:09:03,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8246192e-27 1.0000000e+00 8.8149826e-37 2.1628359e-36 5.7796561e-24], sum to 1.0000
[2019-03-27 03:09:03,433] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4796
[2019-03-27 03:09:03,442] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 85.83333333333334, 1.0, 2.0, 0.5740907330426579, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802241.6998434637, 802241.6998434644, 196012.2042412717], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7146600.0000, 
sim time next is 7147200.0000, 
raw observation next is [26.1, 85.66666666666667, 1.0, 2.0, 0.4558448453866579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636953.8500875017, 636953.8500875017, 176968.3052060626], 
processed observation next is [1.0, 0.7391304347826086, 0.4360189573459717, 0.8566666666666667, 1.0, 1.0, 0.34439137998392516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17693162502430604, 0.17693162502430604, 0.2641317988150188], 
reward next is 0.7359, 
noisyNet noise sample is [array([1.2456027], dtype=float32), 1.8092144]. 
=============================================
[2019-03-27 03:09:08,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1681258e-20 1.0000000e+00 3.2918147e-33 3.9739395e-28 5.2417592e-17], sum to 1.0000
[2019-03-27 03:09:08,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8637
[2019-03-27 03:09:08,379] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 88.0, 1.0, 2.0, 0.8091169421714807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564791188, 1194272.863154518, 1194272.863154519, 254908.783019114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7226400.0000, 
sim time next is 7227000.0000, 
raw observation next is [24.1, 87.5, 1.0, 2.0, 0.8170361164805013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104232, 1207508.216322504, 1207508.216322504, 257209.1984699279], 
processed observation next is [1.0, 0.6521739130434783, 0.3412322274881518, 0.875, 1.0, 1.0, 0.779561586121086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522643, 0.33541894897847335, 0.33541894897847335, 0.3838943260745193], 
reward next is 0.6161, 
noisyNet noise sample is [array([-0.20032951], dtype=float32), 0.14518929]. 
=============================================
[2019-03-27 03:09:08,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.80893 ]
 [72.35739 ]
 [71.457214]
 [70.6488  ]
 [69.69534 ]], R is [[73.52127075]
 [73.40559387]
 [73.26397705]
 [72.53134155]
 [71.80603027]].
[2019-03-27 03:09:08,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4987791e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5008656e-34], sum to 1.0000
[2019-03-27 03:09:08,940] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6084
[2019-03-27 03:09:08,946] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 74.33333333333334, 1.0, 2.0, 0.3809585040310223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575809.4812119086, 575809.4812119086, 172561.7600756117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7333800.0000, 
sim time next is 7334400.0000, 
raw observation next is [25.3, 74.66666666666667, 1.0, 2.0, 0.3851772154447255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582164.1210056443, 582164.1210056443, 173128.0558478968], 
processed observation next is [1.0, 0.9130434782608695, 0.39810426540284366, 0.7466666666666667, 1.0, 1.0, 0.25924965716231985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1617122558349012, 0.1617122558349012, 0.2584000833550698], 
reward next is 0.7416, 
noisyNet noise sample is [array([0.40118638], dtype=float32), 0.49739444]. 
=============================================
[2019-03-27 03:09:14,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6268453e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0065304e-35], sum to 1.0000
[2019-03-27 03:09:14,012] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4550
[2019-03-27 03:09:14,017] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.25, 75.0, 1.0, 2.0, 0.3813718645247738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576381.1285167775, 576381.1285167775, 172610.949388337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7335000.0000, 
sim time next is 7335600.0000, 
raw observation next is [25.2, 75.33333333333333, 1.0, 2.0, 0.3819511153240276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577230.6328833718, 577230.6328833725, 172685.7108491576], 
processed observation next is [1.0, 0.9130434782608695, 0.3933649289099526, 0.7533333333333333, 1.0, 1.0, 0.2553627895470212, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16034184246760327, 0.16034184246760347, 0.25773986693904116], 
reward next is 0.7423, 
noisyNet noise sample is [array([-1.8013035], dtype=float32), 2.6246917]. 
=============================================
[2019-03-27 03:09:15,085] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2655923e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:09:15,091] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9116
[2019-03-27 03:09:15,095] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 82.0, 1.0, 2.0, 0.2876649825195954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462816.0945530452, 462816.0945530459, 164464.6261958088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7416000.0000, 
sim time next is 7416600.0000, 
raw observation next is [21.66666666666666, 82.66666666666667, 1.0, 2.0, 0.2898212026336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465893.8763467673, 465893.8763467673, 164675.0138125497], 
processed observation next is [1.0, 0.8695652173913043, 0.22590837282780388, 0.8266666666666667, 1.0, 1.0, 0.1443628947393278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1294149656518798, 0.1294149656518798, 0.24578360270529806], 
reward next is 0.7542, 
noisyNet noise sample is [array([1.2697341], dtype=float32), -1.402481]. 
=============================================
[2019-03-27 03:09:18,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8115506e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7518328e-35], sum to 1.0000
[2019-03-27 03:09:18,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8799
[2019-03-27 03:09:18,348] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 88.33333333333334, 1.0, 2.0, 0.5347994733389857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747316.278386537, 747316.2783865363, 189228.2115635055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7850400.0000, 
sim time next is 7851000.0000, 
raw observation next is [27.05, 88.16666666666667, 1.0, 2.0, 0.5324171280664189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743986.0789735976, 743986.0789735976, 188831.1792314748], 
processed observation next is [1.0, 0.8695652173913043, 0.4810426540284361, 0.8816666666666667, 1.0, 1.0, 0.4366471422486975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2066627997148882, 0.2066627997148882, 0.28183758094249967], 
reward next is 0.7182, 
noisyNet noise sample is [array([0.73230255], dtype=float32), -0.47081202]. 
=============================================
[2019-03-27 03:09:18,367] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.708176]
 [75.64856 ]
 [75.53158 ]
 [75.75975 ]
 [75.44949 ]], R is [[75.54302979]
 [75.5051651 ]
 [75.46735382]
 [75.42954254]
 [75.39152527]].
[2019-03-27 03:09:18,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:18,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:18,852] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-27 03:09:21,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5113102e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.8875111e-38], sum to 1.0000
[2019-03-27 03:09:21,773] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7008
[2019-03-27 03:09:21,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.66666666666667, 1.0, 2.0, 0.5095597186048926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712035.0144809416, 712035.0144809422, 185107.3575488465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7672800.0000, 
sim time next is 7673400.0000, 
raw observation next is [26.55, 86.5, 1.0, 2.0, 0.5077434834523744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709496.244525124, 709496.244525124, 184818.1776674127], 
processed observation next is [1.0, 0.8260869565217391, 0.4573459715639811, 0.865, 1.0, 1.0, 0.40691985958117394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19708229014586776, 0.19708229014586776, 0.2758480263692727], 
reward next is 0.7242, 
noisyNet noise sample is [array([1.4920582], dtype=float32), -0.63918656]. 
=============================================
[2019-03-27 03:09:24,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5076087e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4254896e-37], sum to 1.0000
[2019-03-27 03:09:24,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2181
[2019-03-27 03:09:24,974] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 89.0, 1.0, 2.0, 0.3449194049264932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536162.2300257292, 536162.2300257292, 169638.0007286085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 87000.0000, 
sim time next is 87600.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.3446772152033712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535782.4271607496, 535782.427160749, 169607.1258193017], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 1.0, 1.0, 0.21045447614864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1488284519890971, 0.14882845198909694, 0.2531449639094055], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.79900837], dtype=float32), -0.27389163]. 
=============================================
[2019-03-27 03:09:25,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:25,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:25,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-27 03:09:29,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2033373e-17 1.0000000e+00 4.6277588e-25 8.6069506e-20 5.3785379e-12], sum to 1.0000
[2019-03-27 03:09:29,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9854
[2019-03-27 03:09:29,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1698651.372741052 W.
[2019-03-27 03:09:29,750] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.41666666666667, 78.33333333333334, 1.0, 2.0, 0.6075280474947024, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.919214177154736, 6.9112, 168.9128673302031, 1698651.372741052, 1692965.842642558, 366721.5431316577], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7809000.0000, 
sim time next is 7809600.0000, 
raw observation next is [28.53333333333334, 77.66666666666667, 1.0, 2.0, 0.6091841993963447, 1.0, 1.0, 0.6091841993963447, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1703272.5190882, 1703272.5190882, 337667.192374045], 
processed observation next is [1.0, 0.391304347826087, 0.5513428120063194, 0.7766666666666667, 1.0, 1.0, 0.5291375896341503, 1.0, 0.5, 0.5291375896341503, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4731312553022778, 0.4731312553022778, 0.5039808841403657], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8717513], dtype=float32), -0.3673074]. 
=============================================
[2019-03-27 03:09:32,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:32,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:32,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-27 03:09:34,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:34,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:34,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-27 03:09:36,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:36,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:36,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-27 03:09:38,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:38,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:38,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-27 03:09:39,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.849183e-19 1.000000e+00 1.993943e-28 7.472960e-23 2.716398e-13], sum to 1.0000
[2019-03-27 03:09:39,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0925
[2019-03-27 03:09:39,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2010685.748702773 W.
[2019-03-27 03:09:39,874] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.88333333333333, 75.66666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.695717584366996, 6.9112, 168.9086500764361, 2010685.748702773, 1454136.162928819, 311350.1993724448], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7811400.0000, 
sim time next is 7812000.0000, 
raw observation next is [29.0, 75.0, 1.0, 2.0, 0.662550784227262, 1.0, 1.0, 0.662550784227262, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1852613.90965665, 1852613.90965665, 358502.5632524777], 
processed observation next is [1.0, 0.43478260869565216, 0.5734597156398105, 0.75, 1.0, 1.0, 0.5934346797918819, 1.0, 0.5, 0.5934346797918819, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5146149749046249, 0.5146149749046249, 0.5350784526156384], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9259204], dtype=float32), 0.33354658]. 
=============================================
[2019-03-27 03:09:39,889] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[56.10157 ]
 [55.637516]
 [54.864838]
 [54.143417]
 [54.41552 ]], R is [[56.47346878]
 [55.90873337]
 [55.86005402]
 [55.30145264]
 [54.74843979]].
[2019-03-27 03:09:42,136] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:42,136] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:42,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-27 03:09:42,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:42,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:42,326] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-27 03:09:43,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:43,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:43,922] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-27 03:09:44,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:44,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:44,396] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-27 03:09:45,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6530342e-23 1.0000000e+00 2.0225583e-33 7.5765597e-29 1.8494241e-18], sum to 1.0000
[2019-03-27 03:09:45,073] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7216
[2019-03-27 03:09:45,077] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 94.0, 1.0, 2.0, 0.7523565089805304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1126767.662118722, 1126767.662118722, 242529.6785571532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 122400.0000, 
sim time next is 123000.0000, 
raw observation next is [22.88333333333333, 94.16666666666667, 1.0, 2.0, 0.7717516638050138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1155141.158100615, 1155141.158100615, 247338.3886303107], 
processed observation next is [1.0, 0.43478260869565216, 0.28357030015797774, 0.9416666666666668, 1.0, 1.0, 0.7250020045843539, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3208725439168375, 0.3208725439168375, 0.3691617740750906], 
reward next is 0.6308, 
noisyNet noise sample is [array([0.5609482], dtype=float32), 0.103359]. 
=============================================
[2019-03-27 03:09:45,089] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[64.46557 ]
 [64.100204]
 [63.84946 ]
 [63.390564]
 [63.17405 ]], R is [[64.71077728]
 [64.70168304]
 [64.71901703]
 [64.73004913]
 [64.71150208]].
[2019-03-27 03:09:45,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4124792e-25 1.0000000e+00 2.3990431e-35 8.3232376e-33 3.1601646e-21], sum to 1.0000
[2019-03-27 03:09:45,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7478
[2019-03-27 03:09:45,152] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 77.0, 1.0, 2.0, 0.5156472956270631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720544.3960978441, 720544.3960978441, 186084.6580546058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7930800.0000, 
sim time next is 7931400.0000, 
raw observation next is [28.41666666666666, 77.66666666666667, 1.0, 2.0, 0.518541432997288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724589.924356995, 724589.924356995, 186552.6544562156], 
processed observation next is [1.0, 0.8260869565217391, 0.5458135860979461, 0.7766666666666667, 1.0, 1.0, 0.41992943734613014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2012749789880542, 0.2012749789880542, 0.2784367976958442], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.9883329], dtype=float32), 0.63816917]. 
=============================================
[2019-03-27 03:09:45,611] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:45,612] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:45,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-27 03:09:46,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:46,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:46,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-27 03:09:46,319] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:46,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:46,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-27 03:09:46,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:46,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:46,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-27 03:09:47,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:47,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,207] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-27 03:09:47,452] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:09:47,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-27 03:09:47,806] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 03:09:47,808] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:09:47,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,811] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:09:47,812] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,812] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:09:47,813] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,813] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:09:47,815] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,815] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:09:47,816] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:09:47,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-27 03:09:47,847] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-27 03:09:47,848] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-27 03:09:47,864] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-27 03:09:47,901] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-27 03:10:05,235] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:10:05,236] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.4, 77.0, 1.0, 2.0, 0.3214824784130845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506240.8193310649, 506240.8193310655, 167451.8144013427]
[2019-03-27 03:10:05,237] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:10:05,239] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6435197e-29 1.0000000e+00 0.0000000e+00 3.9889573e-37 1.1527479e-27], sampled 0.2523187727644328
[2019-03-27 03:10:06,471] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:10:06,472] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.37946825, 56.64332406166666, 1.0, 2.0, 0.3411599101427445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538870.5988795062, 538870.5988795069, 170029.211267771]
[2019-03-27 03:10:06,472] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:10:06,474] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6174988e-28 1.0000000e+00 0.0000000e+00 7.5214441e-36 3.1922400e-26], sampled 0.7218434840899068
[2019-03-27 03:10:16,451] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:10:16,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.01666666666667, 84.66666666666667, 1.0, 2.0, 0.600139910325447, 0.0, 2.0, 0.0, 1.0, 2.0, 1.015924891448812, 6.911200000000001, 6.9112, 168.9129150079353, 1677977.774427574, 1677977.774427573, 361917.8191633914]
[2019-03-27 03:10:16,453] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:10:16,457] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.60112415e-16 9.99995708e-01 1.05376926e-26 2.30454251e-19
 4.28138037e-06], sampled 0.7141750496974567
[2019-03-27 03:10:16,458] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1677977.774427574 W.
[2019-03-27 03:10:47,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:10:47,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.2, 45.5, 1.0, 2.0, 0.597257968276073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834628.6090323686, 834628.6090323686, 200235.6284445353]
[2019-03-27 03:10:47,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:10:47,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5618223e-28 1.0000000e+00 0.0000000e+00 7.8205272e-36 1.1502491e-26], sampled 0.24953448522937727
[2019-03-27 03:11:12,828] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:11:12,829] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.05176964666667, 86.42200052666666, 1.0, 2.0, 0.9251651255578212, 1.0, 2.0, 0.9251651255578212, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 2587718.516314137, 2587718.516314137, 485896.8801050387]
[2019-03-27 03:11:12,829] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:11:12,834] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.3204983e-16 9.9998641e-01 1.0420641e-24 2.5714210e-17 1.3536570e-05], sampled 0.2765885257905567
[2019-03-27 03:11:12,837] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2587718.516314137 W.
[2019-03-27 03:11:21,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:11:21,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.5, 61.16666666666667, 1.0, 2.0, 0.5629165806893255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 786621.0150499424, 786621.0150499418, 194040.5361299126]
[2019-03-27 03:11:21,904] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:11:21,907] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.1180931e-28 1.0000000e+00 0.0000000e+00 2.1008325e-35 4.9503903e-26], sampled 0.7044847200791481
[2019-03-27 03:11:25,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:11:25,065] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.06666666666667, 65.16666666666667, 1.0, 2.0, 0.7738265368431209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1081497.17059915, 1081497.17059915, 237588.0404987097]
[2019-03-27 03:11:25,066] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:11:25,068] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5575134e-26 1.0000000e+00 1.4834286e-37 5.8356324e-34 2.9492581e-24], sampled 0.5080170472796245
[2019-03-27 03:11:36,595] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:11:36,597] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.45, 90.0, 1.0, 2.0, 1.002226478310534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129526085664, 1400918.698316577, 1400918.698316577, 299606.6817248581]
[2019-03-27 03:11:36,598] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:11:36,600] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4526821e-18 1.0000000e+00 2.1587141e-29 1.5537237e-23 1.4399220e-09], sampled 0.08229409907001373
[2019-03-27 03:11:40,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.14562994]
[2019-03-27 03:11:40,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.0, 60.5, 1.0, 2.0, 0.5944983512909222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830770.7181744339, 830770.7181744339, 199726.332131458]
[2019-03-27 03:11:40,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:11:40,285] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2452326e-26 1.0000000e+00 5.9676566e-37 1.2533888e-34 1.3194246e-21], sampled 0.8643905036329571
[2019-03-27 03:11:42,036] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8572.1276 2838372567.7057 961.0000
[2019-03-27 03:11:42,386] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8301.3520 2925630669.0009 1229.0000
[2019-03-27 03:11:42,608] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8682.6503 2778632583.8865 879.0000
[2019-03-27 03:11:42,739] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8038.7935 3156550301.8268 1500.0000
[2019-03-27 03:11:42,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8208.7360 2997152222.9423 1221.0000
[2019-03-27 03:11:43,858] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1700000, evaluation results [1700000.0, 8038.793487158927, 3156550301.8268476, 1500.0, 8301.352031179344, 2925630669.0009313, 1229.0, 8682.6502728017, 2778632583.8865137, 879.0, 8208.736001845737, 2997152222.942343, 1221.0, 8572.127629936795, 2838372567.7056937, 961.0]
[2019-03-27 03:11:53,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6335844e-26 1.0000000e+00 5.7905667e-36 1.3411975e-32 1.1835493e-23], sum to 1.0000
[2019-03-27 03:11:53,354] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2420
[2019-03-27 03:11:53,358] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 53.0, 1.0, 2.0, 0.6158825450571871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005456.406062563, 1005456.406062563, 218956.4809668626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 483000.0000, 
sim time next is 483600.0000, 
raw observation next is [25.3, 53.0, 1.0, 2.0, 0.6183887266573648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1010166.130353235, 1010166.130353234, 219526.8675808142], 
processed observation next is [1.0, 0.6086956521739131, 0.39810426540284366, 0.53, 1.0, 1.0, 0.5402273815148972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28060170287589864, 0.28060170287589836, 0.3276520411653943], 
reward next is 0.6723, 
noisyNet noise sample is [array([0.23339659], dtype=float32), -0.44586998]. 
=============================================
[2019-03-27 03:11:56,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:11:56,455] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8255
[2019-03-27 03:11:56,462] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 92.0, 1.0, 2.0, 0.3002782339955276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479492.7533179404, 479492.753317941, 165603.6485975491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 212400.0000, 
sim time next is 213000.0000, 
raw observation next is [20.88333333333334, 91.66666666666667, 1.0, 2.0, 0.301606091918902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481326.6202784381, 481326.6202784388, 165731.2482351321], 
processed observation next is [0.0, 0.4782608695652174, 0.18878357030015835, 0.9166666666666667, 1.0, 1.0, 0.15856155652879755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1337018389662328, 0.133701838966233, 0.2473600719927345], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.4053625], dtype=float32), 0.050109956]. 
=============================================
[2019-03-27 03:11:56,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.920975]
 [77.837494]
 [77.85055 ]
 [77.86125 ]
 [77.86946 ]], R is [[77.88324738]
 [77.8572464 ]
 [77.83138275]
 [77.80567932]
 [77.78012085]].
[2019-03-27 03:11:56,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.8979790e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5190465e-32], sum to 1.0000
[2019-03-27 03:11:56,774] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8450
[2019-03-27 03:11:56,790] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 86.0, 1.0, 2.0, 0.2767291524129772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446506.1808600551, 446506.1808600551, 163368.3651714692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [20.96666666666667, 86.0, 1.0, 2.0, 0.275679208189076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 445079.7360947166, 445079.736094716, 163273.9126578792], 
processed observation next is [0.0, 0.9130434782608695, 0.1927330173775673, 0.86, 1.0, 1.0, 0.1273243472157542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12363326002631016, 0.12363326002631, 0.24369240695205852], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.25175506], dtype=float32), -0.91089046]. 
=============================================
[2019-03-27 03:12:03,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:12:03,526] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4304
[2019-03-27 03:12:03,531] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.76666666666667, 82.0, 1.0, 2.0, 0.228648518716553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 378149.2841175299, 378149.2841175292, 158790.7606445757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 451200.0000, 
sim time next is 451800.0000, 
raw observation next is [19.8, 82.0, 1.0, 2.0, 0.228391744253959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377599.1667620723, 377599.1667620729, 158776.6473552268], 
processed observation next is [1.0, 0.21739130434782608, 0.13744075829383895, 0.82, 1.0, 1.0, 0.07035149910115542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10488865743390897, 0.10488865743390914, 0.236980070679443], 
reward next is 0.7630, 
noisyNet noise sample is [array([-1.3028504], dtype=float32), -1.6081687]. 
=============================================
[2019-03-27 03:12:05,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:12:05,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6763
[2019-03-27 03:12:05,132] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 88.0, 1.0, 2.0, 0.2563901906964166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417640.42540264, 417640.42540264, 161481.8214746076], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 356400.0000, 
sim time next is 357000.0000, 
raw observation next is [20.18333333333333, 88.16666666666667, 1.0, 2.0, 0.2635817827153241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429329.4209391063, 429329.4209391063, 162210.6649499334], 
processed observation next is [1.0, 0.13043478260869565, 0.1556082148499209, 0.8816666666666667, 1.0, 1.0, 0.11274913580159532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1192581724830851, 0.1192581724830851, 0.24210547007452746], 
reward next is 0.7579, 
noisyNet noise sample is [array([-0.77549225], dtype=float32), 1.0219021]. 
=============================================
[2019-03-27 03:12:05,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.108574]
 [78.02984 ]
 [77.861145]
 [77.81051 ]
 [78.243385]], R is [[78.24954224]
 [78.22602844]
 [78.20288849]
 [78.17974854]
 [78.15698242]].
[2019-03-27 03:12:19,687] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:12:19,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5465
[2019-03-27 03:12:19,701] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.05, 92.0, 1.0, 2.0, 0.2023395649287552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 338178.3682727466, 338178.3682727466, 155585.4864849319], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 617400.0000, 
sim time next is 618000.0000, 
raw observation next is [17.03333333333333, 92.0, 1.0, 2.0, 0.2019158851612504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 337490.9929626672, 337490.9929626672, 155535.2605829881], 
processed observation next is [1.0, 0.13043478260869565, 0.006319115323854638, 0.92, 1.0, 1.0, 0.038452873688253474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09374749804518533, 0.09374749804518533, 0.2321421799746091], 
reward next is 0.7679, 
noisyNet noise sample is [array([0.70458305], dtype=float32), -0.7148004]. 
=============================================
[2019-03-27 03:12:19,720] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.06535 ]
 [74.062004]
 [74.101006]
 [74.107956]
 [74.01374 ]], R is [[74.03450775]
 [74.06195068]
 [74.08893585]
 [74.1151123 ]
 [74.14122772]].
[2019-03-27 03:12:21,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:12:21,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5165
[2019-03-27 03:12:21,189] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.95, 83.0, 1.0, 2.0, 0.2250682785181859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 374501.3804623446, 374501.3804623453, 158155.2901573155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 628200.0000, 
sim time next is 628800.0000, 
raw observation next is [19.16666666666667, 82.0, 1.0, 2.0, 0.2206262193336021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 366859.060484131, 366859.0604841316, 157813.1331042344], 
processed observation next is [1.0, 0.2608695652173913, 0.10742496050552951, 0.82, 1.0, 1.0, 0.060995444980243486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10190529457892528, 0.10190529457892546, 0.23554198970781257], 
reward next is 0.7645, 
noisyNet noise sample is [array([-0.53239155], dtype=float32), 2.1594534]. 
=============================================
[2019-03-27 03:12:22,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.712222e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:12:22,552] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3658
[2019-03-27 03:12:22,562] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2930747842748531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467827.9844454972, 467827.9844454965, 164778.8066060361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.293535123231261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 468562.5456968961, 468562.5456968961, 164830.0733302099], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14883749786898917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13015626269358224, 0.13015626269358224, 0.24601503482120882], 
reward next is 0.7540, 
noisyNet noise sample is [array([0.30357617], dtype=float32), 1.6061007]. 
=============================================
[2019-03-27 03:12:24,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6935187e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3365771e-28], sum to 1.0000
[2019-03-27 03:12:24,524] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2265
[2019-03-27 03:12:24,529] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 80.66666666666667, 1.0, 2.0, 0.2415022161778914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399074.9289187366, 399074.9289187372, 160012.917123301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [19.9, 81.33333333333334, 1.0, 2.0, 0.2408704641226388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 398089.9218956504, 398089.9218956504, 159948.9190431294], 
processed observation next is [1.0, 0.9130434782608695, 0.14218009478672985, 0.8133333333333335, 1.0, 1.0, 0.08538610135257685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11058053385990288, 0.11058053385990288, 0.2387297299151185], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.65245456], dtype=float32), -0.5616228]. 
=============================================
[2019-03-27 03:12:28,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2236739e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:12:28,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8952
[2019-03-27 03:12:28,441] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 67.5, 1.0, 2.0, 0.2984486034324834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 475367.6815119153, 475367.681511916, 165293.4735364873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 832200.0000, 
sim time next is 832800.0000, 
raw observation next is [24.3, 68.0, 1.0, 2.0, 0.3007735365567235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478765.4867511411, 478765.4867511405, 165530.4651628454], 
processed observation next is [0.0, 0.6521739130434783, 0.3507109004739337, 0.68, 1.0, 1.0, 0.15755847777918494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13299041298642808, 0.1329904129864279, 0.24706039576544087], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.24624035], dtype=float32), -0.10195222]. 
=============================================
[2019-03-27 03:12:36,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0551232e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0189222e-30], sum to 1.0000
[2019-03-27 03:12:36,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6649
[2019-03-27 03:12:36,134] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.31666666666667, 73.83333333333333, 1.0, 2.0, 0.7147180959079403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1113985.574402312, 1113985.574402311, 238339.6811111442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1158600.0000, 
sim time next is 1159200.0000, 
raw observation next is [24.5, 73.0, 1.0, 2.0, 0.6965431508689186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084127.396662456, 1084127.396662456, 233760.1793345771], 
processed observation next is [1.0, 0.43478260869565216, 0.3601895734597157, 0.73, 1.0, 1.0, 0.6343893383962874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3011464990729044, 0.3011464990729044, 0.3488957900516076], 
reward next is 0.6511, 
noisyNet noise sample is [array([-1.5237347], dtype=float32), -0.45976877]. 
=============================================
[2019-03-27 03:12:36,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:12:36,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3754
[2019-03-27 03:12:36,172] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.00000000000001, 1.0, 2.0, 0.2925385832152782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466982.5633899804, 466982.5633899804, 164720.0440739929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 893400.0000, 
sim time next is 894000.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2926940999104102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 467221.1734289101, 467221.1734289107, 164736.5207204623], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14782421675953034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12978365928580834, 0.12978365928580854, 0.24587540406039146], 
reward next is 0.7541, 
noisyNet noise sample is [array([0.97020674], dtype=float32), -0.056080464]. 
=============================================
[2019-03-27 03:12:36,194] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.139946]
 [75.10578 ]
 [75.01164 ]
 [75.01053 ]
 [75.0048  ]], R is [[75.14685822]
 [75.14954376]
 [75.15222931]
 [75.15502167]
 [75.1579895 ]].
[2019-03-27 03:12:39,817] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-27 03:12:39,820] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:12:39,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:12:39,822] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:12:39,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:12:39,825] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:12:39,825] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:12:39,827] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:12:39,827] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:12:39,828] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:12:39,828] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:12:39,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-27 03:12:39,884] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-27 03:12:39,884] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-27 03:12:39,903] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-27 03:12:39,941] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-27 03:12:49,171] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1494165]
[2019-03-27 03:12:49,173] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.1852118, 65.778517435, 1.0, 2.0, 0.3246088912402548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536306.8771463971, 536306.8771463966, 169180.5464161967]
[2019-03-27 03:12:49,174] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:12:49,178] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5986553e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.27887470887099686
[2019-03-27 03:13:00,241] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1494165]
[2019-03-27 03:13:00,243] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.83333333333334, 90.0, 1.0, 2.0, 0.6095514060263363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954581.580616492, 954581.5806164913, 214705.5538799792]
[2019-03-27 03:13:00,245] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:13:00,248] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.8744128e-30 1.0000000e+00 0.0000000e+00 2.2149518e-38 6.6145688e-30], sampled 0.555903289919827
[2019-03-27 03:13:22,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1494165]
[2019-03-27 03:13:22,433] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.30213582666666, 82.59719518333333, 1.0, 2.0, 0.66775547614509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 961317.251409417, 961317.251409417, 217737.2082930093]
[2019-03-27 03:13:22,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:13:22,436] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.5462404e-30 1.0000000e+00 0.0000000e+00 4.1498675e-38 3.3409573e-29], sampled 0.8094241172128632
[2019-03-27 03:13:34,207] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1494165]
[2019-03-27 03:13:34,209] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.78333333333333, 71.83333333333333, 1.0, 2.0, 0.6087087755748473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850636.7645723455, 850636.7645723455, 202379.6708047734]
[2019-03-27 03:13:34,210] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:13:34,213] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3446047e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4382781e-31], sampled 0.7805946922541321
[2019-03-27 03:14:04,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1494165]
[2019-03-27 03:14:04,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 78.0, 1.0, 2.0, 0.5979070905298254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 835536.0713503886, 835536.071350388, 200356.1405662806]
[2019-03-27 03:14:04,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:14:04,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.5850870e-30 1.0000000e+00 0.0000000e+00 9.2025994e-38 1.3260299e-28], sampled 0.29984330746923393
[2019-03-27 03:14:34,643] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:14:34,866] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:14:34,964] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2326 2842432146.3999 1132.0000
[2019-03-27 03:14:34,984] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 03:14:34,991] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.8881 3164132157.9525 1782.0000
[2019-03-27 03:14:36,006] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1725000, evaluation results [1725000.0, 7883.888145551205, 3164132157.9525366, 1782.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8497.23261855992, 2842432146.3999343, 1132.0]
[2019-03-27 03:14:40,263] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5457205e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6216806e-29], sum to 1.0000
[2019-03-27 03:14:40,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7251
[2019-03-27 03:14:40,283] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.16666666666667, 1.0, 2.0, 0.3576113633991529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549453.8984225335, 549453.8984225335, 170560.0307967989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [21.7, 97.0, 1.0, 2.0, 0.3573041311786649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549303.5648226578, 549303.5648226573, 170556.7196522344], 
processed observation next is [1.0, 0.782608695652174, 0.2274881516587678, 0.97, 1.0, 1.0, 0.22566762792610226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1525843235618494, 0.15258432356184923, 0.2545622681376633], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.57718194], dtype=float32), -0.5629453]. 
=============================================
[2019-03-27 03:14:56,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:14:56,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8542
[2019-03-27 03:14:56,231] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 96.5, 1.0, 2.0, 0.333327726455469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520395.3048499784, 520395.3048499784, 168438.5593902614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [21.13333333333333, 96.66666666666666, 1.0, 2.0, 0.3322407999722555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519339.8942545966, 519339.8942545972, 168372.2554917981], 
processed observation next is [0.0, 0.08695652173913043, 0.20063191153238533, 0.9666666666666666, 1.0, 1.0, 0.19547084334006684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14426108173738794, 0.1442610817373881, 0.2513018738683554], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.89080626], dtype=float32), 0.88947093]. 
=============================================
[2019-03-27 03:14:56,500] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2541687e-26 1.0000000e+00 6.0289992e-38 2.1953900e-34 2.7823795e-25], sum to 1.0000
[2019-03-27 03:14:56,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7595
[2019-03-27 03:14:56,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 89.5, 1.0, 2.0, 0.6523175260850095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970834.3492990062, 970834.3492990062, 218459.0909019149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1609800.0000, 
sim time next is 1610400.0000, 
raw observation next is [23.6, 90.0, 1.0, 2.0, 0.7090873024859129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1054767.694420521, 1054767.69442052, 231158.8804639621], 
processed observation next is [1.0, 0.6521739130434783, 0.3175355450236968, 0.9, 1.0, 1.0, 0.6495027740794131, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2929910262279225, 0.2929910262279222, 0.345013254423824], 
reward next is 0.6550, 
noisyNet noise sample is [array([0.86832863], dtype=float32), -1.9596997]. 
=============================================
[2019-03-27 03:15:10,875] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9770666e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 6.2592118e-35], sum to 1.0000
[2019-03-27 03:15:10,884] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7260
[2019-03-27 03:15:10,890] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.06666666666667, 91.00000000000001, 1.0, 2.0, 0.3411531859773741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529851.4239202309, 529851.4239202302, 169116.6463757893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1552200.0000, 
sim time next is 1552800.0000, 
raw observation next is [22.03333333333333, 91.0, 1.0, 2.0, 0.3404960885951798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529303.159296839, 529303.1592968385, 169085.5194089038], 
processed observation next is [0.0, 1.0, 0.2432859399684044, 0.91, 1.0, 1.0, 0.20541697421106003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14702865536023307, 0.1470286553602329, 0.2523664468789609], 
reward next is 0.7476, 
noisyNet noise sample is [array([-1.3292105], dtype=float32), 0.8671719]. 
=============================================
[2019-03-27 03:15:13,288] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1156277e-22 1.0000000e+00 4.4618430e-33 6.9598301e-30 1.0857366e-18], sum to 1.0000
[2019-03-27 03:15:13,297] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0965
[2019-03-27 03:15:13,302] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.076714111318651, 6.9112, 168.9119950335501, 1571256.081043624, 1453835.342297662, 311348.1348653549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1684800.0000, 
sim time next is 1685400.0000, 
raw observation next is [26.61666666666667, 85.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.727438285571026, 6.9112, 168.9083712343996, 2033203.395698269, 1454151.582407434, 311348.4293867016], 
processed observation next is [1.0, 0.5217391304347826, 0.4605055292259086, 0.8566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08162382855710257, 0.0, 0.82941742934784, 0.5647787210272969, 0.4039309951131761, 0.4646991483383606], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1566536], dtype=float32), -0.5914067]. 
=============================================
[2019-03-27 03:15:13,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:15:13,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0744
[2019-03-27 03:15:13,929] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 88.66666666666667, 1.0, 2.0, 0.312807323646386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493251.869358558, 493251.869358558, 166492.7018573178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1574400.0000, 
sim time next is 1575000.0000, 
raw observation next is [21.9, 88.5, 1.0, 2.0, 0.3139729591363515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494165.2074130095, 494165.2074130089, 166538.1748206596], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.885, 1.0, 1.0, 0.17346139654982107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13726811317028043, 0.13726811317028026, 0.24856444003083522], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.9035077], dtype=float32), -0.014049389]. 
=============================================
[2019-03-27 03:15:13,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.794464]
 [72.821434]
 [72.917046]
 [72.91849 ]
 [72.94234 ]], R is [[72.77521515]
 [72.79896545]
 [72.81859589]
 [72.84290314]
 [72.86685944]].
[2019-03-27 03:15:16,199] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.0821861e-27 1.0000000e+00 3.9712843e-37 3.3613241e-35 6.0173437e-25], sum to 1.0000
[2019-03-27 03:15:16,207] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0339
[2019-03-27 03:15:16,213] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([0.3063807], dtype=float32), 1.5327374]. 
=============================================
[2019-03-27 03:15:24,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3491189e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6217458e-34], sum to 1.0000
[2019-03-27 03:15:24,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4937
[2019-03-27 03:15:24,232] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4709198399032556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659292.5582719826, 659292.5582719819, 179329.0786444363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2070600.0000, 
sim time next is 2071200.0000, 
raw observation next is [24.56666666666667, 94.0, 1.0, 2.0, 0.4698459700214774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658284.6062091653, 658284.6062091653, 179234.0391854889], 
processed observation next is [0.0, 1.0, 0.3633491311216432, 0.94, 1.0, 1.0, 0.3612602048451535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18285683505810146, 0.18285683505810146, 0.2675134913216252], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.19097832], dtype=float32), -0.1250761]. 
=============================================
[2019-03-27 03:15:27,358] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2488718e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6477121e-38], sum to 1.0000
[2019-03-27 03:15:27,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2603
[2019-03-27 03:15:27,370] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 95.5, 1.0, 2.0, 0.4674151160257047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655788.7725820044, 655788.772582005, 178992.472808752], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [24.33333333333334, 95.66666666666667, 1.0, 2.0, 0.4675708701703993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655868.527327139, 655868.5273271385, 178997.5540942362], 
processed observation next is [0.0, 0.043478260869565216, 0.35229067930489766, 0.9566666666666667, 1.0, 1.0, 0.3585191206872281, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1821857020353164, 0.18218570203531623, 0.26716052849886], 
reward next is 0.7328, 
noisyNet noise sample is [array([0.64646095], dtype=float32), 0.8020908]. 
=============================================
[2019-03-27 03:15:31,988] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-27 03:15:31,991] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:15:31,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:15:31,993] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:15:31,995] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:15:31,998] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:15:31,999] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:15:31,996] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:15:32,002] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:15:32,001] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:15:32,003] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:15:32,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-27 03:15:32,050] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-27 03:15:32,072] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-27 03:15:32,092] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-27 03:15:32,093] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-27 03:15:40,412] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15075098]
[2019-03-27 03:15:40,413] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [18.23333333333333, 89.16666666666667, 1.0, 2.0, 0.2364949611716479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 393337.1472689391, 393337.1472689398, 159239.9564376285]
[2019-03-27 03:15:40,415] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:15:40,417] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6476330519643895
[2019-03-27 03:16:04,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15075098]
[2019-03-27 03:16:04,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 87.0, 1.0, 2.0, 0.6954466490191373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 971903.5948098795, 971903.5948098802, 219864.8691363885]
[2019-03-27 03:16:04,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:16:04,600] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2288107e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.2113807827318166
[2019-03-27 03:16:19,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15075098]
[2019-03-27 03:16:19,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.75, 55.0, 1.0, 2.0, 0.5866092022630386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104199, 819741.9186027434, 819741.9186027428, 198277.9460701474]
[2019-03-27 03:16:19,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:16:19,323] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0527763e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0262715e-30], sampled 0.07370165801127948
[2019-03-27 03:16:55,434] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15075098]
[2019-03-27 03:16:55,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.4, 93.0, 1.0, 2.0, 0.6186091332720554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864477.5959473653, 864477.5959473659, 204263.5554943269]
[2019-03-27 03:16:55,439] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:16:55,441] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3738515403463093
[2019-03-27 03:17:03,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15075098]
[2019-03-27 03:17:03,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.66273260333334, 88.86351158000001, 1.0, 2.0, 0.5082974344211068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710270.5676662996, 710270.5676662996, 184904.9893982311]
[2019-03-27 03:17:03,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:17:03,763] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16572768979289532
[2019-03-27 03:17:26,604] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.2371 3164045347.9980 1783.0000
[2019-03-27 03:17:26,867] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:17:27,094] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2326 2842432146.3999 1132.0000
[2019-03-27 03:17:27,133] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 03:17:27,227] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:17:28,245] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1750000, evaluation results [1750000.0, 7883.237052373875, 3164045347.9979544, 1783.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8497.23261855992, 2842432146.3999343, 1132.0]
[2019-03-27 03:17:28,364] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7742168e-28 1.0000000e+00 1.1396164e-37 2.8379044e-34 1.5225840e-25], sum to 1.0000
[2019-03-27 03:17:28,374] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7382
[2019-03-27 03:17:28,379] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 81.0, 1.0, 2.0, 0.5430588758015013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758861.8969621832, 758861.8969621832, 190617.2628459423], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2332800.0000, 
sim time next is 2333400.0000, 
raw observation next is [28.26666666666667, 81.0, 1.0, 2.0, 0.5422697994897759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757758.8608835214, 757758.8608835208, 190483.6204383477], 
processed observation next is [1.0, 0.0, 0.53870458135861, 0.81, 1.0, 1.0, 0.4485178307105734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21048857246764482, 0.21048857246764466, 0.2843039111020115], 
reward next is 0.7157, 
noisyNet noise sample is [array([-0.38710427], dtype=float32), -0.57399243]. 
=============================================
[2019-03-27 03:17:30,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8568778e-28 1.0000000e+00 0.0000000e+00 8.6672027e-37 5.6180634e-29], sum to 1.0000
[2019-03-27 03:17:30,654] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7333
[2019-03-27 03:17:30,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.65, 82.5, 1.0, 2.0, 0.9261565077601371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326232.476951096, 1326232.476951096, 281979.1999425621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1931400.0000, 
sim time next is 1932000.0000, 
raw observation next is [25.7, 82.33333333333334, 1.0, 2.0, 0.944622855265667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1351154.575088492, 1351154.575088492, 287167.0202374759], 
processed observation next is [1.0, 0.34782608695652173, 0.4170616113744076, 0.8233333333333335, 1.0, 1.0, 0.9332805485128518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3753207153023589, 0.3753207153023589, 0.4286074928917551], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.44821575], dtype=float32), -0.25289792]. 
=============================================
[2019-03-27 03:17:30,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.450657]
 [61.15247 ]
 [63.667145]
 [64.14628 ]
 [64.09657 ]], R is [[58.64907455]
 [58.64171982]
 [58.68162155]
 [58.80550385]
 [58.94290543]].
[2019-03-27 03:17:30,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6507154e-20 1.0000000e+00 1.5775493e-28 4.9741684e-25 1.3829203e-12], sum to 1.0000
[2019-03-27 03:17:30,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5989
[2019-03-27 03:17:30,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1834699.0684054 W.
[2019-03-27 03:17:30,932] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6711077411324686, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.986601091085548, 6.9112, 168.9125067514531, 1834699.0684054, 1781207.081715771, 379224.6970381857], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.6269068515491595, 1.0, 1.0, 0.6269068515491595, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1752865.393363225, 1752865.393363225, 344401.479770033], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.5504901825893488, 1.0, 0.5, 0.5504901825893488, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48690705371200693, 0.48690705371200693, 0.5140320593582581], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7999832], dtype=float32), -0.2984057]. 
=============================================
[2019-03-27 03:17:38,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8705540e-16 9.9999106e-01 2.6107225e-24 3.6518108e-20 8.9941959e-06], sum to 1.0000
[2019-03-27 03:17:38,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2925
[2019-03-27 03:17:38,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1973768.894305488 W.
[2019-03-27 03:17:38,790] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 64.0, 1.0, 2.0, 0.7704864658463291, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999811106479605, 6.9112, 168.9124289551066, 1973768.894305488, 1910905.321948435, 400913.2709473119], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [31.88333333333333, 64.16666666666667, 1.0, 2.0, 0.4147893204397328, 1.0, 1.0, 0.4147893204397328, 1.0, 2.0, 0.7203518766794214, 6.9112, 6.9112, 170.5573041426782, 1739649.272221037, 1739649.272221037, 361226.2643536625], 
processed observation next is [1.0, 0.5652173913043478, 0.7101105845181673, 0.6416666666666667, 1.0, 1.0, 0.29492689209606365, 1.0, 0.5, 0.29492689209606365, 1.0, 1.0, 0.6589657032675869, 0.0, 0.0, 0.8375144448122397, 0.48323590895028806, 0.48323590895028806, 0.5391436781397948], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5158082], dtype=float32), 1.6183189]. 
=============================================
[2019-03-27 03:17:41,531] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9921157e-12 3.0658807e-04 1.7625015e-20 7.8512419e-14 9.9969339e-01], sum to 1.0000
[2019-03-27 03:17:41,540] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3121
[2019-03-27 03:17:41,549] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.91666666666666, 66.83333333333333, 1.0, 2.0, 0.5846560207138954, 1.0, 2.0, 0.5846560207138954, 1.0, 2.0, 1.015354159279445, 6.911200000000001, 6.9112, 170.5573041426782, 2452855.940720946, 2452855.940720945, 478622.4424162976], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2217000.0000, 
sim time next is 2217600.0000, 
raw observation next is [31.9, 67.0, 1.0, 2.0, 0.5873586758488133, 1.0, 2.0, 0.5873586758488133, 1.0, 2.0, 1.020047777467086, 6.911200000000001, 6.9112, 170.5573041426782, 2464205.794463728, 2464205.794463727, 480811.3772733225], 
processed observation next is [1.0, 0.6956521739130435, 0.7109004739336492, 0.67, 1.0, 1.0, 0.5028417781311003, 1.0, 1.0, 0.5028417781311003, 1.0, 1.0, 1.0244485091062023, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6845016095732578, 0.6845016095732576, 0.7176289213034664], 
reward next is 0.2824, 
noisyNet noise sample is [array([0.25500202], dtype=float32), 0.9661197]. 
=============================================
[2019-03-27 03:17:42,151] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0516939e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2061265e-34], sum to 1.0000
[2019-03-27 03:17:42,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3875
[2019-03-27 03:17:42,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 91.5, 1.0, 2.0, 0.407872809442256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 604284.2195635884, 604284.219563589, 174809.7106874887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2662200.0000, 
sim time next is 2662800.0000, 
raw observation next is [23.33333333333333, 92.33333333333334, 1.0, 2.0, 0.404807159499356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 601011.171667493, 601011.1716674924, 174543.0719971536], 
processed observation next is [0.0, 0.8260869565217391, 0.30489731437598716, 0.9233333333333335, 1.0, 1.0, 0.28290019216789875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1669475476854147, 0.16694754768541453, 0.2605120477569457], 
reward next is 0.7395, 
noisyNet noise sample is [array([-1.7557442], dtype=float32), 0.123239174]. 
=============================================
[2019-03-27 03:17:46,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5158658e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4248840e-33], sum to 1.0000
[2019-03-27 03:17:46,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5644
[2019-03-27 03:17:46,788] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3942585765178038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588293.0676393362, 588293.0676393369, 173461.2461105632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2739000.0000, 
sim time next is 2739600.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3940499015925438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587981.5821450595, 587981.5821450589, 173432.7497484515], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2699396404729444, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16332821726251653, 0.16332821726251637, 0.2588548503708231], 
reward next is 0.7411, 
noisyNet noise sample is [array([-0.38979876], dtype=float32), -0.22008929]. 
=============================================
[2019-03-27 03:17:58,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1665010e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7303612e-34], sum to 1.0000
[2019-03-27 03:17:58,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7206
[2019-03-27 03:17:58,702] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3022952447846516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481388.7555171007, 481388.7555171007, 165721.4112967301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3024168540965771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481582.4135163953, 481582.4135163959, 165735.312117779], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15953837842961094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13377289264344314, 0.1337728926434433, 0.24736613748922237], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.1592878], dtype=float32), 0.6047476]. 
=============================================
[2019-03-27 03:17:59,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9192283e-29 1.0000000e+00 0.0000000e+00 2.1782698e-36 6.7366310e-27], sum to 1.0000
[2019-03-27 03:17:59,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5387
[2019-03-27 03:17:59,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.13333333333334, 80.0, 1.0, 2.0, 0.5631975592257362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787013.8006098962, 787013.8006098955, 194090.005640739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2418600.0000, 
sim time next is 2419200.0000, 
raw observation next is [29.1, 80.0, 1.0, 2.0, 0.5620524104198457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785412.9749208044, 785412.974920805, 193889.3269525383], 
processed observation next is [1.0, 0.0, 0.5781990521327015, 0.8, 1.0, 1.0, 0.4723523017106574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21817027081133455, 0.21817027081133472, 0.28938705515304225], 
reward next is 0.7106, 
noisyNet noise sample is [array([-0.77720416], dtype=float32), 0.1231319]. 
=============================================
[2019-03-27 03:18:05,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:18:05,572] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9136
[2019-03-27 03:18:05,576] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 91.66666666666667, 1.0, 2.0, 0.4338477400867704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628544.9911442188, 628544.9911442194, 176746.0535346066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2614800.0000, 
sim time next is 2615400.0000, 
raw observation next is [24.35, 91.0, 1.0, 2.0, 0.4390332765670147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632769.1165571843, 632769.1165571848, 177076.3524584505], 
processed observation next is [0.0, 0.2608695652173913, 0.35308056872037924, 0.91, 1.0, 1.0, 0.324136477791584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17576919904366228, 0.17576919904366245, 0.26429306337082165], 
reward next is 0.7357, 
noisyNet noise sample is [array([-1.5562241], dtype=float32), 0.03424031]. 
=============================================
[2019-03-27 03:18:12,027] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.702446e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:18:12,035] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1311
[2019-03-27 03:18:12,044] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911240502363542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584766.6491672514, 584766.649167252, 173175.038959612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3920897423274511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585345.7712773788, 585345.7712773794, 173201.1637201645], 
processed observation next is [0.0, 0.6521739130434783, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26757800280415794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16259604757704965, 0.16259604757704982, 0.2585091995823351], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.2613632], dtype=float32), 0.14183591]. 
=============================================
[2019-03-27 03:18:13,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:18:13,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6274
[2019-03-27 03:18:13,327] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4754736233187821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664518.4514627971, 664518.4514627971, 179858.8295026866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2627400.0000, 
sim time next is 2628000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.475496912035503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664550.9980319068, 664550.9980319074, 179862.3071620924], 
processed observation next is [0.0, 0.43478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680685687174735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18459749945330745, 0.1845974994533076, 0.26845120471954087], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.00466265], dtype=float32), -1.0247436]. 
=============================================
[2019-03-27 03:18:13,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.400345]
 [78.4036  ]
 [78.40599 ]
 [78.411766]
 [78.4181  ]], R is [[78.44071198]
 [78.38785553]
 [78.33554077]
 [78.28373718]
 [78.23243713]].
[2019-03-27 03:18:18,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:18:18,093] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5112
[2019-03-27 03:18:18,103] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4801442827383569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670918.2743513243, 670918.2743513243, 180543.3134476939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2707200.0000, 
sim time next is 2707800.0000, 
raw observation next is [23.83333333333333, 100.0, 1.0, 2.0, 0.4767107555814358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666119.0120372076, 666119.0120372076, 180027.0951194802], 
processed observation next is [0.0, 0.34782608695652173, 0.32859399684044216, 1.0, 1.0, 1.0, 0.36953103082100697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18503305889922433, 0.18503305889922433, 0.26869715689474655], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.31214178], dtype=float32), 1.107985]. 
=============================================
[2019-03-27 03:18:21,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:18:21,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4958
[2019-03-27 03:18:21,866] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([1.205099], dtype=float32), 0.7418501]. 
=============================================
[2019-03-27 03:18:22,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0767044e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3864649e-34], sum to 1.0000
[2019-03-27 03:18:22,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0254
[2019-03-27 03:18:22,934] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 82.33333333333334, 1.0, 2.0, 0.541350303648928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756473.5150540927, 756473.5150540927, 190328.0800522262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3356400.0000, 
sim time next is 3357000.0000, 
raw observation next is [28.0, 81.5, 1.0, 2.0, 0.5371186532427136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750558.1911082439, 750558.1911082446, 189615.812435779], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.815, 1.0, 1.0, 0.4423116304129079, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20848838641895664, 0.20848838641895684, 0.2830086752772821], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.5544749], dtype=float32), -0.58983856]. 
=============================================
[2019-03-27 03:18:22,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.438896]
 [73.40081 ]
 [73.355606]
 [73.20484 ]
 [73.15749 ]], R is [[73.44410706]
 [73.42559052]
 [73.40605927]
 [73.38598633]
 [73.36580658]].
[2019-03-27 03:18:23,801] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:18:23,810] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0317
[2019-03-27 03:18:23,819] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3648769842953998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 562068.2695467542, 562068.2695467548, 171666.4570487527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3708309786404282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.1525737698, 571243.1525737704, 172456.0830141283], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.24196503450653997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15867865349271384, 0.15867865349271398, 0.25739713882705717], 
reward next is 0.7426, 
noisyNet noise sample is [array([-0.2567982], dtype=float32), 0.84894586]. 
=============================================
[2019-03-27 03:18:23,841] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.96907 ]
 [74.95708 ]
 [75.01504 ]
 [74.982635]
 [74.978516]], R is [[74.91812134]
 [74.91271973]
 [74.9036026 ]
 [74.9021759 ]
 [74.90065765]].
[2019-03-27 03:18:24,310] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 03:18:24,312] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:18:24,312] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:18:24,314] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:18:24,314] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:18:24,315] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:18:24,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:18:24,316] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:18:24,316] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:18:24,319] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:18:24,323] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:18:24,346] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-27 03:18:24,365] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-27 03:18:24,390] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-27 03:18:24,412] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-27 03:18:24,439] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-27 03:18:28,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:18:28,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.49239164666667, 82.57160783666667, 1.0, 2.0, 0.2983372054351436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480104.9729056704, 480104.972905671, 165668.6181560063]
[2019-03-27 03:18:28,951] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:18:28,956] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8018221062411102
[2019-03-27 03:19:05,516] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:05,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.66666666666667, 96.0, 1.0, 2.0, 0.3899526964879866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583306.3079049023, 583306.3079049016, 173051.4434765129]
[2019-03-27 03:19:05,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:19:05,523] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5361898e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.31797157856922764
[2019-03-27 03:19:11,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:11,279] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 77.16666666666667, 1.0, 2.0, 0.7806110625363649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090984.07013579, 1090984.070135789, 239208.3186770421]
[2019-03-27 03:19:11,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:19:11,282] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1117738e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.30811881856631074
[2019-03-27 03:19:24,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:24,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.75, 74.0, 1.0, 2.0, 0.7986961225545973, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981446802056, 6.9112, 168.9123160045701, 2013248.594542179, 1946007.629029062, 407507.5481419365]
[2019-03-27 03:19:24,882] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:19:24,885] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2968660e-26 1.0000000e+00 1.2956531e-36 7.2855932e-34 7.5241877e-24], sampled 0.28337777323635194
[2019-03-27 03:19:24,886] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2013248.594542179 W.
[2019-03-27 03:19:32,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:32,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.81407788, 69.09320754666666, 1.0, 2.0, 0.923206546433084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1290396.994330168, 1290396.994330168, 276400.037483111]
[2019-03-27 03:19:32,682] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:19:32,686] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4204215e-30 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.1433180e-30], sampled 0.44949880898402805
[2019-03-27 03:19:34,757] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:34,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.70948448166666, 63.18789373, 1.0, 2.0, 0.6327672130033978, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989391618661537, 6.9112, 168.9124050127174, 1769279.032097107, 1713807.387785285, 370768.7421419236]
[2019-03-27 03:19:34,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:19:34,763] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0761117e-27 1.0000000e+00 0.0000000e+00 8.6254555e-35 9.5411445e-26], sampled 0.24461458113368317
[2019-03-27 03:19:34,764] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1769279.032097107 W.
[2019-03-27 03:19:57,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:19:57,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.86666666666667, 85.16666666666667, 1.0, 2.0, 0.936310181788323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1308723.677116371, 1308723.677116372, 280119.2126248339]
[2019-03-27 03:19:57,832] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:19:57,835] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1199282e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2371782e-38], sampled 0.18283183556880178
[2019-03-27 03:20:09,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15253736]
[2019-03-27 03:20:09,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 81.66666666666667, 1.0, 2.0, 0.8117461074276572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1134521.736762201, 1134521.7367622, 246818.5057662503]
[2019-03-27 03:20:09,184] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:20:09,187] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0914453e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9166715171725587
[2019-03-27 03:20:19,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:20:19,419] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:20:19,509] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:20:19,513] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:20:19,678] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:20:20,695] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1775000, evaluation results [1775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:20:24,070] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.2675510e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0781527e-38], sum to 1.0000
[2019-03-27 03:20:24,083] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2117
[2019-03-27 03:20:24,087] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.526962387468613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787175.6370659193, 787175.6370659193, 194221.3347785668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2898000.0000, 
sim time next is 2898600.0000, 
raw observation next is [22.83333333333334, 94.00000000000001, 1.0, 2.0, 0.5686072264897417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851744.2705237466, 851744.2705237466, 202139.1769588858], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115327, 0.9400000000000002, 1.0, 1.0, 0.4802496704695683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23659563070104073, 0.23659563070104073, 0.30170026411774], 
reward next is 0.6983, 
noisyNet noise sample is [array([-0.759104], dtype=float32), -1.4114758]. 
=============================================
[2019-03-27 03:20:25,075] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1246465e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0146094e-37], sum to 1.0000
[2019-03-27 03:20:25,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0383
[2019-03-27 03:20:25,096] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.75, 89.83333333333333, 1.0, 2.0, 0.7239476025630669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1107836.51087096, 1107836.51087096, 238384.3031321589], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2891400.0000, 
sim time next is 2892000.0000, 
raw observation next is [22.8, 89.66666666666667, 1.0, 2.0, 0.6677887869136984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1020972.470796245, 1020972.470796246, 225019.6831321651], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.8966666666666667, 1.0, 1.0, 0.5997455264020461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28360346411006804, 0.2836034641100683, 0.33585027333158973], 
reward next is 0.6641, 
noisyNet noise sample is [array([0.6170038], dtype=float32), -0.69479626]. 
=============================================
[2019-03-27 03:20:25,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.352234]
 [72.72264 ]
 [72.57118 ]
 [72.663635]
 [72.77967 ]], R is [[72.38079834]
 [72.30119324]
 [72.25366974]
 [72.20585632]
 [72.16268921]].
[2019-03-27 03:20:37,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8928390e-28 1.0000000e+00 0.0000000e+00 3.6652117e-36 3.6403245e-29], sum to 1.0000
[2019-03-27 03:20:37,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6026
[2019-03-27 03:20:37,126] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.0, 1.0, 2.0, 0.528880145554815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739041.870700125, 739041.8707001257, 188244.5290947481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3376800.0000, 
sim time next is 3377400.0000, 
raw observation next is [26.25, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.153973108898423, 6.9112, 168.9118776273116, 1626103.378697254, 1453872.879719153, 311349.7721406899], 
processed observation next is [1.0, 0.08695652173913043, 0.4431279620853081, 0.9233333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.024277310889842328, 0.0, 0.8294346473422717, 0.45169538297145945, 0.4038535776997647, 0.4647011524487909], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41249412], dtype=float32), -0.63128203]. 
=============================================
[2019-03-27 03:20:41,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5322099e-30 1.0000000e+00 0.0000000e+00 1.5128878e-38 5.4878918e-32], sum to 1.0000
[2019-03-27 03:20:41,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6082
[2019-03-27 03:20:41,538] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4869943801234851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680493.1619666036, 680493.1619666043, 181583.409742752], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4880088542916172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681911.1748592041, 681911.1748592034, 181738.5422265647], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3831431979417075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18941977079422337, 0.18941977079422317, 0.27125155556203684], 
reward next is 0.7287, 
noisyNet noise sample is [array([1.2250124], dtype=float32), -1.7589628]. 
=============================================
[2019-03-27 03:20:45,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:20:45,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2990
[2019-03-27 03:20:45,236] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.495279537346645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 692074.0541447345, 692074.0541447351, 182859.9811153516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.4972080054590089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694769.66157284, 694769.6615728405, 183159.9795494174], 
processed observation next is [0.0, 0.391304347826087, 0.5734597156398105, 0.7, 1.0, 1.0, 0.3942265126012155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1929915726591222, 0.19299157265912237, 0.2733731038051006], 
reward next is 0.7266, 
noisyNet noise sample is [array([-0.6093519], dtype=float32), 1.4490337]. 
=============================================
[2019-03-27 03:20:55,178] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4246654e-19 1.0000000e+00 1.9626460e-30 3.8904005e-24 4.3829174e-15], sum to 1.0000
[2019-03-27 03:20:55,189] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2349
[2019-03-27 03:20:55,193] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 77.33333333333334, 1.0, 2.0, 0.855749390220978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196056.738696544, 1196056.738696544, 258053.4722672796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3482400.0000, 
sim time next is 3483000.0000, 
raw observation next is [28.0, 76.5, 1.0, 2.0, 0.9209319968168733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287215.852155954, 1287215.852155954, 275758.2496791953], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.765, 1.0, 1.0, 0.904737345562498, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3575599589322095, 0.3575599589322095, 0.4115794771331273], 
reward next is 0.5884, 
noisyNet noise sample is [array([-1.0007042], dtype=float32), -0.3705401]. 
=============================================
[2019-03-27 03:20:55,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[50.649906]
 [50.723904]
 [50.953102]
 [50.289913]
 [50.6254  ]], R is [[50.53306961]
 [50.64258575]
 [50.75322723]
 [50.90060425]
 [51.0196991 ]].
[2019-03-27 03:20:56,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.3501594e-29 1.0000000e+00 0.0000000e+00 5.0134378e-37 6.5058684e-28], sum to 1.0000
[2019-03-27 03:20:56,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8091
[2019-03-27 03:20:56,517] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 79.0, 1.0, 2.0, 0.5393362461112337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753658.1075837769, 753658.1075837775, 189988.5131985938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3533400.0000, 
sim time next is 3534000.0000, 
raw observation next is [28.33333333333333, 79.0, 1.0, 2.0, 0.5347848399878199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747295.8228908767, 747295.8228908767, 189225.2850913939], 
processed observation next is [1.0, 0.9130434782608695, 0.541864139020537, 0.79, 1.0, 1.0, 0.4394998072142408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2075821730252435, 0.2075821730252435, 0.28242579864387146], 
reward next is 0.7176, 
noisyNet noise sample is [array([1.2545834], dtype=float32), 0.4276561]. 
=============================================
[2019-03-27 03:20:56,535] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.35098 ]
 [74.49463 ]
 [74.58239 ]
 [74.522545]
 [74.675514]], R is [[74.00830078]
 [73.98465729]
 [73.96008301]
 [73.93458557]
 [73.90854645]].
[2019-03-27 03:20:59,834] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.184891e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:20:59,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0730
[2019-03-27 03:20:59,858] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 93.16666666666667, 1.0, 2.0, 0.5674161613056328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792911.0869564826, 792911.0869564819, 194831.853932362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.5635043300668936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787442.641918431, 787442.6419184317, 194143.037041534], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.47410160249023325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21873406719956417, 0.21873406719956437, 0.28976572692766267], 
reward next is 0.7102, 
noisyNet noise sample is [array([1.2341387], dtype=float32), 0.40636146]. 
=============================================
[2019-03-27 03:21:00,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:21:00,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7313
[2019-03-27 03:21:00,388] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5873009691091451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820708.9838378612, 820708.9838378606, 198403.9046008279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 1.0, 2.0, 0.5879990792192651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821684.9176211624, 821684.9176211624, 198531.4449304526], 
processed observation next is [0.0, 0.30434782608695654, 0.6129541864139019, 0.7983333333333335, 1.0, 1.0, 0.5036133484569458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22824581045032288, 0.22824581045032288, 0.2963155894484367], 
reward next is 0.7037, 
noisyNet noise sample is [array([1.033109], dtype=float32), 1.2744867]. 
=============================================
[2019-03-27 03:21:00,751] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.246551e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:21:00,761] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6510
[2019-03-27 03:21:00,765] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 66.66666666666667, 1.0, 2.0, 0.5594867318126384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781826.3718749982, 781826.3718749982, 193441.8728858511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3608400.0000, 
sim time next is 3609000.0000, 
raw observation next is [31.5, 66.5, 1.0, 2.0, 0.5543957507269193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774709.6440743197, 774709.6440743197, 192558.1793052276], 
processed observation next is [1.0, 0.782608695652174, 0.6919431279620853, 0.665, 1.0, 1.0, 0.4631274105143605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2151971233539777, 0.2151971233539777, 0.2874002676197427], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.20465071], dtype=float32), 0.027209073]. 
=============================================
[2019-03-27 03:21:00,777] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[66.19409 ]
 [66.04386 ]
 [65.28828 ]
 [64.906044]
 [64.87539 ]], R is [[66.28931427]
 [66.33769989]
 [66.38349915]
 [66.43048859]
 [66.47801208]].
[2019-03-27 03:21:05,367] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1567147e-11 4.1605797e-04 2.2930486e-17 6.4729760e-10 9.9958390e-01], sum to 1.0000
[2019-03-27 03:21:05,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4306
[2019-03-27 03:21:05,379] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.4991108328433083, 1.0, 1.0, 0.4991108328433083, 1.0, 2.0, 0.8543499239621454, 6.911199999999999, 6.9112, 170.5573041426782, 2093643.879768375, 2093643.879768375, 412272.7984175681], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4008600.0000, 
sim time next is 4009200.0000, 
raw observation next is [28.33333333333334, 77.33333333333333, 1.0, 2.0, 0.5071671461368045, 1.0, 2.0, 0.5071671461368045, 1.0, 2.0, 0.8701378031259487, 6.9112, 6.9112, 170.5573041426782, 2127471.637780038, 2127471.637780038, 418173.8712207927], 
processed observation next is [1.0, 0.391304347826087, 0.5418641390205374, 0.7733333333333333, 1.0, 1.0, 0.40622547727325836, 1.0, 1.0, 0.40622547727325836, 1.0, 1.0, 0.8416314672267665, 0.0, 0.0, 0.8375144448122397, 0.5909643438277884, 0.5909643438277884, 0.6241401062996906], 
reward next is 0.3759, 
noisyNet noise sample is [array([-0.3802235], dtype=float32), -0.740486]. 
=============================================
[2019-03-27 03:21:16,379] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:21:16,382] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:21:16,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:21:16,384] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:21:16,385] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:21:16,387] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:21:16,389] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:21:16,386] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:21:16,390] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:21:16,391] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:21:16,389] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:21:16,417] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-27 03:21:16,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-27 03:21:16,463] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-27 03:21:16,463] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-27 03:21:16,486] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-27 03:21:30,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1500177]
[2019-03-27 03:21:30,533] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.9, 92.0, 1.0, 2.0, 0.3430515903976248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533491.2233445066, 533491.2233445072, 169427.9569860491]
[2019-03-27 03:21:30,536] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:21:30,539] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9399804765452556
[2019-03-27 03:21:55,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1500177]
[2019-03-27 03:21:55,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.35745466, 89.48066201666666, 1.0, 2.0, 0.4553915575445596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657640.5205524224, 657640.520552423, 179624.7027173504]
[2019-03-27 03:21:55,555] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:21:55,558] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.629627654986531
[2019-03-27 03:22:32,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1500177]
[2019-03-27 03:22:32,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.57155237, 81.96318642333333, 1.0, 2.0, 0.4408382382131275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639007.7387432548, 639007.7387432548, 177792.5019379681]
[2019-03-27 03:22:32,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:22:32,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.438074e-36 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.37024153803270565
[2019-03-27 03:23:10,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:23:11,250] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.9724 3164047750.5212 1866.0000
[2019-03-27 03:23:11,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779103513.6362 932.0000
[2019-03-27 03:23:11,435] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.5613 2842236996.0385 1152.0000
[2019-03-27 03:23:11,609] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.5500 3007491220.8725 1766.0000
[2019-03-27 03:23:12,623] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1800000, evaluation results [1800000.0, 7875.972356213788, 3164047750.521237, 1866.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779103513.636221, 932.0, 8001.549996308511, 3007491220.8725486, 1766.0, 8494.56129159175, 2842236996.038548, 1152.0]
[2019-03-27 03:23:12,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.964642e-35 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:23:12,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5684
[2019-03-27 03:23:12,633] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5654753550618963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790197.9826860642, 790197.9826860642, 194492.0020715309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4039200.0000, 
sim time next is 4039800.0000, 
raw observation next is [29.83333333333333, 79.00000000000001, 1.0, 2.0, 0.5700780396052568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 796632.2081550779, 796632.2081550773, 195304.8225510916], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7900000000000001, 1.0, 1.0, 0.48202173446416485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22128672448752162, 0.22128672448752149, 0.291499735150883], 
reward next is 0.7085, 
noisyNet noise sample is [array([1.6370648], dtype=float32), 0.92721415]. 
=============================================
[2019-03-27 03:23:13,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1529443e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:23:13,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9723
[2019-03-27 03:23:13,087] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5445282443353512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760915.9047814367, 760915.9047814367, 190866.8922560311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5435760300077559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759584.8184712358, 759584.8184712352, 190705.3339932352], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.450091602418983, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2109957829086766, 0.21099578290867646, 0.2846348268555749], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.827289], dtype=float32), -1.0240618]. 
=============================================
[2019-03-27 03:23:18,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:19,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8617
[2019-03-27 03:23:19,012] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.83333333333334, 1.0, 2.0, 0.6496511776503019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 907875.9551292253, 907875.9551292253, 210358.0386491946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856200.0000, 
sim time next is 3856800.0000, 
raw observation next is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.5566666666666668, 1.0, 1.0, 0.5165511763305982, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23241581619798796, 0.23241581619798796, 0.2992705788343672], 
reward next is 0.7007, 
noisyNet noise sample is [array([-0.9931893], dtype=float32), 0.91590947]. 
=============================================
[2019-03-27 03:23:19,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6748103e-14 1.0000000e+00 3.7374104e-24 1.8828609e-17 2.2023558e-08], sum to 1.0000
[2019-03-27 03:23:19,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5800
[2019-03-27 03:23:19,730] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1790395.132860543 W.
[2019-03-27 03:23:19,735] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 0.6403180419674598, 1.0, 1.0, 0.6403180419674598, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1790395.132860543, 1790395.132860543, 349633.7526209999], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4000800.0000, 
sim time next is 4001400.0000, 
raw observation next is [29.5, 84.0, 1.0, 2.0, 0.3906682745602302, 1.0, 2.0, 0.3906682745602302, 1.0, 1.0, 0.6784615969384925, 6.911199999999999, 6.9112, 170.5573041426782, 1638406.940801993, 1638406.940801993, 347783.9581652957], 
processed observation next is [1.0, 0.30434782608695654, 0.5971563981042655, 0.84, 1.0, 1.0, 0.26586539103642187, 1.0, 1.0, 0.26586539103642187, 1.0, 0.5, 0.6078799962664543, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.45511303911166473, 0.45511303911166473, 0.5190805345750682], 
reward next is 0.4809, 
noisyNet noise sample is [array([0.936508], dtype=float32), 0.022936309]. 
=============================================
[2019-03-27 03:23:24,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7428546e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:23:24,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7782
[2019-03-27 03:23:24,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5449808257091711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761548.5621678563, 761548.5621678557, 190943.7741778573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5452465447021885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761920.0073329152, 761920.0073329152, 190988.9409948862], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4521042707255283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21164444648136532, 0.21164444648136532, 0.28505812088788984], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.7186644], dtype=float32), 0.16727217]. 
=============================================
[2019-03-27 03:23:24,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.60859 ]
 [67.37108 ]
 [67.470314]
 [67.56798 ]
 [67.63749 ]], R is [[67.30029297]
 [67.34230042]
 [67.38352966]
 [67.42398834]
 [67.46369171]].
[2019-03-27 03:23:26,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6173334e-28 1.0000000e+00 1.4659559e-37 1.1502231e-33 9.0777452e-30], sum to 1.0000
[2019-03-27 03:23:26,908] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4417
[2019-03-27 03:23:26,917] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 92.33333333333334, 1.0, 2.0, 0.8374828192902236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1170511.996205898, 1170511.996205898, 253321.216635226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4086600.0000, 
sim time next is 4087200.0000, 
raw observation next is [27.33333333333334, 90.66666666666667, 1.0, 2.0, 0.7806793902102795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091079.614132834, 1091079.614132834, 239225.2127699922], 
processed observation next is [1.0, 0.30434782608695654, 0.4944707740916275, 0.9066666666666667, 1.0, 1.0, 0.7357583014581681, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3030776705924539, 0.3030776705924539, 0.3570525563731227], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.1773279], dtype=float32), 3.2155378]. 
=============================================
[2019-03-27 03:23:35,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7580800e-18 1.0000000e+00 4.5226959e-25 1.5827585e-19 8.6869615e-15], sum to 1.0000
[2019-03-27 03:23:35,689] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3620
[2019-03-27 03:23:35,694] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.985891497624193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1378070.735662323, 1378070.735662324, 294662.6440260529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4344600.0000, 
sim time next is 4345200.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.9945036003310147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390116.541133479, 1390116.541133479, 297261.7540957159], 
processed observation next is [1.0, 0.30434782608695654, 0.6208530805687204, 0.84, 1.0, 1.0, 0.9933778317241141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3861434836481886, 0.3861434836481886, 0.44367425984435205], 
reward next is 0.5563, 
noisyNet noise sample is [array([-0.5736045], dtype=float32), 0.26838648]. 
=============================================
[2019-03-27 03:23:45,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:45,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5360
[2019-03-27 03:23:45,382] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5627929715881106, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564928155, 786448.2194417029, 786448.2194417029, 194021.6928220985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4296000.0000, 
sim time next is 4296600.0000, 
raw observation next is [36.5, 49.0, 1.0, 2.0, 0.5515995775436896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104266, 770800.8689649083, 770800.8689649076, 192078.8100410017], 
processed observation next is [1.0, 0.7391304347826086, 0.9289099526066351, 0.49, 1.0, 1.0, 0.4597585271610718, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439945152281, 0.21411135249025232, 0.21411135249025212, 0.28668479110597267], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.07198054], dtype=float32), 1.7584903]. 
=============================================
[2019-03-27 03:23:47,234] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1010452e-31 1.0000000e+00 0.0000000e+00 2.3033006e-37 1.4248075e-33], sum to 1.0000
[2019-03-27 03:23:47,243] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7288
[2019-03-27 03:23:47,252] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6172275254812768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862546.0791018306, 862546.0791018312, 203999.608627941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4406400.0000, 
sim time next is 4407000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6167573076257186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861888.7054186263, 861888.705418627, 203909.6718430076], 
processed observation next is [0.0, 0.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5382618164165284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23941352928295176, 0.23941352928295195, 0.30434279379553375], 
reward next is 0.6957, 
noisyNet noise sample is [array([1.4190601], dtype=float32), -0.069429316]. 
=============================================
[2019-03-27 03:23:47,266] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.342575]
 [67.459564]
 [67.555176]
 [67.61422 ]
 [67.646484]], R is [[63.10884094]
 [63.17327499]
 [63.23699951]
 [63.3000946 ]
 [63.36263657]].
[2019-03-27 03:23:49,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:49,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5040
[2019-03-27 03:23:50,000] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 79.0, 1.0, 2.0, 0.5792908206771125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809511.1376075802, 809511.1376075802, 196950.6193577511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4473000.0000, 
sim time next is 4473600.0000, 
raw observation next is [29.33333333333334, 79.0, 1.0, 2.0, 0.5745304247726876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802856.3628796691, 802856.3628796698, 196096.1862347673], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494474, 0.79, 1.0, 1.0, 0.48738605394299706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22301565635546364, 0.22301565635546383, 0.2926808749772646], 
reward next is 0.7073, 
noisyNet noise sample is [array([-1.5276028], dtype=float32), 0.99939823]. 
=============================================
[2019-03-27 03:23:51,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:51,308] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8395
[2019-03-27 03:23:51,314] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5817448407078488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812941.7405118359, 812941.7405118353, 197394.2623503959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5820091056976335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813311.1710184453, 813311.1710184453, 197442.0592235731], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49639651288871506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2259197697273459, 0.2259197697273459, 0.29468964063219866], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.44076622], dtype=float32), 0.032746296]. 
=============================================
[2019-03-27 03:23:51,333] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.57647]
 [71.57229]
 [71.53344]
 [71.47616]
 [71.4041 ]], R is [[71.76831055]
 [71.75601196]
 [71.74395752]
 [71.73221588]
 [71.7207489 ]].
[2019-03-27 03:23:53,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:23:53,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8582
[2019-03-27 03:23:53,680] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 79.0, 1.0, 2.0, 0.5951730778916804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831713.9715415365, 831713.9715415365, 199850.5617176967], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4436400.0000, 
sim time next is 4437000.0000, 
raw observation next is [30.5, 79.0, 1.0, 2.0, 0.5995807247293435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837875.7882265762, 837875.7882265768, 200668.4283339775], 
processed observation next is [0.0, 0.34782608695652173, 0.6445497630331753, 0.79, 1.0, 1.0, 0.5175671382281247, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23274327450738228, 0.23274327450738244, 0.2995051169163843], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.15435167], dtype=float32), -0.324494]. 
=============================================
[2019-03-27 03:23:53,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.696396]
 [72.70447 ]
 [72.70627 ]
 [72.617645]
 [72.63475 ]], R is [[72.65259552]
 [72.62778473]
 [72.60403442]
 [72.5819931 ]
 [72.5605011 ]].
[2019-03-27 03:23:59,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1533007e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:23:59,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2717
[2019-03-27 03:23:59,338] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 59.0, 1.0, 2.0, 0.5230332995882274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730868.8459204002, 730868.8459204008, 187284.1077407033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5065200.0000, 
sim time next is 5065800.0000, 
raw observation next is [31.83333333333334, 59.66666666666667, 1.0, 2.0, 0.5228005374421864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730543.4802124075, 730543.4802124068, 187246.0124310103], 
processed observation next is [0.0, 0.6521739130434783, 0.7077409162717223, 0.5966666666666667, 1.0, 1.0, 0.42506088848456186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20292874450344653, 0.20292874450344633, 0.27947166034479154], 
reward next is 0.7205, 
noisyNet noise sample is [array([-1.64549], dtype=float32), 0.016302789]. 
=============================================
[2019-03-27 03:24:05,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7550475e-22 1.0000000e+00 7.5734313e-31 1.2112005e-23 2.0177210e-16], sum to 1.0000
[2019-03-27 03:24:05,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7219
[2019-03-27 03:24:05,021] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7526746236852463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051920.691221194, 1051920.691221194, 232622.368919132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4762800.0000, 
sim time next is 4763400.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.703777206773278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 983551.1428812349, 983551.1428812349, 221659.2422921011], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.6431050684015398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.273208650800343, 0.273208650800343, 0.3308346899882106], 
reward next is 0.6692, 
noisyNet noise sample is [array([-0.6196525], dtype=float32), -1.4379394]. 
=============================================
[2019-03-27 03:24:05,560] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8455460e-32 1.0000000e+00 0.0000000e+00 5.0307730e-38 8.3088274e-35], sum to 1.0000
[2019-03-27 03:24:05,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4844
[2019-03-27 03:24:05,578] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 65.33333333333334, 1.0, 2.0, 0.522800453650983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730543.3630852008, 730543.3630852015, 187245.9398241549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5070000.0000, 
sim time next is 5070600.0000, 
raw observation next is [30.5, 66.5, 1.0, 2.0, 0.5255378022905445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734369.7620977546, 734369.7620977546, 187694.3964935441], 
processed observation next is [0.0, 0.6956521739130435, 0.6445497630331753, 0.665, 1.0, 1.0, 0.428358797940415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20399160058270963, 0.20399160058270963, 0.2801408902888718], 
reward next is 0.7199, 
noisyNet noise sample is [array([0.09416274], dtype=float32), 0.2291856]. 
=============================================
[2019-03-27 03:24:08,494] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-27 03:24:08,497] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:24:08,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:24:08,498] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:24:08,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:24:08,500] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:24:08,501] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:24:08,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:24:08,503] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:24:08,503] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:24:08,506] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:24:08,540] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-27 03:24:08,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-27 03:24:08,593] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-27 03:24:08,594] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-27 03:24:08,611] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-27 03:24:19,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1526346]
[2019-03-27 03:24:19,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.77535842, 60.04103425, 1.0, 2.0, 0.3705092122355318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562772.8653072861, 562772.8653072861, 171503.8357289605]
[2019-03-27 03:24:19,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:24:19,416] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5361742e-29 1.0000000e+00 0.0000000e+00 5.1289665e-35 3.8313617e-29], sampled 0.543653059228846
[2019-03-27 03:24:36,407] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1526346]
[2019-03-27 03:24:36,409] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.4, 93.0, 1.0, 2.0, 0.3655558956125144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558362.2124474861, 558362.2124474861, 171218.5538552931]
[2019-03-27 03:24:36,409] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:24:36,412] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9634402e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8527147644843897
[2019-03-27 03:24:41,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1526346]
[2019-03-27 03:24:41,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.55, 91.83333333333333, 1.0, 2.0, 0.4132499646452796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610400.2663982849, 610400.2663982855, 175329.9113797108]
[2019-03-27 03:24:41,111] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:24:41,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.8343628e-29 1.0000000e+00 0.0000000e+00 7.4983614e-34 9.3466401e-28], sampled 0.4989932117435618
[2019-03-27 03:25:00,457] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1526346]
[2019-03-27 03:25:00,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 78.16666666666667, 1.0, 2.0, 0.8500554522355913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1188094.02996442, 1188094.02996442, 256566.9197807291]
[2019-03-27 03:25:00,461] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:25:00,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.36466885e-25 1.00000000e+00 1.51502614e-36 1.05180835e-29
 1.95205761e-23], sampled 0.0054417482793372995
[2019-03-27 03:25:56,634] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1526346]
[2019-03-27 03:25:56,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.2, 89.0, 1.0, 2.0, 0.5917065086266571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 826867.7907030202, 826867.7907030209, 199210.1833325787]
[2019-03-27 03:25:56,637] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:25:56,639] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0897116e-26 1.0000000e+00 2.7514789e-37 5.4276042e-30 5.3263604e-23], sampled 0.9293371101111715
[2019-03-27 03:26:03,666] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8201.0183 3149940230.9047 1059.0000
[2019-03-27 03:26:03,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8451.0348 2989754390.5629 671.0000
[2019-03-27 03:26:03,924] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8490.9758 2922407334.4993 813.0000
[2019-03-27 03:26:03,979] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8711.4222 2834397593.6960 623.0000
[2019-03-27 03:26:03,980] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8778.0916 2777629911.0423 659.0000
[2019-03-27 03:26:04,997] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1825000, evaluation results [1825000.0, 8201.018273706344, 3149940230.9047084, 1059.0, 8490.975832280556, 2922407334.4993052, 813.0, 8778.091553486887, 2777629911.042322, 659.0, 8451.034812156118, 2989754390.5629067, 671.0, 8711.422234175718, 2834397593.6959996, 623.0]
[2019-03-27 03:26:05,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3295125e-21 2.8371494e-17 1.1211694e-30 1.6584347e-16 1.0000000e+00], sum to 1.0000
[2019-03-27 03:26:05,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5753
[2019-03-27 03:26:05,598] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.571248437770778, 1.0, 2.0, 0.571248437770778, 1.0, 2.0, 0.9920696216626841, 6.911200000000001, 6.9112, 170.5573041426782, 2396552.054931919, 2396552.054931919, 467911.7280978712], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.9833083767825556, 6.9112, 6.9112, 170.5573041426782, 2375367.316592513, 2375367.316592513, 463945.6516273014], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4773537141093912, 1.0, 1.0, 0.4773537141093912, 1.0, 1.0, 0.9796443619299456, 0.0, 0.0, 0.8375144448122397, 0.6598242546090314, 0.6598242546090314, 0.6924561964586589], 
reward next is 0.3075, 
noisyNet noise sample is [array([-0.5102147], dtype=float32), -0.047523484]. 
=============================================
[2019-03-27 03:26:15,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3621094e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:26:15,839] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8939
[2019-03-27 03:26:15,846] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5102195583962155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712957.3532896775, 712957.3532896775, 185212.7387147144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5133600.0000, 
sim time next is 5134200.0000, 
raw observation next is [30.16666666666666, 65.5, 1.0, 2.0, 0.5112979475508288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714464.7514776031, 714464.7514776024, 185385.2241250137], 
processed observation next is [0.0, 0.43478260869565216, 0.6287519747235385, 0.655, 1.0, 1.0, 0.41120234644678166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19846243096600086, 0.19846243096600066, 0.27669436436569206], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.05293542], dtype=float32), -0.18200327]. 
=============================================
[2019-03-27 03:26:15,916] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.1914187e-25 1.0000000e+00 4.0852085e-35 1.2815995e-28 1.0083404e-20], sum to 1.0000
[2019-03-27 03:26:15,929] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8235
[2019-03-27 03:26:15,935] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 84.0, 1.0, 2.0, 0.5068348207954212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708226.0998996464, 708226.0998996471, 184673.5229565773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5009400.0000, 
sim time next is 5010000.0000, 
raw observation next is [26.66666666666666, 84.0, 1.0, 2.0, 0.5039392676681161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704178.6549991912, 704178.6549991912, 184215.2127589873], 
processed observation next is [1.0, 1.0, 0.4628751974723536, 0.84, 1.0, 1.0, 0.4023364670700194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19560518194421977, 0.19560518194421977, 0.2749480787447572], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.503043], dtype=float32), 0.45236692]. 
=============================================
[2019-03-27 03:26:15,949] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.34104 ]
 [75.294876]
 [75.230865]
 [75.167854]
 [74.98398 ]], R is [[75.36113739]
 [75.33189392]
 [75.30233002]
 [75.2726059 ]
 [75.24288177]].
[2019-03-27 03:26:21,732] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4791164e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:26:21,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9943
[2019-03-27 03:26:21,747] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.00000000000001, 1.0, 2.0, 0.517809345509258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723566.5849451869, 723566.5849451869, 186433.9142561213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5052000.0000, 
sim time next is 5052600.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5180843330622806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723950.9727686801, 723950.9727686801, 186478.4444092323], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.4193787145328682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20109749243574446, 0.20109749243574446, 0.27832603643169], 
reward next is 0.7217, 
noisyNet noise sample is [array([-0.08262196], dtype=float32), 0.05280352]. 
=============================================
[2019-03-27 03:26:28,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6848655e-29 1.0000000e+00 0.0000000e+00 2.2851968e-34 1.7759459e-30], sum to 1.0000
[2019-03-27 03:26:29,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4765
[2019-03-27 03:26:29,015] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 84.0, 1.0, 2.0, 0.6096307443207217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 851925.6814915441, 851925.6814915447, 202554.0961907282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5355000.0000, 
sim time next is 5355600.0000, 
raw observation next is [29.76666666666667, 84.0, 1.0, 2.0, 0.6082961798061985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850059.9541302391, 850059.9541302391, 202301.9235507029], 
processed observation next is [1.0, 1.0, 0.6097946287519749, 0.84, 1.0, 1.0, 0.5280676865134922, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23612776503617752, 0.23612776503617752, 0.301943169478661], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.643032], dtype=float32), 0.17029247]. 
=============================================
[2019-03-27 03:26:35,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5205282e-20 1.0000000e+00 5.6944688e-28 7.7594565e-22 2.1324218e-17], sum to 1.0000
[2019-03-27 03:26:35,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8489
[2019-03-27 03:26:35,628] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.7360339888944364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028652.85997724, 1028652.859977241, 228813.0086104057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5202000.0000, 
sim time next is 5202600.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8350389186313528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167094.389156702, 1167094.389156702, 252690.9798658147], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.801251709194401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3241928858768617, 0.3241928858768617, 0.3771507162176339], 
reward next is 0.6228, 
noisyNet noise sample is [array([1.2036562], dtype=float32), -0.41996083]. 
=============================================
[2019-03-27 03:26:40,529] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6098268e-17 1.0000000e+00 2.2279998e-25 4.8054086e-19 8.7031483e-15], sum to 1.0000
[2019-03-27 03:26:40,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6443
[2019-03-27 03:26:40,543] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8658029850654017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210116.371567856, 1210116.371567856, 260708.0426498714], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5286600.0000, 
sim time next is 5287200.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.8525897338479069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1191638.098127141, 1191638.098127142, 257233.0642929978], 
processed observation next is [1.0, 0.17391304347826086, 0.5545023696682465, 0.88, 1.0, 1.0, 0.8223972696962734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3310105828130947, 0.33101058281309503, 0.38392994670596686], 
reward next is 0.6161, 
noisyNet noise sample is [array([-2.0272062], dtype=float32), -2.1310341]. 
=============================================
[2019-03-27 03:26:41,793] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.5813744e-25 1.0000000e+00 9.1605090e-36 3.2232070e-28 8.2138229e-25], sum to 1.0000
[2019-03-27 03:26:41,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5697
[2019-03-27 03:26:41,808] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 82.0, 1.0, 2.0, 0.6084672922804134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 850299.169977118, 850299.1699771174, 202333.9938318708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5436000.0000, 
sim time next is 5436600.0000, 
raw observation next is [29.93333333333333, 81.83333333333334, 1.0, 2.0, 0.604512572258046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844770.471102389, 844770.471102389, 201589.642615303], 
processed observation next is [1.0, 0.9565217391304348, 0.6176935229067929, 0.8183333333333335, 1.0, 1.0, 0.523509123202465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23465846419510808, 0.23465846419510808, 0.30088006360492986], 
reward next is 0.6991, 
noisyNet noise sample is [array([-0.48696786], dtype=float32), 0.31829664]. 
=============================================
[2019-03-27 03:26:41,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0417284e-16 3.2169562e-12 2.2780759e-20 1.9338167e-09 1.0000000e+00], sum to 1.0000
[2019-03-27 03:26:41,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6824
[2019-03-27 03:26:41,848] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.73333333333333, 54.0, 1.0, 2.0, 0.7183857854641522, 1.0, 2.0, 0.6797829322463388, 1.0, 2.0, 1.03, 7.005099181902835, 6.9112, 170.5573041426782, 2852405.352825003, 2785141.549253491, 527199.7356752503], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5308800.0000, 
sim time next is 5309400.0000, 
raw observation next is [36.06666666666666, 52.5, 1.0, 2.0, 0.7403819346765855, 1.0, 2.0, 0.6907810068525553, 1.0, 2.0, 1.03, 7.005100916450018, 6.9112, 170.5573041426782, 2898607.402413545, 2831342.356315329, 534387.4803038546], 
processed observation next is [1.0, 0.43478260869565216, 0.9083728278041072, 0.525, 1.0, 1.0, 0.6872071502127536, 1.0, 1.0, 0.6274469962078979, 1.0, 1.0, 1.0365853658536586, 0.009390091645001774, 0.0, 0.8375144448122397, 0.8051687228926514, 0.7864839878653692, 0.7975932541848576], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29569224], dtype=float32), 1.6821775]. 
=============================================
[2019-03-27 03:26:42,980] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.058324e-20 1.000000e+00 1.355355e-27 1.721885e-21 3.133882e-16], sum to 1.0000
[2019-03-27 03:26:42,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6806
[2019-03-27 03:26:42,996] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.98660263004508, 6.9112, 168.9124035686867, 1507284.606958002, 1453791.561157479, 311353.3411929866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5459400.0000, 
sim time next is 5460000.0000, 
raw observation next is [27.63333333333333, 92.0, 1.0, 2.0, 0.9298339555523581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128951144307, 1299666.018307709, 1299666.01830771, 278278.9615278119], 
processed observation next is [1.0, 0.17391304347826086, 0.5086887835703, 0.92, 1.0, 1.0, 0.9154625970510339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439643669834, 0.36101833841880804, 0.36101833841880837, 0.41534173362359983], 
reward next is 0.5847, 
noisyNet noise sample is [array([-0.03384148], dtype=float32), 0.37744898]. 
=============================================
[2019-03-27 03:26:43,004] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[38.32237 ]
 [39.372684]
 [40.126507]
 [40.81676 ]
 [40.883663]], R is [[38.67512894]
 [38.44665909]
 [38.60546875]
 [38.78251648]
 [38.98300171]].
[2019-03-27 03:26:49,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4332933e-27 1.0000000e+00 1.0969037e-37 3.6053564e-31 3.6757177e-26], sum to 1.0000
[2019-03-27 03:26:49,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0945
[2019-03-27 03:26:49,958] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.51666666666667, 81.66666666666667, 1.0, 2.0, 0.5852508628324261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817843.0113683267, 817843.0113683267, 198030.4800677657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5440200.0000, 
sim time next is 5440800.0000, 
raw observation next is [29.43333333333334, 82.33333333333334, 1.0, 2.0, 0.5860506028011502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818961.0172362131, 818961.0172362131, 198176.0797642226], 
processed observation next is [1.0, 1.0, 0.5939968404423385, 0.8233333333333335, 1.0, 1.0, 0.5012657865074098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22748917145450365, 0.22748917145450365, 0.2957851936779442], 
reward next is 0.7042, 
noisyNet noise sample is [array([-1.1738849], dtype=float32), -1.7630215]. 
=============================================
[2019-03-27 03:26:53,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:26:53,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2465
[2019-03-27 03:26:53,787] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.90000000000001, 84.33333333333334, 1.0, 2.0, 0.5078105802265798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709590.0336350915, 709590.0336350915, 184828.9231312728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5638800.0000, 
sim time next is 5639400.0000, 
raw observation next is [27.05, 83.5, 1.0, 2.0, 0.5084926615102403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710543.4599184075, 710543.4599184075, 184937.4575291791], 
processed observation next is [0.0, 0.2608695652173913, 0.4810426540284361, 0.835, 1.0, 1.0, 0.4078224837472775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19737318331066875, 0.19737318331066875, 0.27602605601370017], 
reward next is 0.7240, 
noisyNet noise sample is [array([-1.0166657], dtype=float32), -1.4476329]. 
=============================================
[2019-03-27 03:26:57,546] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8795167e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:26:57,554] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5243
[2019-03-27 03:26:57,559] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 74.0, 1.0, 2.0, 0.5374214912078328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750981.5200375767, 750981.520037576, 189667.3210718998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5648400.0000, 
sim time next is 5649000.0000, 
raw observation next is [29.66666666666667, 73.33333333333334, 1.0, 2.0, 0.5421571164561202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757601.3433202925, 757601.3433202932, 190464.949132379], 
processed observation next is [0.0, 0.391304347826087, 0.6050552922590839, 0.7333333333333334, 1.0, 1.0, 0.4483820680194219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21044481758897016, 0.21044481758897035, 0.2842760434811627], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.3392985], dtype=float32), -1.0060056]. 
=============================================
[2019-03-27 03:26:57,593] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[72.98897 ]
 [72.89229 ]
 [72.89987 ]
 [72.90605 ]
 [72.915726]], R is [[72.97895813]
 [72.96607971]
 [72.95410156]
 [72.94294739]
 [72.93256378]].
[2019-03-27 03:26:59,992] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5736149e-25 1.0000000e+00 3.7179186e-37 9.1017096e-29 1.1995364e-21], sum to 1.0000
[2019-03-27 03:27:00,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0241
[2019-03-27 03:27:00,006] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 88.0, 1.0, 2.0, 0.5331005679995574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744941.4353386366, 744941.435338636, 188944.6496922284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5871600.0000, 
sim time next is 5872200.0000, 
raw observation next is [26.95, 88.33333333333334, 1.0, 2.0, 0.5317020573634238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742986.5080006435, 742986.5080006435, 188712.1889039932], 
processed observation next is [1.0, 1.0, 0.476303317535545, 0.8833333333333334, 1.0, 1.0, 0.43578561128123344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20638514111128986, 0.20638514111128986, 0.28165998343879584], 
reward next is 0.7183, 
noisyNet noise sample is [array([1.3077478], dtype=float32), -0.78091896]. 
=============================================
[2019-03-27 03:27:00,886] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:27:00,887] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:27:00,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:27:00,889] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:27:00,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:27:00,890] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:27:00,894] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:27:00,895] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:27:00,895] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:27:00,896] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:27:00,892] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:27:00,924] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-27 03:27:00,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-27 03:27:00,968] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-27 03:27:00,986] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-27 03:27:00,988] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-27 03:27:19,760] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:27:19,762] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.636897505, 69.967141325, 1.0, 2.0, 0.2830809413047386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460533.6952431877, 460533.6952431871, 164258.011551593]
[2019-03-27 03:27:19,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:27:19,766] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3080668e-31 1.0000000e+00 0.0000000e+00 2.1423145e-37 1.3260677e-32], sampled 0.6835746544308392
[2019-03-27 03:27:23,790] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:27:23,791] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.96666666666667, 57.0, 1.0, 2.0, 0.3308875048659061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518270.5186193026, 518270.5186193032, 168314.5926470887]
[2019-03-27 03:27:23,792] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:27:23,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3838412e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.18810270199417667
[2019-03-27 03:27:42,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:27:42,614] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.83333333333334, 95.0, 1.0, 2.0, 0.3513653200284982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542214.1043638098, 542214.1043638104, 170026.38428374]
[2019-03-27 03:27:42,617] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:27:42,619] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7161811943499551
[2019-03-27 03:27:44,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:27:44,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.6990864653169488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076596.452656342, 1076596.452656341, 233145.4535431424]
[2019-03-27 03:27:44,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:27:44,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1881402e-27 1.0000000e+00 3.2435298e-38 9.8416172e-32 1.0449796e-25], sampled 0.5153899343612767
[2019-03-27 03:27:50,685] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:27:50,686] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.36794706, 89.83799511, 1.0, 2.0, 0.5920453865234211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827341.5329144243, 827341.5329144243, 199273.2532748559]
[2019-03-27 03:27:50,687] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:27:50,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1029472e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7484219389342264
[2019-03-27 03:28:01,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:28:01,054] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.75, 67.0, 1.0, 2.0, 0.5791251803950469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809279.5807963134, 809279.5807963134, 196921.1092845628]
[2019-03-27 03:28:01,057] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:28:01,060] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2531469e-31 1.0000000e+00 0.0000000e+00 2.9253834e-37 7.0736555e-33], sampled 0.9171130221660757
[2019-03-27 03:28:09,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:28:09,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.76666666666667, 56.33333333333333, 1.0, 2.0, 0.5302716969763916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740987.0627717652, 740987.0627717652, 188475.3566198004]
[2019-03-27 03:28:09,980] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:28:09,984] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9013037e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0044658e-37], sampled 0.8516641080496908
[2019-03-27 03:28:19,567] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:28:19,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.29888659, 86.74153320666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.974974178234598, 6.9112, 168.9123272460218, 1499029.340793391, 1453785.912365395, 311353.0135423424]
[2019-03-27 03:28:19,569] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:28:19,574] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2270766e-17 1.0000000e+00 2.1106170e-27 1.1617672e-19 1.1616965e-11], sampled 0.24189941293042994
[2019-03-27 03:28:28,630] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15637977]
[2019-03-27 03:28:28,632] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.05, 94.5, 1.0, 2.0, 0.5340537374898793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746273.8384892357, 746273.8384892357, 189103.2657980068]
[2019-03-27 03:28:28,633] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:28:28,635] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0104371e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.8185995e-36], sampled 0.7336880053717738
[2019-03-27 03:28:55,642] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8109.8293 3002201309.1070 1471.0000
[2019-03-27 03:28:55,782] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8269.2675 2926906020.2554 1309.0000
[2019-03-27 03:28:55,835] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7968.0595 3159726354.7087 1661.0000
[2019-03-27 03:28:55,920] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8539.6550 2839856360.4775 1041.0000
[2019-03-27 03:28:56,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8682.7876 2778869886.6629 888.0000
[2019-03-27 03:28:57,017] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1850000, evaluation results [1850000.0, 7968.0595451332765, 3159726354.708706, 1661.0, 8269.267535984281, 2926906020.255402, 1309.0, 8682.787623142944, 2778869886.662895, 888.0, 8109.829258388447, 3002201309.106975, 1471.0, 8539.654973918841, 2839856360.4775496, 1041.0]
[2019-03-27 03:28:57,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4377353e-31 1.0000000e+00 0.0000000e+00 2.8120262e-37 2.1917365e-31], sum to 1.0000
[2019-03-27 03:28:57,690] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4677
[2019-03-27 03:28:57,698] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 79.0, 1.0, 2.0, 0.5495153097932858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767887.2769922784, 767887.276992279, 191717.473560603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5772600.0000, 
sim time next is 5773200.0000, 
raw observation next is [28.6, 80.0, 1.0, 2.0, 0.5487683742202893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766843.1398199901, 766843.1398199908, 191589.5144488959], 
processed observation next is [0.0, 0.8260869565217391, 0.5545023696682465, 0.8, 1.0, 1.0, 0.4563474388196257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2130119832833306, 0.2130119832833308, 0.2859544991774566], 
reward next is 0.7140, 
noisyNet noise sample is [array([0.8233508], dtype=float32), 1.5280349]. 
=============================================
[2019-03-27 03:29:00,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8082126e-19 1.0000000e+00 2.0619829e-28 3.0984518e-20 1.3214851e-13], sum to 1.0000
[2019-03-27 03:29:00,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5689
[2019-03-27 03:29:00,949] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 89.0, 1.0, 2.0, 0.5339738909064046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746162.2236068867, 746162.2236068867, 189089.9487435603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5796000.0000, 
sim time next is 5796600.0000, 
raw observation next is [26.75, 89.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.297362648904276, 6.9112, 169.1240599340449, 2439023.932197645, 1454403.421568836, 310945.5084374915], 
processed observation next is [1.0, 0.08695652173913043, 0.4668246445497631, 0.8933333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.13861626489042758, 0.0, 0.8304765596058125, 0.6775066478326791, 0.4040009504357878, 0.4640977737873007], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8455855], dtype=float32), -2.861919]. 
=============================================
[2019-03-27 03:29:10,491] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0581112e-32 1.0000000e+00 0.0000000e+00 1.3883841e-37 1.7732286e-33], sum to 1.0000
[2019-03-27 03:29:10,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4973
[2019-03-27 03:29:10,510] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.23333333333333, 82.0, 1.0, 2.0, 0.5136162823872085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717705.3826788549, 717705.3826788554, 185756.5918737479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6389400.0000, 
sim time next is 6390000.0000, 
raw observation next is [27.2, 82.0, 1.0, 2.0, 0.5123752175565013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715970.587428095, 715970.5874280944, 185557.364716329], 
processed observation next is [0.0, 1.0, 0.4881516587677725, 0.82, 1.0, 1.0, 0.41250026211626656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1988807187300264, 0.19888071873002622, 0.27695129062138657], 
reward next is 0.7230, 
noisyNet noise sample is [array([1.8148903], dtype=float32), -1.520588]. 
=============================================
[2019-03-27 03:29:10,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.299614]
 [73.29388 ]
 [73.3053  ]
 [73.313736]
 [73.29884 ]], R is [[73.44044495]
 [73.42878723]
 [73.4169693 ]
 [73.40499878]
 [73.39287567]].
[2019-03-27 03:29:16,536] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8087580e-29 1.0000000e+00 0.0000000e+00 3.4273587e-35 3.4556062e-30], sum to 1.0000
[2019-03-27 03:29:16,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0091
[2019-03-27 03:29:16,556] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 90.0, 1.0, 2.0, 0.5575782937230792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779158.5436857993, 779158.5436857993, 193109.0351841725], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5955600.0000, 
sim time next is 5956200.0000, 
raw observation next is [27.4, 90.5, 1.0, 2.0, 0.5588406130134597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780923.1540094193, 780923.1540094193, 193328.5115715217], 
processed observation next is [1.0, 0.9565217391304348, 0.4976303317535545, 0.905, 1.0, 1.0, 0.46848266628127666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2169230983359498, 0.2169230983359498, 0.2885500172709279], 
reward next is 0.7114, 
noisyNet noise sample is [array([-1.61007], dtype=float32), -1.0903518]. 
=============================================
[2019-03-27 03:29:28,903] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4478307e-14 4.8024172e-05 7.5416208e-22 6.4738617e-11 9.9995196e-01], sum to 1.0000
[2019-03-27 03:29:28,913] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1022
[2019-03-27 03:29:28,919] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.4, 77.5, 1.0, 2.0, 0.4858285218898289, 1.0, 2.0, 0.4858285218898289, 1.0, 2.0, 0.8344575107917509, 6.911200000000001, 6.9112, 170.5573041426782, 2037874.944926236, 2037874.944926235, 403856.987197289], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6425400.0000, 
sim time next is 6426000.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.4696302162077833, 1.0, 2.0, 0.4696302162077833, 1.0, 2.0, 0.8066212765292323, 6.9112, 6.9112, 170.5573041426782, 1969866.443180368, 1969866.443180368, 393263.7959363842], 
processed observation next is [1.0, 0.391304347826087, 0.5497630331753555, 0.77, 1.0, 1.0, 0.3610002604913052, 1.0, 1.0, 0.3610002604913052, 1.0, 1.0, 0.7641722884502832, 0.0, 0.0, 0.8375144448122397, 0.5471851231056578, 0.5471851231056578, 0.5869608894572899], 
reward next is 0.4130, 
noisyNet noise sample is [array([0.1611935], dtype=float32), -0.62293017]. 
=============================================
[2019-03-27 03:29:28,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.785984]
 [61.865803]
 [61.0617  ]
 [60.106083]
 [61.97349 ]], R is [[61.76869583]
 [61.54823685]
 [61.32618332]
 [61.06502533]
 [60.45437622]].
[2019-03-27 03:29:31,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.573666e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:29:31,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5823
[2019-03-27 03:29:31,958] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 90.5, 1.0, 2.0, 0.5249050761389132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733485.3054963945, 733485.3054963939, 187590.4168267733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6237000.0000, 
sim time next is 6237600.0000, 
raw observation next is [26.56666666666667, 90.33333333333334, 1.0, 2.0, 0.5244757669673655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732885.1958526161, 732885.1958526167, 187520.0216786226], 
processed observation next is [0.0, 0.17391304347826086, 0.45813586097946307, 0.9033333333333334, 1.0, 1.0, 0.4270792373100789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20357922107017115, 0.2035792210701713, 0.2798806293710785], 
reward next is 0.7201, 
noisyNet noise sample is [array([-1.8363626], dtype=float32), -0.8382643]. 
=============================================
[2019-03-27 03:29:38,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6060006e-17 1.5981024e-07 4.6408604e-24 1.3539685e-14 9.9999988e-01], sum to 1.0000
[2019-03-27 03:29:38,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0670
[2019-03-27 03:29:38,324] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.507253279712416, 1.0, 2.0, 0.507253279712416, 1.0, 2.0, 0.8689880479328246, 6.9112, 6.9112, 170.5573041426782, 2127833.310935632, 2127833.310935632, 417991.3097346764], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6453000.0000, 
sim time next is 6453600.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.5017872264844458, 1.0, 2.0, 0.5017872264844458, 1.0, 2.0, 0.8595774198484066, 6.911200000000001, 6.9112, 170.5573041426782, 2104881.705125514, 2104881.705125513, 414218.8184977972], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.3997436463668022, 1.0, 1.0, 0.3997436463668022, 1.0, 1.0, 0.8287529510346421, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.584689362534865, 0.5846893625348647, 0.6182370425340257], 
reward next is 0.3818, 
noisyNet noise sample is [array([0.83241117], dtype=float32), -0.72615314]. 
=============================================
[2019-03-27 03:29:38,538] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5113883e-14 6.4183400e-06 7.6193472e-22 8.9853890e-15 9.9999356e-01], sum to 1.0000
[2019-03-27 03:29:38,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0668
[2019-03-27 03:29:38,552] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.63333333333333, 57.0, 1.0, 2.0, 0.4777642860218745, 1.0, 2.0, 0.4777642860218745, 1.0, 2.0, 0.8138579704068603, 6.911200000000001, 6.9112, 170.5573041426782, 2004016.753131374, 2004016.753131374, 397356.1056921789], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6532800.0000, 
sim time next is 6533400.0000, 
raw observation next is [31.51666666666667, 57.5, 1.0, 2.0, 0.4874771393881191, 1.0, 2.0, 0.4874771393881191, 1.0, 2.0, 0.8306674278317507, 6.9112, 6.9112, 170.5573041426782, 2044796.898946655, 2044796.898946655, 403767.44467527], 
processed observation next is [1.0, 0.6086956521739131, 0.6927330173775673, 0.575, 1.0, 1.0, 0.38250257757604705, 1.0, 1.0, 0.38250257757604705, 1.0, 1.0, 0.7934968632094521, 0.0, 0.0, 0.8375144448122397, 0.567999138596293, 0.567999138596293, 0.6026379771272686], 
reward next is 0.3974, 
noisyNet noise sample is [array([-1.6728737], dtype=float32), -0.38516384]. 
=============================================
[2019-03-27 03:29:41,692] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3288615e-19 1.0000000e+00 5.5808539e-29 3.0301175e-21 2.6051581e-14], sum to 1.0000
[2019-03-27 03:29:41,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0933
[2019-03-27 03:29:41,705] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 88.33333333333334, 1.0, 2.0, 0.52873246730019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738835.4375962825, 738835.4375962825, 188220.286648948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6480600.0000, 
sim time next is 6481200.0000, 
raw observation next is [26.83333333333333, 88.66666666666667, 1.0, 2.0, 0.5291646017967019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739439.5000042234, 739439.5000042241, 188291.7101906741], 
processed observation next is [1.0, 0.0, 0.470774091627172, 0.8866666666666667, 1.0, 1.0, 0.4327284358996408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20539986111228428, 0.20539986111228448, 0.28103240326966283], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.04944317], dtype=float32), -1.732901]. 
=============================================
[2019-03-27 03:29:42,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4103353e-13 2.4656809e-04 2.5219295e-21 1.8963070e-13 9.9975342e-01], sum to 1.0000
[2019-03-27 03:29:42,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6182
[2019-03-27 03:29:42,070] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.7, 66.0, 1.0, 2.0, 0.418691027873889, 1.0, 2.0, 0.418691027873889, 1.0, 2.0, 0.7114783744457898, 6.9112, 6.9112, 170.5573041426782, 1756026.656458102, 1756026.656458102, 361134.5338637072], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6712200.0000, 
sim time next is 6712800.0000, 
raw observation next is [29.66666666666667, 66.33333333333333, 1.0, 2.0, 0.4316016749138767, 1.0, 2.0, 0.4316016749138767, 1.0, 2.0, 0.733722633885961, 6.911199999999999, 6.9112, 170.5573041426782, 1810220.773710351, 1810220.773710351, 368619.4806324209], 
processed observation next is [1.0, 0.6956521739130435, 0.6050552922590839, 0.6633333333333333, 1.0, 1.0, 0.31518274086009235, 1.0, 1.0, 0.31518274086009235, 1.0, 1.0, 0.6752715047389769, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5028391038084308, 0.5028391038084308, 0.5501783293021207], 
reward next is 0.4498, 
noisyNet noise sample is [array([1.845728], dtype=float32), -0.36851642]. 
=============================================
[2019-03-27 03:29:49,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:29:49,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6260
[2019-03-27 03:29:49,327] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 62.0, 1.0, 2.0, 0.448749307761302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 642065.4255987656, 642065.4255987662, 177888.9281493924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6948000.0000, 
sim time next is 6948600.0000, 
raw observation next is [29.25, 61.5, 1.0, 2.0, 0.4478497822007853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639732.0855265777, 639732.0855265782, 177625.4804319562], 
processed observation next is [0.0, 0.43478260869565216, 0.5853080568720379, 0.615, 1.0, 1.0, 0.33475877373588586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17770335709071602, 0.1777033570907162, 0.26511265736112866], 
reward next is 0.7349, 
noisyNet noise sample is [array([0.5971349], dtype=float32), 0.64342254]. 
=============================================
[2019-03-27 03:29:52,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4919305e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2656373e-37], sum to 1.0000
[2019-03-27 03:29:52,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5915
[2019-03-27 03:29:52,539] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 92.16666666666667, 1.0, 2.0, 0.6030354978630474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842705.5283739825, 842705.5283739825, 201306.1579086976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6581400.0000, 
sim time next is 6582000.0000, 
raw observation next is [25.83333333333334, 92.33333333333334, 1.0, 2.0, 0.5688200332436789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794873.6009094777, 794873.6009094783, 195076.6319416119], 
processed observation next is [1.0, 0.17391304347826086, 0.42338072669826254, 0.9233333333333335, 1.0, 1.0, 0.4805060641490107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22079822247485492, 0.22079822247485506, 0.29115915215165955], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.12977445], dtype=float32), -0.8731972]. 
=============================================
[2019-03-27 03:29:52,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.735626]
 [63.370266]
 [63.357567]
 [63.48756 ]
 [63.64775 ]], R is [[63.83611298]
 [63.89729691]
 [63.94223785]
 [63.97636414]
 [64.00613403]].
[2019-03-27 03:29:52,837] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 03:29:52,839] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:29:52,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:29:52,840] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:29:52,841] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:29:52,843] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:29:52,843] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:29:52,845] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:29:52,846] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:29:52,846] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:29:52,849] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:29:52,868] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-27 03:29:52,893] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-27 03:29:52,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-27 03:29:52,929] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-27 03:29:52,930] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-27 03:30:22,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:30:22,642] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.77134036333334, 89.83217757166668, 1.0, 2.0, 0.417592814814437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621530.2721666737, 621530.2721666731, 176507.792873487]
[2019-03-27 03:30:22,643] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:30:22,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.9947405e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5864529e-36], sampled 0.6725781087586179
[2019-03-27 03:31:00,137] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:00,139] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.828745, 58.3066284, 1.0, 2.0, 0.5068172867820312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708201.5905626246, 708201.5905626239, 184669.8353800323]
[2019-03-27 03:31:00,139] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:31:00,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0616824e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8943542e-37], sampled 0.22426883071189252
[2019-03-27 03:31:04,914] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:04,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.90805784, 81.485421205, 1.0, 2.0, 0.5006976733155306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717602.5417426274, 717602.5417426274, 185993.5745353693]
[2019-03-27 03:31:04,916] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:31:04,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0166335e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.18344827876506387
[2019-03-27 03:31:12,396] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:12,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.51976926333333, 44.89502365666667, 1.0, 2.0, 0.6923763783077593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 969765.8476843559, 969765.8476843565, 219494.6715414965]
[2019-03-27 03:31:12,400] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:31:12,402] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8542825e-30 1.0000000e+00 0.0000000e+00 4.7140588e-36 3.6775156e-32], sampled 0.7023955485594997
[2019-03-27 03:31:16,059] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:16,061] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.20926552333333, 73.30731818166666, 1.0, 2.0, 0.5605486949720874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806044.908656835, 806044.908656835, 196526.7057483254]
[2019-03-27 03:31:16,062] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:31:16,064] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8221707e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.707080009135554
[2019-03-27 03:31:21,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:21,616] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.55, 87.0, 1.0, 2.0, 0.5927266444802718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828293.9139070093, 828293.9139070093, 199397.9565251236]
[2019-03-27 03:31:21,617] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:31:21,619] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5254928739713498
[2019-03-27 03:31:24,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15711509]
[2019-03-27 03:31:24,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.8, 67.0, 1.0, 2.0, 0.5486896964765844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766733.1566900198, 766733.1566900198, 191575.9012487023]
[2019-03-27 03:31:24,549] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:31:24,553] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3274833e-30 1.0000000e+00 0.0000000e+00 6.2825323e-36 7.6047802e-32], sampled 0.15951300963609416
[2019-03-27 03:31:43,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.8185 2779064209.4398 928.0000
[2019-03-27 03:31:44,115] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7888.2357 3163679519.9585 1855.0000
[2019-03-27 03:31:44,144] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7240 2841997033.6222 1148.0000
[2019-03-27 03:31:44,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.3652 3007106517.7840 1731.0000
[2019-03-27 03:31:44,173] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.1488 2927230509.0498 1325.0000
[2019-03-27 03:31:45,189] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1875000, evaluation results [1875000.0, 7888.235730337704, 3163679519.9585376, 1855.0, 8262.148778938012, 2927230509.049844, 1325.0, 8662.818498771145, 2779064209.4398055, 928.0, 8011.36521816769, 3007106517.7839966, 1731.0, 8496.72399030906, 2841997033.6222267, 1148.0]
[2019-03-27 03:31:46,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5455629e-27 1.0000000e+00 3.0865220e-37 6.1787225e-34 3.4552293e-27], sum to 1.0000
[2019-03-27 03:31:46,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4326
[2019-03-27 03:31:46,277] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 67.0, 1.0, 2.0, 0.4681830520244758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655446.089847496, 655446.0898474953, 178922.3800552485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6718200.0000, 
sim time next is 6718800.0000, 
raw observation next is [28.53333333333333, 67.0, 1.0, 2.0, 0.4637931153206023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652873.4973243435, 652873.497324343, 178739.1000024383], 
processed observation next is [1.0, 0.782608695652174, 0.5513428120063191, 0.67, 1.0, 1.0, 0.3539676088200027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1813537492567621, 0.18135374925676193, 0.26677477612304223], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.01283989], dtype=float32), 1.2180755]. 
=============================================
[2019-03-27 03:31:48,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:48,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6584
[2019-03-27 03:31:48,171] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 52.66666666666667, 1.0, 2.0, 0.4683991813756679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655373.9722792763, 655373.972279277, 178905.9571989406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6957600.0000, 
sim time next is 6958200.0000, 
raw observation next is [31.83333333333333, 52.33333333333334, 1.0, 2.0, 0.471694100601146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659106.9488606076, 659106.9488606076, 179279.9763761303], 
processed observation next is [0.0, 0.5217391304347826, 0.7077409162717218, 0.5233333333333334, 1.0, 1.0, 0.3634868681941518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.183085263572391, 0.183085263572391, 0.2675820542927318], 
reward next is 0.7324, 
noisyNet noise sample is [array([1.1404412], dtype=float32), 1.4457204]. 
=============================================
[2019-03-27 03:31:50,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:31:50,374] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8015
[2019-03-27 03:31:50,379] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 39.33333333333333, 1.0, 2.0, 0.2861820342438035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461700.470572328, 461700.4705723287, 164388.0575221332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6871200.0000, 
sim time next is 6871800.0000, 
raw observation next is [29.31666666666667, 38.16666666666667, 1.0, 2.0, 0.2806612041765062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454205.2490424214, 454205.2490424214, 163873.6512266393], 
processed observation next is [0.0, 0.5217391304347826, 0.5884676145339655, 0.3816666666666667, 1.0, 1.0, 0.13332675201988697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12616812473400593, 0.12616812473400593, 0.24458753914423778], 
reward next is 0.7554, 
noisyNet noise sample is [array([-1.5701804], dtype=float32), -0.74734646]. 
=============================================
[2019-03-27 03:31:56,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8005360e-26 1.0000000e+00 4.6553568e-35 4.1599331e-30 1.1731159e-24], sum to 1.0000
[2019-03-27 03:31:56,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2522
[2019-03-27 03:31:56,478] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1912830.466628258 W.
[2019-03-27 03:31:56,482] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.46666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.55787012185742, 6.9112, 168.9097407400411, 1912830.466628258, 1454069.159593734, 311352.5832319461], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7201200.0000, 
sim time next is 7201800.0000, 
raw observation next is [28.6, 84.0, 1.0, 2.0, 0.6898424509647486, 1.0, 1.0, 0.6898424509647486, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1928995.128486727, 1928995.128486727, 369811.0022974991], 
processed observation next is [1.0, 0.34782608695652173, 0.5545023696682465, 0.84, 1.0, 1.0, 0.6263162059816249, 1.0, 0.5, 0.6263162059816249, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.535831980135202, 0.535831980135202, 0.5519567198470136], 
reward next is 0.4480, 
noisyNet noise sample is [array([0.2703791], dtype=float32), -0.323219]. 
=============================================
[2019-03-27 03:32:00,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:32:00,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1072
[2019-03-27 03:32:00,510] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 82.66666666666667, 1.0, 2.0, 0.3506508702489952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541509.2451905177, 541509.2451905177, 169979.5624931546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6846000.0000, 
sim time next is 6846600.0000, 
raw observation next is [23.5, 82.33333333333334, 1.0, 2.0, 0.352067108603857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542888.3395401777, 542888.339540177, 170070.412581457], 
processed observation next is [0.0, 0.21739130434782608, 0.31279620853080575, 0.8233333333333335, 1.0, 1.0, 0.21935796217332165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15080231653893825, 0.15080231653893805, 0.2538364366887418], 
reward next is 0.7462, 
noisyNet noise sample is [array([-0.36445212], dtype=float32), 1.5011163]. 
=============================================
[2019-03-27 03:32:04,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:32:04,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9452
[2019-03-27 03:32:04,409] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.86666666666667, 57.33333333333333, 1.0, 2.0, 0.3606794020708926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552367.4256874664, 552367.4256874664, 170752.5849879545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6892800.0000, 
sim time next is 6893400.0000, 
raw observation next is [27.73333333333333, 58.16666666666667, 1.0, 2.0, 0.3623217060453695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554432.6237568004, 554432.6237568011, 170914.0386809656], 
processed observation next is [0.0, 0.782608695652174, 0.513428120063191, 0.5816666666666667, 1.0, 1.0, 0.23171289884984278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15400906215466678, 0.15400906215466698, 0.2550955801208442], 
reward next is 0.7449, 
noisyNet noise sample is [array([1.1818998], dtype=float32), 0.9369621]. 
=============================================
[2019-03-27 03:32:13,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:32:13,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3672
[2019-03-27 03:32:13,265] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.36666666666667, 92.0, 1.0, 2.0, 0.5243842318601147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844834.0794679747, 844834.0794679747, 199721.697157826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7375200.0000, 
sim time next is 7375800.0000, 
raw observation next is [20.43333333333334, 92.0, 1.0, 2.0, 0.5385470384313252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 866532.9639977087, 866532.9639977093, 202344.8030189242], 
processed observation next is [1.0, 0.34782608695652173, 0.1674565560821489, 0.92, 1.0, 1.0, 0.44403257642328336, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24070360111047465, 0.2407036011104748, 0.30200716868496147], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.34498736], dtype=float32), 0.47744924]. 
=============================================
[2019-03-27 03:32:19,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2344735e-21 1.0000000e+00 1.2410552e-29 7.1989372e-24 6.2137761e-19], sum to 1.0000
[2019-03-27 03:32:19,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6359
[2019-03-27 03:32:19,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1669403.811589076 W.
[2019-03-27 03:32:19,606] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.26666666666667, 83.66666666666667, 1.0, 2.0, 0.5970803093533367, 1.0, 2.0, 0.5970803093533367, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1669403.811589076, 1669403.811589076, 333165.3615146153], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7141200.0000, 
sim time next is 7141800.0000, 
raw observation next is [26.23333333333333, 83.83333333333333, 1.0, 2.0, 0.6027693604236226, 1.0, 2.0, 0.6027693604236226, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1685322.594084695, 1685322.594084695, 335266.2942122784], 
processed observation next is [1.0, 0.6521739130434783, 0.44233807266982617, 0.8383333333333333, 1.0, 1.0, 0.5214088679802682, 1.0, 1.0, 0.5214088679802682, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4681451650235264, 0.4681451650235264, 0.5003974540481767], 
reward next is 0.4996, 
noisyNet noise sample is [array([0.7265818], dtype=float32), 0.9784021]. 
=============================================
[2019-03-27 03:32:28,579] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:32:28,588] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6525
[2019-03-27 03:32:28,597] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.66666666666666, 1.0, 2.0, 0.3266101256570036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524509.7457347023, 524509.7457347017, 168951.9663311063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7371600.0000, 
sim time next is 7372200.0000, 
raw observation next is [20.3, 91.83333333333333, 1.0, 2.0, 0.3227898399380061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520166.1695543221, 520166.1695543215, 168607.2709799029], 
processed observation next is [1.0, 0.30434782608695654, 0.16113744075829392, 0.9183333333333333, 1.0, 1.0, 0.1840841445036218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14449060265397837, 0.1444906026539782, 0.25165264325358644], 
reward next is 0.7483, 
noisyNet noise sample is [array([-0.11366905], dtype=float32), 0.22041082]. 
=============================================
[2019-03-27 03:32:31,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:32:31,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5014
[2019-03-27 03:32:31,734] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 95.0, 1.0, 2.0, 0.3345162116253118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520405.6269740293, 520405.6269740287, 168389.496491003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7456800.0000, 
sim time next is 7457400.0000, 
raw observation next is [21.55, 95.0, 1.0, 2.0, 0.3350454139268377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520993.1127877971, 520993.1127877978, 168429.1792240475], 
processed observation next is [0.0, 0.30434782608695654, 0.22037914691943136, 0.95, 1.0, 1.0, 0.19884989629739477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14472030910772143, 0.14472030910772163, 0.25138683466275746], 
reward next is 0.7486, 
noisyNet noise sample is [array([-0.52723527], dtype=float32), 0.03293789]. 
=============================================
[2019-03-27 03:32:37,392] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:32:37,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:37,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-27 03:32:40,427] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 03:32:40,428] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:32:40,430] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:32:40,430] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:40,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:40,431] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:32:40,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:32:40,434] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:40,433] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:32:40,435] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:40,438] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:32:40,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-27 03:32:40,483] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-27 03:32:40,505] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-27 03:32:40,505] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-27 03:32:40,535] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-27 03:33:01,442] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:33:01,443] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 47.0, 1.0, 2.0, 0.4259106626163903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699030.1554618551, 699030.1554618557, 183381.0216880592]
[2019-03-27 03:33:01,444] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:33:01,450] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18254148022785655
[2019-03-27 03:33:03,150] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:33:03,152] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.55, 80.0, 1.0, 2.0, 0.2818871900382806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456861.613870458, 456861.613870458, 164044.280688365]
[2019-03-27 03:33:03,154] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:33:03,157] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.728261061302087
[2019-03-27 03:33:50,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:33:50,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.41666666666667, 64.33333333333334, 1.0, 2.0, 1.001371937265291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956500066, 1399723.427767266, 1399723.427767266, 299345.0061587949]
[2019-03-27 03:33:50,186] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:33:50,189] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.036962e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.08816806487743589
[2019-03-27 03:34:12,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:34:12,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.76666666666667, 77.33333333333333, 1.0, 2.0, 0.5475340926732652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765117.746612348, 765117.7466123486, 191377.9076500839]
[2019-03-27 03:34:12,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:34:12,074] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5971288845806854
[2019-03-27 03:34:12,145] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:34:12,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.85, 82.5, 1.0, 2.0, 0.5761609235395938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805135.706836975, 805135.706836975, 196388.1423091108]
[2019-03-27 03:34:12,147] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:34:12,151] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7621892748314331
[2019-03-27 03:34:18,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:34:18,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.9, 64.0, 1.0, 2.0, 0.830091516311883, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.966681961316523, 6.9112, 168.9125816266809, 2057188.488942866, 2017827.763400187, 416691.3123119199]
[2019-03-27 03:34:18,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:34:18,594] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.4735898e-25 1.0000000e+00 4.6866968e-36 4.2392347e-28 2.7823414e-22], sampled 0.011780115870350993
[2019-03-27 03:34:18,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2057188.488942866 W.
[2019-03-27 03:34:25,342] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16069123]
[2019-03-27 03:34:25,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.11204640333334, 85.95588814333334, 1.0, 2.0, 0.5827406740150376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814333.8711175052, 814333.8711175052, 197570.248174245]
[2019-03-27 03:34:25,347] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:34:25,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.07275112987511778
[2019-03-27 03:34:35,395] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:34:35,503] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:34:35,785] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:34:35,805] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:34:35,816] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:34:36,832] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1900000, evaluation results [1900000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:34:37,982] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2330580e-29 1.0000000e+00 0.0000000e+00 2.6749494e-34 6.1384210e-30], sum to 1.0000
[2019-03-27 03:34:37,988] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5945
[2019-03-27 03:34:37,993] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 93.33333333333334, 1.0, 2.0, 0.4637460189102502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654103.4482268982, 654103.4482268982, 178898.9780278839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7604400.0000, 
sim time next is 7605000.0000, 
raw observation next is [24.45, 93.5, 1.0, 2.0, 0.4621301086147338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652642.8195107767, 652642.8195107767, 178766.3776916693], 
processed observation next is [1.0, 0.0, 0.3578199052132702, 0.935, 1.0, 1.0, 0.3519639862828118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18128967208632685, 0.18128967208632685, 0.2668154890920437], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.30830485], dtype=float32), -0.7491626]. 
=============================================
[2019-03-27 03:34:38,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[61.964405]
 [64.57774 ]
 [68.105064]
 [74.8103  ]
 [74.80877 ]], R is [[60.22740173]
 [60.35811615]
 [60.48732376]
 [60.6149559 ]
 [60.74085236]].
[2019-03-27 03:34:38,654] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:38,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:38,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-27 03:34:39,312] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0460013e-10 5.7988024e-01 4.2474960e-17 9.6679507e-08 4.2011973e-01], sum to 1.0000
[2019-03-27 03:34:39,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4655
[2019-03-27 03:34:39,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2266935.612081836 W.
[2019-03-27 03:34:39,330] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.979947773939988, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00516533606139, 6.9112, 168.9123979480386, 2266935.612081836, 2200273.589081721, 457213.5916218003], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7819800.0000, 
sim time next is 7820400.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.4165935325844078, 1.0, 1.0, 0.4165935325844078, 1.0, 2.0, 0.7234851965126481, 6.911199999999999, 6.9112, 170.5573041426782, 1747222.406457732, 1747222.406457733, 362264.4017853387], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.2971006416679612, 1.0, 0.5, 0.2971006416679612, 1.0, 1.0, 0.6627868250154245, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.48533955734937, 0.48533955734937023, 0.5406931369930429], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2619257], dtype=float32), -0.8877895]. 
=============================================
[2019-03-27 03:34:43,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.37929355e-11 9.27063465e-01 4.15152396e-18 8.19723976e-12
 7.29365572e-02], sum to 1.0000
[2019-03-27 03:34:43,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9732
[2019-03-27 03:34:43,525] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.9, 70.0, 1.0, 2.0, 0.4855830641011024, 1.0, 2.0, 0.4855830641011024, 1.0, 1.0, 0.8363029641507845, 6.911200000000001, 6.9112, 170.5573041426782, 2036844.359467261, 2036844.359467261, 404094.6085950785], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7837200.0000, 
sim time next is 7837800.0000, 
raw observation next is [29.75, 71.0, 1.0, 2.0, 0.2274169194951898, 1.0, 2.0, 0.2274169194951898, 1.0, 2.0, 0.3905499350330789, 6.911199999999999, 6.9112, 170.5573041426782, 953449.6124774972, 953449.6124774978, 277662.326258922], 
processed observation next is [1.0, 0.7391304347826086, 0.6090047393364929, 0.71, 1.0, 1.0, 0.0691770114399877, 1.0, 1.0, 0.0691770114399877, 1.0, 1.0, 0.25676821345497425, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.26484711457708254, 0.2648471145770827, 0.41442138247600296], 
reward next is 0.5856, 
noisyNet noise sample is [array([-0.16106048], dtype=float32), 0.003234535]. 
=============================================
[2019-03-27 03:34:44,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:44,015] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:44,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-27 03:34:45,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:45,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:46,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-27 03:34:46,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4707915e-12 1.0122808e-01 9.0678382e-20 6.1413478e-11 8.9877194e-01], sum to 1.0000
[2019-03-27 03:34:46,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8188
[2019-03-27 03:34:46,413] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.76666666666667, 70.33333333333333, 1.0, 2.0, 0.9544355714635535, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.001195801748548, 6.9112, 168.9124212672021, 2231227.971403984, 2167382.054539616, 449779.2477312339], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7818600.0000, 
sim time next is 7819200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5728056079926948, 1.0, 1.0, 0.5728056079926948, 1.0, 2.0, 0.994773911374092, 6.9112, 6.9112, 170.5573041426782, 2403091.114874187, 2403091.114874187, 469142.9251444482], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4853079614369816, 1.0, 0.5, 0.4853079614369816, 1.0, 1.0, 0.993626721187917, 0.0, 0.0, 0.8375144448122397, 0.6675253096872742, 0.6675253096872742, 0.7002133211111167], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08609521], dtype=float32), 0.88840616]. 
=============================================
[2019-03-27 03:34:47,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:47,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:47,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-27 03:34:49,819] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:49,820] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:49,893] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-27 03:34:54,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:54,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:54,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-27 03:34:54,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:54,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:54,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-27 03:34:55,354] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:55,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:55,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6049390e-26 1.0000000e+00 2.3031425e-37 2.0588516e-31 2.4233642e-26], sum to 1.0000
[2019-03-27 03:34:55,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2019
[2019-03-27 03:34:55,392] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 88.0, 1.0, 2.0, 0.6023766940279007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 841784.5251522035, 841784.5251522035, 201182.9611700245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7882200.0000, 
sim time next is 7882800.0000, 
raw observation next is [26.5, 87.66666666666667, 1.0, 2.0, 0.6062125849977412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847147.0855077576, 847147.0855077576, 201902.2198677936], 
processed observation next is [1.0, 0.21739130434782608, 0.4549763033175356, 0.8766666666666667, 1.0, 1.0, 0.5255573313225798, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.235318634863266, 0.235318634863266, 0.3013465968176024], 
reward next is 0.6987, 
noisyNet noise sample is [array([-1.182428], dtype=float32), 0.8970792]. 
=============================================
[2019-03-27 03:34:55,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-27 03:34:56,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:56,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:56,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-27 03:34:58,358] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:58,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:58,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:34:58,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8099
[2019-03-27 03:34:58,438] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.35, 95.33333333333333, 1.0, 2.0, 0.2623846589048661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427461.9519288958, 427461.9519288958, 162091.0591451025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 277800.0000, 
sim time next is 278400.0000, 
raw observation next is [19.5, 94.66666666666666, 1.0, 2.0, 0.2639067504600888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 429446.1926807756, 429446.192680775, 162227.7416935281], 
processed observation next is [0.0, 0.21739130434782608, 0.12322274881516594, 0.9466666666666665, 1.0, 1.0, 0.11314066320492629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11929060907799321, 0.11929060907799306, 0.24213095775153448], 
reward next is 0.7579, 
noisyNet noise sample is [array([0.85792494], dtype=float32), 0.15436058]. 
=============================================
[2019-03-27 03:34:58,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-27 03:34:58,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:58,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:58,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-27 03:34:59,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:59,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:59,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-27 03:34:59,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3546380e-29 1.0000000e+00 0.0000000e+00 4.3484066e-32 4.3167698e-26], sum to 1.0000
[2019-03-27 03:34:59,550] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4739
[2019-03-27 03:34:59,554] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.96666666666667, 85.66666666666667, 1.0, 2.0, 0.3557073192929176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549347.9638717785, 549347.9638717785, 170629.8965344804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 76800.0000, 
sim time next is 77400.0000, 
raw observation next is [22.9, 86.0, 1.0, 2.0, 0.3538089022153792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546752.3518603382, 546752.3518603382, 170422.9375261928], 
processed observation next is [1.0, 0.9130434782608695, 0.2843601895734597, 0.86, 1.0, 1.0, 0.22145650869322794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1518756532945384, 0.1518756532945384, 0.25436259332267586], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.5324263], dtype=float32), 0.544189]. 
=============================================
[2019-03-27 03:34:59,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:59,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:59,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-27 03:34:59,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:59,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:59,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:34:59,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:34:59,996] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-27 03:35:00,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-27 03:35:05,022] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2964584e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0866631e-36], sum to 1.0000
[2019-03-27 03:35:05,030] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8164
[2019-03-27 03:35:05,035] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 87.0, 1.0, 2.0, 0.2605029842532295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424335.7549007521, 424335.7549007521, 161896.8102515732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [20.3, 87.0, 1.0, 2.0, 0.2602382537486013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424014.5985252329, 424014.5985252323, 161874.0045438348], 
processed observation next is [1.0, 0.08695652173913043, 0.16113744075829392, 0.87, 1.0, 1.0, 0.10872078764891725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11778183292367582, 0.11778183292367564, 0.24160299185646986], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.14834072], dtype=float32), -0.52423406]. 
=============================================
[2019-03-27 03:35:12,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:35:12,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5486
[2019-03-27 03:35:12,517] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 96.0, 1.0, 2.0, 0.2844410003306096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458693.0151236086, 458693.0151236086, 164184.2491094271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [19.81666666666667, 96.0, 1.0, 2.0, 0.2839575869409868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458056.6161098718, 458056.6161098718, 164140.8983375772], 
processed observation next is [0.0, 0.17391304347826086, 0.13823064770932092, 0.96, 1.0, 1.0, 0.13729829751926118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12723794891940882, 0.12723794891940882, 0.2449864154292197], 
reward next is 0.7550, 
noisyNet noise sample is [array([-1.8467611], dtype=float32), 1.2333372]. 
=============================================
[2019-03-27 03:35:13,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:35:13,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9673
[2019-03-27 03:35:13,456] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.5, 1.0, 2.0, 0.233696715430483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 385832.222651742, 385832.2226517427, 159297.2695753658], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 441000.0000, 
sim time next is 441600.0000, 
raw observation next is [19.6, 84.33333333333334, 1.0, 2.0, 0.2337647099792791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 386033.3361524688, 386033.3361524688, 159298.7815445992], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.8433333333333334, 1.0, 1.0, 0.07682495178226395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10723148226457467, 0.10723148226457467, 0.23775937543970027], 
reward next is 0.7622, 
noisyNet noise sample is [array([0.24897982], dtype=float32), 1.3536097]. 
=============================================
[2019-03-27 03:35:14,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:35:14,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5787
[2019-03-27 03:35:14,280] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.43333333333333, 91.66666666666666, 1.0, 2.0, 0.2594312418392346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424664.7965696771, 424664.7965696771, 161844.985704398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 783600.0000, 
sim time next is 784200.0000, 
raw observation next is [19.41666666666667, 91.83333333333333, 1.0, 2.0, 0.2596210836681977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424958.8554000512, 424958.8554000505, 161864.003403954], 
processed observation next is [0.0, 0.043478260869565216, 0.11927330173775699, 0.9183333333333333, 1.0, 1.0, 0.1079772092387924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11804412650001422, 0.11804412650001403, 0.2415880647820209], 
reward next is 0.7584, 
noisyNet noise sample is [array([-0.6311932], dtype=float32), 0.69447654]. 
=============================================
[2019-03-27 03:35:26,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5068118e-30 1.0000000e+00 0.0000000e+00 7.2539469e-35 4.9191742e-28], sum to 1.0000
[2019-03-27 03:35:26,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4247
[2019-03-27 03:35:26,954] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 86.0, 1.0, 2.0, 0.2444180700154164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 402250.722132477, 402250.7221324776, 160356.9301463709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 431400.0000, 
sim time next is 432000.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.2442807504006924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402025.3453116264, 402025.3453116264, 160343.6647814425], 
processed observation next is [1.0, 0.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.08949488000083419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11167370703100733, 0.11167370703100733, 0.2393189026588694], 
reward next is 0.7607, 
noisyNet noise sample is [array([0.7507103], dtype=float32), -1.0149618]. 
=============================================
[2019-03-27 03:35:26,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.60323 ]
 [77.583664]
 [77.56612 ]
 [77.54337 ]
 [77.50845 ]], R is [[74.53700256]
 [74.55229187]
 [74.56739044]
 [74.58227539]
 [74.59695435]].
[2019-03-27 03:35:29,264] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 03:35:29,269] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:35:29,270] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:35:29,271] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:35:29,271] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:35:29,273] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:35:29,273] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:35:29,276] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:35:29,277] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:35:29,278] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:35:29,280] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:35:29,299] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-27 03:35:29,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-27 03:35:29,325] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-27 03:35:29,344] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-27 03:35:29,344] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-27 03:35:58,523] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:35:58,524] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.91666666666666, 94.00000000000001, 1.0, 2.0, 0.3984229828551547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596306.8951401236, 596306.8951401236, 174249.9760332016]
[2019-03-27 03:35:58,527] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:35:58,531] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4649102e-31 1.0000000e+00 0.0000000e+00 4.2624240e-35 5.3979766e-29], sampled 0.3951890388335749
[2019-03-27 03:36:02,417] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:02,418] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.7, 75.5, 1.0, 2.0, 0.9109512674912407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997317490489197, 6.9112, 168.9123747944431, 2170365.357789699, 2109270.856504254, 437439.9526290587]
[2019-03-27 03:36:02,420] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:36:02,422] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3184488e-22 1.0000000e+00 1.3574834e-32 4.6433246e-23 6.4731293e-13], sampled 0.12540251347261022
[2019-03-27 03:36:02,424] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2170365.357789699 W.
[2019-03-27 03:36:04,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:04,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.5, 85.0, 1.0, 2.0, 0.3796182574646088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578706.1278035064, 578706.1278035071, 172957.5705968622]
[2019-03-27 03:36:04,302] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:36:04,306] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0294836e-32 1.0000000e+00 0.0000000e+00 5.0816591e-36 1.3317423e-29], sampled 0.5420325406122453
[2019-03-27 03:36:07,876] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:07,878] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.75494529, 71.60312637, 1.0, 2.0, 0.9358139429955404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1308029.63411093, 1308029.634110929, 279979.1750211248]
[2019-03-27 03:36:07,878] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:36:07,880] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.043008e-28 1.000000e+00 0.000000e+00 1.671767e-31 7.057328e-24], sampled 0.238525796761888
[2019-03-27 03:36:16,170] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:16,172] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.84108101333334, 97.63293916666667, 1.0, 2.0, 0.3351844222747196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524679.1143761307, 524679.1143761307, 168809.8574073112]
[2019-03-27 03:36:16,173] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:36:16,176] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.954708083100666
[2019-03-27 03:36:31,374] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:31,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 89.0, 1.0, 2.0, 0.7757193108120517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084143.855049379, 1084143.85504938, 238040.8783472585]
[2019-03-27 03:36:31,377] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:36:31,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1540594e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.7463061e-35], sampled 0.7842650812659898
[2019-03-27 03:36:48,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:48,238] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.605225175, 96.54144203499999, 1.0, 2.0, 0.888213768875966, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987038359262, 6.9112, 168.912315957644, 2138539.172116153, 2071294.239793631, 430803.4612004721]
[2019-03-27 03:36:48,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:36:48,244] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6449524e-21 1.0000000e+00 1.3382142e-30 5.9860103e-22 7.3508604e-12], sampled 0.9063354930049933
[2019-03-27 03:36:48,245] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2138539.172116153 W.
[2019-03-27 03:36:49,458] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16851434]
[2019-03-27 03:36:49,459] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.8, 51.66666666666667, 1.0, 2.0, 1.002777864187599, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993509437348, 6.9112, 168.9123159263004, 2298890.162599381, 2231640.639501236, 464157.4993758462]
[2019-03-27 03:36:49,460] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:36:49,463] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.7162130e-17 9.9941742e-01 3.1105230e-27 3.4261008e-16 5.8256509e-04], sampled 0.4717195541515038
[2019-03-27 03:36:49,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2298890.162599381 W.
[2019-03-27 03:37:24,402] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0944 2842039777.7241 1148.0000
[2019-03-27 03:37:24,566] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.8706 2779070365.8439 930.0000
[2019-03-27 03:37:24,700] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.5350 2927153881.7356 1332.0000
[2019-03-27 03:37:24,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.4815 3163604439.4181 1858.0000
[2019-03-27 03:37:24,794] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8027.2987 3006365629.4520 1694.0000
[2019-03-27 03:37:25,812] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1925000, evaluation results [1925000.0, 7893.481491972477, 3163604439.4181013, 1858.0, 8260.535013346633, 2927153881.7356486, 1332.0, 8662.870637524822, 2779070365.8438535, 930.0, 8027.298739571097, 3006365629.4520106, 1694.0, 8498.094372964055, 2842039777.7241063, 1148.0]
[2019-03-27 03:37:43,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:37:43,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6313
[2019-03-27 03:37:43,684] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3304369664384131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522451.8271187349, 522451.8271187343, 168738.7956749833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1066200.0000, 
sim time next is 1066800.0000, 
raw observation next is [21.1, 93.0, 1.0, 2.0, 0.4328314631610836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 684621.5134002778, 684621.5134002785, 182991.014045255], 
processed observation next is [1.0, 0.34782608695652173, 0.1990521327014219, 0.93, 1.0, 1.0, 0.31666441344708873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1901726426111883, 0.1901726426111885, 0.2731209164854552], 
reward next is 0.7269, 
noisyNet noise sample is [array([-0.74083656], dtype=float32), -1.7610815]. 
=============================================
[2019-03-27 03:37:48,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5817787e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:37:48,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1202
[2019-03-27 03:37:48,176] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 89.0, 1.0, 2.0, 0.3453616369211296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535339.9796372101, 535339.9796372101, 169531.1868940349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1213200.0000, 
sim time next is 1213800.0000, 
raw observation next is [22.35, 89.16666666666667, 1.0, 2.0, 0.3445147210194953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534367.0638527004, 534367.0638527004, 169461.5855176417], 
processed observation next is [1.0, 0.043478260869565216, 0.25829383886255936, 0.8916666666666667, 1.0, 1.0, 0.21025870002348834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.148435295514639, 0.148435295514639, 0.25292773957856973], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.36915877], dtype=float32), -0.49138543]. 
=============================================
[2019-03-27 03:37:54,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1725393e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3351659e-38], sum to 1.0000
[2019-03-27 03:37:54,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7642
[2019-03-27 03:37:54,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.28333333333333, 77.5, 1.0, 2.0, 0.3197143172210333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503939.9799417161, 503939.9799417161, 167288.0769807395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1109400.0000, 
sim time next is 1110000.0000, 
raw observation next is [23.16666666666667, 78.0, 1.0, 2.0, 0.3181376023541407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501965.7990923684, 501965.7990923684, 167150.1890269072], 
processed observation next is [1.0, 0.8695652173913043, 0.2969984202211693, 0.78, 1.0, 1.0, 0.17847903898089237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13943494419232455, 0.13943494419232455, 0.24947789407001075], 
reward next is 0.7505, 
noisyNet noise sample is [array([1.4575846], dtype=float32), 1.2870483]. 
=============================================
[2019-03-27 03:37:54,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.28237 ]
 [79.28358 ]
 [79.240746]
 [79.24723 ]
 [79.25178 ]], R is [[79.25637817]
 [79.21412659]
 [79.17205811]
 [79.13009644]
 [79.08818817]].
[2019-03-27 03:38:07,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:38:07,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0689
[2019-03-27 03:38:07,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333334, 94.83333333333333, 1.0, 2.0, 0.4657838835399976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687380.1757642854, 687380.175764286, 182996.0482022739], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1324200.0000, 
sim time next is 1324800.0000, 
raw observation next is [23.1, 95.0, 1.0, 2.0, 0.440180960308833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651081.9185339112, 651081.9185339105, 179298.5097571632], 
processed observation next is [1.0, 0.34782608695652173, 0.2938388625592418, 0.95, 1.0, 1.0, 0.3255192292877506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.180856088481642, 0.1808560884816418, 0.26760971605546746], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.21268898], dtype=float32), 0.8045791]. 
=============================================
[2019-03-27 03:38:08,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:38:08,274] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8946
[2019-03-27 03:38:08,279] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333334, 96.0, 1.0, 2.0, 0.3475423436181706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537115.7424958579, 537115.7424958579, 169630.9236938214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1471800.0000, 
sim time next is 1472400.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.3458799522248043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535052.604252729, 535052.604252729, 169477.3413485671], 
processed observation next is [0.0, 0.043478260869565216, 0.22274881516587688, 0.96, 1.0, 1.0, 0.21190355689735454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14862572340353583, 0.14862572340353583, 0.25295125574413], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.5914539], dtype=float32), 0.26462212]. 
=============================================
[2019-03-27 03:38:10,374] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0888453e-28 1.0000000e+00 3.0606441e-38 4.3276691e-33 1.3024580e-27], sum to 1.0000
[2019-03-27 03:38:10,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1627
[2019-03-27 03:38:10,393] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.08333333333334, 84.33333333333333, 1.0, 2.0, 0.9243969496240806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1292061.873570758, 1292061.873570759, 276735.2945956454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [27.2, 84.0, 1.0, 2.0, 0.9031370624274827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1262328.52167939, 1262328.52167939, 270800.7116494708], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.84, 1.0, 1.0, 0.8832976655752803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35064681157760835, 0.35064681157760835, 0.40418016664100115], 
reward next is 0.5958, 
noisyNet noise sample is [array([-0.01631344], dtype=float32), -1.8272458]. 
=============================================
[2019-03-27 03:38:11,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.172683e-30 1.000000e+00 0.000000e+00 1.541871e-34 2.865335e-29], sum to 1.0000
[2019-03-27 03:38:11,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3563
[2019-03-27 03:38:11,971] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 90.5, 1.0, 2.0, 0.7373119212329328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1096130.606329716, 1096130.606329715, 237805.394125598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1611000.0000, 
sim time next is 1611600.0000, 
raw observation next is [23.5, 91.0, 1.0, 2.0, 0.76017432775618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1129496.288003538, 1129496.288003538, 243354.765508786], 
processed observation next is [1.0, 0.6521739130434783, 0.31279620853080575, 0.91, 1.0, 1.0, 0.7110534069351566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3137489688898717, 0.3137489688898717, 0.3632160679235612], 
reward next is 0.6368, 
noisyNet noise sample is [array([-0.31823483], dtype=float32), -0.09632202]. 
=============================================
[2019-03-27 03:38:16,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3267589e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8029805e-37], sum to 1.0000
[2019-03-27 03:38:16,455] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9086
[2019-03-27 03:38:16,459] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.7423227785716151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102562.798345055, 1102562.798345055, 238896.4301853346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.8019855926611088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1191201.307154278, 1191201.307154278, 254015.6109170088], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.7614284248929022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33088925198729946, 0.33088925198729946, 0.37912777748807286], 
reward next is 0.6209, 
noisyNet noise sample is [array([0.3500111], dtype=float32), 0.7702966]. 
=============================================
[2019-03-27 03:38:21,376] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 03:38:21,378] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:38:21,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:38:21,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:38:21,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:38:21,383] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:38:21,384] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:38:21,384] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:38:21,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:38:21,386] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:38:21,389] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:38:21,413] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-27 03:38:21,437] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-27 03:38:21,458] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-27 03:38:21,458] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-27 03:38:21,498] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-27 03:38:26,705] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:38:26,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.57508689833334, 86.65978255, 1.0, 2.0, 0.2880653954184605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461088.2162001599, 461088.2162001599, 164329.4509192845]
[2019-03-27 03:38:26,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:38:26,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29840803131297355
[2019-03-27 03:38:36,330] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:38:36,332] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.93333333333333, 94.66666666666666, 1.0, 2.0, 0.5663689881175034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 871975.5004917851, 871975.5004917858, 204341.0745419479]
[2019-03-27 03:38:36,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:38:36,337] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.969952e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.28488637325960875
[2019-03-27 03:39:09,478] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:39:09,481] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.73642307, 91.475040455, 1.0, 2.0, 0.482505508032973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678245.5708681426, 678245.5708681426, 181418.6469255534]
[2019-03-27 03:39:09,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:39:09,486] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7175181437190585
[2019-03-27 03:39:33,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:39:33,017] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.90179558666667, 87.09523881999999, 1.0, 2.0, 0.6602710850948055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 922723.5337971913, 922723.533797192, 212505.3795284828]
[2019-03-27 03:39:33,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:39:33,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24329521669904808
[2019-03-27 03:39:36,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:39:36,530] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.762998875, 74.81597019, 1.0, 2.0, 0.5286394264212044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738705.3797591291, 738705.3797591286, 188204.4161544021]
[2019-03-27 03:39:36,531] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:39:36,534] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.3721165e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1169605e-37], sampled 0.7185530308973772
[2019-03-27 03:39:40,112] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:39:40,113] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.86666666666667, 70.33333333333334, 1.0, 2.0, 0.6017915714257089, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565092009, 840966.5284560139, 840966.5284560146, 201077.6521664786]
[2019-03-27 03:39:40,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:39:40,116] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11709097861250295
[2019-03-27 03:39:52,033] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:39:52,034] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.45, 67.0, 1.0, 2.0, 0.5371098104274004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750545.8299805054, 750545.8299805054, 189614.275385958]
[2019-03-27 03:39:52,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:39:52,039] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.880389e-35 1.000000e+00 0.000000e+00 0.000000e+00 6.443650e-36], sampled 0.11658976598541781
[2019-03-27 03:40:09,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:40:09,282] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.73333333333333, 86.0, 1.0, 2.0, 0.5193039901985296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725655.8565276284, 725655.8565276284, 186675.3450586785]
[2019-03-27 03:40:09,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:40:09,288] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22532029259860853
[2019-03-27 03:40:14,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17010823]
[2019-03-27 03:40:14,671] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.49638819666667, 67.28402142666667, 1.0, 2.0, 1.011615284907947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1420161.336340105, 1420161.336340105, 303449.8650504508]
[2019-03-27 03:40:14,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:40:14,674] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7541101e-31 1.0000000e+00 0.0000000e+00 1.3718440e-35 6.0283535e-31], sampled 0.7353636226006485
[2019-03-27 03:40:16,117] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 03:40:16,210] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:40:16,226] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 03:40:16,283] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 03:40:16,334] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:40:17,348] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1950000, evaluation results [1950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 03:40:18,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:18,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3180
[2019-03-27 03:40:18,067] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.66666666666667, 1.0, 2.0, 0.3063452297009812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483532.9459328792, 483532.9459328799, 165790.6940724863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1570800.0000, 
sim time next is 1571400.0000, 
raw observation next is [21.6, 89.5, 1.0, 2.0, 0.3064054166652293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483867.5143219733, 483867.5143219727, 165820.8009257269], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.895, 1.0, 1.0, 0.16434387550027627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13440764286721482, 0.13440764286721463, 0.2474937327249655], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.26435363], dtype=float32), 1.8551618]. 
=============================================
[2019-03-27 03:40:20,114] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5929005e-31 1.0000000e+00 0.0000000e+00 1.4784459e-37 6.9126191e-32], sum to 1.0000
[2019-03-27 03:40:20,125] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1023
[2019-03-27 03:40:20,132] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333334, 87.66666666666666, 1.0, 2.0, 0.7740902642945382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1154497.710712508, 1154497.710712507, 247411.900702819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1608000.0000, 
sim time next is 1608600.0000, 
raw observation next is [23.76666666666667, 88.33333333333334, 1.0, 2.0, 0.7809257562176644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163778.242535404, 1163778.242535404, 249046.0578140412], 
processed observation next is [1.0, 0.6086956521739131, 0.32543443917851517, 0.8833333333333334, 1.0, 1.0, 0.7360551279730896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3232717340376122, 0.3232717340376122, 0.3717105340508078], 
reward next is 0.6283, 
noisyNet noise sample is [array([0.20144863], dtype=float32), 0.13735153]. 
=============================================
[2019-03-27 03:40:23,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6659510e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.7166547e-38], sum to 1.0000
[2019-03-27 03:40:23,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1526
[2019-03-27 03:40:23,972] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666666, 71.33333333333333, 1.0, 2.0, 0.3508383085864997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540471.1066668897, 540471.1066668897, 169855.8271450381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1536000.0000, 
sim time next is 1536600.0000, 
raw observation next is [24.98333333333333, 72.66666666666667, 1.0, 2.0, 0.3519869660847456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541815.6587861795, 541815.6587861795, 169954.1801147685], 
processed observation next is [0.0, 0.782608695652174, 0.3830963665086887, 0.7266666666666667, 1.0, 1.0, 0.21926140492138024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15050434966282764, 0.15050434966282764, 0.25366295539517686], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.5546863], dtype=float32), -0.13106552]. 
=============================================
[2019-03-27 03:40:46,715] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1834363e-23 1.0000000e+00 3.9322061e-32 3.0488701e-26 9.3662777e-19], sum to 1.0000
[2019-03-27 03:40:46,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5913
[2019-03-27 03:40:46,728] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1869494.136819113 W.
[2019-03-27 03:40:46,736] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.4, 88.0, 1.0, 2.0, 0.668582402121137, 1.0, 2.0, 0.668582402121137, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1869494.136819113, 1869494.136819113, 360958.2726164944], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2455200.0000, 
sim time next is 2455800.0000, 
raw observation next is [26.28333333333333, 88.16666666666667, 1.0, 2.0, 0.4402347740256903, 1.0, 2.0, 0.4402347740256903, 1.0, 1.0, 0.7498232425722106, 6.9112, 6.9112, 170.5573041426782, 1846460.857422298, 1846460.857422298, 373950.1335280221], 
processed observation next is [1.0, 0.43478260869565216, 0.4447077409162717, 0.8816666666666667, 1.0, 1.0, 0.32558406509119314, 1.0, 1.0, 0.32558406509119314, 1.0, 0.5, 0.6949063933807444, 0.0, 0.0, 0.8375144448122397, 0.5129057937284162, 0.5129057937284162, 0.5581345276537644], 
reward next is 0.4419, 
noisyNet noise sample is [array([-1.0031787], dtype=float32), -0.575636]. 
=============================================
[2019-03-27 03:40:48,861] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:40:48,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9331
[2019-03-27 03:40:48,880] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 94.16666666666667, 1.0, 2.0, 0.4744262029531184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662925.7575246658, 662925.7575246652, 179686.0093666017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [24.6, 94.33333333333334, 1.0, 2.0, 0.4738998502137316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662190.044487812, 662190.0444878126, 179607.5331538906], 
processed observation next is [0.0, 0.08695652173913043, 0.36492890995260674, 0.9433333333333335, 1.0, 1.0, 0.36614439784786934, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1839416790243922, 0.18394167902439237, 0.26807094500580686], 
reward next is 0.7319, 
noisyNet noise sample is [array([1.1930066], dtype=float32), -0.34919503]. 
=============================================
[2019-03-27 03:40:52,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8353430e-20 1.0000000e+00 2.5249119e-27 6.0231466e-20 2.4766154e-11], sum to 1.0000
[2019-03-27 03:40:52,683] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8869
[2019-03-27 03:40:52,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1708378.969747058 W.
[2019-03-27 03:40:52,697] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.73333333333333, 89.0, 1.0, 2.0, 0.6110090931661648, 1.0, 2.0, 0.6110090931661648, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1708378.969747058, 1708378.969747058, 338344.3588372448], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [25.76666666666667, 89.0, 1.0, 2.0, 0.6109507873436969, 1.0, 2.0, 0.6109507873436969, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1708215.816966768, 1708215.816966768, 338322.6444792993], 
processed observation next is [1.0, 0.4782608695652174, 0.42022116903633505, 0.89, 1.0, 1.0, 0.5312660088478276, 1.0, 1.0, 0.5312660088478276, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.47450439360187996, 0.47450439360187996, 0.5049591708646258], 
reward next is 0.4950, 
noisyNet noise sample is [array([0.08901507], dtype=float32), 0.8089719]. 
=============================================
[2019-03-27 03:40:52,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.330956]
 [57.481483]
 [57.3647  ]
 [56.752052]
 [55.985664]], R is [[58.52383804]
 [57.93859863]
 [57.35921478]
 [56.78562164]
 [56.21776581]].
[2019-03-27 03:40:53,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2823814e-31 1.0000000e+00 0.0000000e+00 5.3653190e-35 2.8401319e-29], sum to 1.0000
[2019-03-27 03:40:53,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2327
[2019-03-27 03:40:53,571] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 93.0, 1.0, 2.0, 0.5209833541982788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728003.3407383447, 728003.3407383454, 186948.9830338373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2157600.0000, 
sim time next is 2158200.0000, 
raw observation next is [25.95, 93.0, 1.0, 2.0, 0.5198918164369748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726477.5436797593, 726477.5436797593, 186771.2839850073], 
processed observation next is [0.0, 1.0, 0.42890995260663506, 0.93, 1.0, 1.0, 0.4215564053457527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.201799317688822, 0.201799317688822, 0.27876311042538404], 
reward next is 0.7212, 
noisyNet noise sample is [array([1.0136179], dtype=float32), 0.14647637]. 
=============================================
[2019-03-27 03:41:00,220] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1872640e-27 1.0000000e+00 5.7710036e-37 1.0987844e-29 2.7974830e-24], sum to 1.0000
[2019-03-27 03:41:00,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1987
[2019-03-27 03:41:00,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 95.0, 1.0, 2.0, 0.7863950080684936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1099071.911018518, 1099071.911018518, 240595.8262012549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2170800.0000, 
sim time next is 2171400.0000, 
raw observation next is [24.96666666666667, 95.16666666666667, 1.0, 2.0, 0.7088116804154643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 990590.2621577844, 990590.2621577844, 222755.8968294621], 
processed observation next is [1.0, 0.13043478260869565, 0.3823064770932071, 0.9516666666666667, 1.0, 1.0, 0.6491706992957401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2751639617104957, 0.2751639617104957, 0.3324714878051673], 
reward next is 0.6675, 
noisyNet noise sample is [array([0.68504745], dtype=float32), -0.72924024]. 
=============================================
[2019-03-27 03:41:00,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6282040e-23 1.0000000e+00 9.7517671e-34 2.2584718e-25 7.8164233e-18], sum to 1.0000
[2019-03-27 03:41:00,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8778
[2019-03-27 03:41:00,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 80.0, 1.0, 2.0, 0.5620524104198457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785412.9749208044, 785412.974920805, 193889.3269525383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2419200.0000, 
sim time next is 2419800.0000, 
raw observation next is [29.05, 80.0, 1.0, 2.0, 0.5605059738030047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783251.1850596198, 783251.1850596198, 193618.9294176246], 
processed observation next is [1.0, 0.0, 0.575829383886256, 0.8, 1.0, 1.0, 0.4704891250638611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21756977362767219, 0.21756977362767219, 0.2889834767427233], 
reward next is 0.7110, 
noisyNet noise sample is [array([0.6838012], dtype=float32), -1.3971773]. 
=============================================
[2019-03-27 03:41:05,054] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5017365e-26 1.0000000e+00 5.5439107e-37 1.6782846e-31 1.4367568e-23], sum to 1.0000
[2019-03-27 03:41:05,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9326
[2019-03-27 03:41:05,066] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 78.0, 1.0, 2.0, 0.5103374366023732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713122.126165316, 713122.1261653167, 185233.8617384921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2569200.0000, 
sim time next is 2569800.0000, 
raw observation next is [28.76666666666667, 78.5, 1.0, 2.0, 0.5164667385773947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721689.8410657004, 721689.8410656998, 186218.1996147703], 
processed observation next is [1.0, 0.7391304347826086, 0.5624012638230649, 0.785, 1.0, 1.0, 0.4174298055149333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2004694002960279, 0.20046940029602772, 0.2779376113653288], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.1029199], dtype=float32), -0.4975805]. 
=============================================
[2019-03-27 03:41:12,619] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 03:41:12,620] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:41:12,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:41:12,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:41:12,621] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:41:12,622] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:41:12,625] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:41:12,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:41:12,627] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:41:12,627] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:41:12,625] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:41:12,654] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-27 03:41:12,678] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-27 03:41:12,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-27 03:41:12,700] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-27 03:41:12,718] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-27 03:42:47,082] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16913654]
[2019-03-27 03:42:47,083] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.1, 68.0, 1.0, 2.0, 0.9251979983632221, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989348888317, 6.9112, 168.9123159544266, 2190307.31367905, 2123060.742195191, 441133.3952192701]
[2019-03-27 03:42:47,083] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:42:47,085] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.7882399e-16 9.9999988e-01 1.1605327e-24 3.9245842e-14 1.1946506e-07], sampled 0.9073169482931462
[2019-03-27 03:42:47,086] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2190307.31367905 W.
[2019-03-27 03:42:50,939] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16913654]
[2019-03-27 03:42:50,942] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.96666666666667, 83.33333333333334, 1.0, 2.0, 0.5218326018485379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729190.4560791007, 729190.4560791013, 187086.4186834078]
[2019-03-27 03:42:50,942] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:42:50,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1949102e-32 1.0000000e+00 0.0000000e+00 1.4446510e-35 5.7425110e-33], sampled 0.5596525511522266
[2019-03-27 03:42:51,196] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16913654]
[2019-03-27 03:42:51,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.99956182666667, 82.34403213666667, 1.0, 2.0, 0.3331384897244562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522427.6382520947, 522427.6382520947, 168655.1218822314]
[2019-03-27 03:42:51,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:42:51,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40684178634583246
[2019-03-27 03:42:59,154] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16913654]
[2019-03-27 03:42:59,155] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.3, 71.0, 1.0, 2.0, 0.6289932263033753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 878994.8895400735, 878994.8895400729, 206271.4879918262]
[2019-03-27 03:42:59,157] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:42:59,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0201613e-33 1.0000000e+00 0.0000000e+00 1.0358055e-37 1.2619711e-35], sampled 0.5273105147945321
[2019-03-27 03:43:01,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16913654]
[2019-03-27 03:43:01,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.078604365, 93.250484745, 1.0, 2.0, 0.4361294083563689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 631063.8295692845, 631063.8295692845, 176973.7875234751]
[2019-03-27 03:43:01,985] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:43:01,988] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9716136e-34 1.0000000e+00 0.0000000e+00 1.4740533e-37 2.6948894e-35], sampled 0.8787176010628083
[2019-03-27 03:43:07,943] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:43:08,053] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.7488 2842484962.3901 1146.0000
[2019-03-27 03:43:08,170] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9971 3007622987.7552 1772.0000
[2019-03-27 03:43:08,240] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:43:08,328] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164447863.9798 1822.0000
[2019-03-27 03:43:09,346] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1975000, evaluation results [1975000.0, 7876.615405276148, 3164447863.97976, 1822.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7997.997090352206, 3007622987.755177, 1772.0, 8494.748753425478, 2842484962.390094, 1146.0]
[2019-03-27 03:43:09,878] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5067985e-20 1.0000000e+00 1.5859018e-26 1.2754456e-19 5.1999962e-13], sum to 1.0000
[2019-03-27 03:43:09,887] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7099
[2019-03-27 03:43:09,897] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1896049.746803805 W.
[2019-03-27 03:43:09,902] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.5, 87.83333333333334, 1.0, 2.0, 0.452047347475025, 1.0, 2.0, 0.452047347475025, 1.0, 2.0, 0.7723625055712545, 6.911199999999999, 6.9112, 170.5573041426782, 1896049.746803805, 1896049.746803806, 381506.2869631937], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2454600.0000, 
sim time next is 2455200.0000, 
raw observation next is [26.4, 88.0, 1.0, 2.0, 0.7199395563460365, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.976517607252027, 6.9112, 168.9125671562663, 1903031.657677914, 1856693.205847847, 390055.8363474865], 
processed observation next is [1.0, 0.43478260869565216, 0.45023696682464454, 0.88, 1.0, 1.0, 0.6625777787301644, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006531760725202673, 0.0, 0.8294380332451069, 0.5286199049105317, 0.515748112735513, 0.5821728900708754], 
reward next is 0.0912, 
noisyNet noise sample is [array([0.20981039], dtype=float32), 2.2905405]. 
=============================================
[2019-03-27 03:43:11,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:11,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2689
[2019-03-27 03:43:11,922] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 97.0, 1.0, 2.0, 0.3879939931931745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581264.5881379509, 581264.5881379503, 172894.327902857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [22.33333333333334, 98.0, 1.0, 2.0, 0.385555475527079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578518.6681064686, 578518.6681064686, 172675.5154239802], 
processed observation next is [0.0, 0.8260869565217391, 0.2575039494470777, 0.98, 1.0, 1.0, 0.25970539220129996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069963002957463, 0.16069963002957463, 0.2577246498865376], 
reward next is 0.7423, 
noisyNet noise sample is [array([-1.0168911], dtype=float32), 0.5270481]. 
=============================================
[2019-03-27 03:43:17,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4273090e-32 1.0000000e+00 0.0000000e+00 2.1042081e-36 3.8217073e-32], sum to 1.0000
[2019-03-27 03:43:17,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6198
[2019-03-27 03:43:17,747] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.81666666666667, 93.83333333333334, 1.0, 2.0, 0.555026783939127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775591.7684328796, 775591.7684328796, 192666.5527684972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2501400.0000, 
sim time next is 2502000.0000, 
raw observation next is [26.8, 94.0, 1.0, 2.0, 0.5537772372156778, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773845.0217768044, 773845.0217768044, 192450.7340208152], 
processed observation next is [1.0, 1.0, 0.4691943127962086, 0.94, 1.0, 1.0, 0.46238221351286485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21495695049355676, 0.21495695049355676, 0.2872399015236048], 
reward next is 0.7128, 
noisyNet noise sample is [array([1.3716635], dtype=float32), 0.20550017]. 
=============================================
[2019-03-27 03:43:17,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.275475]
 [74.26753 ]
 [74.271576]
 [74.27449 ]
 [74.25972 ]], R is [[74.37754059]
 [74.34620667]
 [74.31518555]
 [74.28503418]
 [74.25553131]].
[2019-03-27 03:43:19,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2261442e-19 1.0000000e+00 4.6062120e-28 2.0305266e-20 3.0738462e-13], sum to 1.0000
[2019-03-27 03:43:19,464] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0236
[2019-03-27 03:43:19,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1873069.396336126 W.
[2019-03-27 03:43:19,478] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 89.66666666666667, 1.0, 2.0, 0.6698598963883633, 1.0, 2.0, 0.6698598963883633, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1873069.396336126, 1873069.396336126, 361487.8584262934], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2537400.0000, 
sim time next is 2538000.0000, 
raw observation next is [27.1, 89.0, 1.0, 2.0, 0.4351568596965524, 1.0, 2.0, 0.4351568596965524, 1.0, 1.0, 0.750959044990375, 6.9112, 6.9112, 170.5573041426782, 1825144.60475876, 1825144.60475876, 372468.8411937505], 
processed observation next is [1.0, 0.391304347826087, 0.4834123222748816, 0.89, 1.0, 1.0, 0.3194660960199426, 1.0, 1.0, 0.3194660960199426, 1.0, 0.5, 0.696291518280945, 0.0, 0.0, 0.8375144448122397, 0.5069846124329889, 0.5069846124329889, 0.5559236435727619], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15945454], dtype=float32), -0.04460315]. 
=============================================
[2019-03-27 03:43:19,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[50.957596]
 [51.50725 ]
 [53.859085]
 [56.674286]
 [61.942364]], R is [[49.03064346]
 [48.54033661]
 [48.48760605]
 [48.43248749]
 [47.94816208]].
[2019-03-27 03:43:19,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1281869e-22 1.0000000e+00 8.0071631e-32 4.0703778e-23 1.2840486e-17], sum to 1.0000
[2019-03-27 03:43:19,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7592
[2019-03-27 03:43:19,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2048085.342975294 W.
[2019-03-27 03:43:19,916] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.8, 91.0, 1.0, 2.0, 0.8235873475599151, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.98749504138869, 6.9112, 168.911635066394, 2048085.342975294, 1993959.438183922, 414432.7069745309], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2536200.0000, 
sim time next is 2536800.0000, 
raw observation next is [26.9, 90.33333333333333, 1.0, 2.0, 0.7402248260871283, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988118678816294, 6.9112, 168.9124363617342, 1931419.05746193, 1876850.467411469, 394276.8926711241], 
processed observation next is [1.0, 0.34782608695652173, 0.4739336492890995, 0.9033333333333333, 1.0, 1.0, 0.6870178627555762, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007691867881629388, 0.0, 0.8294373909840762, 0.536505293739425, 0.5213473520587414, 0.5884729741360061], 
reward next is 0.0269, 
noisyNet noise sample is [array([0.7665266], dtype=float32), -0.72593063]. 
=============================================
[2019-03-27 03:43:30,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:30,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7937
[2019-03-27 03:43:30,883] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3028660219808765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482297.4615000679, 482297.4615000679, 165786.6792181012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2943600.0000, 
sim time next is 2944200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3029009757181943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482353.0474773564, 482353.0474773558, 165790.674409861], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16012165749180035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.133986957632599, 0.13398695763259882, 0.24744876777591196], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.15533318], dtype=float32), -0.1543041]. 
=============================================
[2019-03-27 03:43:39,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7348344e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.4412877e-37], sum to 1.0000
[2019-03-27 03:43:39,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5245
[2019-03-27 03:43:39,880] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 94.0, 1.0, 2.0, 0.7305418848079579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1106169.369365209, 1106169.369365209, 238633.0476941622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [22.33333333333334, 94.0, 1.0, 2.0, 0.7164438127666028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090504.173378281, 1090504.17337828, 235879.3611440067], 
processed observation next is [1.0, 0.5652173913043478, 0.2575039494470777, 0.94, 1.0, 1.0, 0.6583660394778348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30291782593841143, 0.3029178259384111, 0.3520587479761294], 
reward next is 0.6479, 
noisyNet noise sample is [array([0.17921107], dtype=float32), -0.26654562]. 
=============================================
[2019-03-27 03:43:39,919] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:39,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3820
[2019-03-27 03:43:39,935] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3453979577601216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532077.8387668893, 532077.83876689, 169169.26886832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3448394295254273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531217.893419224, 531217.8934192245, 169099.5435714672], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21064991509087624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14756052594978444, 0.14756052594978458, 0.2523873784648764], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.66788906], dtype=float32), 0.46444413]. 
=============================================
[2019-03-27 03:43:44,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4599567e-17 1.0000000e+00 5.3448920e-27 1.2423760e-15 6.7783628e-09], sum to 1.0000
[2019-03-27 03:43:44,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8539
[2019-03-27 03:43:44,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1981305.120441177 W.
[2019-03-27 03:43:44,204] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.33333333333334, 68.66666666666667, 1.0, 2.0, 0.4723547591412751, 1.0, 1.0, 0.4723547591412751, 1.0, 2.0, 0.8037966117148551, 6.9112, 6.9112, 170.5573041426782, 1981305.120441177, 1981305.120441177, 393719.4131201182], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3487200.0000, 
sim time next is 3487800.0000, 
raw observation next is [29.66666666666666, 67.33333333333333, 1.0, 2.0, 0.7145542460026061, 1.0, 2.0, 0.7145542460026061, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1998160.755326305, 1998160.755326304, 380409.873651267], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590835, 0.6733333333333333, 1.0, 1.0, 0.6560894530151881, 1.0, 1.0, 0.6560894530151881, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.555044654257307, 0.5550446542573066, 0.5677759308227865], 
reward next is 0.4322, 
noisyNet noise sample is [array([-0.03514137], dtype=float32), -1.0023023]. 
=============================================
[2019-03-27 03:43:45,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0844728e-12 8.9837933e-01 3.9915078e-17 6.4059036e-10 1.0162065e-01], sum to 1.0000
[2019-03-27 03:43:45,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0027
[2019-03-27 03:43:45,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2861740.474688875 W.
[2019-03-27 03:43:45,991] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.5, 1.0, 2.0, 0.7228301719231829, 1.0, 2.0, 0.682005125475854, 1.0, 1.0, 1.03, 7.005099532362843, 6.9112, 170.5573041426782, 2861740.474688875, 2794476.420068603, 528637.3203325783], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3423000.0000, 
sim time next is 3423600.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.9569115565411078, 1.0, 2.0, 0.9569115565411078, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2676646.448009587, 2676646.448009587, 503496.7739181815], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 0.9480862127001299, 1.0, 1.0, 0.9480862127001299, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7435129022248852, 0.7435129022248852, 0.7514877222659425], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.27139404], dtype=float32), 0.041212585]. 
=============================================
[2019-03-27 03:43:47,116] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7001116e-34 1.0000000e+00 0.0000000e+00 4.6104426e-38 1.3235711e-32], sum to 1.0000
[2019-03-27 03:43:47,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9904
[2019-03-27 03:43:47,130] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8055032758053962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1168167.467344518, 1168167.467344518, 251170.0994153609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3085200.0000, 
sim time next is 3085800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4960027251284814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719722.7308739586, 719722.730873958, 186343.3714263823], 
processed observation next is [1.0, 0.7391304347826086, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3927743676246764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19992298079832183, 0.19992298079832166, 0.2781244349647497], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.09283991], dtype=float32), 0.48730117]. 
=============================================
[2019-03-27 03:43:49,779] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0369684e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3180639e-38], sum to 1.0000
[2019-03-27 03:43:49,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3178
[2019-03-27 03:43:49,790] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 89.0, 1.0, 2.0, 0.7733679151593492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080855.876382472, 1080855.876382473, 237480.0817162002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3393000.0000, 
sim time next is 3393600.0000, 
raw observation next is [27.66666666666666, 89.0, 1.0, 2.0, 0.8138384672348753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137447.650540901, 1137447.650540901, 247337.8339284521], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.89, 1.0, 1.0, 0.7757089966685244, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3159576807058059, 0.3159576807058059, 0.36916094616186884], 
reward next is 0.6308, 
noisyNet noise sample is [array([1.0104445], dtype=float32), 0.32450765]. 
=============================================
[2019-03-27 03:43:50,976] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:50,985] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7557
[2019-03-27 03:43:51,000] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 93.66666666666667, 1.0, 2.0, 0.4849802903213595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677677.9107701418, 677677.9107701418, 181276.2185934498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3201000.0000, 
sim time next is 3201600.0000, 
raw observation next is [25.0, 93.33333333333334, 1.0, 2.0, 0.4839596938315129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676251.3459021929, 676251.3459021929, 181120.9643241533], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.9333333333333335, 1.0, 1.0, 0.37826469136326857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18784759608394247, 0.18784759608394247, 0.27032979749873626], 
reward next is 0.7297, 
noisyNet noise sample is [array([-2.5268457], dtype=float32), 0.2344278]. 
=============================================
[2019-03-27 03:43:57,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1372628e-26 1.0000000e+00 2.5562410e-37 9.3343421e-29 2.8122258e-25], sum to 1.0000
[2019-03-27 03:43:57,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3217
[2019-03-27 03:43:57,804] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5259630076249933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734964.1360236648, 734964.1360236655, 187763.8493002175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3700800.0000, 
sim time next is 3701400.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5250013921539649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733619.94084384, 733619.9408438393, 187605.9176718834], 
processed observation next is [1.0, 0.8695652173913043, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.4277125206674276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20378331690106666, 0.20378331690106646, 0.28000883234609464], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.3866605], dtype=float32), 0.40178925]. 
=============================================
[2019-03-27 03:43:57,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:43:57,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6328
[2019-03-27 03:43:58,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5557936098391063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776663.7190279795, 776663.719027979, 192800.3245058036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3322800.0000, 
sim time next is 3323400.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.5614823178100475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 784616.0323224632, 784616.0323224625, 193790.2628569866], 
processed observation next is [0.0, 0.4782608695652174, 0.6761453396524489, 0.695, 1.0, 1.0, 0.47166544314463554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2179488978673509, 0.2179488978673507, 0.28923919829400985], 
reward next is 0.7108, 
noisyNet noise sample is [array([-0.3348925], dtype=float32), 0.28186586]. 
=============================================
[2019-03-27 03:44:01,746] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0990633e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7895339e-37], sum to 1.0000
[2019-03-27 03:44:01,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-27 03:44:01,757] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.6043945498142296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844605.4761597177, 844605.4761597177, 201567.878803986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3334200.0000, 
sim time next is 3334800.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.5994192432148906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837650.039134775, 837650.039134775, 200638.384614923], 
processed observation next is [0.0, 0.6086956521739131, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5173725821866152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2326805664263264, 0.2326805664263264, 0.2994602755446612], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.04016325], dtype=float32), -0.026205514]. 
=============================================
[2019-03-27 03:44:04,748] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:44:04,750] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:44:04,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:44:04,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:44:04,753] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:44:04,754] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:44:04,756] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:44:04,757] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:44:04,758] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:44:04,761] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:44:04,753] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:44:05,438] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-27 03:44:05,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-27 03:44:05,584] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-27 03:44:05,605] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-27 03:44:05,708] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-27 03:44:11,636] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:11,638] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.53333333333333, 47.33333333333333, 1.0, 2.0, 0.2276353711234594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 378512.9448147927, 378512.9448147933, 158440.3449063933]
[2019-03-27 03:44:11,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:44:11,641] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6299296e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5278088914793942
[2019-03-27 03:44:11,861] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:11,862] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.86666666666667, 52.0, 1.0, 2.0, 0.2071655621469337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 345962.248206405, 345962.248206405, 156159.5066930492]
[2019-03-27 03:44:11,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:44:11,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0128055e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5567805e-38], sampled 0.879301536302841
[2019-03-27 03:44:14,159] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:14,160] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.73758651333333, 56.05491733666666, 1.0, 2.0, 0.3332862005322995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555092.5552845518, 555092.5552845518, 169835.9721341389]
[2019-03-27 03:44:14,161] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:44:14,164] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8433508e-35 1.0000000e+00 0.0000000e+00 2.5238868e-38 2.0917871e-36], sampled 0.7163122698095261
[2019-03-27 03:44:17,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:17,458] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.6, 92.16666666666667, 1.0, 2.0, 0.2156493060691995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 157168.2631591955]
[2019-03-27 03:44:17,459] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:44:17,462] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9092055415454329
[2019-03-27 03:44:19,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:19,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.86212199, 88.40100281333335, 1.0, 2.0, 0.3668033634541551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559837.4309695003, 559837.4309695003, 171331.8396085713]
[2019-03-27 03:44:19,589] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:44:19,591] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.5489337e-34 1.0000000e+00 0.0000000e+00 1.0975949e-36 4.5624435e-34], sampled 0.24150660795937395
[2019-03-27 03:44:21,471] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:21,472] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.53333333333333, 94.0, 1.0, 2.0, 0.301038727251195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 480753.8977186612, 480753.8977186605, 165694.1903822359]
[2019-03-27 03:44:21,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:44:21,477] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9593596910117583
[2019-03-27 03:44:35,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:35,635] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.7, 91.83333333333334, 1.0, 2.0, 0.4098919630864442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602879.7657741401, 602879.7657741407, 174549.898789774]
[2019-03-27 03:44:35,637] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:44:35,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6174646035495145
[2019-03-27 03:44:37,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:44:37,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.06666666666667, 92.33333333333334, 1.0, 2.0, 0.4926859551076981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688448.762804075, 688448.762804075, 182457.3454764661]
[2019-03-27 03:44:37,661] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:44:37,664] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2461111e-34 1.0000000e+00 0.0000000e+00 1.0650065e-37 4.1196822e-36], sampled 0.4974304536566607
[2019-03-27 03:45:16,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17158045]
[2019-03-27 03:45:16,839] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.9, 71.0, 1.0, 2.0, 0.8477595723827576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1184883.36514702, 1184883.36514702, 255979.4793541013]
[2019-03-27 03:45:16,841] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:45:16,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5352546e-25 1.0000000e+00 5.5042575e-35 5.5946533e-26 1.7944507e-20], sampled 0.46965897685335156
[2019-03-27 03:46:01,458] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8274.6339 2926578019.5829 1292.0000
[2019-03-27 03:46:01,515] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7989.3740 3158744796.9318 1638.0000
[2019-03-27 03:46:01,724] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8672.5711 2778927922.8260 908.0000
[2019-03-27 03:46:01,746] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8546.4116 2839757886.3222 1035.0000
[2019-03-27 03:46:01,799] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8132.1139 3000616708.1607 1412.0000
[2019-03-27 03:46:02,815] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2000000, evaluation results [2000000.0, 7989.374013797929, 3158744796.9318266, 1638.0, 8274.63394111798, 2926578019.5828886, 1292.0, 8672.571131252651, 2778927922.8260074, 908.0, 8132.1138737625815, 3000616708.160683, 1412.0, 8546.411561634102, 2839757886.3221703, 1035.0]
[2019-03-27 03:46:09,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2354900e-19 1.4400062e-14 1.3159335e-25 1.8252693e-13 1.0000000e+00], sum to 1.0000
[2019-03-27 03:46:09,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1656
[2019-03-27 03:46:09,124] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.5920424793186645, 1.0, 2.0, 0.5920424793186645, 1.0, 2.0, 1.028181995136741, 6.9112, 6.9112, 170.5573041426782, 2483875.764009276, 2483875.764009276, 484628.1141480715], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.5952662787291026, 1.0, 2.0, 0.5952662787291026, 1.0, 2.0, 1.03, 6.915450010143859, 6.9112, 170.5573041426782, 2497414.521407077, 2494370.066261671, 486267.6080484358], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.59, 1.0, 1.0, 0.5123690105169911, 1.0, 1.0, 0.5123690105169911, 1.0, 1.0, 1.0365853658536586, 0.0004250010143858951, 0.0, 0.8375144448122397, 0.6937262559464104, 0.6928805739615753, 0.7257725493260236], 
reward next is 0.2530, 
noisyNet noise sample is [array([1.1104217], dtype=float32), 0.39893115]. 
=============================================
[2019-03-27 03:46:28,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2374860e-30 1.0000000e+00 0.0000000e+00 2.6728596e-33 3.6305732e-30], sum to 1.0000
[2019-03-27 03:46:28,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8008
[2019-03-27 03:46:28,394] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.0, 1.0, 2.0, 0.4810146527443667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693402.7587438179, 693402.7587438179, 183381.123940139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3735600.0000, 
sim time next is 3736200.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.4825924020485993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695678.6407714129, 695678.6407714129, 183628.2395879161], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 1.0, 1.0, 0.37661735186578227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19324406688094803, 0.19324406688094803, 0.2740719993849494], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.09584815], dtype=float32), 1.3428131]. 
=============================================
[2019-03-27 03:46:31,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5027635e-32 1.0000000e+00 0.0000000e+00 2.0996628e-35 3.1530046e-33], sum to 1.0000
[2019-03-27 03:46:31,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1406
[2019-03-27 03:46:31,762] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5188052856938304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724958.7478168076, 724958.747816807, 186594.8961358836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3797400.0000, 
sim time next is 3798000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5178731589736605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723655.7857654184, 723655.785765419, 186443.8437867631], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41912428792007284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20101549604594957, 0.20101549604594973, 0.2782743937115867], 
reward next is 0.7217, 
noisyNet noise sample is [array([-0.8046664], dtype=float32), 0.56343126]. 
=============================================
[2019-03-27 03:46:31,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.77612 ]
 [70.76077 ]
 [70.739296]
 [70.72409 ]
 [70.69845 ]], R is [[70.99185944]
 [71.00344086]
 [71.0147171 ]
 [71.02583313]
 [71.03679657]].
[2019-03-27 03:46:35,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0626649e-22 1.0000000e+00 2.4330587e-29 1.3528208e-24 2.5410840e-22], sum to 1.0000
[2019-03-27 03:46:35,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6343
[2019-03-27 03:46:35,726] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 84.0, 1.0, 2.0, 0.9560641707630189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336352.111159913, 1336352.111159913, 285829.2553981232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3997200.0000, 
sim time next is 3997800.0000, 
raw observation next is [29.5, 84.0, 1.0, 2.0, 1.003204192902753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1402286.256768923, 1402286.256768923, 299908.4481120037], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.84, 1.0, 1.0, 1.003860473376811, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3895239602135897, 0.3895239602135897, 0.447624549420901], 
reward next is 0.5524, 
noisyNet noise sample is [array([2.524049], dtype=float32), -0.13848692]. 
=============================================
[2019-03-27 03:46:40,313] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0777032e-24 1.0000000e+00 3.3068205e-31 3.5778969e-25 2.5783054e-22], sum to 1.0000
[2019-03-27 03:46:40,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4159
[2019-03-27 03:46:40,330] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5697780837538351, 1.0, 1.0, 0.5697780837538351, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1593011.550848764, 1593011.550848764, 323362.8714968397], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.942727319718486, 6.9112, 168.9124785163633, 1476136.748192963, 1453770.245791528, 311352.5253437084], 
processed observation next is [1.0, 0.17391304347826086, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.003152731971848599, 0.0, 0.8294375979826036, 0.4100379856091564, 0.4038250682754244, 0.4647052617070275], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9317862], dtype=float32), 0.6482974]. 
=============================================
[2019-03-27 03:46:40,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[40.39972 ]
 [40.847595]
 [41.005318]
 [40.722305]
 [40.09655 ]], R is [[41.27545547]
 [41.38006973]
 [40.96627045]
 [40.55660629]
 [40.66074753]].
[2019-03-27 03:46:45,137] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:46:45,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3056
[2019-03-27 03:46:45,149] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 73.0, 1.0, 2.0, 0.6370113381570961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890204.6016731247, 890204.6016731247, 207844.546868255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4447800.0000, 
sim time next is 4448400.0000, 
raw observation next is [32.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6385049556519021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 892292.7649808782, 892292.7649808788, 208139.5210176102], 
processed observation next is [0.0, 0.4782608695652174, 0.7472353870458138, 0.7233333333333333, 1.0, 1.0, 0.5644638019902435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24785910138357728, 0.24785910138357745, 0.3106560015188212], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.5518865], dtype=float32), 1.7443284]. 
=============================================
[2019-03-27 03:46:46,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1993755e-22 1.0000000e+00 2.6418841e-31 2.1459135e-22 2.0230647e-21], sum to 1.0000
[2019-03-27 03:46:46,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4805
[2019-03-27 03:46:46,299] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.9772308967099471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1365957.233263548, 1365957.233263548, 292071.0438376482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.985891497624193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1378070.735662323, 1378070.735662324, 294662.6440260832], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.8483333333333333, 1.0, 1.0, 0.9830018043664976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38279742657286747, 0.38279742657286775, 0.43979499108370623], 
reward next is 0.5602, 
noisyNet noise sample is [array([0.9618772], dtype=float32), -0.32801786]. 
=============================================
[2019-03-27 03:46:58,167] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:46:58,169] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:46:58,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:58,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:46:58,174] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:58,175] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:46:58,176] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:58,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:46:58,180] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:46:58,181] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:58,182] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:46:58,205] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-27 03:46:58,228] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-27 03:46:58,252] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-27 03:46:58,253] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-27 03:46:58,302] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-27 03:47:24,254] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17001845]
[2019-03-27 03:47:24,255] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 84.0, 1.0, 2.0, 0.4239694522869373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628215.4468869021, 628215.4468869021, 177079.1144202538]
[2019-03-27 03:47:24,256] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:47:24,258] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6148e-35 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00], sampled 0.007829205022154784
[2019-03-27 03:47:31,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17001845]
[2019-03-27 03:47:31,948] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 90.66666666666667, 1.0, 2.0, 0.3887246542777541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586633.4272944353, 586633.4272944346, 173505.3499841618]
[2019-03-27 03:47:31,949] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:47:31,952] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8273675e-35 1.0000000e+00 0.0000000e+00 2.6554844e-38 3.1453442e-38], sampled 0.40812565276671586
[2019-03-27 03:48:32,687] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17001845]
[2019-03-27 03:48:32,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.4, 84.33333333333334, 1.0, 2.0, 0.5257489392812211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734664.9002209393, 734664.9002209399, 187728.8035180014]
[2019-03-27 03:48:32,690] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:48:32,693] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2864699e-33 1.0000000e+00 0.0000000e+00 3.4781044e-36 9.7306503e-36], sampled 0.12054702607681556
[2019-03-27 03:48:43,894] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17001845]
[2019-03-27 03:48:43,894] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.38532489, 96.94766395, 1.0, 2.0, 0.3665075149015676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567051.5100708159, 567051.5100708165, 172152.3170761637]
[2019-03-27 03:48:43,896] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:48:43,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8495023e-33 1.0000000e+00 0.0000000e+00 7.9584239e-36 1.8486392e-35], sampled 0.021093725358408033
[2019-03-27 03:48:53,363] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.9459 2927200717.0885 1336.0000
[2019-03-27 03:48:53,537] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.9383 3163936685.2728 1877.0000
[2019-03-27 03:48:53,540] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.0783 2779054998.4099 929.0000
[2019-03-27 03:48:53,662] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5146 2842362399.3880 1159.0000
[2019-03-27 03:48:53,800] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8005.5352 3007408417.8615 1745.0000
[2019-03-27 03:48:54,816] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2025000, evaluation results [2025000.0, 7876.938274145649, 3163936685.2727804, 1877.0, 8256.945892500698, 2927200717.0885367, 1336.0, 8663.078297500135, 2779054998.409887, 929.0, 8005.535155597081, 3007408417.8614855, 1745.0, 8493.514635035299, 2842362399.3880067, 1159.0]
[2019-03-27 03:49:15,766] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0298091e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 03:49:15,772] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6615
[2019-03-27 03:49:15,780] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4830505893968829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675104.7403194243, 675104.740319425, 180998.4027749045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5112600.0000, 
sim time next is 5113200.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.482381796318133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674170.0715703844, 674170.0715703837, 180897.1000542454], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37636361002184704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1872694643251068, 0.1872694643251066, 0.26999567172275435], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.17856653], dtype=float32), 1.3699367]. 
=============================================
[2019-03-27 03:49:18,721] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9932764e-22 1.0000000e+00 3.5210861e-31 5.2181218e-23 1.2703095e-17], sum to 1.0000
[2019-03-27 03:49:18,730] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2896
[2019-03-27 03:49:18,735] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 87.33333333333333, 1.0, 2.0, 0.5210692160865329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728123.3621637849, 728123.3621637849, 186963.5268261094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4671600.0000, 
sim time next is 4672200.0000, 
raw observation next is [27.0, 88.16666666666667, 1.0, 2.0, 0.5249932849139376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733608.6081372208, 733608.6081372208, 187605.2002836865], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8816666666666667, 1.0, 1.0, 0.4277027529083585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2037801689270058, 0.2037801689270058, 0.28000776161744256], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.4359961], dtype=float32), 0.9067863]. 
=============================================
[2019-03-27 03:49:19,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9485222e-17 1.0000000e+00 3.7153025e-26 1.7960031e-15 2.3214255e-09], sum to 1.0000
[2019-03-27 03:49:19,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8335
[2019-03-27 03:49:19,955] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.8620881488115991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1204921.269443467, 1204921.269443467, 259722.8437797384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4692000.0000, 
sim time next is 4692600.0000, 
raw observation next is [28.5, 81.5, 1.0, 2.0, 0.8634290055778411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206796.419912639, 1206796.419912639, 260076.989897066], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.815, 1.0, 1.0, 0.8354566332263146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3352212277535109, 0.3352212277535109, 0.3881746117866657], 
reward next is 0.6118, 
noisyNet noise sample is [array([0.86741614], dtype=float32), 1.611512]. 
=============================================
[2019-03-27 03:49:20,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2574599e-21 1.0000000e+00 5.5068130e-30 1.6212934e-20 1.8650995e-14], sum to 1.0000
[2019-03-27 03:49:20,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0502
[2019-03-27 03:49:20,032] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.83333333333333, 1.0, 2.0, 0.5101189259999527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712816.6870251385, 712816.6870251378, 185196.9682904144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4669800.0000, 
sim time next is 4670400.0000, 
raw observation next is [27.0, 85.66666666666667, 1.0, 2.0, 0.5136318924589645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717727.2028932939, 717727.2028932932, 185759.8312043077], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8566666666666667, 1.0, 1.0, 0.41401432826381257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19936866747035942, 0.19936866747035922, 0.27725347940941447], 
reward next is 0.7227, 
noisyNet noise sample is [array([0.26143205], dtype=float32), 0.042826384]. 
=============================================
[2019-03-27 03:49:21,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7862942e-16 7.9417230e-05 4.6773725e-23 1.2316577e-12 9.9992061e-01], sum to 1.0000
[2019-03-27 03:49:21,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5858
[2019-03-27 03:49:21,426] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.4324726414308352, 1.0, 2.0, 0.4324726414308352, 1.0, 2.0, 0.7452593823608106, 6.911199999999999, 6.9112, 170.5573041426782, 1813876.867443668, 1813876.867443669, 370702.8390163624], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4960800.0000, 
sim time next is 4961400.0000, 
raw observation next is [30.0, 69.33333333333334, 1.0, 2.0, 0.4048118025599658, 1.0, 2.0, 0.4048118025599658, 1.0, 2.0, 0.6963046251673277, 6.9112, 6.9112, 170.5573041426782, 1697769.862994946, 1697769.862994946, 354613.2738751813], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6933333333333335, 1.0, 1.0, 0.28290578621682627, 1.0, 1.0, 0.28290578621682627, 1.0, 1.0, 0.6296397867894241, 0.0, 0.0, 0.8375144448122397, 0.47160273972081834, 0.47160273972081834, 0.5292735430972855], 
reward next is 0.4707, 
noisyNet noise sample is [array([0.67841625], dtype=float32), 1.4309548]. 
=============================================
[2019-03-27 03:49:22,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7008959e-29 1.0000000e+00 0.0000000e+00 2.9177565e-32 9.9272194e-30], sum to 1.0000
[2019-03-27 03:49:22,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8774
[2019-03-27 03:49:22,365] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 66.33333333333334, 1.0, 2.0, 0.5779105018786459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807581.5226610766, 807581.5226610772, 196702.8963192595], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5141400.0000, 
sim time next is 5142000.0000, 
raw observation next is [32.0, 65.66666666666667, 1.0, 2.0, 0.5698275538916122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796282.0459488718, 796282.0459488718, 195259.7981477057], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6566666666666667, 1.0, 1.0, 0.48171994444772553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22118945720801994, 0.22118945720801994, 0.29143253454881446], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.5402671], dtype=float32), -0.49201727]. 
=============================================
[2019-03-27 03:49:22,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.81456]
 [70.86256]
 [70.80177]
 [70.8565 ]
 [70.88951]], R is [[70.79000092]
 [70.78851318]
 [70.78961182]
 [70.79318237]
 [70.79904175]].
[2019-03-27 03:49:22,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6923845e-29 1.0000000e+00 0.0000000e+00 6.5913605e-33 3.0324989e-29], sum to 1.0000
[2019-03-27 03:49:22,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0521
[2019-03-27 03:49:22,678] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5524797156795057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772031.213966883, 772031.213966883, 192227.284804925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5146200.0000, 
sim time next is 5146800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5515039770462621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770667.2291260133, 770667.2291260128, 192059.4410722862], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45964334583886995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2140742303127815, 0.21407423031278133, 0.2866558821974421], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.44044918], dtype=float32), 0.152773]. 
=============================================
[2019-03-27 03:49:25,190] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:49:25,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0168
[2019-03-27 03:49:25,208] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 89.0, 1.0, 2.0, 0.4772217484235572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669114.770606295, 669114.770606295, 180398.2270057079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5025000.0000, 
sim time next is 5025600.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4680428867001918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661094.291969632, 661094.2919696313, 179654.2606958098], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.89, 1.0, 1.0, 0.35908781530143585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18363730332489778, 0.18363730332489758, 0.26814068760568627], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.34750634], dtype=float32), -0.48192006]. 
=============================================
[2019-03-27 03:49:30,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:49:30,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4400
[2019-03-27 03:49:30,486] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5068745005311205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708281.5648995986, 708281.564899598, 184680.3678584687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5055298757377807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706402.0274291529, 706402.0274291529, 184467.3025625779], 
processed observation next is [0.0, 0.43478260869565216, 0.6208530805687204, 0.66, 1.0, 1.0, 0.40425286233467556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1962227853969869, 0.1962227853969869, 0.2753243321829521], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.20273231], dtype=float32), 1.1055512]. 
=============================================
[2019-03-27 03:49:34,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:49:34,999] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1897
[2019-03-27 03:49:35,005] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5220508323514502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729495.5083918261, 729495.5083918266, 187123.1724870216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5125800.0000, 
sim time next is 5126400.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5237423621435907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731860.0070143661, 731860.0070143661, 187399.706347494], 
processed observation next is [0.0, 0.34782608695652173, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4261956170404707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20329444639287947, 0.20329444639287947, 0.27970105424999103], 
reward next is 0.7203, 
noisyNet noise sample is [array([-1.3950245], dtype=float32), 0.16605659]. 
=============================================
[2019-03-27 03:49:38,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0407977e-30 1.0000000e+00 0.0000000e+00 4.0786903e-32 1.1198187e-30], sum to 1.0000
[2019-03-27 03:49:38,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6348
[2019-03-27 03:49:38,816] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5161590341472515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721259.7213000394, 721259.7213000394, 186166.1952553471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5190600.0000, 
sim time next is 5191200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.51552657840625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720375.6536032964, 720375.6536032964, 186064.1725502636], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41629708241716873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20010434822313788, 0.20010434822313788, 0.27770772022427404], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.9452684], dtype=float32), 1.493879]. 
=============================================
[2019-03-27 03:49:43,626] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4688428e-15 6.7772153e-06 6.9645417e-21 9.2677721e-10 9.9999321e-01], sum to 1.0000
[2019-03-27 03:49:43,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8360
[2019-03-27 03:49:43,643] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.26666666666667, 55.33333333333334, 1.0, 2.0, 0.5297382573777845, 1.0, 2.0, 0.5297382573777845, 1.0, 2.0, 0.9195178514672427, 6.9112, 6.9112, 170.5573041426782, 2222250.032870941, 2222250.032870941, 436230.6402336875], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5574000.0000, 
sim time next is 5574600.0000, 
raw observation next is [33.35, 54.5, 1.0, 2.0, 0.4945300998634935, 1.0, 2.0, 0.4945300998634935, 1.0, 2.0, 0.8561357541451295, 6.9112, 6.9112, 170.5573041426782, 2074410.257218136, 2074410.257218136, 410903.1231814919], 
processed observation next is [1.0, 0.5217391304347826, 0.7796208530805688, 0.545, 1.0, 1.0, 0.391000120317462, 1.0, 1.0, 0.391000120317462, 1.0, 1.0, 0.8245557977379628, 0.0, 0.0, 0.8375144448122397, 0.5762250714494822, 0.5762250714494822, 0.6132882435544655], 
reward next is 0.3867, 
noisyNet noise sample is [array([-0.22511512], dtype=float32), 0.49695963]. 
=============================================
[2019-03-27 03:49:43,988] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6914136e-26 1.0000000e+00 4.3962136e-37 9.3337055e-28 1.6860614e-24], sum to 1.0000
[2019-03-27 03:49:43,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7537
[2019-03-27 03:49:44,004] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 78.5, 1.0, 2.0, 0.6276846513960663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 877165.4491132319, 877165.4491132324, 206017.5566451002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5344200.0000, 
sim time next is 5344800.0000, 
raw observation next is [31.16666666666666, 78.66666666666667, 1.0, 2.0, 0.6261678895408834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875044.9588910389, 875044.9588910383, 205722.8452377084], 
processed observation next is [1.0, 0.8695652173913043, 0.6761453396524484, 0.7866666666666667, 1.0, 1.0, 0.549599866916727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2430680441363997, 0.24306804413639954, 0.30704902274284834], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.11408976], dtype=float32), 0.99273837]. 
=============================================
[2019-03-27 03:49:44,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3765524e-26 1.0000000e+00 1.8252471e-37 2.6592906e-27 5.2574834e-23], sum to 1.0000
[2019-03-27 03:49:44,890] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3774
[2019-03-27 03:49:44,898] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 79.0, 1.0, 2.0, 0.5815143960302137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812619.5893772576, 812619.589377257, 197352.7054057249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5516400.0000, 
sim time next is 5517000.0000, 
raw observation next is [29.7, 79.5, 1.0, 2.0, 0.5803214950781144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810951.969889811, 810951.969889811, 197137.1533172601], 
processed observation next is [1.0, 0.8695652173913043, 0.6066350710900474, 0.795, 1.0, 1.0, 0.49436324708206547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22526443608050306, 0.22526443608050306, 0.29423455718994046], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.77853596], dtype=float32), -1.1342362]. 
=============================================
[2019-03-27 03:49:44,921] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.97915]
 [68.19593]
 [68.47441]
 [68.62257]
 [68.45564]], R is [[67.86706543]
 [67.89383698]
 [67.9201355 ]
 [67.94697571]
 [67.97432709]].
[2019-03-27 03:49:45,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6650274e-30 1.0000000e+00 0.0000000e+00 3.1580740e-32 3.3662983e-30], sum to 1.0000
[2019-03-27 03:49:45,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4582
[2019-03-27 03:49:45,450] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5164354593231327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721646.1178474353, 721646.1178474353, 186210.8231642146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5095800.0000, 
sim time next is 5096400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5156883475053693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720601.7797672314, 720601.7797672321, 186090.2565777657], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.416491984946228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20016716104645318, 0.20016716104645338, 0.2777466516086055], 
reward next is 0.7223, 
noisyNet noise sample is [array([0.37851447], dtype=float32), -0.17913938]. 
=============================================
[2019-03-27 03:49:47,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9494188e-20 1.0000000e+00 8.4167058e-30 6.8318429e-19 1.8876660e-16], sum to 1.0000
[2019-03-27 03:49:47,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2464
[2019-03-27 03:49:47,730] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 86.0, 1.0, 2.0, 0.5726736550450254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800260.7110260761, 800260.7110260755, 195765.8786203192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5274000.0000, 
sim time next is 5274600.0000, 
raw observation next is [28.6, 86.16666666666667, 1.0, 2.0, 0.5740438784564088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 802176.1999237338, 802176.1999237344, 196010.3177817738], 
processed observation next is [1.0, 0.043478260869565216, 0.5545023696682465, 0.8616666666666667, 1.0, 1.0, 0.4867998535619383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22282672220103716, 0.22282672220103733, 0.2925527131071251], 
reward next is 0.7074, 
noisyNet noise sample is [array([0.25754935], dtype=float32), 0.032083075]. 
=============================================
[2019-03-27 03:49:50,178] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 03:49:50,180] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:49:50,182] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:49:50,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:49:50,182] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:49:50,184] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:49:50,184] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:49:50,186] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:49:50,188] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:49:50,189] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:49:50,188] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:49:50,211] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-27 03:49:50,234] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-27 03:49:50,260] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-27 03:49:50,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-27 03:49:50,301] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-27 03:49:57,672] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:49:57,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.40187084166666, 82.65363055833333, 1.0, 2.0, 0.2672948782940797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 443672.3447066786, 443672.3447066786, 162427.1257687329]
[2019-03-27 03:49:57,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:49:57,677] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14683778288442617
[2019-03-27 03:50:31,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:50:31,376] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.89614448666667, 100.0, 1.0, 2.0, 0.4237023820157643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616433.5883441039, 616433.5883441039, 175639.703574333]
[2019-03-27 03:50:31,378] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:50:31,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9561167e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.43767085066334066
[2019-03-27 03:50:38,406] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:50:38,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 94.0, 1.0, 2.0, 0.4946109458778465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691139.5004237609, 691139.5004237616, 182755.5751705243]
[2019-03-27 03:50:38,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:50:38,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.5699164e-31 1.0000000e+00 0.0000000e+00 1.9217337e-32 8.3253419e-31], sampled 0.5729604887108788
[2019-03-27 03:50:43,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:50:43,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.36550935, 78.54677682, 1.0, 2.0, 0.7609857793522551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1063541.987327651, 1063541.987327651, 234561.7882525972]
[2019-03-27 03:50:43,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:50:43,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4014891e-31 1.0000000e+00 0.0000000e+00 5.5776683e-34 5.9153778e-34], sampled 0.42028407858179906
[2019-03-27 03:51:15,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:51:15,798] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.28333333333334, 52.0, 1.0, 2.0, 0.6528183292200038, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005972329326546, 6.9112, 168.9123308821149, 1809106.993043567, 1741872.489834934, 374765.3096104144]
[2019-03-27 03:51:15,800] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:51:15,805] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.6045740e-18 1.0000000e+00 1.9275986e-26 2.2238643e-14 3.6793970e-09], sampled 0.6475606771594459
[2019-03-27 03:51:15,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1809106.993043567 W.
[2019-03-27 03:51:23,217] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:51:23,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.83333333333333, 75.16666666666667, 1.0, 2.0, 0.5684669703850409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794380.0435936651, 794380.0435936657, 195017.7795828661]
[2019-03-27 03:51:23,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:51:23,222] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.975578e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.4641760999311322
[2019-03-27 03:51:35,462] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17067002]
[2019-03-27 03:51:35,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.8, 69.66666666666667, 1.0, 2.0, 0.5661203976037767, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9831639117614005, 6.9112, 6.9112, 168.9126555465066, 1582789.053702194, 1582789.053702194, 346355.0432902197]
[2019-03-27 03:51:35,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:51:35,468] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3221365e-19 1.0000000e+00 1.6832175e-28 1.4617325e-17 3.4580804e-14], sampled 0.9782596490208006
[2019-03-27 03:51:45,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8669.2543 2778933704.7839 914.0000
[2019-03-27 03:51:45,797] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8270.2354 2926527047.4690 1303.0000
[2019-03-27 03:51:45,824] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8102.2629 3002322142.7896 1491.0000
[2019-03-27 03:51:45,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8535.8389 2840105188.9486 1053.0000
[2019-03-27 03:51:45,829] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7961.8713 3159864378.9966 1694.0000
[2019-03-27 03:51:46,848] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2050000, evaluation results [2050000.0, 7961.871345026099, 3159864378.9965568, 1694.0, 8270.235445644648, 2926527047.468995, 1303.0, 8669.254340884718, 2778933704.7839484, 914.0, 8102.26292209581, 3002322142.789592, 1491.0, 8535.838861111939, 2840105188.9486327, 1053.0]
[2019-03-27 03:51:52,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2170689e-30 1.0000000e+00 0.0000000e+00 1.9829196e-30 2.8529887e-29], sum to 1.0000
[2019-03-27 03:51:52,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1744
[2019-03-27 03:51:52,136] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 86.66666666666667, 1.0, 2.0, 0.5211365723269962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728217.5156115143, 728217.5156115143, 186973.5565798065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5697600.0000, 
sim time next is 5698200.0000, 
raw observation next is [26.66666666666667, 86.83333333333333, 1.0, 2.0, 0.5193289384998734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725690.7302534821, 725690.7302534827, 186679.516168343], 
processed observation next is [0.0, 0.9565217391304348, 0.4628751974723541, 0.8683333333333333, 1.0, 1.0, 0.42087823915647393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20158075840374504, 0.2015807584037452, 0.2786261435348403], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.76484436], dtype=float32), -1.4819552]. 
=============================================
[2019-03-27 03:51:52,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7506922e-18 1.0000000e+00 2.2926803e-26 2.4185846e-15 9.1984341e-13], sum to 1.0000
[2019-03-27 03:51:52,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3514
[2019-03-27 03:51:52,832] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.88333333333333, 85.33333333333334, 1.0, 2.0, 0.8302677065363072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1160422.247817472, 1160422.247817473, 251477.0291355819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5555400.0000, 
sim time next is 5556000.0000, 
raw observation next is [28.06666666666667, 84.66666666666667, 1.0, 2.0, 0.9078035022282398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1268854.773628794, 1268854.773628793, 272094.4613652963], 
processed observation next is [1.0, 0.30434782608695654, 0.529225908372828, 0.8466666666666667, 1.0, 1.0, 0.8889198822026986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3524596593413316, 0.3524596593413314, 0.40611113636611385], 
reward next is 0.5939, 
noisyNet noise sample is [array([1.5879272], dtype=float32), -0.15124713]. 
=============================================
[2019-03-27 03:51:52,840] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[40.839783]
 [40.647057]
 [40.503597]
 [40.796646]
 [40.900436]], R is [[41.19303513]
 [41.40576553]
 [41.61396027]
 [41.83069229]
 [42.0479126 ]].
[2019-03-27 03:51:56,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7148446e-15 3.8290506e-07 4.8526588e-18 1.2210214e-07 9.9999952e-01], sum to 1.0000
[2019-03-27 03:51:56,654] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9109
[2019-03-27 03:51:56,659] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.35, 61.5, 1.0, 2.0, 0.7183532081549838, 1.0, 2.0, 0.6797666435917544, 1.0, 2.0, 1.03, 7.005099179333985, 6.9112, 170.5573041426782, 2852336.926730822, 2785073.124999481, 527190.5570327281], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5484600.0000, 
sim time next is 5485200.0000, 
raw observation next is [35.46666666666667, 60.33333333333333, 1.0, 2.0, 0.8243785359892873, 1.0, 2.0, 0.7327793075089063, 1.0, 2.0, 1.03, 7.005107541338055, 6.9112, 170.5573041426782, 3075054.697555756, 3007784.905780756, 563493.4618396859], 
processed observation next is [1.0, 0.4782608695652174, 0.8799368088467615, 0.6033333333333333, 1.0, 1.0, 0.7884078746858884, 1.0, 1.0, 0.6780473584444654, 1.0, 1.0, 1.0365853658536586, 0.0093907541338055, 0.0, 0.8375144448122397, 0.8541818604321545, 0.8354958071613211, 0.8410350176711731], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9995757], dtype=float32), 0.4440629]. 
=============================================
[2019-03-27 03:51:59,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6892241e-25 1.0000000e+00 1.5547654e-33 2.3874583e-27 4.6141176e-24], sum to 1.0000
[2019-03-27 03:51:59,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9267
[2019-03-27 03:51:59,798] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.5872359064941861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820618.0285986264, 820618.0285986264, 198393.2900724953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5422800.0000, 
sim time next is 5423400.0000, 
raw observation next is [30.85, 77.0, 1.0, 2.0, 0.5898663324565494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824295.2774574131, 824295.2774574131, 198874.5015047146], 
processed observation next is [1.0, 0.782608695652174, 0.661137440758294, 0.77, 1.0, 1.0, 0.5058630511524691, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.228970910404837, 0.228970910404837, 0.2968276141861412], 
reward next is 0.7032, 
noisyNet noise sample is [array([-1.5392554], dtype=float32), -0.007887513]. 
=============================================
[2019-03-27 03:52:04,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3008340e-13 1.5110010e-06 2.8596655e-20 3.6848078e-08 9.9999845e-01], sum to 1.0000
[2019-03-27 03:52:04,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0639
[2019-03-27 03:52:04,492] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.6635936870087659, 1.0, 2.0, 0.6523868830186454, 1.0, 2.0, 1.03, 7.005094861727689, 6.9112, 170.5573041426782, 2737324.126209341, 2670063.41735502, 510077.3388945533], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5476800.0000, 
sim time next is 5477400.0000, 
raw observation next is [33.0, 70.0, 1.0, 2.0, 0.7039717122669862, 1.0, 2.0, 0.6725758956477558, 1.0, 2.0, 1.03, 7.00509804532388, 6.9112, 170.5573041426782, 2822130.083618509, 2754867.094224736, 522587.8422185939], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.7, 1.0, 1.0, 0.6433394123698628, 1.0, 1.0, 0.6055131272864527, 1.0, 1.0, 1.0365853658536586, 0.009389804532388002, 0.0, 0.8375144448122397, 0.7839250232273637, 0.7652408595068712, 0.7799818540576029], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06540307], dtype=float32), 0.29965764]. 
=============================================
[2019-03-27 03:52:06,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4271367e-26 1.0000000e+00 2.2907278e-36 5.9433582e-26 2.0381977e-24], sum to 1.0000
[2019-03-27 03:52:06,770] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3744
[2019-03-27 03:52:06,783] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 89.0, 1.0, 2.0, 0.5566925219880691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777920.3151722233, 777920.3151722233, 192955.2918158165], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5954400.0000, 
sim time next is 5955000.0000, 
raw observation next is [27.53333333333334, 89.5, 1.0, 2.0, 0.5570404966408042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778406.7519905453, 778406.7519905453, 193015.6653702966], 
processed observation next is [1.0, 0.9565217391304348, 0.5039494470774094, 0.895, 1.0, 1.0, 0.4663138513744629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21622409777515147, 0.21622409777515147, 0.28808308264223376], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.6720302], dtype=float32), -0.667077]. 
=============================================
[2019-03-27 03:52:06,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.912186]
 [65.72573 ]
 [65.65346 ]
 [65.43113 ]
 [65.29662 ]], R is [[65.98599243]
 [66.03813934]
 [66.08966827]
 [66.14044189]
 [66.19033051]].
[2019-03-27 03:52:11,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5424911e-26 1.0000000e+00 4.3358237e-37 2.3369084e-27 2.1236481e-23], sum to 1.0000
[2019-03-27 03:52:11,582] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9576
[2019-03-27 03:52:11,586] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 82.0, 1.0, 2.0, 0.5388427572782306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752968.2713559001, 752968.2713559007, 189906.0592542545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6032400.0000, 
sim time next is 6033000.0000, 
raw observation next is [28.06666666666667, 83.0, 1.0, 2.0, 0.5385921739529465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752617.9870947594, 752617.9870947587, 189863.9064238334], 
processed observation next is [1.0, 0.8260869565217391, 0.529225908372828, 0.83, 1.0, 1.0, 0.44408695656981495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20906055197076648, 0.20906055197076628, 0.2833789648116916], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.0744319], dtype=float32), 0.3550248]. 
=============================================
[2019-03-27 03:52:11,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.22493]
 [73.06688]
 [73.2222 ]
 [73.28339]
 [73.60492]], R is [[73.01043701]
 [72.99689484]
 [72.98327637]
 [72.96961212]
 [72.95615387]].
[2019-03-27 03:52:19,290] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4653206e-28 1.0000000e+00 2.8391020e-37 2.2120562e-30 4.3335537e-24], sum to 1.0000
[2019-03-27 03:52:19,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2377
[2019-03-27 03:52:19,303] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.6, 67.0, 1.0, 2.0, 0.5249808476260251, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564928166, 733591.2226728152, 733591.2226728145, 187605.9966903811], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5851200.0000, 
sim time next is 5851800.0000, 
raw observation next is [31.4, 68.0, 1.0, 2.0, 0.5145617172528325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104266, 719026.9399688101, 719026.9399688095, 185913.1240680042], 
processed observation next is [1.0, 0.7391304347826086, 0.6872037914691943, 0.68, 1.0, 1.0, 0.41513459909979816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439945152281, 0.19972970554689168, 0.1997297055468915, 0.27748227472836445], 
reward next is 0.7225, 
noisyNet noise sample is [array([-0.6217113], dtype=float32), -1.7950376]. 
=============================================
[2019-03-27 03:52:20,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4933039e-27 1.0000000e+00 6.4130673e-37 7.0369824e-30 3.7976235e-26], sum to 1.0000
[2019-03-27 03:52:20,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8826
[2019-03-27 03:52:20,133] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 92.83333333333333, 1.0, 2.0, 0.5275328617399183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737158.5618755145, 737158.5618755138, 188022.3940397492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6054600.0000, 
sim time next is 6055200.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.5268298874518801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736175.90634333, 736175.90634333, 187906.6062386272], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 0.4299155270504579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2044933073175917, 0.2044933073175917, 0.2804576212516824], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.7251504], dtype=float32), -1.136928]. 
=============================================
[2019-03-27 03:52:20,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:52:20,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4431
[2019-03-27 03:52:20,176] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 88.0, 1.0, 2.0, 0.5112322650033273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714372.9387784209, 714372.9387784209, 185374.4662645817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5722800.0000, 
sim time next is 5723400.0000, 
raw observation next is [26.53333333333333, 87.0, 1.0, 2.0, 0.5118142065993034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715186.3914972004, 715186.3914972009, 185467.6518458023], 
processed observation next is [0.0, 0.21739130434782608, 0.45655608214849913, 0.87, 1.0, 1.0, 0.41182434530036555, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1986628865270001, 0.19866288652700026, 0.2768173908146303], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.7153971], dtype=float32), -0.8909125]. 
=============================================
[2019-03-27 03:52:23,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8870420e-27 1.0000000e+00 5.4313101e-38 1.3034656e-29 3.0355720e-25], sum to 1.0000
[2019-03-27 03:52:23,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8040
[2019-03-27 03:52:23,773] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 86.66666666666667, 1.0, 2.0, 0.5397328615130403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754212.5270855926, 754212.527085592, 190055.2146745392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5784000.0000, 
sim time next is 5784600.0000, 
raw observation next is [27.25, 86.83333333333333, 1.0, 2.0, 0.5387466443216276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752833.9174064758, 752833.917406475, 189889.2241655562], 
processed observation next is [0.0, 0.9565217391304348, 0.490521327014218, 0.8683333333333333, 1.0, 1.0, 0.44427306544774403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20912053261290994, 0.20912053261290975, 0.28341675248590475], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.1252208], dtype=float32), 2.066637]. 
=============================================
[2019-03-27 03:52:23,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5581592e-21 1.0000000e+00 6.9999748e-32 8.4206492e-24 9.0891960e-19], sum to 1.0000
[2019-03-27 03:52:23,959] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3193
[2019-03-27 03:52:23,962] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 95.0, 1.0, 2.0, 0.9649197290024352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348737.95474605, 1348737.95474605, 288418.6792180605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5887200.0000, 
sim time next is 5887800.0000, 
raw observation next is [25.8, 95.0, 1.0, 2.0, 0.9620627942517301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1344742.081665313, 1344742.081665313, 287578.9995235306], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.95, 1.0, 1.0, 0.9542925231948556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3735394671292536, 0.3735394671292536, 0.4292223873485531], 
reward next is 0.5708, 
noisyNet noise sample is [array([1.2021836], dtype=float32), 1.5507555]. 
=============================================
[2019-03-27 03:52:24,645] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1861721e-21 1.0000000e+00 1.0200578e-30 5.6745983e-23 1.9539623e-18], sum to 1.0000
[2019-03-27 03:52:24,658] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6547
[2019-03-27 03:52:24,663] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 87.66666666666666, 1.0, 2.0, 0.7530764573281162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1052482.562953504, 1052482.562953503, 232720.0655230329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5899200.0000, 
sim time next is 5899800.0000, 
raw observation next is [27.8, 86.83333333333333, 1.0, 2.0, 0.7572846375360189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1058366.75090455, 1058366.75090455, 233697.3669026125], 
processed observation next is [1.0, 0.2608695652173913, 0.5165876777251186, 0.8683333333333333, 1.0, 1.0, 0.7075718524530349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2939907641401528, 0.2939907641401528, 0.34880204015315297], 
reward next is 0.6512, 
noisyNet noise sample is [array([0.80345255], dtype=float32), 0.14042078]. 
=============================================
[2019-03-27 03:52:25,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2579370e-23 1.0000000e+00 8.0841024e-33 1.1718016e-23 2.5971961e-17], sum to 1.0000
[2019-03-27 03:52:25,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2992
[2019-03-27 03:52:25,151] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 82.83333333333334, 1.0, 2.0, 0.5585933127316969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780577.4499351569, 780577.4499351569, 193285.6528329755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [28.5, 83.0, 1.0, 2.0, 0.5555106039364517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776268.1032273677, 776268.1032273677, 192750.558847794], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.83, 1.0, 1.0, 0.4644706071523514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2156300286742688, 0.2156300286742688, 0.2876874012653642], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.35539111], dtype=float32), -1.6247058]. 
=============================================
[2019-03-27 03:52:25,218] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2807690e-22 1.0000000e+00 7.2247994e-31 2.6419262e-22 1.8543700e-17], sum to 1.0000
[2019-03-27 03:52:25,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5954
[2019-03-27 03:52:25,235] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 91.0, 1.0, 2.0, 0.535181319027493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747850.04845555, 747850.0484555494, 189291.90504616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5965200.0000, 
sim time next is 5965800.0000, 
raw observation next is [26.65, 91.00000000000001, 1.0, 2.0, 0.5334273627281741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745398.2503657328, 745398.2503657328, 188999.2466940823], 
processed observation next is [1.0, 0.043478260869565216, 0.462085308056872, 0.9100000000000001, 1.0, 1.0, 0.43786429244358327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2070550695460369, 0.2070550695460369, 0.28208842790161537], 
reward next is 0.7179, 
noisyNet noise sample is [array([-0.33978477], dtype=float32), 0.76483285]. 
=============================================
[2019-03-27 03:52:26,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0266847e-20 1.0000000e+00 1.0700279e-29 4.1882972e-20 5.3761553e-13], sum to 1.0000
[2019-03-27 03:52:26,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4106
[2019-03-27 03:52:26,520] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 91.5, 1.0, 2.0, 0.5332431872087167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745140.7977649419, 745140.7977649425, 188968.4990649917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6049800.0000, 
sim time next is 6050400.0000, 
raw observation next is [26.5, 91.66666666666666, 1.0, 2.0, 0.5318134151143034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743142.1708332984, 743142.1708332978, 188730.7215990936], 
processed observation next is [1.0, 0.0, 0.4549763033175356, 0.9166666666666665, 1.0, 1.0, 0.4359197772461486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20642838078702733, 0.20642838078702716, 0.28168764417775166], 
reward next is 0.7183, 
noisyNet noise sample is [array([0.2150925], dtype=float32), -0.08117165]. 
=============================================
[2019-03-27 03:52:28,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4505560e-14 9.9926621e-01 1.4258103e-19 1.0265915e-10 7.3383120e-04], sum to 1.0000
[2019-03-27 03:52:28,281] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1346
[2019-03-27 03:52:28,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2138299.240159668 W.
[2019-03-27 03:52:28,300] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.26666666666667, 76.83333333333333, 1.0, 2.0, 0.8880423535971895, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.995569457992102, 6.9112, 168.9124550805469, 2138299.240159668, 2078444.820569972, 431154.2725665441], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6079800.0000, 
sim time next is 6080400.0000, 
raw observation next is [29.4, 76.0, 1.0, 2.0, 0.4653752042859549, 1.0, 1.0, 0.4653752042859549, 1.0, 2.0, 0.8070951780990002, 6.9112, 6.9112, 170.5573041426782, 1952002.513819487, 1952002.513819487, 391844.1453668672], 
processed observation next is [1.0, 0.391304347826087, 0.5924170616113744, 0.76, 1.0, 1.0, 0.3558737401035601, 1.0, 0.5, 0.3558737401035601, 1.0, 1.0, 0.7647502171939026, 0.0, 0.0, 0.8375144448122397, 0.5422229205054131, 0.5422229205054131, 0.5848420080102495], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84068286], dtype=float32), 0.66206276]. 
=============================================
[2019-03-27 03:52:42,391] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 03:52:42,397] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:52:42,397] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:52:42,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:52:42,401] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:52:42,402] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:52:42,402] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:52:42,403] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:52:42,405] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:52:42,406] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:52:42,404] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:52:42,426] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-27 03:52:42,451] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-27 03:52:42,476] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-27 03:52:42,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-27 03:52:42,477] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-27 03:52:48,379] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16743939]
[2019-03-27 03:52:48,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.47347504, 83.06950846000001, 1.0, 2.0, 0.3028108343764901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485856.9215286474, 485856.9215286468, 166076.6265223095]
[2019-03-27 03:52:48,382] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:52:48,387] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0055376e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.29216914937839333
[2019-03-27 03:53:11,653] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16743939]
[2019-03-27 03:53:11,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.51666666666667, 93.33333333333334, 1.0, 2.0, 0.4608808574322754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677225.1522928596, 677225.1522928596, 181884.2629197574]
[2019-03-27 03:53:11,656] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:53:11,659] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5412763906868794
[2019-03-27 03:53:23,904] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16743939]
[2019-03-27 03:53:23,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.26666666666667, 91.0, 1.0, 2.0, 0.5679014227664063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793589.4478426371, 793589.4478426378, 194917.4864212174]
[2019-03-27 03:53:23,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:53:23,911] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.8506885e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7592251303760527
[2019-03-27 03:54:07,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16743939]
[2019-03-27 03:54:07,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.06666666666667, 78.66666666666667, 1.0, 2.0, 0.6003518665035181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 838953.835225372, 838953.8352253726, 200810.7536089746]
[2019-03-27 03:54:07,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:54:07,251] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0919524e-31 1.0000000e+00 0.0000000e+00 6.1843346e-34 2.2179780e-31], sampled 0.8869770514075498
[2019-03-27 03:54:35,388] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:54:35,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:54:35,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.8424 3007586462.8216 1770.0000
[2019-03-27 03:54:35,692] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.6116 2842392415.0444 1152.0000
[2019-03-27 03:54:35,733] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.9527 3164263972.8530 1843.0000
[2019-03-27 03:54:36,749] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2075000, evaluation results [2075000.0, 7876.952658257051, 3164263972.8530235, 1843.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 8000.842350725669, 3007586462.8216085, 1770.0, 8493.611572275498, 2842392415.0443797, 1152.0]
[2019-03-27 03:54:44,337] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5628985e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1797804e-38], sum to 1.0000
[2019-03-27 03:54:44,351] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3122
[2019-03-27 03:54:44,357] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.66666666666666, 1.0, 2.0, 0.5311033001837371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742149.5281373064, 742149.528137307, 188612.7404487609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6309600.0000, 
sim time next is 6310200.0000, 
raw observation next is [27.3, 85.83333333333334, 1.0, 2.0, 0.5321450761777206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743605.7875573632, 743605.7875573632, 188785.7250457925], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.8583333333333334, 1.0, 1.0, 0.4363193688888199, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20655716321037865, 0.20655716321037865, 0.28176973887431717], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.8657974], dtype=float32), 0.7602741]. 
=============================================
[2019-03-27 03:54:51,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3690851e-21 1.0000000e+00 7.1997942e-30 2.4920800e-20 1.8690917e-14], sum to 1.0000
[2019-03-27 03:54:51,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6393
[2019-03-27 03:54:51,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1884169.013657432 W.
[2019-03-27 03:54:51,131] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.28333333333333, 77.83333333333333, 1.0, 2.0, 0.4492172868487428, 1.0, 1.0, 0.4492172868487428, 1.0, 2.0, 0.7672264735406094, 6.911199999999999, 6.9112, 170.5573041426782, 1884169.013657432, 1884169.013657433, 379720.5293275596], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6511800.0000, 
sim time next is 6512400.0000, 
raw observation next is [28.5, 76.0, 1.0, 2.0, 0.6564371778578305, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.981200286305678, 6.9112, 168.9124855933514, 1814170.738760518, 1764510.264881537, 376377.5473359165], 
processed observation next is [1.0, 0.391304347826087, 0.5497630331753555, 0.76, 1.0, 1.0, 0.5860688889853379, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007000028630567812, 0.0, 0.8294376327338551, 0.5039363163223661, 0.4901417402448714, 0.5617575333371888], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9032242], dtype=float32), 1.3136259]. 
=============================================
[2019-03-27 03:54:51,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5159905e-30 1.0000000e+00 0.0000000e+00 3.6389286e-33 2.8411722e-29], sum to 1.0000
[2019-03-27 03:54:51,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3665
[2019-03-27 03:54:51,340] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 81.5, 1.0, 2.0, 0.7733937611344698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1080892.016997347, 1080892.016997347, 237483.7981963268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6420600.0000, 
sim time next is 6421200.0000, 
raw observation next is [27.76666666666667, 81.0, 1.0, 2.0, 0.7373392479066256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1030477.924957285, 1030477.924957284, 229110.326048141], 
processed observation next is [1.0, 0.30434782608695654, 0.515007898894155, 0.81, 1.0, 1.0, 0.6835412625381032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2862438680436903, 0.28624386804369, 0.34195571051961343], 
reward next is 0.6580, 
noisyNet noise sample is [array([-0.23858489], dtype=float32), 0.015624649]. 
=============================================
[2019-03-27 03:54:57,015] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3167711e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5476889e-35], sum to 1.0000
[2019-03-27 03:54:57,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8198
[2019-03-27 03:54:57,023] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 87.83333333333334, 1.0, 2.0, 0.5296403702742243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740104.556893132, 740104.5568931315, 188370.3616267271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6479400.0000, 
sim time next is 6480000.0000, 
raw observation next is [26.9, 88.0, 1.0, 2.0, 0.5286159030669341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738672.4974779445, 738672.4974779452, 188201.0023743313], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.88, 1.0, 1.0, 0.4320673530926916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20518680485498458, 0.20518680485498478, 0.28089701846915116], 
reward next is 0.7191, 
noisyNet noise sample is [array([-1.492469], dtype=float32), -1.3735355]. 
=============================================
[2019-03-27 03:54:57,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.932205]
 [70.93983 ]
 [70.968155]
 [71.00073 ]
 [71.014496]], R is [[65.4896698 ]
 [65.55361938]
 [65.61663055]
 [65.67869568]
 [65.74004364]].
[2019-03-27 03:54:57,846] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.3420517e-14 3.4712760e-03 1.4752378e-21 7.0956340e-12 9.9652869e-01], sum to 1.0000
[2019-03-27 03:54:57,852] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6354
[2019-03-27 03:54:57,858] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.13333333333333, 59.0, 1.0, 2.0, 0.4960210043614247, 1.0, 2.0, 0.4960210043614247, 1.0, 2.0, 0.8448654114065764, 6.9112, 6.9112, 170.5573041426782, 2080670.242433413, 2080670.242433413, 409405.894470403], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6535200.0000, 
sim time next is 6535800.0000, 
raw observation next is [31.0, 59.5, 1.0, 2.0, 0.4967761869680229, 1.0, 2.0, 0.4967761869680229, 1.0, 2.0, 0.8458285918837013, 6.9112, 6.9112, 170.5573041426782, 2083841.104663844, 2083841.104663844, 409854.9711433027], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.595, 1.0, 1.0, 0.3937062493590638, 1.0, 1.0, 0.3937062493590638, 1.0, 1.0, 0.8119860876630502, 0.0, 0.0, 0.8375144448122397, 0.5788447512955123, 0.5788447512955123, 0.6117238375273174], 
reward next is 0.3883, 
noisyNet noise sample is [array([-1.4807553], dtype=float32), 0.14595461]. 
=============================================
[2019-03-27 03:55:01,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5803228e-21 1.0000000e+00 5.2041227e-32 2.9824070e-24 1.3491635e-14], sum to 1.0000
[2019-03-27 03:55:01,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7068
[2019-03-27 03:55:01,277] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 85.0, 1.0, 2.0, 0.5151451390202629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719842.4652826025, 719842.465282603, 186003.2007643265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6637200.0000, 
sim time next is 6637800.0000, 
raw observation next is [27.03333333333333, 85.0, 1.0, 2.0, 0.514325485959629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718696.72860822, 718696.7286082207, 185871.1614102765], 
processed observation next is [1.0, 0.8260869565217391, 0.48025276461295413, 0.85, 1.0, 1.0, 0.4148499830838904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19963798016895, 0.1996379801689502, 0.27741964389593504], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.7172183], dtype=float32), -1.8013543]. 
=============================================
[2019-03-27 03:55:03,634] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.6704003e-23 1.0000000e+00 2.4159518e-34 1.8271610e-24 8.6741181e-14], sum to 1.0000
[2019-03-27 03:55:03,643] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1625
[2019-03-27 03:55:03,648] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.0, 1.0, 2.0, 0.5108819172072843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713883.214045964, 713883.2140459634, 185319.6803338395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6631200.0000, 
sim time next is 6631800.0000, 
raw observation next is [27.28333333333333, 85.00000000000001, 1.0, 2.0, 0.5161159084074503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721199.4386814024, 721199.4386814018, 186160.309053537], 
processed observation next is [1.0, 0.782608695652174, 0.49210110584518163, 0.8500000000000001, 1.0, 1.0, 0.41700711856319306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20033317741150067, 0.2003331774115005, 0.2778512075425925], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.7804215], dtype=float32), -1.1778407]. 
=============================================
[2019-03-27 03:55:09,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0085716e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2334567e-36], sum to 1.0000
[2019-03-27 03:55:09,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3159
[2019-03-27 03:55:09,816] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 71.5, 1.0, 2.0, 0.3928187320567826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587529.9346267583, 587529.9346267583, 173433.5319510599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6903000.0000, 
sim time next is 6903600.0000, 
raw observation next is [26.06666666666667, 72.33333333333333, 1.0, 2.0, 0.3956558540815243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590778.5889644243, 590778.5889644243, 173701.1333464653], 
processed observation next is [0.0, 0.9130434782608695, 0.4344391785150081, 0.7233333333333333, 1.0, 1.0, 0.2718745229897883, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16410516360122898, 0.16410516360122898, 0.25925542290517206], 
reward next is 0.7407, 
noisyNet noise sample is [array([1.0329772], dtype=float32), -0.16940102]. 
=============================================
[2019-03-27 03:55:11,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:55:11,674] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1855
[2019-03-27 03:55:11,680] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 79.33333333333333, 1.0, 2.0, 0.4248822142950228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 619379.4756686519, 619379.4756686526, 175958.7945009129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6936000.0000, 
sim time next is 6936600.0000, 
raw observation next is [25.88333333333334, 78.16666666666667, 1.0, 2.0, 0.4264226533309549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 620671.1091066434, 620671.1091066428, 176057.0423806288], 
processed observation next is [0.0, 0.2608695652173913, 0.4257503949447081, 0.7816666666666667, 1.0, 1.0, 0.30894295582042763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17240864141851203, 0.1724086414185119, 0.2627717050457146], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.5158084], dtype=float32), 0.9826137]. 
=============================================
[2019-03-27 03:55:17,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:55:17,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0064
[2019-03-27 03:55:17,608] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666666, 55.66666666666667, 1.0, 2.0, 0.4587798658030539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649351.7142775995, 649351.7142776002, 178460.079365465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6954000.0000, 
sim time next is 6954600.0000, 
raw observation next is [30.83333333333334, 54.83333333333333, 1.0, 2.0, 0.4578875567716384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648424.7410526436, 648424.7410526443, 178372.5749946614], 
processed observation next is [0.0, 0.4782608695652174, 0.6603475513428123, 0.5483333333333333, 1.0, 1.0, 0.34685247803811853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18011798362573433, 0.18011798362573453, 0.26622772387262894], 
reward next is 0.7338, 
noisyNet noise sample is [array([1.0150719], dtype=float32), -1.3364197]. 
=============================================
[2019-03-27 03:55:17,800] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:55:17,807] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0511
[2019-03-27 03:55:17,812] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 86.33333333333334, 1.0, 2.0, 0.4229799826563206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618794.1464536366, 618794.1464536366, 175963.571477473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6919800.0000, 
sim time next is 6920400.0000, 
raw observation next is [24.5, 86.66666666666667, 1.0, 2.0, 0.4226153594569511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618355.9671132707, 618355.9671132707, 175924.0946681097], 
processed observation next is [0.0, 0.08695652173913043, 0.3601895734597157, 0.8666666666666667, 1.0, 1.0, 0.304355854767411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17176554642035297, 0.17176554642035297, 0.2625732756240443], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.7831366], dtype=float32), 0.58059263]. 
=============================================
[2019-03-27 03:55:17,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2141613e-21 1.0000000e+00 6.5220632e-30 1.3220943e-22 5.1371841e-15], sum to 1.0000
[2019-03-27 03:55:17,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6361
[2019-03-27 03:55:17,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1691430.123442938 W.
[2019-03-27 03:55:17,932] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.11666666666667, 85.66666666666667, 1.0, 2.0, 0.6049474015366939, 0.0, 2.0, 0.0, 1.0, 2.0, 1.013609813938838, 6.911200000000001, 6.9112, 168.9129564996472, 1691430.123442938, 1691430.123442937, 362499.7036408843], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7145400.0000, 
sim time next is 7146000.0000, 
raw observation next is [26.1, 86.0, 1.0, 2.0, 0.5359210461393868, 1.0, 1.0, 0.5359210461393868, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1498286.336459709, 1498286.336459709, 311780.5715826705], 
processed observation next is [1.0, 0.7391304347826086, 0.4360189573459717, 0.86, 1.0, 1.0, 0.4408687302884179, 1.0, 0.5, 0.4408687302884179, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41619064901658587, 0.41619064901658587, 0.46534413669055297], 
reward next is 0.5347, 
noisyNet noise sample is [array([2.6472015], dtype=float32), -0.6946423]. 
=============================================
[2019-03-27 03:55:17,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.4088  ]
 [69.37051 ]
 [69.55954 ]
 [69.742386]
 [69.44246 ]], R is [[69.52950287]
 [69.29315948]
 [69.05023956]
 [68.81030273]
 [68.12220001]].
[2019-03-27 03:55:26,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.349628e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:55:26,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3243
[2019-03-27 03:55:26,782] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 93.33333333333334, 1.0, 2.0, 0.3177128470433545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501348.4929683455, 501348.4929683455, 167104.9131622994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7431600.0000, 
sim time next is 7432200.0000, 
raw observation next is [21.2, 93.0, 1.0, 2.0, 0.3175842133169536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501227.6205230787, 501227.6205230793, 167097.5703452879], 
processed observation next is [0.0, 0.0, 0.20379146919431282, 0.93, 1.0, 1.0, 0.17781230520114888, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13922989458974408, 0.13922989458974425, 0.24939935872431032], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.68362707], dtype=float32), -0.7917382]. 
=============================================
[2019-03-27 03:55:30,686] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 03:55:30,687] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:55:30,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:55:30,689] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:55:30,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:55:30,690] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:55:30,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:55:30,691] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:55:30,692] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:55:30,693] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:55:30,695] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:55:30,725] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-27 03:55:30,750] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-27 03:55:30,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-27 03:55:30,802] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-27 03:55:30,826] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-27 03:55:55,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:55:55,511] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.6, 93.0, 1.0, 2.0, 0.456321370744134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669361.1337237853, 669361.1337237853, 181046.9993763657]
[2019-03-27 03:55:55,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:55:55,515] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.4129903e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.5178886299518836
[2019-03-27 03:56:02,789] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:56:02,790] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.03333333333333, 90.66666666666667, 1.0, 2.0, 0.4836329369629926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676854.2710601749, 676854.2710601743, 181207.5337946926]
[2019-03-27 03:56:02,793] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:56:02,795] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04216889217242603
[2019-03-27 03:56:14,647] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:56:14,648] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.50004367, 97.13489143, 1.0, 2.0, 0.2717077119414961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440513.9550522343, 440513.9550522343, 162963.1743227964]
[2019-03-27 03:56:14,649] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:56:14,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.886446263076737
[2019-03-27 03:56:47,115] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:56:47,116] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.58971451666667, 59.76333297833333, 1.0, 2.0, 0.6345705487223509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886792.246261382, 886792.246261382, 207362.6352578805]
[2019-03-27 03:56:47,117] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:56:47,119] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.944952e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.7326017135993607
[2019-03-27 03:56:54,532] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:56:54,535] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.69414231, 87.41344083666667, 1.0, 2.0, 0.5278108774063409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737547.1874819302, 737547.1874819297, 188067.4261013435]
[2019-03-27 03:56:54,536] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:56:54,538] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9219682e-32 1.0000000e+00 0.0000000e+00 4.0288272e-36 2.9816214e-32], sampled 0.16002591493500806
[2019-03-27 03:56:56,359] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:56:56,361] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 85.0, 1.0, 2.0, 0.5935406849853478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829431.9225854287, 829431.922585428, 199548.0269547565]
[2019-03-27 03:56:56,362] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:56:56,364] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9899659e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6031625e-38], sampled 0.0900477313674618
[2019-03-27 03:57:00,539] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:57:00,541] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.6, 67.5, 1.0, 2.0, 0.7382989382998202, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988408202077637, 6.9112, 168.9124339957317, 1928723.919259508, 1873949.932842563, 393826.6030021757]
[2019-03-27 03:57:00,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:57:00,548] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2818788e-27 1.0000000e+00 5.0022050e-37 4.3326480e-30 1.8440432e-24], sampled 0.3851590755115105
[2019-03-27 03:57:00,549] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1928723.919259508 W.
[2019-03-27 03:57:00,872] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:57:00,873] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.35, 68.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.267736219345004, 6.9112, 168.9105861761808, 2549557.489427427, 2296621.973856491, 475946.391139915]
[2019-03-27 03:57:00,874] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:57:00,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7743740e-17 9.9999464e-01 8.1603963e-27 5.8367458e-16 5.3628728e-06], sampled 0.6149630354797434
[2019-03-27 03:57:00,878] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2549557.489427427 W.
[2019-03-27 03:57:20,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16494213]
[2019-03-27 03:57:20,599] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.12362202333333, 90.90022125333334, 1.0, 2.0, 0.3478725292494426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539682.1924857161, 539682.1924857161, 169896.8779105834]
[2019-03-27 03:57:20,600] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:57:20,603] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6309185435876237
[2019-03-27 03:57:26,400] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6384 2842453144.6515 1135.0000
[2019-03-27 03:57:26,409] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 03:57:26,567] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.4060 3164006223.2629 1789.0000
[2019-03-27 03:57:26,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 03:57:26,728] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.1253 3007569718.1601 1770.0000
[2019-03-27 03:57:27,748] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2100000, evaluation results [2100000.0, 7882.406039141218, 3164006223.2629175, 1789.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.125334496884, 3007569718.1601396, 1770.0, 8496.638377318941, 2842453144.651489, 1135.0]
[2019-03-27 03:57:31,582] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4164303e-29 1.0000000e+00 0.0000000e+00 2.7653955e-33 2.0706299e-28], sum to 1.0000
[2019-03-27 03:57:31,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5442
[2019-03-27 03:57:31,596] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 94.83333333333334, 1.0, 2.0, 0.3180759226206301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501596.6726288902, 501596.6726288902, 167116.5945697944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7427400.0000, 
sim time next is 7428000.0000, 
raw observation next is [21.03333333333333, 94.66666666666667, 1.0, 2.0, 0.3187621316488384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502701.3248446704, 502701.3248446711, 167200.2811768486], 
processed observation next is [1.0, 1.0, 0.19589257503949445, 0.9466666666666668, 1.0, 1.0, 0.1792314839142631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13963925690129733, 0.13963925690129753, 0.24955265847290836], 
reward next is 0.7504, 
noisyNet noise sample is [array([0.00343618], dtype=float32), -2.3742824]. 
=============================================
[2019-03-27 03:57:31,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.58493]
 [76.58263]
 [76.47101]
 [76.4583 ]
 [76.44708]], R is [[76.56164551]
 [76.54660034]
 [76.53185272]
 [76.51744843]
 [76.50334167]].
[2019-03-27 03:57:36,491] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3138349e-28 1.0000000e+00 0.0000000e+00 5.8929319e-31 9.5132693e-26], sum to 1.0000
[2019-03-27 03:57:36,501] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1162
[2019-03-27 03:57:36,510] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 76.33333333333333, 1.0, 2.0, 0.3702879380423064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563862.3652962283, 563862.3652962289, 171640.7311685271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7343400.0000, 
sim time next is 7344000.0000, 
raw observation next is [24.8, 76.0, 1.0, 2.0, 0.3686010774412307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 561849.5578281358, 561849.5578281365, 171483.4946955817], 
processed observation next is [1.0, 0.0, 0.3744075829383887, 0.76, 1.0, 1.0, 0.23927840655569965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15606932161892662, 0.1560693216189268, 0.2559455144710175], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.6039671], dtype=float32), -0.012412203]. 
=============================================
[2019-03-27 03:57:36,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.34002]
 [75.345  ]
 [75.36886]
 [75.39194]
 [75.38522]], R is [[71.55828094]
 [71.58651733]
 [71.61430359]
 [71.641716  ]
 [71.66872406]].
[2019-03-27 03:57:38,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:57:38,654] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0423
[2019-03-27 03:57:38,660] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4052288579085293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596593.7991905018, 596593.7991905018, 173983.2925343863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7521600.0000, 
sim time next is 7522200.0000, 
raw observation next is [23.5, 93.0, 1.0, 2.0, 0.4059162739980898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597606.4208987879, 597606.4208987873, 174076.9876780622], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 1.0, 1.0, 0.28423647469649377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16600178358299664, 0.16600178358299647, 0.2598163995194958], 
reward next is 0.7402, 
noisyNet noise sample is [array([-1.8568581], dtype=float32), 0.90173924]. 
=============================================
[2019-03-27 03:57:40,225] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:57:40,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2992
[2019-03-27 03:57:40,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 86.0, 1.0, 2.0, 0.4160183779387334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606298.3111822369, 606298.3111822363, 174703.335527657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7548000.0000, 
sim time next is 7548600.0000, 
raw observation next is [24.96666666666667, 85.0, 1.0, 2.0, 0.419905837964151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609759.0852984514, 609759.0852984514, 174965.8218824987], 
processed observation next is [0.0, 0.34782608695652173, 0.3823064770932071, 0.85, 1.0, 1.0, 0.30109137104114575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16937752369401426, 0.16937752369401426, 0.2611430177350727], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.46922392], dtype=float32), -0.9113318]. 
=============================================
[2019-03-27 03:57:40,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9198233e-23 1.0000000e+00 1.5506310e-33 1.0146040e-25 6.3823509e-19], sum to 1.0000
[2019-03-27 03:57:40,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4932
[2019-03-27 03:57:40,567] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 67.0, 1.0, 2.0, 0.9418648725348665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1430643.056450117, 1430643.056450116, 298180.3931317443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7297200.0000, 
sim time next is 7297800.0000, 
raw observation next is [26.48333333333333, 66.33333333333334, 1.0, 2.0, 0.8321700845592294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1263723.673949615, 1263723.673949614, 265646.1136713862], 
processed observation next is [1.0, 0.4782608695652174, 0.4541864139020536, 0.6633333333333334, 1.0, 1.0, 0.7977952826014811, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35103435387489307, 0.35103435387489274, 0.3964867368229645], 
reward next is 0.6035, 
noisyNet noise sample is [array([-0.32256502], dtype=float32), -0.75713104]. 
=============================================
[2019-03-27 03:57:48,405] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 03:57:48,415] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0252
[2019-03-27 03:57:48,424] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 90.0, 1.0, 2.0, 0.397737896282547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588892.3818033922, 588892.3818033928, 173374.3776645994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7545000.0000, 
sim time next is 7545600.0000, 
raw observation next is [23.8, 90.0, 1.0, 2.0, 0.3996088689809195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590598.5511635176, 590598.5511635176, 173498.3430802972], 
processed observation next is [0.0, 0.34782608695652173, 0.3270142180094788, 0.9, 1.0, 1.0, 0.27663719154327654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16405515310097712, 0.16405515310097712, 0.25895275086611524], 
reward next is 0.7410, 
noisyNet noise sample is [array([-0.11392783], dtype=float32), -1.6495266]. 
=============================================
[2019-03-27 03:57:48,971] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.737348e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 03:57:48,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2250
[2019-03-27 03:57:48,985] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.88333333333334, 85.66666666666667, 1.0, 2.0, 0.2853818550772286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461983.9437171385, 461983.9437171391, 164396.6209847062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7409400.0000, 
sim time next is 7410000.0000, 
raw observation next is [20.96666666666667, 85.33333333333334, 1.0, 2.0, 0.2874920918785873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465074.6192721507, 465074.6192721507, 164610.5589123887], 
processed observation next is [1.0, 0.782608695652174, 0.1927330173775673, 0.8533333333333334, 1.0, 1.0, 0.14155673720311723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12918739424226408, 0.12918739424226408, 0.24568740136177417], 
reward next is 0.7543, 
noisyNet noise sample is [array([1.0563061], dtype=float32), 0.5509331]. 
=============================================
[2019-03-27 03:57:49,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.28362]
 [73.26765]
 [73.14596]
 [73.12921]
 [73.02537]], R is [[73.30716705]
 [73.32872772]
 [73.35064697]
 [73.37238312]
 [73.39385223]].
[2019-03-27 03:57:51,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:57:51,981] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:52,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-27 03:57:53,739] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2112031: loss 0.2029
[2019-03-27 03:57:53,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2112031: learning rate 0.0001
[2019-03-27 03:57:55,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:57:55,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:57:55,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-27 03:57:56,857] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2113551: loss 0.7078
[2019-03-27 03:57:56,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2113551: learning rate 0.0001
[2019-03-27 03:57:57,601] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0100985e-30 1.0000000e+00 0.0000000e+00 3.7707294e-34 4.1100434e-29], sum to 1.0000
[2019-03-27 03:57:57,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8761
[2019-03-27 03:57:57,614] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 93.0, 1.0, 2.0, 0.4751733762026391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664124.5775233881, 664124.5775233888, 179817.2851133581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7602000.0000, 
sim time next is 7602600.0000, 
raw observation next is [24.66666666666667, 93.0, 1.0, 2.0, 0.4714976481910926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660994.0587389617, 660994.0587389617, 179530.0273143165], 
processed observation next is [0.0, 1.0, 0.36808846761453423, 0.93, 1.0, 1.0, 0.36325017854348507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18360946076082268, 0.18360946076082268, 0.2679552646482336], 
reward next is 0.7320, 
noisyNet noise sample is [array([0.77380943], dtype=float32), 0.9395047]. 
=============================================
[2019-03-27 03:57:59,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4692781e-29 1.0000000e+00 0.0000000e+00 2.9603147e-32 8.8832542e-26], sum to 1.0000
[2019-03-27 03:57:59,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8863
[2019-03-27 03:57:59,372] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 92.5, 1.0, 2.0, 0.4824780337637555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674180.3210257734, 674180.3210257734, 180896.1693598697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7599000.0000, 
sim time next is 7599600.0000, 
raw observation next is [25.0, 93.0, 1.0, 2.0, 0.4821006713018027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673652.8544986771, 673652.8544986764, 180839.0448999551], 
processed observation next is [0.0, 1.0, 0.38388625592417064, 0.93, 1.0, 1.0, 0.3760249051828948, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1871257929162992, 0.187125792916299, 0.26990902223873897], 
reward next is 0.7301, 
noisyNet noise sample is [array([1.679549], dtype=float32), -0.16227981]. 
=============================================
[2019-03-27 03:58:01,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:01,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:01,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-27 03:58:02,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:02,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:02,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-27 03:58:03,030] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2116464: loss 1.1804
[2019-03-27 03:58:03,032] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2116465: learning rate 0.0001
[2019-03-27 03:58:04,060] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2117011: loss 1.0985
[2019-03-27 03:58:04,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2117012: learning rate 0.0001
[2019-03-27 03:58:04,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:04,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:04,373] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-27 03:58:06,104] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2118050: loss 0.5925
[2019-03-27 03:58:06,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2118051: learning rate 0.0001
[2019-03-27 03:58:07,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:07,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:07,857] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-27 03:58:09,138] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2119537: loss 0.0057
[2019-03-27 03:58:09,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2119537: learning rate 0.0001
[2019-03-27 03:58:09,571] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2119728: loss 0.2448
[2019-03-27 03:58:09,574] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2119730: learning rate 0.0001
[2019-03-27 03:58:10,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:10,489] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:10,565] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-27 03:58:10,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:10,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:10,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-27 03:58:11,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:11,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:11,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-27 03:58:11,949] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2120945: loss 0.0008
[2019-03-27 03:58:11,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2120945: learning rate 0.0001
[2019-03-27 03:58:12,089] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2121027: loss 0.1194
[2019-03-27 03:58:12,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2121029: learning rate 0.0001
[2019-03-27 03:58:12,400] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2121210: loss 0.4010
[2019-03-27 03:58:12,401] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2121210: learning rate 0.0001
[2019-03-27 03:58:13,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:13,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:13,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-27 03:58:13,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7918178e-27 1.0000000e+00 2.2579927e-37 2.1332224e-29 7.8020074e-25], sum to 1.0000
[2019-03-27 03:58:13,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2170
[2019-03-27 03:58:13,254] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.98333333333333, 67.83333333333333, 1.0, 2.0, 0.8706346101013434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332474.388643566, 1332474.388643566, 277886.5639423959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 39000.0000, 
sim time next is 39600.0000, 
raw observation next is [26.2, 67.0, 1.0, 2.0, 0.7945696492078416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1213461.36399597, 1213461.36399597, 256228.012275726], 
processed observation next is [1.0, 0.4782608695652174, 0.44075829383886256, 0.67, 1.0, 1.0, 0.7524935532624597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33707260110999165, 0.33707260110999165, 0.3824298690682478], 
reward next is 0.6176, 
noisyNet noise sample is [array([0.5279783], dtype=float32), 0.64026755]. 
=============================================
[2019-03-27 03:58:13,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2121792: loss 0.6670
[2019-03-27 03:58:13,471] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2121792: learning rate 0.0001
[2019-03-27 03:58:14,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9522361e-28 1.0000000e+00 1.8175823e-38 6.3658227e-32 7.0902109e-27], sum to 1.0000
[2019-03-27 03:58:14,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8467
[2019-03-27 03:58:14,383] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 73.0, 1.0, 2.0, 0.3757446313070379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557857.076326464, 557857.076326464, 170641.8735560373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 63000.0000, 
sim time next is 63600.0000, 
raw observation next is [25.93333333333334, 73.66666666666667, 1.0, 2.0, 0.379093561136124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563968.1958391193, 563968.1958391193, 171219.268331546], 
processed observation next is [1.0, 0.7391304347826086, 0.42812006319115364, 0.7366666666666667, 1.0, 1.0, 0.25191995317605304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15665783217753312, 0.15665783217753312, 0.2555511467635015], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.8389981], dtype=float32), 0.86375916]. 
=============================================
[2019-03-27 03:58:14,733] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2122458: loss 0.5472
[2019-03-27 03:58:14,737] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2122459: learning rate 0.0001
[2019-03-27 03:58:15,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:15,382] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:15,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-27 03:58:16,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:16,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:16,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-27 03:58:16,842] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2123554: loss 0.0021
[2019-03-27 03:58:16,845] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2123555: learning rate 0.0001
[2019-03-27 03:58:16,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2123582: loss 0.0954
[2019-03-27 03:58:16,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2123583: learning rate 0.0001
[2019-03-27 03:58:16,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:16,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:16,979] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-27 03:58:17,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:17,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:17,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-27 03:58:17,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:17,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:17,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-27 03:58:17,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3643035e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4619028e-33], sum to 1.0000
[2019-03-27 03:58:17,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5409
[2019-03-27 03:58:17,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.46666666666667, 83.5, 1.0, 2.0, 0.2371244943561485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392546.6935223611, 392546.6935223617, 159545.2269239772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 510600.0000, 
sim time next is 511200.0000, 
raw observation next is [19.4, 84.0, 1.0, 2.0, 0.2367450751573401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 391936.0690190203, 391936.0690190203, 159508.0170305053], 
processed observation next is [1.0, 0.9565217391304348, 0.11848341232227487, 0.84, 1.0, 1.0, 0.08041575320161458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1088711302830612, 0.1088711302830612, 0.23807166720970938], 
reward next is 0.7619, 
noisyNet noise sample is [array([-0.8627387], dtype=float32), -0.4981497]. 
=============================================
[2019-03-27 03:58:17,702] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2124035: loss 0.0015
[2019-03-27 03:58:17,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2124036: learning rate 0.0001
[2019-03-27 03:58:17,736] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2124053: loss 0.0654
[2019-03-27 03:58:17,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2124055: learning rate 0.0001
[2019-03-27 03:58:17,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 03:58:17,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:17,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-27 03:58:18,434] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2124484: loss 0.0349
[2019-03-27 03:58:18,435] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2124485: learning rate 0.0001
[2019-03-27 03:58:18,840] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2124739: loss 0.0313
[2019-03-27 03:58:18,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2124740: learning rate 0.0001
[2019-03-27 03:58:19,091] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2124891: loss 0.1355
[2019-03-27 03:58:19,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2124892: learning rate 0.0001
[2019-03-27 03:58:19,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2124961: loss 0.0013
[2019-03-27 03:58:19,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2124961: learning rate 0.0001
[2019-03-27 03:58:19,335] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 03:58:19,336] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 03:58:19,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:19,339] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 03:58:19,341] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:19,342] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 03:58:19,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 03:58:19,343] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:19,343] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:19,344] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 03:58:19,347] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 03:58:19,376] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-27 03:58:19,398] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-27 03:58:19,421] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-27 03:58:19,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-27 03:58:19,465] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-27 03:58:42,778] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:58:42,780] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.4, 67.33333333333334, 1.0, 2.0, 0.3116620249930265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496353.203611621, 496353.2036116204, 166811.2961052397]
[2019-03-27 03:58:42,782] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:58:42,785] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3439017e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5074397e-38], sampled 0.26702957221983137
[2019-03-27 03:58:48,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:58:48,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.4859258, 92.57583480166666, 1.0, 2.0, 0.3436255715623083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539728.1475091679, 539728.1475091686, 170050.4110713282]
[2019-03-27 03:58:48,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:58:48,506] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9827976616498473
[2019-03-27 03:58:51,799] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:58:51,801] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.92362081, 77.1149704, 1.0, 2.0, 0.5252186733891446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733923.6674942028, 733923.6674942028, 187640.3112595582]
[2019-03-27 03:58:51,801] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 03:58:51,803] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5464138e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0523235e-35], sampled 0.6115396147882524
[2019-03-27 03:59:42,178] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:59:42,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.5, 78.33333333333334, 1.0, 2.0, 0.6449179007698282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901258.4783738443, 901258.4783738443, 209412.7989374808]
[2019-03-27 03:59:42,181] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 03:59:42,183] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9714601828227021
[2019-03-27 03:59:47,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:59:47,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.83333333333333, 91.0, 1.0, 2.0, 0.5408372192221654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755756.2847229472, 755756.2847229472, 190241.7570227987]
[2019-03-27 03:59:47,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 03:59:47,128] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1807898e-32 1.0000000e+00 0.0000000e+00 4.7072149e-35 1.7266064e-30], sampled 0.11974914820183358
[2019-03-27 03:59:48,187] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:59:48,188] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.25, 87.83333333333334, 1.0, 2.0, 0.6923113884531765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 967519.99606289, 967519.99606289, 219194.797811153]
[2019-03-27 03:59:48,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 03:59:48,193] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.623355104936401
[2019-03-27 03:59:55,162] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16027686]
[2019-03-27 03:59:55,164] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.35855582000001, 49.7768262, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.780329006315362, 6.9112, 168.9080291683942, 2072282.547976104, 1455710.504266891, 311593.93433156]
[2019-03-27 03:59:55,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 03:59:55,168] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1015013e-26 1.0000000e+00 5.5385414e-37 1.1078042e-28 6.9167955e-22], sampled 0.3164230241452102
[2019-03-27 03:59:55,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2072282.547976104 W.
[2019-03-27 04:00:14,498] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.1705 3007815300.0558 1764.0000
[2019-03-27 04:00:14,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:00:14,761] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.9433 3164080884.0684 1860.0000
[2019-03-27 04:00:14,816] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:00:14,818] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.7695 2842312088.1615 1157.0000
[2019-03-27 04:00:15,836] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2125000, evaluation results [2125000.0, 7877.943328177434, 3164080884.068411, 1860.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 8002.170453951162, 3007815300.0557814, 1764.0, 8494.769547608814, 2842312088.1615043, 1157.0]
[2019-03-27 04:00:16,163] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2125158: loss 0.0717
[2019-03-27 04:00:16,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2125159: learning rate 0.0001
[2019-03-27 04:00:17,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:17,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1713
[2019-03-27 04:00:17,690] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 88.0, 1.0, 2.0, 0.3124876121438484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494685.9445756486, 494685.9445756486, 166640.4764981013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 219600.0000, 
sim time next is 220200.0000, 
raw observation next is [21.78333333333333, 87.66666666666667, 1.0, 2.0, 0.3134003991873988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 495727.3285072395, 495727.3285072388, 166709.726636497], 
processed observation next is [0.0, 0.5652173913043478, 0.2314375987361769, 0.8766666666666667, 1.0, 1.0, 0.17277156528602267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13770203569645542, 0.13770203569645523, 0.2488204875171597], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.6612718], dtype=float32), -0.08417671]. 
=============================================
[2019-03-27 04:00:19,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2126543: loss 0.6597
[2019-03-27 04:00:19,153] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2126544: learning rate 0.0001
[2019-03-27 04:00:19,679] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2126788: loss 0.0006
[2019-03-27 04:00:19,680] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2126788: learning rate 0.0001
[2019-03-27 04:00:22,629] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2128176: loss 0.0945
[2019-03-27 04:00:22,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2128176: learning rate 0.0001
[2019-03-27 04:00:22,885] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2128293: loss 0.0005
[2019-03-27 04:00:22,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2128293: learning rate 0.0001
[2019-03-27 04:00:23,222] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2128448: loss 0.0043
[2019-03-27 04:00:23,224] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2128449: learning rate 0.0001
[2019-03-27 04:00:24,426] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2129200: loss 0.0024
[2019-03-27 04:00:24,430] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2129203: learning rate 0.0001
[2019-03-27 04:00:25,913] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2129866: loss 0.0056
[2019-03-27 04:00:25,915] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2129866: learning rate 0.0001
[2019-03-27 04:00:27,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5128076e-32 1.0000000e+00 0.0000000e+00 8.0688929e-38 4.9075322e-29], sum to 1.0000
[2019-03-27 04:00:27,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8339
[2019-03-27 04:00:27,936] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 78.5, 1.0, 2.0, 0.5127419440580625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821956.079211097, 821956.079211097, 197287.736454933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 402600.0000, 
sim time next is 403200.0000, 
raw observation next is [22.3, 79.0, 1.0, 2.0, 0.5194579589806202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832403.5516147703, 832403.5516147703, 198503.980414632], 
processed observation next is [1.0, 0.6956521739130435, 0.25592417061611383, 0.79, 1.0, 1.0, 0.4210336855188195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23122320878188066, 0.23122320878188066, 0.2962745976337791], 
reward next is 0.7037, 
noisyNet noise sample is [array([1.0355096], dtype=float32), 0.020942146]. 
=============================================
[2019-03-27 04:00:28,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:28,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6846
[2019-03-27 04:00:28,146] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.13333333333333, 86.0, 1.0, 2.0, 0.3189720881451341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502668.9189887386, 502668.9189887393, 167189.863854938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 224400.0000, 
sim time next is 225000.0000, 
raw observation next is [22.1, 86.0, 1.0, 2.0, 0.3176992795855395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501055.3737746762, 501055.3737746769, 167077.0104738689], 
processed observation next is [0.0, 0.6086956521739131, 0.24644549763033188, 0.86, 1.0, 1.0, 0.17795093925968614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13918204827074337, 0.13918204827074357, 0.24936867234905805], 
reward next is 0.7506, 
noisyNet noise sample is [array([1.3621567], dtype=float32), -1.2584785]. 
=============================================
[2019-03-27 04:00:28,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.59165 ]
 [75.599434]
 [75.604065]
 [75.540405]
 [75.565895]], R is [[75.5885849 ]
 [75.58316803]
 [75.57764435]
 [75.57200623]
 [75.56636047]].
[2019-03-27 04:00:28,876] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2131184: loss 0.0588
[2019-03-27 04:00:28,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2131184: learning rate 0.0001
[2019-03-27 04:00:29,015] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2131244: loss 0.0022
[2019-03-27 04:00:29,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2131245: learning rate 0.0001
[2019-03-27 04:00:29,065] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0514243e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9218747e-34], sum to 1.0000
[2019-03-27 04:00:29,076] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9321
[2019-03-27 04:00:29,082] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 80.0, 1.0, 2.0, 0.2510384003182333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 412765.0068877604, 412765.0068877604, 161009.5422260896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 417600.0000, 
sim time next is 418200.0000, 
raw observation next is [20.45, 80.33333333333334, 1.0, 2.0, 0.2496725839189731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 410709.9933251419, 410709.9933251413, 160872.2938672576], 
processed observation next is [1.0, 0.8695652173913043, 0.16824644549763035, 0.8033333333333335, 1.0, 1.0, 0.09599106496261818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11408610925698386, 0.1140861092569837, 0.24010790129441434], 
reward next is 0.7599, 
noisyNet noise sample is [array([-1.3927072], dtype=float32), -0.46586937]. 
=============================================
[2019-03-27 04:00:30,331] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2131829: loss 0.1906
[2019-03-27 04:00:30,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2131829: learning rate 0.0001
[2019-03-27 04:00:30,555] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2131927: loss 0.0009
[2019-03-27 04:00:30,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2131927: learning rate 0.0001
[2019-03-27 04:00:31,650] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2132412: loss 0.0063
[2019-03-27 04:00:31,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2132413: learning rate 0.0001
[2019-03-27 04:00:32,499] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2132793: loss 0.0025
[2019-03-27 04:00:32,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2132794: learning rate 0.0001
[2019-03-27 04:00:32,778] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2132915: loss 0.0037
[2019-03-27 04:00:32,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2132915: learning rate 0.0001
[2019-03-27 04:00:32,876] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2132960: loss 0.2529
[2019-03-27 04:00:32,879] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2132962: learning rate 0.0001
[2019-03-27 04:00:33,327] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2133159: loss 0.0036
[2019-03-27 04:00:33,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2133159: learning rate 0.0001
[2019-03-27 04:00:33,577] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1741280e-31 1.0000000e+00 0.0000000e+00 7.1804079e-36 1.2254833e-29], sum to 1.0000
[2019-03-27 04:00:33,586] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5924
[2019-03-27 04:00:33,591] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 54.0, 1.0, 2.0, 0.5626927310481771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924898.0743983721, 924898.0743983714, 207834.4372550697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 493200.0000, 
sim time next is 493800.0000, 
raw observation next is [24.36666666666667, 55.16666666666666, 1.0, 2.0, 0.3100019470312426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509637.4464753962, 509637.4464753969, 167449.2733693903], 
processed observation next is [1.0, 0.7391304347826086, 0.3538704581358612, 0.5516666666666665, 1.0, 1.0, 0.16867704461595495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14156595735427674, 0.14156595735427693, 0.24992428861103033], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.73504704], dtype=float32), -0.21018817]. 
=============================================
[2019-03-27 04:00:35,309] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1963713e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2327763e-33], sum to 1.0000
[2019-03-27 04:00:35,318] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7868
[2019-03-27 04:00:35,322] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 80.33333333333333, 1.0, 2.0, 0.2564244061805682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420498.6523125551, 420498.6523125558, 161549.4921758973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 416400.0000, 
sim time next is 417000.0000, 
raw observation next is [20.58333333333334, 80.16666666666667, 1.0, 2.0, 0.2540036223956345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417094.4037067279, 417094.4037067285, 161307.9435036959], 
processed observation next is [1.0, 0.8260869565217391, 0.17456556082148533, 0.8016666666666667, 1.0, 1.0, 0.1012091836091982, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1158595565852022, 0.11585955658520236, 0.24075812463238191], 
reward next is 0.7592, 
noisyNet noise sample is [array([-1.8357445], dtype=float32), 0.53450036]. 
=============================================
[2019-03-27 04:00:35,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[75.61399 ]
 [75.610214]
 [75.5872  ]
 [75.57572 ]
 [75.54728 ]], R is [[75.62502289]
 [75.62765503]
 [75.62999725]
 [75.63196564]
 [75.63341522]].
[2019-03-27 04:00:36,486] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2134569: loss 0.0048
[2019-03-27 04:00:36,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2134569: learning rate 0.0001
[2019-03-27 04:00:36,899] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2134751: loss 0.0376
[2019-03-27 04:00:36,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2134752: learning rate 0.0001
[2019-03-27 04:00:37,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:37,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9151
[2019-03-27 04:00:37,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 86.0, 1.0, 2.0, 0.2135148031529875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 355961.443750149, 355961.4437501484, 156949.4386614617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 626400.0000, 
sim time next is 627000.0000, 
raw observation next is [18.51666666666667, 85.0, 1.0, 2.0, 0.2146485999096978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357625.0040675116, 357625.0040675116, 157118.3436851489], 
processed observation next is [1.0, 0.2608695652173913, 0.07661927330173794, 0.85, 1.0, 1.0, 0.05379349386710579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.0993402789076421, 0.0993402789076421, 0.2345049905748491], 
reward next is 0.7655, 
noisyNet noise sample is [array([2.1029308], dtype=float32), 0.95209074]. 
=============================================
[2019-03-27 04:00:37,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.66324 ]
 [78.61906 ]
 [78.63991 ]
 [78.67052 ]
 [78.714874]], R is [[78.61772919]
 [78.59729767]
 [78.57746887]
 [78.55780792]
 [78.5375824 ]].
[2019-03-27 04:00:38,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:38,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5295
[2019-03-27 04:00:38,195] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.66666666666667, 93.0, 1.0, 2.0, 0.2334899960315701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388733.2498148087, 388733.2498148087, 158874.2273807432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [17.65, 93.0, 1.0, 2.0, 0.2227443206335127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370888.6993214728, 370888.6993214734, 157886.9921243822], 
processed observation next is [1.0, 0.08695652173913043, 0.035545023696682464, 0.93, 1.0, 1.0, 0.06354737425724422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10302463870040911, 0.10302463870040929, 0.23565222705131672], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.25076705], dtype=float32), 2.2659323]. 
=============================================
[2019-03-27 04:00:40,048] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2136145: loss 0.0183
[2019-03-27 04:00:40,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2136145: learning rate 0.0001
[2019-03-27 04:00:40,492] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2136344: loss 0.0386
[2019-03-27 04:00:40,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2136344: learning rate 0.0001
[2019-03-27 04:00:40,663] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2136419: loss 0.0314
[2019-03-27 04:00:40,665] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2136420: learning rate 0.0001
[2019-03-27 04:00:41,314] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:41,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8136
[2019-03-27 04:00:41,336] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 82.0, 1.0, 2.0, 0.2303264801330011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380402.9795722726, 380402.9795722726, 158978.7952441557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 453600.0000, 
sim time next is 454200.0000, 
raw observation next is [19.95, 81.83333333333334, 1.0, 2.0, 0.2303358724793361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 380295.462992765, 380295.4629927656, 158986.5359185053], 
processed observation next is [1.0, 0.2608695652173913, 0.14454976303317538, 0.8183333333333335, 1.0, 1.0, 0.07269382226426034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10563762860910139, 0.10563762860910156, 0.23729333719179896], 
reward next is 0.7627, 
noisyNet noise sample is [array([-1.0522637], dtype=float32), 1.6280019]. 
=============================================
[2019-03-27 04:00:42,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2137249: loss 0.1443
[2019-03-27 04:00:42,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2137249: learning rate 0.0001
[2019-03-27 04:00:43,767] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2137806: loss 0.0182
[2019-03-27 04:00:43,772] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2137807: learning rate 0.0001
[2019-03-27 04:00:46,881] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2139193: loss 0.0127
[2019-03-27 04:00:46,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2139194: learning rate 0.0001
[2019-03-27 04:00:47,016] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2139250: loss 0.0213
[2019-03-27 04:00:47,019] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2139251: learning rate 0.0001
[2019-03-27 04:00:47,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:47,996] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1422
[2019-03-27 04:00:48,000] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 74.83333333333334, 1.0, 2.0, 0.296635130069516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472871.0608527992, 472871.0608527992, 165123.0257339152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 906600.0000, 
sim time next is 907200.0000, 
raw observation next is [23.3, 74.0, 1.0, 2.0, 0.2965594300636533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472648.2819308657, 472648.2819308657, 165105.7264027745], 
processed observation next is [0.0, 0.5217391304347826, 0.3033175355450238, 0.74, 1.0, 1.0, 0.15248124104054617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13129118942524048, 0.13129118942524048, 0.2464264573175739], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.36327395], dtype=float32), 1.6337806]. 
=============================================
[2019-03-27 04:00:48,258] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8892634e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5088663e-36], sum to 1.0000
[2019-03-27 04:00:48,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4691
[2019-03-27 04:00:48,267] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2139807: loss 0.0090
[2019-03-27 04:00:48,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2139808: learning rate 0.0001
[2019-03-27 04:00:48,273] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [24.78333333333333, 54.5, 1.0, 2.0, 0.6412729187898063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051154.283686693, 1051154.283686693, 224749.5555619333], 
processed observation next is [1.0, 0.4782608695652174, 0.37361769352290675, 0.545, 1.0, 1.0, 0.567798697337116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919873010240814, 0.2919873010240814, 0.33544709785363175], 
reward next is 0.6646, 
noisyNet noise sample is [array([1.0626961], dtype=float32), -2.2928915]. 
=============================================
[2019-03-27 04:00:48,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.545555]
 [72.53044 ]
 [72.52459 ]
 [72.43061 ]
 [72.48723 ]], R is [[72.53026581]
 [72.47229767]
 [72.42103577]
 [72.3711319 ]
 [72.32215881]].
[2019-03-27 04:00:48,544] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2139925: loss 0.0830
[2019-03-27 04:00:48,546] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2139925: learning rate 0.0001
[2019-03-27 04:00:49,609] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2140404: loss 0.0687
[2019-03-27 04:00:49,613] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2140404: learning rate 0.0001
[2019-03-27 04:00:50,489] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2140799: loss 0.0335
[2019-03-27 04:00:50,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2140800: learning rate 0.0001
[2019-03-27 04:00:50,828] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2140950: loss 0.0357
[2019-03-27 04:00:50,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2140950: learning rate 0.0001
[2019-03-27 04:00:50,899] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2140980: loss 0.0007
[2019-03-27 04:00:50,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2140981: learning rate 0.0001
[2019-03-27 04:00:51,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:00:51,030] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4065
[2019-03-27 04:00:51,036] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 66.66666666666666, 1.0, 2.0, 0.3079135301212406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486547.4680176543, 486547.4680176543, 166023.0520876434], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 913800.0000, 
sim time next is 914400.0000, 
raw observation next is [25.0, 66.0, 1.0, 2.0, 0.3084907552966523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487224.2454706419, 487224.2454706419, 166067.1253174499], 
processed observation next is [0.0, 0.6086956521739131, 0.38388625592417064, 0.66, 1.0, 1.0, 0.16685633168271363, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1353400681862894, 0.1353400681862894, 0.24786138107082076], 
reward next is 0.7521, 
noisyNet noise sample is [array([0.32949576], dtype=float32), 1.9088212]. 
=============================================
[2019-03-27 04:00:51,401] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2141203: loss 0.0368
[2019-03-27 04:00:51,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2141203: learning rate 0.0001
[2019-03-27 04:00:54,415] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2142542: loss 3.8073
[2019-03-27 04:00:54,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2142542: learning rate 0.0001
[2019-03-27 04:00:54,915] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2142766: loss 0.0007
[2019-03-27 04:00:54,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2142768: learning rate 0.0001
[2019-03-27 04:00:58,154] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2144207: loss 4.1670
[2019-03-27 04:00:58,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2144208: learning rate 0.0001
[2019-03-27 04:00:58,272] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2144260: loss 0.0017
[2019-03-27 04:00:58,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2144260: learning rate 0.0001
[2019-03-27 04:00:58,709] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2144456: loss 0.0026
[2019-03-27 04:00:58,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2144457: learning rate 0.0001
[2019-03-27 04:01:00,398] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2145215: loss 0.0006
[2019-03-27 04:01:00,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2145215: learning rate 0.0001
[2019-03-27 04:01:01,785] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2145835: loss 0.0043
[2019-03-27 04:01:01,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2145835: learning rate 0.0001
[2019-03-27 04:01:04,770] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2147165: loss 0.0006
[2019-03-27 04:01:04,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2147166: learning rate 0.0001
[2019-03-27 04:01:04,908] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2147228: loss 2.3942
[2019-03-27 04:01:04,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2147229: learning rate 0.0001
[2019-03-27 04:01:05,239] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:01:05,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9593
[2019-03-27 04:01:05,258] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 92.16666666666667, 1.0, 2.0, 0.3322008264389752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516494.7689646671, 516494.7689646671, 168073.3060268565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 969000.0000, 
sim time next is 969600.0000, 
raw observation next is [21.9, 92.33333333333334, 1.0, 2.0, 0.3238541162463477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503245.2882189518, 503245.2882189525, 167039.1819228981], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9233333333333335, 1.0, 1.0, 0.18536640511608155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13979035783859772, 0.13979035783859792, 0.24931221182522106], 
reward next is 0.7507, 
noisyNet noise sample is [array([-1.1789296], dtype=float32), 0.13503]. 
=============================================
[2019-03-27 04:01:06,153] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2147785: loss 1.9779
[2019-03-27 04:01:06,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2147786: learning rate 0.0001
[2019-03-27 04:01:06,434] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2147912: loss 0.0018
[2019-03-27 04:01:06,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2147912: learning rate 0.0001
[2019-03-27 04:01:07,498] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2148387: loss 0.0008
[2019-03-27 04:01:07,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2148388: learning rate 0.0001
[2019-03-27 04:01:08,311] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2148751: loss 0.0007
[2019-03-27 04:01:08,313] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2148751: learning rate 0.0001
[2019-03-27 04:01:08,649] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2148900: loss 2.9584
[2019-03-27 04:01:08,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2148901: learning rate 0.0001
[2019-03-27 04:01:08,701] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2148926: loss 0.0030
[2019-03-27 04:01:08,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2148926: learning rate 0.0001
[2019-03-27 04:01:09,392] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2149210: loss 0.0019
[2019-03-27 04:01:09,395] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2149210: learning rate 0.0001
[2019-03-27 04:01:09,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1667079e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 04:01:09,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3465
[2019-03-27 04:01:09,874] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3388869456484538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524994.6937871921, 524994.6937871921, 168689.7937031262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 949800.0000, 
sim time next is 950400.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3406862791570941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 527781.4865893979, 527781.4865893973, 168912.608624645], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.94, 1.0, 1.0, 0.20564611946637842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14660596849705496, 0.14660596849705482, 0.2521083710815597], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.10516915], dtype=float32), 0.8070461]. 
=============================================
[2019-03-27 04:01:11,171] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-27 04:01:11,171] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:01:11,172] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:01:11,173] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:01:11,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:01:11,174] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:01:11,176] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:01:11,175] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:01:11,177] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:01:11,179] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:01:11,183] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:01:11,202] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-27 04:01:11,227] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-27 04:01:11,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-27 04:01:11,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-27 04:01:11,272] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-27 04:01:42,036] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:01:42,037] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.93410023333334, 79.41580645166667, 1.0, 2.0, 0.484033333488567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676354.2775536714, 676354.2775536714, 181132.0341734318]
[2019-03-27 04:01:42,038] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:01:42,044] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5438315798516873
[2019-03-27 04:01:45,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:01:45,499] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.388746075, 71.98427529, 1.0, 2.0, 0.9448888073571919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129087367018, 1338689.134213204, 1338689.134213204, 285313.3766880037]
[2019-03-27 04:01:45,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:01:45,504] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1418236e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4537469e-36], sampled 0.9787272337048228
[2019-03-27 04:01:49,147] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:01:49,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.561048545, 99.22385886333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.374157035989995, 6.9112, 168.9102055485127, 1782412.183909212, 1453979.877850571, 311353.1035293365]
[2019-03-27 04:01:49,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:01:49,152] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9660050886984839
[2019-03-27 04:01:49,153] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1782412.183909212 W.
[2019-03-27 04:01:51,333] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:01:51,335] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.09500538, 89.51832662166667, 1.0, 2.0, 0.3857012143155107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584879.9475028896, 584879.9475028896, 173425.42394234]
[2019-03-27 04:01:51,336] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:01:51,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8683513425891948
[2019-03-27 04:02:05,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:05,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.23333333333333, 52.33333333333333, 1.0, 2.0, 0.6627035353436327, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005972951062384, 6.9112, 168.9123227268644, 1822939.124835738, 1755704.183794106, 376766.5920724807]
[2019-03-27 04:02:05,657] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:02:05,660] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3902422e-32 1.0000000e+00 0.0000000e+00 1.0039501e-34 1.6288986e-29], sampled 0.43667027118164903
[2019-03-27 04:02:05,660] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1822939.124835738 W.
[2019-03-27 04:02:27,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:27,133] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.494091, 85.02866252000001, 1.0, 2.0, 0.5842992532132147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 816512.6990747814, 816512.6990747814, 197856.214179477]
[2019-03-27 04:02:27,134] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:02:27,136] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4996347e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.3542906135155417
[2019-03-27 04:02:28,360] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:28,361] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.785653965, 85.14843048, 1.0, 2.0, 0.6648360860862905, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.945917365844732, 6.9112, 168.9127561194202, 1848929.693847958, 1824300.02885779, 381740.6532804682]
[2019-03-27 04:02:28,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:02:28,364] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5826544e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3242444e-38], sampled 0.2712205039100126
[2019-03-27 04:02:28,365] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1848929.693847958 W.
[2019-03-27 04:02:30,908] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:30,910] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.99529635833333, 98.07067246499999, 1.0, 2.0, 0.8288335152782217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1158416.65940755, 1158416.659407549, 251114.8773065596]
[2019-03-27 04:02:30,911] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:02:30,914] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.696662413996803
[2019-03-27 04:02:32,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:32,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.06666666666666, 56.0, 1.0, 2.0, 0.632092506918192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 883327.8212416727, 883327.8212416727, 206875.7216936872]
[2019-03-27 04:02:32,597] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:02:32,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7589233e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5215203e-36], sampled 0.8987177198532246
[2019-03-27 04:02:33,278] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:33,279] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.96666666666667, 93.33333333333334, 1.0, 2.0, 0.6916743215923123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966629.276296676, 966629.2762966754, 219057.41474128]
[2019-03-27 04:02:33,281] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:02:33,284] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.99486076483857
[2019-03-27 04:02:39,796] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15835463]
[2019-03-27 04:02:39,797] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.31666666666666, 88.5, 1.0, 2.0, 0.521320295754019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728474.3321562267, 728474.3321562262, 187003.1208757628]
[2019-03-27 04:02:39,798] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:02:39,800] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3692851e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1342039e-37], sampled 0.7093145243206749
[2019-03-27 04:03:06,785] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:03:06,938] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:03:06,981] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.3294 3164009906.7813 1780.0000
[2019-03-27 04:03:07,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7842 2842404695.6908 1130.0000
[2019-03-27 04:03:07,267] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 04:03:08,288] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2150000, evaluation results [2150000.0, 7884.329371215033, 3164009906.7813215, 1780.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8497.784196207376, 2842404695.6907825, 1130.0]
[2019-03-27 04:03:09,660] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2150638: loss 0.0366
[2019-03-27 04:03:09,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2150638: learning rate 0.0001
[2019-03-27 04:03:09,842] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2150721: loss 3.1775
[2019-03-27 04:03:09,845] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2150721: learning rate 0.0001
[2019-03-27 04:03:10,914] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:10,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7305
[2019-03-27 04:03:10,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 96.33333333333334, 1.0, 2.0, 0.3560687070328912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547699.3542263196, 547699.3542263196, 170431.1093911203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1017600.0000, 
sim time next is 1018200.0000, 
raw observation next is [21.78333333333333, 96.16666666666666, 1.0, 2.0, 0.3549598945735055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546046.257277699, 546046.2572776984, 170294.8354384893], 
processed observation next is [1.0, 0.782608695652174, 0.2314375987361769, 0.9616666666666666, 1.0, 1.0, 0.22284324647410303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15167951591047193, 0.15167951591047177, 0.2541713961768497], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.11217573], dtype=float32), 0.61726177]. 
=============================================
[2019-03-27 04:03:13,008] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2152200: loss 1.7344
[2019-03-27 04:03:13,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2152201: learning rate 0.0001
[2019-03-27 04:03:13,203] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2152285: loss 0.0305
[2019-03-27 04:03:13,205] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2152285: learning rate 0.0001
[2019-03-27 04:03:13,482] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2152417: loss 1.8385
[2019-03-27 04:03:13,484] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2152419: learning rate 0.0001
[2019-03-27 04:03:13,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.143061e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 04:03:13,745] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8169
[2019-03-27 04:03:13,752] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 69.5, 1.0, 2.0, 0.6672565713197245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1028934.593009282, 1028934.593009282, 225863.8729400148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [25.6, 69.0, 1.0, 2.0, 0.7022262944596471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080421.196625199, 1080421.196625199, 233784.8565052438], 
processed observation next is [1.0, 0.6086956521739131, 0.4123222748815167, 0.69, 1.0, 1.0, 0.6412364993489724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30011699906255523, 0.30011699906255523, 0.34893262164961764], 
reward next is 0.6511, 
noisyNet noise sample is [array([-0.22330025], dtype=float32), 0.6996874]. 
=============================================
[2019-03-27 04:03:15,070] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2153151: loss 1.5827
[2019-03-27 04:03:15,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2153152: learning rate 0.0001
[2019-03-27 04:03:16,381] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2153782: loss 1.5468
[2019-03-27 04:03:16,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2153782: learning rate 0.0001
[2019-03-27 04:03:18,986] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2155134: loss 2.2850
[2019-03-27 04:03:18,989] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2155134: learning rate 0.0001
[2019-03-27 04:03:19,377] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2155308: loss 0.0409
[2019-03-27 04:03:19,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2155308: learning rate 0.0001
[2019-03-27 04:03:20,475] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2155800: loss 2.4075
[2019-03-27 04:03:20,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2155800: learning rate 0.0001
[2019-03-27 04:03:20,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2155893: loss 0.0376
[2019-03-27 04:03:20,693] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2155894: learning rate 0.0001
[2019-03-27 04:03:21,718] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2156351: loss 2.5806
[2019-03-27 04:03:21,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2156352: learning rate 0.0001
[2019-03-27 04:03:22,379] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2156643: loss 2.2212
[2019-03-27 04:03:22,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2156643: learning rate 0.0001
[2019-03-27 04:03:22,898] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2156877: loss 2.9481
[2019-03-27 04:03:22,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2156877: learning rate 0.0001
[2019-03-27 04:03:23,076] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2156953: loss 0.0078
[2019-03-27 04:03:23,079] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2156954: learning rate 0.0001
[2019-03-27 04:03:23,564] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2157170: loss 3.0831
[2019-03-27 04:03:23,567] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2157173: learning rate 0.0001
[2019-03-27 04:03:26,908] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2158664: loss 0.0080
[2019-03-27 04:03:26,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2158664: learning rate 0.0001
[2019-03-27 04:03:27,052] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2158722: loss 0.0008
[2019-03-27 04:03:27,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2158723: learning rate 0.0001
[2019-03-27 04:03:28,865] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.078137e-38 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-27 04:03:28,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5903
[2019-03-27 04:03:28,877] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.33333333333334, 1.0, 2.0, 0.611183759918883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970856.4212880665, 970856.4212880665, 216282.768564925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1782600.0000, 
sim time next is 1783200.0000, 
raw observation next is [21.0, 92.66666666666667, 1.0, 2.0, 0.5659994561674413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898306.8904581534, 898306.8904581541, 206835.93483588], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9266666666666667, 1.0, 1.0, 0.4771077785149895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2495296917939315, 0.2495296917939317, 0.30871035050131346], 
reward next is 0.6913, 
noisyNet noise sample is [array([1.278305], dtype=float32), 1.2405266]. 
=============================================
[2019-03-27 04:03:30,330] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2160184: loss 0.0160
[2019-03-27 04:03:30,332] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2160184: learning rate 0.0001
[2019-03-27 04:03:30,548] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2160280: loss 0.0071
[2019-03-27 04:03:30,550] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2160280: learning rate 0.0001
[2019-03-27 04:03:30,972] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2160472: loss 0.0121
[2019-03-27 04:03:30,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2160472: learning rate 0.0001
[2019-03-27 04:03:32,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2161163: loss 0.0226
[2019-03-27 04:03:32,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2161165: learning rate 0.0001
[2019-03-27 04:03:34,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2161827: loss 0.0154
[2019-03-27 04:03:34,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2161827: learning rate 0.0001
[2019-03-27 04:03:37,037] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2163181: loss 0.0065
[2019-03-27 04:03:37,042] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2163182: learning rate 0.0001
[2019-03-27 04:03:37,273] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2163283: loss 0.0029
[2019-03-27 04:03:37,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2163283: learning rate 0.0001
[2019-03-27 04:03:38,462] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2163812: loss 0.0053
[2019-03-27 04:03:38,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2163812: learning rate 0.0001
[2019-03-27 04:03:38,633] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2163890: loss 0.0018
[2019-03-27 04:03:38,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2163892: learning rate 0.0001
[2019-03-27 04:03:39,755] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2164388: loss 0.0079
[2019-03-27 04:03:39,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2164390: learning rate 0.0001
[2019-03-27 04:03:40,171] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.2041518e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6774051e-34], sum to 1.0000
[2019-03-27 04:03:40,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6998
[2019-03-27 04:03:40,185] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 94.0, 1.0, 2.0, 0.7547092638807567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1118050.769086616, 1118050.769086617, 241571.1496205619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1614600.0000, 
sim time next is 1615200.0000, 
raw observation next is [23.13333333333333, 94.66666666666667, 1.0, 2.0, 0.7187299363156185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1064111.398798067, 1064111.398798067, 232801.3717641809], 
processed observation next is [1.0, 0.6956521739130435, 0.29541864139020524, 0.9466666666666668, 1.0, 1.0, 0.6611204051995404, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2955864996661297, 0.2955864996661297, 0.3474647339763894], 
reward next is 0.6525, 
noisyNet noise sample is [array([-2.2430644], dtype=float32), 1.5416287]. 
=============================================
[2019-03-27 04:03:40,370] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2164663: loss 0.0232
[2019-03-27 04:03:40,371] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2164663: learning rate 0.0001
[2019-03-27 04:03:40,882] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2164893: loss 0.0211
[2019-03-27 04:03:40,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2164893: learning rate 0.0001
[2019-03-27 04:03:41,123] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2164999: loss 0.0020
[2019-03-27 04:03:41,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2164999: learning rate 0.0001
[2019-03-27 04:03:41,518] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2165170: loss 0.0370
[2019-03-27 04:03:41,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2165170: learning rate 0.0001
[2019-03-27 04:03:43,226] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:43,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5801
[2019-03-27 04:03:43,243] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 94.0, 1.0, 2.0, 0.5054078413888767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706231.4460809233, 706231.4460809233, 184448.0160963756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2019600.0000, 
sim time next is 2020200.0000, 
raw observation next is [25.58333333333334, 94.00000000000001, 1.0, 2.0, 0.5055580618583821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706441.4263932309, 706441.4263932309, 184471.7488103448], 
processed observation next is [0.0, 0.391304347826087, 0.4115323854660351, 0.9400000000000002, 1.0, 1.0, 0.40428682151612305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19623372955367524, 0.19623372955367524, 0.27533096837364895], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.04648044], dtype=float32), -1.2640331]. 
=============================================
[2019-03-27 04:03:43,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:43,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0622
[2019-03-27 04:03:43,872] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3261569680534232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511568.5270110629, 511568.5270110629, 167812.3356408702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1561200.0000, 
sim time next is 1561800.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3260160186877732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511347.4340524618, 511347.4340524618, 167795.3117008773], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18797110685273877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1420409539034616, 0.1420409539034616, 0.25044076373265267], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.14513159], dtype=float32), -0.49241847]. 
=============================================
[2019-03-27 04:03:44,958] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2166706: loss 0.0007
[2019-03-27 04:03:44,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2166706: learning rate 0.0001
[2019-03-27 04:03:45,218] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2166822: loss 0.0147
[2019-03-27 04:03:45,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2166822: learning rate 0.0001
[2019-03-27 04:03:48,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:48,209] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0317
[2019-03-27 04:03:48,214] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 93.83333333333334, 1.0, 2.0, 0.5569936859691708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792635.6746041526, 792635.674604152, 194838.0820007253], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1746600.0000, 
sim time next is 1747200.0000, 
raw observation next is [24.3, 93.66666666666667, 1.0, 2.0, 0.4933639726387568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701216.7262283522, 701216.7262283515, 184076.0281955756], 
processed observation next is [1.0, 0.21739130434782608, 0.3507109004739337, 0.9366666666666668, 1.0, 1.0, 0.38959514775753834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19478242395232004, 0.19478242395231984, 0.27474034059041136], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.9230023], dtype=float32), 0.50013334]. 
=============================================
[2019-03-27 04:03:48,356] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2168206: loss 0.0010
[2019-03-27 04:03:48,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2168208: learning rate 0.0001
[2019-03-27 04:03:48,683] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2168347: loss 0.0036
[2019-03-27 04:03:48,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2168348: learning rate 0.0001
[2019-03-27 04:03:48,935] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2168459: loss 0.0058
[2019-03-27 04:03:48,939] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2168459: learning rate 0.0001
[2019-03-27 04:03:50,547] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2169174: loss 0.0026
[2019-03-27 04:03:50,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2169176: learning rate 0.0001
[2019-03-27 04:03:51,051] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:03:51,058] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9659
[2019-03-27 04:03:51,063] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 94.0, 1.0, 2.0, 0.490176374327415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 696470.3907968022, 696470.3907968029, 183547.8172041914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1744200.0000, 
sim time next is 1744800.0000, 
raw observation next is [24.23333333333333, 94.0, 1.0, 2.0, 0.4880200951249754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 693905.3480997995, 693905.3480997989, 183274.3070114815], 
processed observation next is [1.0, 0.17391304347826086, 0.3475513428120062, 0.94, 1.0, 1.0, 0.3831567411144282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19275148558327762, 0.19275148558327748, 0.2735437418081813], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.63390875], dtype=float32), 0.6923082]. 
=============================================
[2019-03-27 04:03:52,015] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2169829: loss 0.0052
[2019-03-27 04:03:52,016] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2169829: learning rate 0.0001
[2019-03-27 04:03:54,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3523339e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9725154e-38], sum to 1.0000
[2019-03-27 04:03:54,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5161
[2019-03-27 04:03:54,214] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 96.0, 1.0, 2.0, 0.4667344213826579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657379.6927654248, 657379.6927654248, 179220.1032692424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1908000.0000, 
sim time next is 1908600.0000, 
raw observation next is [24.15, 95.83333333333333, 1.0, 2.0, 0.7425589817473437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1048103.116334434, 1048103.116334434, 231712.7912930236], 
processed observation next is [1.0, 0.08695652173913043, 0.34360189573459715, 0.9583333333333333, 1.0, 1.0, 0.6898300984907755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2911397545373428, 0.2911397545373428, 0.3458399870045128], 
reward next is 0.6542, 
noisyNet noise sample is [array([1.5305476], dtype=float32), 0.68578416]. 
=============================================
[2019-03-27 04:03:54,989] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2841835e-28 1.0000000e+00 0.0000000e+00 1.5655988e-31 3.6974049e-25], sum to 1.0000
[2019-03-27 04:03:55,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9279
[2019-03-27 04:03:55,006] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 87.33333333333334, 1.0, 2.0, 0.686824532390248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083132.043582678, 1083132.043582678, 232826.8951584674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1788000.0000, 
sim time next is 1788600.0000, 
raw observation next is [22.16666666666667, 85.66666666666666, 1.0, 2.0, 0.6861049299627314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1081520.073867193, 1081520.073867193, 232610.5293168236], 
processed observation next is [1.0, 0.6956521739130435, 0.24960505529225935, 0.8566666666666666, 1.0, 1.0, 0.6218131686297969, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3004222427408869, 0.3004222427408869, 0.3471798945027218], 
reward next is 0.6528, 
noisyNet noise sample is [array([1.0641074], dtype=float32), -1.2762735]. 
=============================================
[2019-03-27 04:03:55,010] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2171163: loss 0.0039
[2019-03-27 04:03:55,012] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2171163: learning rate 0.0001
[2019-03-27 04:03:55,393] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2171334: loss 0.0064
[2019-03-27 04:03:55,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2171335: learning rate 0.0001
[2019-03-27 04:03:56,299] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2171740: loss 0.0010
[2019-03-27 04:03:56,300] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2171740: learning rate 0.0001
[2019-03-27 04:03:56,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6068794e-20 1.0000000e+00 2.0690193e-28 4.0498624e-20 9.0855629e-10], sum to 1.0000
[2019-03-27 04:03:56,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0921
[2019-03-27 04:03:56,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1698449.779215432 W.
[2019-03-27 04:03:56,325] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.95, 63.5, 1.0, 2.0, 0.4049737916324324, 1.0, 2.0, 0.4049737916324324, 1.0, 2.0, 0.7033055491861195, 6.9112, 6.9112, 170.5573041426782, 1698449.779215432, 1698449.779215432, 355658.6754484336], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2291400.0000, 
sim time next is 2292000.0000, 
raw observation next is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.7050384503147125, 1.0, 2.0, 0.7050384503147125, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1971526.549389405, 1971526.549389405, 376291.5322644623], 
processed observation next is [1.0, 0.5217391304347826, 0.7124802527646128, 0.6366666666666667, 1.0, 1.0, 0.6446246389333885, 1.0, 1.0, 0.6446246389333885, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5476462637192792, 0.5476462637192792, 0.5616291526335259], 
reward next is 0.4384, 
noisyNet noise sample is [array([0.57385284], dtype=float32), -0.24119255]. 
=============================================
[2019-03-27 04:03:56,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.402466]
 [66.74969 ]
 [66.11457 ]
 [65.63072 ]
 [65.46544 ]], R is [[66.85092926]
 [66.65158844]
 [65.9850769 ]
 [65.32522583]
 [65.02144623]].
[2019-03-27 04:03:56,421] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2171792: loss 0.0055
[2019-03-27 04:03:56,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2171792: learning rate 0.0001
[2019-03-27 04:03:57,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2172345: loss 0.0075
[2019-03-27 04:03:57,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2172345: learning rate 0.0001
[2019-03-27 04:03:58,261] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2172612: loss 0.0080
[2019-03-27 04:03:58,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2172612: learning rate 0.0001
[2019-03-27 04:03:58,684] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2172797: loss 0.0066
[2019-03-27 04:03:58,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2172797: learning rate 0.0001
[2019-03-27 04:03:59,290] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2173066: loss 0.0142
[2019-03-27 04:03:59,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2173067: learning rate 0.0001
[2019-03-27 04:03:59,376] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2173103: loss 0.0019
[2019-03-27 04:03:59,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2173103: learning rate 0.0001
[2019-03-27 04:04:02,679] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2174577: loss 0.0099
[2019-03-27 04:04:02,684] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2174577: learning rate 0.0001
[2019-03-27 04:04:02,990] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:04:02,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0660
[2019-03-27 04:04:03,007] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 96.16666666666666, 1.0, 2.0, 0.6231774175942596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 870864.1788365295, 870864.1788365302, 205134.0757047103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [24.7, 96.33333333333333, 1.0, 2.0, 0.5916131441029758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826737.2696399585, 826737.2696399585, 199186.5375727023], 
processed observation next is [1.0, 0.17391304347826086, 0.3696682464454976, 0.9633333333333333, 1.0, 1.0, 0.5079676434975612, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22964924156665514, 0.22964924156665514, 0.2972933396607497], 
reward next is 0.7027, 
noisyNet noise sample is [array([-0.9000554], dtype=float32), 0.32370844]. 
=============================================
[2019-03-27 04:04:03,635] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 04:04:03,638] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:04:03,639] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:04:03,639] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:04:03,641] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:04:03,641] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:04:03,642] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:04:03,643] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:04:03,644] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:04:03,643] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:04:03,645] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:04:03,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-27 04:04:03,703] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-27 04:04:03,704] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-27 04:04:03,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-27 04:04:03,745] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-27 04:04:16,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:04:16,879] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.83333333333334, 62.83333333333333, 1.0, 2.0, 0.2893466080141956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463352.7591582451, 463352.7591582457, 164487.1777494199]
[2019-03-27 04:04:16,879] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:04:16,882] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2813619298827137
[2019-03-27 04:04:26,491] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:04:26,491] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.52829873333334, 89.75005805333333, 1.0, 2.0, 0.2912541092316352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469476.7559802331, 469476.7559802331, 164923.2355595961]
[2019-03-27 04:04:26,492] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:04:26,496] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2026662774414384
[2019-03-27 04:04:31,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:04:31,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.41817563333333, 93.5322913, 1.0, 2.0, 0.6590285770823913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 920986.3859525837, 920986.385952583, 212247.9295247142]
[2019-03-27 04:04:31,855] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:04:31,859] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7939225e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.35618743453254575
[2019-03-27 04:04:57,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:04:57,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.08333333333334, 53.0, 1.0, 2.0, 0.7559863049380554, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005978779477902, 6.9112, 168.912316023492, 1953476.345747323, 1886237.272511056, 397250.8282555407]
[2019-03-27 04:04:57,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:04:57,382] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6419326e-26 1.0000000e+00 1.2243768e-35 1.4737572e-27 4.9932687e-20], sampled 0.4534110816373995
[2019-03-27 04:04:57,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1953476.345747323 W.
[2019-03-27 04:05:15,240] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:05:15,241] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.46666666666667, 79.33333333333334, 1.0, 2.0, 0.6470674339065612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 904263.6821124855, 904263.6821124855, 209842.7567770381]
[2019-03-27 04:05:15,242] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:05:15,244] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1203439e-31 1.0000000e+00 0.0000000e+00 2.1931634e-34 3.6792631e-28], sampled 0.027926430447878237
[2019-03-27 04:05:45,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:05:45,352] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.63333333333333, 76.33333333333334, 1.0, 2.0, 0.5673794508785499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792859.76841729, 792859.76841729, 194825.5785559783]
[2019-03-27 04:05:45,353] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:05:45,355] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0872592174961806
[2019-03-27 04:05:51,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:05:51,383] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.4, 89.16666666666667, 1.0, 2.0, 0.4810742533325779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721864.3952393344, 721864.3952393337, 186815.3841790281]
[2019-03-27 04:05:51,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:05:51,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6584144240430151
[2019-03-27 04:05:57,047] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1561104]
[2019-03-27 04:05:57,048] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 85.0, 1.0, 2.0, 0.5165411808302021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721793.8990162755, 721793.899016275, 186228.7630499495]
[2019-03-27 04:05:57,049] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:05:57,051] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.0177784e-33 1.0000000e+00 0.0000000e+00 1.5644785e-36 5.3448200e-31], sampled 0.671554463693562
[2019-03-27 04:05:59,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:05:59,496] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:05:59,517] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.7761 3007626247.9629 1771.0000
[2019-03-27 04:05:59,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.0715 3164241762.8392 1812.0000
[2019-03-27 04:05:59,640] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.7769 2842408948.1557 1146.0000
[2019-03-27 04:06:00,658] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2175000, evaluation results [2175000.0, 7878.071481439704, 3164241762.8391542, 1812.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 8000.7760726877905, 3007626247.962901, 1771.0, 8494.776888467544, 2842408948.1557455, 1146.0]
[2019-03-27 04:06:00,786] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2175070: loss 0.0840
[2019-03-27 04:06:00,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2175070: learning rate 0.0001
[2019-03-27 04:06:00,890] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:00,898] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0642
[2019-03-27 04:06:00,906] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 96.0, 1.0, 2.0, 0.4763922924712487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665673.8767111677, 665673.8767111677, 179979.8213440143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2008800.0000, 
sim time next is 2009400.0000, 
raw observation next is [24.58333333333334, 95.83333333333333, 1.0, 2.0, 0.478267751760229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 668295.3233105448, 668295.3233105455, 180261.1107287601], 
processed observation next is [0.0, 0.2608695652173913, 0.3641390205371251, 0.9583333333333333, 1.0, 1.0, 0.37140692983160123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1856375898084847, 0.18563758980848488, 0.26904643392352257], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.5062159], dtype=float32), 1.4855657]. 
=============================================
[2019-03-27 04:06:03,186] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2176183: loss 0.0073
[2019-03-27 04:06:03,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2176183: learning rate 0.0001
[2019-03-27 04:06:03,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.8548482e-15 9.5834744e-01 2.2113718e-24 1.3902398e-14 4.1652530e-02], sum to 1.0000
[2019-03-27 04:06:03,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5120
[2019-03-27 04:06:03,494] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 86.33333333333334, 1.0, 2.0, 0.5682309389942238, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9616300022286505, 6.911199999999999, 6.9112, 168.9125571617357, 1627564.411860662, 1627564.411860663, 345114.9664499494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1960800.0000, 
sim time next is 1961400.0000, 
raw observation next is [24.76666666666667, 87.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.236117603758872, 6.9112, 168.910981452731, 1728092.462134648, 1497587.426497489, 318295.4454105516], 
processed observation next is [1.0, 0.6956521739130435, 0.3728278041074251, 0.8716666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.03249176037588723, 0.0, 0.8294302467148104, 0.48002568392629114, 0.4159965073604136, 0.47506782897097255], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5661734], dtype=float32), -0.4371886]. 
=============================================
[2019-03-27 04:06:03,696] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2176414: loss 0.0023
[2019-03-27 04:06:03,699] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2176415: learning rate 0.0001
[2019-03-27 04:06:03,913] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2176514: loss 0.6396
[2019-03-27 04:06:03,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2176515: learning rate 0.0001
[2019-03-27 04:06:05,053] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2177039: loss 0.0096
[2019-03-27 04:06:05,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2177040: learning rate 0.0001
[2019-03-27 04:06:06,724] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2177817: loss 0.0083
[2019-03-27 04:06:06,725] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2177817: learning rate 0.0001
[2019-03-27 04:06:09,379] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0597763e-14 9.9816066e-01 3.4360509e-23 2.7865711e-12 1.8393077e-03], sum to 1.0000
[2019-03-27 04:06:09,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1470
[2019-03-27 04:06:09,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1939359.541631129 W.
[2019-03-27 04:06:09,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.2, 72.0, 1.0, 2.0, 0.6935455950507654, 1.0, 1.0, 0.6935455950507654, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1939359.541631129, 1939359.541631128, 371369.0103131908], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2277000.0000, 
sim time next is 2277600.0000, 
raw observation next is [29.4, 71.33333333333333, 1.0, 2.0, 0.5827998206034316, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9908756327826729, 6.911200000000001, 6.9112, 168.9129563788852, 1629458.05271159, 1629458.052711589, 352101.7706541671], 
processed observation next is [1.0, 0.34782608695652173, 0.5924170616113744, 0.7133333333333333, 1.0, 1.0, 0.4973491814499176, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.9888727229056986, 8.881784197001253e-17, 0.0, 0.8294399445063526, 0.45262723686433054, 0.45262723686433026, 0.5255250308271151], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5359272], dtype=float32), 0.91623414]. 
=============================================
[2019-03-27 04:06:09,597] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2179138: loss 0.0073
[2019-03-27 04:06:09,601] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2179139: learning rate 0.0001
[2019-03-27 04:06:10,153] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2179479: loss 1.0213
[2019-03-27 04:06:10,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2179479: learning rate 0.0001
[2019-03-27 04:06:10,558] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2179742: loss 0.0070
[2019-03-27 04:06:10,559] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2179743: learning rate 0.0001
[2019-03-27 04:06:10,668] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2179816: loss 0.3853
[2019-03-27 04:06:10,670] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2179818: learning rate 0.0001
[2019-03-27 04:06:11,752] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2180325: loss 0.0016
[2019-03-27 04:06:11,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2180326: learning rate 0.0001
[2019-03-27 04:06:12,046] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5700821e-19 3.1719623e-11 1.5041666e-27 2.2407673e-18 1.0000000e+00], sum to 1.0000
[2019-03-27 04:06:12,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8765
[2019-03-27 04:06:12,060] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.7, 66.33333333333334, 1.0, 2.0, 0.5626537996515648, 1.0, 2.0, 0.5626537996515648, 1.0, 2.0, 0.9771435775398696, 6.911200000000001, 6.9112, 170.5573041426782, 2360461.013351772, 2360461.013351772, 461175.9546210251], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2208000.0000, 
sim time next is 2208600.0000, 
raw observation next is [31.8, 66.0, 1.0, 2.0, 0.5989030022993579, 1.0, 2.0, 0.5989030022993579, 1.0, 2.0, 1.03, 6.922549922928749, 6.9112, 170.5573041426782, 2512687.582641244, 2504557.171162035, 487580.5580146465], 
processed observation next is [1.0, 0.5652173913043478, 0.7061611374407584, 0.66, 1.0, 1.0, 0.5167506051799492, 1.0, 1.0, 0.5167506051799492, 1.0, 1.0, 1.0365853658536586, 0.001134992292874859, 0.0, 0.8375144448122397, 0.6979687729559011, 0.6957103253227874, 0.7277321761412635], 
reward next is 0.2155, 
noisyNet noise sample is [array([0.983599], dtype=float32), -0.08387989]. 
=============================================
[2019-03-27 04:06:12,522] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2180669: loss 0.0092
[2019-03-27 04:06:12,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2180669: learning rate 0.0001
[2019-03-27 04:06:12,608] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2180708: loss 0.0028
[2019-03-27 04:06:12,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2180708: learning rate 0.0001
[2019-03-27 04:06:13,428] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2181074: loss 0.0136
[2019-03-27 04:06:13,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2181076: learning rate 0.0001
[2019-03-27 04:06:13,526] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2181120: loss 0.0019
[2019-03-27 04:06:13,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2181122: learning rate 0.0001
[2019-03-27 04:06:14,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1816873e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3340749e-35], sum to 1.0000
[2019-03-27 04:06:14,287] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5072
[2019-03-27 04:06:14,292] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 92.66666666666667, 1.0, 2.0, 0.4839522424962031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676240.9306145759, 676240.9306145759, 181119.3198428449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2594400.0000, 
sim time next is 2595000.0000, 
raw observation next is [24.78333333333333, 92.83333333333333, 1.0, 2.0, 0.4811914095476058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672381.9153298624, 672381.9153298624, 180701.0240746483], 
processed observation next is [0.0, 0.0, 0.37361769352290675, 0.9283333333333332, 1.0, 1.0, 0.37492940909350103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18677275425829512, 0.18677275425829512, 0.26970302100693777], 
reward next is 0.7303, 
noisyNet noise sample is [array([2.0903811], dtype=float32), 0.3705116]. 
=============================================
[2019-03-27 04:06:14,308] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.61497 ]
 [73.94739 ]
 [73.701675]
 [74.03224 ]
 [73.91533 ]], R is [[73.18506622]
 [73.18289185]
 [73.18015289]
 [73.17697144]
 [73.17356873]].
[2019-03-27 04:06:15,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.4989620e-27 1.0000000e+00 1.2944519e-37 5.3211399e-31 6.8947163e-23], sum to 1.0000
[2019-03-27 04:06:15,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0856
[2019-03-27 04:06:15,562] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 78.33333333333333, 1.0, 2.0, 0.5689007990590577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794986.5059331937, 794986.5059331937, 195095.3099613687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2322600.0000, 
sim time next is 2323200.0000, 
raw observation next is [29.5, 78.66666666666667, 1.0, 2.0, 0.5676985508345127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793305.8473093299, 793305.8473093299, 194882.5793136188], 
processed observation next is [1.0, 0.9130434782608695, 0.5971563981042655, 0.7866666666666667, 1.0, 1.0, 0.47915488052350924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22036273536370274, 0.22036273536370274, 0.29086952136361016], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.8424516], dtype=float32), -1.24495]. 
=============================================
[2019-03-27 04:06:17,020] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2182677: loss 0.0999
[2019-03-27 04:06:17,023] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2182677: learning rate 0.0001
[2019-03-27 04:06:17,790] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2183019: loss 0.0446
[2019-03-27 04:06:17,793] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2183021: learning rate 0.0001
[2019-03-27 04:06:20,202] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2184087: loss 0.0741
[2019-03-27 04:06:20,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2184087: learning rate 0.0001
[2019-03-27 04:06:20,825] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2184367: loss 0.0115
[2019-03-27 04:06:20,828] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2184367: learning rate 0.0001
[2019-03-27 04:06:20,884] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2184389: loss 0.0227
[2019-03-27 04:06:20,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2184389: learning rate 0.0001
[2019-03-27 04:06:22,694] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2185194: loss 0.2003
[2019-03-27 04:06:22,697] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2185195: learning rate 0.0001
[2019-03-27 04:06:24,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5286686e-16 1.1830654e-05 9.5679401e-24 7.4195229e-13 9.9998820e-01], sum to 1.0000
[2019-03-27 04:06:24,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1298
[2019-03-27 04:06:24,253] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.23333333333333, 64.0, 1.0, 2.0, 0.5837095758360308, 1.0, 2.0, 0.5837095758360308, 1.0, 2.0, 1.013710497520698, 6.9112, 6.9112, 170.5573041426782, 2448881.351091825, 2448881.351091825, 477858.0242572089], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2304600.0000, 
sim time next is 2305200.0000, 
raw observation next is [32.26666666666667, 64.0, 1.0, 2.0, 0.540458077669526, 1.0, 2.0, 0.540458077669526, 1.0, 2.0, 0.938596948694493, 6.911199999999999, 6.9112, 170.5573041426782, 2267260.429977565, 2267260.429977565, 444251.32959517], 
processed observation next is [1.0, 0.6956521739130435, 0.7282780410742499, 0.64, 1.0, 1.0, 0.4463350333367783, 1.0, 1.0, 0.4463350333367783, 1.0, 1.0, 0.9251182301152353, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.629794563882657, 0.629794563882657, 0.6630616859629402], 
reward next is 0.3369, 
noisyNet noise sample is [array([0.17055365], dtype=float32), -0.67613137]. 
=============================================
[2019-03-27 04:06:24,409] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2185960: loss 0.0495
[2019-03-27 04:06:24,412] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2185960: learning rate 0.0001
[2019-03-27 04:06:27,459] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2187268: loss 0.0107
[2019-03-27 04:06:27,460] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2187268: learning rate 0.0001
[2019-03-27 04:06:27,818] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2187427: loss 0.0089
[2019-03-27 04:06:27,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2187428: learning rate 0.0001
[2019-03-27 04:06:28,505] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2187730: loss 0.0049
[2019-03-27 04:06:28,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2187730: learning rate 0.0001
[2019-03-27 04:06:28,529] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2187741: loss 0.0042
[2019-03-27 04:06:28,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2187741: learning rate 0.0001
[2019-03-27 04:06:29,959] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2188375: loss 0.0582
[2019-03-27 04:06:29,961] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2188375: learning rate 0.0001
[2019-03-27 04:06:30,828] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2188762: loss 0.1511
[2019-03-27 04:06:30,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2188763: learning rate 0.0001
[2019-03-27 04:06:30,916] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2188802: loss 0.1654
[2019-03-27 04:06:30,919] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2188802: learning rate 0.0001
[2019-03-27 04:06:31,611] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2189115: loss 0.0079
[2019-03-27 04:06:31,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2189115: learning rate 0.0001
[2019-03-27 04:06:31,698] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2189150: loss 0.0753
[2019-03-27 04:06:31,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2189152: learning rate 0.0001
[2019-03-27 04:06:32,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:32,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8819
[2019-03-27 04:06:32,357] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 99.0, 1.0, 2.0, 0.332306189633539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515151.0291003835, 515151.0291003842, 167922.4051474607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2778600.0000, 
sim time next is 2779200.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3307810144630378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513519.0467604169, 513519.0467604169, 167817.558638361], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19371206561811785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14264417965567136, 0.14264417965567136, 0.2504739681169567], 
reward next is 0.7495, 
noisyNet noise sample is [array([1.469535], dtype=float32), 0.06498507]. 
=============================================
[2019-03-27 04:06:33,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4858371e-20 1.0000000e+00 4.6158108e-30 5.0483800e-24 3.7536857e-12], sum to 1.0000
[2019-03-27 04:06:33,926] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8888
[2019-03-27 04:06:33,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2172330.153147584 W.
[2019-03-27 04:06:33,944] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.86666666666666, 89.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.923425060609212, 6.9112, 168.9073116729262, 2172330.153147584, 1454246.855206624, 311347.8409812104], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2461800.0000, 
sim time next is 2462400.0000, 
raw observation next is [25.9, 89.0, 1.0, 2.0, 0.4377926301785827, 1.0, 1.0, 0.4377926301785827, 1.0, 1.0, 0.742267434343071, 6.9112, 6.9112, 170.5573041426782, 1836209.084608829, 1836209.084608829, 371947.3226839838], 
processed observation next is [1.0, 0.5217391304347826, 0.42654028436018954, 0.89, 1.0, 1.0, 0.3226417231067261, 1.0, 0.5, 0.3226417231067261, 1.0, 0.5, 0.6856919931013059, 0.0, 0.0, 0.8375144448122397, 0.510058079058008, 0.510058079058008, 0.5551452577372892], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.85486394], dtype=float32), 0.09417238]. 
=============================================
[2019-03-27 04:06:34,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2190555: loss 0.0048
[2019-03-27 04:06:34,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2190555: learning rate 0.0001
[2019-03-27 04:06:35,129] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2190688: loss 0.0045
[2019-03-27 04:06:35,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2190689: learning rate 0.0001
[2019-03-27 04:06:37,773] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3079848e-29 1.0000000e+00 0.0000000e+00 5.5990960e-33 5.3210167e-24], sum to 1.0000
[2019-03-27 04:06:37,783] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1046
[2019-03-27 04:06:37,790] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.51666666666667, 85.33333333333334, 1.0, 2.0, 0.5366543953401602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749909.2179798448, 749909.2179798455, 189538.286373994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2578200.0000, 
sim time next is 2578800.0000, 
raw observation next is [27.43333333333334, 85.66666666666667, 1.0, 2.0, 0.5364686463239524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749649.5647398575, 749649.5647398575, 189507.0929359912], 
processed observation next is [1.0, 0.8695652173913043, 0.49921011058451853, 0.8566666666666667, 1.0, 1.0, 0.4415284895469306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20823599020551595, 0.20823599020551595, 0.28284640736715105], 
reward next is 0.7172, 
noisyNet noise sample is [array([1.0146098], dtype=float32), -2.1453674]. 
=============================================
[2019-03-27 04:06:38,258] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2192082: loss 0.0355
[2019-03-27 04:06:38,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2192082: learning rate 0.0001
[2019-03-27 04:06:38,372] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2192133: loss 0.0307
[2019-03-27 04:06:38,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2192133: learning rate 0.0001
[2019-03-27 04:06:38,730] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2192293: loss 0.0025
[2019-03-27 04:06:38,733] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2192293: learning rate 0.0001
[2019-03-27 04:06:39,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.83704697e-20 1.00000000e+00 1.35220485e-27 4.90425853e-21
 2.56526072e-12], sum to 1.0000
[2019-03-27 04:06:39,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2091
[2019-03-27 04:06:39,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1748825.762814029 W.
[2019-03-27 04:06:39,591] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.56666666666667, 72.66666666666666, 1.0, 2.0, 0.4169755121629435, 1.0, 2.0, 0.4169755121629435, 1.0, 1.0, 0.7183465157818184, 6.9112, 6.9112, 170.5573041426782, 1748825.762814029, 1748825.762814029, 361632.0932516519], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2551200.0000, 
sim time next is 2551800.0000, 
raw observation next is [29.68333333333333, 71.83333333333334, 1.0, 2.0, 0.4139243759530828, 1.0, 2.0, 0.4139243759530828, 1.0, 2.0, 0.7130628755320051, 6.911200000000001, 6.9112, 170.5573041426782, 1736018.708885669, 1736018.708885668, 359887.4528520682], 
processed observation next is [1.0, 0.5217391304347826, 0.6058451816745655, 0.7183333333333334, 1.0, 1.0, 0.29388479030491904, 1.0, 1.0, 0.29388479030491904, 1.0, 1.0, 0.6500766774780549, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4822274191349081, 0.4822274191349078, 0.5371454520180122], 
reward next is 0.4629, 
noisyNet noise sample is [array([-0.09583833], dtype=float32), -1.0682054]. 
=============================================
[2019-03-27 04:06:40,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4359326e-23 1.0000000e+00 1.5197245e-31 1.1220066e-25 5.9016602e-16], sum to 1.0000
[2019-03-27 04:06:40,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0591
[2019-03-27 04:06:40,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1801651.510593038 W.
[2019-03-27 04:06:40,375] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.56666666666667, 72.66666666666666, 1.0, 2.0, 0.6474901765334682, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986501135152325, 6.9112, 168.9124453315584, 1801651.510593038, 1748230.455318788, 374401.0508602227], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2551200.0000, 
sim time next is 2551800.0000, 
raw observation next is [29.68333333333333, 71.83333333333334, 1.0, 2.0, 0.417291561212253, 1.0, 1.0, 0.417291561212253, 1.0, 2.0, 0.7182422061684971, 6.9112, 6.9112, 170.5573041426782, 1750152.378092367, 1750152.378092367, 361716.994516446], 
processed observation next is [1.0, 0.5217391304347826, 0.6058451816745655, 0.7183333333333334, 1.0, 1.0, 0.2979416400147627, 1.0, 0.5, 0.2979416400147627, 1.0, 1.0, 0.6563929343518257, 0.0, 0.0, 0.8375144448122397, 0.48615343835899083, 0.48615343835899083, 0.5398761112185761], 
reward next is 0.4601, 
noisyNet noise sample is [array([1.1702906], dtype=float32), -2.0089362]. 
=============================================
[2019-03-27 04:06:40,917] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2193257: loss 0.0137
[2019-03-27 04:06:40,920] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2193257: learning rate 0.0001
[2019-03-27 04:06:42,857] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2194124: loss 0.0017
[2019-03-27 04:06:42,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2194124: learning rate 0.0001
[2019-03-27 04:06:45,417] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2195266: loss 0.0062
[2019-03-27 04:06:45,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2195268: learning rate 0.0001
[2019-03-27 04:06:45,700] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2195391: loss 0.0057
[2019-03-27 04:06:45,703] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2195391: learning rate 0.0001
[2019-03-27 04:06:45,995] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2195524: loss 0.0043
[2019-03-27 04:06:45,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2195525: learning rate 0.0001
[2019-03-27 04:06:46,969] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2195964: loss 0.0303
[2019-03-27 04:06:46,970] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2195964: learning rate 0.0001
[2019-03-27 04:06:48,239] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2196528: loss 0.0076
[2019-03-27 04:06:48,241] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2196529: learning rate 0.0001
[2019-03-27 04:06:49,165] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2196938: loss 0.0191
[2019-03-27 04:06:49,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2196940: learning rate 0.0001
[2019-03-27 04:06:49,195] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2196948: loss 0.0163
[2019-03-27 04:06:49,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2196949: learning rate 0.0001
[2019-03-27 04:06:49,295] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2197000: loss 0.0008
[2019-03-27 04:06:49,297] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2197000: learning rate 0.0001
[2019-03-27 04:06:50,008] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2197315: loss 0.0174
[2019-03-27 04:06:50,012] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2197316: learning rate 0.0001
[2019-03-27 04:06:50,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:50,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1576
[2019-03-27 04:06:50,970] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3937032667045529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587464.4672788488, 587464.4672788488, 173385.4817238476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3931210197645402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586595.8043169726, 586595.804316972, 173306.165648713], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2688205057404099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16294327897693683, 0.16294327897693667, 0.2586659188786761], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.22133], dtype=float32), -0.8294976]. 
=============================================
[2019-03-27 04:06:50,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.04327 ]
 [77.03349 ]
 [77.03428 ]
 [77.027   ]
 [76.948845]], R is [[77.03243256]
 [77.00332642]
 [76.97444153]
 [76.94586182]
 [76.91754913]].
[2019-03-27 04:06:52,395] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:06:52,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5936
[2019-03-27 04:06:52,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 98.0, 1.0, 2.0, 0.3415665411043997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529250.0020647112, 529250.0020647112, 169033.531024559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2780400.0000, 
sim time next is 2781000.0000, 
raw observation next is [21.5, 97.0, 1.0, 2.0, 0.3434163734979812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531381.400996574, 531381.400996574, 169184.0196654944], 
processed observation next is [1.0, 0.17391304347826086, 0.21800947867298584, 0.97, 1.0, 1.0, 0.20893538975660386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14760594472127056, 0.14760594472127056, 0.2525134621873051], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.25581166], dtype=float32), -1.7957758]. 
=============================================
[2019-03-27 04:06:52,424] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.49734]
 [74.54029]
 [74.56625]
 [74.4077 ]
 [74.16955]], R is [[74.52307892]
 [74.52555847]
 [74.52471161]
 [74.5289917 ]
 [74.5330658 ]].
[2019-03-27 04:06:52,593] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2198458: loss 0.0020
[2019-03-27 04:06:52,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2198458: learning rate 0.0001
[2019-03-27 04:06:53,085] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2198685: loss 0.0402
[2019-03-27 04:06:53,088] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2198685: learning rate 0.0001
[2019-03-27 04:06:55,085] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0354362e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.2896055e-33], sum to 1.0000
[2019-03-27 04:06:55,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9019
[2019-03-27 04:06:55,103] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5993207983065164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947479.7354895928, 947479.7354895921, 213367.7240488339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2995200.0000, 
sim time next is 2995800.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.6029224671931377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953142.5479992046, 953142.547999204, 214125.411955064], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.5215933339676357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26476181888866795, 0.2647618188886678, 0.3195901670971104], 
reward next is 0.6804, 
noisyNet noise sample is [array([1.3800273], dtype=float32), 0.5815137]. 
=============================================
[2019-03-27 04:06:56,023] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-27 04:06:56,026] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:06:56,027] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:06:56,027] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:06:56,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:56,029] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:06:56,031] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:56,029] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:06:56,031] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:56,036] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:56,035] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:06:56,062] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-27 04:06:56,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-27 04:06:56,086] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-27 04:06:56,127] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-27 04:06:56,127] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-27 04:07:12,675] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:12,676] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.93333333333334, 78.0, 1.0, 2.0, 0.5614630102908827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891187.9750266799, 891187.9750266799, 205939.0914790704]
[2019-03-27 04:07:12,677] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:07:12,679] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3513722e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.8190119966656546
[2019-03-27 04:07:14,533] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:14,536] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.74756234333334, 69.92780149, 1.0, 2.0, 0.6347508918025419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887044.3752493825, 887044.3752493825, 207388.8839614365]
[2019-03-27 04:07:14,538] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:07:14,540] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.605471e-36 1.000000e+00 0.000000e+00 0.000000e+00 8.898879e-36], sampled 0.8179383673499917
[2019-03-27 04:07:17,570] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:17,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.14662332833333, 100.0, 1.0, 2.0, 0.2683442335550708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440203.6490165567, 440203.6490165567, 162774.3041579119]
[2019-03-27 04:07:17,573] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:07:17,575] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6204689598844582
[2019-03-27 04:07:22,416] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:22,417] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.6, 94.0, 1.0, 2.0, 0.4742197296898489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662885.1596225568, 662885.1596225568, 179687.1728541443]
[2019-03-27 04:07:22,420] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:07:22,422] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.024422627188368518
[2019-03-27 04:07:26,512] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:26,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.37815327, 95.91350367, 1.0, 2.0, 0.5603560149377994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783041.5552636194, 783041.55526362, 193591.8388110695]
[2019-03-27 04:07:26,514] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:07:26,516] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6302886611187514
[2019-03-27 04:07:43,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:43,330] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.85, 69.83333333333334, 1.0, 2.0, 0.5673226497277568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792780.3644948784, 792780.3644948779, 194815.7322171064]
[2019-03-27 04:07:43,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:07:43,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8855346606673631
[2019-03-27 04:07:48,092] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:48,095] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.41784048, 80.1462478, 1.0, 2.0, 0.7636462899874581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067262.144483597, 1067262.144483597, 235185.9675241979]
[2019-03-27 04:07:48,098] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:07:48,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7174804142464416
[2019-03-27 04:07:56,790] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:07:56,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.205612135, 96.34089401, 1.0, 2.0, 0.5781506269043221, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004056441598824, 6.9112, 6.9112, 168.9128793239793, 1616449.4098356, 1616449.4098356, 353811.1308909917]
[2019-03-27 04:07:56,792] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:07:56,794] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.6221085e-33 1.0000000e+00 0.0000000e+00 3.1829927e-37 1.0054886e-31], sampled 0.588694213914221
[2019-03-27 04:08:05,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:05,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.114414435, 70.151836055, 1.0, 2.0, 0.4328950763541808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632987.0650672348, 632987.0650672355, 177335.5588594052]
[2019-03-27 04:08:05,136] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:08:05,138] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6289612e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.7348087e-36], sampled 0.7671660151004752
[2019-03-27 04:08:16,372] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:16,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.89792825, 81.69295801999999, 1.0, 2.0, 0.5741613622453045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802340.4352679327, 802340.4352679327, 196030.2642043213]
[2019-03-27 04:08:16,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:08:16,379] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6885616e-34 1.0000000e+00 0.0000000e+00 6.7988373e-38 1.1111927e-31], sampled 0.7680654714968983
[2019-03-27 04:08:16,545] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:16,550] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.58506724333333, 91.87385832333334, 1.0, 2.0, 0.6042883787172222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844457.0493782263, 844457.0493782269, 201548.061476829]
[2019-03-27 04:08:16,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:08:16,554] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22974002805896665
[2019-03-27 04:08:26,417] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:26,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.59932634333333, 88.64035381333333, 1.0, 2.0, 0.6863457297310172, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.993486815354958, 6.9112, 168.9119073886879, 1856021.815418042, 1797645.079092946, 382195.2814464805]
[2019-03-27 04:08:26,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:08:26,422] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0987388e-28 1.0000000e+00 0.0000000e+00 1.1702824e-31 6.8440073e-24], sampled 0.5820055161524278
[2019-03-27 04:08:26,424] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1856021.815418042 W.
[2019-03-27 04:08:28,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:28,418] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.40161472, 62.18993823, 1.0, 2.0, 0.5479048793386133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765636.0663036009, 765636.0663036009, 191440.9258306166]
[2019-03-27 04:08:28,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:08:28,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1806547e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 9.4455902e-38], sampled 0.9860892222197624
[2019-03-27 04:08:50,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:08:51,297] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1521772]
[2019-03-27 04:08:51,298] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.88155492, 81.87079155, 1.0, 2.0, 0.5365966667606632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749828.5208419813, 749828.5208419813, 189525.8256541776]
[2019-03-27 04:08:51,299] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:08:51,302] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3371603e-34 1.0000000e+00 0.0000000e+00 1.2336531e-38 1.3716523e-32], sampled 0.4918850811152382
[2019-03-27 04:08:51,386] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:08:51,583] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.8137 2842408425.1921 1144.0000
[2019-03-27 04:08:51,613] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.6445 3164150752.2332 1841.0000
[2019-03-27 04:08:51,754] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.9654 3007707631.6573 1774.0000
[2019-03-27 04:08:52,772] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2200000, evaluation results [2200000.0, 7875.64449201546, 3164150752.2331676, 1841.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.965375024541, 3007707631.657273, 1774.0, 8494.813668043382, 2842408425.1921024, 1144.0]
[2019-03-27 04:08:52,830] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2200029: loss 0.0039
[2019-03-27 04:08:52,833] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2200030: learning rate 0.0001
[2019-03-27 04:08:52,935] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2200074: loss 0.0451
[2019-03-27 04:08:52,937] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2200075: learning rate 0.0001
[2019-03-27 04:08:53,397] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2200296: loss 0.0050
[2019-03-27 04:08:53,398] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2200297: learning rate 0.0001
[2019-03-27 04:08:55,484] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2201272: loss 0.0045
[2019-03-27 04:08:55,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2201274: learning rate 0.0001
[2019-03-27 04:08:56,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8315900e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3540789e-34], sum to 1.0000
[2019-03-27 04:08:56,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5535
[2019-03-27 04:08:56,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.6692399577254918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027449.271806899, 1027449.271806899, 225822.4052598489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.6907842199617018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1058590.6258932, 1058590.625893201, 230597.951246472], 
processed observation next is [1.0, 0.6086956521739131, 0.2654028436018958, 0.915, 1.0, 1.0, 0.6274508674237371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.294052951637, 0.2940529516370003, 0.34417604663652535], 
reward next is 0.6558, 
noisyNet noise sample is [array([0.31103736], dtype=float32), -0.72256523]. 
=============================================
[2019-03-27 04:08:57,168] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2202054: loss 0.0025
[2019-03-27 04:08:57,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2202055: learning rate 0.0001
[2019-03-27 04:08:59,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2203296: loss 0.0354
[2019-03-27 04:08:59,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2203296: learning rate 0.0001
[2019-03-27 04:09:00,116] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2203425: loss 0.0195
[2019-03-27 04:09:00,120] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2203427: learning rate 0.0001
[2019-03-27 04:09:00,377] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2203545: loss 0.0146
[2019-03-27 04:09:00,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2203545: learning rate 0.0001
[2019-03-27 04:09:01,125] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2203894: loss 0.0046
[2019-03-27 04:09:01,126] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2203894: learning rate 0.0001
[2019-03-27 04:09:01,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:01,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0769
[2019-03-27 04:09:01,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.5720936681797003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 882650.21200267, 882650.2120026695, 205658.5751015131], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2969400.0000, 
sim time next is 2970000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5432639263750915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837069.2685101344, 837069.2685101344, 199991.506046124], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4497155739458933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23251924125281512, 0.23251924125281512, 0.29849478514346867], 
reward next is 0.7015, 
noisyNet noise sample is [array([0.41074625], dtype=float32), 1.5899441]. 
=============================================
[2019-03-27 04:09:01,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.56312 ]
 [73.5406  ]
 [73.51056 ]
 [73.54531 ]
 [73.826996]], R is [[73.68079376]
 [73.63703156]
 [73.59713745]
 [73.56002045]
 [73.54218292]].
[2019-03-27 04:09:02,376] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2204487: loss 0.0034
[2019-03-27 04:09:02,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2204487: learning rate 0.0001
[2019-03-27 04:09:02,996] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2204871: loss 0.0097
[2019-03-27 04:09:02,999] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2204871: learning rate 0.0001
[2019-03-27 04:09:03,112] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2204944: loss 0.0025
[2019-03-27 04:09:03,113] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2204944: loss 0.0051
[2019-03-27 04:09:03,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2204944: learning rate 0.0001
[2019-03-27 04:09:03,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2204944: learning rate 0.0001
[2019-03-27 04:09:03,709] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2205282: loss 0.0058
[2019-03-27 04:09:03,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2205282: learning rate 0.0001
[2019-03-27 04:09:04,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:04,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5082
[2019-03-27 04:09:04,468] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.569583847953161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865544.4901854828, 865544.4901854828, 203733.7914649566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [22.0, 99.0, 1.0, 2.0, 0.5795431279035387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877316.8717465064, 877316.8717465064, 205300.9956921553], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.99, 1.0, 1.0, 0.4934254553054683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24369913104069624, 0.24369913104069624, 0.30641939655545564], 
reward next is 0.6936, 
noisyNet noise sample is [array([1.4111103], dtype=float32), 1.4893196]. 
=============================================
[2019-03-27 04:09:06,360] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2206488: loss 0.0029
[2019-03-27 04:09:06,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2206488: learning rate 0.0001
[2019-03-27 04:09:07,321] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2206921: loss 0.0069
[2019-03-27 04:09:07,324] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2206921: learning rate 0.0001
[2019-03-27 04:09:07,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6896461e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8171327e-37], sum to 1.0000
[2019-03-27 04:09:07,437] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8502
[2019-03-27 04:09:07,441] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5223146229001158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729864.2466818137, 729864.2466818132, 187165.8974737502], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4244754492772479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20274006852272602, 0.20274006852272589, 0.2793520857817167], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.17075953], dtype=float32), 0.82816315]. 
=============================================
[2019-03-27 04:09:09,774] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2208009: loss 0.0135
[2019-03-27 04:09:09,779] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2208011: learning rate 0.0001
[2019-03-27 04:09:10,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2898164e-31 1.0000000e+00 0.0000000e+00 3.5504165e-34 3.7351992e-25], sum to 1.0000
[2019-03-27 04:09:10,043] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9899
[2019-03-27 04:09:10,051] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 66.33333333333334, 1.0, 2.0, 0.5509519625397519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769895.5689331697, 769895.5689331697, 191964.5982093486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3609600.0000, 
sim time next is 3610200.0000, 
raw observation next is [31.16666666666667, 66.16666666666666, 1.0, 2.0, 0.5461988551441728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763251.2309266883, 763251.2309266883, 191151.2970614113], 
processed observation next is [1.0, 0.782608695652174, 0.6761453396524489, 0.6616666666666666, 1.0, 1.0, 0.45325163270382257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21201423081296897, 0.21201423081296897, 0.28530044337524074], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.119208], dtype=float32), 1.9760965]. 
=============================================
[2019-03-27 04:09:10,132] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2208169: loss 0.0135
[2019-03-27 04:09:10,134] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2208170: learning rate 0.0001
[2019-03-27 04:09:10,403] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2208296: loss 0.0232
[2019-03-27 04:09:10,405] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2208296: learning rate 0.0001
[2019-03-27 04:09:11,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6452897e-33 1.0000000e+00 0.0000000e+00 6.1474681e-37 3.8312953e-33], sum to 1.0000
[2019-03-27 04:09:11,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6665
[2019-03-27 04:09:11,596] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5941244627154119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830248.030297597, 830248.030297597, 199656.6287523603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3342000.0000, 
sim time next is 3342600.0000, 
raw observation next is [30.5, 77.0, 1.0, 2.0, 0.5900030291869829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824486.3753322603, 824486.3753322603, 198898.6271320656], 
processed observation next is [0.0, 0.6956521739130435, 0.6445497630331753, 0.77, 1.0, 1.0, 0.506027746008413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22902399314785007, 0.22902399314785007, 0.2968636225851725], 
reward next is 0.7031, 
noisyNet noise sample is [array([0.83013034], dtype=float32), -0.8087553]. 
=============================================
[2019-03-27 04:09:12,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:12,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4870
[2019-03-27 04:09:12,307] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 83.5, 1.0, 2.0, 0.442769047961424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635719.2119125779, 635719.2119125779, 177306.7193752463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3306600.0000, 
sim time next is 3307200.0000, 
raw observation next is [25.66666666666666, 83.66666666666667, 1.0, 2.0, 0.4506962289047836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642078.7491417435, 642078.7491417429, 177819.6168346419], 
processed observation next is [0.0, 0.2608695652173913, 0.4154818325434437, 0.8366666666666667, 1.0, 1.0, 0.3381882275961249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17835520809492875, 0.1783552080949286, 0.2654024131860327], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.02212544], dtype=float32), 0.59654176]. 
=============================================
[2019-03-27 04:09:12,582] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2209267: loss 0.0047
[2019-03-27 04:09:12,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2209267: learning rate 0.0001
[2019-03-27 04:09:14,325] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2210040: loss 0.0067
[2019-03-27 04:09:14,326] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2210041: learning rate 0.0001
[2019-03-27 04:09:15,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3376021e-16 5.8989157e-04 6.9571064e-22 3.2675952e-12 9.9941015e-01], sum to 1.0000
[2019-03-27 04:09:15,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9987
[2019-03-27 04:09:15,385] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 70.83333333333333, 1.0, 2.0, 0.5031994759301275, 1.0, 2.0, 0.5031994759301275, 1.0, 2.0, 0.8738910790810381, 6.911199999999999, 6.9112, 170.5573041426782, 2110811.602567621, 2110811.602567622, 417365.5308082329], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3405000.0000, 
sim time next is 3405600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5482075014778531, 1.0, 2.0, 0.5482075014778531, 1.0, 2.0, 0.9520551350759422, 6.911199999999999, 6.9112, 170.5573041426782, 2299799.731538553, 2299799.731538554, 450083.2941820152], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.7, 1.0, 1.0, 0.45567168852753387, 1.0, 1.0, 0.45567168852753387, 1.0, 1.0, 0.9415306525316367, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6388332587607091, 0.6388332587607095, 0.6717661107194256], 
reward next is 0.3282, 
noisyNet noise sample is [array([-1.6171005], dtype=float32), 2.8789635]. 
=============================================
[2019-03-27 04:09:17,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2211364: loss 0.0135
[2019-03-27 04:09:17,280] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2211364: learning rate 0.0001
[2019-03-27 04:09:17,362] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2211403: loss 4.1177
[2019-03-27 04:09:17,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2211403: learning rate 0.0001
[2019-03-27 04:09:17,725] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2211566: loss 0.4305
[2019-03-27 04:09:17,733] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2211568: learning rate 0.0001
[2019-03-27 04:09:18,263] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2211808: loss 0.0167
[2019-03-27 04:09:18,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2211808: learning rate 0.0001
[2019-03-27 04:09:18,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5659497e-17 4.7492151e-07 5.5087211e-23 8.8771687e-15 9.9999952e-01], sum to 1.0000
[2019-03-27 04:09:18,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9272
[2019-03-27 04:09:18,364] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 70.5, 1.0, 2.0, 0.5711738650834395, 1.0, 2.0, 0.5711738650834395, 1.0, 2.0, 0.9919401135663416, 6.911199999999999, 6.9112, 170.5573041426782, 2396238.901027008, 2396238.901027009, 467853.3634708498], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [31.66666666666666, 70.66666666666666, 1.0, 2.0, 0.5810219986920018, 1.0, 2.0, 0.5810219986920018, 1.0, 2.0, 1.009043064816863, 6.911199999999999, 6.9112, 170.5573041426782, 2437594.955798845, 2437594.955798846, 475695.507876124], 
processed observation next is [1.0, 0.43478260869565216, 0.6998420221169034, 0.7066666666666666, 1.0, 1.0, 0.49520722733976114, 1.0, 1.0, 0.49520722733976114, 1.0, 1.0, 1.0110281278254425, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6771097099441236, 0.6771097099441239, 0.7099932953374986], 
reward next is 0.2900, 
noisyNet noise sample is [array([1.1166414], dtype=float32), 0.7325232]. 
=============================================
[2019-03-27 04:09:18,376] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[47.50667 ]
 [47.22728 ]
 [46.907475]
 [46.49841 ]
 [46.17989 ]], R is [[47.76366806]
 [47.58774567]
 [47.41580963]
 [47.28950119]
 [47.14484406]].
[2019-03-27 04:09:19,657] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2212432: loss 0.0243
[2019-03-27 04:09:19,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2212433: learning rate 0.0001
[2019-03-27 04:09:19,878] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3860473e-29 1.0000000e+00 4.0935659e-37 2.5025752e-30 5.6783169e-25], sum to 1.0000
[2019-03-27 04:09:19,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5416
[2019-03-27 04:09:19,892] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 78.16666666666667, 1.0, 2.0, 0.8500554522355913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1188094.02996442, 1188094.02996442, 256566.9197807291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [28.0, 77.33333333333334, 1.0, 2.0, 0.855749390220978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196056.738696544, 1196056.738696544, 258053.4722672796], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.7733333333333334, 1.0, 1.0, 0.8262040846035879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3322379829712622, 0.3322379829712622, 0.3851544362198203], 
reward next is 0.6148, 
noisyNet noise sample is [array([-0.58458203], dtype=float32), -0.92589915]. 
=============================================
[2019-03-27 04:09:20,555] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2212835: loss 0.0196
[2019-03-27 04:09:20,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2212837: learning rate 0.0001
[2019-03-27 04:09:20,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2212908: loss 0.0159
[2019-03-27 04:09:20,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2212909: learning rate 0.0001
[2019-03-27 04:09:20,995] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2213032: loss 0.0217
[2019-03-27 04:09:21,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2213032: learning rate 0.0001
[2019-03-27 04:09:21,464] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2213243: loss 0.0279
[2019-03-27 04:09:21,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2213244: learning rate 0.0001
[2019-03-27 04:09:22,621] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7567048e-34 1.0000000e+00 0.0000000e+00 4.1868528e-38 2.7695339e-33], sum to 1.0000
[2019-03-27 04:09:22,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3823
[2019-03-27 04:09:22,641] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5855459607091553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818255.5467687334, 818255.546768734, 198084.2774686287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [30.0, 79.00000000000001, 1.0, 2.0, 0.5863806676398069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819422.435591174, 819422.435591174, 198236.2585279407], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7900000000000001, 1.0, 1.0, 0.5016634549877191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22761734321977056, 0.22761734321977056, 0.2958750127282697], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.7801763], dtype=float32), -0.56467783]. 
=============================================
[2019-03-27 04:09:22,665] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.44598]
 [73.3308 ]
 [73.29218]
 [73.25233]
 [73.23887]], R is [[73.42633057]
 [73.39642334]
 [73.36636353]
 [73.33621216]
 [73.3059845 ]].
[2019-03-27 04:09:24,245] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2214482: loss 0.0120
[2019-03-27 04:09:24,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2214483: learning rate 0.0001
[2019-03-27 04:09:25,313] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2214963: loss 0.0027
[2019-03-27 04:09:25,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2214965: learning rate 0.0001
[2019-03-27 04:09:26,465] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.7638718e-25 1.0000000e+00 1.5038177e-33 3.3356774e-26 7.0270430e-19], sum to 1.0000
[2019-03-27 04:09:26,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0817
[2019-03-27 04:09:26,480] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.7802758105508892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090515.280487867, 1090515.280487866, 239123.9517923286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3465000.0000, 
sim time next is 3465600.0000, 
raw observation next is [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.7623205926511752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1065408.437642088, 1065408.437642088, 234869.0653913397], 
processed observation next is [1.0, 0.08695652173913043, 0.4628751974723541, 0.8233333333333335, 1.0, 1.0, 0.713639268254428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2959467882339133, 0.2959467882339133, 0.3505508438676712], 
reward next is 0.6494, 
noisyNet noise sample is [array([1.338834], dtype=float32), -2.3579664]. 
=============================================
[2019-03-27 04:09:27,653] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2216005: loss 0.4804
[2019-03-27 04:09:27,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2216006: learning rate 0.0001
[2019-03-27 04:09:28,093] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2216201: loss 0.0274
[2019-03-27 04:09:28,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2216202: learning rate 0.0001
[2019-03-27 04:09:28,257] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2216275: loss 0.2917
[2019-03-27 04:09:28,261] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2216276: learning rate 0.0001
[2019-03-27 04:09:30,585] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2217315: loss 0.0412
[2019-03-27 04:09:30,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2217315: learning rate 0.0001
[2019-03-27 04:09:31,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:31,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9698
[2019-03-27 04:09:31,414] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6134717693504033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857295.4705314585, 857295.4705314592, 203283.5157147623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3841200.0000, 
sim time next is 3841800.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6240732084916257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872116.5228211972, 872116.5228211966, 205316.8012722254], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5470761548091876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24225458967255478, 0.24225458967255462, 0.3064429869734707], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.37130865], dtype=float32), -2.03032]. 
=============================================
[2019-03-27 04:09:32,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:32,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9243
[2019-03-27 04:09:32,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991590283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865603.2364676001, 865603.2364676001, 204419.2682650468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853200.0000, 
sim time next is 3853800.0000, 
raw observation next is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.58, 1.0, 1.0, 0.5347890327718859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23829418017325096, 0.23829418017325096, 0.30352234154607943], 
reward next is 0.6965, 
noisyNet noise sample is [array([1.6535716], dtype=float32), -1.5029869]. 
=============================================
[2019-03-27 04:09:32,259] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2218045: loss 0.5479
[2019-03-27 04:09:32,262] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2218045: learning rate 0.0001
[2019-03-27 04:09:34,947] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2219249: loss 0.0070
[2019-03-27 04:09:34,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2219249: learning rate 0.0001
[2019-03-27 04:09:35,205] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2219364: loss 0.3586
[2019-03-27 04:09:35,208] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2219365: learning rate 0.0001
[2019-03-27 04:09:35,463] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2219478: loss 0.0061
[2019-03-27 04:09:35,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2219479: learning rate 0.0001
[2019-03-27 04:09:35,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:09:35,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5444
[2019-03-27 04:09:35,888] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.596136839469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833061.2910813255, 833061.2910813255, 200028.4995750689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.598966487685373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837017.0929512661, 837017.0929512654, 200553.6879196656], 
processed observation next is [0.0, 0.34782608695652173, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5168270935968349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23250474804201837, 0.23250474804201818, 0.29933386256666505], 
reward next is 0.7007, 
noisyNet noise sample is [array([-0.036003], dtype=float32), 1.2810683]. 
=============================================
[2019-03-27 04:09:36,490] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2219936: loss 0.1124
[2019-03-27 04:09:36,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2219937: learning rate 0.0001
[2019-03-27 04:09:37,877] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2220559: loss 0.0471
[2019-03-27 04:09:37,879] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2220560: learning rate 0.0001
[2019-03-27 04:09:38,632] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2220898: loss 0.0437
[2019-03-27 04:09:38,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2220899: learning rate 0.0001
[2019-03-27 04:09:38,636] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2220900: loss 0.0153
[2019-03-27 04:09:38,639] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2220900: learning rate 0.0001
[2019-03-27 04:09:38,750] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2220953: loss 0.1260
[2019-03-27 04:09:38,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2220954: learning rate 0.0001
[2019-03-27 04:09:39,494] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2221277: loss 0.5884
[2019-03-27 04:09:39,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2221278: learning rate 0.0001
[2019-03-27 04:09:42,082] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2222432: loss 0.0115
[2019-03-27 04:09:42,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2222432: learning rate 0.0001
[2019-03-27 04:09:43,355] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2222998: loss 10.2955
[2019-03-27 04:09:43,358] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2222999: learning rate 0.0001
[2019-03-27 04:09:45,316] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2223879: loss 0.0138
[2019-03-27 04:09:45,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2223880: learning rate 0.0001
[2019-03-27 04:09:46,068] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2224217: loss 4.8416
[2019-03-27 04:09:46,071] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2224217: learning rate 0.0001
[2019-03-27 04:09:46,078] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2224220: loss 0.0050
[2019-03-27 04:09:46,081] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2224220: learning rate 0.0001
[2019-03-27 04:09:47,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.37304883e-16 5.47557011e-06 1.19168575e-20 7.33317920e-11
 9.99994516e-01], sum to 1.0000
[2019-03-27 04:09:47,676] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3289
[2019-03-27 04:09:47,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.16666666666666, 63.66666666666666, 1.0, 2.0, 0.6982564282605798, 1.0, 2.0, 0.6697182536445524, 1.0, 2.0, 1.03, 7.005097594677369, 6.9112, 170.5573041426782, 2810125.924116104, 2742863.257538757, 520779.1636553849], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3759000.0000, 
sim time next is 3759600.0000, 
raw observation next is [33.33333333333334, 64.33333333333334, 1.0, 2.0, 0.6162210186232925, 1.0, 2.0, 0.6162210186232925, 1.0, 2.0, 1.03, 6.956360777401248, 6.9112, 170.5573041426782, 2585420.204079451, 2553069.702203963, 493936.0120320166], 
processed observation next is [1.0, 0.5217391304347826, 0.7788309636650873, 0.6433333333333334, 1.0, 1.0, 0.5376156850883042, 1.0, 1.0, 0.5376156850883042, 1.0, 1.0, 1.0365853658536586, 0.004516077740124835, 0.0, 0.8375144448122397, 0.7181722789109586, 0.7091860283899897, 0.7372179284059949], 
reward next is 0.0370, 
noisyNet noise sample is [array([2.0316803], dtype=float32), -2.1879637]. 
=============================================
[2019-03-27 04:09:47,836] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:09:47,840] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:09:47,842] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:47,843] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:09:47,845] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:47,846] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:09:47,846] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:47,846] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:09:47,848] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:09:47,849] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:47,850] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:09:47,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-27 04:09:47,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-27 04:09:47,919] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-27 04:09:47,919] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-27 04:09:47,946] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-27 04:10:03,554] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:03,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.8, 83.16666666666667, 1.0, 2.0, 0.3026255061786514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484424.6163609365, 484424.6163609365, 165967.935274268]
[2019-03-27 04:10:03,556] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:10:03,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8234135e-34 1.0000000e+00 0.0000000e+00 1.5219061e-38 2.9231584e-33], sampled 0.7181513525588349
[2019-03-27 04:10:14,692] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:14,693] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.11866719333334, 95.54050666, 1.0, 2.0, 0.4547414220650766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 642859.4659012399, 642859.4659012406, 177771.2676588568]
[2019-03-27 04:10:14,695] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:10:14,697] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7935082839174405
[2019-03-27 04:10:39,395] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:39,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.5126816872501607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716398.979103515, 716398.979103515, 185606.722174719]
[2019-03-27 04:10:39,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:10:39,398] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.1366871e-33 1.0000000e+00 0.0000000e+00 1.8188465e-36 3.5327796e-30], sampled 0.19514669814060148
[2019-03-27 04:10:41,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:41,380] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.83333333333333, 67.5, 1.0, 2.0, 0.6067918558539755, 1.0, 2.0, 0.6067918558539755, 1.0, 2.0, 1.03, 6.937951493861656, 6.9112, 170.5573041426782, 2545818.854477916, 2526655.671243615, 490454.3083369368]
[2019-03-27 04:10:41,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:10:41,386] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9760185e-17 2.4498640e-06 1.8168837e-24 1.1910601e-12 9.9999750e-01], sampled 0.23609333843227176
[2019-03-27 04:10:42,028] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:42,029] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.07898281666667, 72.68113092333334, 1.0, 2.0, 0.4987208055199346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696884.2541285475, 696884.2541285469, 183395.0164591079]
[2019-03-27 04:10:42,031] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:10:42,033] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9887525e-33 1.0000000e+00 0.0000000e+00 5.8642991e-37 7.4783484e-31], sampled 0.5435949638151302
[2019-03-27 04:10:42,685] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:42,687] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.3, 45.33333333333334, 1.0, 2.0, 0.5598351123056137, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9659031749754642, 6.911200000000001, 6.9112, 168.9129352107123, 1565203.365008987, 1565203.365008987, 341201.8788836189]
[2019-03-27 04:10:42,688] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:10:42,690] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5440687e-24 1.0000000e+00 5.3256021e-33 1.1939467e-24 9.5537534e-17], sampled 0.045324202412741155
[2019-03-27 04:10:48,263] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:10:48,266] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666666, 88.16666666666667, 1.0, 2.0, 0.5406849000012719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755543.3608614736, 755543.3608614736, 190215.8729619542]
[2019-03-27 04:10:48,267] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:10:48,270] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.0339403e-32 1.0000000e+00 0.0000000e+00 9.6327794e-36 3.3232129e-29], sampled 0.7693183202436749
[2019-03-27 04:11:01,432] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:11:01,433] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.0, 65.0, 1.0, 2.0, 0.5400141967622287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754605.7993023755, 754605.7993023748, 190102.7456558862]
[2019-03-27 04:11:01,434] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:11:01,437] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4253902e-32 1.0000000e+00 0.0000000e+00 5.1677520e-36 1.0833185e-29], sampled 0.8942087960334123
[2019-03-27 04:11:09,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:11:09,406] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.29022741, 55.82524437, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.924514503735026, 6.9112, 168.9073558540769, 3003197.411205765, 2284341.062612994, 473765.3441660694]
[2019-03-27 04:11:09,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:11:09,411] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5284394e-14 1.5857150e-01 1.8110478e-22 3.3587193e-12 8.4142852e-01], sampled 0.06390699693030677
[2019-03-27 04:11:09,414] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3003197.411205765 W.
[2019-03-27 04:11:30,398] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15101764]
[2019-03-27 04:11:30,399] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.95, 94.16666666666667, 1.0, 2.0, 0.5342054339577943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746485.88996673, 746485.88996673, 189128.168108512]
[2019-03-27 04:11:30,399] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:11:30,404] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3577084e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.0083122e-38], sampled 0.046585526676500955
[2019-03-27 04:11:42,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.5074 2927179192.1078 1333.0000
[2019-03-27 04:11:43,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8039.3931 3005606845.4167 1653.0000
[2019-03-27 04:11:43,551] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7911.3646 3162892300.3330 1819.0000
[2019-03-27 04:11:43,631] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8666.0287 2778944534.3761 920.0000
[2019-03-27 04:11:43,647] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8506.3878 2841429626.1654 1123.0000
[2019-03-27 04:11:44,665] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2225000, evaluation results [2225000.0, 7911.364619798107, 3162892300.3330307, 1819.0, 8260.507362674507, 2927179192.107773, 1333.0, 8666.028732336548, 2778944534.376089, 920.0, 8039.393143710551, 3005606845.4167113, 1653.0, 8506.387775178238, 2841429626.165431, 1123.0]
[2019-03-27 04:11:45,229] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2225264: loss 0.0045
[2019-03-27 04:11:45,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2225264: learning rate 0.0001
[2019-03-27 04:11:46,886] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2226033: loss 0.0059
[2019-03-27 04:11:46,888] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2226033: learning rate 0.0001
[2019-03-27 04:11:49,642] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2227314: loss 5.5547
[2019-03-27 04:11:49,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2227315: learning rate 0.0001
[2019-03-27 04:11:49,790] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2227384: loss 0.0077
[2019-03-27 04:11:49,793] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2227384: learning rate 0.0001
[2019-03-27 04:11:49,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2227421: loss 3.5557
[2019-03-27 04:11:49,873] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2227421: learning rate 0.0001
[2019-03-27 04:11:50,956] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2227933: loss 0.0076
[2019-03-27 04:11:50,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2227933: learning rate 0.0001
[2019-03-27 04:11:52,224] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2228514: loss 0.0082
[2019-03-27 04:11:52,226] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2228514: learning rate 0.0001
[2019-03-27 04:11:52,967] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2228859: loss 8.9324
[2019-03-27 04:11:52,970] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2228860: learning rate 0.0001
[2019-03-27 04:11:53,114] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2228926: loss 0.0341
[2019-03-27 04:11:53,117] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2228926: learning rate 0.0001
[2019-03-27 04:11:53,152] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2228945: loss 0.0108
[2019-03-27 04:11:53,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2228948: learning rate 0.0001
[2019-03-27 04:11:53,853] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2229269: loss 0.0097
[2019-03-27 04:11:53,858] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2229270: learning rate 0.0001
[2019-03-27 04:11:56,198] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2230500: loss 2.1231
[2019-03-27 04:11:56,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2230502: learning rate 0.0001
[2019-03-27 04:11:57,147] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2230954: loss 0.1208
[2019-03-27 04:11:57,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2230955: learning rate 0.0001
[2019-03-27 04:11:59,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2231956: loss 17.2864
[2019-03-27 04:11:59,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2231956: learning rate 0.0001
[2019-03-27 04:11:59,761] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2232126: loss 0.0391
[2019-03-27 04:11:59,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2232126: learning rate 0.0001
[2019-03-27 04:11:59,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6409623e-30 1.0000000e+00 0.0000000e+00 5.5053367e-32 4.6453751e-25], sum to 1.0000
[2019-03-27 04:11:59,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2435
[2019-03-27 04:11:59,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5247837973173619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733315.7759126496, 733315.7759126503, 187570.242284233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [31.0, 65.66666666666667, 1.0, 2.0, 0.5321880424947747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743665.8485963726, 743665.8485963732, 188793.6441412979], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.6566666666666667, 1.0, 1.0, 0.4363711355358731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20657384683232574, 0.2065738468323259, 0.28178155841984764], 
reward next is 0.7182, 
noisyNet noise sample is [array([1.2259568], dtype=float32), -0.5905772]. 
=============================================
[2019-03-27 04:12:00,019] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2232239: loss 15.9220
[2019-03-27 04:12:00,025] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2232239: learning rate 0.0001
[2019-03-27 04:12:01,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5640592e-33 1.0000000e+00 0.0000000e+00 1.2708255e-36 5.7213015e-33], sum to 1.0000
[2019-03-27 04:12:01,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:12:01,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2191
[2019-03-27 04:12:01,775] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5441893921847297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760442.2281859026, 760442.228185902, 190809.369833871], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4052400.0000, 
sim time next is 4053000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5439485633559369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760105.5771243477, 760105.5771243477, 190768.5081987891], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4505404377782372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21114043809009658, 0.21114043809009658, 0.2847291167146106], 
reward next is 0.7153, 
noisyNet noise sample is [array([0.06364303], dtype=float32), -0.88215035]. 
=============================================
[2019-03-27 04:12:01,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7508
[2019-03-27 04:12:01,781] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 52.5, 1.0, 2.0, 0.5339334248901505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746105.6575136452, 746105.657513646, 189084.4687387619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5366912403143218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749960.7225235285, 749960.7225235285, 189545.519842363], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44179667507749615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20832242292320235, 0.20832242292320235, 0.28290376095875075], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.70866144], dtype=float32), -0.05511881]. 
=============================================
[2019-03-27 04:12:01,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.6482  ]
 [68.88619 ]
 [68.6941  ]
 [68.55589 ]
 [68.478195]], R is [[68.38781738]
 [68.41915131]
 [68.45002747]
 [68.48062134]
 [68.51117706]].
[2019-03-27 04:12:02,436] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2233316: loss 7.7956
[2019-03-27 04:12:02,440] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2233317: learning rate 0.0001
[2019-03-27 04:12:04,220] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2234108: loss 6.6726
[2019-03-27 04:12:04,223] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2234109: learning rate 0.0001
[2019-03-27 04:12:06,565] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2235152: loss 0.0169
[2019-03-27 04:12:06,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2235152: learning rate 0.0001
[2019-03-27 04:12:07,054] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2235371: loss 0.0038
[2019-03-27 04:12:07,058] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2235372: learning rate 0.0001
[2019-03-27 04:12:07,163] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2235416: loss 2.4667
[2019-03-27 04:12:07,166] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2235416: learning rate 0.0001
[2019-03-27 04:12:08,402] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2235969: loss 1.3759
[2019-03-27 04:12:08,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2235970: learning rate 0.0001
[2019-03-27 04:12:09,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2236535: loss 0.9410
[2019-03-27 04:12:09,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2236536: learning rate 0.0001
[2019-03-27 04:12:09,850] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1979072e-23 1.0000000e+00 5.2097588e-34 6.7213322e-27 1.4013220e-18], sum to 1.0000
[2019-03-27 04:12:09,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2580
[2019-03-27 04:12:09,865] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.5, 1.0, 2.0, 0.5881749164391958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821930.732113032, 821930.732113032, 198565.1163955955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4303800.0000, 
sim time next is 4304400.0000, 
raw observation next is [34.66666666666666, 58.66666666666667, 1.0, 2.0, 0.5933207032444365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 829124.3933819556, 829124.393381955, 199509.7389313164], 
processed observation next is [1.0, 0.8260869565217391, 0.842022116903633, 0.5866666666666667, 1.0, 1.0, 0.5100249436679958, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23031233149498767, 0.2303123314949875, 0.2977757297482334], 
reward next is 0.7022, 
noisyNet noise sample is [array([-0.82378423], dtype=float32), -0.278348]. 
=============================================
[2019-03-27 04:12:10,479] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2236858: loss 0.0346
[2019-03-27 04:12:10,485] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2236859: learning rate 0.0001
[2019-03-27 04:12:10,733] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2236966: loss 10.3439
[2019-03-27 04:12:10,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2236966: learning rate 0.0001
[2019-03-27 04:12:10,883] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2237031: loss 17.1952
[2019-03-27 04:12:10,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2237031: learning rate 0.0001
[2019-03-27 04:12:11,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2237271: loss 12.6547
[2019-03-27 04:12:11,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2237271: learning rate 0.0001
[2019-03-27 04:12:14,132] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2238477: loss 0.0119
[2019-03-27 04:12:14,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2238479: learning rate 0.0001
[2019-03-27 04:12:15,208] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0900405e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.0250483e-36], sum to 1.0000
[2019-03-27 04:12:15,222] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5350
[2019-03-27 04:12:15,226] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.5955079195960712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832182.0733987939, 832182.0733987939, 199913.1216876702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4464000.0000, 
sim time next is 4464600.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6051286509504571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 845631.7472661235, 845631.7472661241, 201705.7378540938], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5242513866872976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2348977075739232, 0.23489770757392336, 0.30105334008073703], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.23076667], dtype=float32), -0.394999]. 
=============================================
[2019-03-27 04:12:15,329] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.0278405e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 5.1875254e-35], sum to 1.0000
[2019-03-27 04:12:15,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9865
[2019-03-27 04:12:15,342] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6055076074234279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846161.5276253807, 846161.5276253807, 201776.9095753855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4466400.0000, 
sim time next is 4467000.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6063457077888629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847333.1911276529, 847333.1911276529, 201934.4634184723], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5257177202275456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23537033086879247, 0.23537033086879247, 0.3013947215201079], 
reward next is 0.6986, 
noisyNet noise sample is [array([0.23076667], dtype=float32), -0.394999]. 
=============================================
[2019-03-27 04:12:15,356] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[74.347534]
 [74.39064 ]
 [74.45836 ]
 [74.535095]
 [74.62498 ]], R is [[74.27449799]
 [74.23059082]
 [74.18768311]
 [74.14533997]
 [74.10282898]].
[2019-03-27 04:12:15,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2239043: loss 0.5111
[2019-03-27 04:12:15,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2239043: learning rate 0.0001
[2019-03-27 04:12:17,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2239922: loss 0.0026
[2019-03-27 04:12:17,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2239924: learning rate 0.0001
[2019-03-27 04:12:17,804] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2240119: loss 0.3556
[2019-03-27 04:12:17,809] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2240121: learning rate 0.0001
[2019-03-27 04:12:17,847] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3837249e-20 1.0007992e-12 4.8584575e-26 2.8339737e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 04:12:17,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2580
[2019-03-27 04:12:17,861] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.8316838007819889, 1.0, 2.0, 0.736431939905257, 1.0, 2.0, 1.03, 7.005108117599336, 6.9112, 170.5573041426782, 3090401.665956218, 3023131.461381821, 566148.4415998085], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.8501707011436521, 1.0, 2.0, 0.7456753900860886, 1.0, 2.0, 1.03, 7.005109575964928, 6.9112, 170.5573041426782, 3129239.961136452, 3061968.711875459, 572957.5299761081], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.8194827724622314, 1.0, 1.0, 0.6935848073326369, 1.0, 1.0, 1.0365853658536586, 0.009390957596492822, 0.0, 0.8375144448122397, 0.8692333225379033, 0.8505468644098497, 0.8551604925016539], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9799976], dtype=float32), 0.25354528]. 
=============================================
[2019-03-27 04:12:17,957] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2240185: loss 0.0095
[2019-03-27 04:12:17,960] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2240185: learning rate 0.0001
[2019-03-27 04:12:18,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6386209e-29 1.0000000e+00 0.0000000e+00 1.0238114e-31 6.0346533e-25], sum to 1.0000
[2019-03-27 04:12:18,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1549
[2019-03-27 04:12:18,726] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5514555675697278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770599.5575740221, 770599.5575740221, 192050.3046828069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4484400.0000, 
sim time next is 4485000.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5512385033362721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770296.1237828465, 770296.1237828465, 192013.0073858922], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45932349799550853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21397114549523513, 0.21397114549523513, 0.28658657818789884], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.44933677], dtype=float32), -0.6532472]. 
=============================================
[2019-03-27 04:12:18,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.99709 ]
 [68.01231 ]
 [68.063614]
 [68.094406]
 [68.11407 ]], R is [[68.02030182]
 [68.05345917]
 [68.08654785]
 [68.11937714]
 [68.15120697]].
[2019-03-27 04:12:20,311] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2241237: loss 0.0055
[2019-03-27 04:12:20,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2241239: learning rate 0.0001
[2019-03-27 04:12:22,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.671076e-30 1.000000e+00 0.000000e+00 6.528543e-35 4.958264e-29], sum to 1.0000
[2019-03-27 04:12:22,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8173
[2019-03-27 04:12:22,131] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6332765027489061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884983.1045701196, 884983.1045701191, 207098.9061980101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4766400.0000, 
sim time next is 4767000.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.658356801585593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 920047.1792237106, 920047.1792237106, 212110.0031102683], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.5883816886573409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25556866089547514, 0.25556866089547514, 0.3165820941944303], 
reward next is 0.6834, 
noisyNet noise sample is [array([-0.39401284], dtype=float32), 0.24389866]. 
=============================================
[2019-03-27 04:12:22,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.114185]
 [58.69885 ]
 [58.93347 ]
 [59.30502 ]
 [59.240013]], R is [[59.14185715]
 [59.24133682]
 [59.31668472]
 [59.39564133]
 [59.48623657]].
[2019-03-27 04:12:22,317] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2242126: loss 0.0015
[2019-03-27 04:12:22,321] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2242126: learning rate 0.0001
[2019-03-27 04:12:24,768] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2243211: loss 0.1854
[2019-03-27 04:12:24,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2243214: learning rate 0.0001
[2019-03-27 04:12:24,922] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2243279: loss 0.2508
[2019-03-27 04:12:24,924] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2243280: learning rate 0.0001
[2019-03-27 04:12:24,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2196568e-19 1.0000000e+00 1.5052464e-26 1.2650566e-19 1.1366944e-11], sum to 1.0000
[2019-03-27 04:12:24,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1029
[2019-03-27 04:12:24,947] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.7939864652292251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1109687.335726188, 1109687.335726188, 242440.7364978558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4690200.0000, 
sim time next is 4690800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.8087786723191052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1130372.149146386, 1130372.149146385, 246077.820539602], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.84, 1.0, 1.0, 0.7696128582157894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31399226365177385, 0.31399226365177363, 0.36728032916358505], 
reward next is 0.6327, 
noisyNet noise sample is [array([-0.13273472], dtype=float32), -0.75422364]. 
=============================================
[2019-03-27 04:12:25,233] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2243405: loss 0.0035
[2019-03-27 04:12:25,237] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2243408: learning rate 0.0001
[2019-03-27 04:12:26,485] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2243960: loss 0.0035
[2019-03-27 04:12:26,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2243960: learning rate 0.0001
[2019-03-27 04:12:27,435] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5901697e-22 1.0000000e+00 8.0947527e-32 5.0378210e-23 3.1757377e-16], sum to 1.0000
[2019-03-27 04:12:27,443] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7017
[2019-03-27 04:12:27,448] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5069772357225876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708425.1698864554, 708425.169886456, 184696.6834716284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4669200.0000, 
sim time next is 4669800.0000, 
raw observation next is [27.0, 84.83333333333333, 1.0, 2.0, 0.5101189259999527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712816.6870251385, 712816.6870251378, 185196.9682904144], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.8483333333333333, 1.0, 1.0, 0.40978183855415984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1980046352847607, 0.1980046352847605, 0.2764133855080812], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.6321802], dtype=float32), -0.23158756]. 
=============================================
[2019-03-27 04:12:27,688] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2244500: loss 0.0061
[2019-03-27 04:12:27,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2244504: learning rate 0.0001
[2019-03-27 04:12:28,523] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2244871: loss 0.0408
[2019-03-27 04:12:28,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2244871: learning rate 0.0001
[2019-03-27 04:12:28,827] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2245013: loss 0.0024
[2019-03-27 04:12:28,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2245013: learning rate 0.0001
[2019-03-27 04:12:28,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4725225e-25 1.0000000e+00 3.6409402e-34 1.2587022e-27 1.0697686e-18], sum to 1.0000
[2019-03-27 04:12:29,000] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2245086: loss 0.0053
[2019-03-27 04:12:29,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2245086: learning rate 0.0001
[2019-03-27 04:12:29,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5002
[2019-03-27 04:12:29,019] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4847111144948532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677301.663182531, 677301.6631825304, 181235.0571620968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4846000987575116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677146.4880560861, 677146.4880560861, 181218.174291246], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37903626356326703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18809624668224614, 0.18809624668224614, 0.2704748870018597], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.13452701], dtype=float32), -0.054725274]. 
=============================================
[2019-03-27 04:12:29,477] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2245297: loss 0.0156
[2019-03-27 04:12:29,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2245297: learning rate 0.0001
[2019-03-27 04:12:30,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8837988e-33 1.0000000e+00 0.0000000e+00 1.5354649e-37 2.3556996e-31], sum to 1.0000
[2019-03-27 04:12:30,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9395
[2019-03-27 04:12:30,020] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 84.0, 1.0, 2.0, 0.4850840028576757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677822.8777297436, 677822.8777297442, 181291.6110183666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5013600.0000, 
sim time next is 5014200.0000, 
raw observation next is [26.08333333333334, 84.0, 1.0, 2.0, 0.4821069802189829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673661.6729216995, 673661.6729216989, 180839.6229727611], 
processed observation next is [0.0, 0.0, 0.43522906793049004, 0.84, 1.0, 1.0, 0.37603250628793117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18712824247824986, 0.1871282424782497, 0.2699098850339718], 
reward next is 0.7301, 
noisyNet noise sample is [array([-0.7047491], dtype=float32), -0.074181214]. 
=============================================
[2019-03-27 04:12:32,154] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2246497: loss 0.0303
[2019-03-27 04:12:32,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2246499: learning rate 0.0001
[2019-03-27 04:12:33,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5030244e-28 1.0000000e+00 0.0000000e+00 4.7866812e-31 2.5356891e-23], sum to 1.0000
[2019-03-27 04:12:33,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6636
[2019-03-27 04:12:33,042] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 82.33333333333334, 1.0, 2.0, 0.5351957552558767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747870.2284100746, 747870.2284100752, 189294.3949329485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4570800.0000, 
sim time next is 4571400.0000, 
raw observation next is [28.0, 83.16666666666666, 1.0, 2.0, 0.539813751100717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754325.6008058469, 754325.6008058469, 190069.2940472838], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.8316666666666666, 1.0, 1.0, 0.4455587362659241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20953488911273524, 0.20953488911273524, 0.28368551350340865], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.45004305], dtype=float32), -0.084202506]. 
=============================================
[2019-03-27 04:12:33,335] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2247022: loss 0.0358
[2019-03-27 04:12:33,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2247022: learning rate 0.0001
[2019-03-27 04:12:34,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6390346e-16 6.5491302e-05 2.9059952e-22 4.7040581e-14 9.9993455e-01], sum to 1.0000
[2019-03-27 04:12:34,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2414
[2019-03-27 04:12:34,731] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.6, 64.0, 1.0, 2.0, 0.5019865378793361, 1.0, 2.0, 0.5019865378793361, 1.0, 2.0, 0.8599373720682024, 6.911200000000001, 6.9112, 170.5573041426782, 2105718.592209962, 2105718.592209962, 414358.7801528477], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4975200.0000, 
sim time next is 4975800.0000, 
raw observation next is [30.63333333333334, 63.83333333333334, 1.0, 2.0, 0.5143378550182304, 1.0, 2.0, 0.5143378550182304, 1.0, 2.0, 0.8815124856792285, 6.9112, 6.9112, 170.5573041426782, 2157581.718582322, 2157581.718582322, 423003.7029564735], 
processed observation next is [1.0, 0.6086956521739131, 0.6508688783570303, 0.6383333333333334, 1.0, 1.0, 0.4148648855641329, 1.0, 1.0, 0.4148648855641329, 1.0, 1.0, 0.8555030313161323, 0.0, 0.0, 0.8375144448122397, 0.599328255161756, 0.599328255161756, 0.6313488103827963], 
reward next is 0.3687, 
noisyNet noise sample is [array([0.90700454], dtype=float32), -0.37536544]. 
=============================================
[2019-03-27 04:12:35,359] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2247922: loss 0.3189
[2019-03-27 04:12:35,362] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2247922: learning rate 0.0001
[2019-03-27 04:12:35,802] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2248120: loss 0.0144
[2019-03-27 04:12:35,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2248121: learning rate 0.0001
[2019-03-27 04:12:35,920] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2248171: loss 0.1702
[2019-03-27 04:12:35,925] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2248171: learning rate 0.0001
[2019-03-27 04:12:36,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:12:36,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5746
[2019-03-27 04:12:36,960] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5230494490516497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730891.4203920225, 730891.4203920232, 187285.3622219982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5130600.0000, 
sim time next is 5131200.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5121259760916383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715622.1910551882, 715622.1910551875, 185517.6304398416], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.41219997119474494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1987839419597745, 0.1987839419597743, 0.27689198573110685], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.507985], dtype=float32), 0.3344971]. 
=============================================
[2019-03-27 04:12:37,283] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9189611e-14 9.9984622e-01 5.6106473e-21 2.4847293e-13 1.5369761e-04], sum to 1.0000
[2019-03-27 04:12:37,293] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5798
[2019-03-27 04:12:37,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2047332.558637462 W.
[2019-03-27 04:12:37,306] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7321215908182132, 1.0, 1.0, 0.7321215908182132, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2047332.558637462, 2047332.558637462, 388178.0602035209], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4711800.0000, 
sim time next is 4712400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.6460081488701168, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.991558551175693, 6.9112, 168.9124774100128, 1799577.774671803, 1742568.814439107, 373927.7783395555], 
processed observation next is [1.0, 0.5652173913043478, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5735037938194179, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008035855117569301, 0.0, 0.8294375925499162, 0.4998827151866119, 0.48404689289975195, 0.5581011617008291], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24213754], dtype=float32), -0.3232092]. 
=============================================
[2019-03-27 04:12:38,459] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2249303: loss 5.1161
[2019-03-27 04:12:38,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2249303: learning rate 0.0001
[2019-03-27 04:12:40,023] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 04:12:40,024] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:12:40,025] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:12:40,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:12:40,027] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:12:40,027] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:12:40,026] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:12:40,029] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:12:40,029] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:12:40,031] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:12:40,031] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:12:40,056] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-27 04:12:40,083] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-27 04:12:40,084] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-27 04:12:40,084] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-27 04:12:40,113] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-27 04:12:41,235] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:12:41,236] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.55, 84.83333333333333, 1.0, 2.0, 0.2821738985598682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458816.9973463417, 458816.9973463417, 164148.3392930017]
[2019-03-27 04:12:41,238] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:12:41,240] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9368535425728298
[2019-03-27 04:12:55,684] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:12:55,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.00000000000001, 1.0, 2.0, 0.654795584693358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008446.270262785, 1008446.270262785, 222905.818017141]
[2019-03-27 04:12:55,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:12:55,690] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.5829026e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1106647e-35], sampled 0.9637318349444076
[2019-03-27 04:13:01,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:13:01,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.23333333333333, 51.00000000000001, 1.0, 2.0, 0.2807627868745183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 455597.0635380739, 455597.0635380746, 163951.2364826456]
[2019-03-27 04:13:01,614] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:13:01,616] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.609619976698186
[2019-03-27 04:13:11,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:13:11,808] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.53333333333333, 89.0, 1.0, 2.0, 0.4911684500016392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686327.6103849333, 686327.6103849333, 182223.3364288543]
[2019-03-27 04:13:11,810] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:13:11,812] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.734337e-37 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.8738014419632502
[2019-03-27 04:13:42,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:13:42,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.16666666666667, 47.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.381035183556896, 6.9112, 168.9102592247794, 1787295.147027603, 1453983.219908032, 311353.0116879437]
[2019-03-27 04:13:42,351] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:13:42,354] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6077947e-20 1.0000000e+00 7.3868899e-29 6.0817440e-20 2.7855376e-11], sampled 0.5221294319570448
[2019-03-27 04:13:42,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1787295.147027603 W.
[2019-03-27 04:13:56,127] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:13:56,128] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5188716237815977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725051.4777706177, 725051.4777706177, 186605.2996532201]
[2019-03-27 04:13:56,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:13:56,134] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.0039708e-31 1.0000000e+00 0.0000000e+00 3.6150324e-34 1.0080484e-27], sampled 0.37729428323458347
[2019-03-27 04:14:18,366] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15100744]
[2019-03-27 04:14:18,367] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.73333333333333, 95.0, 1.0, 2.0, 0.5815770692908994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812707.2037236409, 812707.2037236402, 197356.9951960758]
[2019-03-27 04:14:18,369] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:14:18,372] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.060851028141639674
[2019-03-27 04:14:35,675] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.9651 2927275069.8945 1339.0000
[2019-03-27 04:14:35,717] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.9437 2779057634.1924 927.0000
[2019-03-27 04:14:35,814] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.9753 2842048708.6929 1144.0000
[2019-03-27 04:14:36,004] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.1071 3164610337.2996 1865.0000
[2019-03-27 04:14:36,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8018.5913 3007223037.2495 1718.0000
[2019-03-27 04:14:37,077] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2250000, evaluation results [2250000.0, 7885.107140648634, 3164610337.2996287, 1865.0, 8257.965097745078, 2927275069.8945174, 1339.0, 8663.943747650854, 2779057634.1923647, 927.0, 8018.591293662564, 3007223037.249502, 1718.0, 8497.975253239365, 2842048708.692927, 1144.0]
[2019-03-27 04:14:37,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3689001e-16 9.9854761e-01 8.6233358e-23 7.0432362e-16 1.4523496e-03], sum to 1.0000
[2019-03-27 04:14:37,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6573
[2019-03-27 04:14:37,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2184221.505550161 W.
[2019-03-27 04:14:37,156] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333334, 72.0, 1.0, 2.0, 0.7810229192353078, 1.0, 2.0, 0.7810229192353078, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2184221.505550161, 2184221.505550161, 410739.3311741314], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4718400.0000, 
sim time next is 4719000.0000, 
raw observation next is [30.16666666666666, 73.5, 1.0, 2.0, 0.7856520115906532, 1.0, 2.0, 0.7856520115906532, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2197180.578909952, 2197180.578909953, 412948.2037367375], 
processed observation next is [1.0, 0.6086956521739131, 0.6287519747235385, 0.735, 1.0, 1.0, 0.7417494115550038, 1.0, 1.0, 0.7417494115550038, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6103279385860978, 0.6103279385860981, 0.6163406025921455], 
reward next is 0.3837, 
noisyNet noise sample is [array([-0.24333006], dtype=float32), -0.9142046]. 
=============================================
[2019-03-27 04:14:37,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.94687 ]
 [60.321503]
 [59.9879  ]
 [59.6044  ]
 [58.767746]], R is [[60.77024841]
 [60.54950333]
 [60.30161667]
 [60.05730057]
 [59.82014084]].
[2019-03-27 04:14:37,519] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2250206: loss 0.1282
[2019-03-27 04:14:37,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2250206: learning rate 0.0001
[2019-03-27 04:14:38,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0286289e-21 1.8535616e-12 1.7654065e-25 5.5509055e-18 1.0000000e+00], sum to 1.0000
[2019-03-27 04:14:38,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4296
[2019-03-27 04:14:38,192] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 68.5, 1.0, 2.0, 0.7206347392912351, 1.0, 2.0, 0.6809074091598801, 1.0, 2.0, 1.03, 7.00509935924238, 6.9112, 170.5573041426782, 2857129.113795102, 2789865.18318806, 527925.2792208119], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5243400.0000, 
sim time next is 5244000.0000, 
raw observation next is [31.33333333333333, 69.0, 1.0, 2.0, 0.6319072256295463, 1.0, 2.0, 0.6319072256295463, 1.0, 2.0, 1.03, 6.986987463736942, 6.9112, 170.5573041426782, 2651303.168021915, 2597013.524910223, 499837.3897802044], 
processed observation next is [1.0, 0.6956521739130435, 0.6840442338072668, 0.69, 1.0, 1.0, 0.556514729674152, 1.0, 1.0, 0.556514729674152, 1.0, 1.0, 1.0365853658536586, 0.007578746373694223, 0.0, 0.8375144448122397, 0.7364731022283098, 0.7213926458083953, 0.7460259548958275], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6542205], dtype=float32), -0.9237943]. 
=============================================
[2019-03-27 04:14:38,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.728325]
 [68.02402 ]
 [67.55092 ]
 [67.337524]
 [66.7251  ]], R is [[68.66698456]
 [67.98031616]
 [67.30051422]
 [66.62751007]
 [66.25733948]].
[2019-03-27 04:14:39,502] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2251132: loss 0.0161
[2019-03-27 04:14:39,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2251133: learning rate 0.0001
[2019-03-27 04:14:39,677] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2251211: loss 0.0080
[2019-03-27 04:14:39,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2251212: learning rate 0.0001
[2019-03-27 04:14:40,319] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2251508: loss 21.9611
[2019-03-27 04:14:40,320] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2251508: learning rate 0.0001
[2019-03-27 04:14:41,520] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2252063: loss -10.7991
[2019-03-27 04:14:41,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2252064: learning rate 0.0001
[2019-03-27 04:14:42,127] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:14:42,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5374
[2019-03-27 04:14:42,139] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 64.0, 1.0, 2.0, 0.5497218410204334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768175.986051796, 768175.9860517967, 191753.2879752476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5156400.0000, 
sim time next is 5157000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.5469941449358731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764362.9584741134, 764362.9584741141, 191286.8474243076], 
processed observation next is [0.0, 0.6956521739130435, 0.6919431279620853, 0.645, 1.0, 1.0, 0.45420981317575065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21232304402058705, 0.21232304402058724, 0.28550275734971287], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.49796176], dtype=float32), 0.2868936]. 
=============================================
[2019-03-27 04:14:42,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.41441 ]
 [67.43991 ]
 [67.477974]
 [67.43966 ]
 [67.46841 ]], R is [[67.42713928]
 [67.46666718]
 [67.50427246]
 [67.54254913]
 [67.5806427 ]].
[2019-03-27 04:14:42,669] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2252590: loss 4.3343
[2019-03-27 04:14:42,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2252590: learning rate 0.0001
[2019-03-27 04:14:43,313] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2252891: loss 0.0339
[2019-03-27 04:14:43,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2252891: learning rate 0.0001
[2019-03-27 04:14:43,682] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2253061: loss 5.0368
[2019-03-27 04:14:43,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2253061: learning rate 0.0001
[2019-03-27 04:14:43,826] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2253128: loss 8.4729
[2019-03-27 04:14:43,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2253129: learning rate 0.0001
[2019-03-27 04:14:44,177] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2253289: loss -1.5195
[2019-03-27 04:14:44,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2253290: learning rate 0.0001
[2019-03-27 04:14:46,501] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2254368: loss 0.0036
[2019-03-27 04:14:46,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2254369: learning rate 0.0001
[2019-03-27 04:14:47,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8422107e-16 9.9998069e-01 6.8078742e-21 6.1548998e-16 1.9299730e-05], sum to 1.0000
[2019-03-27 04:14:47,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6307
[2019-03-27 04:14:47,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2594483.209841512 W.
[2019-03-27 04:14:47,296] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333333, 69.0, 1.0, 2.0, 0.9275683453602099, 1.0, 2.0, 0.9275683453602099, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2594483.209841512, 2594483.209841513, 486755.5744384911], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5244000.0000, 
sim time next is 5244600.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.6087448469966578, 1.0, 2.0, 0.6087448469966578, 1.0, 1.0, 1.03, 6.941764421612807, 6.9112, 170.5573041426782, 2554021.079331841, 2532126.540849407, 491170.8597786054], 
processed observation next is [1.0, 0.6956521739130435, 0.6761453396524489, 0.695, 1.0, 1.0, 0.5286082493935635, 1.0, 1.0, 0.5286082493935635, 1.0, 0.5, 1.0365853658536586, 0.0030564421612806923, 0.0, 0.8375144448122397, 0.7094502998144002, 0.7033684835692797, 0.7330908354904558], 
reward next is 0.1141, 
noisyNet noise sample is [array([1.1107744], dtype=float32), -0.41024622]. 
=============================================
[2019-03-27 04:14:48,060] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2255087: loss -16.6061
[2019-03-27 04:14:48,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2255088: learning rate 0.0001
[2019-03-27 04:14:49,329] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2255854: loss 0.0121
[2019-03-27 04:14:49,330] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2255854: learning rate 0.0001
[2019-03-27 04:14:49,746] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2256074: loss 0.0071
[2019-03-27 04:14:49,750] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2256074: learning rate 0.0001
[2019-03-27 04:14:49,789] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2256093: loss -2.4675
[2019-03-27 04:14:49,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2256094: learning rate 0.0001
[2019-03-27 04:14:52,431] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2257267: loss 0.0117
[2019-03-27 04:14:52,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2257268: learning rate 0.0001
[2019-03-27 04:14:54,464] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2258172: loss 0.0059
[2019-03-27 04:14:54,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2258175: learning rate 0.0001
[2019-03-27 04:14:56,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2259164: loss 0.5249
[2019-03-27 04:14:56,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2259164: learning rate 0.0001
[2019-03-27 04:14:56,778] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2259205: loss 0.5782
[2019-03-27 04:14:56,781] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2259205: learning rate 0.0001
[2019-03-27 04:14:56,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.179100e-33 1.000000e+00 0.000000e+00 8.314173e-38 7.324055e-32], sum to 1.0000
[2019-03-27 04:14:56,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2220
[2019-03-27 04:14:56,897] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 63.0, 1.0, 2.0, 0.531511780117165, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742720.5266115637, 742720.5266115637, 188680.8784446958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [31.16666666666667, 63.0, 1.0, 2.0, 0.5270728645859347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736515.5528379027, 736515.5528379027, 187946.740540994], 
processed observation next is [0.0, 0.5652173913043478, 0.6761453396524489, 0.63, 1.0, 1.0, 0.4302082705854635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20458765356608408, 0.20458765356608408, 0.28051752319551343], 
reward next is 0.7195, 
noisyNet noise sample is [array([-2.7881284], dtype=float32), 0.6154544]. 
=============================================
[2019-03-27 04:14:56,915] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.72284 ]
 [71.69796 ]
 [71.68788 ]
 [71.69888 ]
 [71.713585]], R is [[71.75054932]
 [71.7514267 ]
 [71.75121307]
 [71.74991608]
 [71.74819946]].
[2019-03-27 04:14:57,221] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.0072083e-19 4.4849088e-13 2.7848962e-25 1.6073678e-15 1.0000000e+00], sum to 1.0000
[2019-03-27 04:14:57,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0188
[2019-03-27 04:14:57,237] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.6319089126376916, 1.0, 2.0, 0.6319089126376916, 1.0, 2.0, 1.03, 6.986990757639054, 6.9112, 170.5573041426782, 2651310.253739168, 2597018.251071406, 499838.305000209], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5238000.0000, 
sim time next is 5238600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6433684403416272, 1.0, 2.0, 0.6422742596850761, 1.0, 2.0, 1.03, 7.005093267235104, 6.9112, 170.5573041426782, 2694847.209118191, 2627587.642463767, 504036.8289056235], 
processed observation next is [1.0, 0.6521739130434783, 0.7156398104265403, 0.67, 1.0, 1.0, 0.570323422098346, 1.0, 1.0, 0.5690051321506941, 1.0, 1.0, 1.0365853658536586, 0.009389326723510383, 0.0, 0.8375144448122397, 0.7485686691994975, 0.7298854562399353, 0.7522937744860052], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56362987], dtype=float32), -1.3496052]. 
=============================================
[2019-03-27 04:14:57,624] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2259578: loss 0.0139
[2019-03-27 04:14:57,625] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2259578: learning rate 0.0001
[2019-03-27 04:14:58,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2260073: loss 0.0061
[2019-03-27 04:14:58,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2260073: learning rate 0.0001
[2019-03-27 04:15:00,009] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2260609: loss 0.0058
[2019-03-27 04:15:00,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2260611: learning rate 0.0001
[2019-03-27 04:15:00,473] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2260818: loss 0.1123
[2019-03-27 04:15:00,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2260819: learning rate 0.0001
[2019-03-27 04:15:01,088] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2261089: loss 0.0080
[2019-03-27 04:15:01,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2261090: learning rate 0.0001
[2019-03-27 04:15:01,200] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2261139: loss 0.0138
[2019-03-27 04:15:01,202] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2261140: learning rate 0.0001
[2019-03-27 04:15:01,759] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2261392: loss 0.0122
[2019-03-27 04:15:01,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2261392: learning rate 0.0001
[2019-03-27 04:15:03,894] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2262344: loss 0.1506
[2019-03-27 04:15:03,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2262345: learning rate 0.0001
[2019-03-27 04:15:05,595] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2263102: loss 0.1998
[2019-03-27 04:15:05,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2263102: learning rate 0.0001
[2019-03-27 04:15:07,322] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2263872: loss 0.4080
[2019-03-27 04:15:07,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2263873: learning rate 0.0001
[2019-03-27 04:15:07,638] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2264014: loss 0.4231
[2019-03-27 04:15:07,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2264014: learning rate 0.0001
[2019-03-27 04:15:07,762] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2264070: loss 0.3119
[2019-03-27 04:15:07,764] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2264070: learning rate 0.0001
[2019-03-27 04:15:10,395] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2265247: loss 0.0521
[2019-03-27 04:15:10,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2265247: learning rate 0.0001
[2019-03-27 04:15:12,578] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2266223: loss 0.1286
[2019-03-27 04:15:12,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2266224: learning rate 0.0001
[2019-03-27 04:15:13,077] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2089168e-21 8.9968547e-18 1.9977442e-26 1.2109796e-14 1.0000000e+00], sum to 1.0000
[2019-03-27 04:15:13,085] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9268
[2019-03-27 04:15:13,091] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.9, 54.0, 1.0, 2.0, 0.8288515770943371, 1.0, 2.0, 0.7350158280614312, 1.0, 2.0, 1.03, 7.005107894183323, 6.9112, 170.5573041426782, 3084451.681637937, 3017181.637105519, 565116.0755495914], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5331600.0000, 
sim time next is 5332200.0000, 
raw observation next is [35.65, 55.16666666666666, 1.0, 2.0, 0.3078660433690849, 1.0, 2.0, 0.3078660433690849, 1.0, 2.0, 0.5346615044757667, 6.9112, 6.9112, 170.5573041426782, 1290937.090043369, 1290937.090043369, 307754.8314481062], 
processed observation next is [1.0, 0.7391304347826086, 0.8886255924170615, 0.5516666666666665, 1.0, 1.0, 0.16610366670974083, 1.0, 1.0, 0.16610366670974083, 1.0, 1.0, 0.4325140298484959, 0.0, 0.0, 0.8375144448122397, 0.35859363612315803, 0.35859363612315803, 0.45933556932553166], 
reward next is 0.5407, 
noisyNet noise sample is [array([-0.24165936], dtype=float32), 1.585671]. 
=============================================
[2019-03-27 04:15:14,661] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2267135: loss 0.0908
[2019-03-27 04:15:14,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2267135: learning rate 0.0001
[2019-03-27 04:15:14,743] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2267167: loss 0.0928
[2019-03-27 04:15:14,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2267167: learning rate 0.0001
[2019-03-27 04:15:15,761] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2267623: loss -0.0791
[2019-03-27 04:15:15,763] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2267623: learning rate 0.0001
[2019-03-27 04:15:16,912] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2268134: loss -42.2593
[2019-03-27 04:15:16,918] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2268135: learning rate 0.0001
[2019-03-27 04:15:17,960] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2268602: loss 0.8497
[2019-03-27 04:15:17,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2268604: learning rate 0.0001
[2019-03-27 04:15:18,364] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2268776: loss 0.2324
[2019-03-27 04:15:18,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2268777: learning rate 0.0001
[2019-03-27 04:15:18,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2876311e-24 1.0000000e+00 3.0067952e-32 6.5767408e-26 9.7559297e-20], sum to 1.0000
[2019-03-27 04:15:18,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3948
[2019-03-27 04:15:18,420] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 95.0, 1.0, 2.0, 0.8046205679501416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1124557.586466148, 1124557.586466147, 245046.9223346355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5889600.0000, 
sim time next is 5890200.0000, 
raw observation next is [25.68333333333333, 95.16666666666667, 1.0, 2.0, 0.6947619233359846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970946.2365854465, 970946.2365854458, 219717.5590062633], 
processed observation next is [1.0, 0.17391304347826086, 0.41627172195892564, 0.9516666666666667, 1.0, 1.0, 0.6322432811276923, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26970728794040183, 0.2697072879404016, 0.3279366552332288], 
reward next is 0.6721, 
noisyNet noise sample is [array([0.5403194], dtype=float32), -0.34428406]. 
=============================================
[2019-03-27 04:15:19,188] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2269148: loss 0.7661
[2019-03-27 04:15:19,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2269148: learning rate 0.0001
[2019-03-27 04:15:19,250] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2269176: loss 0.8229
[2019-03-27 04:15:19,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2269177: learning rate 0.0001
[2019-03-27 04:15:19,556] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2269310: loss 0.4016
[2019-03-27 04:15:19,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2269313: learning rate 0.0001
[2019-03-27 04:15:19,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6567045e-18 3.8187736e-08 1.7562655e-24 8.9724403e-13 1.0000000e+00], sum to 1.0000
[2019-03-27 04:15:19,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6431
[2019-03-27 04:15:19,944] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.9, 71.0, 1.0, 2.0, 0.740194037774532, 1.0, 2.0, 0.6906870584015286, 1.0, 2.0, 1.03, 7.005100901632516, 6.9112, 170.5573041426782, 2898212.725096152, 2830947.689612315, 534324.1357831956], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5418000.0000, 
sim time next is 5418600.0000, 
raw observation next is [30.9, 71.66666666666667, 1.0, 2.0, 0.2866932548271915, 1.0, 2.0, 0.2866932548271915, 1.0, 2.0, 0.4978913727266647, 6.9112, 6.9112, 170.5573041426782, 1202106.045943506, 1202106.045943506, 299040.601895802], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7166666666666667, 1.0, 1.0, 0.1405942829243271, 1.0, 1.0, 0.1405942829243271, 1.0, 1.0, 0.38767240576422524, 0.0, 0.0, 0.8375144448122397, 0.3339183460954184, 0.3339183460954184, 0.4463292565608985], 
reward next is 0.5537, 
noisyNet noise sample is [array([-0.13622716], dtype=float32), 1.4808567]. 
=============================================
[2019-03-27 04:15:21,795] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2270313: loss 0.2607
[2019-03-27 04:15:21,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2270313: learning rate 0.0001
[2019-03-27 04:15:23,636] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2271128: loss 65.1228
[2019-03-27 04:15:23,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2271128: learning rate 0.0001
[2019-03-27 04:15:25,247] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2271851: loss 0.3954
[2019-03-27 04:15:25,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2271851: learning rate 0.0001
[2019-03-27 04:15:25,570] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2271991: loss 0.7178
[2019-03-27 04:15:25,572] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2271992: learning rate 0.0001
[2019-03-27 04:15:25,849] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2272114: loss 40.4070
[2019-03-27 04:15:25,853] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2272114: learning rate 0.0001
[2019-03-27 04:15:27,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4593966e-19 1.0000000e+00 1.3314643e-27 9.2086936e-19 1.6801225e-09], sum to 1.0000
[2019-03-27 04:15:27,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2542
[2019-03-27 04:15:27,911] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 84.66666666666667, 1.0, 2.0, 0.9078035022282398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1268854.773628794, 1268854.773628793, 272094.4613652963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5556000.0000, 
sim time next is 5556600.0000, 
raw observation next is [28.25, 84.0, 1.0, 2.0, 0.9526288836298518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331547.379096038, 1331547.379096038, 284826.1462349914], 
processed observation next is [1.0, 0.30434782608695654, 0.537914691943128, 0.84, 1.0, 1.0, 0.9429263658190985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.36987427197112166, 0.36987427197112166, 0.42511365109700205], 
reward next is 0.5749, 
noisyNet noise sample is [array([0.5187922], dtype=float32), 0.46710265]. 
=============================================
[2019-03-27 04:15:28,400] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2273250: loss 0.1671
[2019-03-27 04:15:28,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2273251: learning rate 0.0001
[2019-03-27 04:15:29,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8468081e-26 1.0000000e+00 3.6182296e-36 5.1168269e-29 9.5073749e-22], sum to 1.0000
[2019-03-27 04:15:29,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0160
[2019-03-27 04:15:29,177] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 90.83333333333334, 1.0, 2.0, 0.5259481155828085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734943.3191541422, 734943.3191541422, 187761.2183691676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5615400.0000, 
sim time next is 5616000.0000, 
raw observation next is [26.3, 91.0, 1.0, 2.0, 0.5255056797706216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734324.8595977512, 734324.8595977512, 187688.4802586822], 
processed observation next is [0.0, 0.0, 0.4454976303317536, 0.91, 1.0, 1.0, 0.4283200961091826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20397912766604198, 0.20397912766604198, 0.28013206008758534], 
reward next is 0.7199, 
noisyNet noise sample is [array([1.7905895], dtype=float32), 0.3399858]. 
=============================================
[2019-03-27 04:15:29,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.55514 ]
 [73.52875 ]
 [73.523506]
 [73.51863 ]
 [73.49361 ]], R is [[71.78172302]
 [71.78366089]
 [71.78552246]
 [71.7871933 ]
 [71.78842163]].
[2019-03-27 04:15:30,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8292970e-14 3.0756152e-01 1.1982277e-20 1.9628096e-10 6.9243854e-01], sum to 1.0000
[2019-03-27 04:15:30,016] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8305
[2019-03-27 04:15:30,021] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.2, 78.0, 1.0, 2.0, 0.5151789663869464, 1.0, 2.0, 0.5151789663869464, 1.0, 2.0, 0.8946954923265005, 6.9112, 6.9112, 170.5573041426782, 2161113.632846708, 2161113.632846708, 425801.9890810646], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5907600.0000, 
sim time next is 5908200.0000, 
raw observation next is [30.35, 77.33333333333333, 1.0, 2.0, 0.5434907572759269, 1.0, 2.0, 0.5434907572759269, 1.0, 2.0, 0.9438637102483398, 6.9112, 6.9112, 170.5573041426782, 2279994.34371901, 2279994.34371901, 446524.3876894973], 
processed observation next is [1.0, 0.391304347826087, 0.637440758293839, 0.7733333333333333, 1.0, 1.0, 0.4499888641878637, 1.0, 1.0, 0.4499888641878637, 1.0, 1.0, 0.931541110058951, 0.0, 0.0, 0.8375144448122397, 0.6333317621441694, 0.6333317621441694, 0.6664543099843243], 
reward next is 0.3335, 
noisyNet noise sample is [array([0.72717917], dtype=float32), 1.5026412]. 
=============================================
[2019-03-27 04:15:30,340] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2274125: loss 0.1520
[2019-03-27 04:15:30,342] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2274126: learning rate 0.0001
[2019-03-27 04:15:30,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4755465e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1329240e-37], sum to 1.0000
[2019-03-27 04:15:30,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3142
[2019-03-27 04:15:30,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 91.33333333333334, 1.0, 2.0, 0.6156870231956513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 860392.4285413763, 860392.4285413756, 203698.7310376298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5982000.0000, 
sim time next is 5982600.0000, 
raw observation next is [26.65, 90.66666666666666, 1.0, 2.0, 0.6207729797828606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867502.7060887204, 867502.7060887197, 204673.5221518826], 
processed observation next is [1.0, 0.21739130434782608, 0.462085308056872, 0.9066666666666666, 1.0, 1.0, 0.5430999756420007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24097297391353342, 0.24097297391353323, 0.30548286888340687], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.09805946], dtype=float32), -0.51883566]. 
=============================================
[2019-03-27 04:15:32,310] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-27 04:15:32,314] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:15:32,315] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:15:32,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:15:32,316] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:15:32,316] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:15:32,317] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:15:32,318] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:15:32,319] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:15:32,320] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:15:32,320] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:15:32,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-27 04:15:32,379] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-27 04:15:32,400] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-27 04:15:32,401] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-27 04:15:32,402] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-27 04:15:35,606] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:15:35,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.46666666666667, 48.33333333333333, 1.0, 2.0, 0.2696469000663331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 444421.7228460534, 444421.7228460534, 162889.8281713623]
[2019-03-27 04:15:35,609] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:35,612] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7871479801026939
[2019-03-27 04:15:48,638] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:15:48,640] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.2, 95.33333333333333, 1.0, 2.0, 0.2901553888209243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465414.2904711285, 465414.2904711285, 164636.3037502427]
[2019-03-27 04:15:48,642] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:48,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45666673046419326
[2019-03-27 04:15:51,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:15:51,089] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.56666666666667, 83.66666666666667, 1.0, 2.0, 0.2429532329118352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401637.3796628706, 401637.3796628706, 160141.5883826435]
[2019-03-27 04:15:51,091] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:15:51,096] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0631961698243575
[2019-03-27 04:15:54,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:15:54,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.46666666666667, 88.33333333333334, 1.0, 2.0, 0.3831691465845746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 577820.2629453858, 577820.2629453852, 172701.2652246524]
[2019-03-27 04:15:54,005] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:15:54,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5448160863386109
[2019-03-27 04:16:12,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:16:12,784] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.76541263, 88.84586873, 1.0, 2.0, 0.3376283390598541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530865.7923827062, 530865.7923827068, 169350.104611596]
[2019-03-27 04:16:12,786] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:16:12,788] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6225917343932237
[2019-03-27 04:16:44,179] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:16:44,182] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.45, 82.00000000000001, 1.0, 2.0, 0.6308363294428004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 881571.6273099186, 881571.6273099186, 206631.2309251413]
[2019-03-27 04:16:44,183] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:16:44,185] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7579707e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7629461e-37], sampled 0.03509212975913156
[2019-03-27 04:16:47,704] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15995356]
[2019-03-27 04:16:47,705] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.981651055, 61.93840478, 1.0, 2.0, 0.5449765746850477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761542.6197169274, 761542.6197169281, 190943.2904926376]
[2019-03-27 04:16:47,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:16:47,712] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4023872760211895
[2019-03-27 04:17:27,850] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:17:28,073] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:17:28,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.4136 2842425198.9829 1132.0000
[2019-03-27 04:17:28,101] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:17:28,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.0972 3163982779.7066 1777.0000
[2019-03-27 04:17:29,125] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2275000, evaluation results [2275000.0, 7885.097150888136, 3163982779.7066116, 1777.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.413602286724, 2842425198.982875, 1132.0]
[2019-03-27 04:17:29,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:17:29,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4635
[2019-03-27 04:17:29,216] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4998017135289544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698395.1503723983, 698395.1503723983, 183564.7280372388], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5631600.0000, 
sim time next is 5632200.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4993119367968658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697710.5388176047, 697710.5388176052, 183488.0833109441], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 1.0, 1.0, 0.39676136963477804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19380848300489018, 0.19380848300489034, 0.2738628109118568], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.3209172], dtype=float32), -1.8151804]. 
=============================================
[2019-03-27 04:17:29,475] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2275163: loss 46.6185
[2019-03-27 04:17:29,479] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2275163: learning rate 0.0001
[2019-03-27 04:17:29,551] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2275199: loss 7.3740
[2019-03-27 04:17:29,553] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2275199: learning rate 0.0001
[2019-03-27 04:17:30,286] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2275542: loss 0.3215
[2019-03-27 04:17:30,287] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2275542: learning rate 0.0001
[2019-03-27 04:17:31,478] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2276095: loss 0.8089
[2019-03-27 04:17:31,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2276095: learning rate 0.0001
[2019-03-27 04:17:32,446] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2276544: loss 0.3844
[2019-03-27 04:17:32,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2276546: learning rate 0.0001
[2019-03-27 04:17:33,204] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2276894: loss -69.4434
[2019-03-27 04:17:33,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2276894: learning rate 0.0001
[2019-03-27 04:17:33,211] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3224997e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 04:17:33,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4300
[2019-03-27 04:17:33,223] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5141382698666679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718435.0322735875, 718435.0322735881, 185840.5033543064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5703000.0000, 
sim time next is 5703600.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5148567721783132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719439.3769543099, 719439.3769543099, 185956.1199489629], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41549008696182305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19984427137619717, 0.19984427137619717, 0.27754644768501924], 
reward next is 0.7225, 
noisyNet noise sample is [array([-0.5885866], dtype=float32), -0.21036476]. 
=============================================
[2019-03-27 04:17:33,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1470795e-18 1.0000000e+00 6.6775648e-24 2.1845234e-17 5.4885421e-09], sum to 1.0000
[2019-03-27 04:17:33,547] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6841
[2019-03-27 04:17:33,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2321795.672682328 W.
[2019-03-27 04:17:33,560] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2277054: loss 0.5607
[2019-03-27 04:17:33,565] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2277054: learning rate 0.0001
[2019-03-27 04:17:33,567] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.9, 72.0, 1.0, 2.0, 0.8301687726308448, 1.0, 2.0, 0.8301687726308448, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2321795.672682328, 2321795.672682328, 434822.8962234743], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6190200.0000, 
sim time next is 6190800.0000, 
raw observation next is [29.8, 72.33333333333333, 1.0, 2.0, 0.9703221443545629, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.991733636456333, 6.9112, 168.9124776560352, 2253463.171500736, 2196330.000013079, 454841.3471011581], 
processed observation next is [1.0, 0.6521739130434783, 0.6113744075829385, 0.7233333333333333, 1.0, 1.0, 0.964243547415136, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008053363645633294, 0.0, 0.8294375937579989, 0.6259619920835378, 0.6100916666702997, 0.6788676822405345], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.50910217], dtype=float32), -0.9535688]. 
=============================================
[2019-03-27 04:17:33,669] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2277107: loss 0.5561
[2019-03-27 04:17:33,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2277107: learning rate 0.0001
[2019-03-27 04:17:33,788] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2277165: loss 0.7413
[2019-03-27 04:17:33,791] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2277166: learning rate 0.0001
[2019-03-27 04:17:34,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5349353e-30 1.0000000e+00 4.4789703e-38 4.9018509e-34 3.5783128e-29], sum to 1.0000
[2019-03-27 04:17:34,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5929
[2019-03-27 04:17:34,978] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 84.0, 1.0, 2.0, 0.751059915969747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049662.897646974, 1049662.897646974, 232255.1746011922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5990400.0000, 
sim time next is 5991000.0000, 
raw observation next is [28.86666666666667, 83.16666666666667, 1.0, 2.0, 0.8239101206838724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1151531.760519927, 1151531.760519927, 249867.2128934293], 
processed observation next is [1.0, 0.34782608695652173, 0.567140600315956, 0.8316666666666667, 1.0, 1.0, 0.7878435188962317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3198699334777575, 0.3198699334777575, 0.3729361386469094], 
reward next is 0.6271, 
noisyNet noise sample is [array([-1.4587135], dtype=float32), 0.5993555]. 
=============================================
[2019-03-27 04:17:34,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[42.493855]
 [42.369747]
 [42.624626]
 [42.56128 ]
 [42.6659  ]], R is [[42.23519135]
 [42.46619034]
 [42.69497299]
 [42.92419434]
 [43.14983749]].
[2019-03-27 04:17:36,306] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2278331: loss 59.9827
[2019-03-27 04:17:36,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2278332: learning rate 0.0001
[2019-03-27 04:17:38,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2279169: loss 2.7098
[2019-03-27 04:17:38,135] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2279170: learning rate 0.0001
[2019-03-27 04:17:39,797] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2279940: loss 64.6046
[2019-03-27 04:17:39,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2279940: learning rate 0.0001
[2019-03-27 04:17:40,265] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2280162: loss 41.8012
[2019-03-27 04:17:40,269] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2280164: learning rate 0.0001
[2019-03-27 04:17:40,380] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2305236e-22 1.0000000e+00 1.5906417e-31 1.1158510e-26 2.6245255e-13], sum to 1.0000
[2019-03-27 04:17:40,390] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3315
[2019-03-27 04:17:40,394] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 80.0, 1.0, 2.0, 0.5675700002849948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793126.1428223223, 793126.1428223223, 194861.2474477109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5940000.0000, 
sim time next is 5940600.0000, 
raw observation next is [29.68333333333334, 80.33333333333334, 1.0, 2.0, 0.5710748388473635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798025.667698816, 798025.667698816, 195481.9636586449], 
processed observation next is [1.0, 0.782608695652174, 0.6058451816745659, 0.8033333333333335, 1.0, 1.0, 0.48322269740646207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22167379658300446, 0.22167379658300446, 0.2917641248636491], 
reward next is 0.7082, 
noisyNet noise sample is [array([-0.47555628], dtype=float32), -0.8514619]. 
=============================================
[2019-03-27 04:17:40,492] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2280263: loss 3.1425
[2019-03-27 04:17:40,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2280263: learning rate 0.0001
[2019-03-27 04:17:42,276] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2281272: loss 45.5818
[2019-03-27 04:17:42,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2281272: learning rate 0.0001
[2019-03-27 04:17:44,096] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2282096: loss 0.4650
[2019-03-27 04:17:44,100] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2282097: learning rate 0.0001
[2019-03-27 04:17:46,727] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2283267: loss 2.9884
[2019-03-27 04:17:46,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2283267: learning rate 0.0001
[2019-03-27 04:17:46,794] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2283298: loss 2.7962
[2019-03-27 04:17:46,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2283298: learning rate 0.0001
[2019-03-27 04:17:46,962] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2283373: loss 26.8687
[2019-03-27 04:17:46,965] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2283373: learning rate 0.0001
[2019-03-27 04:17:48,483] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2284051: loss 23.8593
[2019-03-27 04:17:48,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2284051: learning rate 0.0001
[2019-03-27 04:17:49,563] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2284530: loss 7.4811
[2019-03-27 04:17:49,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2284530: learning rate 0.0001
[2019-03-27 04:17:50,270] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2284849: loss 0.7651
[2019-03-27 04:17:50,274] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2284849: learning rate 0.0001
[2019-03-27 04:17:50,651] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2285017: loss -43.2354
[2019-03-27 04:17:50,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2285019: learning rate 0.0001
[2019-03-27 04:17:50,825] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2285094: loss 14.8973
[2019-03-27 04:17:50,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2285095: learning rate 0.0001
[2019-03-27 04:17:51,012] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2285177: loss -14.4876
[2019-03-27 04:17:51,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2285177: learning rate 0.0001
[2019-03-27 04:17:53,289] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2286192: loss 5.2044
[2019-03-27 04:17:53,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2286192: learning rate 0.0001
[2019-03-27 04:17:53,398] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8899596e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7666469e-36], sum to 1.0000
[2019-03-27 04:17:53,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8068
[2019-03-27 04:17:53,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.0, 1.0, 2.0, 0.8073139519324175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1128323.926483824, 1128323.926483823, 245713.4375419818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6057600.0000, 
sim time next is 6058200.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.7618482518234919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1064747.969854066, 1064747.969854066, 234760.9513913244], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 0.7130701829198698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2957633249594628, 0.2957633249594628, 0.3503894796885439], 
reward next is 0.6496, 
noisyNet noise sample is [array([0.17826523], dtype=float32), -0.9843649]. 
=============================================
[2019-03-27 04:17:55,621] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2287226: loss -21.2762
[2019-03-27 04:17:55,623] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2287226: learning rate 0.0001
[2019-03-27 04:17:55,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3353456e-19 9.4708998e-12 3.3009451e-25 6.4127689e-16 1.0000000e+00], sum to 1.0000
[2019-03-27 04:17:55,991] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9418
[2019-03-27 04:17:55,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.33333333333334, 1.0, 2.0, 0.6185452064412394, 1.0, 2.0, 0.6185452064412394, 1.0, 2.0, 1.03, 6.958159676386282, 6.9112, 170.5573041426782, 2595181.706215639, 2561542.579822639, 494915.9013383458], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6099600.0000, 
sim time next is 6100200.0000, 
raw observation next is [30.95, 65.5, 1.0, 2.0, 0.6211328733439261, 1.0, 2.0, 0.6211328733439261, 1.0, 2.0, 1.03, 6.963149875076406, 6.9112, 170.5573041426782, 2606049.900403398, 2568836.091700359, 495885.9780398524], 
processed observation next is [1.0, 0.6086956521739131, 0.6658767772511848, 0.655, 1.0, 1.0, 0.5435335823420796, 1.0, 1.0, 0.5435335823420796, 1.0, 1.0, 1.0365853658536586, 0.005194987507640558, 0.0, 0.8375144448122397, 0.7239027501120551, 0.7135655810278775, 0.7401283254326155], 
reward next is 0.0001, 
noisyNet noise sample is [array([-1.125108], dtype=float32), 1.6225941]. 
=============================================
[2019-03-27 04:17:57,208] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2287931: loss 3.6349
[2019-03-27 04:17:57,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2287931: learning rate 0.0001
[2019-03-27 04:17:57,491] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4052362e-24 1.0000000e+00 7.8396855e-33 1.1413886e-27 3.4075320e-19], sum to 1.0000
[2019-03-27 04:17:57,500] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0081
[2019-03-27 04:17:57,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1800415.887093655 W.
[2019-03-27 04:17:57,516] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.66666666666667, 92.0, 1.0, 2.0, 0.6438988624880327, 1.0, 1.0, 0.6438988624880327, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1800415.887093655, 1800415.887093656, 351032.0641389362], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6142800.0000, 
sim time next is 6143400.0000, 
raw observation next is [26.65, 92.0, 1.0, 2.0, 0.501212984407451, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8579971479779009, 6.911200000000001, 6.9112, 168.9129561858748, 1401197.7314545, 1401197.731454499, 306537.3999890592], 
processed observation next is [1.0, 0.08695652173913043, 0.462085308056872, 0.92, 1.0, 1.0, 0.399051788442712, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.8268257902169523, 8.881784197001253e-17, 0.0, 0.8294399435585832, 0.38922159207069446, 0.3892215920706942, 0.457518507446357], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4318902], dtype=float32), 1.0183274]. 
=============================================
[2019-03-27 04:17:57,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1020023e-28 1.0000000e+00 9.7067653e-38 9.3832643e-34 2.3193241e-27], sum to 1.0000
[2019-03-27 04:17:57,598] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7928
[2019-03-27 04:17:57,604] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.0, 1.0, 2.0, 0.725539458453854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1013979.104788194, 1013979.104788194, 226453.5873591808], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6494400.0000, 
sim time next is 6495000.0000, 
raw observation next is [26.18333333333333, 92.16666666666667, 1.0, 2.0, 0.6882910113865879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 961898.8885065397, 961898.8885065403, 218337.324987471], 
processed observation next is [1.0, 0.17391304347826086, 0.4399684044233806, 0.9216666666666667, 1.0, 1.0, 0.6244470016705878, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26719413569626105, 0.2671941356962612, 0.3258766044589119], 
reward next is 0.6741, 
noisyNet noise sample is [array([0.81103855], dtype=float32), 0.8476436]. 
=============================================
[2019-03-27 04:17:57,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.39291 ]
 [57.494164]
 [57.563988]
 [57.825947]
 [58.16949 ]], R is [[57.72224426]
 [57.80703354]
 [57.88728333]
 [57.96060562]
 [58.02939224]].
[2019-03-27 04:17:57,644] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2288124: loss 2.2069
[2019-03-27 04:17:57,647] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2288124: learning rate 0.0001
[2019-03-27 04:17:58,177] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2288360: loss -10.3905
[2019-03-27 04:17:58,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2288361: learning rate 0.0001
[2019-03-27 04:17:59,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3927348e-18 9.9999952e-01 1.2361526e-25 1.2785631e-18 4.4824060e-07], sum to 1.0000
[2019-03-27 04:17:59,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5494
[2019-03-27 04:17:59,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1891083.689849643 W.
[2019-03-27 04:17:59,163] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.58333333333334, 84.83333333333334, 1.0, 2.0, 0.450864407270105, 1.0, 2.0, 0.450864407270105, 1.0, 2.0, 0.777584420318927, 6.9112, 6.9112, 170.5573041426782, 1891083.689849643, 1891083.689849643, 381960.4663985565], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6627000.0000, 
sim time next is 6627600.0000, 
raw observation next is [27.5, 85.0, 1.0, 2.0, 0.7553737587577377, 1.0, 2.0, 0.7553737587577377, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2112419.913388797, 2112419.913388797, 398727.1557511501], 
processed observation next is [1.0, 0.7391304347826086, 0.5023696682464456, 0.85, 1.0, 1.0, 0.7052695888647442, 1.0, 1.0, 0.7052695888647442, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5867833092746658, 0.5867833092746658, 0.5951151578375374], 
reward next is 0.4049, 
noisyNet noise sample is [array([-0.45898938], dtype=float32), 0.29800963]. 
=============================================
[2019-03-27 04:18:00,249] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2289282: loss 1.6294
[2019-03-27 04:18:00,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2289282: learning rate 0.0001
[2019-03-27 04:18:01,791] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:18:01,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8813
[2019-03-27 04:18:01,803] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.01666666666667, 94.16666666666667, 1.0, 2.0, 0.5159751547977751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721002.6885459501, 721002.6885459494, 186135.2218646462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6673800.0000, 
sim time next is 6674400.0000, 
raw observation next is [25.1, 94.0, 1.0, 2.0, 0.5193463287648424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725715.0390567803, 725715.0390567797, 186680.9908676649], 
processed observation next is [1.0, 0.2608695652173913, 0.38862559241706174, 0.94, 1.0, 1.0, 0.4208991912829427, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20158751084910564, 0.20158751084910548, 0.2786283445786043], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.32482302], dtype=float32), 0.15156691]. 
=============================================
[2019-03-27 04:18:02,128] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2290126: loss 2.5303
[2019-03-27 04:18:02,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2290126: learning rate 0.0001
[2019-03-27 04:18:04,765] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2291276: loss 0.4091
[2019-03-27 04:18:04,769] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2291276: learning rate 0.0001
[2019-03-27 04:18:04,810] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2291299: loss 1.8187
[2019-03-27 04:18:04,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2291301: learning rate 0.0001
[2019-03-27 04:18:04,982] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2291374: loss 44.4080
[2019-03-27 04:18:04,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2291374: learning rate 0.0001
[2019-03-27 04:18:06,263] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2291940: loss 0.6189
[2019-03-27 04:18:06,266] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2291941: learning rate 0.0001
[2019-03-27 04:18:07,594] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2292533: loss 1.2935
[2019-03-27 04:18:07,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2292534: learning rate 0.0001
[2019-03-27 04:18:08,307] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2292849: loss 95.6835
[2019-03-27 04:18:08,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2292850: learning rate 0.0001
[2019-03-27 04:18:08,686] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2293019: loss 2.8688
[2019-03-27 04:18:08,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2293019: learning rate 0.0001
[2019-03-27 04:18:08,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2293072: loss 2.2758
[2019-03-27 04:18:08,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2293072: learning rate 0.0001
[2019-03-27 04:18:08,956] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2293139: loss 2.4236
[2019-03-27 04:18:08,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2293140: learning rate 0.0001
[2019-03-27 04:18:11,744] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2294391: loss 1.3262
[2019-03-27 04:18:11,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2294391: learning rate 0.0001
[2019-03-27 04:18:13,432] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2295144: loss 9.8520
[2019-03-27 04:18:13,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2295145: learning rate 0.0001
[2019-03-27 04:18:15,424] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2296035: loss -15.9522
[2019-03-27 04:18:15,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2296035: learning rate 0.0001
[2019-03-27 04:18:15,847] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2296223: loss 13.0011
[2019-03-27 04:18:15,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2296223: learning rate 0.0001
[2019-03-27 04:18:15,942] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2296268: loss 1.8629
[2019-03-27 04:18:15,944] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2296269: learning rate 0.0001
[2019-03-27 04:18:17,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.6506802e-25 1.0000000e+00 2.2724157e-36 7.0803039e-31 1.9437986e-19], sum to 1.0000
[2019-03-27 04:18:17,526] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4691
[2019-03-27 04:18:17,532] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 85.0, 1.0, 2.0, 0.514325485959629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718696.72860822, 718696.7286082207, 185871.1614102765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6637800.0000, 
sim time next is 6638400.0000, 
raw observation next is [27.0, 85.0, 1.0, 2.0, 0.5135239074215581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717576.2582617819, 717576.2582617819, 185742.2329555833], 
processed observation next is [1.0, 0.8695652173913043, 0.4786729857819906, 0.85, 1.0, 1.0, 0.41388422580910605, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19932673840605053, 0.19932673840605053, 0.27722721336654227], 
reward next is 0.7228, 
noisyNet noise sample is [array([1.1723373], dtype=float32), 0.017138747]. 
=============================================
[2019-03-27 04:18:18,383] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2297348: loss -1.1670
[2019-03-27 04:18:18,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2297351: learning rate 0.0001
[2019-03-27 04:18:20,212] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2298156: loss 8.1552
[2019-03-27 04:18:20,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2298156: learning rate 0.0001
[2019-03-27 04:18:22,660] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2299240: loss 3.6773
[2019-03-27 04:18:22,662] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2299241: learning rate 0.0001
[2019-03-27 04:18:22,783] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2299295: loss 8.8303
[2019-03-27 04:18:22,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2299295: learning rate 0.0001
[2019-03-27 04:18:22,855] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2299324: loss 8.3307
[2019-03-27 04:18:22,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2299324: learning rate 0.0001
[2019-03-27 04:18:24,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2299935: loss 0.0939
[2019-03-27 04:18:24,219] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2299935: learning rate 0.0001
[2019-03-27 04:18:24,369] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-27 04:18:24,371] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:18:24,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:18:24,372] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:18:24,373] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:18:24,374] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:18:24,373] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:18:24,374] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:18:24,377] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:18:24,377] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:18:24,378] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:18:24,405] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-27 04:18:24,430] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-27 04:18:24,448] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-27 04:18:24,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-27 04:18:24,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-27 04:18:31,255] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:18:31,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.84577509666666, 76.24678257333333, 1.0, 2.0, 0.279570616029273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 456083.4902086437, 456083.4902086443, 163923.404693986]
[2019-03-27 04:18:31,259] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:18:31,264] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.38543272578744037
[2019-03-27 04:18:39,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:18:39,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.57374767833333, 93.70500456333335, 1.0, 2.0, 0.3507765026424214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544907.3573852736, 544907.3573852743, 170343.4118023678]
[2019-03-27 04:18:39,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:18:39,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.19936880589345896
[2019-03-27 04:18:47,301] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:18:47,303] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.32989864, 86.68226265, 1.0, 2.0, 0.3003652557195564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481716.2226617856, 481716.2226617862, 165779.9537194512]
[2019-03-27 04:18:47,304] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:18:47,306] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6398584301121368
[2019-03-27 04:18:56,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:18:56,868] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.416819254629796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614472.0860957283, 614472.086095729, 175681.6144706148]
[2019-03-27 04:18:56,871] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:18:56,873] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9732248e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8726027e-36], sampled 0.02316241968095989
[2019-03-27 04:19:41,731] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:19:41,733] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.355890295, 87.13680035833333, 1.0, 2.0, 0.5036660040710291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703841.5715404736, 703841.5715404736, 184175.9552269502]
[2019-03-27 04:19:41,734] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:19:41,736] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.898675e-32 1.000000e+00 0.000000e+00 4.047374e-36 4.505845e-29], sampled 0.6220630264212033
[2019-03-27 04:19:55,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:19:55,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.813242475, 92.97821306499999, 1.0, 2.0, 0.5493647864344579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767676.8611256193, 767676.8611256198, 191689.2490885375]
[2019-03-27 04:19:55,748] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:19:55,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6062035582210388
[2019-03-27 04:20:10,450] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15162179]
[2019-03-27 04:20:10,451] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.92096589666667, 86.91203847166666, 1.0, 2.0, 0.3797903585086949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568526.0604939738, 568526.0604939738, 171742.6381739937]
[2019-03-27 04:20:10,452] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:20:10,455] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.030236872817213212
[2019-03-27 04:20:16,129] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8512.7159 2841315538.5293 1111.0000
[2019-03-27 04:20:16,400] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.7120 2927086375.7111 1317.0000
[2019-03-27 04:20:16,489] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7917.9360 3162494691.3305 1807.0000
[2019-03-27 04:20:16,620] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.8000 2778990388.3128 925.0000
[2019-03-27 04:20:16,680] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8053.2527 3004762938.2334 1611.0000
[2019-03-27 04:20:17,697] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2300000, evaluation results [2300000.0, 7917.936033382501, 3162494691.330502, 1807.0, 8263.712037985732, 2927086375.7111254, 1317.0, 8663.800032876408, 2778990388.312817, 925.0, 8053.252743232445, 3004762938.2334085, 1611.0, 8512.715908830085, 2841315538.5293107, 1111.0]
[2019-03-27 04:20:18,956] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2300595: loss 6.0266
[2019-03-27 04:20:18,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2300595: learning rate 0.0001
[2019-03-27 04:20:19,467] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2300829: loss 10.8157
[2019-03-27 04:20:19,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2300830: learning rate 0.0001
[2019-03-27 04:20:19,864] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2301010: loss 2.9834
[2019-03-27 04:20:19,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2301011: learning rate 0.0001
[2019-03-27 04:20:19,987] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2301072: loss 15.0690
[2019-03-27 04:20:19,989] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2301072: learning rate 0.0001
[2019-03-27 04:20:20,180] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2301158: loss 4.3058
[2019-03-27 04:20:20,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2301158: learning rate 0.0001
[2019-03-27 04:20:22,537] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2302257: loss 9.2319
[2019-03-27 04:20:22,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2302258: learning rate 0.0001
[2019-03-27 04:20:23,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:23,872] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4498
[2019-03-27 04:20:23,878] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.88333333333334, 63.16666666666667, 1.0, 2.0, 0.399229534664357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591997.600673984, 591997.6006739847, 173689.046321372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6988200.0000, 
sim time next is 6988800.0000, 
raw observation next is [27.76666666666667, 64.33333333333334, 1.0, 2.0, 0.4067242851163128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601176.8986169929, 601176.8986169929, 174480.027484207], 
processed observation next is [0.0, 0.9130434782608695, 0.515007898894155, 0.6433333333333334, 1.0, 1.0, 0.28520998206784676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669935829491647, 0.1669935829491647, 0.2604179514689657], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.23297672], dtype=float32), 0.27383217]. 
=============================================
[2019-03-27 04:20:24,552] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2303190: loss 83.2013
[2019-03-27 04:20:24,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2303190: learning rate 0.0001
[2019-03-27 04:20:24,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7151832e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0314758e-30], sum to 1.0000
[2019-03-27 04:20:24,797] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6941
[2019-03-27 04:20:24,805] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 74.0, 1.0, 2.0, 0.3480497775111033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538613.2762694726, 538613.2762694726, 169773.31635188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6737400.0000, 
sim time next is 6738000.0000, 
raw observation next is [24.4, 74.66666666666667, 1.0, 2.0, 0.3468501450369242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537429.8862201667, 537429.8862201673, 169695.2257226839], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.7466666666666667, 1.0, 1.0, 0.21307246389990867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14928607950560185, 0.14928607950560202, 0.25327645630251333], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.2994909], dtype=float32), 0.72395945]. 
=============================================
[2019-03-27 04:20:24,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.20249 ]
 [72.19777 ]
 [72.17765 ]
 [72.16381 ]
 [72.022995]], R is [[72.23329163]
 [72.25756073]
 [72.28141022]
 [72.30467987]
 [72.32725525]].
[2019-03-27 04:20:25,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3594566e-14 2.4456231e-01 6.3899324e-22 1.0180024e-14 7.5543767e-01], sum to 1.0000
[2019-03-27 04:20:25,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3680
[2019-03-27 04:20:25,332] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.13333333333334, 61.00000000000001, 1.0, 2.0, 0.4724721224157296, 1.0, 2.0, 0.4724721224157296, 1.0, 2.0, 0.7984073249499578, 6.9112, 6.9112, 170.5573041426782, 1981797.85927008, 1981797.85927008, 392808.7389719645], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6704400.0000, 
sim time next is 6705000.0000, 
raw observation next is [30.1, 61.5, 1.0, 2.0, 0.4834559432463699, 1.0, 2.0, 0.4834559432463699, 1.0, 2.0, 0.8177527157447768, 6.9112, 6.9112, 170.5573041426782, 2027913.416018545, 2027913.416018545, 400022.3442748123], 
processed observation next is [1.0, 0.6086956521739131, 0.6255924170616115, 0.615, 1.0, 1.0, 0.3776577629474337, 1.0, 1.0, 0.3776577629474337, 1.0, 1.0, 0.7777472143228986, 0.0, 0.0, 0.8375144448122397, 0.5633092822273736, 0.5633092822273736, 0.5970482750370333], 
reward next is 0.4030, 
noisyNet noise sample is [array([0.64433265], dtype=float32), -0.103502944]. 
=============================================
[2019-03-27 04:20:25,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.45591 ]
 [68.35962 ]
 [68.757164]
 [68.00406 ]
 [67.551605]], R is [[68.68573761]
 [68.41259766]
 [68.14261627]
 [67.89749908]
 [67.64906311]].
[2019-03-27 04:20:26,195] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2303949: loss 8.7408
[2019-03-27 04:20:26,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2303949: learning rate 0.0001
[2019-03-27 04:20:26,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3617183e-28 1.0000000e+00 2.7099260e-38 3.3136504e-32 4.7554828e-25], sum to 1.0000
[2019-03-27 04:20:26,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1191
[2019-03-27 04:20:26,488] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 67.0, 1.0, 2.0, 0.4170665138822869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611146.1835130262, 611146.1835130256, 175261.3542101542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6723600.0000, 
sim time next is 6724200.0000, 
raw observation next is [27.33333333333334, 67.0, 1.0, 2.0, 0.4122982921770527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606925.3413061538, 606925.3413061538, 174943.6983084941], 
processed observation next is [1.0, 0.8260869565217391, 0.4944707740916275, 0.67, 1.0, 1.0, 0.29192565322536473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16859037258504272, 0.16859037258504272, 0.2611099974753643], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.27495503], dtype=float32), 1.3010975]. 
=============================================
[2019-03-27 04:20:26,836] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2304240: loss 9.8250
[2019-03-27 04:20:26,839] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2304240: learning rate 0.0001
[2019-03-27 04:20:26,905] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2304273: loss -29.0842
[2019-03-27 04:20:26,908] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2304274: learning rate 0.0001
[2019-03-27 04:20:29,042] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2305255: loss 7.5450
[2019-03-27 04:20:29,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2305255: learning rate 0.0001
[2019-03-27 04:20:30,704] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2306028: loss 9.7622
[2019-03-27 04:20:30,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2306029: learning rate 0.0001
[2019-03-27 04:20:31,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1348531e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6256869e-38], sum to 1.0000
[2019-03-27 04:20:31,488] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8396
[2019-03-27 04:20:31,491] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 74.0, 1.0, 2.0, 0.7306945428776342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131166.859759534, 1131166.859759534, 241497.8793449228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7292400.0000, 
sim time next is 7293000.0000, 
raw observation next is [24.78333333333333, 73.0, 1.0, 2.0, 0.7518275171007635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161936.169716239, 1161936.169716239, 246643.2278884322], 
processed observation next is [1.0, 0.391304347826087, 0.37361769352290675, 0.73, 1.0, 1.0, 0.7009970085551367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32276004714339973, 0.32276004714339973, 0.36812422072900325], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.01811353], dtype=float32), -0.5945158]. 
=============================================
[2019-03-27 04:20:31,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.900856]
 [69.097466]
 [69.30924 ]
 [69.54097 ]
 [69.59057 ]], R is [[68.72490692]
 [68.67721558]
 [68.64511871]
 [68.62386322]
 [68.61507416]].
[2019-03-27 04:20:32,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:32,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0706
[2019-03-27 04:20:32,088] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 75.0, 1.0, 2.0, 0.3646007499554474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557398.0143201961, 557398.0143201961, 171150.8276421347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7345800.0000, 
sim time next is 7346400.0000, 
raw observation next is [24.86666666666667, 74.66666666666666, 1.0, 2.0, 0.3644191952882042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557667.0587665866, 557667.0587665872, 171189.7534645393], 
processed observation next is [1.0, 0.0, 0.3775671406003162, 0.7466666666666666, 1.0, 1.0, 0.2342399943231376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15490751632405184, 0.154907516324052, 0.255507094723193], 
reward next is 0.7445, 
noisyNet noise sample is [array([0.00234995], dtype=float32), -0.70514655]. 
=============================================
[2019-03-27 04:20:32,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:32,265] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5404
[2019-03-27 04:20:32,270] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 89.66666666666666, 1.0, 2.0, 0.5690731935054382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795227.5014856862, 795227.5014856867, 195121.7565226524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7191600.0000, 
sim time next is 7192200.0000, 
raw observation next is [26.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5704420546150114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797141.0768989144, 797141.0768989144, 195364.6506978352], 
processed observation next is [1.0, 0.21739130434782608, 0.45339652448657203, 0.8933333333333333, 1.0, 1.0, 0.48246030676507395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22142807691636512, 0.22142807691636512, 0.2915890308922913], 
reward next is 0.7084, 
noisyNet noise sample is [array([0.7894994], dtype=float32), 0.17502984]. 
=============================================
[2019-03-27 04:20:32,787] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2307185: loss 7.3661
[2019-03-27 04:20:32,793] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2307186: learning rate 0.0001
[2019-03-27 04:20:33,362] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2307439: loss 50.8934
[2019-03-27 04:20:33,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2307441: learning rate 0.0001
[2019-03-27 04:20:33,435] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2307467: loss 87.4248
[2019-03-27 04:20:33,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2307467: learning rate 0.0001
[2019-03-27 04:20:34,332] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2307875: loss 7.3147
[2019-03-27 04:20:34,336] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2307876: learning rate 0.0001
[2019-03-27 04:20:35,855] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2308555: loss 5.7211
[2019-03-27 04:20:35,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2308555: learning rate 0.0001
[2019-03-27 04:20:36,811] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2308982: loss 23.9293
[2019-03-27 04:20:36,812] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2308982: learning rate 0.0001
[2019-03-27 04:20:36,837] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2308994: loss 8.4251
[2019-03-27 04:20:36,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2308995: learning rate 0.0001
[2019-03-27 04:20:36,880] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2309011: loss 7.8695
[2019-03-27 04:20:36,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2309011: learning rate 0.0001
[2019-03-27 04:20:37,172] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2309139: loss 7.2909
[2019-03-27 04:20:37,174] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2309140: learning rate 0.0001
[2019-03-27 04:20:38,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9948265e-31 1.0000000e+00 0.0000000e+00 2.5491935e-35 7.1047152e-29], sum to 1.0000
[2019-03-27 04:20:38,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9083
[2019-03-27 04:20:38,627] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 64.33333333333334, 1.0, 2.0, 0.7743874539833375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1179373.118487598, 1179373.118487598, 250502.7218039786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7299600.0000, 
sim time next is 7300200.0000, 
raw observation next is [26.81666666666667, 63.66666666666666, 1.0, 2.0, 0.7866281003577029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199196.73302138, 1199196.733021379, 253857.3074249674], 
processed observation next is [1.0, 0.4782608695652174, 0.46998420221169057, 0.6366666666666666, 1.0, 1.0, 0.7429254221177144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33311020361704996, 0.33311020361704974, 0.37889150361935436], 
reward next is 0.6211, 
noisyNet noise sample is [array([-0.1718468], dtype=float32), -0.67082375]. 
=============================================
[2019-03-27 04:20:40,057] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2310424: loss 24.6119
[2019-03-27 04:20:40,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2310425: learning rate 0.0001
[2019-03-27 04:20:41,456] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2311045: loss 0.0155
[2019-03-27 04:20:41,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2311046: learning rate 0.0001
[2019-03-27 04:20:43,818] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2312105: loss 55.8191
[2019-03-27 04:20:43,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2312105: learning rate 0.0001
[2019-03-27 04:20:43,834] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2312112: loss 0.0026
[2019-03-27 04:20:43,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2312112: learning rate 0.0001
[2019-03-27 04:20:44,453] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2312384: loss 93.5240
[2019-03-27 04:20:44,455] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2312384: learning rate 0.0001
[2019-03-27 04:20:46,679] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2313386: loss -30.6135
[2019-03-27 04:20:46,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2313386: learning rate 0.0001
[2019-03-27 04:20:47,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3148678e-28 1.0000000e+00 8.9685960e-38 2.4062941e-34 7.2192490e-27], sum to 1.0000
[2019-03-27 04:20:47,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9150
[2019-03-27 04:20:47,086] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 98.16666666666667, 1.0, 2.0, 0.8762086344217833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1224668.537556989, 1224668.537556989, 263473.1249735965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7218600.0000, 
sim time next is 7219200.0000, 
raw observation next is [24.3, 96.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.228599121237546, 6.9112, 168.9115459792073, 1679081.121289232, 1453909.140994393, 311346.3904547298], 
processed observation next is [1.0, 0.5652173913043478, 0.3507109004739337, 0.9633333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03173991212375462, 0.0, 0.8294330187982503, 0.46641142258034224, 0.4038636502762203, 0.4646961051563131], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4654229], dtype=float32), -0.9945249]. 
=============================================
[2019-03-27 04:20:47,991] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2314130: loss 35.8796
[2019-03-27 04:20:47,993] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2314131: learning rate 0.0001
[2019-03-27 04:20:50,352] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2315220: loss 0.0019
[2019-03-27 04:20:50,354] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2315220: learning rate 0.0001
[2019-03-27 04:20:50,427] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2315252: loss 0.0015
[2019-03-27 04:20:50,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2315254: learning rate 0.0001
[2019-03-27 04:20:50,490] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2315280: loss -39.6931
[2019-03-27 04:20:50,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2315282: learning rate 0.0001
[2019-03-27 04:20:51,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:51,866] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9144
[2019-03-27 04:20:51,873] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 81.33333333333334, 1.0, 2.0, 0.3996700933779856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590680.1827695712, 590680.1827695712, 173505.6169786183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7478400.0000, 
sim time next is 7479000.0000, 
raw observation next is [25.1, 81.0, 1.0, 2.0, 0.4017569457259792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 592685.9122017049, 592685.9122017055, 173657.7022912811], 
processed observation next is [0.0, 0.5652173913043478, 0.38862559241706174, 0.81, 1.0, 1.0, 0.27922523581443276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1646349756115847, 0.16463497561158486, 0.2591906004347479], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.02295513], dtype=float32), 0.0955891]. 
=============================================
[2019-03-27 04:20:51,902] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.155876]
 [73.22496 ]
 [73.2769  ]
 [73.23685 ]
 [73.27333 ]], R is [[73.11431885]
 [73.12421417]
 [73.13431549]
 [73.14459229]
 [73.15499115]].
[2019-03-27 04:20:52,202] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2316040: loss 37.3109
[2019-03-27 04:20:52,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2316040: learning rate 0.0001
[2019-03-27 04:20:53,581] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2316668: loss 16.5936
[2019-03-27 04:20:53,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2316668: learning rate 0.0001
[2019-03-27 04:20:53,851] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2316786: loss 0.0019
[2019-03-27 04:20:53,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2316787: learning rate 0.0001
[2019-03-27 04:20:54,522] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2317087: loss 56.3073
[2019-03-27 04:20:54,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2317087: learning rate 0.0001
[2019-03-27 04:20:54,647] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2317143: loss -207.3750
[2019-03-27 04:20:54,651] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2317143: learning rate 0.0001
[2019-03-27 04:20:54,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:20:54,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2400
[2019-03-27 04:20:54,884] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.21666666666667, 84.50000000000001, 1.0, 2.0, 0.386239480210638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 577380.7410647481, 577380.7410647476, 172505.4138475132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7474200.0000, 
sim time next is 7474800.0000, 
raw observation next is [24.33333333333334, 84.0, 1.0, 2.0, 0.3879838743994324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579066.6838532434, 579066.6838532427, 172627.6638041803], 
processed observation next is [0.0, 0.5217391304347826, 0.35229067930489766, 0.84, 1.0, 1.0, 0.26263117397521973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16085185662590093, 0.16085185662590074, 0.2576532295584781], 
reward next is 0.7423, 
noisyNet noise sample is [array([0.45697162], dtype=float32), 1.8981689]. 
=============================================
[2019-03-27 04:20:54,966] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2317284: loss -12.2654
[2019-03-27 04:20:54,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2317284: learning rate 0.0001
[2019-03-27 04:20:57,075] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2318223: loss 0.0014
[2019-03-27 04:20:57,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2318223: learning rate 0.0001
[2019-03-27 04:20:59,262] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2319195: loss -54.8390
[2019-03-27 04:20:59,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2319196: learning rate 0.0001
[2019-03-27 04:21:00,823] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2319892: loss 0.0019
[2019-03-27 04:21:00,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2319892: learning rate 0.0001
[2019-03-27 04:21:01,576] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2320224: loss -56.1191
[2019-03-27 04:21:01,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2320224: learning rate 0.0001
[2019-03-27 04:21:01,591] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2320231: loss 0.0021
[2019-03-27 04:21:01,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2320231: learning rate 0.0001
[2019-03-27 04:21:03,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2321178: loss 0.0014
[2019-03-27 04:21:03,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2321179: learning rate 0.0001
[2019-03-27 04:21:04,776] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:21:04,784] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6581
[2019-03-27 04:21:04,789] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 95.0, 1.0, 2.0, 0.3293756249311166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513858.8195547291, 513858.8195547291, 167917.9562271235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7454400.0000, 
sim time next is 7455000.0000, 
raw observation next is [21.46666666666667, 95.0, 1.0, 2.0, 0.3307627266314528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515566.3560394669, 515566.3560394669, 168038.2414907735], 
processed observation next is [0.0, 0.2608695652173913, 0.21642969984202226, 0.95, 1.0, 1.0, 0.19369003208608768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1432128766776297, 0.1432128766776297, 0.25080334550861716], 
reward next is 0.7492, 
noisyNet noise sample is [array([0.54096454], dtype=float32), -1.401248]. 
=============================================
[2019-03-27 04:21:04,806] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.32372]
 [73.35173]
 [73.39247]
 [73.45354]
 [73.49437]], R is [[73.31407166]
 [73.33030701]
 [73.34648895]
 [73.36258698]
 [73.37858582]].
[2019-03-27 04:21:05,517] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2321988: loss 0.0032
[2019-03-27 04:21:05,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2321988: learning rate 0.0001
[2019-03-27 04:21:08,130] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2323153: loss 0.0064
[2019-03-27 04:21:08,132] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2323153: learning rate 0.0001
[2019-03-27 04:21:08,586] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2323359: loss -86.0364
[2019-03-27 04:21:08,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2323360: learning rate 0.0001
[2019-03-27 04:21:08,626] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2323376: loss 14.1803
[2019-03-27 04:21:08,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2323376: learning rate 0.0001
[2019-03-27 04:21:08,883] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:21:08,884] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:08,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-27 04:21:09,551] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2323831: loss 0.0011
[2019-03-27 04:21:09,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2323833: learning rate 0.0001
[2019-03-27 04:21:10,835] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2324471: loss 0.0011
[2019-03-27 04:21:10,840] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2324473: learning rate 0.0001
[2019-03-27 04:21:10,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:21:10,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:10,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-27 04:21:11,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:21:11,393] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6364
[2019-03-27 04:21:11,397] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4040069532945039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595397.4831866066, 595397.4831866072, 173891.3984539635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7508400.0000, 
sim time next is 7509000.0000, 
raw observation next is [23.95, 89.5, 1.0, 2.0, 0.4045870985590354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595888.5956028645, 595888.5956028645, 173925.6337767656], 
processed observation next is [0.0, 0.9130434782608695, 0.3341232227488152, 0.895, 1.0, 1.0, 0.2826350585048619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1655246098896846, 0.1655246098896846, 0.25959049817427704], 
reward next is 0.7404, 
noisyNet noise sample is [array([-1.1709126], dtype=float32), -0.1785683]. 
=============================================
[2019-03-27 04:21:11,409] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.90466 ]
 [73.905716]
 [73.91786 ]
 [73.9415  ]
 [73.96678 ]], R is [[73.9108429 ]
 [73.91220093]
 [73.9135437 ]
 [73.9147644 ]
 [73.91579437]].
[2019-03-27 04:21:11,503] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2324807: loss -77.5795
[2019-03-27 04:21:11,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2324807: learning rate 0.0001
[2019-03-27 04:21:11,547] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2324835: loss 0.0049
[2019-03-27 04:21:11,551] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2324835: learning rate 0.0001
[2019-03-27 04:21:11,602] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2324863: loss 0.0019
[2019-03-27 04:21:11,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2324864: learning rate 0.0001
[2019-03-27 04:21:11,844] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2325000: loss 0.0013
[2019-03-27 04:21:11,846] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-27 04:21:11,846] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:21:11,846] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2325000: learning rate 0.0001
[2019-03-27 04:21:11,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:11,847] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:21:11,848] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:11,849] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:21:11,849] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:11,850] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:21:11,852] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:11,853] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:21:11,856] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:21:11,876] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-27 04:21:11,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-27 04:21:11,910] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-27 04:21:11,931] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-27 04:21:11,951] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-27 04:21:14,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15559407]
[2019-03-27 04:21:14,177] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.13333333333334, 80.66666666666667, 1.0, 2.0, 0.4206274997741491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619947.8642677453, 619947.8642677453, 176199.7177891952]
[2019-03-27 04:21:14,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:21:14,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3567333e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9973693997711282
[2019-03-27 04:21:15,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15559407]
[2019-03-27 04:21:15,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.36119497, 99.94215914, 1.0, 2.0, 0.3870575270011855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580720.1535578115, 580720.1535578115, 172872.2740782258]
[2019-03-27 04:21:15,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:21:15,270] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.046395412951750314
[2019-03-27 04:22:18,858] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15559407]
[2019-03-27 04:22:18,859] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 87.33333333333334, 1.0, 2.0, 0.5031500513758868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703075.4800365015, 703075.4800365015, 184090.356017219]
[2019-03-27 04:22:18,863] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:22:18,866] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7227868293961742
[2019-03-27 04:22:47,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.15559407]
[2019-03-27 04:22:47,163] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.541416995, 50.80345517, 1.0, 2.0, 0.6375171235002473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 911861.1428538065, 911861.1428538058, 210670.5715191218]
[2019-03-27 04:22:47,164] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:22:47,166] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.511796645841701
[2019-03-27 04:23:07,303] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:23:07,485] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:23:07,552] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0214 3007604021.6667 1766.0000
[2019-03-27 04:23:07,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1329 3163952035.2649 1781.0000
[2019-03-27 04:23:07,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.2288 2842451885.9470 1132.0000
[2019-03-27 04:23:08,768] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2325000, evaluation results [2325000.0, 7884.132872102445, 3163952035.264906, 1781.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.021354497961, 3007604021.6667104, 1766.0, 8497.228767578754, 2842451885.947025, 1132.0]
[2019-03-27 04:23:11,307] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2326181: loss -4.1341
[2019-03-27 04:23:11,308] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2326181: learning rate 0.0001
[2019-03-27 04:23:14,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:14,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:14,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:14,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:14,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-27 04:23:14,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-27 04:23:14,711] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2327766: loss 23.0465
[2019-03-27 04:23:14,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2327766: learning rate 0.0001
[2019-03-27 04:23:15,056] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2327978: loss -63.6251
[2019-03-27 04:23:15,058] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2327981: learning rate 0.0001
[2019-03-27 04:23:16,644] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2328805: loss 24.3953
[2019-03-27 04:23:16,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2328807: learning rate 0.0001
[2019-03-27 04:23:16,794] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:16,796] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:16,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-27 04:23:18,091] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2329596: loss 59.4205
[2019-03-27 04:23:18,092] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2329596: learning rate 0.0001
[2019-03-27 04:23:19,353] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:19,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:19,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-27 04:23:20,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2282417e-29 1.0000000e+00 0.0000000e+00 4.9887231e-33 2.8097244e-27], sum to 1.0000
[2019-03-27 04:23:20,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4059
[2019-03-27 04:23:20,037] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 89.0, 1.0, 2.0, 0.5148074242942342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719370.3969303103, 719370.3969303109, 185948.5945981542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7776000.0000, 
sim time next is 7776600.0000, 
raw observation next is [26.4, 88.66666666666667, 1.0, 2.0, 0.5131699739753436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717081.5199289086, 717081.5199289093, 185685.245727635], 
processed observation next is [1.0, 0.0, 0.45023696682464454, 0.8866666666666667, 1.0, 1.0, 0.41345779997029347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19918931109136348, 0.19918931109136367, 0.2771421578024403], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.7327955], dtype=float32), -0.27153122]. 
=============================================
[2019-03-27 04:23:20,095] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2330575: loss -48.4862
[2019-03-27 04:23:20,097] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2330576: learning rate 0.0001
[2019-03-27 04:23:20,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:20,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2078
[2019-03-27 04:23:20,770] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 91.0, 1.0, 2.0, 0.3677547410208171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557765.5716413929, 557765.5716413935, 171045.9753533538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 111600.0000, 
sim time next is 112200.0000, 
raw observation next is [22.91666666666666, 91.00000000000001, 1.0, 2.0, 0.4676925561097179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708839.6244483565, 708839.6244483565, 185466.9295031413], 
processed observation next is [1.0, 0.30434782608695654, 0.2851500789889413, 0.9100000000000001, 1.0, 1.0, 0.3586657302526722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19689989568009902, 0.19689989568009902, 0.2768163126912557], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.14148235], dtype=float32), -0.19406012]. 
=============================================
[2019-03-27 04:23:21,813] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2331441: loss 11.1306
[2019-03-27 04:23:21,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2331441: learning rate 0.0001
[2019-03-27 04:23:22,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:22,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-27 04:23:22,862] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2331982: loss 0.0402
[2019-03-27 04:23:22,865] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2331983: learning rate 0.0001
[2019-03-27 04:23:22,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:22,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:22,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-27 04:23:23,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4409067e-16 4.0095812e-03 1.9062198e-22 5.9428433e-15 9.9599046e-01], sum to 1.0000
[2019-03-27 04:23:23,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3098
[2019-03-27 04:23:23,201] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.76666666666667, 70.33333333333333, 1.0, 2.0, 0.954436101927435, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00120476934473, 6.9112, 168.9124212542718, 2231228.713846517, 2167376.435085824, 449779.0360225776], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7818600.0000, 
sim time next is 7819200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5728058057953834, 1.0, 1.0, 0.5728058057953834, 1.0, 2.0, 0.9947742548919479, 6.9112, 6.9112, 170.5573041426782, 2403091.945513641, 2403091.945513641, 469143.0817489544], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.485308199753474, 1.0, 0.5, 0.485308199753474, 1.0, 1.0, 0.9936271401121315, 0.0, 0.0, 0.8375144448122397, 0.6675255404204559, 0.6675255404204559, 0.7002135548491857], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21846567], dtype=float32), 1.6705084]. 
=============================================
[2019-03-27 04:23:23,429] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2332322: loss -28.2967
[2019-03-27 04:23:23,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2332322: learning rate 0.0001
[2019-03-27 04:23:23,570] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2332416: loss -5.1431
[2019-03-27 04:23:23,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2332418: learning rate 0.0001
[2019-03-27 04:23:23,735] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2332521: loss -17.6793
[2019-03-27 04:23:23,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2332521: learning rate 0.0001
[2019-03-27 04:23:24,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:24,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:24,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-27 04:23:25,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:25,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:25,879] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-27 04:23:26,319] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:26,327] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0046
[2019-03-27 04:23:26,333] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 87.0, 1.0, 2.0, 0.2569084511794936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418598.7393006994, 418598.7393006994, 161538.0374686603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 367200.0000, 
sim time next is 367800.0000, 
raw observation next is [20.36666666666667, 86.5, 1.0, 2.0, 0.2609633783394137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 425208.3495388778, 425208.3495388784, 161948.2201520966], 
processed observation next is [1.0, 0.2608695652173913, 0.1642969984202214, 0.865, 1.0, 1.0, 0.10959443173423335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11811343042746605, 0.11811343042746623, 0.24171376142103967], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.26430532], dtype=float32), 0.08298342]. 
=============================================
[2019-03-27 04:23:27,752] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:27,753] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:27,824] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-27 04:23:29,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:29,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:29,294] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-27 04:23:30,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:30,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:30,224] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-27 04:23:31,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:31,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:31,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:31,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:31,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-27 04:23:31,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-27 04:23:31,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-27 04:23:31,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:23:31,295] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-27 04:23:31,386] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:31,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2710
[2019-03-27 04:23:31,396] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 84.0, 1.0, 2.0, 0.3279442786595308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519472.6537201722, 519472.6537201722, 168521.9721615786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 27600.0000, 
sim time next is 28200.0000, 
raw observation next is [22.25, 84.0, 1.0, 2.0, 0.3407849861179714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539199.0849194026, 539199.0849194026, 170068.0473758531], 
processed observation next is [1.0, 0.30434782608695654, 0.2535545023696683, 0.84, 1.0, 1.0, 0.20576504351562822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14977752358872296, 0.14977752358872296, 0.253832906531124], 
reward next is 0.7462, 
noisyNet noise sample is [array([-1.0521795], dtype=float32), 1.6121553]. 
=============================================
[2019-03-27 04:23:49,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:49,230] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1395
[2019-03-27 04:23:49,238] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 86.66666666666667, 1.0, 2.0, 0.2858559580874585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458183.616533574, 458183.616533574, 164137.3242651715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 285600.0000, 
sim time next is 286200.0000, 
raw observation next is [21.45, 86.0, 1.0, 2.0, 0.2878227717398767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460736.3540841753, 460736.3540841753, 164305.3936473863], 
processed observation next is [0.0, 0.30434782608695654, 0.2156398104265403, 0.86, 1.0, 1.0, 0.14195514667455023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12798232057893758, 0.12798232057893758, 0.2452319308169945], 
reward next is 0.7548, 
noisyNet noise sample is [array([0.15493435], dtype=float32), 0.32136998]. 
=============================================
[2019-03-27 04:23:53,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:53,945] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2033
[2019-03-27 04:23:53,955] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([1.1887827], dtype=float32), -0.0961558]. 
=============================================
[2019-03-27 04:23:57,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:57,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8955
[2019-03-27 04:23:57,718] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333334, 79.0, 1.0, 2.0, 0.2216664275007999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367760.7904005787, 367760.7904005787, 158041.4913121994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 630600.0000, 
sim time next is 631200.0000, 
raw observation next is [20.06666666666667, 78.0, 1.0, 2.0, 0.2314189605157331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 383596.3348350081, 383596.3348350075, 158964.5010119097], 
processed observation next is [1.0, 0.30434782608695654, 0.1500789889415484, 0.78, 1.0, 1.0, 0.07399874760931698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10655453745416892, 0.10655453745416875, 0.23726044927150702], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.6250171], dtype=float32), -0.5466428]. 
=============================================
[2019-03-27 04:23:58,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:23:58,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3582
[2019-03-27 04:23:58,690] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 70.0, 1.0, 2.0, 0.4933154680641298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808044.4208022965, 808044.4208022971, 194610.3667792293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 722400.0000, 
sim time next is 723000.0000, 
raw observation next is [22.41666666666667, 69.0, 1.0, 2.0, 0.5024554194723655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 822644.3336216464, 822644.3336216458, 196248.3106412334], 
processed observation next is [1.0, 0.34782608695652173, 0.26145339652448685, 0.69, 1.0, 1.0, 0.40054869815947647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22851231489490179, 0.22851231489490162, 0.29290792633019913], 
reward next is 0.7071, 
noisyNet noise sample is [array([-0.46414173], dtype=float32), 1.0557239]. 
=============================================
[2019-03-27 04:23:58,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.12924]
 [75.13733]
 [75.18858]
 [75.41638]
 [75.44185]], R is [[75.05791473]
 [75.01686859]
 [74.97863007]
 [74.95679474]
 [74.96392822]].
[2019-03-27 04:24:00,964] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 04:24:00,967] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:24:00,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:24:00,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:24:00,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:24:00,969] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:24:00,970] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:24:00,973] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:24:00,976] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:24:00,972] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:24:00,977] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:24:00,993] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-27 04:24:01,017] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-27 04:24:01,042] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-27 04:24:01,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-27 04:24:01,080] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-27 04:24:06,546] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:24:06,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 46.0, 1.0, 2.0, 0.210499329748817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 351778.6919763075, 351778.6919763081, 156288.1288917225]
[2019-03-27 04:24:06,549] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:24:06,555] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10197715149855635
[2019-03-27 04:24:48,298] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:24:48,300] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.0, 51.66666666666667, 1.0, 2.0, 0.8698706086557378, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005985892476044, 6.9112, 168.9123159734695, 2112864.535457876, 2045620.416054964, 425836.2126753146]
[2019-03-27 04:24:48,301] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:24:48,304] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6006685e-33 1.0000000e+00 0.0000000e+00 2.9114005e-37 1.2862217e-30], sampled 0.35401483574964576
[2019-03-27 04:24:48,305] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2112864.535457876 W.
[2019-03-27 04:24:55,698] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:24:55,699] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.05, 51.0, 1.0, 2.0, 0.7382925308690785, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002517836472157, 6.9112, 168.9123389187271, 1928714.953529864, 1863931.174287123, 393299.7402712579]
[2019-03-27 04:24:55,700] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:24:55,705] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3896855e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.4148911e-34], sampled 0.023649210587096348
[2019-03-27 04:24:55,707] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1928714.953529864 W.
[2019-03-27 04:25:22,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:25:22,071] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.40246678, 92.89109905333333, 1.0, 2.0, 0.8316733782842125, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983506511447, 6.9112, 168.9123159898192, 2059402.459053198, 1992160.032322763, 415816.3088856092]
[2019-03-27 04:25:22,072] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:25:22,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6176777633511146
[2019-03-27 04:25:22,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2059402.459053198 W.
[2019-03-27 04:25:27,826] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:25:27,827] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4964164556559862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695820.5177295408, 695820.5177295408, 183313.2839438968]
[2019-03-27 04:25:27,829] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:25:27,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2653732e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0723737e-36], sampled 0.3980022636464252
[2019-03-27 04:25:29,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16276106]
[2019-03-27 04:25:29,960] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 86.0, 1.0, 2.0, 0.5387169063691704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752792.347454091, 752792.3474540904, 189884.8297704713]
[2019-03-27 04:25:29,962] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:25:29,965] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5342523e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.11432329500327065
[2019-03-27 04:25:56,707] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:25:57,060] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:25:57,062] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:25:57,080] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:25:57,123] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:25:58,140] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2350000, evaluation results [2350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:25:58,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:25:58,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6597
[2019-03-27 04:25:58,709] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 96.0, 1.0, 2.0, 0.3177467728688037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500179.4133182035, 500179.4133182035, 166989.4617915878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1051200.0000, 
sim time next is 1051800.0000, 
raw observation next is [20.75, 96.0, 1.0, 2.0, 0.3479490270058536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549723.8524454507, 549723.8524454507, 170910.5122204271], 
processed observation next is [1.0, 0.17391304347826086, 0.18246445497630337, 0.96, 1.0, 1.0, 0.2143964180793417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15270107012373632, 0.15270107012373632, 0.25509031674690613], 
reward next is 0.7449, 
noisyNet noise sample is [array([1.6215105], dtype=float32), 0.75278777]. 
=============================================
[2019-03-27 04:26:18,132] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8212704e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2176212e-35], sum to 1.0000
[2019-03-27 04:26:18,141] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5637
[2019-03-27 04:26:18,149] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666667, 65.5, 1.0, 2.0, 0.7063718803452317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1079570.602610625, 1079570.602610624, 233967.8584736896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1167000.0000, 
sim time next is 1167600.0000, 
raw observation next is [26.53333333333333, 65.0, 1.0, 2.0, 0.6571980537275418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1003648.483374566, 1003648.483374566, 222509.327096323], 
processed observation next is [1.0, 0.5217391304347826, 0.45655608214849913, 0.65, 1.0, 1.0, 0.5869856069006527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27879124538182387, 0.27879124538182387, 0.33210347327809403], 
reward next is 0.6679, 
noisyNet noise sample is [array([1.043277], dtype=float32), 1.2383112]. 
=============================================
[2019-03-27 04:26:28,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3136291e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 04:26:28,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7644
[2019-03-27 04:26:28,659] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.83333333333334, 1.0, 2.0, 0.4068980605363464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623561.2886847962, 623561.2886847956, 177085.8042402147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1012200.0000, 
sim time next is 1012800.0000, 
raw observation next is [21.7, 97.66666666666667, 1.0, 2.0, 0.3574286508168133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548152.3398437325, 548152.3398437331, 170420.9135442379], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.9766666666666667, 1.0, 1.0, 0.22581765158652203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15226453884548127, 0.1522645388454814, 0.2543595724540864], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.01319052], dtype=float32), 1.0340253]. 
=============================================
[2019-03-27 04:26:33,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8302987e-33 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.7856983e-32], sum to 1.0000
[2019-03-27 04:26:33,890] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6743
[2019-03-27 04:26:33,897] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 87.0, 1.0, 2.0, 0.7590619797247687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1133001.807641249, 1133001.807641248, 243730.3944333877], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1607400.0000, 
sim time next is 1608000.0000, 
raw observation next is [23.83333333333334, 87.66666666666666, 1.0, 2.0, 0.7740902642945382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1154497.710712508, 1154497.710712507, 247411.900702819], 
processed observation next is [1.0, 0.6086956521739131, 0.32859399684044266, 0.8766666666666666, 1.0, 1.0, 0.7278195955355882, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32069380853125223, 0.3206938085312519, 0.369271493586297], 
reward next is 0.6307, 
noisyNet noise sample is [array([0.6150518], dtype=float32), 0.7750391]. 
=============================================
[2019-03-27 04:26:33,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.982025]
 [73.948814]
 [73.92779 ]
 [73.56689 ]
 [73.223404]], R is [[73.8997879 ]
 [73.79701233]
 [73.70805359]
 [73.6315155 ]
 [73.52554321]].
[2019-03-27 04:26:45,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1883179e-30 1.0000000e+00 0.0000000e+00 6.3141091e-34 1.2288257e-29], sum to 1.0000
[2019-03-27 04:26:45,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9564
[2019-03-27 04:26:45,819] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([-0.3910522], dtype=float32), -0.3429327]. 
=============================================
[2019-03-27 04:26:53,091] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 04:26:53,093] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:26:53,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:53,096] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:26:53,096] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:53,097] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:26:53,099] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:26:53,100] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:53,100] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:26:53,103] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:53,104] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:26:53,131] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-27 04:26:53,155] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-27 04:26:53,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-27 04:26:53,174] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-27 04:26:53,214] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-27 04:27:08,898] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:27:08,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.95, 79.5, 1.0, 2.0, 0.2944164624273032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474313.4534726765, 474313.4534726765, 165260.5082189013]
[2019-03-27 04:27:08,900] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:27:08,902] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4843646946906285
[2019-03-27 04:27:18,762] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:27:18,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.4014556, 88.50602164, 1.0, 2.0, 0.7160097341727807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1003627.74405753, 1003627.744057531, 224739.5748417979]
[2019-03-27 04:27:18,767] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:27:18,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9655489511298636
[2019-03-27 04:27:43,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:27:43,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.9, 57.0, 1.0, 2.0, 0.5229567710521783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730761.8707773351, 730761.8707773351, 187270.3717174848]
[2019-03-27 04:27:43,383] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:27:43,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05909540108011846
[2019-03-27 04:27:50,695] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:27:50,698] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.33333333333333, 69.5, 1.0, 2.0, 0.5482789331545527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766158.9533746095, 766158.95337461, 191505.6622400055]
[2019-03-27 04:27:50,699] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:27:50,703] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4075294231296481
[2019-03-27 04:28:05,091] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:28:05,092] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.98039278666667, 75.56186923333334, 1.0, 2.0, 0.6902374565452332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 964620.3154949155, 964620.315494916, 218752.0422798065]
[2019-03-27 04:28:05,094] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:28:05,096] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5630985946646336
[2019-03-27 04:28:11,556] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:28:11,557] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 76.33333333333333, 1.0, 2.0, 0.5604489721486559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783171.5015685011, 783171.5015685018, 193608.5928146428]
[2019-03-27 04:28:11,559] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:28:11,563] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44935864227935973
[2019-03-27 04:28:17,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:28:17,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.0, 92.0, 1.0, 2.0, 0.6356294536471616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 888272.6511717208, 888272.6511717202, 207571.2578006209]
[2019-03-27 04:28:17,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:28:17,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9596240448663035
[2019-03-27 04:28:32,342] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16460142]
[2019-03-27 04:28:32,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.06673221, 59.85004109, 1.0, 2.0, 0.8237195040332504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1236860.450523209, 1236860.450523209, 261538.8301378661]
[2019-03-27 04:28:32,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:28:32,348] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8039896286316015
[2019-03-27 04:28:48,759] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-27 04:28:48,816] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:28:48,868] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:28:48,915] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-03-27 04:28:48,921] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:28:49,938] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2375000, evaluation results [2375000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-27 04:28:54,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:54,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7415
[2019-03-27 04:28:54,183] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.16666666666667, 1.0, 2.0, 0.4111456044390976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607956.8574825437, 607956.8574825437, 175119.2022666656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [23.1, 95.0, 1.0, 2.0, 0.4099282734275939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606616.0465647251, 606616.0465647251, 175006.7955966006], 
processed observation next is [1.0, 0.8260869565217391, 0.2938388625592418, 0.95, 1.0, 1.0, 0.28907020894890834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1685044573790903, 0.1685044573790903, 0.2612041725322397], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.3406946], dtype=float32), 0.52901053]. 
=============================================
[2019-03-27 04:28:54,584] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:28:54,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2886
[2019-03-27 04:28:54,603] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3260876342298332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511458.9051673762, 511458.9051673768, 167803.87279087], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1560000.0000, 
sim time next is 1560600.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3260588268832261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511414.5032452514, 511414.5032452508, 167800.4732867103], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.1880226829918387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14205958423479206, 0.1420595842347919, 0.25044846759210493], 
reward next is 0.7496, 
noisyNet noise sample is [array([0.572226], dtype=float32), -1.875207]. 
=============================================
[2019-03-27 04:28:58,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8270521e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4460942e-37], sum to 1.0000
[2019-03-27 04:28:58,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5253
[2019-03-27 04:28:58,994] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.0, 1.0, 2.0, 0.6004387425506822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 953896.1516256345, 953896.1516256345, 213998.0517939191], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1782000.0000, 
sim time next is 1782600.0000, 
raw observation next is [21.0, 92.33333333333334, 1.0, 2.0, 0.611183759918883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970856.4212880665, 970856.4212880665, 216282.768564925], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9233333333333335, 1.0, 1.0, 0.5315466986974493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26968233924668517, 0.26968233924668517, 0.32281010233570895], 
reward next is 0.6772, 
noisyNet noise sample is [array([-0.3906487], dtype=float32), -0.098396435]. 
=============================================
[2019-03-27 04:29:00,948] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.7852682e-28 1.0000000e+00 4.4611388e-36 1.3656080e-30 1.6755663e-23], sum to 1.0000
[2019-03-27 04:29:00,954] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5880
[2019-03-27 04:29:00,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1948871.809756075 W.
[2019-03-27 04:29:00,963] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.7, 75.5, 1.0, 2.0, 0.6969442411876783, 1.0, 2.0, 0.6969442411876783, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1948871.809756075, 1948871.809756075, 372814.9285160323], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1701000.0000, 
sim time next is 1701600.0000, 
raw observation next is [28.76666666666667, 75.0, 1.0, 2.0, 0.6855023418846883, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.981298074698392, 6.9112, 168.9125386495935, 1854841.633150266, 1805111.769392899, 382422.0623156885], 
processed observation next is [1.0, 0.6956521739130435, 0.5624012638230649, 0.75, 1.0, 1.0, 0.6210871588972148, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007009807469839213, 0.0, 0.8294378932642931, 0.515233786986185, 0.5014199359424719, 0.5707791974861023], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7567312], dtype=float32), 0.05123309]. 
=============================================
[2019-03-27 04:29:11,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:29:11,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4759
[2019-03-27 04:29:11,177] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 95.5, 1.0, 2.0, 0.6251284712388214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873591.8153861965, 873591.8153861965, 205511.5342816726], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2172600.0000, 
sim time next is 2173200.0000, 
raw observation next is [24.86666666666667, 95.66666666666667, 1.0, 2.0, 0.6136397413985974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857530.2976508441, 857530.2976508441, 203305.9632519845], 
processed observation next is [1.0, 0.13043478260869565, 0.3775671406003162, 0.9566666666666667, 1.0, 1.0, 0.5345057125284306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2382028604585678, 0.2382028604585678, 0.3034417361969918], 
reward next is 0.6966, 
noisyNet noise sample is [array([-0.05546344], dtype=float32), 0.05500251]. 
=============================================
[2019-03-27 04:29:12,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:29:12,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4410
[2019-03-27 04:29:12,474] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.66666666666667, 1.0, 2.0, 0.7640117855382538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1121185.983993121, 1121185.983993122, 242503.4252908333], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1844400.0000, 
sim time next is 1845000.0000, 
raw observation next is [23.9, 91.0, 1.0, 2.0, 0.8382250692899407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1227939.058312327, 1227939.058312327, 261448.1424986667], 
processed observation next is [1.0, 0.34782608695652173, 0.33175355450236965, 0.91, 1.0, 1.0, 0.8050904449276394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34109418286453524, 0.34109418286453524, 0.39022110820696526], 
reward next is 0.6098, 
noisyNet noise sample is [array([0.11559591], dtype=float32), -0.83484477]. 
=============================================
[2019-03-27 04:29:12,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.20738 ]
 [67.807014]
 [68.72696 ]
 [68.75285 ]
 [68.872314]], R is [[63.51195526]
 [63.51488876]
 [63.58108902]
 [63.67211533]
 [63.76786423]].
[2019-03-27 04:29:12,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:29:12,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1997
[2019-03-27 04:29:12,521] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 93.83333333333334, 1.0, 2.0, 0.3328415724815518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521531.8351264926, 521531.835126492, 168574.6074704665], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1806600.0000, 
sim time next is 1807200.0000, 
raw observation next is [21.4, 94.0, 1.0, 2.0, 0.332216873817985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520286.0949662406, 520286.0949662399, 168470.6441603976], 
processed observation next is [1.0, 0.9565217391304348, 0.21327014218009477, 0.94, 1.0, 1.0, 0.19544201664817468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14452391526840017, 0.14452391526839997, 0.2514487226274591], 
reward next is 0.7486, 
noisyNet noise sample is [array([-1.4050248], dtype=float32), -0.6336407]. 
=============================================
[2019-03-27 04:29:20,018] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9105493e-27 1.0000000e+00 1.1891317e-34 1.4781362e-29 2.0674475e-22], sum to 1.0000
[2019-03-27 04:29:20,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3900
[2019-03-27 04:29:20,032] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.8730031739614281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1220185.721320816, 1220185.721320815, 262619.1292893169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2438400.0000, 
sim time next is 2439000.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.9602712200657973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1342236.290751315, 1342236.290751314, 287054.3852909019], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.952134000079274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3728434140975875, 0.37284341409758726, 0.42843938103119683], 
reward next is 0.5716, 
noisyNet noise sample is [array([-0.40199476], dtype=float32), -0.32649574]. 
=============================================
[2019-03-27 04:29:20,058] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[51.476154]
 [51.41017 ]
 [53.521507]
 [53.413403]
 [53.264828]], R is [[50.73309326]
 [50.83379364]
 [50.87409973]
 [51.02046204]
 [51.17301559]].
[2019-03-27 04:29:28,844] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:29:28,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6273
[2019-03-27 04:29:28,857] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 96.0, 1.0, 2.0, 0.4764538974751408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667692.2994593258, 667692.2994593258, 180238.4013870275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [24.46666666666667, 95.5, 1.0, 2.0, 0.4784877433853632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668602.8197707537, 668602.8197707537, 180293.7844910112], 
processed observation next is [0.0, 0.21739130434782608, 0.3586097946287521, 0.955, 1.0, 1.0, 0.37167197998236534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18572300549187604, 0.18572300549187604, 0.26909520073285254], 
reward next is 0.7309, 
noisyNet noise sample is [array([0.970735], dtype=float32), 1.1917993]. 
=============================================
[2019-03-27 04:29:29,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:29:29,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6814
[2019-03-27 04:29:29,314] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 97.0, 1.0, 2.0, 0.460795891596776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651176.1322495863, 651176.1322495863, 178624.0099571674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [23.96666666666667, 97.16666666666667, 1.0, 2.0, 0.4599709408056562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650217.9598427864, 650217.9598427871, 178529.635563173], 
processed observation next is [0.0, 0.17391304347826086, 0.33491311216429714, 0.9716666666666667, 1.0, 1.0, 0.34936257928392317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18061609995632957, 0.18061609995632977, 0.2664621426316015], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.35387647], dtype=float32), 0.43685967]. 
=============================================
[2019-03-27 04:29:44,989] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 04:29:44,991] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:29:44,991] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:29:44,992] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:29:44,992] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:44,993] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:44,994] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:29:44,994] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:44,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:29:44,995] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:44,997] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:29:45,032] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-27 04:29:45,057] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-27 04:29:45,060] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-27 04:29:45,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-27 04:29:45,112] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-27 04:30:10,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17146415]
[2019-03-27 04:30:10,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.9, 89.0, 1.0, 2.0, 0.4215177632871042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 623304.8735194353, 623304.8735194353, 176574.8017098883]
[2019-03-27 04:30:10,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:30:10,713] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.9300872e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8361144e-37], sampled 0.9522371405292914
[2019-03-27 04:30:31,524] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17146415]
[2019-03-27 04:30:31,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.15985338, 93.18925541, 1.0, 2.0, 0.5533113416133114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773193.7450765282, 773193.7450765289, 192368.3328929132]
[2019-03-27 04:30:31,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:30:31,531] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24430930475787083
[2019-03-27 04:31:12,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17146415]
[2019-03-27 04:31:12,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.13722007666667, 80.95624624999999, 1.0, 2.0, 0.566069190287612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 791028.1198849457, 791028.1198849451, 194595.1586221715]
[2019-03-27 04:31:12,328] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:31:12,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8195894161850276
[2019-03-27 04:31:39,958] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:31:41,021] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.6546 3163908836.6008 1783.0000
[2019-03-27 04:31:41,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-27 04:31:41,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-27 04:31:41,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3587 2842405711.7591 1132.0000
[2019-03-27 04:31:42,288] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2400000, evaluation results [2400000.0, 7883.6545625497365, 3163908836.600792, 1783.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.358665957467, 2842405711.7590613, 1132.0]
[2019-03-27 04:31:47,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6297265e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 7.6183535e-36], sum to 1.0000
[2019-03-27 04:31:47,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8767
[2019-03-27 04:31:47,343] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4835738062725922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712301.6879033858, 712301.6879033853, 185650.9953942904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4047702488944265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596324.3255622144, 596324.3255622144, 173971.008130057], 
processed observation next is [1.0, 0.7391304347826086, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28285572155954997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.165645645989504, 0.165645645989504, 0.2596582210896373], 
reward next is 0.7403, 
noisyNet noise sample is [array([1.5677767], dtype=float32), 0.8047545]. 
=============================================
[2019-03-27 04:31:47,765] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:47,772] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7393
[2019-03-27 04:31:47,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3940499015925438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587981.5821450595, 587981.5821450589, 173432.7497484515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2739600.0000, 
sim time next is 2740200.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3939518268025217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587835.3044585264, 587835.3044585264, 173419.3759234499], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.26982147807532736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1632875845718129, 0.1632875845718129, 0.25883488943798494], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.98233265], dtype=float32), 1.6351984]. 
=============================================
[2019-03-27 04:31:48,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:48,465] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5234
[2019-03-27 04:31:48,471] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3355473044568527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521168.9336773183, 521168.9336773177, 168425.7389128151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2964000.0000, 
sim time next is 2964600.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3404207429783243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528735.8043458932, 528735.8043458932, 169027.8158574268], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20532619635942687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14687105676274811, 0.14687105676274811, 0.25228032217526386], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.2851896], dtype=float32), -0.73010236]. 
=============================================
[2019-03-27 04:31:54,272] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:54,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4042
[2019-03-27 04:31:54,286] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 98.0, 1.0, 2.0, 0.3900875902028152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585944.8756998703, 585944.8756998696, 173364.0270374702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2730000.0000, 
sim time next is 2730600.0000, 
raw observation next is [22.5, 97.0, 1.0, 2.0, 0.3908136046864867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 586100.983660108, 586100.983660108, 173350.5651956135], 
processed observation next is [0.0, 0.6086956521739131, 0.2654028436018958, 0.97, 1.0, 1.0, 0.26604048757408033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16280582879447442, 0.16280582879447442, 0.25873218685912464], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.6364185], dtype=float32), 0.4704357]. 
=============================================
[2019-03-27 04:31:57,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:31:57,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4405
[2019-03-27 04:31:57,271] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3097695162867541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493276.0703199785, 493276.0703199792, 166583.9649269864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3110172995467023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 495273.0358376002, 495273.0358375995, 166730.9243207221], 
processed observation next is [1.0, 0.782608695652174, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16990036089964133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13757584328822228, 0.1375758432882221, 0.248852125851824], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.5973485], dtype=float32), 0.25732026]. 
=============================================
[2019-03-27 04:32:09,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1788739e-35 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5123331e-35], sum to 1.0000
[2019-03-27 04:32:09,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0990
[2019-03-27 04:32:09,133] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5695950113844704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900436.0167523412, 900436.0167523412, 207253.4065095986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2992800.0000, 
sim time next is 2993400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.6067918806907895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959263.9464168317, 959263.9464168324, 214947.5562249226], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.5262552779407103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2664622073380088, 0.266462207338009, 0.3208172480968994], 
reward next is 0.6792, 
noisyNet noise sample is [array([-0.24429104], dtype=float32), 0.6533896]. 
=============================================
[2019-03-27 04:32:18,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:32:18,388] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1008
[2019-03-27 04:32:18,393] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3300879593457952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513241.08261196, 513241.0826119606, 167820.2310250227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3042000.0000, 
sim time next is 3042600.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3399735611237027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528076.5213985668, 528076.5213985668, 168976.0132091819], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20478742304060565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.146687922610713, 0.146687922610713, 0.2522030047898237], 
reward next is 0.7478, 
noisyNet noise sample is [array([0.75367975], dtype=float32), 0.033618454]. 
=============================================
[2019-03-27 04:32:20,920] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:32:20,927] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8204
[2019-03-27 04:32:20,933] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4645528505562885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652432.7671732802, 652432.7671732797, 178656.4304518052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [25.83333333333334, 84.83333333333333, 1.0, 2.0, 0.4667282380299969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654101.1035456455, 654101.1035456449, 178797.7069489905], 
processed observation next is [0.0, 0.2608695652173913, 0.42338072669826254, 0.8483333333333333, 1.0, 1.0, 0.35750390124096015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1816947509849015, 0.18169475098490137, 0.26686224917759777], 
reward next is 0.7331, 
noisyNet noise sample is [array([0.23032168], dtype=float32), -1.2697436]. 
=============================================
[2019-03-27 04:32:31,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.63187865e-20 9.99999642e-01 2.16501383e-29 1.02429775e-19
 3.96345882e-07], sum to 1.0000
[2019-03-27 04:32:31,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7383
[2019-03-27 04:32:31,487] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.66666666666667, 1.0, 2.0, 0.1824043205396289, 1.0, 2.0, 0.1824043205396289, 1.0, 2.0, 0.3167759827467583, 6.9112, 6.9112, 170.5573041426782, 764666.1867380766, 764666.1867380766, 265189.2711585786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3691200.0000, 
sim time next is 3691800.0000, 
raw observation next is [31.0, 71.0, 1.0, 2.0, 0.5405206662443391, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755313.7819664311, 755313.7819664304, 190190.9253109562], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.71, 1.0, 1.0, 0.4464104412582399, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2098093838795642, 0.209809383879564, 0.2838670527029197], 
reward next is 0.7161, 
noisyNet noise sample is [array([-0.66533417], dtype=float32), -0.10861546]. 
=============================================
[2019-03-27 04:32:33,859] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.3054729e-33 1.0000000e+00 0.0000000e+00 2.7670178e-35 6.1809039e-30], sum to 1.0000
[2019-03-27 04:32:33,868] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4704
[2019-03-27 04:32:33,871] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3338400.0000, 
sim time next is 3339000.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5949757077889694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831438.0522696073, 831438.0522696073, 199814.0484636618], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.512018925046951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23095501451933537, 0.23095501451933537, 0.2982299230800923], 
reward next is 0.7018, 
noisyNet noise sample is [array([-0.09452013], dtype=float32), -0.36078858]. 
=============================================
[2019-03-27 04:32:33,890] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.995125]
 [73.98762 ]
 [73.975426]
 [73.89252 ]
 [73.863396]], R is [[73.96878815]
 [73.9304657 ]
 [73.89241028]
 [73.85505676]
 [73.81758118]].
[2019-03-27 04:32:34,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9079473e-23 1.0000000e+00 3.4641964e-31 3.8790046e-24 1.0714400e-16], sum to 1.0000
[2019-03-27 04:32:34,533] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7652
[2019-03-27 04:32:34,537] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7757666765312647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084210.087084852, 1084210.087084853, 238048.6408186783], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3382800.0000, 
sim time next is 3383400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7686523751373718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1074262.123078537, 1074262.123078537, 236360.9018411843], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.94, 1.0, 1.0, 0.7212679218522552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2984061452995936, 0.2984061452995936, 0.3527774654346034], 
reward next is 0.6472, 
noisyNet noise sample is [array([0.20759323], dtype=float32), -1.165183]. 
=============================================
[2019-03-27 04:32:37,090] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-27 04:32:37,091] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:32:37,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:32:37,092] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:32:37,093] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:32:37,093] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:32:37,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:32:37,095] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:32:37,097] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:32:37,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:32:37,098] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:32:37,128] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-27 04:32:37,155] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-27 04:32:37,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-27 04:32:37,179] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-27 04:32:37,219] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-27 04:32:59,676] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:32:59,677] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.41666666666666, 98.33333333333333, 1.0, 2.0, 0.3094574603397627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490732.9703280488, 490732.9703280488, 166365.2974024624]
[2019-03-27 04:32:59,678] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:32:59,684] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.12510036312116413
[2019-03-27 04:33:21,473] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:33:21,475] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.78378078, 99.88025784999999, 1.0, 2.0, 0.2427566542787169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401050.5713290761, 401050.5713290761, 160138.8873866447]
[2019-03-27 04:33:21,477] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:33:21,480] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1949315e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3597493e-38], sampled 0.4674643648444964
[2019-03-27 04:33:24,402] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:33:24,404] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.13404990666667, 98.74114530333334, 1.0, 2.0, 0.4539493495303244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656931.6920642955, 656931.6920642955, 179582.4107710703]
[2019-03-27 04:33:24,405] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:33:24,409] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6762098e-33 1.0000000e+00 0.0000000e+00 4.4472882e-37 3.8401245e-30], sampled 0.8530978487620691
[2019-03-27 04:33:32,857] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:33:32,857] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.71833150333333, 78.21868163666666, 1.0, 2.0, 0.7787569507829843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1088391.430675007, 1088391.430675007, 238773.9933809354]
[2019-03-27 04:33:32,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:33:32,864] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.186129e-30 1.000000e+00 0.000000e+00 7.947181e-33 3.457459e-26], sampled 0.26169073782961605
[2019-03-27 04:34:02,944] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:34:02,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.63333333333333, 86.33333333333334, 1.0, 2.0, 0.5921137924867346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827437.1626422076, 827437.1626422076, 199285.1497912102]
[2019-03-27 04:34:02,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:34:02,948] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4266396e-34 1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5094545e-34], sampled 0.08299242484436331
[2019-03-27 04:34:13,967] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:34:13,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 90.0, 1.0, 2.0, 0.6755937162824901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 944146.3058865666, 944146.3058865673, 215663.5895416749]
[2019-03-27 04:34:13,970] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:34:13,975] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.36195974e-33 1.00000000e+00 0.00000000e+00 1.71965229e-37
 1.22402846e-32], sampled 0.8916319141036577
[2019-03-27 04:34:25,464] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17013653]
[2019-03-27 04:34:25,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.45, 88.0, 1.0, 2.0, 0.5897622103177494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824149.7178947759, 824149.7178947759, 198853.70176682]
[2019-03-27 04:34:25,467] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:34:25,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9326429e-30 1.0000000e+00 0.0000000e+00 1.2539352e-32 1.6452582e-24], sampled 0.49298738653229146
[2019-03-27 04:34:32,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8677.5170 2778768493.4068 891.0000
[2019-03-27 04:34:32,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8120.4166 3001275425.9372 1437.0000
[2019-03-27 04:34:33,010] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7944.0636 3160898268.8226 1742.0000
[2019-03-27 04:34:33,022] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8542.7642 2839818279.5747 1037.0000
[2019-03-27 04:34:33,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8279.7125 2926770134.6914 1276.0000
[2019-03-27 04:34:34,057] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2425000, evaluation results [2425000.0, 7944.063571268634, 3160898268.822645, 1742.0, 8279.712491468883, 2926770134.6914186, 1276.0, 8677.516980568742, 2778768493.406807, 891.0, 8120.416629162231, 3001275425.9371786, 1437.0, 8542.76416669916, 2839818279.574671, 1037.0]
[2019-03-27 04:34:36,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.8487487e-30 1.0000000e+00 0.0000000e+00 2.2433919e-34 2.3426945e-25], sum to 1.0000
[2019-03-27 04:34:36,575] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4334
[2019-03-27 04:34:36,583] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 72.66666666666666, 1.0, 2.0, 0.5334975255055133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745496.3285065243, 745496.3285065236, 189011.3577720588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [29.33333333333333, 73.33333333333334, 1.0, 2.0, 0.5277161059006845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737414.7106371127, 737414.7106371133, 188052.9137473982], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494469, 0.7333333333333334, 1.0, 1.0, 0.4309832601213066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048374196214202, 0.20483741962142035, 0.2806759906677585], 
reward next is 0.7193, 
noisyNet noise sample is [array([-1.4087269], dtype=float32), -0.18410107]. 
=============================================
[2019-03-27 04:34:43,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.2094214e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-27 04:34:43,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6402
[2019-03-27 04:34:43,937] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6578589395102451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 919351.1207138947, 919351.120713894, 212008.7175901417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3468600.0000, 
sim time next is 3469200.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7048481348767504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985048.4932546489, 985048.4932546489, 221891.7660491438], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.6443953432250004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2736245814596247, 0.2736245814596247, 0.33118174037185644], 
reward next is 0.6688, 
noisyNet noise sample is [array([0.06905127], dtype=float32), -1.4485612]. 
=============================================
[2019-03-27 04:34:45,669] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1749571e-22 1.0000000e+00 1.3212288e-27 1.3607565e-22 1.9196017e-17], sum to 1.0000
[2019-03-27 04:34:45,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7211
[2019-03-27 04:34:45,681] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1840695.267558282 W.
[2019-03-27 04:34:45,688] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 83.16666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.456257545473607, 6.9112, 168.9098587435018, 1840695.267558282, 1454019.77690357, 311353.9994023915], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3988200.0000, 
sim time next is 3988800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.580307437914303, 1.0, 1.0, 0.580307437914303, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1622472.283032682, 1622472.283032682, 327100.7487105789], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49434631074012403, 1.0, 0.5, 0.49434631074012403, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4506867452868561, 0.4506867452868561, 0.48821007270235656], 
reward next is 0.5118, 
noisyNet noise sample is [array([-0.33635697], dtype=float32), -0.9172527]. 
=============================================
[2019-03-27 04:34:49,849] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:49,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5006
[2019-03-27 04:34:49,864] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.83333333333333, 1.0, 2.0, 0.5501884926071485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768828.3164633423, 768828.3164633423, 191832.396313325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3909000.0000, 
sim time next is 3909600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5456499313002581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762483.8965219074, 762483.8965219074, 191056.8420247099], 
processed observation next is [0.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4525902786750097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2118010823671965, 0.2118010823671965, 0.28515946570852224], 
reward next is 0.7148, 
noisyNet noise sample is [array([-0.12522368], dtype=float32), -0.57802504]. 
=============================================
[2019-03-27 04:34:54,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5671568e-15 9.5809525e-01 6.8867917e-20 2.4322363e-12 4.1904699e-02], sum to 1.0000
[2019-03-27 04:34:54,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4702
[2019-03-27 04:34:54,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2376146.235062529 W.
[2019-03-27 04:34:54,774] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666667, 61.66666666666667, 1.0, 2.0, 0.5663890729823965, 1.0, 2.0, 0.5663890729823965, 1.0, 2.0, 0.9836305120417576, 6.911199999999999, 6.9112, 170.5573041426782, 2376146.235062529, 2376146.23506253, 464090.8776007957], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [32.5, 63.0, 1.0, 2.0, 0.5683069124695426, 1.0, 2.0, 0.5683069124695426, 1.0, 2.0, 0.9869611649917202, 6.9112, 6.9112, 170.5573041426782, 2384199.734932865, 2384199.734932865, 465595.0551800104], 
processed observation next is [1.0, 0.6956521739130435, 0.7393364928909952, 0.63, 1.0, 1.0, 0.47988784634884646, 1.0, 1.0, 0.47988784634884646, 1.0, 1.0, 0.9840989816972195, 0.0, 0.0, 0.8375144448122397, 0.662277704148018, 0.662277704148018, 0.6949179928059857], 
reward next is 0.3051, 
noisyNet noise sample is [array([-0.5364846], dtype=float32), 1.4296803]. 
=============================================
[2019-03-27 04:34:55,917] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0150863e-14 9.5272256e-04 3.0007778e-19 8.9362281e-11 9.9904722e-01], sum to 1.0000
[2019-03-27 04:34:55,926] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4594
[2019-03-27 04:34:55,931] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.33333333333334, 62.0, 1.0, 2.0, 0.7337854781347666, 1.0, 2.0, 0.6874827785816459, 1.0, 2.0, 1.03, 7.005100396260729, 6.9112, 170.5573041426782, 2884751.624507109, 2817486.951041641, 532212.499809958], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3771600.0000, 
sim time next is 3772200.0000, 
raw observation next is [33.16666666666666, 62.5, 1.0, 2.0, 0.730089052872812, 1.0, 2.0, 0.6856345659506686, 1.0, 2.0, 1.03, 7.00510010476975, 6.9112, 170.5573041426782, 2876987.394738802, 2809722.930080179, 531001.0353097037], 
processed observation next is [1.0, 0.6521739130434783, 0.7709320695102682, 0.625, 1.0, 1.0, 0.6748060877985687, 1.0, 1.0, 0.6212464650008056, 1.0, 1.0, 1.0365853658536586, 0.009390010476974986, 0.0, 0.8375144448122397, 0.7991631652052228, 0.7804785916889386, 0.7925388586711996], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.984801], dtype=float32), -0.8139779]. 
=============================================
[2019-03-27 04:34:57,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:57,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1715
[2019-03-27 04:34:57,770] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6167753717940899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861913.9594688974, 861913.9594688974, 203913.5549506455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3844800.0000, 
sim time next is 3845400.0000, 
raw observation next is [34.08333333333334, 62.66666666666666, 1.0, 2.0, 0.6280475815689817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877672.8397971793, 877672.8397971793, 206087.6677657315], 
processed observation next is [0.0, 0.5217391304347826, 0.8143759873617699, 0.6266666666666666, 1.0, 1.0, 0.5518645561072067, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24379801105477203, 0.24379801105477203, 0.3075935339787037], 
reward next is 0.6924, 
noisyNet noise sample is [array([0.5503359], dtype=float32), 0.43863457]. 
=============================================
[2019-03-27 04:34:57,794] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:34:57,806] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0937
[2019-03-27 04:34:57,809] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.08333333333334, 62.66666666666666, 1.0, 2.0, 0.6280475815689817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 877672.8397971793, 877672.8397971793, 206087.6677657315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3845400.0000, 
sim time next is 3846000.0000, 
raw observation next is [34.16666666666667, 62.33333333333334, 1.0, 2.0, 0.6188923233385047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864873.5020697841, 864873.5020697841, 204319.0439386505], 
processed observation next is [0.0, 0.5217391304347826, 0.8183254344391787, 0.6233333333333334, 1.0, 1.0, 0.5408341245042225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24024263946382893, 0.24024263946382893, 0.30495379692335894], 
reward next is 0.6950, 
noisyNet noise sample is [array([0.5503359], dtype=float32), 0.43863457]. 
=============================================
[2019-03-27 04:34:57,827] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[71.69365 ]
 [71.71678 ]
 [71.643135]
 [71.63147 ]
 [71.60186 ]], R is [[71.68386078]
 [71.65943146]
 [71.63848877]
 [71.61784363]
 [71.59740448]].
[2019-03-27 04:35:01,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:35:01,436] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6909
[2019-03-27 04:35:01,440] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 92.33333333333334, 1.0, 2.0, 0.5635043300668936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787442.641918431, 787442.6419184317, 194143.037041534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [27.0, 91.5, 1.0, 2.0, 0.5593800536740255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 781677.2450658308, 781677.2450658302, 193421.7347716822], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.915, 1.0, 1.0, 0.4691325947879826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21713256807384188, 0.2171325680738417, 0.2886891563756451], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.93536985], dtype=float32), -0.34310192]. 
=============================================
[2019-03-27 04:35:02,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:35:02,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3291
[2019-03-27 04:35:02,094] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.00000000000001, 1.0, 2.0, 0.6183279808014628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864084.53857606, 864084.5385760607, 204210.7737611511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6156361652512266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860321.3282730926, 860321.3282730932, 203695.9286405581], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5369110424713572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23897814674252574, 0.2389781467425259, 0.3040237740903852], 
reward next is 0.6960, 
noisyNet noise sample is [array([0.7368148], dtype=float32), -0.5257663]. 
=============================================
[2019-03-27 04:35:02,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.89849 ]
 [70.85639 ]
 [70.867096]
 [70.79701 ]
 [70.79108 ]], R is [[70.91999817]
 [70.90600586]
 [70.89050293]
 [70.87818909]
 [70.86631775]].
[2019-03-27 04:35:05,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6460650e-33 1.0000000e+00 0.0000000e+00 2.4880417e-36 8.6187916e-32], sum to 1.0000
[2019-03-27 04:35:05,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0704
[2019-03-27 04:35:05,890] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5419617560366935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757328.2525986155, 757328.2525986155, 190432.0600581613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4060800.0000, 
sim time next is 4061400.0000, 
raw observation next is [27.96666666666667, 84.16666666666667, 1.0, 2.0, 0.5423837146603072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757918.1008522207, 757918.1008522207, 190503.3959678632], 
processed observation next is [1.0, 0.0, 0.524486571879937, 0.8416666666666667, 1.0, 1.0, 0.44865507790398457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2105328057922835, 0.2105328057922835, 0.2843334268177063], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.054574], dtype=float32), 0.27137965]. 
=============================================
[2019-03-27 04:35:09,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.03331335e-33 1.00000000e+00 0.00000000e+00 2.60801105e-37
 2.17432469e-30], sum to 1.0000
[2019-03-27 04:35:09,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0270
[2019-03-27 04:35:09,619] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6225070610073073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869927.0001083423, 869927.0001083417, 205014.3152212159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6216299483060438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868700.7713449328, 868700.7713449328, 204845.1600819521], 
processed observation next is [1.0, 0.9565217391304348, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5441324678386069, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2413057698180369, 0.2413057698180369, 0.305739044898436], 
reward next is 0.6943, 
noisyNet noise sample is [array([0.17515428], dtype=float32), -0.49056846]. 
=============================================
[2019-03-27 04:35:14,261] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.4953416e-13 2.3323061e-02 1.2075773e-15 2.6421173e-08 9.7667694e-01], sum to 1.0000
[2019-03-27 04:35:14,267] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8434
[2019-03-27 04:35:14,273] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 58.66666666666667, 1.0, 2.0, 0.9221586890786171, 1.0, 2.0, 0.781669384053571, 1.0, 2.0, 1.03, 7.005115255704278, 6.9112, 170.5573041426782, 3280487.788327605, 3213212.470438279, 600687.4455002234], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [35.0, 58.0, 1.0, 2.0, 0.9073175194173062, 1.0, 2.0, 0.7742487992229156, 1.0, 2.0, 1.03, 7.005114084646754, 6.9112, 170.5573041426782, 3249304.7723832, 3182030.293370014, 594812.0043996938], 
processed observation next is [1.0, 0.43478260869565216, 0.8578199052132701, 0.58, 1.0, 1.0, 0.8883343607437424, 1.0, 1.0, 0.7280106014733922, 1.0, 1.0, 1.0365853658536586, 0.009391408464675432, 0.0, 0.8375144448122397, 0.9025846589953334, 0.8838973037138927, 0.8877791110443191], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.15465295], dtype=float32), -0.3924434]. 
=============================================
[2019-03-27 04:35:14,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[40.099163]
 [39.70474 ]
 [39.453735]
 [38.525787]
 [38.50291 ]], R is [[40.34355545]
 [39.9401207 ]
 [39.54071808]
 [39.36377716]
 [39.18657303]].
[2019-03-27 04:35:26,376] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4911909e-36 1.0000000e+00 0.0000000e+00 0.0000000e+00 2.6449412e-37], sum to 1.0000
[2019-03-27 04:35:26,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3707
[2019-03-27 04:35:26,387] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527835446855871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315176, 192278.7896705648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4580091082558088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.2863071135554025], 
reward next is 0.7137, 
noisyNet noise sample is [array([-0.8922915], dtype=float32), -0.66333574]. 
=============================================
[2019-03-27 04:35:28,935] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-27 04:35:28,938] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:35:28,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:35:28,939] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:35:28,940] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:35:28,941] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:35:28,941] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:35:28,941] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:35:28,944] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:35:28,944] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:35:28,945] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:35:28,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-27 04:35:28,996] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-27 04:35:29,021] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-27 04:35:29,021] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-27 04:35:29,039] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-27 04:35:31,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:35:31,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.38333333333333, 89.0, 1.0, 2.0, 0.3465905827298968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537635.9804984198, 537635.9804984193, 169728.4461324258]
[2019-03-27 04:35:31,062] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:35:31,064] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2616150181167931
[2019-03-27 04:35:57,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:35:57,648] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.9, 94.00000000000001, 1.0, 2.0, 0.406230069021754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 608233.121295503, 608233.1212955036, 175352.1507993203]
[2019-03-27 04:35:57,649] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:35:57,651] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.979521e-37 1.000000e+00 0.000000e+00 0.000000e+00 9.704734e-37], sampled 0.9591290411989993
[2019-03-27 04:36:03,409] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:03,411] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.32224688333334, 90.08313740333332, 1.0, 2.0, 0.6274344503352215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876815.6583114971, 876815.6583114971, 205960.552329382]
[2019-03-27 04:36:03,414] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:36:03,417] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3987164131502723
[2019-03-27 04:36:11,770] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:11,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.95, 77.83333333333333, 1.0, 2.0, 0.5230005542807501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730823.0730070027, 730823.0730070027, 187277.3895765281]
[2019-03-27 04:36:11,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:36:11,775] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.797192928766488
[2019-03-27 04:36:13,550] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:13,551] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.01235766166667, 99.322911655, 1.0, 2.0, 0.2452201880166368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 404758.4476553897, 404758.447655389, 160395.5621000023]
[2019-03-27 04:36:13,552] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:36:13,555] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.20862852607023807
[2019-03-27 04:36:25,188] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:25,190] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.53787433833334, 79.97612720500001, 1.0, 2.0, 0.5162520529730716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734067.043655716, 734067.043655716, 187802.9069570918]
[2019-03-27 04:36:25,191] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:36:25,193] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3781790703567539
[2019-03-27 04:36:40,904] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:40,907] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.83333333333334, 80.33333333333334, 1.0, 2.0, 0.6363130332834463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 889228.3326375772, 889228.3326375779, 207705.9219504403]
[2019-03-27 04:36:40,908] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:36:40,913] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.7495194e-32 1.0000000e+00 0.0000000e+00 2.1147841e-34 2.0216831e-29], sampled 0.3972567668769871
[2019-03-27 04:36:45,367] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:45,368] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5363698225970476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749511.4219028318, 749511.4219028311, 189490.8007199865]
[2019-03-27 04:36:45,369] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:36:45,372] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3192465e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6546925e-38], sampled 0.09305486675575825
[2019-03-27 04:36:46,571] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.17256892]
[2019-03-27 04:36:46,572] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 72.5, 1.0, 2.0, 0.5311632453183409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565103247, 742233.3231288036, 742233.3231288036, 188625.4607047525]
[2019-03-27 04:36:46,574] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:36:46,576] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2518149e-37 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6748277802005821
[2019-03-27 04:37:24,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.9108 3164536274.6413 1844.0000
[2019-03-27 04:37:25,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.3589 3007754266.6397 1776.0000
[2019-03-27 04:37:25,385] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779103513.6362 932.0000
[2019-03-27 04:37:25,396] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-27 04:37:25,445] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.8841 2842328613.7993 1145.0000
[2019-03-27 04:37:26,465] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2450000, evaluation results [2450000.0, 7875.910768388362, 3164536274.6412764, 1844.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779103513.636221, 932.0, 7999.358885595918, 3007754266.639721, 1776.0, 8494.884118026464, 2842328613.7993364, 1145.0]
[2019-03-27 04:37:39,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.64609572e-19 1.00000000e+00 3.57320661e-24 1.00641675e-17
 7.84186049e-09], sum to 1.0000
[2019-03-27 04:37:39,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0821
[2019-03-27 04:37:39,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1839017.97250032 W.
[2019-03-27 04:37:39,053] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.4, 65.0, 1.0, 2.0, 0.6741942082275565, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977082378527915, 6.9112, 168.9125109375901, 1839017.97250032, 1792278.868968819, 380167.3039271091], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [30.43333333333333, 64.83333333333334, 1.0, 2.0, 0.5904857584906099, 1.0, 1.0, 0.5904857584906099, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1650951.606452352, 1650951.606452352, 330760.2125480813], 
processed observation next is [1.0, 0.5652173913043478, 0.6413902053712479, 0.6483333333333334, 1.0, 1.0, 0.5066093475790481, 1.0, 0.5, 0.5066093475790481, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.45859766845898664, 0.45859766845898664, 0.493671959026987], 
reward next is 0.5063, 
noisyNet noise sample is [array([0.18698215], dtype=float32), -0.91055447]. 
=============================================
[2019-03-27 04:37:40,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:37:40,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4869
[2019-03-27 04:37:40,646] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4953754810496339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692208.163827431, 692208.163827431, 182873.8577909603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4506000.0000, 
sim time next is 4506600.0000, 
raw observation next is [26.0, 84.83333333333333, 1.0, 2.0, 0.49100172176835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686094.5597005656, 686094.5597005662, 182197.3889295147], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8483333333333333, 1.0, 1.0, 0.38674906237150597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.190581822139046, 0.19058182213904618, 0.2719364013873354], 
reward next is 0.7281, 
noisyNet noise sample is [array([-0.8232318], dtype=float32), 1.8011168]. 
=============================================
[2019-03-27 04:37:41,363] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3991375e-28 1.0000000e+00 4.0245437e-35 7.0401280e-29 1.5135943e-20], sum to 1.0000
[2019-03-27 04:37:41,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1148
[2019-03-27 04:37:41,378] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4518132224550012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647699.9483840843, 647699.9483840849, 178493.5296850415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4654800.0000, 
sim time next is 4655400.0000, 
raw observation next is [24.08333333333333, 94.00000000000001, 1.0, 2.0, 0.4527319079443171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648668.7607734681, 648668.7607734681, 178584.2729055045], 
processed observation next is [1.0, 0.9130434782608695, 0.34044233807266966, 0.9400000000000002, 1.0, 1.0, 0.34064085294496044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1801857668815189, 0.1801857668815189, 0.2665436909037381], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.01006032], dtype=float32), 1.3418008]. 
=============================================
[2019-03-27 04:38:02,025] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1752562e-16 9.9999988e-01 6.4002353e-22 2.0644138e-14 7.8239204e-08], sum to 1.0000
[2019-03-27 04:38:02,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0401
[2019-03-27 04:38:02,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2057858.678355115 W.
[2019-03-27 04:38:02,047] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8305703603841987, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990061226517246, 6.9112, 168.9124233454095, 2057858.678355115, 2001911.986663235, 416126.369746398], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4878000.0000, 
sim time next is 4878600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.7480444457920963, 1.0, 1.0, 0.7480444457920963, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2091903.309572905, 2091903.309572905, 395369.1896971656], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.6964390913157786, 1.0, 0.5, 0.6964390913157786, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5810842526591403, 0.5810842526591403, 0.5901032682047248], 
reward next is 0.4099, 
noisyNet noise sample is [array([-1.4507383], dtype=float32), 0.5100847]. 
=============================================
[2019-03-27 04:38:02,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3237518e-13 6.7829233e-01 3.4705371e-19 3.0111406e-11 3.2170770e-01], sum to 1.0000
[2019-03-27 04:38:02,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6862
[2019-03-27 04:38:02,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.536320991850381, 1.0, 2.0, 0.536320991850381, 1.0, 2.0, 0.9314121987818251, 6.911199999999999, 6.9112, 170.5573041426782, 2249889.435909631, 2249889.435909632, 441172.0183298081], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5235600.0000, 
sim time next is 5236200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5180848042294943, 1.0, 2.0, 0.5180848042294943, 1.0, 2.0, 0.8997419716837471, 6.9112, 6.9112, 170.5573041426782, 2173315.638294342, 2173315.638294342, 427877.851618221], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.41937928220421, 1.0, 1.0, 0.41937928220421, 1.0, 1.0, 0.8777341118094477, 0.0, 0.0, 0.8375144448122397, 0.603698788415095, 0.603698788415095, 0.6386236591316731], 
reward next is 0.3614, 
noisyNet noise sample is [array([0.5547309], dtype=float32), 1.2789605]. 
=============================================
[2019-03-27 04:38:12,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7832000e-22 8.3504886e-19 1.8017238e-25 5.8885752e-15 1.0000000e+00], sum to 1.0000
[2019-03-27 04:38:12,132] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2075
[2019-03-27 04:38:12,143] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.93333333333333, 56.0, 1.0, 2.0, 0.9483856156006855, 1.0, 2.0, 0.7947828473146055, 1.0, 2.0, 1.03, 7.005117325309397, 6.9112, 170.5573041426782, 3335595.501829841, 3268318.701398182, 611272.6989571152], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5406000.0000, 
sim time next is 5406600.0000, 
raw observation next is [36.91666666666666, 55.0, 1.0, 2.0, 0.9157275387574808, 1.0, 2.0, 0.778453808893003, 1.0, 2.0, 1.03, 7.005114748240672, 6.9112, 170.5573041426782, 3266975.101223368, 3199700.146850865, 598132.416154909], 
processed observation next is [1.0, 0.5652173913043478, 0.9486571879936805, 0.55, 1.0, 1.0, 0.8984669141656395, 1.0, 1.0, 0.733076878184341, 1.0, 1.0, 1.0365853658536586, 0.009391474824067192, 0.0, 0.8375144448122397, 0.9074930836731578, 0.8888055963474625, 0.8927349494849387], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49744406], dtype=float32), 0.8623798]. 
=============================================
[2019-03-27 04:38:21,144] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-27 04:38:21,146] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:38:21,147] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:38:21,148] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:38:21,149] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:38:21,151] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:38:21,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:38:21,152] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:38:21,152] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:38:21,154] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:38:21,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:38:21,179] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-27 04:38:21,204] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-27 04:38:21,205] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-27 04:38:21,225] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-27 04:38:21,268] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-27 04:38:52,573] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:38:52,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.83333333333333, 94.0, 1.0, 2.0, 0.4923217855668255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687939.7301255949, 687939.7301255956, 182401.0855384014]
[2019-03-27 04:38:52,578] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:38:52,581] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7669116678136102
[2019-03-27 04:38:57,982] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:38:57,985] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.85433126, 77.997978965, 1.0, 2.0, 0.668920847915184, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.974421962377445, 6.9112, 168.9125224168962, 1831638.951618361, 1786787.230841616, 379142.7413292563]
[2019-03-27 04:38:57,986] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:38:57,990] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.6570399e-26 1.0000000e+00 1.8827164e-33 9.1345193e-27 1.6957903e-22], sampled 0.03639329421760995
[2019-03-27 04:38:57,993] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1831638.951618361 W.
[2019-03-27 04:39:22,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:39:22,213] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.12705158833333, 94.353152045, 1.0, 2.0, 0.9961827894021584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104264, 1392465.250005736, 1392465.250005737, 297762.2058767775]
[2019-03-27 04:39:22,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:39:22,218] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1412918e-31 1.0000000e+00 0.0000000e+00 1.8731375e-34 1.9751202e-31], sampled 0.17462600022054486
[2019-03-27 04:39:28,321] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:39:28,323] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.85917449, 81.09056789666667, 1.0, 2.0, 0.5060072999515962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707069.3780474188, 707069.3780474188, 184541.6066637946]
[2019-03-27 04:39:28,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:39:28,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.5361712e-33 1.0000000e+00 0.0000000e+00 4.3223008e-36 2.2382750e-31], sampled 0.7188893707007507
[2019-03-27 04:39:39,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:39:39,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.93976103666667, 68.88444311666667, 1.0, 2.0, 0.4451492968822011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654689.644211447, 654689.644211447, 179583.0745986911]
[2019-03-27 04:39:39,510] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-27 04:39:39,511] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.22986434e-36 1.00000000e+00 0.00000000e+00 0.00000000e+00
 1.17757945e-35], sampled 0.24582177074435207
[2019-03-27 04:39:49,513] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:39:49,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 89.0, 1.0, 2.0, 0.5566925219880691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777920.3151722233, 777920.3151722233, 192955.2918158165]
[2019-03-27 04:39:49,515] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:39:49,516] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.5390344e-32 1.0000000e+00 0.0000000e+00 1.9981475e-34 3.0065819e-29], sampled 0.052217648903631786
[2019-03-27 04:39:56,476] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.1710327]
[2019-03-27 04:39:56,477] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 74.33333333333333, 1.0, 2.0, 0.7560486899641692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1056638.55446752, 1056638.554467521, 233410.8965456752]
[2019-03-27 04:39:56,477] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:39:56,480] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.6595051e-31 1.0000000e+00 0.0000000e+00 3.6679729e-33 2.1006947e-29], sampled 0.7497726813888287
[2019-03-27 04:40:14,988] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.0387 3164277649.6619 1859.0000
[2019-03-27 04:40:15,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.3489 2842157405.1871 1150.0000
[2019-03-27 04:40:15,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.8388 3007797799.3287 1753.0000
[2019-03-27 04:40:15,459] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6313 2927313249.4896 1340.0000
[2019-03-27 04:40:15,582] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.7721 2779096540.4800 930.0000
[2019-03-27 04:40:16,597] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2475000, evaluation results [2475000.0, 7882.038683703866, 3164277649.661918, 1859.0, 8254.63133097416, 2927313249.4895597, 1340.0, 8662.772132397648, 2779096540.479963, 930.0, 8002.838784506788, 3007797799.3286734, 1753.0, 8494.348853287362, 2842157405.1870756, 1150.0]
[2019-03-27 04:40:17,907] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.7119215e-19 1.0000000e+00 3.0530365e-26 4.8324702e-18 1.2684430e-13], sum to 1.0000
[2019-03-27 04:40:17,913] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2560
[2019-03-27 04:40:17,917] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 84.0, 1.0, 2.0, 0.9591801204144395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1340710.223663296, 1340710.223663295, 286740.5327547045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5295600.0000, 
sim time next is 5296200.0000, 
raw observation next is [29.83333333333333, 83.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.112382654854306, 6.9112, 168.9119771824497, 1596577.743955647, 1453852.671929375, 311355.8802414009], 
processed observation next is [1.0, 0.30434782608695654, 0.6129541864139019, 0.835, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.02011826548543061, 0.0, 0.8294351362035762, 0.44349381776545754, 0.4038479644248264, 0.4647102690170163], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7442589], dtype=float32), -0.80230886]. 
=============================================
[2019-03-27 04:40:24,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0804906e-23 1.0000000e+00 9.9149047e-33 7.0920001e-26 2.3888449e-18], sum to 1.0000
[2019-03-27 04:40:24,107] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9892
[2019-03-27 04:40:24,112] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.8, 80.33333333333334, 1.0, 2.0, 0.6167525996765194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861882.12361638, 861882.12361638, 203909.3314807037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5426400.0000, 
sim time next is 5427000.0000, 
raw observation next is [30.8, 81.0, 1.0, 2.0, 0.6219566251502986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 869157.4747960069, 869157.4747960076, 204908.6164966634], 
processed observation next is [1.0, 0.8260869565217391, 0.6587677725118484, 0.81, 1.0, 1.0, 0.5445260543979501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2414326318877797, 0.24143263188777989, 0.30583375596516926], 
reward next is 0.6942, 
noisyNet noise sample is [array([0.6751384], dtype=float32), -1.5980893]. 
=============================================
[2019-03-27 04:40:24,123] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[67.81117]
 [67.97284]
 [68.07245]
 [68.14522]
 [67.87856]], R is [[67.94428253]
 [67.960495  ]
 [67.97800446]
 [67.99716187]
 [68.01818085]].
[2019-03-27 04:40:30,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2236921e-19 1.0000000e+00 4.7983759e-26 1.4710519e-17 1.7530541e-11], sum to 1.0000
[2019-03-27 04:40:30,757] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6263
[2019-03-27 04:40:30,763] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 95.0, 1.0, 2.0, 0.6727435266841869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940161.3856267879, 940161.3856267885, 215070.7680563582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5546400.0000, 
sim time next is 5547000.0000, 
raw observation next is [25.63333333333333, 95.0, 1.0, 2.0, 0.6666946170284421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 931704.3056290358, 931704.3056290364, 213819.2751754902], 
processed observation next is [1.0, 0.17391304347826086, 0.4139020537124801, 0.95, 1.0, 1.0, 0.598427249431858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25880675156362104, 0.2588067515636212, 0.3191332465305824], 
reward next is 0.6809, 
noisyNet noise sample is [array([0.7786225], dtype=float32), 0.5631849]. 
=============================================
[2019-03-27 04:40:30,771] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[51.28481 ]
 [51.978237]
 [52.08074 ]
 [52.696465]
 [53.130146]], R is [[51.13899994]
 [51.30660629]
 [51.47011566]
 [51.62583542]
 [51.76173782]].
[2019-03-27 04:40:31,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.5741366e-31 1.0000000e+00 0.0000000e+00 4.6273977e-36 6.8944883e-28], sum to 1.0000
[2019-03-27 04:40:31,797] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9667
[2019-03-27 04:40:31,801] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.46666666666667, 58.33333333333334, 1.0, 2.0, 0.4923990788460354, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688047.7699068038, 688047.7699068038, 182416.9684772616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5592000.0000, 
sim time next is 5592600.0000, 
raw observation next is [32.2, 60.0, 1.0, 2.0, 0.487147862103459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680707.6960670656, 680707.6960670656, 181610.6422025247], 
processed observation next is [1.0, 0.7391304347826086, 0.7251184834123224, 0.6, 1.0, 1.0, 0.38210585795597474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18908547112974045, 0.18908547112974045, 0.2710606600037682], 
reward next is 0.7289, 
noisyNet noise sample is [array([-1.0082784], dtype=float32), -0.7434269]. 
=============================================
[2019-03-27 04:40:38,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8975314e-18 9.9999881e-01 6.5344701e-26 7.2081015e-18 1.1574185e-06], sum to 1.0000
[2019-03-27 04:40:38,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6069
[2019-03-27 04:40:38,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2056317.354432209 W.
[2019-03-27 04:40:38,528] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.73333333333333, 66.33333333333334, 1.0, 2.0, 0.8294690958859292, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989564079449371, 6.9112, 168.9124903347831, 2056317.354432209, 2000723.332819347, 415858.6532873502], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6103200.0000, 
sim time next is 6103800.0000, 
raw observation next is [30.7, 66.5, 1.0, 2.0, 0.5038215712395286, 1.0, 1.0, 0.5038215712395286, 1.0, 2.0, 0.8691628372863667, 6.9112, 6.9112, 170.5573041426782, 2113423.73040635, 2113423.73040635, 416739.8548799303], 
processed observation next is [1.0, 0.6521739130434783, 0.6540284360189573, 0.665, 1.0, 1.0, 0.4021946641440104, 1.0, 0.5, 0.4021946641440104, 1.0, 1.0, 0.8404424844955691, 0.0, 0.0, 0.8375144448122397, 0.5870621473350972, 0.5870621473350972, 0.6219997834028811], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.71030194], dtype=float32), -0.40758538]. 
=============================================
[2019-03-27 04:40:39,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9297685e-17 1.8541391e-03 9.2498533e-22 3.4877793e-12 9.9814582e-01], sum to 1.0000
[2019-03-27 04:40:39,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5386
[2019-03-27 04:40:39,294] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.2, 70.0, 1.0, 2.0, 0.5097929222731457, 1.0, 2.0, 0.5097929222731457, 1.0, 2.0, 0.8824258954296049, 6.911200000000001, 6.9112, 170.5573041426782, 2138497.273968051, 2138497.273968051, 421446.6386900129], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6084000.0000, 
sim time next is 6084600.0000, 
raw observation next is [30.33333333333334, 69.16666666666667, 1.0, 2.0, 0.446731950783992, 1.0, 2.0, 0.446731950783992, 1.0, 2.0, 0.7724646746877338, 6.911199999999999, 6.9112, 170.5573041426782, 1873735.559754465, 1873735.559754466, 379731.9148941583], 
processed observation next is [1.0, 0.43478260869565216, 0.6366508688783573, 0.6916666666666668, 1.0, 1.0, 0.33341198889637597, 1.0, 1.0, 0.33341198889637597, 1.0, 1.0, 0.7225178959606509, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5204820999317958, 0.5204820999317961, 0.5667640520808332], 
reward next is 0.4332, 
noisyNet noise sample is [array([-1.6001195], dtype=float32), -0.11894627]. 
=============================================
[2019-03-27 04:40:41,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:40:41,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2171
[2019-03-27 04:40:41,590] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 91.33333333333334, 1.0, 2.0, 0.5038736434684384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704086.9247482967, 704086.9247482962, 184204.8727349343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5629200.0000, 
sim time next is 5629800.0000, 
raw observation next is [25.61666666666667, 91.16666666666667, 1.0, 2.0, 0.5019505234115789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701398.7692300805, 701398.7692300805, 183901.9238461858], 
processed observation next is [0.0, 0.13043478260869565, 0.41311216429699865, 0.9116666666666667, 1.0, 1.0, 0.3999403896525046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19483299145280014, 0.19483299145280014, 0.2744804833525161], 
reward next is 0.7255, 
noisyNet noise sample is [array([0.8026956], dtype=float32), 0.8101669]. 
=============================================
[2019-03-27 04:40:54,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7070137e-15 7.3766150e-06 3.9918263e-19 7.6846005e-09 9.9999261e-01], sum to 1.0000
[2019-03-27 04:40:54,251] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3621
[2019-03-27 04:40:54,256] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 74.0, 1.0, 2.0, 0.5628246543293312, 1.0, 2.0, 0.5628246543293312, 1.0, 2.0, 0.9774402956126443, 6.9112, 6.9112, 170.5573041426782, 2361178.464851165, 2361178.464851165, 461308.7111499668], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5821200.0000, 
sim time next is 5821800.0000, 
raw observation next is [30.25, 73.33333333333334, 1.0, 2.0, 0.5583791395033646, 1.0, 2.0, 0.5583791395033646, 1.0, 2.0, 0.9697199065141585, 6.9112, 6.9112, 170.5573041426782, 2342511.031141739, 2342511.031141739, 457863.6001325911], 
processed observation next is [1.0, 0.391304347826087, 0.6327014218009479, 0.7333333333333334, 1.0, 1.0, 0.46792667410043925, 1.0, 1.0, 0.46792667410043925, 1.0, 1.0, 0.9630730567245834, 0.0, 0.0, 0.8375144448122397, 0.650697508650483, 0.650697508650483, 0.6833785076605837], 
reward next is 0.3166, 
noisyNet noise sample is [array([1.7007015], dtype=float32), 1.7002057]. 
=============================================
[2019-03-27 04:40:54,454] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-27 04:40:54,465] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6201
[2019-03-27 04:40:54,472] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 90.5, 1.0, 2.0, 0.5208180888733931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 727772.3258259722, 727772.325825973, 186922.3721718123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6226200.0000, 
sim time next is 6226800.0000, 
raw observation next is [26.43333333333333, 90.66666666666667, 1.0, 2.0, 0.521970848881068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729383.7039258432, 729383.7039258427, 187110.2387879196], 
processed observation next is [0.0, 0.043478260869565216, 0.4518167456556081, 0.9066666666666667, 1.0, 1.0, 0.42406126371213015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20260658442384535, 0.2026065844238452, 0.2792690131162979], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.4995363], dtype=float32), 0.85816824]. 
=============================================
[2019-03-27 04:41:07,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9716010e-28 1.0000000e+00 5.6571805e-37 3.9351812e-30 4.2824454e-24], sum to 1.0000
[2019-03-27 04:41:07,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8359
[2019-03-27 04:41:07,685] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 88.33333333333334, 1.0, 2.0, 0.540081286645159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754699.582762176, 754699.5827621754, 190114.1432237246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6041400.0000, 
sim time next is 6042000.0000, 
raw observation next is [27.1, 88.66666666666667, 1.0, 2.0, 0.5386464313859132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752693.832214706, 752693.832214706, 189872.6832112029], 
processed observation next is [1.0, 0.9565217391304348, 0.4834123222748816, 0.8866666666666667, 1.0, 1.0, 0.44415232697097967, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20908162005964057, 0.20908162005964057, 0.2833920644943327], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.24660936], dtype=float32), 1.2420729]. 
=============================================
[2019-03-27 04:41:07,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.3175 ]
 [73.2971 ]
 [73.31986]
 [73.14886]
 [73.27592]], R is [[73.32674408]
 [73.3097229 ]
 [73.2926712 ]
 [73.27590179]
 [73.25938416]].
[2019-03-27 04:41:10,035] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-27 04:41:10,038] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-27 04:41:10,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:41:10,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-27 04:41:10,042] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:41:10,042] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-27 04:41:10,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-27 04:41:10,043] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:41:10,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-27 04:41:10,045] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:41:10,047] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-27 04:41:10,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-27 04:41:11,014] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-27 04:41:11,038] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-27 04:41:11,057] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-27 04:41:11,077] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-27 04:41:24,785] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16800076]
[2019-03-27 04:41:24,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.2, 70.0, 1.0, 2.0, 0.3041022323692932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481904.677424536, 481904.677424536, 165716.1897875097]
[2019-03-27 04:41:24,787] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:41:24,789] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5815825058832068
[2019-03-27 04:41:25,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16800076]
[2019-03-27 04:41:25,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.5, 86.66666666666667, 1.0, 2.0, 0.3023417340367312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482581.6482711594, 482581.6482711588, 165822.1985499161]
[2019-03-27 04:41:25,980] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-27 04:41:25,983] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13334735849416612
[2019-03-27 04:41:33,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16800076]
[2019-03-27 04:41:33,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 48.0, 1.0, 2.0, 0.2854998437869897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461540.0882468025, 461540.0882468025, 164372.2947905182]
[2019-03-27 04:41:33,143] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-27 04:41:33,148] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3858516e-38 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6745273982464032
[2019-03-27 04:42:16,637] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16800076]
[2019-03-27 04:42:16,638] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.05347665, 83.50013954, 1.0, 2.0, 0.6758823534531891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944549.857456027, 944549.857456027, 215737.0567369184]
[2019-03-27 04:42:16,640] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-27 04:42:16,642] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7259740089805792
[2019-03-27 04:42:47,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00239478], dtype=float32), -0.16800076]
[2019-03-27 04:42:47,542] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.1, 61.33333333333334, 1.0, 2.0, 0.749686032124815, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.962276480480314, 6.9112, 168.9126097237712, 1944659.416633999, 1908424.077857571, 397175.3557903498]
[2019-03-27 04:42:47,544] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-27 04:42:47,547] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5968023e-21 1.0000000e+00 4.4548603e-30 8.0129826e-21 4.2700274e-11], sampled 0.2044967186560549
[2019-03-27 04:42:47,548] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1944659.416633999 W.
[2019-03-27 04:43:04,803] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7869.7123 3164870342.5639 1930.0000
[2019-03-27 04:43:05,675] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.1654 3007401104.8375 1739.0000
[2019-03-27 04:43:05,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.2815 2778986023.2611 927.0000
[2019-03-27 04:43:05,717] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.3157 2927323374.4794 1336.0000
[2019-03-27 04:43:05,758] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.0809 2842113054.0524 1159.0000
[2019-03-27 04:43:06,774] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2500000, evaluation results [2500000.0, 7869.712315271004, 3164870342.563862, 1930.0, 8258.315653501411, 2927323374.4793715, 1336.0, 8663.28145970366, 2778986023.261072, 927.0, 8011.165388548541, 3007401104.837529, 1739.0, 8495.080868981859, 2842113054.0524225, 1159.0]
