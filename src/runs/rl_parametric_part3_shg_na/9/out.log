Using TensorFlow backend.
[2019-04-10 12:08:11,357] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=1.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=1000000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-04-10 12:08:11,357] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-10 12:08:11.426761: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-10 12:08:38,378] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-10 12:08:38,378] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-04-10 12:08:38,390] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-04-10 12:08:38,396] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-04-10 12:08:38,402] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-04-10 12:08:38,408] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-04-10 12:08:38,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-04-10 12:08:38,412] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:38,412] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-10 12:08:38,475] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:38,476] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-04-10 12:08:39,413] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:39,415] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-10 12:08:39,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-04-10 12:08:39,887] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 12:08:39,887] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:08:39,888] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:08:39,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,888] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:08:39,889] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:08:39,889] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:08:39,890] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,888] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,890] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,889] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:39,894] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-04-10 12:08:39,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-04-10 12:08:39,904] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-04-10 12:08:39,905] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-04-10 12:08:39,943] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-04-10 12:08:40,416] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:40,417] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-10 12:08:40,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:40,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-04-10 12:08:41,442] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:41,459] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-10 12:08:41,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:41,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-04-10 12:08:42,460] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:42,461] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-10 12:08:42,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:42,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-04-10 12:08:43,462] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:43,466] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-10 12:08:43,548] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:43,572] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-04-10 12:08:44,466] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:44,470] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-10 12:08:44,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:44,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-04-10 12:08:45,469] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:45,474] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-10 12:08:45,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:45,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-04-10 12:08:46,473] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:46,478] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-10 12:08:46,570] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:46,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-04-10 12:08:47,478] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:47,485] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-10 12:08:47,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:47,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-04-10 12:08:48,483] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:48,497] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-10 12:08:48,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:48,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-04-10 12:08:49,485] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:49,492] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-10 12:08:49,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:49,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-04-10 12:08:50,491] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:50,495] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-10 12:08:50,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:50,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-04-10 12:08:51,495] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:51,499] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-10 12:08:51,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:51,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-04-10 12:08:52,497] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:52,499] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-10 12:08:52,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:52,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-04-10 12:08:53,500] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-10 12:08:53,504] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-10 12:08:53,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:08:53,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-04-10 12:08:59,084] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:08:59,086] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.65042455, 65.39113766, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 1.0, 0.2686800290986726, 6.9112, 6.9112, 184.5923449428631, 698248.293710052, 698248.293710052, 262718.9118920114]
[2019-04-10 12:08:59,087] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:08:59,089] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.23276551 0.09461279 0.14740458 0.34773296 0.17748407], sampled 0.3957507784153631
[2019-04-10 12:09:04,174] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:09:04,175] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.14177877333334, 96.58700059333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8621811489413753, 6.9112, 6.9112, 168.912956510431, 713169.5215938382, 713169.5215938382, 214886.5274351313]
[2019-04-10 12:09:04,176] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:09:04,178] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.2587783  0.11256438 0.12897307 0.33445275 0.1652315 ], sampled 0.7061517835129562
[2019-04-10 12:09:08,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:09:08,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 90.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8112875556881052, 6.911200000000001, 6.9112, 168.912956510431, 678846.1498860896, 678846.149886089, 204148.6134759127]
[2019-04-10 12:09:08,148] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:09:08,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.25533298 0.11865596 0.15575022 0.30066356 0.16959728], sampled 0.36021032989551827
[2019-04-10 12:09:20,227] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:09:20,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.60964138833333, 68.20370212833333, 1.0, 1.0, 0.3150424500726052, 1.0, 1.0, 0.3150424500726052, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 880512.7569672462, 880512.7569672462, 253347.7548474724]
[2019-04-10 12:09:20,229] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:09:20,230] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.24624425 0.10729605 0.17063776 0.2862945  0.18952747], sampled 0.21165193207198363
[2019-04-10 12:09:20,230] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 880512.7569672462 W.
[2019-04-10 12:09:26,240] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:09:26,242] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 63.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9620566444750406, 6.9112, 6.9112, 168.912956510431, 781459.2991604363, 781459.2991604363, 237791.0140095666]
[2019-04-10 12:09:26,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:09:26,245] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.22596204 0.12950255 0.14090353 0.30924472 0.19438718], sampled 0.8298089102121892
[2019-04-10 12:09:29,528] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:09:29,529] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.13333333333333, 47.66666666666667, 1.0, 2.0, 0.5538323678304948, 1.0, 2.0, 0.5538323678304948, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 1548382.67192945, 1548382.671929451, 318317.8031746204]
[2019-04-10 12:09:29,530] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:09:29,533] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.25065818 0.1387103  0.13501893 0.2992633  0.1763493 ], sampled 0.3984063850237123
[2019-04-10 12:10:03,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-10 12:10:03,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.7, 86.5, 1.0, 2.0, 0.5706784194436841, 1.0, 2.0, 0.5706784194436841, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1595541.282636538, 1595541.282636538, 323395.8187785964]
[2019-04-10 12:10:03,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:10:03,785] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.22879392 0.12669256 0.15736678 0.2995515  0.18759525], sampled 0.5973320795650088
[2019-04-10 12:10:11,206] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3805.6640 3235574015.8819 694.0000
[2019-04-10 12:10:11,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3780.4695 3318874286.4471 860.0000
[2019-04-10 12:10:11,751] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3518.2921 3533437101.0681 1236.0000
[2019-04-10 12:10:12,043] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3827.0999 3199425974.6949 562.0000
[2019-04-10 12:10:12,074] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3674.4845 3370835781.4718 1079.0000
[2019-04-10 12:10:13,089] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3518.292079860058, 3533437101.068095, 1236.0, 3780.4695149858685, 3318874286.4470778, 860.0, 3827.0998975476477, 3199425974.694903, 562.0, 3674.484547538369, 3370835781.471782, 1079.0, 3805.664033341436, 3235574015.8818617, 694.0]
[2019-04-10 12:10:19,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.2388011  0.0804437  0.163241   0.33085313 0.18666102], sum to 1.0000
[2019-04-10 12:10:19,732] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-10 12:10:20,022] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.51666666666667, 83.0, 1.0, 1.0, 0.1789191738727701, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3220347897273707, 6.911200000000001, 6.9112, 168.912956510431, 562941.8508476615, 562941.8508476609, 197366.9113684911], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [22.73333333333333, 82.0, 1.0, 2.0, 0.2740823183862777, 1.0, 1.0, 0.2740823183862777, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 855331.2134074823, 855331.2134074823, 256402.0637122701], 
processed observation next is [1.0, 0.34782608695652173, 0.27646129541864134, 0.82, 1.0, 1.0, 0.12540038359792496, 1.0, 0.5, 0.12540038359792496, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23759200372430062, 0.23759200372430062, 0.3826896473317464], 
reward next is 0.6173, 
noisyNet noise sample is [array([-1.3627245], dtype=float32), -0.6324009]. 
=============================================
[2019-04-10 12:10:20,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.08968388]
 [-0.17420745]
 [-0.04346876]
 [-0.04578039]
 [-0.24028215]], R is [[0.51825535]
 [1.21849537]
 [1.95974731]
 [1.9401499 ]
 [1.92074847]].
[2019-04-10 12:10:23,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.18543984 0.10251798 0.18901737 0.31533846 0.2076863 ], sum to 1.0000
[2019-04-10 12:10:23,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3249
[2019-04-10 12:10:24,298] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.26666666666667, 76.33333333333334, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.2274836247660532, 6.911199999999999, 6.9112, 170.5573041426782, 584406.5950386381, 584406.5950386388, 242762.9356040794], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 66000.0000, 
sim time next is 66600.0000, 
raw observation next is [25.1, 77.0, 1.0, 2.0, 0.1939972504986588, 0.0, 1.0, 0.0, 1.0, 2.0, 0.3378039143371969, 6.911199999999999, 6.9112, 168.912956510431, 579234.8431236421, 579234.8431236428, 197618.585006166], 
processed observation next is [1.0, 0.782608695652174, 0.38862559241706174, 0.77, 1.0, 1.0, 0.028912349998384074, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.19244379797219135, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16089856753434503, 0.16089856753434523, 0.29495311194950147], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19447863], dtype=float32), 1.8114891]. 
=============================================
[2019-04-10 12:10:32,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.15540928 0.09922682 0.10369258 0.5533213  0.08834991], sum to 1.0000
[2019-04-10 12:10:32,615] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6821
[2019-04-10 12:10:32,622] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.58333333333334, 96.0, 1.0, 2.0, 0.4105429193159997, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7133871411297602, 6.911199999999999, 6.9112, 168.912956510431, 1222329.738009549, 1222329.73800955, 269531.3514810386], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [22.56666666666667, 96.0, 1.0, 2.0, 0.3905490964428907, 1.0, 1.0, 0.3905490964428907, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1155032.819133669, 1155032.819133669, 277352.5100003597], 
processed observation next is [1.0, 0.6956521739130435, 0.26856240126382325, 0.96, 1.0, 1.0, 0.2657218029432418, 1.0, 0.5, 0.2657218029432418, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3208424497593525, 0.3208424497593525, 0.41395897014979055], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38727817], dtype=float32), -0.49905246]. 
=============================================
[2019-04-10 12:10:37,911] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.72659016 0.15017322 0.0203035  0.09274493 0.01018822], sum to 1.0000
[2019-04-10 12:10:37,922] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9694
[2019-04-10 12:10:38,126] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.86666666666667, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5743734452001964, 6.9112, 6.9112, 168.912956510431, 503183.2770729339, 503183.2770729339, 162201.1109416525], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [21.95, 87.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 506279.2670007389, 506279.2670007389, 234844.0102282678], 
processed observation next is [0.0, 0.5652173913043478, 0.2393364928909953, 0.87, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14063312972242747, 0.14063312972242747, 0.3505134481018923], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82921547], dtype=float32), -2.1584477]. 
=============================================
[2019-04-10 12:10:42,203] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7730: loss 0.1275
[2019-04-10 12:10:42,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7730: learning rate 0.0001
[2019-04-10 12:10:42,303] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7748: loss 0.0634
[2019-04-10 12:10:42,305] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7749: learning rate 0.0001
[2019-04-10 12:10:42,421] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7825: loss 0.0250
[2019-04-10 12:10:42,423] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7826: learning rate 0.0001
[2019-04-10 12:10:42,435] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7833: loss 0.0184
[2019-04-10 12:10:42,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7834: learning rate 0.0001
[2019-04-10 12:10:42,584] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7932: loss 0.0238
[2019-04-10 12:10:42,585] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7932: learning rate 0.0001
[2019-04-10 12:10:42,618] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7951: loss 0.0109
[2019-04-10 12:10:42,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7953: learning rate 0.0001
[2019-04-10 12:10:42,644] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7969: loss 0.0799
[2019-04-10 12:10:42,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7969: learning rate 0.0001
[2019-04-10 12:10:42,717] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8014: loss 0.0084
[2019-04-10 12:10:42,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8014: learning rate 0.0001
[2019-04-10 12:10:42,724] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8016: loss 0.0036
[2019-04-10 12:10:42,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8018: learning rate 0.0001
[2019-04-10 12:10:42,745] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8032: loss 0.0080
[2019-04-10 12:10:42,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8033: learning rate 0.0001
[2019-04-10 12:10:42,771] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8047: loss 0.0013
[2019-04-10 12:10:42,773] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8047: loss 0.0221
[2019-04-10 12:10:42,776] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8047: learning rate 0.0001
[2019-04-10 12:10:42,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8048: learning rate 0.0001
[2019-04-10 12:10:42,806] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9995410e-01 4.2413743e-05 3.8240300e-11 3.5383857e-06 1.4878195e-12], sum to 1.0000
[2019-04-10 12:10:42,807] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0957
[2019-04-10 12:10:42,929] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.6, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5734090475732164, 6.911200000000001, 6.9112, 168.912956510431, 500832.1816611232, 500832.1816611226, 162098.7177990802], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [23.53333333333333, 76.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5712133717710882, 6.911200000000001, 6.9112, 168.912956510431, 499111.3738603964, 499111.3738603957, 161776.8199846132], 
processed observation next is [0.0, 0.6086956521739131, 0.3143759873617693, 0.7666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47708947776961974, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13864204829455457, 0.13864204829455437, 0.24145794027554207], 
reward next is 0.7585, 
noisyNet noise sample is [array([-0.92422795], dtype=float32), -1.2215234]. 
=============================================
[2019-04-10 12:10:42,949] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[77.37918 ]
 [77.31907 ]
 [77.24691 ]
 [77.175285]
 [77.10919 ]], R is [[77.4230957 ]
 [77.40692902]
 [77.39041901]
 [77.3736496 ]
 [77.35702515]].
[2019-04-10 12:10:42,988] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8116: loss 5.8064
[2019-04-10 12:10:42,989] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8116: learning rate 0.0001
[2019-04-10 12:10:43,040] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8145: loss 0.0291
[2019-04-10 12:10:43,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8147: learning rate 0.0001
[2019-04-10 12:10:43,046] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8150: loss 0.0009
[2019-04-10 12:10:43,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8153: learning rate 0.0001
[2019-04-10 12:10:43,119] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8199: loss 0.0068
[2019-04-10 12:10:43,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8201: learning rate 0.0001
[2019-04-10 12:10:43,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9995983e-01 2.2732707e-05 1.1146587e-09 1.7486445e-05 8.2915684e-13], sum to 1.0000
[2019-04-10 12:10:43,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8254
[2019-04-10 12:10:43,799] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.73333333333333, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5486962894910635, 6.911199999999999, 6.9112, 168.912956510431, 481738.2384693777, 481738.2384693783, 158536.3214382841], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 319200.0000, 
sim time next is 319800.0000, 
raw observation next is [22.66666666666667, 78.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5458456543218331, 6.9112, 6.9112, 168.912956510431, 479424.4883262644, 479424.4883262644, 158138.1995075665], 
processed observation next is [0.0, 0.6956521739130435, 0.27330173775671435, 0.7883333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4461532369778452, 0.0, 0.0, 0.8294399451523027, 0.1331734689795179, 0.1331734689795179, 0.23602716344412908], 
reward next is 0.7640, 
noisyNet noise sample is [array([-1.4085045], dtype=float32), -1.2131191]. 
=============================================
[2019-04-10 12:10:50,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.4524429e-08 6.6128253e-12 2.4930841e-08 8.7689992e-15], sum to 1.0000
[2019-04-10 12:10:50,056] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3076
[2019-04-10 12:10:50,175] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.78333333333333, 71.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.782034824870524, 6.9112, 6.9112, 168.912956510431, 698069.3188745037, 698069.3188745037, 197462.6553902574], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 467400.0000, 
sim time next is 468000.0000, 
raw observation next is [21.9, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8728079916673964, 6.9112, 6.9112, 168.912956510431, 779014.4280267974, 779014.4280267974, 216701.3366611994], 
processed observation next is [1.0, 0.43478260869565216, 0.23696682464454974, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.844887794716337, 0.0, 0.0, 0.8294399451523027, 0.21639289667411038, 0.21639289667411038, 0.32343483083761104], 
reward next is 0.6766, 
noisyNet noise sample is [array([0.98698103], dtype=float32), 0.6093833]. 
=============================================
[2019-04-10 12:10:50,197] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.12075]
 [86.97839]
 [86.81593]
 [86.67157]
 [86.46752]], R is [[87.17683411]
 [87.01034546]
 [86.84306335]
 [86.66545105]
 [86.49786377]].
[2019-04-10 12:10:50,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.00000000e+00 5.24380761e-10 5.86641792e-13 1.00477326e-09
 1.06972435e-13], sum to 1.0000
[2019-04-10 12:10:50,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0402
[2019-04-10 12:10:50,311] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.3, 62.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9229249961515594, 6.9112, 6.9112, 168.912956510431, 823132.6356550305, 823132.6356550305, 228318.4645276018], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 474000.0000, 
sim time next is 474600.0000, 
raw observation next is [23.45, 61.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9500722800940797, 6.9112, 6.9112, 168.912956510431, 847277.6678214751, 847277.6678214751, 234872.8006658693], 
processed observation next is [1.0, 0.4782608695652174, 0.3104265402843602, 0.6183333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9391125367000972, 0.0, 0.0, 0.8294399451523027, 0.2353549077281875, 0.2353549077281875, 0.3505564189042825], 
reward next is 0.6494, 
noisyNet noise sample is [array([-1.0729572], dtype=float32), -0.122413985]. 
=============================================
[2019-04-10 12:10:50,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9999988e-01 2.4961576e-08 1.3212224e-10 1.0579224e-07 2.2886179e-10], sum to 1.0000
[2019-04-10 12:10:50,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6999
[2019-04-10 12:10:50,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 916981.428718838 W.
[2019-04-10 12:10:51,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 53.0, 1.0, 2.0, 0.1884818771963168, 1.0, 2.0, 0.1884818771963168, 1.0, 1.0, 0.344958809628476, 6.911199999999999, 6.9112, 170.5573041426782, 916981.428718838, 916981.4287188387, 279863.6615891094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 489000.0000, 
sim time next is 489600.0000, 
raw observation next is [24.9, 53.0, 1.0, 2.0, 0.5523597203275125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510429, 903758.228400286, 903758.2284002868, 205717.0791890168], 
processed observation next is [1.0, 0.6956521739130435, 0.3791469194312796, 0.53, 1.0, 1.0, 0.4606743618403764, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522929, 0.2510439523334128, 0.251043952333413, 0.3070404167000251], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1450223], dtype=float32), -0.5934534]. 
=============================================
[2019-04-10 12:10:52,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 8.9664434e-15 1.8308523e-16 1.8779503e-14 1.3324523e-16], sum to 1.0000
[2019-04-10 12:10:52,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9435
[2019-04-10 12:10:52,146] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.53333333333333, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4387175992452306, 6.9112, 6.9112, 168.912956510431, 393541.1061144191, 393541.1061144191, 144570.0111877521], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [19.46666666666667, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4375965691058968, 6.911199999999999, 6.9112, 168.912956510431, 392546.6935223611, 392546.6935223617, 144449.0050016589], 
processed observation next is [1.0, 0.9130434782608695, 0.12164296998420236, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3141421574462156, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10904074820065586, 0.10904074820065603, 0.21559552985322222], 
reward next is 0.7844, 
noisyNet noise sample is [array([0.04591952], dtype=float32), 0.6553842]. 
=============================================
[2019-04-10 12:10:53,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.2148280e-12 7.9682185e-14 1.4256872e-12 1.3666130e-16], sum to 1.0000
[2019-04-10 12:10:53,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2948
[2019-04-10 12:10:53,095] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.0, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3907907126505288, 6.9112, 6.9112, 168.912956510431, 352579.6429575228, 352579.6429575228, 139556.5941869392], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 537600.0000, 
sim time next is 538200.0000, 
raw observation next is [18.2, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3955346941538453, 6.911200000000001, 6.9112, 168.912956510431, 356631.4660712971, 356631.4660712964, 140028.0687536102], 
processed observation next is [1.0, 0.21739130434782608, 0.06161137440758297, 0.885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.26284718799249424, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09906429613091587, 0.09906429613091568, 0.20899711754270178], 
reward next is 0.7910, 
noisyNet noise sample is [array([1.2777914], dtype=float32), -0.44894993]. 
=============================================
[2019-04-10 12:10:53,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 7.3969564e-14 2.3089919e-15 3.8236158e-14 6.4046490e-17], sum to 1.0000
[2019-04-10 12:10:53,952] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-10 12:10:54,072] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.7, 55.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6557557234161436, 6.911199999999999, 6.9112, 168.912956510431, 583751.5172619796, 583751.5172619803, 174488.123209069], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 563400.0000, 
sim time next is 564000.0000, 
raw observation next is [24.6, 55.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6440724073814056, 6.9112, 6.9112, 168.912956510431, 573934.1409533488, 573934.1409533488, 172529.4109002703], 
processed observation next is [1.0, 0.5217391304347826, 0.36492890995260674, 0.5533333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5659419602212263, 0.0, 0.0, 0.8294399451523027, 0.15942615026481913, 0.15942615026481913, 0.25750658343323923], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.36150354], dtype=float32), 0.57341367]. 
=============================================
[2019-04-10 12:10:54,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.73962 ]
 [75.41709 ]
 [75.426476]
 [76.18838 ]
 [77.75368 ]], R is [[76.14080811]
 [76.11897278]
 [76.05280304]
 [75.89823151]
 [75.13925171]].
[2019-04-10 12:10:55,280] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 15723: loss 1.0214
[2019-04-10 12:10:55,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 15723: learning rate 0.0001
[2019-04-10 12:10:55,359] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15770: loss 0.6587
[2019-04-10 12:10:55,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15770: learning rate 0.0001
[2019-04-10 12:10:55,418] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15808: loss 1.2678
[2019-04-10 12:10:55,420] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15809: learning rate 0.0001
[2019-04-10 12:10:55,442] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15823: loss 0.6063
[2019-04-10 12:10:55,445] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15824: learning rate 0.0001
[2019-04-10 12:10:55,589] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15912: loss 0.7269
[2019-04-10 12:10:55,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15912: learning rate 0.0001
[2019-04-10 12:10:55,682] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15970: loss 0.9270
[2019-04-10 12:10:55,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15970: learning rate 0.0001
[2019-04-10 12:10:55,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15992: loss 0.9689
[2019-04-10 12:10:55,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15992: learning rate 0.0001
[2019-04-10 12:10:55,781] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16032: loss 0.6129
[2019-04-10 12:10:55,783] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16032: learning rate 0.0001
[2019-04-10 12:10:55,800] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16042: loss 0.8665
[2019-04-10 12:10:55,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16043: learning rate 0.0001
[2019-04-10 12:10:55,827] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16058: loss 0.5562
[2019-04-10 12:10:55,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16058: learning rate 0.0001
[2019-04-10 12:10:55,836] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16064: loss 0.9510
[2019-04-10 12:10:55,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16064: loss 0.9676
[2019-04-10 12:10:55,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16064: learning rate 0.0001
[2019-04-10 12:10:55,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16064: learning rate 0.0001
[2019-04-10 12:10:55,888] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16096: loss 0.5682
[2019-04-10 12:10:55,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16098: learning rate 0.0001
[2019-04-10 12:10:55,910] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16109: loss 1.1320
[2019-04-10 12:10:55,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16109: learning rate 0.0001
[2019-04-10 12:10:55,945] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16130: loss 0.6451
[2019-04-10 12:10:55,947] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16130: learning rate 0.0001
[2019-04-10 12:10:56,035] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16185: loss 0.4857
[2019-04-10 12:10:56,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16186: learning rate 0.0001
[2019-04-10 12:10:56,062] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 7.2560417e-15 1.6682903e-14 2.1455456e-14 3.2233497e-16], sum to 1.0000
[2019-04-10 12:10:56,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-10 12:10:56,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.7, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.388830609113168, 6.9112, 6.9112, 168.912956510431, 351343.47900249, 351343.47900249, 139318.4312760092], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 608400.0000, 
sim time next is 609000.0000, 
raw observation next is [17.61666666666667, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3871050981920338, 6.9112, 6.9112, 168.912956510431, 349862.5436621279, 349862.5436621279, 139149.3200173427], 
processed observation next is [1.0, 0.043478260869565216, 0.03396524486571906, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2525671929171144, 0.0, 0.0, 0.8294399451523027, 0.09718403990614664, 0.09718403990614664, 0.20768555226469057], 
reward next is 0.7923, 
noisyNet noise sample is [array([1.5257086], dtype=float32), -1.4427673]. 
=============================================
[2019-04-10 12:10:56,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.883064]
 [68.12166 ]
 [68.17724 ]
 [68.3602  ]
 [68.50305 ]], R is [[67.89841461]
 [68.01148987]
 [68.12319946]
 [68.23352051]
 [68.34244537]].
[2019-04-10 12:10:57,104] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 4.3160200e-15 2.6206259e-15 3.2896356e-15 4.8901243e-18], sum to 1.0000
[2019-04-10 12:10:57,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-10 12:10:57,225] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.66666666666666, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8693386412571236, 6.911199999999999, 6.9112, 168.912956510431, 776222.2224531063, 776222.2224531069, 215900.4873356264], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 639600.0000, 
sim time next is 640200.0000, 
raw observation next is [22.83333333333334, 64.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8936263271712818, 6.911199999999999, 6.9112, 168.912956510431, 797712.83561018, 797712.8356101806, 221414.0058092975], 
processed observation next is [1.0, 0.391304347826087, 0.2812006319115327, 0.6483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8702760087454654, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22158689878060556, 0.22158689878060572, 0.3304686653870112], 
reward next is 0.6695, 
noisyNet noise sample is [array([-1.0570844], dtype=float32), -0.686786]. 
=============================================
[2019-04-10 12:10:57,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.0366118e-11 3.7538648e-12 5.1268456e-12 2.5531574e-14], sum to 1.0000
[2019-04-10 12:10:57,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3544
[2019-04-10 12:10:57,999] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1027814.441741859 W.
[2019-04-10 12:10:58,003] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.0, 1.0, 2.0, 0.3139267895535337, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5770743362703227, 6.9112, 6.9112, 168.912956510431, 1027814.441741859, 1027814.441741859, 238519.5980625758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 658200.0000, 
sim time next is 658800.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.5585242263889559, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918044.9533558163, 918044.9533558163, 206994.7565023953], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.53, 1.0, 1.0, 0.46810147757705534, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25501248704328233, 0.25501248704328233, 0.3089473977647691], 
reward next is 0.6911, 
noisyNet noise sample is [array([-0.2724518], dtype=float32), -0.06723847]. 
=============================================
[2019-04-10 12:11:02,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.7071894e-09 1.2297778e-10 1.2633672e-10 2.8225956e-11], sum to 1.0000
[2019-04-10 12:11:02,587] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2531
[2019-04-10 12:11:02,591] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 953639.8538339088 W.
[2019-04-10 12:11:02,717] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.85, 50.0, 1.0, 2.0, 0.1941325035559733, 1.0, 1.0, 0.1941325035559733, 1.0, 1.0, 0.3569079205618412, 6.9112, 6.9112, 170.5573041426782, 953639.8538339088, 953639.8538339088, 281619.9717367936], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 750600.0000, 
sim time next is 751200.0000, 
raw observation next is [24.76666666666667, 50.0, 1.0, 2.0, 0.2951473867784769, 1.0, 2.0, 0.2951473867784769, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 968280.7358518847, 968280.7358518847, 263311.3877791067], 
processed observation next is [1.0, 0.6956521739130435, 0.3728278041074251, 0.5, 1.0, 1.0, 0.1507799840704541, 1.0, 1.0, 0.1507799840704541, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.26896687106996797, 0.26896687106996797, 0.3930020713120995], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3110829], dtype=float32), 0.0923122]. 
=============================================
[2019-04-10 12:11:03,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 3.9507530e-13 1.5156010e-14 1.4085863e-15 2.4493892e-17], sum to 1.0000
[2019-04-10 12:11:03,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9111
[2019-04-10 12:11:03,249] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.13333333333333, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4743530103711999, 6.911200000000001, 6.9112, 168.912956510431, 423251.0919288568, 423251.0919288562, 148720.9539194463], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 769200.0000, 
sim time next is 769800.0000, 
raw observation next is [20.06666666666667, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4736610345797139, 6.911200000000001, 6.9112, 168.912956510431, 422645.2378359324, 422645.2378359318, 148639.1787237169], 
processed observation next is [1.0, 0.9130434782608695, 0.1500789889415484, 0.845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35812321290209015, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11740145495442567, 0.11740145495442551, 0.22184952048315956], 
reward next is 0.7782, 
noisyNet noise sample is [array([-0.8090349], dtype=float32), 1.546222]. 
=============================================
[2019-04-10 12:11:04,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 3.2792994e-11 1.5351669e-10 4.7471042e-12 3.8660782e-13], sum to 1.0000
[2019-04-10 12:11:04,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0612
[2019-04-10 12:11:04,446] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.55, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4823138928547357, 6.911200000000001, 6.9112, 168.912956510431, 429084.8088431604, 429084.8088431598, 149736.8219791289], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 796200.0000, 
sim time next is 796800.0000, 
raw observation next is [19.7, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4847678031863053, 6.9112, 6.9112, 168.912956510431, 431111.7935411089, 431111.7935411089, 150039.4805573341], 
processed observation next is [0.0, 0.21739130434782608, 0.1327014218009479, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.37166805266622593, 0.0, 0.0, 0.8294399451523027, 0.11975327598364136, 0.11975327598364136, 0.22393952321990163], 
reward next is 0.7761, 
noisyNet noise sample is [array([1.1044945], dtype=float32), 0.5014418]. 
=============================================
[2019-04-10 12:11:04,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 5.8147098e-12 1.4056844e-12 8.2856162e-15 8.1090974e-16], sum to 1.0000
[2019-04-10 12:11:04,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2070
[2019-04-10 12:11:04,936] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5231599570978436, 6.911200000000001, 6.9112, 168.912956510431, 461288.1406071365, 461288.1406071359, 155033.9772769387], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [23.28333333333334, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5237705490984925, 6.911200000000001, 6.9112, 168.912956510431, 461782.0591718462, 461782.0591718456, 155115.6606750371], 
processed observation next is [0.0, 0.391304347826087, 0.30252764612954214, 0.72, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4192323769493811, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12827279421440171, 0.12827279421440155, 0.23151591145527925], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.3396285], dtype=float32), -1.0917943]. 
=============================================
[2019-04-10 12:11:05,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.6950257e-14 3.4712308e-14 2.7313058e-15 4.0775703e-17], sum to 1.0000
[2019-04-10 12:11:05,952] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7234
[2019-04-10 12:11:06,081] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.6, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5178131595527333, 6.9112, 6.9112, 168.912956510431, 456857.1522414534, 456857.1522414534, 154326.8702430403], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 828000.0000, 
sim time next is 828600.0000, 
raw observation next is [24.56666666666667, 63.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5199277941264846, 6.911200000000001, 6.9112, 168.912956510431, 458666.5581729931, 458666.5581729925, 154603.521625961], 
processed observation next is [0.0, 0.6086956521739131, 0.3633491311216432, 0.6366666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41454609039815193, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12740737727027585, 0.12740737727027568, 0.23075152481486716], 
reward next is 0.7692, 
noisyNet noise sample is [array([-0.6539128], dtype=float32), 0.5897105]. 
=============================================
[2019-04-10 12:11:08,615] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 23698: loss 0.0023
[2019-04-10 12:11:08,618] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 23698: learning rate 0.0001
[2019-04-10 12:11:08,623] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23701: loss 0.0001
[2019-04-10 12:11:08,629] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23703: learning rate 0.0001
[2019-04-10 12:11:08,666] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23727: loss 0.0177
[2019-04-10 12:11:08,669] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23728: learning rate 0.0001
[2019-04-10 12:11:08,774] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23796: loss 0.0052
[2019-04-10 12:11:08,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23796: learning rate 0.0001
[2019-04-10 12:11:08,989] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23928: loss 0.0255
[2019-04-10 12:11:08,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23929: learning rate 0.0001
[2019-04-10 12:11:09,151] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24028: loss 0.0041
[2019-04-10 12:11:09,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24028: learning rate 0.0001
[2019-04-10 12:11:09,167] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24039: loss 0.0090
[2019-04-10 12:11:09,168] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24039: learning rate 0.0001
[2019-04-10 12:11:09,170] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24039: loss 0.0132
[2019-04-10 12:11:09,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24039: learning rate 0.0001
[2019-04-10 12:11:09,216] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24066: loss 0.0133
[2019-04-10 12:11:09,216] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24066: loss 0.0306
[2019-04-10 12:11:09,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24068: learning rate 0.0001
[2019-04-10 12:11:09,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24068: learning rate 0.0001
[2019-04-10 12:11:09,227] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24071: loss 0.0421
[2019-04-10 12:11:09,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24071: learning rate 0.0001
[2019-04-10 12:11:09,236] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24076: loss 0.0241
[2019-04-10 12:11:09,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24077: learning rate 0.0001
[2019-04-10 12:11:09,241] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24080: loss 0.0170
[2019-04-10 12:11:09,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24082: learning rate 0.0001
[2019-04-10 12:11:09,320] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24129: loss 0.0034
[2019-04-10 12:11:09,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24129: learning rate 0.0001
[2019-04-10 12:11:09,390] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24175: loss 0.0089
[2019-04-10 12:11:09,392] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24175: learning rate 0.0001
[2019-04-10 12:11:09,419] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24189: loss 0.0255
[2019-04-10 12:11:09,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24190: learning rate 0.0001
[2019-04-10 12:11:10,733] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:11:10,734] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:11:10,734] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:11:10,735] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:10,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:11:10,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:10,738] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:10,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:11:10,739] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:11:10,742] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:10,743] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:11:10,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-04-10 12:11:10,771] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-04-10 12:11:10,773] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-04-10 12:11:10,807] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-04-10 12:11:10,825] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-04-10 12:11:12,512] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.015378508]
[2019-04-10 12:11:12,512] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.9, 61.5, 1.0, 2.0, 0.80055427292565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1118871.43788565, 1118871.43788565, 244043.0491753223]
[2019-04-10 12:11:12,514] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:11:12,516] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.0139684e-12 5.7520736e-13 1.9319166e-14 7.8544815e-16], sampled 0.5743667741863002
[2019-04-10 12:11:12,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1118871.43788565 W.
[2019-04-10 12:11:33,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.015378508]
[2019-04-10 12:11:33,485] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.65, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9897989548222047, 6.9112, 6.9112, 168.912956510431, 796860.8592093464, 796860.8592093464, 244381.8115075757]
[2019-04-10 12:11:33,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:11:33,490] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.00000000e+00 1.22698957e-14 5.90020027e-15 1.05055213e-16
 2.27355513e-18], sampled 0.0171570993846919
[2019-04-10 12:12:36,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8231 2937707334.6482 1381.0000
[2019-04-10 12:12:36,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.6151 2989320491.5456 1566.0000
[2019-04-10 12:12:36,799] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.3998 3105589387.5413 2010.0000
[2019-04-10 12:12:36,979] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3468 3185027435.2466 2464.0000
[2019-04-10 12:12:37,036] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2436 3319525848.5521 2143.0000
[2019-04-10 12:12:38,051] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 25000, evaluation results [25000.0, 7288.243641670224, 3319525848.552096, 2143.0, 7348.39983316503, 3105589387.541278, 2010.0, 8061.823074652101, 2937707334.648202, 1381.0, 7031.346810217107, 3185027435.246569, 2464.0, 7924.615084586526, 2989320491.545559, 1566.0]
[2019-04-10 12:12:42,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.5287966e-12 6.6330370e-12 2.0497458e-12 5.0001049e-15], sum to 1.0000
[2019-04-10 12:12:42,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6547
[2019-04-10 12:12:42,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 889981.6723193923 W.
[2019-04-10 12:12:42,093] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.98333333333333, 94.16666666666667, 1.0, 2.0, 0.5787494474108197, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889981.6723193923, 889981.6723193923, 206669.1902470433], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 990600.0000, 
sim time next is 991200.0000, 
raw observation next is [21.96666666666667, 94.33333333333334, 1.0, 2.0, 0.2512940256041262, 0.0, 2.0, 0.0, 1.0, 1.0, 0.445497633815694, 6.911199999999999, 6.9112, 168.912956510431, 771455.5556118315, 771455.5556118322, 213754.3961875825], 
processed observation next is [1.0, 0.4782608695652174, 0.24012638230647723, 0.9433333333333335, 1.0, 1.0, 0.09794460916159785, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.323777602214261, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2142932098921754, 0.2142932098921756, 0.31903641222027235], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6166685], dtype=float32), 0.3353709]. 
=============================================
[2019-04-10 12:12:50,847] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.7486060e-12 3.6512057e-11 1.8038012e-13 2.5051757e-13], sum to 1.0000
[2019-04-10 12:12:50,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8955
[2019-04-10 12:12:50,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.06666666666667, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5885447920363531, 6.9112, 6.9112, 168.912956510431, 513846.8382420852, 513846.8382420852, 164326.7257872792], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1105800.0000, 
sim time next is 1106400.0000, 
raw observation next is [23.93333333333334, 74.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5923361707617953, 6.911199999999999, 6.9112, 168.912956510431, 517333.6962939556, 517333.6962939562, 164889.4241083104], 
processed observation next is [1.0, 0.8260869565217391, 0.3333333333333337, 0.7433333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5028489887338967, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1437038045260988, 0.14370380452609893, 0.24610361807210507], 
reward next is 0.7539, 
noisyNet noise sample is [array([0.59982926], dtype=float32), 0.7355674]. 
=============================================
[2019-04-10 12:12:54,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 4.2034930e-17 1.2362680e-15 2.3791425e-17 1.5478298e-18], sum to 1.0000
[2019-04-10 12:12:54,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4631
[2019-04-10 12:12:54,412] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.45, 86.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6051846813594639, 6.9112, 6.9112, 168.912956510431, 532985.2542815327, 532985.2542815327, 166707.4694120849], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1149000.0000, 
sim time next is 1149600.0000, 
raw observation next is [21.6, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5753745220297216, 6.911200000000001, 6.9112, 168.912956510431, 506436.7575140374, 506436.7575140367, 162272.7565116079], 
processed observation next is [1.0, 0.30434782608695654, 0.22274881516587688, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48216405125575806, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14067687708723262, 0.14067687708723242, 0.242198144047176], 
reward next is 0.7578, 
noisyNet noise sample is [array([1.444085], dtype=float32), 0.7687998]. 
=============================================
[2019-04-10 12:12:56,943] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31649: loss 0.2167
[2019-04-10 12:12:56,944] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31649: learning rate 0.0001
[2019-04-10 12:12:56,986] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31669: loss 0.7652
[2019-04-10 12:12:56,988] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31670: learning rate 0.0001
[2019-04-10 12:12:57,004] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31678: loss 0.8706
[2019-04-10 12:12:57,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31678: learning rate 0.0001
[2019-04-10 12:12:57,541] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31916: loss 0.0906
[2019-04-10 12:12:57,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31916: learning rate 0.0001
[2019-04-10 12:12:57,561] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31926: loss 0.0706
[2019-04-10 12:12:57,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31927: learning rate 0.0001
[2019-04-10 12:12:57,641] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31982: loss 0.1499
[2019-04-10 12:12:57,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31984: learning rate 0.0001
[2019-04-10 12:12:57,674] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32001: loss 0.2173
[2019-04-10 12:12:57,675] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32002: learning rate 0.0001
[2019-04-10 12:12:57,681] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32006: loss 0.5199
[2019-04-10 12:12:57,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32006: learning rate 0.0001
[2019-04-10 12:12:57,709] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32024: loss 0.6410
[2019-04-10 12:12:57,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32025: learning rate 0.0001
[2019-04-10 12:12:57,788] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32077: loss 0.0298
[2019-04-10 12:12:57,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32078: learning rate 0.0001
[2019-04-10 12:12:57,824] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32100: loss 0.1505
[2019-04-10 12:12:57,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32102: learning rate 0.0001
[2019-04-10 12:12:57,831] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32104: loss 0.2319
[2019-04-10 12:12:57,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32105: learning rate 0.0001
[2019-04-10 12:12:57,862] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32125: loss 0.0372
[2019-04-10 12:12:57,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32125: learning rate 0.0001
[2019-04-10 12:12:57,892] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32148: loss 0.1342
[2019-04-10 12:12:57,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32148: learning rate 0.0001
[2019-04-10 12:12:57,972] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32197: loss 0.2207
[2019-04-10 12:12:57,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32198: learning rate 0.0001
[2019-04-10 12:12:57,996] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32214: loss 0.2434
[2019-04-10 12:12:57,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32215: learning rate 0.0001
[2019-04-10 12:13:04,184] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 5.7004488e-12 1.4397160e-09 1.1480599e-14 1.9208874e-13], sum to 1.0000
[2019-04-10 12:13:04,189] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5169
[2019-04-10 12:13:04,193] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.95, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5781975155603524, 6.9112, 6.9112, 168.912956510431, 506182.9164492211, 506182.9164492211, 162766.9757065172], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1373400.0000, 
sim time next is 1374000.0000, 
raw observation next is [20.93333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5756675590736543, 6.911199999999999, 6.9112, 168.912956510431, 504050.8725107391, 504050.8725107398, 162396.0946339814], 
processed observation next is [1.0, 0.9130434782608695, 0.19115323854660338, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48252141350445643, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14001413125298307, 0.14001413125298326, 0.24238223079698717], 
reward next is 0.7576, 
noisyNet noise sample is [array([-1.8825425], dtype=float32), -1.1868621]. 
=============================================
[2019-04-10 12:13:04,204] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.043144]
 [61.6406  ]
 [61.953274]
 [62.0323  ]
 [61.969746]], R is [[62.10970688]
 [62.24567413]
 [62.37994385]
 [62.5128212 ]
 [62.64400482]].
[2019-04-10 12:13:04,547] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 5.2427800e-13 4.0433667e-11 9.0210531e-17 1.9101440e-15], sum to 1.0000
[2019-04-10 12:13:04,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7874
[2019-04-10 12:13:04,558] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5816013793528527, 6.911199999999999, 6.9112, 168.912956510431, 508922.2850250268, 508922.2850250274, 163271.9865205714], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1371600.0000, 
sim time next is 1372200.0000, 
raw observation next is [20.98333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5799487948958942, 6.911200000000001, 6.9112, 168.912956510431, 507551.1779769111, 507551.1779769105, 163027.4877680755], 
processed observation next is [1.0, 0.9130434782608695, 0.1935229067930489, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.487742432799871, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14098643832691973, 0.1409864383269196, 0.2433246086090679], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.43655676], dtype=float32), 1.0215771]. 
=============================================
[2019-04-10 12:13:05,530] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.4676555e-13 1.2122954e-09 5.9786834e-15 2.8359233e-12], sum to 1.0000
[2019-04-10 12:13:05,537] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-10 12:13:05,540] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.18333333333334, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5827091560689328, 6.9112, 6.9112, 168.912956510431, 509367.5124877075, 509367.5124877075, 163447.9902834612], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1404600.0000, 
sim time next is 1405200.0000, 
raw observation next is [21.26666666666667, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5854849590767943, 6.911199999999999, 6.9112, 168.912956510431, 511585.9685040126, 511585.9685040131, 163862.944190414], 
processed observation next is [0.0, 0.2608695652173913, 0.2069510268562403, 0.9433333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49449385253267597, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14210721347333685, 0.14210721347333696, 0.24457155849315523], 
reward next is 0.7554, 
noisyNet noise sample is [array([-0.94014806], dtype=float32), 0.1994836]. 
=============================================
[2019-04-10 12:13:05,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.1898880e-12 1.0346355e-09 7.1902425e-15 3.9833647e-13], sum to 1.0000
[2019-04-10 12:13:05,936] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6466
[2019-04-10 12:13:05,941] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.51666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5658535480661468, 6.911199999999999, 6.9112, 168.912956510431, 495934.7954368402, 495934.7954368409, 160969.4053526908], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1397400.0000, 
sim time next is 1398000.0000, 
raw observation next is [20.53333333333333, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5671589794941152, 6.9112, 6.9112, 168.912956510431, 496983.3143754987, 496983.3143754987, 161158.5679617174], 
processed observation next is [0.0, 0.17391304347826086, 0.17219589257503945, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4721450969440429, 0.0, 0.0, 0.8294399451523027, 0.13805092065986074, 0.13805092065986074, 0.2405351760622648], 
reward next is 0.7595, 
noisyNet noise sample is [array([-0.5744756], dtype=float32), 0.67800546]. 
=============================================
[2019-04-10 12:13:05,953] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.12121 ]
 [63.147152]
 [63.45433 ]
 [63.56905 ]
 [63.89574 ]], R is [[63.05497742]
 [63.18417358]
 [63.31212997]
 [63.4387207 ]
 [63.56435013]].
[2019-04-10 12:13:08,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 9.3537637e-15 1.3217594e-10 1.9614922e-16 6.8548818e-15], sum to 1.0000
[2019-04-10 12:13:08,089] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7383
[2019-04-10 12:13:08,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.75, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6855595491387647, 6.911199999999999, 6.9112, 168.912956510431, 587622.5538527947, 587622.5538527954, 180169.3406760591], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1452600.0000, 
sim time next is 1453200.0000, 
raw observation next is [23.5, 89.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6814490741356267, 6.911199999999999, 6.9112, 168.912956510431, 584580.0612230237, 584580.0612230243, 179450.4414657121], 
processed observation next is [0.0, 0.8260869565217391, 0.31279620853080575, 0.8933333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6115232611410082, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1623833503397288, 0.16238335033972898, 0.2678364797995703], 
reward next is 0.7322, 
noisyNet noise sample is [array([1.2657851], dtype=float32), -0.6790653]. 
=============================================
[2019-04-10 12:13:09,377] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39538: loss 0.0007
[2019-04-10 12:13:09,380] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39539: learning rate 0.0001
[2019-04-10 12:13:09,531] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39634: loss 0.0014
[2019-04-10 12:13:09,534] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39634: learning rate 0.0001
[2019-04-10 12:13:09,599] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39677: loss 0.0178
[2019-04-10 12:13:09,600] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39678: learning rate 0.0001
[2019-04-10 12:13:09,938] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39888: loss 0.0318
[2019-04-10 12:13:09,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39888: learning rate 0.0001
[2019-04-10 12:13:10,033] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39948: loss 0.0184
[2019-04-10 12:13:10,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39948: learning rate 0.0001
[2019-04-10 12:13:10,110] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39989: loss 0.0206
[2019-04-10 12:13:10,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39990: learning rate 0.0001
[2019-04-10 12:13:10,121] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40000: loss 0.0123
[2019-04-10 12:13:10,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40000: learning rate 0.0001
[2019-04-10 12:13:10,136] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40007: loss 0.0209
[2019-04-10 12:13:10,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40008: learning rate 0.0001
[2019-04-10 12:13:10,220] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40067: loss 0.0069
[2019-04-10 12:13:10,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40067: learning rate 0.0001
[2019-04-10 12:13:10,260] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40087: loss 0.0091
[2019-04-10 12:13:10,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40089: learning rate 0.0001
[2019-04-10 12:13:10,268] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40091: loss 0.0063
[2019-04-10 12:13:10,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40091: learning rate 0.0001
[2019-04-10 12:13:10,332] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40137: loss 0.0007
[2019-04-10 12:13:10,339] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40141: loss 0.0008
[2019-04-10 12:13:10,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40138: learning rate 0.0001
[2019-04-10 12:13:10,344] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40145: loss 0.0006
[2019-04-10 12:13:10,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40145: learning rate 0.0001
[2019-04-10 12:13:10,347] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40145: learning rate 0.0001
[2019-04-10 12:13:10,488] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40232: loss 0.0058
[2019-04-10 12:13:10,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40233: learning rate 0.0001
[2019-04-10 12:13:10,510] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40244: loss 0.0162
[2019-04-10 12:13:10,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40244: learning rate 0.0001
[2019-04-10 12:13:13,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.8092622e-13 1.5435259e-10 2.5877825e-16 1.0255206e-14], sum to 1.0000
[2019-04-10 12:13:13,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-10 12:13:13,291] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.61666666666667, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5811829668432207, 6.9112, 6.9112, 168.912956510431, 508495.0533680868, 508495.0533680868, 163212.0452266411], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1569000.0000, 
sim time next is 1569600.0000, 
raw observation next is [21.6, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5761892847959047, 6.911200000000001, 6.9112, 168.912956510431, 504207.7298161962, 504207.7298161956, 162479.7527818637], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4831576643852496, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14005770272672116, 0.140057702726721, 0.24250709370427417], 
reward next is 0.7575, 
noisyNet noise sample is [array([0.26477027], dtype=float32), -1.4590714]. 
=============================================
[2019-04-10 12:13:21,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9999917e-01 1.7012386e-10 7.8908039e-07 5.3380591e-13 2.3127644e-10], sum to 1.0000
[2019-04-10 12:13:21,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9971
[2019-04-10 12:13:21,349] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1348405.672081477 W.
[2019-04-10 12:13:21,352] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.78333333333333, 89.0, 1.0, 2.0, 0.9506364289953411, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348405.672081477, 1348405.672081477, 287238.8188559496], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1759800.0000, 
sim time next is 1760400.0000, 
raw observation next is [24.7, 89.0, 1.0, 2.0, 0.3214149406981315, 1.0, 1.0, 0.3214149406981315, 1.0, 1.0, 0.5354289160438372, 6.9112, 6.9112, 170.5573041426782, 1347785.741477567, 1347785.741477567, 311108.6101544391], 
processed observation next is [1.0, 0.391304347826087, 0.3696682464454976, 0.89, 1.0, 1.0, 0.18242763939533913, 1.0, 0.5, 0.18242763939533913, 1.0, 0.5, 0.43344989761443564, 0.0, 0.0, 0.8375144448122397, 0.37438492818821306, 0.37438492818821306, 0.46434120918573], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3608297], dtype=float32), -1.9596422]. 
=============================================
[2019-04-10 12:13:22,513] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47564: loss 0.8016
[2019-04-10 12:13:22,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47564: learning rate 0.0001
[2019-04-10 12:13:22,708] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47682: loss 0.4442
[2019-04-10 12:13:22,709] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47682: learning rate 0.0001
[2019-04-10 12:13:22,728] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47696: loss 1.1814
[2019-04-10 12:13:22,730] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47696: learning rate 0.0001
[2019-04-10 12:13:23,058] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47895: loss 0.4577
[2019-04-10 12:13:23,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47895: learning rate 0.0001
[2019-04-10 12:13:23,075] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47903: loss 0.0952
[2019-04-10 12:13:23,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47903: learning rate 0.0001
[2019-04-10 12:13:23,146] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47950: loss 0.2419
[2019-04-10 12:13:23,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47950: learning rate 0.0001
[2019-04-10 12:13:23,180] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47970: loss 0.2313
[2019-04-10 12:13:23,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47970: learning rate 0.0001
[2019-04-10 12:13:23,285] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48031: loss 0.6781
[2019-04-10 12:13:23,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48031: learning rate 0.0001
[2019-04-10 12:13:23,336] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48067: loss 0.1400
[2019-04-10 12:13:23,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48067: learning rate 0.0001
[2019-04-10 12:13:23,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48090: loss 0.2426
[2019-04-10 12:13:23,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48090: learning rate 0.0001
[2019-04-10 12:13:23,387] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48092: loss 0.0278
[2019-04-10 12:13:23,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48094: learning rate 0.0001
[2019-04-10 12:13:23,453] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48134: loss 0.1444
[2019-04-10 12:13:23,454] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48134: loss 0.0956
[2019-04-10 12:13:23,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48134: learning rate 0.0001
[2019-04-10 12:13:23,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48134: learning rate 0.0001
[2019-04-10 12:13:23,505] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48161: loss 0.0752
[2019-04-10 12:13:23,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48162: learning rate 0.0001
[2019-04-10 12:13:23,573] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48200: loss 0.1982
[2019-04-10 12:13:23,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48202: learning rate 0.0001
[2019-04-10 12:13:23,745] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48307: loss 0.0400
[2019-04-10 12:13:23,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48307: learning rate 0.0001
[2019-04-10 12:13:25,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00000000e+00 1.15247853e-14 1.07341108e-10 1.09176816e-16
 5.54289424e-15], sum to 1.0000
[2019-04-10 12:13:25,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1156
[2019-04-10 12:13:25,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1307325.220879985 W.
[2019-04-10 12:13:25,103] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 91.66666666666667, 1.0, 2.0, 0.4556290104653452, 1.0, 1.0, 0.4556290104653452, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1307325.220879985, 1307325.220879985, 291063.8148337391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [23.9, 92.0, 1.0, 2.0, 0.9006772378854, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1304994.708035541, 1304994.708035541, 276883.0709735457], 
processed observation next is [1.0, 0.391304347826087, 0.33175355450236965, 0.92, 1.0, 1.0, 0.8803340215486747, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3624985300098725, 0.3624985300098725, 0.41325831488588916], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2096851], dtype=float32), -0.1673307]. 
=============================================
[2019-04-10 12:13:26,500] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 12:13:26,501] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:13:26,502] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:26,502] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:13:26,503] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:26,503] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:13:26,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:13:26,505] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:26,506] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:26,506] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:13:26,508] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:13:26,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-04-10 12:13:26,520] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-04-10 12:13:26,535] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-04-10 12:13:26,567] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-04-10 12:13:26,584] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-04-10 12:13:32,373] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:13:32,374] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.7, 71.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8940156630328343, 6.911199999999999, 6.9112, 168.912956510431, 753317.9006770018, 753317.9006770024, 222480.3192790409]
[2019-04-10 12:13:32,374] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:13:32,376] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 7.8955479e-12 3.8137003e-08 2.6784139e-13 5.2770257e-11], sampled 0.12868294333617258
[2019-04-10 12:13:53,249] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:13:53,251] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.3, 81.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9229508483024264, 6.9112, 6.9112, 168.912956510431, 754360.6123915026, 754360.6123915026, 228519.1684418817]
[2019-04-10 12:13:53,252] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:13:53,254] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.4968751e-13 6.1745364e-09 1.2337058e-14 3.5943982e-12], sampled 0.37582392285652955
[2019-04-10 12:13:54,113] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:13:54,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.5, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8010631732951466, 6.9112, 6.9112, 168.912956510431, 673994.8656830549, 673994.8656830549, 202106.4176599437]
[2019-04-10 12:13:54,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:13:54,118] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.3063678e-12 1.7647148e-08 8.0353720e-14 1.6638441e-11], sampled 0.9113212896887162
[2019-04-10 12:14:16,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:16,234] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.812146252354405, 6.9112, 6.9112, 168.912956510431, 679974.7968419387, 679974.7968419387, 204338.2289806299]
[2019-04-10 12:14:16,234] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:14:16,236] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.0033519e-12 3.1421543e-08 1.9939507e-13 3.9662124e-11], sampled 0.9810495569757715
[2019-04-10 12:14:16,721] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:16,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.93333333333333, 79.83333333333334, 1.0, 2.0, 0.6368201988233145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889937.3781689276, 889937.3781689276, 207805.9087211093]
[2019-04-10 12:14:16,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:14:16,725] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999988e-01 3.0081122e-11 6.2121508e-08 8.5685040e-13 1.1850772e-10], sampled 0.9923412066416687
[2019-04-10 12:14:16,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 889937.3781689276 W.
[2019-04-10 12:14:19,634] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:19,636] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8854733527503029, 6.911199999999999, 6.9112, 168.912956510431, 731325.0086577468, 731325.0086577475, 220096.3765542446]
[2019-04-10 12:14:19,637] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:14:19,641] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.7281081e-12 1.2716147e-08 4.8437013e-14 1.2488575e-11], sampled 0.5172890288754419
[2019-04-10 12:14:23,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:23,005] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.33528061, 73.70646924, 1.0, 2.0, 0.9175314837049482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510413, 1282459.976459974, 1282459.976459975, 274813.2902209928]
[2019-04-10 12:14:23,006] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:14:23,007] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9999976e-01 1.4312547e-10 2.1657962e-07 5.6681383e-12 6.3107169e-10], sampled 0.4203286399593502
[2019-04-10 12:14:23,008] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1282459.976459974 W.
[2019-04-10 12:14:41,716] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:41,717] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.24900527166667, 96.51317148333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6362306623705118, 6.911200000000001, 6.9112, 168.912956510431, 550974.9599844884, 550974.9599844877, 171814.330867621]
[2019-04-10 12:14:41,718] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:14:41,721] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.9192252e-12 2.6392254e-08 1.3643129e-13 3.6321182e-11], sampled 0.095782236422843
[2019-04-10 12:14:50,585] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8058.8968 2937904999.7342 1381.0000
[2019-04-10 12:14:50,659] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0077402336]
[2019-04-10 12:14:50,659] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.41666666666667, 68.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6137634083192219, 6.911199999999999, 6.9112, 168.912956510431, 533455.1137057621, 533455.1137057628, 168215.5826051612]
[2019-04-10 12:14:50,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:14:50,671] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.0521471e-11 4.5979199e-08 3.5793796e-13 7.7292193e-11], sampled 0.7911865190187308
[2019-04-10 12:14:50,704] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2220 3319603786.4240 2143.0000
[2019-04-10 12:14:50,907] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7345.3635 3105862206.9964 2010.0000
[2019-04-10 12:14:50,910] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.5854 2989401565.3634 1566.0000
[2019-04-10 12:14:51,173] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8378 3185168584.1276 2464.0000
[2019-04-10 12:14:52,188] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 50000, evaluation results [50000.0, 7288.222042515761, 3319603786.4240336, 2143.0, 7345.36346201804, 3105862206.996442, 2010.0, 8058.896788048391, 2937904999.73417, 1381.0, 7029.837783793094, 3185168584.1276493, 2464.0, 7924.58543915495, 2989401565.3634357, 1566.0]
[2019-04-10 12:15:08,407] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9999988e-01 6.4187183e-15 9.6278271e-08 3.5279020e-16 8.4796851e-13], sum to 1.0000
[2019-04-10 12:15:08,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-10 12:15:08,419] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.83333333333334, 97.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7648054150691853, 6.9112, 6.9112, 168.912956510431, 644983.9256269227, 644983.9256269227, 194827.171626894], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2091000.0000, 
sim time next is 2091600.0000, 
raw observation next is [23.8, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.763919515928722, 6.911199999999999, 6.9112, 168.912956510431, 644345.0983357441, 644345.0983357447, 194654.8579162625], 
processed observation next is [0.0, 0.21739130434782608, 0.3270142180094788, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.712096970644783, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1789847495377067, 0.17898474953770685, 0.2905296386809888], 
reward next is 0.7095, 
noisyNet noise sample is [array([0.46013564], dtype=float32), -0.0008410297]. 
=============================================
[2019-04-10 12:15:08,692] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55521: loss 0.0029
[2019-04-10 12:15:08,693] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55521: learning rate 0.0001
[2019-04-10 12:15:09,109] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55616: loss 0.0134
[2019-04-10 12:15:09,114] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55616: learning rate 0.0001
[2019-04-10 12:15:09,479] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55707: loss 0.0091
[2019-04-10 12:15:09,499] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55709: learning rate 0.0001
[2019-04-10 12:15:10,052] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55856: loss 0.0065
[2019-04-10 12:15:10,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55856: learning rate 0.0001
[2019-04-10 12:15:10,135] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55888: loss 0.0292
[2019-04-10 12:15:10,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55888: learning rate 0.0001
[2019-04-10 12:15:10,386] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55969: loss 0.0164
[2019-04-10 12:15:10,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55969: learning rate 0.0001
[2019-04-10 12:15:10,435] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55982: loss 0.0147
[2019-04-10 12:15:10,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55991: learning rate 0.0001
[2019-04-10 12:15:10,575] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56032: loss 0.0088
[2019-04-10 12:15:10,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56032: learning rate 0.0001
[2019-04-10 12:15:10,701] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56071: loss 0.0001
[2019-04-10 12:15:10,702] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56071: learning rate 0.0001
[2019-04-10 12:15:10,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56084: loss 0.0012
[2019-04-10 12:15:10,732] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56084: learning rate 0.0001
[2019-04-10 12:15:10,783] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56104: loss 0.0002
[2019-04-10 12:15:10,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56105: learning rate 0.0001
[2019-04-10 12:15:10,851] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56128: loss 0.0013
[2019-04-10 12:15:10,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56128: learning rate 0.0001
[2019-04-10 12:15:11,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56192: loss 0.0007
[2019-04-10 12:15:11,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56192: learning rate 0.0001
[2019-04-10 12:15:11,070] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56204: loss 0.0001
[2019-04-10 12:15:11,070] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56204: loss 0.0001
[2019-04-10 12:15:11,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56204: learning rate 0.0001
[2019-04-10 12:15:11,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56204: learning rate 0.0001
[2019-04-10 12:15:11,270] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56267: loss 0.0093
[2019-04-10 12:15:11,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56267: learning rate 0.0001
[2019-04-10 12:15:13,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.4169341e-15 4.4724269e-10 4.3833975e-17 3.1506397e-12], sum to 1.0000
[2019-04-10 12:15:13,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9825
[2019-04-10 12:15:13,932] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333333, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.015962420400266, 6.9112, 6.9112, 168.9128561911062, 817501.0754936758, 817501.0754936758, 251112.2892793942], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [30.36666666666667, 75.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9996665288136783, 6.911200000000001, 6.9112, 168.9129408759536, 804383.5101098062, 804383.5101098056, 246883.5431196016], 
processed observation next is [0.0, 0.6521739130434783, 0.6382306477093209, 0.7533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.999593327821559, 8.881784197001253e-17, 0.0, 0.8294398683798612, 0.2234398639193906, 0.22343986391939044, 0.3684829001785099], 
reward next is 0.6315, 
noisyNet noise sample is [array([-0.08287866], dtype=float32), -0.78070045]. 
=============================================
[2019-04-10 12:15:20,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999952e-01 2.5994393e-10 4.7418473e-07 9.7623203e-14 3.3471002e-09], sum to 1.0000
[2019-04-10 12:15:20,456] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2747
[2019-04-10 12:15:20,462] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1056220.684549051 W.
[2019-04-10 12:15:20,466] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.16666666666666, 86.16666666666667, 1.0, 2.0, 0.377874921623174, 0.0, 1.0, 0.0, 1.0, 2.0, 0.634472827047563, 6.9112, 6.9112, 168.912956510431, 1056220.684549051, 1056220.684549051, 247323.1578146426], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2257800.0000, 
sim time next is 2258400.0000, 
raw observation next is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.3635292923372661, 1.0, 1.0, 0.3635292923372661, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1016098.491711816, 1016098.491711816, 263322.8498656053], 
processed observation next is [1.0, 0.13043478260869565, 0.43759873617693507, 0.8633333333333334, 1.0, 1.0, 0.2331678220930917, 1.0, 0.5, 0.2331678220930917, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.28224958103106, 0.28224958103106, 0.3930191789038885], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8043975], dtype=float32), -0.27201694]. 
=============================================
[2019-04-10 12:15:20,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9997354e-01 7.2248305e-08 2.5917345e-05 5.1478705e-10 4.4090626e-07], sum to 1.0000
[2019-04-10 12:15:20,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7077
[2019-04-10 12:15:20,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 998794.0183844514 W.
[2019-04-10 12:15:20,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 87.33333333333334, 1.0, 2.0, 0.3573411573785847, 1.0, 1.0, 0.3573411573785847, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 998794.0183844514, 998794.0183844514, 261907.3342482907], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2262000.0000, 
sim time next is 2262600.0000, 
raw observation next is [25.85, 87.5, 1.0, 2.0, 0.2341329506209323, 1.0, 2.0, 0.2341329506209323, 1.0, 1.0, 0.3940037741317343, 6.911199999999999, 6.9112, 170.5573041426782, 981619.5837450122, 981619.5837450128, 279193.9443494201], 
processed observation next is [1.0, 0.17391304347826086, 0.4241706161137442, 0.875, 1.0, 1.0, 0.07726861520594254, 1.0, 1.0, 0.07726861520594254, 1.0, 0.5, 0.26098021235577357, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.27267210659583674, 0.2726721065958369, 0.41670737962600013], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8010029], dtype=float32), -0.014905869]. 
=============================================
[2019-04-10 12:15:25,174] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9368167e-01 7.8950125e-06 5.9848856e-03 1.2655194e-08 3.2549450e-04], sum to 1.0000
[2019-04-10 12:15:25,179] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2659
[2019-04-10 12:15:25,188] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2527105.662680532 W.
[2019-04-10 12:15:25,194] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.2, 60.0, 1.0, 2.0, 0.9035041536467526, 1.0, 1.0, 0.9035041536467526, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2527105.662680532, 2527105.662680532, 473408.2112925252], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2394000.0000, 
sim time next is 2394600.0000, 
raw observation next is [33.06666666666667, 60.83333333333333, 1.0, 2.0, 0.3984951397776946, 1.0, 2.0, 0.3984951397776946, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1113882.080833656, 1113882.080833656, 271757.298784712], 
processed observation next is [1.0, 0.7391304347826086, 0.7661927330173778, 0.6083333333333333, 1.0, 1.0, 0.2752953491297525, 1.0, 1.0, 0.2752953491297525, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.30941168912046, 0.30941168912046, 0.40560790863389845], 
reward next is 0.5944, 
noisyNet noise sample is [array([-2.317262], dtype=float32), -0.1596575]. 
=============================================
[2019-04-10 12:15:25,363] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63626: loss 18.8975
[2019-04-10 12:15:25,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63626: learning rate 0.0001
[2019-04-10 12:15:25,389] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 63642: loss 13.9294
[2019-04-10 12:15:25,390] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63643: loss 17.0978
[2019-04-10 12:15:25,391] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 63643: learning rate 0.0001
[2019-04-10 12:15:25,392] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63643: learning rate 0.0001
[2019-04-10 12:15:25,689] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63831: loss 14.5349
[2019-04-10 12:15:25,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63832: learning rate 0.0001
[2019-04-10 12:15:25,817] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63902: loss 15.4367
[2019-04-10 12:15:25,819] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63905: learning rate 0.0001
[2019-04-10 12:15:25,836] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63918: loss 10.0360
[2019-04-10 12:15:25,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63918: learning rate 0.0001
[2019-04-10 12:15:25,894] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63953: loss 12.7404
[2019-04-10 12:15:25,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63954: learning rate 0.0001
[2019-04-10 12:15:25,921] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63969: loss 10.6657
[2019-04-10 12:15:25,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63970: learning rate 0.0001
[2019-04-10 12:15:26,002] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64016: loss 13.0204
[2019-04-10 12:15:26,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64016: learning rate 0.0001
[2019-04-10 12:15:26,092] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64073: loss 15.1459
[2019-04-10 12:15:26,094] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64074: learning rate 0.0001
[2019-04-10 12:15:26,127] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64093: loss 15.7687
[2019-04-10 12:15:26,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64093: learning rate 0.0001
[2019-04-10 12:15:26,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999988e-01 3.8000991e-12 1.7012815e-07 3.1241282e-17 4.1028531e-10], sum to 1.0000
[2019-04-10 12:15:26,318] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64219: loss 10.6721
[2019-04-10 12:15:26,319] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64219: loss 10.3945
[2019-04-10 12:15:26,319] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64219: learning rate 0.0001
[2019-04-10 12:15:26,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64219: learning rate 0.0001
[2019-04-10 12:15:26,323] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5210
[2019-04-10 12:15:26,326] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.2, 76.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.025187178276875, 6.9112, 6.9112, 168.9128060043675, 824926.7222772983, 824926.7222772983, 253537.5795925737], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2409600.0000, 
sim time next is 2410200.0000, 
raw observation next is [30.1, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.915207306887302, 6.9112, 168.9127378146793, 831644.9435775626, 828802.0258316519, 254811.8847137523], 
processed observation next is [1.0, 0.9130434782608695, 0.6255924170616115, 0.77, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0004007306887301887, 0.0, 0.8294388712560425, 0.23101248432710073, 0.23022278495323664, 0.3803162458414214], 
reward next is 0.5996, 
noisyNet noise sample is [array([2.3928185], dtype=float32), -0.69926894]. 
=============================================
[2019-04-10 12:15:26,401] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64263: loss 10.6938
[2019-04-10 12:15:26,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64263: learning rate 0.0001
[2019-04-10 12:15:26,470] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64309: loss 7.3998
[2019-04-10 12:15:26,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64309: learning rate 0.0001
[2019-04-10 12:15:26,485] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64319: loss 7.0554
[2019-04-10 12:15:26,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64321: learning rate 0.0001
[2019-04-10 12:15:28,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9749303e-01 4.1142110e-07 2.4492803e-03 1.3933532e-10 5.7321089e-05], sum to 1.0000
[2019-04-10 12:15:28,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3996
[2019-04-10 12:15:28,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1238506.508485583 W.
[2019-04-10 12:15:28,134] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.4430542200202273, 1.0, 1.0, 0.4430542200202273, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1238506.508485583, 1238506.508485583, 283515.8729386198], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2439600.0000, 
sim time next is 2440200.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.4246297338221346, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7241826653881016, 6.911199999999999, 6.9112, 168.912956510431, 1186980.887112584, 1186980.887112585, 268481.2656678859], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.3067828118338971, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6636373968147581, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3297169130868289, 0.32971691308682916, 0.40071830696699384], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.04416386], dtype=float32), -1.8635688]. 
=============================================
[2019-04-10 12:15:29,850] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.9999595e-01 6.2502074e-15 4.0288837e-06 3.7980831e-20 3.8224707e-10], sum to 1.0000
[2019-04-10 12:15:29,856] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2405
[2019-04-10 12:15:29,860] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.71666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9436263965539362, 6.911200000000001, 6.9112, 168.912956510431, 768550.7971480859, 768550.7971480853, 233367.8133829183], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2505000.0000, 
sim time next is 2505600.0000, 
raw observation next is [26.7, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9418686152786663, 6.9112, 6.9112, 168.912956510431, 767321.5878844807, 767321.5878844807, 232950.4457128838], 
processed observation next is [1.0, 0.0, 0.46445497630331756, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9291080674130076, 0.0, 0.0, 0.8294399451523027, 0.21314488552346686, 0.21314488552346686, 0.34768723240728927], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.5979021], dtype=float32), -1.555621]. 
=============================================
[2019-04-10 12:15:34,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9972612e-01 1.6515921e-16 2.7384801e-04 2.7465628e-22 2.9616969e-12], sum to 1.0000
[2019-04-10 12:15:34,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5254
[2019-04-10 12:15:34,456] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.2, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8730983833858917, 6.9112, 6.9112, 168.912956510431, 722868.206990218, 722868.206990218, 217351.9687913321], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2586000.0000, 
sim time next is 2586600.0000, 
raw observation next is [26.1, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8674640588042508, 6.9112, 6.9112, 168.912956510431, 718765.6378186536, 718765.6378186536, 216106.2254185574], 
processed observation next is [1.0, 0.9565217391304348, 0.4360189573459717, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8383708034198181, 0.0, 0.0, 0.8294399451523027, 0.19965712161629268, 0.19965712161629268, 0.3225466051023245], 
reward next is 0.6775, 
noisyNet noise sample is [array([-0.8634538], dtype=float32), -0.29518417]. 
=============================================
[2019-04-10 12:15:38,172] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71529: loss 0.2067
[2019-04-10 12:15:38,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71529: learning rate 0.0001
[2019-04-10 12:15:38,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9998820e-01 1.0836915e-19 1.1861179e-05 2.6828089e-25 5.6805458e-14], sum to 1.0000
[2019-04-10 12:15:38,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4265
[2019-04-10 12:15:38,290] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6892225252512192, 6.911199999999999, 6.9112, 168.912956510431, 590966.8437370453, 590966.8437370459, 180813.7544057624], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.689095361915469, 6.911200000000001, 6.9112, 168.912956510431, 590857.7785036261, 590857.7785036255, 180791.323971712], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6208480023359377, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1641271606954517, 0.16412716069545155, 0.2698377969727045], 
reward next is 0.7302, 
noisyNet noise sample is [array([-0.67341834], dtype=float32), -1.1765862]. 
=============================================
[2019-04-10 12:15:38,382] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71660: loss 0.3428
[2019-04-10 12:15:38,383] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71660: learning rate 0.0001
[2019-04-10 12:15:38,410] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71677: loss 0.3451
[2019-04-10 12:15:38,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71677: learning rate 0.0001
[2019-04-10 12:15:38,644] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71815: loss 0.4077
[2019-04-10 12:15:38,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71817: learning rate 0.0001
[2019-04-10 12:15:38,707] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71850: loss 0.2485
[2019-04-10 12:15:38,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71850: learning rate 0.0001
[2019-04-10 12:15:38,808] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71917: loss 0.2196
[2019-04-10 12:15:38,811] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71917: learning rate 0.0001
[2019-04-10 12:15:38,915] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71982: loss 0.1934
[2019-04-10 12:15:38,918] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71983: learning rate 0.0001
[2019-04-10 12:15:38,935] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71994: loss 0.1758
[2019-04-10 12:15:38,938] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71997: learning rate 0.0001
[2019-04-10 12:15:38,955] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72005: loss 0.1731
[2019-04-10 12:15:38,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72006: learning rate 0.0001
[2019-04-10 12:15:38,989] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72023: loss 0.1544
[2019-04-10 12:15:38,990] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72023: learning rate 0.0001
[2019-04-10 12:15:38,990] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72023: loss 0.1142
[2019-04-10 12:15:38,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72023: learning rate 0.0001
[2019-04-10 12:15:39,268] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72197: loss 0.0444
[2019-04-10 12:15:39,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72197: learning rate 0.0001
[2019-04-10 12:15:39,351] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72251: loss 0.0046
[2019-04-10 12:15:39,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72251: learning rate 0.0001
[2019-04-10 12:15:39,355] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72252: loss 0.0101
[2019-04-10 12:15:39,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72252: learning rate 0.0001
[2019-04-10 12:15:39,449] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72311: loss 0.0007
[2019-04-10 12:15:39,450] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72313: loss 0.0002
[2019-04-10 12:15:39,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72315: learning rate 0.0001
[2019-04-10 12:15:39,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72316: learning rate 0.0001
[2019-04-10 12:15:41,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9999905e-01 7.9579307e-21 9.8991541e-07 9.3475578e-28 6.6977140e-16], sum to 1.0000
[2019-04-10 12:15:41,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6213
[2019-04-10 12:15:41,153] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6630724072552058, 6.911199999999999, 6.9112, 168.912956510431, 570630.7573937544, 570630.757393755, 176288.4399205728], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2751000.0000, 
sim time next is 2751600.0000, 
raw observation next is [22.0, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6577407589980202, 6.911200000000001, 6.9112, 168.912956510431, 566741.8131268523, 566741.8131268517, 175386.0194536074], 
processed observation next is [0.0, 0.8695652173913043, 0.2417061611374408, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5826106817049026, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15742828142412563, 0.15742828142412546, 0.26177017828896626], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.19835868], dtype=float32), 0.541142]. 
=============================================
[2019-04-10 12:15:42,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999785e-01 2.4986058e-19 2.1939618e-06 2.3074960e-25 3.1761565e-15], sum to 1.0000
[2019-04-10 12:15:42,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9090
[2019-04-10 12:15:42,373] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6192565782253998, 6.911200000000001, 6.9112, 168.912956510431, 537003.0584279208, 537003.0584279202, 169093.9494506349], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2769000.0000, 
sim time next is 2769600.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.619235138015804, 6.9112, 6.9112, 168.912956510431, 536984.4613737067, 536984.4613737067, 169090.574990612], 
processed observation next is [1.0, 0.043478260869565216, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5356526073363463, 0.0, 0.0, 0.8294399451523027, 0.1491623503815852, 0.1491623503815852, 0.25237399252330145], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.8127016], dtype=float32), 1.1665429]. 
=============================================
[2019-04-10 12:15:42,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9991596e-01 6.7781661e-17 8.3981460e-05 6.0173290e-22 1.4264704e-12], sum to 1.0000
[2019-04-10 12:15:42,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2684
[2019-04-10 12:15:42,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 963476.9151166704 W.
[2019-04-10 12:15:42,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9999976e-01 2.5304453e-20 2.2776908e-07 1.4998469e-26 2.4970341e-15], sum to 1.0000
[2019-04-10 12:15:42,863] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6264195388108045, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565104282, 963476.9151166704, 963476.9151166704, 216542.8973876841], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2797200.0000, 
sim time next is 2797800.0000, 
raw observation next is [22.0, 93.00000000000001, 1.0, 2.0, 0.1892917623715394, 1.0, 1.0, 0.1892917623715394, 1.0, 1.0, 0.3334296015965619, 6.9112, 6.9112, 170.5573041426782, 862972.334569945, 862972.334569945, 275300.438440644], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9300000000000002, 1.0, 1.0, 0.023243087194625783, 1.0, 0.5, 0.023243087194625783, 1.0, 0.5, 0.1871092702397096, 0.0, 0.0, 0.8375144448122397, 0.23971453738054027, 0.23971453738054027, 0.4108961767770806], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4623538], dtype=float32), -1.2673695]. 
=============================================
[2019-04-10 12:15:42,866] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-10 12:15:42,872] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.594341977602364, 6.9112, 6.9112, 168.912956510431, 516223.059063176, 516223.059063176, 165244.775205951], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2778000.0000, 
sim time next is 2778600.0000, 
raw observation next is [21.16666666666666, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5927952505378549, 6.9112, 6.9112, 168.912956510431, 515151.0291003835, 515151.0291003835, 165007.920620609], 
processed observation next is [1.0, 0.13043478260869565, 0.2022116903633489, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5034088421193351, 0.0, 0.0, 0.8294399451523027, 0.14309750808343988, 0.14309750808343988, 0.24628047853822238], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.67190874], dtype=float32), -0.13002717]. 
=============================================
[2019-04-10 12:15:43,821] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-10 12:15:43,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:15:43,822] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:15:43,824] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:43,825] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:15:43,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:15:43,826] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:43,827] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:43,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:15:43,828] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:43,830] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:15:43,842] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-04-10 12:15:43,858] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-04-10 12:15:43,858] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-04-10 12:15:43,872] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-04-10 12:15:43,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-04-10 12:15:45,546] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0043339306]
[2019-04-10 12:15:45,547] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.43333333333334, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6753328307969947, 6.911199999999999, 6.9112, 168.912956510431, 579468.8303373059, 579468.8303373066, 178389.3021293406]
[2019-04-10 12:15:45,547] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:15:45,549] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.8759101e-21 3.6539125e-09 7.3254671e-28 5.0106043e-16], sampled 0.7080616434133835
[2019-04-10 12:15:56,685] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0043339306]
[2019-04-10 12:15:56,686] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.00305512, 75.93290932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6498987518270903, 6.9112, 6.9112, 168.912956510431, 562252.9191342865, 562252.9191342865, 174059.9919180644]
[2019-04-10 12:15:56,687] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:15:56,688] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.1332617e-21 1.0051757e-09 1.9762310e-28 1.5215168e-16], sampled 0.8484368085104245
[2019-04-10 12:16:04,489] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0043339306]
[2019-04-10 12:16:04,489] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.859034395, 91.12490695833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6206869505763696, 6.9112, 6.9112, 168.912956510431, 540240.1391895899, 540240.1391895899, 169287.8435566327]
[2019-04-10 12:16:04,491] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:16:04,493] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 8.4285682e-21 4.1542525e-09 2.6966280e-27 1.2826693e-15], sampled 0.23372376456959276
[2019-04-10 12:16:45,897] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0043339306]
[2019-04-10 12:16:45,898] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 93.0, 1.0, 2.0, 0.7618482563944421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 1064747.976245562, 1064747.976245561, 234760.9524036036]
[2019-04-10 12:16:45,899] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:16:45,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9999988e-01 1.7754563e-17 1.5107301e-07 9.8312674e-23 2.3826438e-13], sampled 0.7859147804738419
[2019-04-10 12:16:45,903] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1064747.976245562 W.
[2019-04-10 12:16:54,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([2.8920368e-07], dtype=float32), 0.0043339306]
[2019-04-10 12:16:54,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.23333333333333, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7304282488134751, 6.911200000000001, 6.9112, 168.912956510431, 622474.592731903, 622474.5927319024, 188300.3847245593]
[2019-04-10 12:16:54,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:16:54,832] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.3722796e-21 1.6645186e-09 5.6161404e-27 3.7051873e-16], sampled 0.8735112188042583
[2019-04-10 12:17:08,313] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7285.9166 3319626785.1727 2143.0000
[2019-04-10 12:17:08,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.0634 2989460342.8589 1566.0000
[2019-04-10 12:17:08,669] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.7833 2937732720.8594 1381.0000
[2019-04-10 12:17:08,871] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.2610 3185088381.1007 2464.0000
[2019-04-10 12:17:09,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6665 3105726324.4656 2010.0000
[2019-04-10 12:17:10,165] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 75000, evaluation results [75000.0, 7285.91655207562, 3319626785.1726947, 2143.0, 7347.66647891646, 3105726324.4655676, 2010.0, 8061.783282309715, 2937732720.8593965, 1381.0, 7031.260974738479, 3185088381.1007032, 2464.0, 7923.063364588191, 2989460342.858924, 1566.0]
[2019-04-10 12:17:23,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 6.8286571e-21 2.9935316e-09 2.1945257e-28 2.0693250e-15], sum to 1.0000
[2019-04-10 12:17:23,642] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8440
[2019-04-10 12:17:23,690] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.603815012802483, 6.911200000000001, 6.9112, 168.912956510431, 525244.7149331791, 525244.7149331785, 166671.0304537062], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2966400.0000, 
sim time next is 2967000.0000, 
raw observation next is [21.16666666666667, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6419135283737306, 6.9112, 6.9112, 168.912956510431, 558262.3869942317, 558262.3869942317, 172704.6814177723], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.9900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5633091809435739, 0.0, 0.0, 0.8294399451523027, 0.1550728852761755, 0.1550728852761755, 0.25776818122055567], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.82325304], dtype=float32), 0.21943574]. 
=============================================
[2019-04-10 12:17:23,746] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.48761 ]
 [80.37537 ]
 [80.319115]
 [80.24719 ]
 [80.1631  ]], R is [[80.47654724]
 [80.42301941]
 [80.36980438]
 [80.31706238]
 [80.26421356]].
[2019-04-10 12:17:24,192] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79603: loss -118.7014
[2019-04-10 12:17:24,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79603: learning rate 0.0001
[2019-04-10 12:17:24,587] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79633: loss -127.3165
[2019-04-10 12:17:24,588] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79633: learning rate 0.0001
[2019-04-10 12:17:25,266] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79749: loss 26.4904
[2019-04-10 12:17:25,266] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79749: learning rate 0.0001
[2019-04-10 12:17:25,952] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79852: loss 12.0970
[2019-04-10 12:17:25,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79852: learning rate 0.0001
[2019-04-10 12:17:25,957] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79852: loss 29.1201
[2019-04-10 12:17:26,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79852: learning rate 0.0001
[2019-04-10 12:17:26,280] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79896: loss -40.8187
[2019-04-10 12:17:26,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79896: learning rate 0.0001
[2019-04-10 12:17:26,571] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79907: loss 3.4908
[2019-04-10 12:17:26,571] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79907: learning rate 0.0001
[2019-04-10 12:17:26,805] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79925: loss -60.5275
[2019-04-10 12:17:26,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79925: learning rate 0.0001
[2019-04-10 12:17:27,197] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79988: loss -53.6152
[2019-04-10 12:17:27,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79988: learning rate 0.0001
[2019-04-10 12:17:27,456] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80014: loss -39.4677
[2019-04-10 12:17:27,468] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80015: learning rate 0.0001
[2019-04-10 12:17:27,683] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80053: loss -73.5772
[2019-04-10 12:17:27,684] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80053: learning rate 0.0001
[2019-04-10 12:17:27,893] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80076: loss -21.1324
[2019-04-10 12:17:27,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80076: learning rate 0.0001
[2019-04-10 12:17:28,214] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80146: loss -68.6376
[2019-04-10 12:17:28,217] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80148: loss -30.9920
[2019-04-10 12:17:28,217] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80146: learning rate 0.0001
[2019-04-10 12:17:28,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80158: learning rate 0.0001
[2019-04-10 12:17:28,895] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80255: loss 10.5568
[2019-04-10 12:17:28,899] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80255: learning rate 0.0001
[2019-04-10 12:17:29,053] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80268: loss -108.7567
[2019-04-10 12:17:29,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80268: learning rate 0.0001
[2019-04-10 12:17:37,812] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999499e-01 3.9689766e-15 4.9624837e-06 5.3326473e-19 7.6182172e-09], sum to 1.0000
[2019-04-10 12:17:37,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8888
[2019-04-10 12:17:37,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1096425.545898507 W.
[2019-04-10 12:17:37,830] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.66666666666667, 89.0, 1.0, 2.0, 0.2500131467771011, 1.0, 1.0, 0.2500131467771011, 1.0, 1.0, 0.4291771145873012, 6.9112, 6.9112, 170.5573041426782, 1096425.545898507, 1096425.545898507, 289938.9095708007], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3145200.0000, 
sim time next is 3145800.0000, 
raw observation next is [23.83333333333333, 89.0, 1.0, 2.0, 0.2615909003114193, 1.0, 2.0, 0.2615909003114193, 1.0, 2.0, 0.4462357393336722, 6.911200000000001, 6.9112, 170.5573041426782, 1136691.674493036, 1136691.674493035, 292835.5651059475], 
processed observation next is [1.0, 0.391304347826087, 0.32859399684044216, 0.89, 1.0, 1.0, 0.1103504823029148, 1.0, 1.0, 0.1103504823029148, 1.0, 1.0, 0.3246777308947222, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3157476873591767, 0.3157476873591764, 0.43706800762081716], 
reward next is 0.5629, 
noisyNet noise sample is [array([1.3345202], dtype=float32), 0.7027642]. 
=============================================
[2019-04-10 12:17:38,332] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.9979156e-01 6.2897750e-16 2.0847897e-04 8.9070123e-20 2.1035508e-08], sum to 1.0000
[2019-04-10 12:17:38,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3751
[2019-04-10 12:17:38,341] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8303939245050324, 6.9112, 6.9112, 168.912956510431, 687893.6597000076, 687893.6597000076, 207992.3360983176], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3174600.0000, 
sim time next is 3175200.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8389376702297524, 6.911200000000001, 6.9112, 168.912956510431, 695109.4536510569, 695109.4536510563, 209832.8345447041], 
processed observation next is [1.0, 0.782608695652174, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8035825246704298, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1930859593475158, 0.19308595934751563, 0.3131833351413494], 
reward next is 0.6868, 
noisyNet noise sample is [array([1.3145299], dtype=float32), 1.2489736]. 
=============================================
[2019-04-10 12:17:41,409] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9997783e-01 7.7853619e-18 2.2119086e-05 1.9060251e-23 1.3571157e-11], sum to 1.0000
[2019-04-10 12:17:41,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9650
[2019-04-10 12:17:41,519] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.795430660070072, 6.9112, 6.9112, 168.912956510431, 666247.2117308198, 666247.2117308198, 200884.6848172308], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3225600.0000, 
sim time next is 3226200.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.798492037627295, 6.9112, 6.9112, 168.912956510431, 668480.6591032878, 668480.6591032878, 201505.0160464478], 
processed observation next is [0.0, 0.34782608695652173, 0.4865718799368086, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.754258582472311, 0.0, 0.0, 0.8294399451523027, 0.1856890719731355, 0.1856890719731355, 0.30075375529320564], 
reward next is 0.6992, 
noisyNet noise sample is [array([-0.02200063], dtype=float32), 0.34851432]. 
=============================================
[2019-04-10 12:17:43,370] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 87591: loss 0.0741
[2019-04-10 12:17:43,372] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 87593: learning rate 0.0001
[2019-04-10 12:17:43,397] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87608: loss 0.0824
[2019-04-10 12:17:43,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87608: learning rate 0.0001
[2019-04-10 12:17:43,550] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87705: loss 0.0316
[2019-04-10 12:17:43,553] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87705: learning rate 0.0001
[2019-04-10 12:17:43,650] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87766: loss 0.0109
[2019-04-10 12:17:43,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87766: learning rate 0.0001
[2019-04-10 12:17:43,720] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87810: loss 0.0154
[2019-04-10 12:17:43,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87810: learning rate 0.0001
[2019-04-10 12:17:43,785] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87848: loss 0.0138
[2019-04-10 12:17:43,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87848: learning rate 0.0001
[2019-04-10 12:17:43,970] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87965: loss 0.0138
[2019-04-10 12:17:43,973] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87966: learning rate 0.0001
[2019-04-10 12:17:44,019] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87995: loss 0.0300
[2019-04-10 12:17:44,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87995: learning rate 0.0001
[2019-04-10 12:17:44,027] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87995: loss 0.0339
[2019-04-10 12:17:44,028] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87997: learning rate 0.0001
[2019-04-10 12:17:44,126] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88061: loss 0.0304
[2019-04-10 12:17:44,129] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88061: learning rate 0.0001
[2019-04-10 12:17:44,139] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88068: loss 0.0139
[2019-04-10 12:17:44,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88069: learning rate 0.0001
[2019-04-10 12:17:44,293] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88160: loss 0.0066
[2019-04-10 12:17:44,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88163: learning rate 0.0001
[2019-04-10 12:17:44,380] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88220: loss 0.0068
[2019-04-10 12:17:44,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88220: learning rate 0.0001
[2019-04-10 12:17:44,520] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88306: loss 0.0301
[2019-04-10 12:17:44,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88306: learning rate 0.0001
[2019-04-10 12:17:44,614] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88365: loss 0.0057
[2019-04-10 12:17:44,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88365: learning rate 0.0001
[2019-04-10 12:17:44,632] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88377: loss 0.0153
[2019-04-10 12:17:44,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88377: learning rate 0.0001
[2019-04-10 12:17:50,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9997067e-01 6.0283667e-14 2.9215702e-05 3.4825483e-18 1.4698523e-07], sum to 1.0000
[2019-04-10 12:17:50,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2031
[2019-04-10 12:17:50,228] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8735516997030445, 6.9112, 6.9112, 168.912956510431, 721473.5317996818, 721473.5317996818, 217392.077834779], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8749207799094907, 6.911200000000001, 6.9112, 168.912956510431, 722605.9851274666, 722605.9851274659, 217700.734128221], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8474643657432813, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20072388475762962, 0.20072388475762942, 0.32492646884809107], 
reward next is 0.6751, 
noisyNet noise sample is [array([-1.6250912], dtype=float32), 1.371893]. 
=============================================
[2019-04-10 12:17:51,780] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.5058590e-02 4.0906585e-09 9.0137661e-01 5.1703225e-11 3.5648295e-03], sum to 1.0000
[2019-04-10 12:17:51,784] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5478
[2019-04-10 12:17:51,790] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.66666666666666, 67.33333333333333, 1.0, 2.0, 0.8181553676742813, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.972437662017423, 6.9112, 168.9125435808486, 2040482.85790442, 1997038.859036329, 413511.3840076718], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.7039861681761728, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.973691903391463, 6.9112, 168.9125341933866, 1880706.869141759, 1836373.072914393, 386622.1917076039], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.643356829127919, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00624919033914626, 0.0, 0.8294378713822771, 0.5224185747615997, 0.5101036313651092, 0.577048047324782], 
reward next is 0.1105, 
noisyNet noise sample is [array([0.2446983], dtype=float32), 1.0751507]. 
=============================================
[2019-04-10 12:17:53,334] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.5513170e-03 1.4382647e-09 9.9466658e-01 6.3363120e-11 2.7821374e-03], sum to 1.0000
[2019-04-10 12:17:53,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4516
[2019-04-10 12:17:53,342] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.33333333333333, 69.0, 1.0, 2.0, 0.2730576381995816, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4736659490640674, 6.9112, 6.9112, 168.912956510431, 763134.3971128488, 763134.3971128488, 213648.0639491381], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3519600.0000, 
sim time next is 3520200.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.2737203770559531, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4742915319883996, 6.911199999999999, 6.9112, 168.912956510431, 764987.2694295056, 764987.2694295062, 213795.2592414421], 
processed observation next is [1.0, 0.7391304347826086, 0.6761453396524489, 0.695, 1.0, 1.0, 0.12496430970596757, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35889211218097505, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21249646373041822, 0.2124964637304184, 0.31909740185289864], 
reward next is 0.6809, 
noisyNet noise sample is [array([-0.88298976], dtype=float32), 1.1551372]. 
=============================================
[2019-04-10 12:17:54,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9997342e-01 1.5087289e-13 2.6431566e-05 3.0684176e-16 1.5955565e-07], sum to 1.0000
[2019-04-10 12:17:54,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0768
[2019-04-10 12:17:54,969] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.875170700229064, 6.9112, 6.9112, 168.912956510431, 722810.2866421308, 722810.2866421308, 217757.0450725606], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3538200.0000, 
sim time next is 3538800.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.874417656720864, 6.911200000000001, 6.9112, 168.912956510431, 722190.0658495104, 722190.0658495097, 217587.2562636054], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8468508008791024, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20060835162486398, 0.2006083516248638, 0.3247570989009036], 
reward next is 0.6752, 
noisyNet noise sample is [array([-1.6535882], dtype=float32), 0.5766311]. 
=============================================
[2019-04-10 12:17:56,798] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95782: loss 11.3905
[2019-04-10 12:17:56,800] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95784: learning rate 0.0001
[2019-04-10 12:17:56,806] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 95785: loss -16.1929
[2019-04-10 12:17:56,809] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 95785: learning rate 0.0001
[2019-04-10 12:17:56,836] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95799: loss -17.9198
[2019-04-10 12:17:56,839] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95800: loss 3.8895
[2019-04-10 12:17:56,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95800: learning rate 0.0001
[2019-04-10 12:17:56,841] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95800: learning rate 0.0001
[2019-04-10 12:17:56,883] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95828: loss 8.3390
[2019-04-10 12:17:56,886] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95829: learning rate 0.0001
[2019-04-10 12:17:56,932] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95858: loss -21.5474
[2019-04-10 12:17:56,935] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95859: learning rate 0.0001
[2019-04-10 12:17:57,088] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95951: loss 20.0605
[2019-04-10 12:17:57,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95951: learning rate 0.0001
[2019-04-10 12:17:57,112] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95964: loss -8.1236
[2019-04-10 12:17:57,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95965: learning rate 0.0001
[2019-04-10 12:17:57,149] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95985: loss 3.9563
[2019-04-10 12:17:57,151] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95985: learning rate 0.0001
[2019-04-10 12:17:57,226] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96032: loss 2.9789
[2019-04-10 12:17:57,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96032: learning rate 0.0001
[2019-04-10 12:17:57,307] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96085: loss 2.7602
[2019-04-10 12:17:57,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96085: learning rate 0.0001
[2019-04-10 12:17:57,339] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96102: loss -11.0546
[2019-04-10 12:17:57,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96103: learning rate 0.0001
[2019-04-10 12:17:57,418] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96148: loss 1.8100
[2019-04-10 12:17:57,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96148: learning rate 0.0001
[2019-04-10 12:17:57,483] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96190: loss -14.0819
[2019-04-10 12:17:57,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96190: learning rate 0.0001
[2019-04-10 12:17:57,612] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96269: loss -12.1708
[2019-04-10 12:17:57,613] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96269: learning rate 0.0001
[2019-04-10 12:17:57,650] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96291: loss -24.4104
[2019-04-10 12:17:57,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96291: learning rate 0.0001
[2019-04-10 12:18:01,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999905e-01 3.3541565e-15 8.5260712e-07 8.1769807e-19 6.7507230e-08], sum to 1.0000
[2019-04-10 12:18:01,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1260
[2019-04-10 12:18:01,861] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8211699504949304, 6.911199999999999, 6.9112, 168.912956510431, 685871.4193265308, 685871.4193265315, 206196.0124789512], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3712200.0000, 
sim time next is 3712800.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.82018535729212, 6.9112, 6.9112, 168.912956510431, 685271.7453175467, 685271.7453175467, 205993.5404675966], 
processed observation next is [1.0, 1.0, 0.5102685624012641, 0.7566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.780713850356244, 0.0, 0.0, 0.8294399451523027, 0.1903532625882074, 0.1903532625882074, 0.3074530454740248], 
reward next is 0.6925, 
noisyNet noise sample is [array([-0.21355699], dtype=float32), 0.09788976]. 
=============================================
[2019-04-10 12:18:02,734] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.6263744e-14 5.9218117e-08 3.6378878e-18 8.9479339e-09], sum to 1.0000
[2019-04-10 12:18:02,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5769
[2019-04-10 12:18:02,746] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1986764.315635421 W.
[2019-04-10 12:18:02,750] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 8.483147619271673, 6.9112, 168.9043600233182, 1986764.315635421, 871627.3278177263, 256528.9795455071], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3723000.0000, 
sim time next is 3723600.0000, 
raw observation next is [27.0, 74.0, 1.0, 1.0, 0.457593145433079, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7703580267756699, 6.911199999999999, 6.9112, 168.9116765553204, 1300650.604502021, 1300650.604502022, 283623.8408429837], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.74, 1.0, 0.5, 0.34649776558202294, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7199488131410607, -8.881784197001253e-17, 0.0, 0.8294336599867437, 0.36129183458389474, 0.361291834583895, 0.4233191654372891], 
reward next is 0.5767, 
noisyNet noise sample is [array([0.33389217], dtype=float32), -0.29547957]. 
=============================================
[2019-04-10 12:18:03,731] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 12:18:03,732] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:18:03,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:18:03,734] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:18:03,734] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:18:03,734] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:18:03,735] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:18:03,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:18:03,736] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:18:03,737] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:18:03,738] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:18:03,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-04-10 12:18:03,749] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-04-10 12:18:03,777] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-04-10 12:18:03,778] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-04-10 12:18:03,820] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-04-10 12:18:12,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:18:12,792] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.164765965, 61.56150157833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8148776748689688, 6.911200000000001, 6.9112, 168.912956510431, 723163.2859000519, 723163.2859000512, 204460.9033927917]
[2019-04-10 12:18:12,792] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:18:12,795] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.8887982e-15 2.0072719e-09 6.8169521e-19 3.2837263e-10], sampled 0.8144644092706852
[2019-04-10 12:18:13,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:18:13,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.46666666666667, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6126671664037046, 6.9112, 6.9112, 168.912956510431, 532914.5172154653, 532914.5172154653, 168037.9772795755]
[2019-04-10 12:18:13,383] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:18:13,385] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 6.2763924e-16 9.7798269e-10 1.4186132e-19 1.0301611e-10], sampled 0.9686050568685739
[2019-04-10 12:18:27,259] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:18:27,261] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.29146515, 95.04653423000002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9643893497848237, 6.9112, 6.9112, 168.912956510431, 805670.9509490972, 805670.9509490972, 239272.3804736761]
[2019-04-10 12:18:27,261] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:18:27,263] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.00000000e+00 4.21149059e-15 1.33938345e-08 2.32164493e-18
 1.98938843e-09], sampled 0.27642154756250814
[2019-04-10 12:18:27,757] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:18:27,757] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.85046001666667, 79.49352414166667, 1.0, 2.0, 0.6165392019669493, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.957410704072617, 6.9112, 168.912576585018, 1723867.069982746, 1691083.67980175, 367697.6084088419]
[2019-04-10 12:18:27,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:18:27,761] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9995005e-01 3.0322706e-11 1.8079725e-05 7.1595345e-14 3.1854550e-05], sampled 0.27800152267142086
[2019-04-10 12:18:27,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1723867.069982746 W.
[2019-04-10 12:18:50,835] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:18:50,837] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8345565224598325, 6.9112, 6.9112, 168.912956510431, 696322.1120910884, 696322.1120910884, 209033.3002637448]
[2019-04-10 12:18:50,838] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:18:50,841] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.9567700e-15 2.5861042e-09 6.3399584e-19 3.3734029e-10], sampled 0.17737402197387442
[2019-04-10 12:19:08,488] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:19:08,488] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.979826151279679, 6.9112, 6.9112, 168.912956510431, 795113.7761830268, 795113.7761830268, 242199.1984662594]
[2019-04-10 12:19:08,489] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:19:08,491] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.8723494e-14 1.6111120e-08 2.5129137e-17 4.7058677e-09], sampled 0.2742907756090922
[2019-04-10 12:19:14,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.01779828], dtype=float32), 0.0034544717]
[2019-04-10 12:19:14,105] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.1, 50.0, 1.0, 2.0, 0.9962828163074118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1536412.822916233, 1536412.822916233, 318786.9920947404]
[2019-04-10 12:19:14,107] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:19:14,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9923956e-01 2.8332348e-10 1.5495381e-04 4.7954078e-13 6.0544052e-04], sampled 0.37041016851435815
[2019-04-10 12:19:14,110] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1536412.822916233 W.
[2019-04-10 12:19:25,575] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7028.6918 3185173715.3969 2461.0000
[2019-04-10 12:19:25,668] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8058.8686 2937869554.7507 1385.0000
[2019-04-10 12:19:26,086] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7351.4597 3105860599.9327 2014.0000
[2019-04-10 12:19:26,242] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7920.4380 2989562531.4232 1570.0000
[2019-04-10 12:19:26,329] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7269.6736 3319855553.1724 2166.0000
[2019-04-10 12:19:27,347] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 100000, evaluation results [100000.0, 7269.673609068372, 3319855553.1724286, 2166.0, 7351.459669237774, 3105860599.932717, 2014.0, 8058.868632606771, 2937869554.7507195, 1385.0, 7028.691782929314, 3185173715.3968997, 2461.0, 7920.438023656255, 2989562531.4231596, 1570.0]
[2019-04-10 12:19:31,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.0072112e-18 3.3465795e-12 1.3534454e-24 1.5587118e-10], sum to 1.0000
[2019-04-10 12:19:31,717] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2188
[2019-04-10 12:19:31,720] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8745576424963991, 6.911199999999999, 6.9112, 168.912956510431, 722306.5037944611, 722306.5037944618, 217618.8486004149], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3808200.0000, 
sim time next is 3808800.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8741744364587297, 6.9112, 6.9112, 168.912956510431, 721989.9022757778, 721989.9022757778, 217532.4556386065], 
processed observation next is [0.0, 0.08695652173913043, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8465541908033288, 0.0, 0.0, 0.8294399451523027, 0.2005527506321605, 0.2005527506321605, 0.32467530692329327], 
reward next is 0.6753, 
noisyNet noise sample is [array([0.07158425], dtype=float32), 1.50627]. 
=============================================
[2019-04-10 12:19:37,648] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103758: loss 0.0424
[2019-04-10 12:19:37,661] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103758: learning rate 0.0001
[2019-04-10 12:19:37,663] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103762: loss 0.0899
[2019-04-10 12:19:37,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103762: learning rate 0.0001
[2019-04-10 12:19:37,683] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103776: loss 0.1040
[2019-04-10 12:19:37,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103783: loss 0.0245
[2019-04-10 12:19:37,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103783: learning rate 0.0001
[2019-04-10 12:19:37,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103776: learning rate 0.0001
[2019-04-10 12:19:37,848] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 103839: loss 0.1130
[2019-04-10 12:19:37,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 103839: learning rate 0.0001
[2019-04-10 12:19:37,953] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103871: loss 0.0802
[2019-04-10 12:19:37,997] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103871: learning rate 0.0001
[2019-04-10 12:19:37,998] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103883: loss 0.0188
[2019-04-10 12:19:38,004] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103883: learning rate 0.0001
[2019-04-10 12:19:38,308] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103975: loss 0.0770
[2019-04-10 12:19:38,312] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103979: learning rate 0.0001
[2019-04-10 12:19:38,635] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104067: loss 0.3341
[2019-04-10 12:19:38,673] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104076: loss 0.1938
[2019-04-10 12:19:38,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104076: learning rate 0.0001
[2019-04-10 12:19:38,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104076: learning rate 0.0001
[2019-04-10 12:19:38,844] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104116: loss 0.1585
[2019-04-10 12:19:38,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104116: learning rate 0.0001
[2019-04-10 12:19:38,884] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104131: loss 0.0971
[2019-04-10 12:19:38,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104132: learning rate 0.0001
[2019-04-10 12:19:38,960] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104164: loss 0.0876
[2019-04-10 12:19:38,961] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104164: learning rate 0.0001
[2019-04-10 12:19:39,046] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104188: loss 0.0419
[2019-04-10 12:19:39,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104188: learning rate 0.0001
[2019-04-10 12:19:39,132] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.9844244e-22 1.1101244e-17 4.3145284e-27 2.0802927e-17], sum to 1.0000
[2019-04-10 12:19:39,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2853
[2019-04-10 12:19:39,139] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104220: loss 0.0227
[2019-04-10 12:19:39,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104225: learning rate 0.0001
[2019-04-10 12:19:39,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 895217.6949554542 W.
[2019-04-10 12:19:39,328] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.83333333333333, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.004783231924198, 6.9112, 168.9122133885263, 895217.6949554542, 828826.8214259084, 254812.1300186529], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3916200.0000, 
sim time next is 3916800.0000, 
raw observation next is [30.0, 79.0, 1.0, 1.0, 0.3002714599145234, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5214722245823207, 6.9112, 6.9112, 168.9128803030628, 839220.9258134653, 839220.9258134653, 222042.9451648281], 
processed observation next is [0.0, 0.34782608695652173, 0.6208530805687204, 0.79, 1.0, 0.5, 0.1569535661620764, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41642954217356176, 0.0, 0.0, 0.8294395709392369, 0.2331169238370737, 0.2331169238370737, 0.331407380843027], 
reward next is 0.6686, 
noisyNet noise sample is [array([0.4124581], dtype=float32), 0.9394329]. 
=============================================
[2019-04-10 12:19:39,331] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104241: loss 0.0270
[2019-04-10 12:19:39,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104241: learning rate 0.0001
[2019-04-10 12:19:51,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 3.4105004e-17 4.2515427e-13 7.0036837e-18 1.5180451e-11], sum to 1.0000
[2019-04-10 12:19:51,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9446
[2019-04-10 12:19:51,690] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.6, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9275182866508495, 6.9112, 6.9112, 168.912956510431, 758448.3370883181, 758448.3370883181, 229624.5532676869], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [27.56666666666667, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 11.62841168339526, 6.9112, 169.2515456508764, 4196869.631790559, 843609.0837670752, 255622.7764432643], 
processed observation next is [1.0, 0.08695652173913043, 0.505529225908373, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.47172116833952604, 0.0, 0.8311025728386688, 1.1657971199418218, 0.23433585660196535, 0.3815265320048721], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06883349], dtype=float32), -0.30597317]. 
=============================================
[2019-04-10 12:19:51,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.0687736e-18 3.5249737e-14 1.4276965e-18 3.8099190e-12], sum to 1.0000
[2019-04-10 12:19:51,697] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-10 12:19:51,714] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.93333333333333, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9283627727704795, 6.9112, 6.9112, 168.912956510431, 758207.6030335304, 758207.6030335304, 229784.3471498776], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4062000.0000, 
sim time next is 4062600.0000, 
raw observation next is [27.9, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9278643797180826, 6.9112, 6.9112, 168.912956510431, 757867.3616318501, 757867.3616318501, 229668.1666411956], 
processed observation next is [1.0, 0.0, 0.5213270142180094, 0.845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9120297313635153, 0.0, 0.0, 0.8294399451523027, 0.21051871156440283, 0.21051871156440283, 0.34278830841969493], 
reward next is 0.6572, 
noisyNet noise sample is [array([0.24821423], dtype=float32), 0.13602541]. 
=============================================
[2019-04-10 12:19:57,599] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111591: loss 1.2099
[2019-04-10 12:19:57,601] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111593: learning rate 0.0001
[2019-04-10 12:19:57,719] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111667: loss 1.2410
[2019-04-10 12:19:57,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111667: learning rate 0.0001
[2019-04-10 12:19:57,765] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 111696: loss 1.0051
[2019-04-10 12:19:57,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 111697: learning rate 0.0001
[2019-04-10 12:19:57,794] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111711: loss 1.1078
[2019-04-10 12:19:57,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111712: learning rate 0.0001
[2019-04-10 12:19:57,912] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111789: loss 1.0258
[2019-04-10 12:19:57,913] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111789: learning rate 0.0001
[2019-04-10 12:19:58,124] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111929: loss 0.7968
[2019-04-10 12:19:58,129] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111931: learning rate 0.0001
[2019-04-10 12:19:58,219] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111990: loss 0.8335
[2019-04-10 12:19:58,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111990: learning rate 0.0001
[2019-04-10 12:19:58,254] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112012: loss 0.7691
[2019-04-10 12:19:58,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112012: learning rate 0.0001
[2019-04-10 12:19:58,268] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112021: loss 0.8128
[2019-04-10 12:19:58,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112021: learning rate 0.0001
[2019-04-10 12:19:58,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112029: loss 0.7789
[2019-04-10 12:19:58,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112029: learning rate 0.0001
[2019-04-10 12:19:58,393] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112100: loss 0.7019
[2019-04-10 12:19:58,393] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112100: learning rate 0.0001
[2019-04-10 12:19:58,462] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112148: loss 0.6698
[2019-04-10 12:19:58,465] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112148: learning rate 0.0001
[2019-04-10 12:19:58,584] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112228: loss 0.6072
[2019-04-10 12:19:58,585] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112229: loss 0.5501
[2019-04-10 12:19:58,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112229: learning rate 0.0001
[2019-04-10 12:19:58,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112229: learning rate 0.0001
[2019-04-10 12:19:58,660] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112276: loss 0.4100
[2019-04-10 12:19:58,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112276: learning rate 0.0001
[2019-04-10 12:19:58,782] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112354: loss 0.5021
[2019-04-10 12:19:58,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112354: learning rate 0.0001
[2019-04-10 12:19:58,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8167002e-02 3.8611679e-12 1.4726444e-10 1.1272361e-08 9.7183299e-01], sum to 1.0000
[2019-04-10 12:19:58,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5197
[2019-04-10 12:19:58,958] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.83333333333334, 61.83333333333333, 1.0, 2.0, 0.1997435983867646, 1.0, 2.0, 0.1997435983867646, 1.0, 2.0, 0.3468885741804255, 6.911200000000001, 6.9112, 170.5573041426782, 837383.4087539273, 837383.4087539267, 269773.0887568645], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4216200.0000, 
sim time next is 4216800.0000, 
raw observation next is [33.66666666666667, 63.66666666666667, 1.0, 2.0, 0.2007849239655482, 1.0, 2.0, 0.2007849239655482, 1.0, 2.0, 0.3486970123391413, 6.9112, 6.9112, 170.5573041426782, 841750.662410756, 841750.662410756, 270061.8277117345], 
processed observation next is [1.0, 0.8260869565217391, 0.7946287519747238, 0.6366666666666667, 1.0, 1.0, 0.037090269838009865, 1.0, 1.0, 0.037090269838009865, 1.0, 1.0, 0.2057280638282211, 0.0, 0.0, 0.8375144448122397, 0.23381962844743223, 0.23381962844743223, 0.4030773547936336], 
reward next is 0.5969, 
noisyNet noise sample is [array([-1.6853173], dtype=float32), 1.8053119]. 
=============================================
[2019-04-10 12:19:59,733] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2990263e-03 7.3303880e-10 1.0966094e-07 2.6171492e-07 9.9670058e-01], sum to 1.0000
[2019-04-10 12:19:59,738] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3169
[2019-04-10 12:19:59,742] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.3604194280653801, 1.0, 2.0, 0.3604194280653801, 1.0, 2.0, 0.6259293540233355, 6.9112, 6.9112, 170.5573041426782, 1511458.145438461, 1511458.145438461, 332060.3345725171], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.3406586780760691, 1.0, 2.0, 0.3406586780760691, 1.0, 2.0, 0.591611466271784, 6.911199999999999, 6.9112, 170.5573041426782, 1428534.064080506, 1428534.064080507, 322472.294817715], 
processed observation next is [1.0, 0.17391304347826086, 0.6208530805687204, 0.75, 1.0, 1.0, 0.20561286515189045, 1.0, 1.0, 0.20561286515189045, 1.0, 1.0, 0.5019652027704682, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.39681501780014056, 0.39681501780014083, 0.48130193256375375], 
reward next is 0.5187, 
noisyNet noise sample is [array([-0.4812406], dtype=float32), 0.5052927]. 
=============================================
[2019-04-10 12:19:59,756] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[21.76251 ]
 [21.32691 ]
 [20.917341]
 [20.50559 ]
 [19.90114 ]], R is [[22.91509247]
 [23.1903286 ]
 [23.46202087]
 [23.73697472]
 [24.00646782]].
[2019-04-10 12:20:00,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8763900e-02 1.9640012e-08 1.1103715e-07 1.4016915e-06 9.8123461e-01], sum to 1.0000
[2019-04-10 12:20:00,410] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2555
[2019-04-10 12:20:00,412] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 76.33333333333334, 1.0, 2.0, 0.4050936947738857, 1.0, 2.0, 0.4050936947738857, 1.0, 2.0, 0.7035137812902491, 6.9112, 6.9112, 170.5573041426782, 1698953.048568889, 1698953.048568889, 355726.3463770226], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4243200.0000, 
sim time next is 4243800.0000, 
raw observation next is [30.0, 75.66666666666666, 1.0, 2.0, 0.3795396647885076, 1.0, 2.0, 0.3795396647885076, 1.0, 2.0, 0.6591348820525, 6.9112, 6.9112, 170.5573041426782, 1591700.482280023, 1591700.482280023, 341851.6317991028], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.7566666666666666, 1.0, 1.0, 0.2524574274560332, 1.0, 1.0, 0.2524574274560332, 1.0, 1.0, 0.5843108317713415, 0.0, 0.0, 0.8375144448122397, 0.44213902285556195, 0.44213902285556195, 0.5102263161180639], 
reward next is 0.4898, 
noisyNet noise sample is [array([1.4626153], dtype=float32), 0.2859055]. 
=============================================
[2019-04-10 12:20:00,884] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9524657e-01 9.4845204e-13 4.8606341e-11 9.3438243e-12 7.0475340e-01], sum to 1.0000
[2019-04-10 12:20:00,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0184
[2019-04-10 12:20:00,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1476915.162509306 W.
[2019-04-10 12:20:00,900] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.3521880454220662, 1.0, 2.0, 0.3521880454220662, 1.0, 2.0, 0.6116341645317364, 6.9112, 6.9112, 170.5573041426782, 1476915.162509306, 1476915.162509306, 328001.5159808996], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4258800.0000, 
sim time next is 4259400.0000, 
raw observation next is [30.16666666666666, 79.83333333333334, 1.0, 2.0, 0.6530935152064218, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005972311479645, 6.9112, 168.9123932577426, 1809492.05118132, 1742257.535805621, 374813.2304189418], 
processed observation next is [1.0, 0.30434782608695654, 0.6287519747235385, 0.7983333333333335, 1.0, 1.0, 0.5820403797667733, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00947723114796446, 0.0, 0.8294371793237444, 0.5026366808837001, 0.48396042661267247, 0.5594227319685698], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3170972], dtype=float32), 0.16965774]. 
=============================================
[2019-04-10 12:20:04,462] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0083809e-03 6.7780792e-10 3.0920777e-09 7.9830652e-06 9.9498367e-01], sum to 1.0000
[2019-04-10 12:20:04,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0092
[2019-04-10 12:20:04,468] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 63.66666666666667, 1.0, 2.0, 0.7850095788441774, 1.0, 2.0, 0.7130948289363513, 1.0, 2.0, 1.03, 7.005104436039825, 6.9112, 170.5573041426782, 2992351.271668314, 2925083.704344752, 549525.5441932543], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4358400.0000, 
sim time next is 4359000.0000, 
raw observation next is [35.66666666666666, 61.83333333333333, 1.0, 2.0, 0.8087282973138487, 1.0, 2.0, 0.724954188171187, 1.0, 2.0, 1.03, 7.005106306847746, 6.9112, 170.5573041426782, 3042177.164945466, 2974908.25748609, 557871.9423735642], 
processed observation next is [1.0, 0.43478260869565216, 0.889415481832543, 0.6183333333333333, 1.0, 1.0, 0.7695521654383719, 1.0, 1.0, 0.6686195038207071, 1.0, 1.0, 1.0365853658536586, 0.009390630684774592, 0.0, 0.8375144448122397, 0.8450492124848517, 0.8263634048572472, 0.8326446901097974], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2229655], dtype=float32), 0.9567316]. 
=============================================
[2019-04-10 12:20:04,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[13.79498 ]
 [13.383247]
 [13.122774]
 [12.674161]
 [12.394315]], R is [[13.87533855]
 [13.73658562]
 [13.59922028]
 [13.46322823]
 [13.32859612]].
[2019-04-10 12:20:04,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0434346e-03 1.4388138e-09 1.3547901e-08 2.1478929e-05 9.9893504e-01], sum to 1.0000
[2019-04-10 12:20:04,953] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0469
[2019-04-10 12:20:04,958] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.33333333333334, 73.66666666666667, 1.0, 2.0, 0.7387806796334383, 1.0, 2.0, 0.6899803793309817, 1.0, 2.0, 1.03, 7.00510079017575, 6.9112, 170.5573041426782, 2895243.97126072, 2827979.015617899, 533859.9639714885], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4353600.0000, 
sim time next is 4354200.0000, 
raw observation next is [33.5, 73.0, 1.0, 2.0, 0.7075896724032685, 1.0, 2.0, 0.6743848757158968, 1.0, 2.0, 1.03, 7.005098330602212, 6.9112, 170.5573041426782, 2829729.165531729, 2762465.971781482, 523739.4587895692], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.73, 1.0, 1.0, 0.6476984004858657, 1.0, 1.0, 0.6076926213444539, 1.0, 1.0, 1.0365853658536586, 0.009389833060221165, 0.0, 0.8375144448122397, 0.7860358793143691, 0.7673516588281895, 0.7817006847605511], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0057015], dtype=float32), 0.98206127]. 
=============================================
[2019-04-10 12:20:05,936] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5400575e-02 3.8342305e-11 8.7116347e-10 3.0781305e-07 9.5459920e-01], sum to 1.0000
[2019-04-10 12:20:05,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5077
[2019-04-10 12:20:05,950] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.1860177783379762, 1.0, 2.0, 0.1860177783379762, 1.0, 2.0, 0.3230513639537331, 6.911199999999999, 6.9112, 170.5573041426782, 779819.8490807762, 779819.8490807769, 266110.0546514487], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.1862961656419402, 1.0, 2.0, 0.1862961656419402, 1.0, 2.0, 0.3235348306366302, 6.9112, 6.9112, 170.5573041426782, 780987.3234330616, 780987.3234330616, 266181.6014675603], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.019633934508361685, 1.0, 1.0, 0.019633934508361685, 1.0, 1.0, 0.17504247638613438, 0.0, 0.0, 0.8375144448122397, 0.21694092317585045, 0.21694092317585045, 0.3972859723396422], 
reward next is 0.6027, 
noisyNet noise sample is [array([-0.2597132], dtype=float32), -0.7067775]. 
=============================================
[2019-04-10 12:20:08,086] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 7.9046782e-16 1.4778655e-14 4.1785111e-14 9.3022708e-12], sum to 1.0000
[2019-04-10 12:20:08,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7927
[2019-04-10 12:20:08,098] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.924713578983991, 6.9112, 168.912701847377, 838391.6407423094, 828804.6571634912, 254811.930342767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4422600.0000, 
sim time next is 4423200.0000, 
raw observation next is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.946388069068156, 6.9112, 168.9125521620476, 853774.2243628836, 828810.6568446655, 254811.9297633494], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00351880690681563, 0.0, 0.8294379596166287, 0.23715950676746766, 0.23022518245685153, 0.380316313079626], 
reward next is 0.4437, 
noisyNet noise sample is [array([-0.21039683], dtype=float32), 0.76550376]. 
=============================================
[2019-04-10 12:20:09,601] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.4932963e-22 4.4503456e-21 3.0496090e-22 3.5115587e-18], sum to 1.0000
[2019-04-10 12:20:09,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8808
[2019-04-10 12:20:09,609] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666667, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9202038166197339, 6.9112, 6.9112, 168.912956510431, 755858.8442419033, 755858.8442419033, 228028.8854125719], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4461600.0000, 
sim time next is 4462200.0000, 
raw observation next is [31.0, 68.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9476024001867306, 6.9112, 6.9112, 168.912956510431, 773484.0417379951, 773484.0417379951, 234418.1964691754], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9361004880325982, 0.0, 0.0, 0.8294399451523027, 0.21485667826055418, 0.21485667826055418, 0.3498779051778737], 
reward next is 0.6501, 
noisyNet noise sample is [array([-0.21567404], dtype=float32), -0.30590862]. 
=============================================
[2019-04-10 12:20:10,261] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119547: loss 0.1370
[2019-04-10 12:20:10,263] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119547: learning rate 0.0001
[2019-04-10 12:20:10,410] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119633: loss 0.0541
[2019-04-10 12:20:10,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119634: learning rate 0.0001
[2019-04-10 12:20:10,450] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119658: loss 0.0683
[2019-04-10 12:20:10,451] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119658: learning rate 0.0001
[2019-04-10 12:20:10,546] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119716: loss 0.0245
[2019-04-10 12:20:10,551] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119717: learning rate 0.0001
[2019-04-10 12:20:10,643] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119770: loss 0.0385
[2019-04-10 12:20:10,647] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119771: learning rate 0.0001
[2019-04-10 12:20:10,985] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119981: loss 0.2809
[2019-04-10 12:20:10,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119982: learning rate 0.0001
[2019-04-10 12:20:11,013] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119995: loss 0.3032
[2019-04-10 12:20:11,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119996: learning rate 0.0001
[2019-04-10 12:20:11,021] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120000: loss 0.2716
[2019-04-10 12:20:11,022] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120000: learning rate 0.0001
[2019-04-10 12:20:11,120] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120061: loss 0.2521
[2019-04-10 12:20:11,121] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120062: learning rate 0.0001
[2019-04-10 12:20:11,150] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120078: loss 0.1766
[2019-04-10 12:20:11,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120080: learning rate 0.0001
[2019-04-10 12:20:11,195] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120104: loss 0.1630
[2019-04-10 12:20:11,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120106: learning rate 0.0001
[2019-04-10 12:20:11,260] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120145: loss 0.2077
[2019-04-10 12:20:11,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120145: learning rate 0.0001
[2019-04-10 12:20:11,276] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120153: loss 0.2031
[2019-04-10 12:20:11,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120155: learning rate 0.0001
[2019-04-10 12:20:11,457] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120262: loss 0.2534
[2019-04-10 12:20:11,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120262: learning rate 0.0001
[2019-04-10 12:20:11,492] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120283: loss 0.2803
[2019-04-10 12:20:11,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120284: learning rate 0.0001
[2019-04-10 12:20:11,772] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120453: loss 0.2633
[2019-04-10 12:20:11,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120453: learning rate 0.0001
[2019-04-10 12:20:19,199] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 12:20:19,201] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:20:19,201] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:20:19,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:19,202] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:19,202] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:20:19,203] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:20:19,203] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:19,204] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:19,204] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:20:19,206] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:20:19,222] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-04-10 12:20:19,222] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-04-10 12:20:19,222] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-04-10 12:20:19,223] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-04-10 12:20:19,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-04-10 12:20:23,426] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:20:23,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.93333333333333, 69.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4908989493153136, 6.9112, 6.9112, 168.912956510431, 438524.6002043821, 438524.6002043821, 150664.7701118436]
[2019-04-10 12:20:23,427] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:20:23,429] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.1892720e-23 1.1815921e-22 2.5739841e-20 9.7304325e-20], sampled 0.00016873590774502123
[2019-04-10 12:20:24,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:20:24,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.5, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4337553667279662, 6.9112, 6.9112, 168.912956510431, 389871.4067562209, 389871.4067562209, 143978.0905924572]
[2019-04-10 12:20:24,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:20:24,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.1584784e-26 9.3464351e-26 3.4891971e-23 1.3292347e-22], sampled 0.3169578895806414
[2019-04-10 12:20:41,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:20:41,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.06666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7634768043130746, 6.911199999999999, 6.9112, 168.912956510431, 646939.6954289344, 646939.695428935, 194610.5394351037]
[2019-04-10 12:20:41,300] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:20:41,302] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.6397874e-25 7.0591011e-25 4.6329238e-22 1.3520026e-21], sampled 0.4670074662697483
[2019-04-10 12:20:52,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:20:52,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 100.0, 1.0, 2.0, 0.6210489193874738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956507057, 935167.4875272322, 935167.4875272315, 213147.1987681837]
[2019-04-10 12:20:52,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:20:52,015] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.1055252e-19 3.5478728e-18 1.8918985e-15 1.3019152e-14], sampled 0.7245640363474115
[2019-04-10 12:20:52,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 935167.4875272322 W.
[2019-04-10 12:21:04,330] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:04,333] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.7, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9289881237676787, 6.911200000000001, 6.9112, 168.912956510431, 758981.1874363091, 758981.1874363085, 229946.085132974]
[2019-04-10 12:21:04,335] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:21:04,337] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.2193956e-23 2.8939167e-22 1.6386606e-19 4.6667741e-19], sampled 0.8012173114008008
[2019-04-10 12:21:08,811] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:08,812] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.83333333333334, 70.0, 1.0, 2.0, 0.3883785465797757, 1.0, 2.0, 0.3883785465797757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1085589.649171695, 1085589.649171695, 269245.447160802]
[2019-04-10 12:21:08,815] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:21:08,819] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.9915152e-17 2.6585042e-16 2.2226252e-11 8.3513596e-09], sampled 0.8876043677299044
[2019-04-10 12:21:08,821] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1085589.649171695 W.
[2019-04-10 12:21:12,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:12,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8697083488618673, 6.9112, 6.9112, 168.912956510431, 719738.71351649, 719738.71351649, 216579.0932432204]
[2019-04-10 12:21:12,878] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:21:12,879] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.2381083e-23 6.2687848e-23 3.1737692e-20 9.1926488e-20], sampled 0.6206361822081932
[2019-04-10 12:21:13,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:13,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.96962588333333, 68.50653393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7632535424078968, 6.911199999999999, 6.9112, 168.912956510431, 647929.935568798, 647929.9355687986, 194580.5366902728]
[2019-04-10 12:21:13,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:21:13,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.1435989e-26 3.7276815e-26 1.6703276e-22 6.1395704e-22], sampled 0.08390630655002584
[2019-04-10 12:21:22,901] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:22,901] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.75, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8943513994986035, 6.911199999999999, 6.9112, 168.912956510431, 733954.1797674078, 733954.1797674084, 221951.794797739]
[2019-04-10 12:21:22,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:21:22,904] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.1230681e-26 1.7942477e-25 1.8746653e-21 1.7705366e-20], sampled 0.7868092322744914
[2019-04-10 12:21:31,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.03387724], dtype=float32), 0.0008338663]
[2019-04-10 12:21:31,543] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.70759234, 81.37075737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7841384077355201, 6.9112, 6.9112, 168.912956510431, 662885.0995151849, 662885.0995151849, 198704.6509570553]
[2019-04-10 12:21:31,544] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:21:31,546] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.3619864e-26 7.7784674e-26 6.2535225e-23 1.6986585e-22], sampled 0.6316899468089356
[2019-04-10 12:21:40,048] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.0949 3105611505.3693 2010.0000
[2019-04-10 12:21:40,388] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7027.7038 3185324772.0070 2464.0000
[2019-04-10 12:21:40,873] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8961 2989329913.4592 1566.0000
[2019-04-10 12:21:40,922] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.6301 2937834777.0231 1381.0000
[2019-04-10 12:21:40,948] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.7082 3319658808.3144 2143.0000
[2019-04-10 12:21:41,961] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 125000, evaluation results [125000.0, 7286.708153391335, 3319658808.3144107, 2143.0, 7349.094853641485, 3105611505.369342, 2010.0, 8059.630121208328, 2937834777.0230637, 1381.0, 7027.703839004308, 3185324772.0069947, 2464.0, 7923.896070981788, 2989329913.459176, 1566.0]
[2019-04-10 12:21:42,768] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7830698e-01 4.8986433e-09 3.0697187e-08 6.7316671e-04 8.2101977e-01], sum to 1.0000
[2019-04-10 12:21:42,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8223
[2019-04-10 12:21:42,777] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.5004898068022139, 1.0, 2.0, 0.5004898068022139, 1.0, 2.0, 0.8691852799071261, 6.9112, 6.9112, 170.5573041426782, 2099433.995571846, 2099433.995571846, 415484.7376724026], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.4893219708323802, 1.0, 2.0, 0.4893219708323802, 1.0, 2.0, 0.8497904420873166, 6.911200000000001, 6.9112, 170.5573041426782, 2052542.738031986, 2052542.738031986, 407840.264523695], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.75, 1.0, 1.0, 0.38472526606310875, 1.0, 1.0, 0.38472526606310875, 1.0, 1.0, 0.8168176123016055, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5701507605644406, 0.5701507605644406, 0.6087168127219328], 
reward next is 0.3913, 
noisyNet noise sample is [array([-0.78515726], dtype=float32), -0.6152129]. 
=============================================
[2019-04-10 12:21:42,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[13.179154]
 [12.442084]
 [12.522253]
 [12.510209]
 [12.761821]], R is [[14.23478889]
 [14.47231483]
 [14.3275919 ]
 [14.5654726 ]
 [14.74529934]].
[2019-04-10 12:21:45,975] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.6805093e-01 1.7946368e-10 4.2238009e-09 3.2932134e-04 1.3161986e-01], sum to 1.0000
[2019-04-10 12:21:45,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2678
[2019-04-10 12:21:45,986] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2091784.527814167 W.
[2019-04-10 12:21:45,991] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.4986680080075099, 1.0, 2.0, 0.4986680080075099, 1.0, 2.0, 0.8625670856329066, 6.9112, 6.9112, 170.5573041426782, 2091784.527814167, 2091784.527814167, 413604.6879403972], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4786800.0000, 
sim time next is 4787400.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.7499409193553183, 1.0, 2.0, 0.7499409193553183, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2097211.981849927, 2097211.981849927, 396235.2119038652], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.698723999223275, 1.0, 1.0, 0.698723999223275, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5825588838472019, 0.5825588838472019, 0.5913958386624854], 
reward next is 0.4086, 
noisyNet noise sample is [array([2.1413136], dtype=float32), -0.217232]. 
=============================================
[2019-04-10 12:21:46,375] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127578: loss -175.3764
[2019-04-10 12:21:46,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127578: learning rate 0.0001
[2019-04-10 12:21:46,578] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127694: loss -212.3330
[2019-04-10 12:21:46,579] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127694: learning rate 0.0001
[2019-04-10 12:21:46,587] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127698: loss -114.8616
[2019-04-10 12:21:46,591] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127699: learning rate 0.0001
[2019-04-10 12:21:46,598] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127704: loss -82.2050
[2019-04-10 12:21:46,600] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127704: learning rate 0.0001
[2019-04-10 12:21:46,707] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127766: loss -170.1973
[2019-04-10 12:21:46,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127767: learning rate 0.0001
[2019-04-10 12:21:46,898] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127877: loss -260.0967
[2019-04-10 12:21:46,899] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127877: learning rate 0.0001
[2019-04-10 12:21:47,034] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127955: loss -67.9922
[2019-04-10 12:21:47,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127955: learning rate 0.0001
[2019-04-10 12:21:47,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127966: loss -89.2206
[2019-04-10 12:21:47,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127967: learning rate 0.0001
[2019-04-10 12:21:47,190] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128045: loss -339.1274
[2019-04-10 12:21:47,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128046: learning rate 0.0001
[2019-04-10 12:21:47,243] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128076: loss -318.1058
[2019-04-10 12:21:47,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128076: learning rate 0.0001
[2019-04-10 12:21:47,334] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128129: loss -131.9056
[2019-04-10 12:21:47,336] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128129: learning rate 0.0001
[2019-04-10 12:21:47,384] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128158: loss -286.1542
[2019-04-10 12:21:47,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128158: learning rate 0.0001
[2019-04-10 12:21:47,572] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128265: loss -232.2607
[2019-04-10 12:21:47,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128265: learning rate 0.0001
[2019-04-10 12:21:47,614] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128286: loss -95.6350
[2019-04-10 12:21:47,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128286: learning rate 0.0001
[2019-04-10 12:21:47,647] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128306: loss -298.5086
[2019-04-10 12:21:47,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128309: learning rate 0.0001
[2019-04-10 12:21:47,712] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128344: loss -322.7164
[2019-04-10 12:21:47,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128344: learning rate 0.0001
[2019-04-10 12:22:05,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 7.6433935e-27 1.6487686e-26 7.4395680e-23 3.8164294e-19], sum to 1.0000
[2019-04-10 12:22:05,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2209
[2019-04-10 12:22:05,602] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.16666666666666, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8524847306382284, 6.911200000000001, 6.9112, 168.912956510431, 706172.1259651266, 706172.1259651259, 212779.0896019528], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5047800.0000, 
sim time next is 5048400.0000, 
raw observation next is [30.33333333333334, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8577218483342937, 6.9112, 6.9112, 168.912956510431, 709948.3427707345, 709948.3427707345, 213914.4487818885], 
processed observation next is [0.0, 0.43478260869565216, 0.6366508688783573, 0.65, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8264900589442605, 0.0, 0.0, 0.8294399451523027, 0.19720787299187068, 0.19720787299187068, 0.3192752966893858], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.51720124], dtype=float32), -1.0158598]. 
=============================================
[2019-04-10 12:22:07,736] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135578: loss 13.8524
[2019-04-10 12:22:07,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135578: learning rate 0.0001
[2019-04-10 12:22:07,794] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135595: loss 14.4818
[2019-04-10 12:22:07,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135595: learning rate 0.0001
[2019-04-10 12:22:07,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 6.5411201e-31 6.2368744e-29 3.4209887e-25 4.1375421e-21], sum to 1.0000
[2019-04-10 12:22:07,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7090
[2019-04-10 12:22:07,881] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.881308949503709, 6.911200000000001, 6.9112, 168.912956510431, 728349.6647396589, 728349.6647396584, 219164.1039075704], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5088000.0000, 
sim time next is 5088600.0000, 
raw observation next is [27.5, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8786775523064634, 6.911199999999999, 6.9112, 168.912956510431, 726538.1833107037, 726538.1833107043, 218579.6936266805], 
processed observation next is [0.0, 0.9130434782608695, 0.5023696682464456, 0.815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.852045795495687, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20181616203075103, 0.2018161620307512, 0.32623834869653806], 
reward next is 0.6738, 
noisyNet noise sample is [array([-0.07975535], dtype=float32), -1.8937278]. 
=============================================
[2019-04-10 12:22:08,025] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 135680: loss 12.2147
[2019-04-10 12:22:08,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 135680: learning rate 0.0001
[2019-04-10 12:22:08,171] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135729: loss 11.7894
[2019-04-10 12:22:08,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135729: learning rate 0.0001
[2019-04-10 12:22:08,176] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135732: loss 12.6195
[2019-04-10 12:22:08,198] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135732: learning rate 0.0001
[2019-04-10 12:22:08,449] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135823: loss 10.4719
[2019-04-10 12:22:08,452] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135824: learning rate 0.0001
[2019-04-10 12:22:08,650] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135918: loss 11.2200
[2019-04-10 12:22:08,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135919: learning rate 0.0001
[2019-04-10 12:22:08,684] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135935: loss 12.7495
[2019-04-10 12:22:08,685] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135935: learning rate 0.0001
[2019-04-10 12:22:08,716] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135945: loss 11.1486
[2019-04-10 12:22:08,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135945: learning rate 0.0001
[2019-04-10 12:22:09,217] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136141: loss 10.4732
[2019-04-10 12:22:09,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136141: learning rate 0.0001
[2019-04-10 12:22:09,242] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136151: loss 11.1214
[2019-04-10 12:22:09,255] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136151: learning rate 0.0001
[2019-04-10 12:22:09,294] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136172: loss 9.9698
[2019-04-10 12:22:09,297] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136172: learning rate 0.0001
[2019-04-10 12:22:09,381] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136196: loss 10.3790
[2019-04-10 12:22:09,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136196: learning rate 0.0001
[2019-04-10 12:22:09,714] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136328: loss 11.5167
[2019-04-10 12:22:09,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136329: learning rate 0.0001
[2019-04-10 12:22:09,857] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136370: loss 10.6558
[2019-04-10 12:22:09,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136370: learning rate 0.0001
[2019-04-10 12:22:10,227] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136525: loss 11.6161
[2019-04-10 12:22:10,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136525: learning rate 0.0001
[2019-04-10 12:22:22,623] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3418012e-10 1.4976423e-16 1.1036358e-15 1.2861323e-09 1.0000000e+00], sum to 1.0000
[2019-04-10 12:22:22,631] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3358
[2019-04-10 12:22:22,636] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.4, 56.33333333333334, 1.0, 2.0, 0.1948491081350609, 1.0, 2.0, 0.1948491081350609, 1.0, 2.0, 0.3383884632458765, 6.911199999999999, 6.9112, 170.5573041426782, 816856.4642462756, 816856.4642462762, 268437.5852663728], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5332800.0000, 
sim time next is 5333400.0000, 
raw observation next is [35.15, 57.5, 1.0, 2.0, 0.1917997721827258, 1.0, 2.0, 0.1917997721827258, 1.0, 2.0, 0.3330927751274797, 6.9112, 6.9112, 170.5573041426782, 804068.0890560624, 804068.0890560624, 267622.0982506046], 
processed observation next is [1.0, 0.7391304347826086, 0.8649289099526066, 0.575, 1.0, 1.0, 0.02626478576232021, 1.0, 1.0, 0.02626478576232021, 1.0, 1.0, 0.18669850625302403, 0.0, 0.0, 0.8375144448122397, 0.22335224696001732, 0.22335224696001732, 0.3994359675382158], 
reward next is 0.6006, 
noisyNet noise sample is [array([2.0139098], dtype=float32), -0.6613691]. 
=============================================
[2019-04-10 12:22:23,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9546844e-02 1.9264636e-14 3.6696267e-13 1.4860871e-08 9.1045308e-01], sum to 1.0000
[2019-04-10 12:22:23,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-10 12:22:23,782] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.41666666666666, 78.16666666666667, 1.0, 2.0, 0.2102591450990506, 1.0, 2.0, 0.2102591450990506, 1.0, 2.0, 0.3651506012752285, 6.911200000000001, 6.9112, 170.5573041426782, 881485.7639876582, 881485.7639876575, 272756.3857457343], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5343000.0000, 
sim time next is 5343600.0000, 
raw observation next is [31.33333333333334, 78.33333333333334, 1.0, 2.0, 0.2099101772206287, 1.0, 2.0, 0.2099101772206287, 1.0, 2.0, 0.3645445594758505, 6.9112, 6.9112, 170.5573041426782, 880022.1585555411, 880022.1585555411, 272654.8870587913], 
processed observation next is [1.0, 0.8695652173913043, 0.6840442338072673, 0.7833333333333334, 1.0, 1.0, 0.048084550868227355, 1.0, 1.0, 0.048084550868227355, 1.0, 1.0, 0.22505434082420794, 0.0, 0.0, 0.8375144448122397, 0.24445059959876142, 0.24445059959876142, 0.40694759262506164], 
reward next is 0.5931, 
noisyNet noise sample is [array([0.3108272], dtype=float32), 0.60695726]. 
=============================================
[2019-04-10 12:22:24,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0998645e-03 1.9718409e-13 2.5455259e-12 8.9120302e-09 9.9590009e-01], sum to 1.0000
[2019-04-10 12:22:24,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-10 12:22:24,796] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.05, 88.5, 1.0, 2.0, 0.3771680643743978, 1.0, 2.0, 0.3771680643743978, 1.0, 2.0, 0.655016196433433, 6.9112, 6.9112, 170.5573041426782, 1581747.205941369, 1581747.205941369, 340610.9132824632], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5367000.0000, 
sim time next is 5367600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.425736981944347, 1.0, 2.0, 0.425736981944347, 1.0, 2.0, 0.739364344265953, 6.911200000000001, 6.9112, 170.5573041426782, 1785602.6237566, 1785602.623756599, 367594.2553995744], 
processed observation next is [1.0, 0.13043478260869565, 0.5734597156398105, 0.89, 1.0, 1.0, 0.3081168457160808, 1.0, 1.0, 0.3081168457160808, 1.0, 1.0, 0.6821516393487231, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4960007288212778, 0.4960007288212775, 0.5486481423874245], 
reward next is 0.4514, 
noisyNet noise sample is [array([-0.7408776], dtype=float32), 0.7139495]. 
=============================================
[2019-04-10 12:22:25,358] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143608: loss 1.6468
[2019-04-10 12:22:25,361] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143610: learning rate 0.0001
[2019-04-10 12:22:25,401] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143635: loss 1.8936
[2019-04-10 12:22:25,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143635: learning rate 0.0001
[2019-04-10 12:22:25,404] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 143635: loss 1.4602
[2019-04-10 12:22:25,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 143635: learning rate 0.0001
[2019-04-10 12:22:25,509] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143708: loss 0.9420
[2019-04-10 12:22:25,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143708: learning rate 0.0001
[2019-04-10 12:22:25,607] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143768: loss 1.9970
[2019-04-10 12:22:25,608] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143768: learning rate 0.0001
[2019-04-10 12:22:25,616] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143774: loss 1.0656
[2019-04-10 12:22:25,617] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143774: learning rate 0.0001
[2019-04-10 12:22:25,704] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143829: loss 1.1674
[2019-04-10 12:22:25,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143829: learning rate 0.0001
[2019-04-10 12:22:25,845] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143923: loss 0.4023
[2019-04-10 12:22:25,846] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143923: learning rate 0.0001
[2019-04-10 12:22:25,851] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 143927: loss 0.6480
[2019-04-10 12:22:25,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 143927: learning rate 0.0001
[2019-04-10 12:22:26,042] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144051: loss 1.3559
[2019-04-10 12:22:26,045] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144052: learning rate 0.0001
[2019-04-10 12:22:26,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144105: loss 1.0380
[2019-04-10 12:22:26,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144105: learning rate 0.0001
[2019-04-10 12:22:26,134] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144116: loss 0.1786
[2019-04-10 12:22:26,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144116: learning rate 0.0001
[2019-04-10 12:22:26,252] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144192: loss 0.4414
[2019-04-10 12:22:26,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144193: learning rate 0.0001
[2019-04-10 12:22:26,538] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144383: loss 0.6982
[2019-04-10 12:22:26,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144383: learning rate 0.0001
[2019-04-10 12:22:26,616] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144440: loss 0.3148
[2019-04-10 12:22:26,620] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144441: learning rate 0.0001
[2019-04-10 12:22:27,031] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144701: loss 0.5335
[2019-04-10 12:22:27,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144701: learning rate 0.0001
[2019-04-10 12:22:30,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0152389e-04 2.3265061e-10 6.0368086e-11 1.1217173e-05 9.9978727e-01], sum to 1.0000
[2019-04-10 12:22:30,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5150
[2019-04-10 12:22:30,199] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.13333333333333, 65.0, 1.0, 2.0, 0.1851324325650307, 1.0, 2.0, 0.1851324325650307, 1.0, 2.0, 0.3215138111344479, 6.911200000000001, 6.9112, 170.5573041426782, 776106.978139926, 776106.9781399254, 265882.5410130631], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5510400.0000, 
sim time next is 5511000.0000, 
raw observation next is [31.86666666666667, 66.5, 1.0, 2.0, 0.1864119635585564, 1.0, 2.0, 0.1864119635585564, 1.0, 2.0, 0.3237359333228366, 6.9112, 6.9112, 170.5573041426782, 781472.9461606705, 781472.9461606705, 266211.3460635582], 
processed observation next is [1.0, 0.782608695652174, 0.7093206951026858, 0.665, 1.0, 1.0, 0.019773450070549876, 1.0, 1.0, 0.019773450070549876, 1.0, 1.0, 0.1752877235644349, 0.0, 0.0, 0.8375144448122397, 0.21707581837796402, 0.21707581837796402, 0.3973303672590421], 
reward next is 0.6027, 
noisyNet noise sample is [array([-1.535082], dtype=float32), 1.9529496]. 
=============================================
[2019-04-10 12:22:30,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[8.830599 ]
 [8.672978 ]
 [8.366636 ]
 [8.073976 ]
 [7.7812576]], R is [[ 9.47742939]
 [ 9.98581505]
 [10.48939037]
 [10.98816967]
 [11.48201752]].
[2019-04-10 12:22:30,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.6070534e-17 1.7236432e-17 5.3761758e-13 1.9082782e-09], sum to 1.0000
[2019-04-10 12:22:30,651] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7624
[2019-04-10 12:22:30,654] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.83333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.945291411300542, 6.9112, 168.9125555715131, 852995.9163908745, 828810.3532864552, 254811.9736350152], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5516400.0000, 
sim time next is 5517000.0000, 
raw observation next is [29.7, 79.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.957651604027112, 6.9112, 168.9124612868671, 861768.0447664171, 828813.7747452587, 254811.9148177416], 
processed observation next is [1.0, 0.8695652173913043, 0.6066350710900474, 0.795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004645160402711212, 0.0, 0.8294375133778896, 0.23938001243511586, 0.23022604854034964, 0.38031629077274864], 
reward next is 0.3874, 
noisyNet noise sample is [array([0.32549167], dtype=float32), -0.21146807]. 
=============================================
[2019-04-10 12:22:30,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[27.18666 ]
 [26.981258]
 [27.037546]
 [27.235386]
 [27.20868 ]], R is [[27.38160133]
 [27.55701256]
 [27.82665062]
 [28.17292786]
 [28.5221405 ]].
[2019-04-10 12:22:34,996] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.6581082e-28 3.4342268e-29 2.3293546e-25 1.5776332e-21], sum to 1.0000
[2019-04-10 12:22:35,004] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-10 12:22:35,009] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 91.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8833966453732549, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 219626.9823085113], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5618400.0000, 
sim time next is 5619000.0000, 
raw observation next is [26.05, 91.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8805771788224406, 6.911199999999999, 6.9112, 168.912956510431, 727694.291853428, 727694.2918534287, 218996.0109904025], 
processed observation next is [0.0, 0.0, 0.43364928909952616, 0.9183333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8543624131980982, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2021373032926189, 0.20213730329261909, 0.3268597178961231], 
reward next is 0.6731, 
noisyNet noise sample is [array([-0.00936221], dtype=float32), 1.3276794]. 
=============================================
[2019-04-10 12:22:35,023] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[65.46196]
 [65.01845]
 [64.7964 ]
 [65.06354]
 [65.74183]], R is [[65.53990936]
 [65.55670929]
 [65.57319641]
 [65.58911896]
 [65.60391235]].
[2019-04-10 12:22:35,720] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 12:22:35,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:22:35,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:35,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:22:35,724] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:22:35,725] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:35,725] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:22:35,725] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:35,726] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:22:35,727] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:35,729] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:22:35,743] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-04-10 12:22:35,743] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-04-10 12:22:35,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-04-10 12:22:35,757] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-04-10 12:22:35,820] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-04-10 12:22:51,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:22:51,656] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.3, 49.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5431627245458043, 6.9112, 6.9112, 168.912956510431, 479155.6067026513, 479155.6067026513, 157699.0548725621]
[2019-04-10 12:22:51,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:22:51,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00000000e+00 1.27017995e-30 7.03488230e-32 2.50306329e-29
 5.14105675e-26], sampled 0.031383389093312286
[2019-04-10 12:22:54,960] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:22:54,961] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.79354437333333, 88.75218665333333, 1.0, 2.0, 0.8119701202856826, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103291, 1193626.684864209, 1193626.684864209, 255014.2310100097]
[2019-04-10 12:22:54,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:22:54,965] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.7732872e-23 1.7212134e-23 2.3439969e-20 1.2398662e-16], sampled 0.7623946396362381
[2019-04-10 12:22:54,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1193626.684864209 W.
[2019-04-10 12:23:07,281] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:23:07,282] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5636059239149904, 6.911200000000001, 6.9112, 168.912956510431, 495273.0358376002, 495273.0358375995, 160610.3272704468]
[2019-04-10 12:23:07,283] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:23:07,286] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.6150439e-30 1.0616349e-31 1.4828753e-28 5.0927297e-25], sampled 0.2109995484305387
[2019-04-10 12:23:17,006] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:23:17,009] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.954103745, 94.88219723333333, 1.0, 2.0, 0.6347887732968941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564841279, 887097.3355526391, 887097.3355526391, 207406.9315726784]
[2019-04-10 12:23:17,009] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:23:17,011] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.2319638e-25 4.1332722e-26 3.1723603e-23 1.4950655e-19], sampled 0.6485877206475851
[2019-04-10 12:23:17,012] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 887097.3355526391 W.
[2019-04-10 12:23:30,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:23:30,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.5, 77.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9767532746566998, 6.911199999999999, 6.9112, 168.912956510431, 792087.9826247962, 792087.9826247968, 241396.3407639142]
[2019-04-10 12:23:30,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:23:30,892] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.4031682e-29 4.5616523e-30 7.6078177e-27 1.4294259e-23], sampled 0.5416461811844149
[2019-04-10 12:23:31,315] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.04122781], dtype=float32), 0.009827003]
[2019-04-10 12:23:31,316] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.86721855, 82.2047961, 1.0, 2.0, 0.7225995792470148, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005977074341009, 6.9112, 168.9116398884979, 1906754.085035426, 1839516.490622176, 389704.9617061394]
[2019-04-10 12:23:31,318] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:23:31,320] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.8387366e-20 6.8372068e-21 2.4575505e-17 1.9581919e-12], sampled 0.04279010610924627
[2019-04-10 12:23:31,321] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1906754.085035426 W.
[2019-04-10 12:23:57,363] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.2590 2989314505.7283 1566.0000
[2019-04-10 12:23:57,390] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4668 3319594355.9036 2143.0000
[2019-04-10 12:23:57,472] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8663 3185166411.8696 2464.0000
[2019-04-10 12:23:57,594] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-04-10 12:23:57,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6659 3105612604.1257 2010.0000
[2019-04-10 12:23:58,619] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 150000, evaluation results [150000.0, 7287.466840982828, 3319594355.9035697, 2143.0, 7347.66593259902, 3105612604.1257057, 2010.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7029.866250067219, 3185166411.869619, 2464.0, 7925.258991697124, 2989314505.7282696, 1566.0]
[2019-04-10 12:24:01,216] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151590: loss 0.1932
[2019-04-10 12:24:01,218] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151591: learning rate 0.0001
[2019-04-10 12:24:01,243] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151608: loss 0.0678
[2019-04-10 12:24:01,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151608: learning rate 0.0001
[2019-04-10 12:24:01,322] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151654: loss 0.0193
[2019-04-10 12:24:01,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151654: learning rate 0.0001
[2019-04-10 12:24:01,343] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151667: loss 0.0105
[2019-04-10 12:24:01,344] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151667: learning rate 0.0001
[2019-04-10 12:24:01,429] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151722: loss 0.0232
[2019-04-10 12:24:01,432] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151723: learning rate 0.0001
[2019-04-10 12:24:01,433] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151724: loss 0.0113
[2019-04-10 12:24:01,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151724: learning rate 0.0001
[2019-04-10 12:24:01,479] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151750: loss 0.0081
[2019-04-10 12:24:01,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151751: learning rate 0.0001
[2019-04-10 12:24:01,543] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151787: loss 0.0281
[2019-04-10 12:24:01,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151788: learning rate 0.0001
[2019-04-10 12:24:01,784] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151937: loss 0.0060
[2019-04-10 12:24:01,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151937: learning rate 0.0001
[2019-04-10 12:24:01,910] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152014: loss 0.0074
[2019-04-10 12:24:01,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152016: learning rate 0.0001
[2019-04-10 12:24:02,088] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152121: loss 0.0031
[2019-04-10 12:24:02,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152121: learning rate 0.0001
[2019-04-10 12:24:02,250] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152220: loss 0.0006
[2019-04-10 12:24:02,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152221: learning rate 0.0001
[2019-04-10 12:24:02,338] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152274: loss 0.0049
[2019-04-10 12:24:02,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152274: learning rate 0.0001
[2019-04-10 12:24:02,562] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152408: loss 0.0108
[2019-04-10 12:24:02,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152409: learning rate 0.0001
[2019-04-10 12:24:02,676] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152478: loss 0.0251
[2019-04-10 12:24:02,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152479: learning rate 0.0001
[2019-04-10 12:24:03,208] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152803: loss 0.0779
[2019-04-10 12:24:03,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152803: learning rate 0.0001
[2019-04-10 12:24:03,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.7024990e-33 2.6401457e-35 4.5426503e-32 4.0634040e-27], sum to 1.0000
[2019-04-10 12:24:03,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4290
[2019-04-10 12:24:03,960] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9223761330071919, 6.911199999999999, 6.9112, 168.912956510431, 752631.2448169832, 752631.2448169838, 228325.073772697], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5749200.0000, 
sim time next is 5749800.0000, 
raw observation next is [33.8, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9271623271392812, 6.9112, 6.9112, 168.912956510431, 756707.0241387349, 756707.0241387349, 229473.2505438974], 
processed observation next is [0.0, 0.5652173913043478, 0.800947867298578, 0.53, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9111735696820501, 0.0, 0.0, 0.8294399451523027, 0.21019639559409303, 0.21019639559409303, 0.3424973888714886], 
reward next is 0.6575, 
noisyNet noise sample is [array([-0.7109915], dtype=float32), -1.9291197]. 
=============================================
[2019-04-10 12:24:06,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7427370e-02 2.3623487e-10 2.3820623e-10 2.4765541e-06 9.8257011e-01], sum to 1.0000
[2019-04-10 12:24:06,790] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6480
[2019-04-10 12:24:06,794] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.65, 76.0, 1.0, 2.0, 0.6722748646499827, 1.0, 1.0, 0.6567274718392538, 1.0, 1.0, 1.03, 7.005095546156705, 6.9112, 170.5573041426782, 2755556.715846769, 2688295.516708103, 512713.7476796252], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5819400.0000, 
sim time next is 5820000.0000, 
raw observation next is [29.8, 75.33333333333333, 1.0, 2.0, 0.5671628977059183, 1.0, 2.0, 0.5671628977059183, 1.0, 2.0, 0.9849743896787685, 6.9112, 6.9112, 170.5573041426782, 2379395.717291569, 2379395.717291569, 464696.8791431283], 
processed observation next is [1.0, 0.34782608695652173, 0.6113744075829385, 0.7533333333333333, 1.0, 1.0, 0.4785095153083353, 1.0, 1.0, 0.4785095153083353, 1.0, 1.0, 0.9816760849741077, 0.0, 0.0, 0.8375144448122397, 0.6609432548032136, 0.6609432548032136, 0.6935774315569079], 
reward next is 0.3064, 
noisyNet noise sample is [array([0.9850979], dtype=float32), -0.7988351]. 
=============================================
[2019-04-10 12:24:06,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[18.45522 ]
 [20.24982 ]
 [24.263983]
 [23.697905]
 [23.268661]], R is [[19.02224731]
 [18.83202553]
 [18.64370537]
 [18.99946594]
 [19.35112572]].
[2019-04-10 12:24:09,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9179661e-01 3.0955481e-16 4.2130597e-15 4.2884701e-10 8.2033360e-03], sum to 1.0000
[2019-04-10 12:24:09,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4246
[2019-04-10 12:24:09,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2872103.74375907 W.
[2019-04-10 12:24:09,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.15, 64.0, 1.0, 2.0, 0.7277640131995883, 1.0, 2.0, 0.6844720461140567, 1.0, 2.0, 1.03, 7.005099921424712, 6.9112, 170.5573041426782, 2872103.74375907, 2804839.410437955, 530241.0006258729], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5848200.0000, 
sim time next is 5848800.0000, 
raw observation next is [32.1, 64.33333333333333, 1.0, 2.0, 0.6979215753711963, 1.0, 2.0, 0.6695508271998607, 1.0, 2.0, 1.03, 7.005097568274701, 6.9112, 170.5573041426782, 2809422.615751235, 2742159.968087194, 520672.7500312392], 
processed observation next is [1.0, 0.6956521739130435, 0.7203791469194314, 0.6433333333333333, 1.0, 1.0, 0.6360500908086701, 1.0, 1.0, 0.6018684665058562, 1.0, 1.0, 1.0365853658536586, 0.00938975682747012, 0.0, 0.8375144448122397, 0.7803951710420096, 0.7617111022464428, 0.7771235075093123], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36080155], dtype=float32), -0.55464756]. 
=============================================
[2019-04-10 12:24:11,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4424409e-01 1.3868323e-13 4.3342996e-13 7.6010602e-09 5.5755924e-02], sum to 1.0000
[2019-04-10 12:24:11,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9925
[2019-04-10 12:24:11,725] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 81.83333333333333, 1.0, 2.0, 0.9165422586442749, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1281076.474127711, 1281076.474127712, 274531.0883147441], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5903400.0000, 
sim time next is 5904000.0000, 
raw observation next is [29.2, 81.0, 1.0, 2.0, 0.339275295113518, 1.0, 1.0, 0.339275295113518, 1.0, 1.0, 0.5892089875575695, 6.9112, 6.9112, 170.5573041426782, 1422729.071182221, 1422729.071182221, 321821.5539739826], 
processed observation next is [1.0, 0.34782608695652173, 0.5829383886255924, 0.81, 1.0, 1.0, 0.20394613869098555, 1.0, 0.5, 0.20394613869098555, 1.0, 0.5, 0.49903535067996274, 0.0, 0.0, 0.8375144448122397, 0.39520251977283916, 0.39520251977283916, 0.4803306775731083], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31252164], dtype=float32), -0.31002432]. 
=============================================
[2019-04-10 12:24:11,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[31.8838  ]
 [32.857838]
 [33.78503 ]
 [33.45654 ]
 [34.712833]], R is [[32.05735016]
 [32.32703018]
 [32.56110382]
 [32.73248291]
 [32.405159  ]].
[2019-04-10 12:24:14,619] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159556: loss 5.5422
[2019-04-10 12:24:14,622] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159557: learning rate 0.0001
[2019-04-10 12:24:14,778] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159650: loss 7.6808
[2019-04-10 12:24:14,779] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159650: learning rate 0.0001
[2019-04-10 12:24:14,813] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159668: loss 3.8788
[2019-04-10 12:24:14,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159668: learning rate 0.0001
[2019-04-10 12:24:14,838] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159680: loss 7.6673
[2019-04-10 12:24:14,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159680: learning rate 0.0001
[2019-04-10 12:24:14,912] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159720: loss 3.4938
[2019-04-10 12:24:14,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159721: learning rate 0.0001
[2019-04-10 12:24:14,968] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159744: loss 1.9424
[2019-04-10 12:24:14,970] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159744: learning rate 0.0001
[2019-04-10 12:24:15,057] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159783: loss 5.8018
[2019-04-10 12:24:15,061] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159783: learning rate 0.0001
[2019-04-10 12:24:15,148] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159827: loss 51.6673
[2019-04-10 12:24:15,150] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159827: learning rate 0.0001
[2019-04-10 12:24:15,348] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159914: loss 2.5062
[2019-04-10 12:24:15,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159916: learning rate 0.0001
[2019-04-10 12:24:15,560] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160013: loss 23.0296
[2019-04-10 12:24:15,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160013: learning rate 0.0001
[2019-04-10 12:24:15,602] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160034: loss 33.0827
[2019-04-10 12:24:15,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160034: learning rate 0.0001
[2019-04-10 12:24:16,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160283: loss 63.8898
[2019-04-10 12:24:16,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160283: learning rate 0.0001
[2019-04-10 12:24:16,448] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160326: loss 8.7455
[2019-04-10 12:24:16,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160326: learning rate 0.0001
[2019-04-10 12:24:16,733] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160449: loss 11.1771
[2019-04-10 12:24:16,734] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160449: learning rate 0.0001
[2019-04-10 12:24:16,739] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160450: loss 8.6879
[2019-04-10 12:24:16,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160454: learning rate 0.0001
[2019-04-10 12:24:17,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160775: loss -24.0004
[2019-04-10 12:24:17,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160775: learning rate 0.0001
[2019-04-10 12:24:26,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5087526e-02 1.0343150e-15 1.8803062e-13 9.3349550e-09 9.7491246e-01], sum to 1.0000
[2019-04-10 12:24:26,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-10 12:24:26,502] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.61666666666667, 92.0, 1.0, 2.0, 0.2647479705201053, 1.0, 2.0, 0.2647479705201053, 1.0, 2.0, 0.4550421402119886, 6.9112, 6.9112, 170.5573041426782, 1110041.740844297, 1110041.740844297, 290274.7255403757], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6144600.0000, 
sim time next is 6145200.0000, 
raw observation next is [26.6, 92.0, 1.0, 2.0, 0.2564459222623862, 1.0, 2.0, 0.2564459222623862, 1.0, 2.0, 0.4405322659031426, 6.9112, 6.9112, 170.5573041426782, 1075215.25754746, 1075215.25754746, 287290.2049292846], 
processed observation next is [1.0, 0.13043478260869565, 0.4597156398104266, 0.92, 1.0, 1.0, 0.10415171356914, 1.0, 1.0, 0.10415171356914, 1.0, 1.0, 0.3177222754916373, 0.0, 0.0, 0.8375144448122397, 0.29867090487429443, 0.29867090487429443, 0.42879135064072327], 
reward next is 0.5712, 
noisyNet noise sample is [array([0.6074275], dtype=float32), -1.0302882]. 
=============================================
[2019-04-10 12:24:27,194] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9590945e-01 2.1064651e-17 9.0657153e-16 2.8606884e-10 4.0905303e-03], sum to 1.0000
[2019-04-10 12:24:27,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1934
[2019-04-10 12:24:27,217] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1530154.724757026 W.
[2019-04-10 12:24:27,238] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.88333333333333, 86.5, 1.0, 2.0, 0.3648745984996589, 1.0, 1.0, 0.3648745984996589, 1.0, 2.0, 0.6320898060354007, 6.9112, 6.9112, 170.5573041426782, 1530154.724757026, 1530154.724757026, 334100.2583219613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6163800.0000, 
sim time next is 6164400.0000, 
raw observation next is [27.96666666666667, 86.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 10.448004306582, 6.9112, 168.8923935073365, 3964126.613773495, 1455301.366009602, 304713.5946563032], 
processed observation next is [1.0, 0.34782608695652173, 0.524486571879937, 0.86, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.3536804306582, 0.0, 0.8293389713933251, 1.1011462816037485, 0.4042503794471117, 0.4547964099347809], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4705642], dtype=float32), 1.3141574]. 
=============================================
[2019-04-10 12:24:36,267] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167420: loss 0.2178
[2019-04-10 12:24:36,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167421: learning rate 0.0001
[2019-04-10 12:24:36,871] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167601: loss 0.0636
[2019-04-10 12:24:36,901] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167607: loss 0.0551
[2019-04-10 12:24:36,905] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167601: learning rate 0.0001
[2019-04-10 12:24:36,927] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167607: learning rate 0.0001
[2019-04-10 12:24:36,932] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167611: loss 0.0456
[2019-04-10 12:24:36,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167611: learning rate 0.0001
[2019-04-10 12:24:36,989] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167621: loss 0.0395
[2019-04-10 12:24:36,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167621: learning rate 0.0001
[2019-04-10 12:24:37,070] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167649: loss 0.0425
[2019-04-10 12:24:37,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167649: learning rate 0.0001
[2019-04-10 12:24:37,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.4373958e-30 6.9356055e-30 2.6725980e-23 3.1398690e-21], sum to 1.0000
[2019-04-10 12:24:37,628] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5579
[2019-04-10 12:24:37,670] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8958389694465051, 6.9112, 6.9112, 168.912956510431, 737517.5194075225, 737517.5194075225, 222389.045911443], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6306600.0000, 
sim time next is 6307200.0000, 
raw observation next is [27.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8965079774894937, 6.911200000000001, 6.9112, 168.912956510431, 738068.8950873374, 738068.8950873368, 222543.7018388376], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8737902164506022, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20501913752426038, 0.2050191375242602, 0.3321547788639367], 
reward next is 0.6678, 
noisyNet noise sample is [array([1.669556], dtype=float32), 0.3258045]. 
=============================================
[2019-04-10 12:24:37,697] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167816: loss 0.1118
[2019-04-10 12:24:37,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167816: learning rate 0.0001
[2019-04-10 12:24:38,011] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167909: loss 0.1103
[2019-04-10 12:24:38,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167909: learning rate 0.0001
[2019-04-10 12:24:38,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167942: loss 0.1046
[2019-04-10 12:24:38,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167942: learning rate 0.0001
[2019-04-10 12:24:38,336] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167991: loss 0.0905
[2019-04-10 12:24:38,337] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167991: learning rate 0.0001
[2019-04-10 12:24:38,497] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168025: loss 0.0625
[2019-04-10 12:24:38,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168027: learning rate 0.0001
[2019-04-10 12:24:39,120] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168191: loss 0.0770
[2019-04-10 12:24:39,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168191: learning rate 0.0001
[2019-04-10 12:24:39,765] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168376: loss 0.0074
[2019-04-10 12:24:39,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168376: learning rate 0.0001
[2019-04-10 12:24:40,281] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168483: loss 0.0702
[2019-04-10 12:24:40,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168483: learning rate 0.0001
[2019-04-10 12:24:40,361] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.6165393e-26 3.9017939e-27 2.7051376e-20 1.8370908e-18], sum to 1.0000
[2019-04-10 12:24:40,361] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3813
[2019-04-10 12:24:40,432] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.9, 80.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.885307853482234, 6.9112, 6.9112, 168.912956510431, 729979.741927517, 729979.741927517, 220013.9376543119], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6333000.0000, 
sim time next is 6333600.0000, 
raw observation next is [28.0, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8846017260375793, 6.911200000000001, 6.9112, 168.912956510431, 729438.5545294692, 729438.5545294686, 219854.3365984189], 
processed observation next is [0.0, 0.30434782608695654, 0.5260663507109005, 0.7966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8592703976068038, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20262182070263035, 0.20262182070263018, 0.3281408008931625], 
reward next is 0.6719, 
noisyNet noise sample is [array([1.1953709], dtype=float32), -0.6920402]. 
=============================================
[2019-04-10 12:24:40,884] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168587: loss 0.1643
[2019-04-10 12:24:40,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168587: learning rate 0.0001
[2019-04-10 12:24:43,181] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 169023: loss 0.0628
[2019-04-10 12:24:43,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 169023: learning rate 0.0001
[2019-04-10 12:24:47,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 7.4622876e-28 6.6047456e-27 7.9007389e-21 4.9844410e-19], sum to 1.0000
[2019-04-10 12:24:47,539] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-10 12:24:47,604] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.7, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8609864916019218, 6.9112, 6.9112, 168.912956510431, 713577.2958313425, 713577.2958313425, 214668.5084910003], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6357600.0000, 
sim time next is 6358200.0000, 
raw observation next is [30.75, 62.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8742620548354357, 6.911199999999999, 6.9112, 168.912956510431, 724294.8103942728, 724294.8103942734, 217630.0527166241], 
processed observation next is [0.0, 0.6086956521739131, 0.6563981042654029, 0.6266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8466610424822386, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.201193002887298, 0.20119300288729816, 0.3248209742039166], 
reward next is 0.6752, 
noisyNet noise sample is [array([-1.0638754], dtype=float32), -0.044855133]. 
=============================================
[2019-04-10 12:24:56,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.99648333e-01 4.37455757e-17 3.41926660e-16 1.01953834e-10
 3.51707422e-04], sum to 1.0000
[2019-04-10 12:24:56,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-10 12:24:56,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2267123.393413396 W.
[2019-04-10 12:24:56,496] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.66666666666667, 1.0, 2.0, 0.9800819381744683, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98230842370171, 6.9112, 168.9125330103086, 2267123.393413396, 2216676.756756926, 458085.4097172], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6448800.0000, 
sim time next is 6449400.0000, 
raw observation next is [30.0, 67.5, 1.0, 2.0, 0.9857033835835668, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97895407690331, 6.9112, 168.912495704146, 2274991.476419392, 2226924.533479143, 459891.5747369955], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.675, 1.0, 1.0, 0.9827751609440564, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006775407690331026, 0.0, 0.8294376823824854, 0.6319420767831645, 0.6185901481886508, 0.6864053354283515], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9826209], dtype=float32), -0.93633443]. 
=============================================
[2019-04-10 12:24:58,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9922586e-04 1.1982456e-16 3.3732430e-14 3.9742751e-09 9.9950075e-01], sum to 1.0000
[2019-04-10 12:24:58,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-10 12:24:58,908] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.28333333333333, 91.16666666666667, 1.0, 2.0, 0.2778410099513395, 1.0, 2.0, 0.2778410099513395, 1.0, 2.0, 0.4741725229601204, 6.911200000000001, 6.9112, 170.5573041426782, 1164968.379473672, 1164968.379473672, 294838.2879442319], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6491400.0000, 
sim time next is 6492000.0000, 
raw observation next is [26.26666666666667, 91.33333333333334, 1.0, 2.0, 0.2404449054559187, 1.0, 2.0, 0.2404449054559187, 1.0, 2.0, 0.4103331969925126, 6.9112, 6.9112, 170.5573041426782, 1008095.356947682, 1008095.356947682, 281644.6230348025], 
processed observation next is [1.0, 0.13043478260869565, 0.44391785150079005, 0.9133333333333334, 1.0, 1.0, 0.08487338006737193, 1.0, 1.0, 0.08487338006737193, 1.0, 1.0, 0.2808941426737958, 0.0, 0.0, 0.8375144448122397, 0.28002648804102276, 0.28002648804102276, 0.4203651090071679], 
reward next is 0.5796, 
noisyNet noise sample is [array([0.20928428], dtype=float32), -0.42662376]. 
=============================================
[2019-04-10 12:24:58,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[52.09844 ]
 [51.262722]
 [49.7792  ]
 [48.098217]
 [46.887615]], R is [[53.07667923]
 [53.10585785]
 [53.16046906]
 [53.21202087]
 [52.67990112]].
[2019-04-10 12:25:00,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9063945e-01 4.2061200e-16 1.8328187e-14 3.4047054e-09 9.3605984e-03], sum to 1.0000
[2019-04-10 12:25:00,840] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9389
[2019-04-10 12:25:00,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 920108.156565167 W.
[2019-04-10 12:25:00,847] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.13333333333333, 87.33333333333333, 1.0, 2.0, 0.329200208079482, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5615632234416501, 6.911200000000001, 6.9112, 168.912956510431, 920108.156565167, 920108.1565651664, 230642.8510130141], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6508200.0000, 
sim time next is 6508800.0000, 
raw observation next is [27.2, 87.0, 1.0, 2.0, 0.3911339022357923, 1.0, 1.0, 0.3911339022357923, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1093295.302814227, 1093295.302814227, 269916.4723852144], 
processed observation next is [1.0, 0.34782608695652173, 0.4881516587677725, 0.87, 1.0, 1.0, 0.2664263882358943, 1.0, 0.5, 0.2664263882358943, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3036931396706186, 0.3036931396706186, 0.40286040654509614], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2592188], dtype=float32), -2.271599]. 
=============================================
[2019-04-10 12:25:01,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9972802e-01 4.1239803e-16 2.2021690e-15 2.9163074e-09 2.7201662e-04], sum to 1.0000
[2019-04-10 12:25:01,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6321
[2019-04-10 12:25:01,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 921146.7023879327 W.
[2019-04-10 12:25:01,441] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.13333333333333, 87.33333333333333, 1.0, 2.0, 0.65914324469916, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 921146.7023879327, 921146.7023879321, 212273.4334065625], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6508200.0000, 
sim time next is 6508800.0000, 
raw observation next is [27.2, 87.0, 1.0, 2.0, 0.3905709759666158, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6656051572470304, 6.911199999999999, 6.9112, 168.912956510431, 1091726.423965612, 1091726.423965612, 253779.8760274744], 
processed observation next is [1.0, 0.34782608695652173, 0.4881516587677725, 0.87, 1.0, 1.0, 0.2657481638151997, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.5922014112768663, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3032573399904478, 0.3032573399904478, 0.3787759343693648], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14269663], dtype=float32), 0.48924193]. 
=============================================
[2019-04-10 12:25:02,061] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-10 12:25:02,063] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:25:02,064] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:25:02,065] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:25:02,065] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:25:02,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:25:02,065] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:25:02,067] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:25:02,067] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:25:02,071] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:25:02,072] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:25:02,078] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-04-10 12:25:02,078] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-04-10 12:25:02,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-04-10 12:25:02,091] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-04-10 12:25:02,119] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-04-10 12:25:15,483] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.08260304], dtype=float32), 0.020440796]
[2019-04-10 12:25:15,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.78126955, 58.251809725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.708893472141906, 6.9112, 6.9112, 168.912956510431, 608667.1880208105, 608667.1880208105, 184334.8108410504]
[2019-04-10 12:25:15,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:25:15,486] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.5866957e-26 3.9214328e-26 3.7942035e-20 5.5332369e-16], sampled 0.21489331833218528
[2019-04-10 12:25:19,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.08260304], dtype=float32), 0.020440796]
[2019-04-10 12:25:19,977] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.96666666666667, 88.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7195812302975884, 6.911199999999999, 6.9112, 168.912956510431, 614916.6447807577, 614916.6447807584, 186290.4653530441]
[2019-04-10 12:25:19,978] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:25:19,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.0387841e-24 2.1872174e-24 2.3887173e-18 1.0036296e-13], sampled 0.5239683556474554
[2019-04-10 12:25:27,797] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.08260304], dtype=float32), 0.020440796]
[2019-04-10 12:25:27,797] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.81666666666667, 74.33333333333334, 1.0, 1.0, 0.5966216597458452, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128875642674, 833739.0609768108, 833739.0609768108, 200117.7397485784]
[2019-04-10 12:25:27,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:25:27,802] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.9843983e-27 1.2641059e-27 4.7261409e-21 2.4864244e-17], sampled 0.09561665035332445
[2019-04-10 12:25:36,914] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08260304], dtype=float32), 0.020440796]
[2019-04-10 12:25:36,914] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.96727144166667, 80.26426558666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9195802425425889, 6.9112, 6.9112, 168.9129565104298, 754937.8105646111, 754937.8105646111, 227863.2070338964]
[2019-04-10 12:25:36,916] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:25:36,917] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.0235098e-23 8.6943528e-23 1.2132348e-16 7.1850442e-12], sampled 0.10382457843073056
[2019-04-10 12:26:08,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.08260304], dtype=float32), 0.020440796]
[2019-04-10 12:26:08,076] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.09153432833333, 65.762768525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8689193058149915, 6.9112, 6.9112, 168.912956510431, 721796.620413296, 721796.620413296, 216491.5172671554]
[2019-04-10 12:26:08,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:26:08,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 9.4840424e-23 2.8852089e-22 1.7197953e-16 7.4923696e-12], sampled 0.031623376095690436
[2019-04-10 12:26:22,822] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7002.0412 3190530499.7019 2433.0000
[2019-04-10 12:26:23,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8105.4994 2939446490.0859 1196.0000
[2019-04-10 12:26:23,519] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7238.2116 3324085294.1824 2114.0000
[2019-04-10 12:26:23,593] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7442.9415 3104369960.4701 1747.0000
[2019-04-10 12:26:23,642] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7900.0906 2992535982.6966 1551.0000
[2019-04-10 12:26:24,654] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 175000, evaluation results [175000.0, 7238.211565118061, 3324085294.182362, 2114.0, 7442.941523877926, 3104369960.470079, 1747.0, 8105.499427302627, 2939446490.085919, 1196.0, 7002.041210102836, 3190530499.70194, 2433.0, 7900.090585632163, 2992535982.6965528, 1551.0]
[2019-04-10 12:26:25,442] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175485: loss 84.4717
[2019-04-10 12:26:25,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175485: learning rate 0.0001
[2019-04-10 12:26:25,583] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175571: loss 7.7127
[2019-04-10 12:26:25,585] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175572: learning rate 0.0001
[2019-04-10 12:26:25,592] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175577: loss 4.5319
[2019-04-10 12:26:25,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175577: learning rate 0.0001
[2019-04-10 12:26:25,615] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175588: loss 20.0842
[2019-04-10 12:26:25,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175590: learning rate 0.0001
[2019-04-10 12:26:25,693] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175642: loss -54.5485
[2019-04-10 12:26:25,696] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175643: learning rate 0.0001
[2019-04-10 12:26:25,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175729: loss -50.2951
[2019-04-10 12:26:25,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175730: learning rate 0.0001
[2019-04-10 12:26:26,097] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175883: loss 3.6697
[2019-04-10 12:26:26,099] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175884: learning rate 0.0001
[2019-04-10 12:26:26,104] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175888: loss -48.0968
[2019-04-10 12:26:26,106] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175888: learning rate 0.0001
[2019-04-10 12:26:26,131] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175901: loss -45.2765
[2019-04-10 12:26:26,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175902: learning rate 0.0001
[2019-04-10 12:26:26,136] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175904: loss 62.3387
[2019-04-10 12:26:26,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175905: learning rate 0.0001
[2019-04-10 12:26:26,276] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175989: loss 43.9581
[2019-04-10 12:26:26,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175990: learning rate 0.0001
[2019-04-10 12:26:26,467] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0213621e-03 2.6862438e-16 5.0229143e-15 7.6194064e-09 9.9897861e-01], sum to 1.0000
[2019-04-10 12:26:26,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1537
[2019-04-10 12:26:26,476] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.25, 61.5, 1.0, 2.0, 0.5747775920354676, 1.0, 1.0, 0.5747775920354676, 1.0, 2.0, 0.9871297170125632, 6.911199999999999, 6.9112, 170.5573041426782, 2411372.160931296, 2411372.160931296, 468363.4539916083], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6618600.0000, 
sim time next is 6619200.0000, 
raw observation next is [31.16666666666666, 62.0, 1.0, 2.0, 0.5436651814395869, 1.0, 2.0, 0.5436651814395869, 1.0, 2.0, 0.9346249072200797, 6.9112, 6.9112, 170.5573041426782, 2280726.737017771, 2280726.737017771, 444756.0691746846], 
processed observation next is [1.0, 0.6086956521739131, 0.6761453396524484, 0.62, 1.0, 1.0, 0.45019901378263477, 1.0, 1.0, 0.45019901378263477, 1.0, 1.0, 0.9202742770976581, 0.0, 0.0, 0.8375144448122397, 0.6335352047271586, 0.6335352047271586, 0.6638150286189323], 
reward next is 0.3362, 
noisyNet noise sample is [array([0.33777165], dtype=float32), -0.9423951]. 
=============================================
[2019-04-10 12:26:26,543] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176153: loss 20.3765
[2019-04-10 12:26:26,544] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176153: learning rate 0.0001
[2019-04-10 12:26:26,638] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176213: loss 19.3648
[2019-04-10 12:26:26,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176213: learning rate 0.0001
[2019-04-10 12:26:27,363] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176660: loss 90.8112
[2019-04-10 12:26:27,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176662: learning rate 0.0001
[2019-04-10 12:26:27,403] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176685: loss 5.5544
[2019-04-10 12:26:27,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176685: learning rate 0.0001
[2019-04-10 12:26:27,955] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 177026: loss 23.3921
[2019-04-10 12:26:27,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 177027: learning rate 0.0001
[2019-04-10 12:26:29,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5119931e-04 1.6114065e-18 3.2315522e-16 5.9213928e-10 9.9934882e-01], sum to 1.0000
[2019-04-10 12:26:29,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1849
[2019-04-10 12:26:29,348] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.2, 93.66666666666667, 1.0, 2.0, 0.2214613051309129, 1.0, 2.0, 0.2214613051309129, 1.0, 2.0, 0.3733762631921111, 6.9112, 6.9112, 170.5573041426782, 928469.7806658145, 928469.7806658145, 275367.1203335764], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6675000.0000, 
sim time next is 6675600.0000, 
raw observation next is [25.3, 93.33333333333334, 1.0, 2.0, 0.1927291891626703, 1.0, 2.0, 0.1927291891626703, 1.0, 2.0, 0.3251933199555825, 6.911200000000001, 6.9112, 170.5573041426782, 807965.8831889399, 807965.8831889393, 267378.1965086691], 
processed observation next is [1.0, 0.2608695652173913, 0.39810426540284366, 0.9333333333333335, 1.0, 1.0, 0.02738456525622925, 1.0, 1.0, 0.02738456525622925, 1.0, 1.0, 0.17706502433607618, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2244349675524833, 0.22443496755248313, 0.39907193508756583], 
reward next is 0.6009, 
noisyNet noise sample is [array([0.5452273], dtype=float32), 0.44639328]. 
=============================================
[2019-04-10 12:26:37,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 6.2895152e-27 2.9122723e-26 7.1645110e-21 1.4406670e-12], sum to 1.0000
[2019-04-10 12:26:37,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-10 12:26:37,610] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.45, 64.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5836035866318908, 6.911200000000001, 6.9112, 168.912956510431, 510224.5839530891, 510224.5839530884, 163578.0672118959], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6816600.0000, 
sim time next is 6817200.0000, 
raw observation next is [25.33333333333334, 65.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.58711231795406, 6.911199999999999, 6.9112, 168.912956510431, 513167.4999976011, 513167.4999976018, 164100.5379156285], 
processed observation next is [1.0, 0.9130434782608695, 0.3996840442338076, 0.6533333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4964784365293415, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1425465277771114, 0.1425465277771116, 0.24492617599347535], 
reward next is 0.7551, 
noisyNet noise sample is [array([-1.6652566], dtype=float32), -0.20039247]. 
=============================================
[2019-04-10 12:26:37,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.0419347e-30 2.6433671e-30 9.6771411e-25 2.0843814e-19], sum to 1.0000
[2019-04-10 12:26:37,951] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-10 12:26:37,955] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.55, 40.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5209175459165685, 6.911200000000001, 6.9112, 168.912956510431, 459727.3100937979, 459727.3100937972, 154725.2865187401], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6881400.0000, 
sim time next is 6882000.0000, 
raw observation next is [29.5, 41.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5308856540474365, 6.9112, 6.9112, 168.912956510431, 467815.3221836995, 467815.3221836995, 156064.4432790795], 
processed observation next is [0.0, 0.6521739130434783, 0.5971563981042655, 0.41, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4279093342041908, 0.0, 0.0, 0.8294399451523027, 0.1299487006065832, 0.1299487006065832, 0.23293200489414853], 
reward next is 0.7671, 
noisyNet noise sample is [array([0.21172096], dtype=float32), -0.34333324]. 
=============================================
[2019-04-10 12:26:37,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.83316 ]
 [86.740364]
 [86.62209 ]
 [86.49879 ]
 [86.350914]], R is [[86.82263947]
 [86.72348022]
 [86.62763214]
 [86.53327942]
 [86.44168091]].
[2019-04-10 12:26:38,748] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183548: loss 0.1285
[2019-04-10 12:26:38,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183549: learning rate 0.0001
[2019-04-10 12:26:38,772] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183562: loss 0.1096
[2019-04-10 12:26:38,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183564: learning rate 0.0001
[2019-04-10 12:26:38,844] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183606: loss 0.0807
[2019-04-10 12:26:38,848] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183609: learning rate 0.0001
[2019-04-10 12:26:38,859] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183618: loss 0.0619
[2019-04-10 12:26:38,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183618: learning rate 0.0001
[2019-04-10 12:26:38,878] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183627: loss 0.0925
[2019-04-10 12:26:38,883] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183629: learning rate 0.0001
[2019-04-10 12:26:38,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183644: loss 0.0706
[2019-04-10 12:26:38,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183644: learning rate 0.0001
[2019-04-10 12:26:39,092] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183752: loss 0.0211
[2019-04-10 12:26:39,094] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183752: learning rate 0.0001
[2019-04-10 12:26:39,194] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183815: loss 0.0110
[2019-04-10 12:26:39,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183816: learning rate 0.0001
[2019-04-10 12:26:39,265] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183861: loss 0.0034
[2019-04-10 12:26:39,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183862: learning rate 0.0001
[2019-04-10 12:26:39,276] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183869: loss 0.0047
[2019-04-10 12:26:39,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183871: learning rate 0.0001
[2019-04-10 12:26:39,451] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183973: loss 0.0060
[2019-04-10 12:26:39,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183973: learning rate 0.0001
[2019-04-10 12:26:39,659] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184100: loss 0.0134
[2019-04-10 12:26:39,662] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184103: learning rate 0.0001
[2019-04-10 12:26:39,835] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184208: loss 0.0177
[2019-04-10 12:26:39,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184208: learning rate 0.0001
[2019-04-10 12:26:40,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184752: loss 0.0027
[2019-04-10 12:26:40,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184753: learning rate 0.0001
[2019-04-10 12:26:40,762] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184781: loss 0.0026
[2019-04-10 12:26:40,763] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184782: learning rate 0.0001
[2019-04-10 12:26:41,164] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 185030: loss 0.0026
[2019-04-10 12:26:41,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 185030: learning rate 0.0001
[2019-04-10 12:26:52,244] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 191534: loss -142.6472
[2019-04-10 12:26:52,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 191535: learning rate 0.0001
[2019-04-10 12:26:52,312] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191568: loss -167.2480
[2019-04-10 12:26:52,315] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191569: learning rate 0.0001
[2019-04-10 12:26:52,340] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191581: loss -82.2589
[2019-04-10 12:26:52,341] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191581: learning rate 0.0001
[2019-04-10 12:26:52,360] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191588: loss -53.5049
[2019-04-10 12:26:52,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191588: learning rate 0.0001
[2019-04-10 12:26:52,372] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191593: loss 3.5697
[2019-04-10 12:26:52,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191593: learning rate 0.0001
[2019-04-10 12:26:52,688] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191695: loss -80.2418
[2019-04-10 12:26:52,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191695: learning rate 0.0001
[2019-04-10 12:26:53,029] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191787: loss -31.7828
[2019-04-10 12:26:53,030] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191787: learning rate 0.0001
[2019-04-10 12:26:53,268] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191830: loss -308.0594
[2019-04-10 12:26:53,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191830: learning rate 0.0001
[2019-04-10 12:26:53,535] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191913: loss -196.0251
[2019-04-10 12:26:53,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191913: learning rate 0.0001
[2019-04-10 12:26:53,548] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191923: loss -172.9092
[2019-04-10 12:26:53,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191923: learning rate 0.0001
[2019-04-10 12:26:53,653] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191972: loss -138.5666
[2019-04-10 12:26:53,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191972: learning rate 0.0001
[2019-04-10 12:26:53,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9999499e-01 1.5224946e-18 7.1413480e-18 1.4675031e-12 4.9714363e-06], sum to 1.0000
[2019-04-10 12:26:53,926] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4044
[2019-04-10 12:26:53,927] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1190251.840454398 W.
[2019-04-10 12:26:53,943] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4258015265854859, 1.0, 1.0, 0.4258015265854859, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1190251.840454398, 1190251.840454398, 278811.7366532444], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7218000.0000, 
sim time next is 7218600.0000, 
raw observation next is [24.15, 98.16666666666667, 1.0, 2.0, 0.4376318009120065, 0.0, 1.0, 0.0, 1.0, 1.0, 0.730882740934837, 6.9112, 6.9112, 168.912956510431, 1223346.913462089, 1223346.913462089, 271837.9086851227], 
processed observation next is [1.0, 0.5652173913043478, 0.34360189573459715, 0.9816666666666667, 1.0, 1.0, 0.3224479529060319, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6718082206522401, 0.0, 0.0, 0.8294399451523027, 0.3398185870728025, 0.3398185870728025, 0.40572822191809355], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.35198796], dtype=float32), 0.011127737]. 
=============================================
[2019-04-10 12:26:54,130] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192113: loss -102.5554
[2019-04-10 12:26:54,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192113: learning rate 0.0001
[2019-04-10 12:26:54,218] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192146: loss -222.8966
[2019-04-10 12:26:54,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192146: learning rate 0.0001
[2019-04-10 12:26:56,323] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192712: loss -49.7676
[2019-04-10 12:26:56,352] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192712: learning rate 0.0001
[2019-04-10 12:26:56,637] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192771: loss 11.4979
[2019-04-10 12:26:56,637] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192771: learning rate 0.0001
[2019-04-10 12:26:57,907] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 193032: loss -132.8687
[2019-04-10 12:26:57,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 193032: learning rate 0.0001
[2019-04-10 12:27:04,073] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 3.4552922e-21 5.2991306e-20 1.0070668e-15 2.8241587e-10], sum to 1.0000
[2019-04-10 12:27:04,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5179
[2019-04-10 12:27:04,088] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5944369345417867, 6.911200000000001, 6.9112, 168.912956510431, 518613.2219775501, 518613.2219775495, 165217.2641012385], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7261200.0000, 
sim time next is 7261800.0000, 
raw observation next is [21.98333333333333, 88.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5921353439632104, 6.9112, 6.9112, 168.912956510431, 516851.6012024212, 516851.6012024212, 164866.1499431068], 
processed observation next is [1.0, 0.043478260869565216, 0.24091627172195884, 0.8883333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5026040780039152, 0.0, 0.0, 0.8294399451523027, 0.14356988922289476, 0.14356988922289476, 0.2460688805120997], 
reward next is 0.7539, 
noisyNet noise sample is [array([3.340964], dtype=float32), 1.2273057]. 
=============================================
[2019-04-10 12:27:07,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5045487e-01 1.4601172e-15 9.2656858e-15 7.9840373e-10 1.4954507e-01], sum to 1.0000
[2019-04-10 12:27:07,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-10 12:27:07,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1264140.076875506 W.
[2019-04-10 12:27:07,338] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.46666666666667, 70.33333333333334, 1.0, 2.0, 0.829533408864499, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1264140.076875506, 1264140.076875506, 265456.8613277265], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7294800.0000, 
sim time next is 7295400.0000, 
raw observation next is [25.7, 69.5, 1.0, 2.0, 0.4379642100766686, 1.0, 1.0, 0.4379642100766686, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1319618.471707946, 1319618.471707946, 293035.6171496611], 
processed observation next is [1.0, 0.43478260869565216, 0.4170616113744076, 0.695, 1.0, 1.0, 0.3228484458755044, 1.0, 0.5, 0.3228484458755044, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.36656068658554053, 0.36656068658554053, 0.43736659276068823], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.19077076], dtype=float32), 0.8310958]. 
=============================================
[2019-04-10 12:27:11,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.2079336e-23 1.6779849e-22 4.0717258e-17 5.9335835e-11], sum to 1.0000
[2019-04-10 12:27:11,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8624
[2019-04-10 12:27:11,159] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.4, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6844128327097868, 6.9112, 6.9112, 168.912956510431, 593244.4596555493, 593244.4596555493, 179926.5243060023], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7365600.0000, 
sim time next is 7366200.0000, 
raw observation next is [22.21666666666667, 90.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6589470205007713, 6.9112, 6.9112, 168.912956510431, 572208.7360456684, 572208.7360456684, 175542.4283631891], 
processed observation next is [1.0, 0.2608695652173913, 0.2519747235387047, 0.9016666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5840817323180136, 0.0, 0.0, 0.8294399451523027, 0.15894687112379677, 0.15894687112379677, 0.2620036244226703], 
reward next is 0.7380, 
noisyNet noise sample is [array([1.0904101], dtype=float32), 0.45944086]. 
=============================================
[2019-04-10 12:27:14,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2102641e-01 3.0541772e-14 1.1169910e-12 2.9162711e-08 7.8973591e-02], sum to 1.0000
[2019-04-10 12:27:14,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-10 12:27:14,742] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.03333333333333, 93.0, 1.0, 2.0, 0.3368085097020818, 0.0, 1.0, 0.0, 1.0, 2.0, 0.606208762273748, 6.9112, 6.9112, 168.912956510431, 1059936.589790813, 1059936.589790813, 244952.7677413252], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7381200.0000, 
sim time next is 7381800.0000, 
raw observation next is [21.1, 93.0, 1.0, 2.0, 0.2362289077310656, 1.0, 1.0, 0.2362289077310656, 1.0, 2.0, 0.422286490040746, 6.911200000000001, 6.9112, 170.5573041426782, 1102590.186174757, 1102590.186174756, 292450.342606001], 
processed observation next is [1.0, 0.43478260869565216, 0.1990521327014219, 0.93, 1.0, 1.0, 0.07979386473622362, 1.0, 0.5, 0.07979386473622362, 1.0, 1.0, 0.29547132931798287, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3062750517152103, 0.30627505171520997, 0.4364930486656731], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.918978], dtype=float32), -0.1861162]. 
=============================================
[2019-04-10 12:27:19,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.7016324e-33 1.4261467e-31 1.0332590e-24 6.0680221e-21], sum to 1.0000
[2019-04-10 12:27:19,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-10 12:27:19,919] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.3, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5697777360855009, 6.911199999999999, 6.9112, 168.912956510431, 498901.1413231689, 498901.1413231696, 161544.2635254112], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7434000.0000, 
sim time next is 7434600.0000, 
raw observation next is [21.31666666666667, 91.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5689502068094378, 6.9112, 6.9112, 168.912956510431, 498204.7504799939, 498204.7504799939, 161424.6169455413], 
processed observation next is [0.0, 0.043478260869565216, 0.20932069510268583, 0.9183333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4743295204993143, 0.0, 0.0, 0.8294399451523027, 0.13839020846666497, 0.13839020846666497, 0.2409322640978228], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.5504236], dtype=float32), 1.2560682]. 
=============================================
[2019-04-10 12:27:24,516] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199452: loss 0.0006
[2019-04-10 12:27:24,517] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199452: learning rate 0.0001
[2019-04-10 12:27:24,765] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199532: loss 0.0199
[2019-04-10 12:27:24,770] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199532: learning rate 0.0001
[2019-04-10 12:27:24,780] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199542: loss 0.0075
[2019-04-10 12:27:24,784] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199543: learning rate 0.0001
[2019-04-10 12:27:24,813] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 199556: loss 0.0007
[2019-04-10 12:27:24,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 199556: learning rate 0.0001
[2019-04-10 12:27:24,882] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199567: loss 0.0012
[2019-04-10 12:27:24,882] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199567: learning rate 0.0001
[2019-04-10 12:27:24,980] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199608: loss 0.0083
[2019-04-10 12:27:24,981] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199608: learning rate 0.0001
[2019-04-10 12:27:25,157] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199665: loss 0.0062
[2019-04-10 12:27:25,173] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199665: learning rate 0.0001
[2019-04-10 12:27:25,285] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199705: loss 0.0008
[2019-04-10 12:27:25,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199705: learning rate 0.0001
[2019-04-10 12:27:25,719] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199869: loss 0.0778
[2019-04-10 12:27:25,720] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199869: learning rate 0.0001
[2019-04-10 12:27:25,921] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199953: loss 0.0712
[2019-04-10 12:27:25,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199953: learning rate 0.0001
[2019-04-10 12:27:26,077] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-10 12:27:26,083] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:27:26,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:27:26,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:27:26,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-04-10 12:27:26,132] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:27:26,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:27:26,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:27:26,137] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-04-10 12:27:26,136] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:27:26,143] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:27:26,143] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:27:26,163] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:27:26,163] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-04-10 12:27:26,165] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-04-10 12:27:26,197] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-04-10 12:28:05,950] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:05,950] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.33333333333334, 52.66666666666667, 1.0, 2.0, 0.5787725909171952, 0.0, 2.0, 0.0, 1.0, 1.0, 1.005136587402542, 6.9112, 6.9112, 168.9125180817103, 1618189.685229256, 1618189.685229256, 354205.9221395258]
[2019-04-10 12:28:05,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:05,954] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999738e-01 2.6751566e-19 1.8083012e-18 6.9651571e-13 2.6462703e-06], sampled 0.9253537472571265
[2019-04-10 12:28:05,954] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1618189.685229256 W.
[2019-04-10 12:28:07,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:07,287] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.3, 45.33333333333334, 1.0, 2.0, 0.5598351123056137, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9659031749754642, 6.911200000000001, 6.9112, 168.9129352107123, 1565203.365008987, 1565203.365008987, 341201.8788836189]
[2019-04-10 12:28:07,288] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:28:07,290] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 5.1610089e-23 1.3331347e-22 4.0125184e-16 3.5241758e-09], sampled 0.727724602162027
[2019-04-10 12:28:07,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1565203.365008987 W.
[2019-04-10 12:28:09,623] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:09,623] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.3, 45.66666666666667, 1.0, 2.0, 0.6266465805455589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564614202, 875714.1864946062, 875714.1864946062, 205813.1489036417]
[2019-04-10 12:28:09,624] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:09,626] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.99999642e-01 3.13311263e-18 1.00239335e-17 1.58272744e-12
 3.13477670e-07], sampled 0.6274110805816702
[2019-04-10 12:28:09,626] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 875714.1864946062 W.
[2019-04-10 12:28:10,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:10,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.67212669, 83.24586419, 1.0, 2.0, 0.6846869623807934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564866349, 956859.8971162622, 956859.8971162622, 217587.7012105054]
[2019-04-10 12:28:10,420] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:28:10,421] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.9809867e-23 3.2262783e-23 9.4549151e-18 5.2565976e-13], sampled 0.30864516143397913
[2019-04-10 12:28:10,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 956859.8971162622 W.
[2019-04-10 12:28:19,742] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:19,743] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 66.0, 1.0, 2.0, 0.9186748496813602, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.982787297510394, 6.9112, 168.912470067159, 2181176.444889999, 2130390.098448764, 440122.4951423819]
[2019-04-10 12:28:19,744] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:19,746] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9762458e-01 9.7513417e-19 7.3403453e-18 2.4557264e-11 2.3754509e-03], sampled 0.705284294849584
[2019-04-10 12:28:19,747] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2181176.444889999 W.
[2019-04-10 12:28:26,212] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:26,213] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.16666666666666, 95.0, 1.0, 2.0, 0.9604171825158274, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510202, 1342440.441373412, 1342440.441373412, 287097.4860622329]
[2019-04-10 12:28:26,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:26,215] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9986482e-01 3.7379309e-17 2.2641545e-16 1.1388914e-10 1.3512258e-04], sampled 0.8586417004455604
[2019-04-10 12:28:26,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1342440.441373412 W.
[2019-04-10 12:28:28,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:28,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.56666666666666, 60.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9488830601016538, 6.9112, 6.9112, 168.912956510431, 771572.8402517404, 771572.8402517404, 234588.3611678001]
[2019-04-10 12:28:28,592] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:28:28,594] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.3316963e-27 4.1711101e-27 2.8830538e-21 3.8405748e-17], sampled 0.2165592580179776
[2019-04-10 12:28:34,339] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:34,341] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.3, 63.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9944110513446366, 6.9112, 6.9112, 168.912956510431, 803973.0247912912, 803973.0247912912, 245751.9699097128]
[2019-04-10 12:28:34,341] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:28:34,344] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 4.1945656e-27 3.3268945e-27 3.0052641e-21 5.0936637e-17], sampled 0.31219026890946067
[2019-04-10 12:28:41,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.09378961], dtype=float32), 0.027069202]
[2019-04-10 12:28:41,386] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.96666666666667, 88.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9989721334168808, 6.9112, 6.9112, 168.9128741838601, 807708.8278925741, 807708.8278925741, 246926.3616677433]
[2019-04-10 12:28:41,386] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:28:41,388] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 5.8579467e-23 1.2661023e-22 8.2070597e-17 3.7730305e-12], sampled 0.476978696128349
[2019-04-10 12:28:50,048] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7419.6111 3104510727.4746 1799.0000
[2019-04-10 12:28:50,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7007.9869 3188213029.4787 2443.0000
[2019-04-10 12:28:50,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7914.3430 2989979930.0031 1570.0000
[2019-04-10 12:28:50,231] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8085.3987 2938476090.8500 1257.0000
[2019-04-10 12:28:50,268] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7254.8750 3321881418.9840 2168.0000
[2019-04-10 12:28:51,280] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 200000, evaluation results [200000.0, 7254.875036849195, 3321881418.9839516, 2168.0, 7419.611121744015, 3104510727.4745526, 1799.0, 8085.398715027787, 2938476090.8499746, 1257.0, 7007.986949148485, 3188213029.4787183, 2443.0, 7914.342963463797, 2989979930.0030527, 1570.0]
[2019-04-10 12:28:51,487] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200129: loss 0.0671
[2019-04-10 12:28:51,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200129: learning rate 0.0001
[2019-04-10 12:28:51,521] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200149: loss 0.0654
[2019-04-10 12:28:51,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200150: learning rate 0.0001
[2019-04-10 12:28:51,594] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200199: loss 0.0841
[2019-04-10 12:28:51,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200199: learning rate 0.0001
[2019-04-10 12:28:52,437] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200716: loss 0.0338
[2019-04-10 12:28:52,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200716: learning rate 0.0001
[2019-04-10 12:28:52,648] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200848: loss 0.0057
[2019-04-10 12:28:52,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200848: learning rate 0.0001
[2019-04-10 12:28:53,254] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 201220: loss 0.0134
[2019-04-10 12:28:53,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 201220: learning rate 0.0001
[2019-04-10 12:28:53,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.1427192e-24 6.3452377e-25 3.4995884e-19 5.3519637e-15], sum to 1.0000
[2019-04-10 12:28:53,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4774
[2019-04-10 12:28:53,604] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333333, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7001921838328337, 6.9112, 6.9112, 168.912956510431, 598194.3017368683, 598194.3017368683, 182761.2968547458], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7525200.0000, 
sim time next is 7525800.0000, 
raw observation next is [23.4, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6955793315709009, 6.9112, 6.9112, 168.912956510431, 594625.5350755848, 594625.5350755848, 181938.0277279823], 
processed observation next is [0.0, 0.08695652173913043, 0.30805687203791465, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6287552824035377, 0.0, 0.0, 0.8294399451523027, 0.165173759743218, 0.165173759743218, 0.2715492951163915], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.95799947], dtype=float32), -0.41314957]. 
=============================================
[2019-04-10 12:28:54,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.0277757e-26 8.9287745e-27 5.3479039e-22 1.0979405e-17], sum to 1.0000
[2019-04-10 12:28:54,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9500
[2019-04-10 12:28:54,977] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.73333333333333, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7922268558895516, 6.9112, 6.9112, 168.912956510431, 663884.479673243, 663884.479673243, 200237.4665909382], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7586400.0000, 
sim time next is 7587000.0000, 
raw observation next is [26.6, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.79712910569625, 6.9112, 6.9112, 168.912956510431, 667378.291899461, 667378.291899461, 201226.0518548288], 
processed observation next is [0.0, 0.8260869565217391, 0.4597156398104266, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7525964703612804, 0.0, 0.0, 0.8294399451523027, 0.18538285886096137, 0.18538285886096137, 0.3003373908281027], 
reward next is 0.6997, 
noisyNet noise sample is [array([0.706351], dtype=float32), 1.2640365]. 
=============================================
[2019-04-10 12:28:54,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.72446]
 [71.7075 ]
 [71.66582]
 [71.58946]
 [71.57161]], R is [[71.70503998]
 [71.68913269]
 [71.67481995]
 [71.66215515]
 [71.65116119]].
[2019-04-10 12:28:55,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 3.6561172e-23 1.8409947e-22 2.2046416e-16 4.1287831e-10], sum to 1.0000
[2019-04-10 12:28:55,671] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7874
[2019-04-10 12:28:55,676] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.23333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8180642050289684, 6.9112, 6.9112, 168.912956510431, 698227.8911076504, 698227.8911076504, 205779.4463208021], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7620600.0000, 
sim time next is 7621200.0000, 
raw observation next is [23.2, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7735948378935193, 6.9112, 6.9112, 168.912956510431, 660530.4493628057, 660530.4493628057, 196654.3149188465], 
processed observation next is [1.0, 0.21739130434782608, 0.29857819905213273, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7238961437725846, 0.0, 0.0, 0.8294399451523027, 0.18348068037855714, 0.18348068037855714, 0.29351390286395], 
reward next is 0.7065, 
noisyNet noise sample is [array([1.123484], dtype=float32), 1.4266444]. 
=============================================
[2019-04-10 12:28:55,921] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 8.6541069e-25 5.2546763e-23 1.0007545e-17 3.4680017e-10], sum to 1.0000
[2019-04-10 12:28:55,928] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7670
[2019-04-10 12:28:55,933] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.7, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7252716267193908, 6.911200000000001, 6.9112, 168.912956510431, 616986.5242341505, 616986.52423415, 187329.1429037053], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7624800.0000, 
sim time next is 7625400.0000, 
raw observation next is [23.8, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8026423354009748, 6.9112, 6.9112, 168.912956510431, 682312.3596196418, 682312.3596196418, 202533.8564614881], 
processed observation next is [1.0, 0.2608695652173913, 0.3270142180094788, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.759319921220701, 0.0, 0.0, 0.8294399451523027, 0.18953121100545606, 0.18953121100545606, 0.302289338002221], 
reward next is 0.6977, 
noisyNet noise sample is [array([-0.68190354], dtype=float32), 2.4192066]. 
=============================================
[2019-04-10 12:29:03,459] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207495: loss -46.5935
[2019-04-10 12:29:03,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207496: learning rate 0.0001
[2019-04-10 12:29:03,537] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207544: loss 2.4919
[2019-04-10 12:29:03,539] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207545: learning rate 0.0001
[2019-04-10 12:29:03,646] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207612: loss -31.1717
[2019-04-10 12:29:03,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207612: learning rate 0.0001
[2019-04-10 12:29:03,690] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207641: loss -43.6863
[2019-04-10 12:29:03,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207642: learning rate 0.0001
[2019-04-10 12:29:03,699] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207645: loss -29.0683
[2019-04-10 12:29:03,700] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207646: learning rate 0.0001
[2019-04-10 12:29:03,717] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207656: loss 7.7946
[2019-04-10 12:29:03,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207656: learning rate 0.0001
[2019-04-10 12:29:03,720] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207656: loss -18.0140
[2019-04-10 12:29:03,724] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207657: learning rate 0.0001
[2019-04-10 12:29:03,734] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207662: loss -23.6121
[2019-04-10 12:29:03,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207662: learning rate 0.0001
[2019-04-10 12:29:04,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2285742e-05 5.7006118e-17 8.1965250e-16 4.7721631e-12 9.9997771e-01], sum to 1.0000
[2019-04-10 12:29:04,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8073
[2019-04-10 12:29:04,037] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.83333333333334, 88.33333333333333, 1.0, 2.0, 0.2131620723463146, 1.0, 2.0, 0.2131620723463146, 1.0, 2.0, 0.3591547562793884, 6.911200000000001, 6.9112, 170.5573041426782, 893661.0023635273, 893661.0023635267, 272925.8812231348], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7789200.0000, 
sim time next is 7789800.0000, 
raw observation next is [25.76666666666667, 88.66666666666667, 1.0, 2.0, 0.2107358498234264, 1.0, 2.0, 0.2107358498234264, 1.0, 2.0, 0.3549493765812188, 6.9112, 6.9112, 170.5573041426782, 883485.1134800512, 883485.1134800512, 272226.924895158], 
processed observation next is [1.0, 0.13043478260869565, 0.42022116903633505, 0.8866666666666667, 1.0, 1.0, 0.04907933713665832, 1.0, 1.0, 0.04907933713665832, 1.0, 1.0, 0.213352898269779, 0.0, 0.0, 0.8375144448122397, 0.24541253152223644, 0.24541253152223644, 0.40630884312710147], 
reward next is 0.5937, 
noisyNet noise sample is [array([-0.5381368], dtype=float32), -1.22439]. 
=============================================
[2019-04-10 12:29:04,111] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207897: loss 34.7395
[2019-04-10 12:29:04,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207899: learning rate 0.0001
[2019-04-10 12:29:04,361] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208053: loss 1.3435
[2019-04-10 12:29:04,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208053: learning rate 0.0001
[2019-04-10 12:29:04,463] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208114: loss 45.4032
[2019-04-10 12:29:04,464] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208114: learning rate 0.0001
[2019-04-10 12:29:04,521] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208150: loss 1.1834
[2019-04-10 12:29:04,525] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208152: learning rate 0.0001
[2019-04-10 12:29:04,586] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208188: loss 9.0201
[2019-04-10 12:29:04,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208189: learning rate 0.0001
[2019-04-10 12:29:05,223] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.6310292e-08 2.3658398e-16 1.6841343e-15 3.3110026e-10 1.0000000e+00], sum to 1.0000
[2019-04-10 12:29:05,231] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7599
[2019-04-10 12:29:05,234] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5466838231674485, 1.0, 2.0, 0.5466838231674485, 1.0, 2.0, 0.9410628629446954, 6.9112, 6.9112, 170.5573041426782, 2293401.843411985, 2293401.843411985, 447260.7868602994], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7833600.0000, 
sim time next is 7834200.0000, 
raw observation next is [30.81666666666667, 64.16666666666667, 1.0, 2.0, 0.5255198111123108, 1.0, 2.0, 0.5255198111123108, 1.0, 2.0, 0.9042486950068166, 6.911200000000001, 6.9112, 170.5573041426782, 2204536.868909821, 2204536.86890982, 431632.569900471], 
processed observation next is [1.0, 0.6956521739130435, 0.6595576619273303, 0.6416666666666667, 1.0, 1.0, 0.42833712182206124, 1.0, 1.0, 0.42833712182206124, 1.0, 1.0, 0.8832301158619713, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6123713524749502, 0.61237135247495, 0.6442277162693597], 
reward next is 0.3558, 
noisyNet noise sample is [array([1.1523062], dtype=float32), -0.037930615]. 
=============================================
[2019-04-10 12:29:05,304] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208628: loss -12.6373
[2019-04-10 12:29:05,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208628: learning rate 0.0001
[2019-04-10 12:29:05,360] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208659: loss 0.9508
[2019-04-10 12:29:05,363] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208659: learning rate 0.0001
[2019-04-10 12:29:06,066] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 209096: loss 3.1108
[2019-04-10 12:29:06,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 209097: learning rate 0.0001
[2019-04-10 12:29:10,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:10,821] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:10,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-04-10 12:29:10,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:10,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:10,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-04-10 12:29:10,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:10,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:10,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-04-10 12:29:11,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-04-10 12:29:11,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,029] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-04-10 12:29:11,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-04-10 12:29:11,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-04-10 12:29:11,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-04-10 12:29:11,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-04-10 12:29:11,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-04-10 12:29:11,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,506] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,506] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-04-10 12:29:11,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,584] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-04-10 12:29:11,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-04-10 12:29:11,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-04-10 12:29:11,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,782] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-04-10 12:29:11,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:29:11,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:11,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-04-10 12:29:15,779] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 7.5129698e-25 2.4240034e-25 1.3095564e-21 2.8718803e-13], sum to 1.0000
[2019-04-10 12:29:15,786] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2652
[2019-04-10 12:29:15,790] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.1, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6749011622021226, 6.9112, 6.9112, 168.912956510431, 579913.6199351875, 579913.6199351875, 178313.3632042666], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [24.93333333333333, 77.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6680483166029633, 6.911199999999999, 6.9112, 168.912956510431, 574435.8514086513, 574435.851408652, 177136.1809778212], 
processed observation next is [1.0, 0.782608695652174, 0.38072669826224315, 0.7766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5951808739060527, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15956551428018093, 0.15956551428018112, 0.26438235966838985], 
reward next is 0.7356, 
noisyNet noise sample is [array([-0.45028102], dtype=float32), 0.7259436]. 
=============================================
[2019-04-10 12:29:18,214] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 7.8212991e-22 9.2940477e-22 2.4225144e-18 4.1918827e-10], sum to 1.0000
[2019-04-10 12:29:18,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9306
[2019-04-10 12:29:18,229] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.73333333333333, 91.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6373911899239041, 6.911200000000001, 6.9112, 168.912956510431, 550705.3616051899, 550705.3616051892, 172015.1603082792], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 108600.0000, 
sim time next is 109200.0000, 
raw observation next is [22.76666666666667, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6307271454710516, 6.9112, 6.9112, 168.912956510431, 544761.4352237931, 544761.4352237931, 170940.8706362358], 
processed observation next is [1.0, 0.2608695652173913, 0.2780410742496052, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5496672505744531, 0.0, 0.0, 0.8294399451523027, 0.15132262089549808, 0.15132262089549808, 0.2551356278152773], 
reward next is 0.7449, 
noisyNet noise sample is [array([-0.89151853], dtype=float32), 1.4650829]. 
=============================================
[2019-04-10 12:29:21,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.9189845e-24 1.8834711e-25 1.2020328e-21 1.2258614e-15], sum to 1.0000
[2019-04-10 12:29:21,509] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-10 12:29:21,512] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.26666666666667, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5288443731972283, 6.9112, 6.9112, 168.912956510431, 466795.059711286, 466795.059711286, 155763.9742882101], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [20.3, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5296652954683974, 6.9112, 6.9112, 168.912956510431, 467461.3572708468, 467461.3572708468, 155875.0827940959], 
processed observation next is [0.0, 0.30434782608695654, 0.16113744075829392, 0.935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4264210920346309, 0.0, 0.0, 0.8294399451523027, 0.12985037701967966, 0.12985037701967966, 0.23264937730462076], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.6186038], dtype=float32), 1.0575668]. 
=============================================
[2019-04-10 12:29:23,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 6.8567531e-26 6.8412036e-27 1.6574927e-23 1.6939326e-17], sum to 1.0000
[2019-04-10 12:29:23,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8226
[2019-04-10 12:29:23,224] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.11666666666667, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5783628081509081, 6.9112, 6.9112, 168.912956510431, 506100.4631545788, 506100.4631545788, 162797.0272298123], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 222600.0000, 
sim time next is 223200.0000, 
raw observation next is [22.2, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5781552716813843, 6.9112, 6.9112, 168.912956510431, 505721.5080215401, 505721.5080215401, 162771.6652833342], 
processed observation next is [0.0, 0.6086956521739131, 0.2511848341232228, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48555520936754176, 0.0, 0.0, 0.8294399451523027, 0.14047819667265005, 0.14047819667265005, 0.24294278400497643], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.4904859], dtype=float32), -2.20006]. 
=============================================
[2019-04-10 12:29:24,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 9.7707227e-26 2.9117486e-27 1.4425891e-23 9.4595018e-17], sum to 1.0000
[2019-04-10 12:29:24,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4299
[2019-04-10 12:29:24,876] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5197053530690805, 6.911199999999999, 6.9112, 168.912956510431, 458531.0327374417, 458531.0327374424, 154572.2567901608], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 264600.0000, 
sim time next is 265200.0000, 
raw observation next is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5192829102648093, 6.9112, 6.9112, 168.912956510431, 458158.2345127119, 458158.2345127119, 154517.3587339385], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41375964666440157, 0.0, 0.0, 0.8294399451523027, 0.12726617625353107, 0.12726617625353107, 0.2306229234834903], 
reward next is 0.7694, 
noisyNet noise sample is [array([0.65253484], dtype=float32), -0.7410239]. 
=============================================
[2019-04-10 12:29:29,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.7082489e-23 7.6506105e-24 2.2245575e-20 2.1269720e-12], sum to 1.0000
[2019-04-10 12:29:29,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0566
[2019-04-10 12:29:29,603] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.2, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4699680779368596, 6.9112, 6.9112, 168.912956510431, 417437.4633412592, 417437.4633412592, 148316.302975383], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [20.25, 87.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4708413482603907, 6.9112, 6.9112, 168.912956510431, 418251.4206480645, 418251.4206480645, 148416.0307794948], 
processed observation next is [1.0, 0.21739130434782608, 0.1587677725118484, 0.875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35468457104925694, 0.0, 0.0, 0.8294399451523027, 0.11618095018001792, 0.11618095018001792, 0.2215164638499922], 
reward next is 0.7785, 
noisyNet noise sample is [array([-0.58788055], dtype=float32), 0.5194803]. 
=============================================
[2019-04-10 12:29:33,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.9135778e-24 7.0173214e-26 2.0931202e-21 4.7520207e-13], sum to 1.0000
[2019-04-10 12:29:33,728] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3479
[2019-04-10 12:29:33,733] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.7, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4522250132062239, 6.9112, 6.9112, 168.912956510431, 404076.2279371776, 404076.2279371776, 146156.3836156132], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 429600.0000, 
sim time next is 430200.0000, 
raw observation next is [19.7, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.451436257672818, 6.911200000000001, 6.9112, 168.912956510431, 403372.8761155003, 403372.8761154997, 146068.6561529763], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3310198264302658, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11204802114319452, 0.11204802114319436, 0.2180129196313079], 
reward next is 0.7820, 
noisyNet noise sample is [array([1.6498454], dtype=float32), 0.91084445]. 
=============================================
[2019-04-10 12:29:34,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.5552323e-24 3.1009051e-24 5.3696159e-21 5.7095864e-13], sum to 1.0000
[2019-04-10 12:29:34,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4442
[2019-04-10 12:29:34,778] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4311064937535722, 6.9112, 6.9112, 168.912956510431, 386033.3361524688, 386033.3361524688, 143811.1894109145], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 441600.0000, 
sim time next is 442200.0000, 
raw observation next is [19.6, 84.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4284981050419669, 6.911200000000001, 6.9112, 168.912956510431, 383754.6070810872, 383754.6070810866, 143533.0703700372], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.8416666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3030464695633742, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10659850196696867, 0.1065985019669685, 0.2142284632388615], 
reward next is 0.7858, 
noisyNet noise sample is [array([0.10911004], dtype=float32), -0.2993991]. 
=============================================
[2019-04-10 12:29:37,408] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 12:29:37,409] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:29:37,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:37,415] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-04-10 12:29:37,415] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:29:37,441] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:29:37,443] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:37,453] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-04-10 12:29:37,443] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:37,476] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-04-10 12:29:37,443] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:29:37,497] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:37,499] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-04-10 12:29:37,497] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:29:37,516] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:29:37,523] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-04-10 12:29:43,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:29:43,038] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.33333333333333, 44.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3981205645424735, 6.911199999999999, 6.9112, 168.912956510431, 360062.2333015347, 360062.2333015353, 140163.0463584264]
[2019-04-10 12:29:43,039] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:29:43,062] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.0210946e-24 1.4572337e-25 7.1927681e-22 1.1821007e-16], sampled 0.4533390498410974
[2019-04-10 12:29:43,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:29:43,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.90592723666667, 67.55827561333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4461651736036419, 6.9112, 6.9112, 168.912956510431, 399708.2115570548, 399708.2115570548, 145415.2320790293]
[2019-04-10 12:29:43,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:29:43,210] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 5.1552214e-24 5.1238301e-25 3.0278526e-21 9.8694437e-16], sampled 0.2950867399349355
[2019-04-10 12:30:01,749] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:01,750] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.811845346194779, 6.9112, 6.9112, 168.912956510431, 683501.997988747, 683501.997988747, 204355.8127045074]
[2019-04-10 12:30:01,751] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:30:01,752] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.2454874e-21 8.2496645e-22 7.6518012e-18 2.3456355e-11], sampled 0.29044636808326174
[2019-04-10 12:30:34,677] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:34,678] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.6460253911139731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 902806.8294009389, 902806.8294009395, 209623.9441562719]
[2019-04-10 12:30:34,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:30:34,680] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.6296185e-01 1.4020575e-16 2.1388696e-16 2.6566928e-12 3.7038188e-02], sampled 0.6749871332710448
[2019-04-10 12:30:34,682] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 902806.8294009389 W.
[2019-04-10 12:30:34,809] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:34,810] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.9, 57.0, 1.0, 2.0, 0.5293004784924044, 1.0, 2.0, 0.5293004784924044, 1.0, 2.0, 0.919219888798265, 6.9112, 6.9112, 169.0403247858759, 2220429.621556726, 2220429.621556726, 435699.9162917897]
[2019-04-10 12:30:34,811] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:30:34,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5196407e-01 1.4141920e-19 8.6588426e-20 7.3766615e-14 3.4803596e-01], sampled 0.26440204776244447
[2019-04-10 12:30:34,813] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2220429.621556726 W.
[2019-04-10 12:30:39,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:39,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.16988421666667, 74.11473927666667, 1.0, 2.0, 0.826256487048425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564385846, 1154812.925060224, 1154812.925060225, 250462.6746487024]
[2019-04-10 12:30:39,807] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:30:39,809] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.4568069e-23 1.1691806e-23 3.8718003e-19 8.1289047e-12], sampled 0.26006257975900937
[2019-04-10 12:30:39,811] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1154812.925060224 W.
[2019-04-10 12:30:40,514] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:40,514] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.87898562, 87.62517113999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9199521288197554, 6.9112, 6.9112, 168.912956510431, 755285.6918795053, 755285.6918795053, 227953.2699604584]
[2019-04-10 12:30:40,515] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:30:40,517] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.2106991e-27 4.9750881e-29 1.2396814e-24 1.9447341e-18], sampled 0.4357794965997558
[2019-04-10 12:30:48,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:48,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.97897572, 75.172355755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7364874165897752, 6.911199999999999, 6.9112, 168.912956510431, 629019.0368934305, 629019.0368934311, 189449.824761217]
[2019-04-10 12:30:48,278] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:30:48,280] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.7219804e-25 3.1900244e-26 4.5451000e-22 2.2104860e-16], sampled 0.9548593780049871
[2019-04-10 12:30:51,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:51,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.03333333333333, 86.33333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8710883729486766, 6.9112, 6.9112, 168.912956510431, 717415.9823265229, 717415.9823265229, 216763.3240279568]
[2019-04-10 12:30:51,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:30:51,503] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.6548707e-23 5.3288169e-24 5.6638835e-19 1.8756354e-09], sampled 0.35547525208629205
[2019-04-10 12:30:59,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:30:59,843] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.29369300833333, 98.456098575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.724641288887312, 6.9112, 6.9112, 168.912956510431, 617114.1648497508, 617114.1648497508, 187217.140010297]
[2019-04-10 12:30:59,844] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:30:59,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.6793617e-22 4.4233958e-23 2.5708158e-19 4.0692111e-13], sampled 0.2904179692828983
[2019-04-10 12:31:02,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07103223], dtype=float32), 0.038916245]
[2019-04-10 12:31:02,385] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.06666666666666, 89.0, 1.0, 2.0, 0.7769095113845937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1085808.127590084, 1085808.127590085, 238324.86808242]
[2019-04-10 12:31:02,386] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:31:02,389] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.85614467e-01 3.34027099e-16 3.00084318e-16 1.25577925e-11
 3.14385504e-01], sampled 0.8927803266112273
[2019-04-10 12:31:04,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7479.9558 3107386032.0282 1658.0000
[2019-04-10 12:31:04,724] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7018.6729 3197249827.7172 2334.0000
[2019-04-10 12:31:04,746] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7896.5750 3000720911.2272 1499.0000
[2019-04-10 12:31:04,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8113.9260 2945477591.0577 1135.0000
[2019-04-10 12:31:04,860] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7237.9970 3332905678.1852 2079.0000
[2019-04-10 12:31:05,874] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 225000, evaluation results [225000.0, 7237.9970460969635, 3332905678.1852026, 2079.0, 7479.955801536724, 3107386032.028151, 1658.0, 8113.926045306119, 2945477591.0577035, 1135.0, 7018.672923635853, 3197249827.717249, 2334.0, 7896.574977039754, 3000720911.227186, 1499.0]
[2019-04-10 12:31:06,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 7.2277031e-21 8.3772490e-21 1.1849675e-16 3.2721257e-08], sum to 1.0000
[2019-04-10 12:31:06,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-10 12:31:06,830] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.86666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4304468509604041, 6.911199999999999, 6.9112, 168.912956510431, 386578.0685461686, 386578.0685461692, 143655.6061347921], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 516000.0000, 
sim time next is 516600.0000, 
raw observation next is [18.85, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4294395550112745, 6.9112, 6.9112, 168.912956510431, 385719.082205185, 385719.082205185, 143546.2576579197], 
processed observation next is [1.0, 1.0, 0.09241706161137453, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3041945792820421, 0.0, 0.0, 0.8294399451523027, 0.10714418950144028, 0.10714418950144028, 0.2142481457580891], 
reward next is 0.7858, 
noisyNet noise sample is [array([1.0205755], dtype=float32), -1.0794816]. 
=============================================
[2019-04-10 12:31:07,215] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.2321037e-23 2.1434426e-22 3.3054693e-17 2.5643352e-09], sum to 1.0000
[2019-04-10 12:31:07,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-10 12:31:07,229] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.78333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4227328471180814, 6.9112, 6.9112, 168.912956510431, 379870.5742341023, 379870.5742341023, 142835.3955875177], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 519000.0000, 
sim time next is 519600.0000, 
raw observation next is [18.76666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4213352835277946, 6.911200000000001, 6.9112, 168.912956510431, 378658.161109756, 378658.1611097553, 142688.2175878654], 
processed observation next is [1.0, 0.0, 0.08846761453396543, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.29431132137535926, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10518282253048777, 0.10518282253048758, 0.21296748893711254], 
reward next is 0.7870, 
noisyNet noise sample is [array([0.14119893], dtype=float32), -1.0144717]. 
=============================================
[2019-04-10 12:31:13,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1589821e-02 5.6871863e-17 2.2033044e-17 2.1989280e-11 9.8841023e-01], sum to 1.0000
[2019-04-10 12:31:13,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0616
[2019-04-10 12:31:13,488] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 53.33333333333334, 1.0, 2.0, 0.1882818517421136, 1.0, 2.0, 0.1882818517421136, 1.0, 2.0, 0.3451150420133368, 6.9112, 6.9112, 170.5573041426782, 918850.2231812226, 918850.2231812226, 279869.64837676], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [24.7, 53.5, 1.0, 2.0, 0.2006805094168115, 1.0, 2.0, 0.2006805094168115, 1.0, 2.0, 0.3676329811852029, 6.9112, 6.9112, 170.5573041426782, 978238.1216292833, 978238.1216292833, 283575.0484164411], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.535, 1.0, 1.0, 0.03696446917688132, 1.0, 1.0, 0.03696446917688132, 1.0, 1.0, 0.22882070876244254, 0.0, 0.0, 0.8375144448122397, 0.2717328115636898, 0.2717328115636898, 0.4232463409200613], 
reward next is 0.5768, 
noisyNet noise sample is [array([0.2491403], dtype=float32), -0.16483925]. 
=============================================
[2019-04-10 12:31:15,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.5116604e-21 2.2781879e-21 5.7497913e-17 1.4881334e-10], sum to 1.0000
[2019-04-10 12:31:15,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1356
[2019-04-10 12:31:15,734] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.26666666666667, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4195625049736318, 6.911200000000001, 6.9112, 168.912956510431, 377038.4691451734, 377038.4691451727, 142509.1123618307], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 714000.0000, 
sim time next is 714600.0000, 
raw observation next is [19.5, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.421011282702985, 6.911199999999999, 6.9112, 168.912956510431, 378105.1292275349, 378105.1292275355, 142676.4877266204], 
processed observation next is [1.0, 0.2608695652173913, 0.12322274881516594, 0.825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2939161984182744, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10502920256320415, 0.1050292025632043, 0.212949981681523], 
reward next is 0.7871, 
noisyNet noise sample is [array([1.2322297], dtype=float32), 0.2243219]. 
=============================================
[2019-04-10 12:31:17,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.99999881e-01 6.74240976e-21 2.38773972e-20 2.15539667e-15
 1.04469216e-07], sum to 1.0000
[2019-04-10 12:31:17,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0523
[2019-04-10 12:31:17,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 951200.2934678276 W.
[2019-04-10 12:31:17,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333334, 50.0, 1.0, 2.0, 0.1940643976411088, 1.0, 2.0, 0.1940643976411088, 1.0, 1.0, 0.356434216432222, 6.9112, 6.9112, 170.5573041426782, 951200.2934678276, 951200.2934678276, 281613.0773589438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [24.85, 50.0, 1.0, 2.0, 0.5774673582308113, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 951198.3945357748, 951198.3945357755, 210845.5835166563], 
processed observation next is [1.0, 0.6956521739130435, 0.37677725118483424, 0.5, 1.0, 1.0, 0.49092452798892927, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2642217762599375, 0.26422177625993765, 0.3146949007711288], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6320212], dtype=float32), -0.85560566]. 
=============================================
[2019-04-10 12:31:19,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.9077740e-26 1.4534577e-26 9.0377609e-24 9.8177690e-17], sum to 1.0000
[2019-04-10 12:31:19,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2497
[2019-04-10 12:31:19,110] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.73333333333333, 79.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4687162961363223, 6.9112, 6.9112, 168.912956510431, 418193.4120498577, 418193.4120498577, 148066.1551432626], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 763800.0000, 
sim time next is 764400.0000, 
raw observation next is [20.66666666666667, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4698991978359895, 6.9112, 6.9112, 168.912956510431, 419233.0722709419, 419233.0722709419, 148204.2001329629], 
processed observation next is [1.0, 0.8695652173913043, 0.17851500789889443, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35353560711706034, 0.0, 0.0, 0.8294399451523027, 0.11645363118637275, 0.11645363118637275, 0.22120029870591476], 
reward next is 0.7788, 
noisyNet noise sample is [array([1.2947904], dtype=float32), 0.00811952]. 
=============================================
[2019-04-10 12:31:19,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.1261934e-23 6.9850123e-24 4.6917616e-20 5.0434516e-13], sum to 1.0000
[2019-04-10 12:31:19,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7688
[2019-04-10 12:31:19,745] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.4, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4763041481993923, 6.9112, 6.9112, 168.912956510431, 424217.3553294067, 424217.3553294067, 148996.3897842847], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [19.4, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4774180325073318, 6.911200000000001, 6.9112, 168.912956510431, 425209.6171913288, 425209.6171913282, 149127.8376406992], 
processed observation next is [0.0, 0.13043478260869565, 0.11848341232227487, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.362704917691868, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11811378255314689, 0.11811378255314672, 0.22257886215029732], 
reward next is 0.7774, 
noisyNet noise sample is [array([-1.4849027], dtype=float32), -1.0200536]. 
=============================================
[2019-04-10 12:31:21,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.5716389e-28 1.5041077e-29 1.3965775e-26 4.4999542e-21], sum to 1.0000
[2019-04-10 12:31:21,499] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0459
[2019-04-10 12:31:21,506] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5448206345712833, 6.911200000000001, 6.9112, 168.912956510431, 478765.4867511411, 478765.4867511405, 157990.1913123926], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 832800.0000, 
sim time next is 833400.0000, 
raw observation next is [24.25, 68.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5462848799578471, 6.911199999999999, 6.9112, 168.912956510431, 479914.674252031, 479914.6742520316, 158195.3230036377], 
processed observation next is [0.0, 0.6521739130434783, 0.3483412322274882, 0.685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44668887799737456, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1333096317366753, 0.13330963173667545, 0.2361124223934891], 
reward next is 0.7639, 
noisyNet noise sample is [array([-0.72437596], dtype=float32), 0.971165]. 
=============================================
[2019-04-10 12:31:21,543] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 7.2942045e-29 2.3349177e-30 1.5744315e-25 3.5092670e-19], sum to 1.0000
[2019-04-10 12:31:21,549] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7412
[2019-04-10 12:31:21,553] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.2, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5462197537316721, 6.9112, 6.9112, 168.912956510431, 479727.3012637906, 479727.3012637906, 158190.4080404961], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [24.15, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5466985247612454, 6.911199999999999, 6.9112, 168.912956510431, 480020.9036320557, 480020.9036320564, 158260.0684337316], 
processed observation next is [0.0, 0.6521739130434783, 0.34360189573459715, 0.695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4471933228795675, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13333913989779325, 0.13333913989779345, 0.23620905736377848], 
reward next is 0.7638, 
noisyNet noise sample is [array([1.1228112], dtype=float32), -0.6603126]. 
=============================================
[2019-04-10 12:31:25,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.00000000e+00 4.87793566e-27 3.27757647e-28 9.72043322e-24
 1.33679956e-17], sum to 1.0000
[2019-04-10 12:31:25,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7863
[2019-04-10 12:31:25,525] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.86666666666667, 66.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5554373945133564, 6.9112, 6.9112, 168.912956510431, 486547.4680176543, 486547.4680176543, 159504.545544869], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 913800.0000, 
sim time next is 914400.0000, 
raw observation next is [25.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5563245382366822, 6.911200000000001, 6.9112, 168.912956510431, 487224.2454706425, 487224.2454706419, 159631.441449538], 
processed observation next is [0.0, 0.6086956521739131, 0.38388625592417064, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4589323637032709, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13534006818628957, 0.1353400681862894, 0.23825588276050447], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.7375872], dtype=float32), -1.187292]. 
=============================================
[2019-04-10 12:31:28,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 9.8764084e-24 3.4948719e-23 1.9017726e-18 4.3693497e-13], sum to 1.0000
[2019-04-10 12:31:28,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7037
[2019-04-10 12:31:28,381] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5941734604393879, 6.911199999999999, 6.9112, 168.912956510431, 516468.1102545601, 516468.1102545608, 165213.3266816037], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 979200.0000, 
sim time next is 979800.0000, 
raw observation next is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6181089625548146, 6.911199999999999, 6.9112, 168.912956510431, 537278.6143519953, 537278.6143519959, 168894.7038893374], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5342792226278227, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14924405954222092, 0.14924405954222109, 0.25208164759602597], 
reward next is 0.7479, 
noisyNet noise sample is [array([-1.4739394], dtype=float32), -1.0734785]. 
=============================================
[2019-04-10 12:31:29,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9956447e-01 2.0319161e-16 1.8503065e-16 4.2127128e-13 4.3547648e-04], sum to 1.0000
[2019-04-10 12:31:29,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-10 12:31:29,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 935456.3973617879 W.
[2019-04-10 12:31:29,205] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.2050484818026003, 1.0, 2.0, 0.2050484818026003, 1.0, 1.0, 0.3613382053643222, 6.9112, 6.9112, 170.5573041426782, 935456.3973617879, 935456.3973617879, 279919.0437727129], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1001400.0000, 
sim time next is 1002000.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.2949408503059498, 1.0, 2.0, 0.2949408503059498, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 900557.9644609852, 900557.9644609852, 258608.5860308277], 
processed observation next is [1.0, 0.6086956521739131, 0.22274881516587688, 0.96, 1.0, 1.0, 0.15053114494692746, 1.0, 1.0, 0.15053114494692746, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2501549901280514, 0.2501549901280514, 0.38598296422511597], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0138603], dtype=float32), 1.1119796]. 
=============================================
[2019-04-10 12:31:29,218] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[57.880363]
 [57.49482 ]
 [56.631622]
 [58.541267]
 [58.053665]], R is [[57.21244812]
 [57.22253418]
 [57.26516342]
 [57.30822754]
 [57.36343765]].
[2019-04-10 12:31:30,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999988e-01 2.9191100e-20 2.6521675e-20 1.4296059e-15 1.0419142e-07], sum to 1.0000
[2019-04-10 12:31:30,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2709
[2019-04-10 12:31:30,298] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.3, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6622508661880673, 6.9112, 6.9112, 168.912956510431, 570211.8655197289, 570211.8655197289, 176148.216799224], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1038600.0000, 
sim time next is 1039200.0000, 
raw observation next is [22.33333333333333, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6644440148083247, 6.9112, 6.9112, 168.912956510431, 571882.005666602, 571882.005666602, 176520.7402666984], 
processed observation next is [1.0, 0.0, 0.2575039494470772, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5907853839125911, 0.0, 0.0, 0.8294399451523027, 0.15885611268516725, 0.15885611268516725, 0.26346379144283344], 
reward next is 0.7365, 
noisyNet noise sample is [array([-0.6755508], dtype=float32), -0.33411562]. 
=============================================
[2019-04-10 12:31:31,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.9582435e-20 2.4459866e-20 9.5801756e-17 3.7848157e-08], sum to 1.0000
[2019-04-10 12:31:31,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8661
[2019-04-10 12:31:31,485] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.85, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5568144087775558, 6.9112, 6.9112, 168.912956510431, 488194.5129736612, 488194.5129736612, 159685.399976887], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1065000.0000, 
sim time next is 1065600.0000, 
raw observation next is [20.9, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5619427961256707, 6.9112, 6.9112, 168.912956510431, 492449.6841941559, 492449.6841941559, 160414.9690607733], 
processed observation next is [1.0, 0.34782608695652173, 0.1895734597156398, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4657838977142325, 0.0, 0.0, 0.8294399451523027, 0.1367915789428211, 0.1367915789428211, 0.23942532695637805], 
reward next is 0.7606, 
noisyNet noise sample is [array([0.7590312], dtype=float32), -0.5237294]. 
=============================================
[2019-04-10 12:31:35,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.5757326e-21 8.1912439e-22 1.3344870e-16 3.6153558e-08], sum to 1.0000
[2019-04-10 12:31:35,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7713
[2019-04-10 12:31:35,129] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.9, 94.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4978351957573227, 6.9112, 6.9112, 168.912956510431, 440664.0383251192, 440664.0383251192, 151734.8176618723], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [20.05, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5031750929391163, 6.9112, 6.9112, 168.912956510431, 445192.6284017785, 445192.6284017785, 152409.3691454873], 
processed observation next is [1.0, 0.21739130434782608, 0.14928909952606645, 0.935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3941159669989223, 0.0, 0.0, 0.8294399451523027, 0.12366461900049404, 0.12366461900049404, 0.22747667036639893], 
reward next is 0.7725, 
noisyNet noise sample is [array([-0.6211752], dtype=float32), 0.42152837]. 
=============================================
[2019-04-10 12:31:35,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.66859 ]
 [70.676254]
 [70.626854]
 [70.55302 ]
 [70.53751 ]], R is [[70.72780609]
 [70.79405975]
 [70.85977936]
 [70.92428589]
 [70.98795319]].
[2019-04-10 12:31:35,952] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4118690e-03 2.6237918e-16 5.3703621e-15 3.5749296e-11 9.9858814e-01], sum to 1.0000
[2019-04-10 12:31:35,960] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7456
[2019-04-10 12:31:35,964] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.2, 78.83333333333333, 1.0, 2.0, 0.322766938816045, 1.0, 1.0, 0.322766938816045, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1004297.014027228, 1004297.014027228, 266514.3941159696], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1155000.0000, 
sim time next is 1155600.0000, 
raw observation next is [23.4, 78.0, 1.0, 2.0, 0.2316483844475696, 1.0, 2.0, 0.2316483844475696, 1.0, 1.0, 0.4118120059113592, 6.911199999999999, 6.9112, 170.5573041426782, 1071582.486653656, 1071582.486653657, 289934.523596428], 
processed observation next is [1.0, 0.391304347826087, 0.30805687203791465, 0.78, 1.0, 1.0, 0.07427516198502361, 1.0, 1.0, 0.07427516198502361, 1.0, 0.5, 0.28269756818458436, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.29766180184823776, 0.29766180184823804, 0.4327380949200418], 
reward next is 0.5673, 
noisyNet noise sample is [array([0.83135027], dtype=float32), -2.1607816]. 
=============================================
[2019-04-10 12:31:36,096] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9638180e-03 7.2831665e-18 5.2036741e-18 1.1181275e-13 9.9503618e-01], sum to 1.0000
[2019-04-10 12:31:36,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1552
[2019-04-10 12:31:36,107] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.9, 71.66666666666667, 1.0, 2.0, 0.2508893188236547, 1.0, 2.0, 0.2508893188236547, 1.0, 2.0, 0.4411875075448324, 6.911199999999999, 6.9112, 170.5573041426782, 1140960.893056794, 1140960.893056795, 294843.9557689018], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1160400.0000, 
sim time next is 1161000.0000, 
raw observation next is [25.1, 71.0, 1.0, 2.0, 0.2562580966168747, 1.0, 2.0, 0.2562580966168747, 1.0, 2.0, 0.4497608180697699, 6.9112, 6.9112, 170.5573041426782, 1161930.064448741, 1161930.064448741, 296465.660729765], 
processed observation next is [1.0, 0.43478260869565216, 0.38862559241706174, 0.71, 1.0, 1.0, 0.1039254176106924, 1.0, 1.0, 0.1039254176106924, 1.0, 1.0, 0.3289766074021584, 0.0, 0.0, 0.8375144448122397, 0.3227583512357614, 0.3227583512357614, 0.4424860607906941], 
reward next is 0.5575, 
noisyNet noise sample is [array([1.7664503], dtype=float32), -1.7371683]. 
=============================================
[2019-04-10 12:31:36,112] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.48286 ]
 [73.95958 ]
 [73.320496]
 [73.07059 ]
 [72.72603 ]], R is [[74.47867584]
 [74.29382324]
 [74.11212158]
 [73.93730164]
 [73.76071167]].
[2019-04-10 12:31:45,180] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 3.7296768e-24 6.7928936e-25 7.7546745e-19 6.0999617e-12], sum to 1.0000
[2019-04-10 12:31:45,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0249
[2019-04-10 12:31:45,193] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5609799279275384, 6.911200000000001, 6.9112, 168.912956510431, 492672.6424364992, 492672.6424364986, 160247.5616229053], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1360800.0000, 
sim time next is 1361400.0000, 
raw observation next is [20.93333333333333, 93.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5700606380525706, 6.9112, 6.9112, 168.912956510431, 500459.3537640856, 500459.3537640856, 161547.8388148783], 
processed observation next is [1.0, 0.782608695652174, 0.19115323854660338, 0.9316666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4756837049421592, 0.0, 0.0, 0.8294399451523027, 0.13901648715669046, 0.13901648715669046, 0.24111617733563928], 
reward next is 0.7589, 
noisyNet noise sample is [array([-0.40684527], dtype=float32), -0.7683014]. 
=============================================
[2019-04-10 12:31:45,287] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 8.3621623e-25 5.3237955e-25 1.0468257e-20 6.1421164e-13], sum to 1.0000
[2019-04-10 12:31:45,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6424
[2019-04-10 12:31:45,298] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.96666666666667, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.579726526112482, 6.911199999999999, 6.9112, 168.912956510431, 507438.32780335, 507438.3278033506, 162992.8125763692], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1372800.0000, 
sim time next is 1373400.0000, 
raw observation next is [20.95, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5781975155603529, 6.9112, 6.9112, 168.912956510431, 506182.9164492211, 506182.9164492211, 162766.9757065173], 
processed observation next is [1.0, 0.9130434782608695, 0.19194312796208532, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4856067262931133, 0.0, 0.0, 0.8294399451523027, 0.14060636568033918, 0.14060636568033918, 0.24293578463659296], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.4401413], dtype=float32), 0.8234721]. 
=============================================
[2019-04-10 12:31:47,634] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-10 12:31:47,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:31:47,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:47,637] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:31:47,638] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:47,639] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:31:47,639] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:47,639] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:31:47,640] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:47,640] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:31:47,642] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:31:47,652] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-04-10 12:31:47,667] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-04-10 12:31:47,684] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-04-10 12:31:47,700] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-04-10 12:31:47,719] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-04-10 12:31:57,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:31:57,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.4, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.619044800091426, 6.911200000000001, 6.9112, 168.912956510431, 539159.07280322, 539159.0728032193, 169022.8208339346]
[2019-04-10 12:31:57,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:31:57,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 6.8976622e-23 5.1849060e-23 1.2354364e-18 1.0761000e-11], sampled 0.7475433038706499
[2019-04-10 12:32:27,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:32:27,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.75872282166667, 90.52410666666668, 1.0, 1.0, 0.5904985115607108, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129002240004, 825179.0450276983, 825179.0450276976, 198987.7901656386]
[2019-04-10 12:32:27,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:32:27,265] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.9584303e-23 1.7515295e-23 2.9552020e-18 1.6093291e-10], sampled 0.4982368191201956
[2019-04-10 12:32:36,883] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:32:36,884] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.35300882, 90.1593489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5246161006987093, 6.911200000000001, 6.9112, 168.912956510431, 464340.2079050522, 464340.2079050516, 155152.9904431089]
[2019-04-10 12:32:36,885] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:32:36,888] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.6525295e-24 8.3765925e-25 3.9963179e-20 5.0279686e-13], sampled 0.6723629955457605
[2019-04-10 12:32:48,592] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:32:48,593] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.00662144666667, 90.11444481666666, 1.0, 1.0, 0.6007494761854603, 1.0, 1.0, 0.6007494761854603, 1.0, 2.0, 1.021084437284007, 6.911199999999999, 6.9112, 184.5923449428631, 2520248.9952929, 2520248.995292901, 491012.4553485069]
[2019-04-10 12:32:48,594] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:32:48,597] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8489791e-01 9.8310449e-17 2.6942498e-16 6.8090075e-11 5.1510209e-01], sampled 0.14404463687840918
[2019-04-10 12:32:48,598] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2520248.9952929 W.
[2019-04-10 12:33:06,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:33:06,541] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.40000000000001, 63.0, 1.0, 2.0, 0.7880945095650298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104262, 1101448.379699809, 1101448.379699808, 241019.4133327028]
[2019-04-10 12:33:06,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:33:06,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9995184e-01 2.7600610e-22 1.9919753e-22 2.3197120e-16 4.8123198e-05], sampled 0.73446687442046
[2019-04-10 12:33:06,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1101448.379699809 W.
[2019-04-10 12:33:07,718] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:33:07,720] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.45479182666667, 82.23546844666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9701629830678018, 6.9112, 6.9112, 168.912956510431, 788205.062093003, 788205.062093003, 239818.0561495743]
[2019-04-10 12:33:07,721] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:33:07,723] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 5.99908182e-31 6.46434319e-32 3.19192351e-26
 1.05860836e-19], sampled 0.5856432800736842
[2019-04-10 12:33:10,458] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07180206], dtype=float32), 0.04681704]
[2019-04-10 12:33:10,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.19969524, 87.08282115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8493031796298658, 6.9112, 6.9112, 168.912956510431, 708861.8657110467, 708861.8657110467, 212242.0195549059]
[2019-04-10 12:33:10,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:33:10,462] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 7.4144058e-24 3.7515373e-24 5.0916963e-19 2.2251532e-11], sampled 0.5718822183383891
[2019-04-10 12:33:27,067] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7062.3594 3209746267.3731 2219.0000
[2019-04-10 12:33:27,293] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.9319 3346117157.3085 1958.0000
[2019-04-10 12:33:27,454] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7496.9734 3114728570.8759 1616.0000
[2019-04-10 12:33:27,497] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8132.2987 2960477425.3220 1059.0000
[2019-04-10 12:33:27,605] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7873.8668 3018702372.5080 1440.0000
[2019-04-10 12:33:28,618] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 250000, evaluation results [250000.0, 7286.9319250106955, 3346117157.30852, 1958.0, 7496.973404093408, 3114728570.8758554, 1616.0, 8132.298667333763, 2960477425.3219566, 1059.0, 7062.359382594959, 3209746267.373092, 2219.0, 7873.866836308637, 3018702372.507994, 1440.0]
[2019-04-10 12:33:31,863] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.0783895e-25 1.5292364e-26 4.8717742e-22 2.9996855e-14], sum to 1.0000
[2019-04-10 12:33:31,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8313
[2019-04-10 12:33:31,875] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.33333333333334, 74.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6474794579323483, 6.9112, 6.9112, 168.912956510431, 557566.4620109147, 557566.4620109147, 173679.4472120746], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1500600.0000, 
sim time next is 1501200.0000, 
raw observation next is [25.6, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6498715252366635, 6.9112, 6.9112, 168.912956510431, 560039.5352985215, 560039.5352985215, 174072.9296752624], 
processed observation next is [0.0, 0.391304347826087, 0.4123222748815167, 0.72, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5730140551666627, 0.0, 0.0, 0.8294399451523027, 0.15556653758292263, 0.15556653758292263, 0.2598103427988991], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.5647365], dtype=float32), -0.31977466]. 
=============================================
[2019-04-10 12:33:34,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9999976e-01 3.2385394e-20 1.2089909e-20 1.0326493e-14 2.5324564e-07], sum to 1.0000
[2019-04-10 12:33:34,979] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7134
[2019-04-10 12:33:34,983] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.9, 85.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6740109701876545, 6.9112, 6.9112, 168.912956510431, 585719.1605597021, 585719.1605597021, 178098.6328158171], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1581000.0000, 
sim time next is 1581600.0000, 
raw observation next is [23.0, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7201599846540213, 6.9112, 6.9112, 168.912956510431, 625351.703891111, 625351.703891111, 186339.2627679073], 
processed observation next is [1.0, 0.30434782608695654, 0.28909952606635075, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6587316886024649, 0.0, 0.0, 0.8294399451523027, 0.17370880663641972, 0.17370880663641972, 0.2781183026386676], 
reward next is 0.7219, 
noisyNet noise sample is [array([1.5270721], dtype=float32), -0.46181065]. 
=============================================
[2019-04-10 12:33:35,171] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.21450043e-02 1.03136154e-17 1.54656967e-16 2.03848765e-12
 9.47855055e-01], sum to 1.0000
[2019-04-10 12:33:35,180] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0999
[2019-04-10 12:33:35,184] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.5, 85.0, 1.0, 2.0, 0.2272372590522657, 1.0, 2.0, 0.2272372590522657, 1.0, 2.0, 0.39528040458326, 6.9112, 6.9112, 170.5573041426782, 1016360.97182488, 1016360.97182488, 284680.101093077], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [23.51666666666667, 85.00000000000001, 1.0, 2.0, 0.261884718206215, 1.0, 2.0, 0.261884718206215, 1.0, 2.0, 0.4553503027587433, 6.9112, 6.9112, 170.5573041426782, 1170639.114895153, 1170639.114895153, 296749.7333040058], 
processed observation next is [1.0, 0.391304347826087, 0.31358609794628767, 0.8500000000000001, 1.0, 1.0, 0.11070447976652409, 1.0, 1.0, 0.11070447976652409, 1.0, 1.0, 0.33579305214480887, 0.0, 0.0, 0.8375144448122397, 0.3251775319153203, 0.3251775319153203, 0.4429100497074714], 
reward next is 0.5571, 
noisyNet noise sample is [array([-0.56774664], dtype=float32), 2.4401584]. 
=============================================
[2019-04-10 12:33:38,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.761186e-01 1.259721e-17 5.165176e-17 7.104337e-12 6.238814e-01], sum to 1.0000
[2019-04-10 12:33:38,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9840
[2019-04-10 12:33:38,412] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.33333333333334, 99.00000000000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2448590964223171, 6.911200000000001, 6.9112, 170.5573041426782, 619387.2691861233, 619387.2691861226, 247977.088880799], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1657200.0000, 
sim time next is 1657800.0000, 
raw observation next is [23.35, 99.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2476863118237464, 6.911200000000001, 6.9112, 170.5573041426782, 626423.824571575, 626423.8245715743, 248995.2763440237], 
processed observation next is [1.0, 0.17391304347826086, 0.3056872037914693, 0.99, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08254428271188585, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.17400661793654862, 0.17400661793654842, 0.3716347408119757], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41125336], dtype=float32), 0.72962433]. 
=============================================
[2019-04-10 12:33:39,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.4233032e-23 1.2978389e-22 3.5511496e-18 4.2573063e-12], sum to 1.0000
[2019-04-10 12:33:39,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3958
[2019-04-10 12:33:39,393] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.45, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8140042497097965, 6.911200000000001, 6.9112, 168.912956510431, 688882.3257555472, 688882.3257555466, 204867.0848343912], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1661400.0000, 
sim time next is 1662000.0000, 
raw observation next is [23.46666666666667, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8033430593259956, 6.9112, 6.9112, 168.912956510431, 679709.2318490242, 679709.2318490242, 202641.2248287609], 
processed observation next is [1.0, 0.21739130434782608, 0.31121642969984215, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7601744625926775, 0.0, 0.0, 0.8294399451523027, 0.18880811995806226, 0.18880811995806226, 0.30244958929665805], 
reward next is 0.6976, 
noisyNet noise sample is [array([1.2030147], dtype=float32), -0.5736995]. 
=============================================
[2019-04-10 12:33:39,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.199745]
 [65.94332 ]
 [65.8328  ]
 [65.52117 ]
 [65.586266]], R is [[66.55016327]
 [66.57888794]
 [66.61114502]
 [66.63217163]
 [66.68088531]].
[2019-04-10 12:33:42,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7346504e-01 1.5227447e-14 2.6525892e-13 8.9847568e-10 7.2653496e-01], sum to 1.0000
[2019-04-10 12:33:42,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1695
[2019-04-10 12:33:42,801] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2583657912340235, 6.9112, 6.9112, 170.5573041426782, 649592.9516025329, 649592.9516025329, 252378.1389331888], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1740600.0000, 
sim time next is 1741200.0000, 
raw observation next is [24.36666666666667, 94.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7612951708150942, 6.911200000000001, 6.9112, 168.912956510431, 639676.1341451551, 639676.1341451544, 194095.5715380517], 
processed observation next is [1.0, 0.13043478260869565, 0.3538704581358612, 0.94, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.708896549774505, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17768781504032086, 0.17768781504032066, 0.28969488289261447], 
reward next is 0.7103, 
noisyNet noise sample is [array([-2.4909637], dtype=float32), 0.22787844]. 
=============================================
[2019-04-10 12:33:43,613] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 8.6227011e-18 6.6436144e-18 1.5713744e-14 1.6742884e-08], sum to 1.0000
[2019-04-10 12:33:43,614] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1113
[2019-04-10 12:33:43,621] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 892389.9307546557 W.
[2019-04-10 12:33:43,624] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.03333333333333, 85.33333333333334, 1.0, 2.0, 0.5787381869452344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 892389.9307546557, 892389.9307546552, 206923.1759837889], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [22.96666666666667, 85.16666666666667, 1.0, 2.0, 0.2747418837519046, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4882989534557383, 6.911199999999999, 6.9112, 168.912956510431, 846861.7074088061, 846861.7074088067, 221104.5972939321], 
processed observation next is [1.0, 0.5217391304347826, 0.2875197472353872, 0.8516666666666667, 1.0, 1.0, 0.1261950406649453, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.37597433348260767, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2352393631691128, 0.23523936316911295, 0.3300068616327345], 
reward next is 0.6700, 
noisyNet noise sample is [array([-0.42449573], dtype=float32), -1.2608489]. 
=============================================
[2019-04-10 12:33:45,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.9519735e-22 7.5935057e-24 4.5156069e-19 6.8641887e-10], sum to 1.0000
[2019-04-10 12:33:45,209] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2400
[2019-04-10 12:33:45,216] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.71666666666667, 94.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6159706622754322, 6.9112, 6.9112, 168.912956510431, 535202.0222888836, 535202.0222888836, 168563.0200367314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1815000.0000, 
sim time next is 1815600.0000, 
raw observation next is [21.73333333333333, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6156947964323972, 6.911199999999999, 6.9112, 168.912956510431, 534966.4854327029, 534966.4854327035, 168519.7965465441], 
processed observation next is [1.0, 0.0, 0.22906793048973137, 0.9466666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5313351176004844, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14860180150908414, 0.1486018015090843, 0.25152208439782703], 
reward next is 0.7485, 
noisyNet noise sample is [array([-0.10166212], dtype=float32), -0.8125114]. 
=============================================
[2019-04-10 12:33:49,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.5288364e-23 2.9433686e-24 3.9532564e-20 3.7476194e-12], sum to 1.0000
[2019-04-10 12:33:49,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4077
[2019-04-10 12:33:49,200] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.65, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7963809750842894, 6.911199999999999, 6.9112, 168.912956510431, 667660.4420200373, 667660.442020038, 201093.2698860834], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1884600.0000, 
sim time next is 1885200.0000, 
raw observation next is [25.56666666666666, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7917821674655381, 6.911199999999999, 6.9112, 168.912956510431, 664543.1879125264, 664543.187912527, 200169.3097036107], 
processed observation next is [1.0, 0.8260869565217391, 0.41074249605055263, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7460758139823636, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18459532997570177, 0.18459532997570194, 0.29876016373673236], 
reward next is 0.7012, 
noisyNet noise sample is [array([-1.6353742], dtype=float32), 0.6264075]. 
=============================================
[2019-04-10 12:33:50,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 3.8653837e-25 4.1685851e-25 5.4169117e-22 1.5634339e-14], sum to 1.0000
[2019-04-10 12:33:50,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0462
[2019-04-10 12:33:50,132] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1888321.622914535 W.
[2019-04-10 12:33:50,137] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.6, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 8.34313852358876, 6.9112, 168.9050786026862, 1888321.622914535, 872502.5296144446, 256544.3593152334], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1930800.0000, 
sim time next is 1931400.0000, 
raw observation next is [25.65, 82.5, 1.0, 1.0, 0.3661842956361348, 1.0, 1.0, 0.3661842956361348, 1.0, 2.0, 0.6134829435503902, 6.9112, 6.9112, 170.5573041426782, 1535651.065048117, 1535651.065048117, 332040.23190705], 
processed observation next is [1.0, 0.34782608695652173, 0.41469194312796204, 0.825, 1.0, 0.5, 0.2363666212483552, 1.0, 0.5, 0.2363666212483552, 1.0, 1.0, 0.5286377360370612, 0.0, 0.0, 0.8375144448122397, 0.4265697402911436, 0.4265697402911436, 0.4955824356821642], 
reward next is 0.5044, 
noisyNet noise sample is [array([-0.6211292], dtype=float32), -1.6902916]. 
=============================================
[2019-04-10 12:33:55,550] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.8677627e-29 8.7890595e-30 1.6465303e-27 3.4502441e-23], sum to 1.0000
[2019-04-10 12:33:55,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8720
[2019-04-10 12:33:55,561] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.46666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8707294550761677, 6.9112, 6.9112, 168.912956510431, 718991.4838812008, 718991.4838812008, 216752.0599150543], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2038800.0000, 
sim time next is 2039400.0000, 
raw observation next is [27.4, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8694440902921632, 6.9112, 6.9112, 168.912956510431, 718328.0743165503, 718328.0743165503, 216478.0319856968], 
processed observation next is [0.0, 0.6086956521739131, 0.4976303317535545, 0.825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8407854759660527, 0.0, 0.0, 0.8294399451523027, 0.19953557619904175, 0.19953557619904175, 0.3231015402771594], 
reward next is 0.6769, 
noisyNet noise sample is [array([1.2806559], dtype=float32), 0.07498115]. 
=============================================
[2019-04-10 12:34:00,588] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 6.3634078e-28 4.6935304e-29 7.4177934e-26 4.7362320e-20], sum to 1.0000
[2019-04-10 12:34:00,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-10 12:34:00,597] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.26666666666667, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8962060893148841, 6.911200000000001, 6.9112, 168.912956510431, 737405.3500806413, 737405.3500806406, 222457.5918547321], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2155200.0000, 
sim time next is 2155800.0000, 
raw observation next is [26.18333333333334, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8932122333310409, 6.9112, 6.9112, 168.912956510431, 735359.3039055692, 735359.3039055692, 221783.2071644946], 
processed observation next is [0.0, 0.9565217391304348, 0.4399684044233811, 0.9266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8697710162573669, 0.0, 0.0, 0.8294399451523027, 0.20426647330710257, 0.20426647330710257, 0.33101971218581283], 
reward next is 0.6690, 
noisyNet noise sample is [array([-1.7902809], dtype=float32), -1.2097316]. 
=============================================
[2019-04-10 12:34:07,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.7317920e-25 1.8932597e-26 5.4271236e-24 4.1057299e-16], sum to 1.0000
[2019-04-10 12:34:08,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0711
[2019-04-10 12:34:08,006] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.4, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.977613284126837, 6.9112, 6.9112, 168.912956510431, 790352.3401507188, 790352.3401507188, 241482.5233547413], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [29.3, 79.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9726425645308953, 6.9112, 6.9112, 168.912956510431, 786894.0305718322, 786894.0305718322, 240264.6271167764], 
processed observation next is [1.0, 0.9130434782608695, 0.5876777251184835, 0.7933333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9666372738181648, 0.0, 0.0, 0.8294399451523027, 0.21858167515884228, 0.21858167515884228, 0.35860392106981553], 
reward next is 0.6414, 
noisyNet noise sample is [array([-0.1890612], dtype=float32), -1.9152267]. 
=============================================
[2019-04-10 12:34:08,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.1713719e-24 1.7382221e-25 3.1526599e-23 2.6361644e-17], sum to 1.0000
[2019-04-10 12:34:08,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8077
[2019-04-10 12:34:08,357] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.93333333333334, 80.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9604415821880583, 6.9112, 6.9112, 168.912956510431, 779410.8681605118, 779410.8681605118, 237353.2333212982], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2326800.0000, 
sim time next is 2327400.0000, 
raw observation next is [28.85, 80.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9559364464808874, 6.9112, 6.9112, 168.912956510431, 776428.3434747506, 776428.3434747506, 236275.3609448196], 
processed observation next is [1.0, 0.9565217391304348, 0.5663507109004741, 0.805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9462639591230333, 0.0, 0.0, 0.8294399451523027, 0.2156745398540974, 0.2156745398540974, 0.35264979245495465], 
reward next is 0.6474, 
noisyNet noise sample is [array([0.60632265], dtype=float32), -0.7852238]. 
=============================================
[2019-04-10 12:34:08,474] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 6.5380407e-27 1.7465952e-27 2.7265449e-25 9.8601661e-20], sum to 1.0000
[2019-04-10 12:34:08,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2008
[2019-04-10 12:34:08,483] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.23333333333334, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9217137710184704, 6.911199999999999, 6.9112, 168.912956510431, 754962.0681075352, 754962.0681075358, 228296.4768147859], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [28.2, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9186261816176503, 6.9112, 6.9112, 168.912956510431, 752814.0882196674, 752814.0882196674, 227580.3080076669], 
processed observation next is [1.0, 0.0, 0.5355450236966824, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9007636361190856, 0.0, 0.0, 0.8294399451523027, 0.20911502450546318, 0.20911502450546318, 0.3396721015039805], 
reward next is 0.6603, 
noisyNet noise sample is [array([0.46883667], dtype=float32), 0.87027186]. 
=============================================
[2019-04-10 12:34:09,463] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 12:34:09,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:34:09,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:34:09,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:34:09,465] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:34:09,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:34:09,466] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:34:09,467] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:34:09,468] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:34:09,468] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:34:09,468] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:34:09,481] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-04-10 12:34:09,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-04-10 12:34:09,512] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-04-10 12:34:09,514] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-04-10 12:34:09,529] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-04-10 12:34:20,354] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:34:20,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.35, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.601281617220787, 6.9112, 6.9112, 168.912956510431, 522615.8658173371, 522615.8658173371, 166290.9532503944]
[2019-04-10 12:34:20,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:34:20,358] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.3701341e-28 4.4149680e-30 1.5874012e-28 2.4703704e-25], sampled 0.3056758910959583
[2019-04-10 12:34:22,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:34:22,823] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.11666666666667, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7475832575059425, 6.911199999999999, 6.9112, 168.912956510431, 662479.346389274, 662479.3463892746, 191053.0810258591]
[2019-04-10 12:34:22,826] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:34:22,828] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.5355615e-26 4.8481959e-28 1.0308328e-26 1.2565986e-23], sampled 0.2209275216781077
[2019-04-10 12:34:54,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:34:54,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.938743155, 98.111717955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7494827228625146, 6.911200000000001, 6.9112, 168.912956510431, 637372.1061606121, 637372.1061606115, 191912.3588078753]
[2019-04-10 12:34:54,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:34:54,029] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.7858740e-27 5.1737863e-29 2.1811484e-27 5.9950766e-24], sampled 0.39046004162813475
[2019-04-10 12:35:14,715] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:35:14,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 82.0, 1.0, 2.0, 0.613394147089727, 1.0, 2.0, 0.613394147089727, 1.0, 1.0, 1.03, 6.94904235727116, 6.9112, 178.6582176852504, 2573427.383975119, 2545031.821942209, 494975.4675217276]
[2019-04-10 12:35:14,718] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:35:14,719] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.5589525e-01 4.3769318e-13 6.0099365e-13 7.9106555e-11 4.4104733e-02], sampled 0.8451799521361602
[2019-04-10 12:35:14,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2573427.383975119 W.
[2019-04-10 12:35:16,264] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:35:16,265] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.93333333333333, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9961844732212152, 6.911200000000001, 6.9112, 168.9128765402168, 807315.9963057182, 807315.9963057176, 246311.0187306427]
[2019-04-10 12:35:16,265] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:35:16,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 6.0925865e-28 1.9215680e-29 1.1518468e-27 4.2517784e-24], sampled 0.14696973716729067
[2019-04-10 12:35:18,469] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:35:18,470] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 82.16666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 12.00942576439878, 6.9112, 168.8645286099359, 4446295.466529717, 830475.6066303484, 255719.1868624922]
[2019-04-10 12:35:18,471] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:35:18,473] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 6.9754778e-17 2.6198537e-17 3.5015853e-15 4.7835083e-11], sampled 0.6727891497489814
[2019-04-10 12:35:18,474] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4446295.466529717 W.
[2019-04-10 12:35:20,271] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:35:20,272] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.33333333333334, 93.0, 1.0, 2.0, 0.6171518474510204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564647761, 862440.279720177, 862440.2797201775, 203984.2609655975]
[2019-04-10 12:35:20,273] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:35:20,274] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 8.5088186e-20 3.8381523e-20 1.5672285e-18 2.5557117e-13], sampled 0.9081114447021015
[2019-04-10 12:35:21,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.06767553], dtype=float32), 0.04096981]
[2019-04-10 12:35:21,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.42593090666666, 97.43368954666667, 1.0, 2.0, 0.7189176010434873, 1.0, 1.0, 0.6800488400360062, 1.0, 1.0, 1.03, 6.997952286103072, 6.9112, 184.5923449428631, 2853274.626516363, 2786016.633248498, 531588.38480084]
[2019-04-10 12:35:21,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:35:21,798] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.4560987e-01 7.5008964e-11 4.0602535e-11 1.4157400e-08 5.4390147e-02], sampled 0.9583691602694084
[2019-04-10 12:35:42,368] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.9410 2938046316.8907 1376.0000
[2019-04-10 12:35:42,438] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7921.6712 2989371204.5541 1570.0000
[2019-04-10 12:35:42,455] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7277.1843 3319576698.3542 2154.0000
[2019-04-10 12:35:42,484] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7353.7371 3105976059.7508 2009.0000
[2019-04-10 12:35:42,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.8701 3185143916.3805 2464.0000
[2019-04-10 12:35:43,533] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 275000, evaluation results [275000.0, 7277.18428056667, 3319576698.354235, 2154.0, 7353.737064422803, 3105976059.7507973, 2009.0, 8061.940964995036, 2938046316.8906803, 1376.0, 7030.8701094293765, 3185143916.3805237, 2464.0, 7921.671203157862, 2989371204.554099, 1570.0]
[2019-04-10 12:35:43,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.6338706e-01 1.5751267e-12 3.1687736e-12 5.6548416e-10 3.3661297e-01], sum to 1.0000
[2019-04-10 12:35:43,679] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-10 12:35:43,685] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2400102.778244474 W.
[2019-04-10 12:35:43,690] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.26666666666667, 64.0, 1.0, 2.0, 0.8581409782349235, 1.0, 2.0, 0.8581409782349235, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2400102.778244474, 2400102.778244474, 449170.8550737593], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2373600.0000, 
sim time next is 2374200.0000, 
raw observation next is [32.3, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.132077115538964, 6.9112, 168.9052793270598, 3150569.906026716, 2284477.027085396, 473158.4517369971], 
processed observation next is [1.0, 0.4782608695652174, 0.7298578199052131, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.12208771155389639, 0.0, 0.8294022466673203, 0.8751583072296433, 0.6345769519681655, 0.7062066443835778], 
reward next is 0.0000, 
noisyNet noise sample is [array([3.0205722], dtype=float32), 0.5267434]. 
=============================================
[2019-04-10 12:35:43,988] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8542285e-01 6.4672531e-14 1.7462117e-13 4.0419876e-11 1.4577167e-02], sum to 1.0000
[2019-04-10 12:35:43,997] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-10 12:35:44,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2115607.786309838 W.
[2019-04-10 12:35:44,007] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.85, 67.5, 1.0, 2.0, 0.504341717256579, 1.0, 2.0, 0.504341717256579, 1.0, 2.0, 0.8747067194788583, 6.9112, 6.9112, 170.5573041426782, 2115607.786309838, 2115607.786309838, 417948.6590748964], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2367000.0000, 
sim time next is 2367600.0000, 
raw observation next is [31.0, 67.0, 1.0, 2.0, 0.5099923731967035, 1.0, 2.0, 0.5099923731967035, 1.0, 2.0, 0.8853229169870467, 6.9112, 6.9112, 170.5573041426782, 2139334.773200108, 2139334.773200108, 422057.0809701181], 
processed observation next is [1.0, 0.391304347826087, 0.6682464454976303, 0.67, 1.0, 1.0, 0.40962936529723315, 1.0, 1.0, 0.40962936529723315, 1.0, 1.0, 0.8601498987646911, 0.0, 0.0, 0.8375144448122397, 0.5942596592222523, 0.5942596592222523, 0.629935941746445], 
reward next is 0.3701, 
noisyNet noise sample is [array([0.5072257], dtype=float32), 0.30070248]. 
=============================================
[2019-04-10 12:35:44,376] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8151173e-03 1.2648533e-11 1.6005813e-11 6.0031624e-09 9.9018490e-01], sum to 1.0000
[2019-04-10 12:35:44,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2988
[2019-04-10 12:35:44,386] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.4, 64.0, 1.0, 2.0, 0.5630905658290752, 1.0, 2.0, 0.5630905658290752, 1.0, 1.0, 0.977902095949423, 6.9112, 6.9112, 170.5573041426782, 2362295.078962586, 2362295.078962586, 461516.0630764737], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2376000.0000, 
sim time next is 2376600.0000, 
raw observation next is [32.43333333333334, 63.83333333333334, 1.0, 2.0, 0.4984351500023399, 1.0, 2.0, 0.4984351500023399, 1.0, 2.0, 0.8656170205311318, 6.911200000000001, 6.9112, 170.5573041426782, 2090806.794908013, 2090806.794908013, 414065.7576403989], 
processed observation next is [1.0, 0.5217391304347826, 0.7361769352290681, 0.6383333333333334, 1.0, 1.0, 0.3957050000028192, 1.0, 1.0, 0.3957050000028192, 1.0, 1.0, 0.8361183177208924, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5807796652522258, 0.5807796652522258, 0.6180085934931328], 
reward next is 0.3820, 
noisyNet noise sample is [array([1.8561571], dtype=float32), 0.72397]. 
=============================================
[2019-04-10 12:35:47,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9395657e-01 3.7409430e-11 4.4378980e-11 4.0089136e-09 6.0433815e-03], sum to 1.0000
[2019-04-10 12:35:47,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3803
[2019-04-10 12:35:47,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1994275.332740768 W.
[2019-04-10 12:35:47,152] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.7, 87.5, 1.0, 2.0, 0.7851391810324027, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97795990861364, 6.9112, 168.9125038976512, 1994275.332740768, 1946913.682780353, 405184.9941779895], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2453400.0000, 
sim time next is 2454000.0000, 
raw observation next is [26.6, 87.66666666666667, 1.0, 2.0, 0.6908787082597775, 1.0, 1.0, 0.6908787082597775, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1931895.410265573, 1931895.410265573, 370238.693382306], 
processed observation next is [1.0, 0.391304347826087, 0.4597156398104266, 0.8766666666666667, 1.0, 1.0, 0.6275647087467199, 1.0, 0.5, 0.6275647087467199, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5366376139626592, 0.5366376139626592, 0.5525950647497104], 
reward next is 0.4474, 
noisyNet noise sample is [array([0.17208557], dtype=float32), 1.2823317]. 
=============================================
[2019-04-10 12:35:47,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[26.6906  ]
 [26.420671]
 [25.965094]
 [26.956495]
 [26.51375 ]], R is [[26.36508369]
 [26.16287994]
 [25.96551132]
 [25.74032402]
 [25.94301987]].
[2019-04-10 12:35:48,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4426358e-03 6.9553963e-12 1.4414699e-12 4.2231899e-10 9.9855739e-01], sum to 1.0000
[2019-04-10 12:35:48,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1045
[2019-04-10 12:35:48,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.3, 89.0, 1.0, 2.0, 0.5198356879236696, 1.0, 1.0, 0.5198356879236696, 1.0, 1.0, 0.8874544687334616, 6.9112, 6.9112, 170.5573041426782, 2180667.902440237, 2180667.902440237, 426214.5392805912], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2467800.0000, 
sim time next is 2468400.0000, 
raw observation next is [26.33333333333333, 89.0, 1.0, 2.0, 0.4761119295621337, 1.0, 2.0, 0.4761119295621337, 1.0, 2.0, 0.8138279266508711, 6.9112, 6.9112, 170.5573041426782, 1997079.364687248, 1997079.364687248, 396775.0375216737], 
processed observation next is [1.0, 0.5652173913043478, 0.44707740916271704, 0.89, 1.0, 1.0, 0.3688095536893178, 1.0, 1.0, 0.3688095536893178, 1.0, 1.0, 0.7729608861595988, 0.0, 0.0, 0.8375144448122397, 0.5547442679686799, 0.5547442679686799, 0.5922015485398116], 
reward next is 0.4078, 
noisyNet noise sample is [array([1.6788626], dtype=float32), 0.5373495]. 
=============================================
[2019-04-10 12:35:49,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.3603404e-31 3.0502201e-33 3.9752680e-31 2.9641108e-25], sum to 1.0000
[2019-04-10 12:35:49,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6038
[2019-04-10 12:35:49,096] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333334, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9530463701569707, 6.9112, 6.9112, 168.912956510431, 775571.0519174506, 775571.0519174506, 235638.9452174524], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2500800.0000, 
sim time next is 2501400.0000, 
raw observation next is [26.81666666666667, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9531646949301831, 6.9112, 6.9112, 168.912956510431, 775591.8715183616, 775591.8715183616, 235664.300605921], 
processed observation next is [1.0, 0.9565217391304348, 0.46998420221169057, 0.9383333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9428837743051013, 0.0, 0.0, 0.8294399451523027, 0.21544218653287822, 0.21544218653287822, 0.35173776209838953], 
reward next is 0.6483, 
noisyNet noise sample is [array([-0.361074], dtype=float32), -0.08745308]. 
=============================================
[2019-04-10 12:35:49,305] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.5936689e-34 2.3101942e-34 1.0511850e-32 5.4245697e-27], sum to 1.0000
[2019-04-10 12:35:49,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-10 12:35:49,314] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.7, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9284279509288499, 6.911199999999999, 6.9112, 168.912956510431, 756698.6227808386, 756698.6227808392, 229727.2882116576], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [27.63333333333333, 87.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9345042978915631, 6.911199999999999, 6.9112, 168.912956510431, 761286.4016643214, 761286.401664322, 231168.2649779206], 
processed observation next is [1.0, 0.782608695652174, 0.5086887835703, 0.8766666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9201271925506865, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21146844490675595, 0.21146844490675612, 0.3450272611610755], 
reward next is 0.6550, 
noisyNet noise sample is [array([-0.14011621], dtype=float32), 0.5217169]. 
=============================================
[2019-04-10 12:35:50,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9995279e-01 1.7805113e-12 3.8632071e-12 8.8826611e-12 4.7179452e-05], sum to 1.0000
[2019-04-10 12:35:50,468] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1133
[2019-04-10 12:35:50,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 970507.4006818135 W.
[2019-04-10 12:35:50,476] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.25, 96.5, 1.0, 2.0, 0.6944480571114527, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970507.4006818135, 970507.4006818135, 219652.3340250519], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2521800.0000, 
sim time next is 2522400.0000, 
raw observation next is [26.23333333333333, 96.66666666666666, 1.0, 2.0, 0.3257672347915148, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5583143551715454, 6.911199999999999, 6.9112, 168.912956510431, 910508.9502354981, 910508.9502354988, 229761.8240097378], 
processed observation next is [1.0, 0.17391304347826086, 0.44233807266982617, 0.9666666666666666, 1.0, 1.0, 0.18767136721869254, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.4613589697213968, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25291915284319394, 0.2529191528431941, 0.34292809553692205], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34896672], dtype=float32), 0.29699013]. 
=============================================
[2019-04-10 12:35:52,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5657013e-01 1.4847230e-12 3.0883334e-11 1.1243372e-09 7.4342984e-01], sum to 1.0000
[2019-04-10 12:35:52,112] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7401
[2019-04-10 12:35:52,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.1, 76.0, 1.0, 2.0, 0.7195923649504653, 1.0, 2.0, 0.7195923649504653, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2012262.450357346, 2012262.450357346, 382623.4069016842], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2548800.0000, 
sim time next is 2549400.0000, 
raw observation next is [29.21666666666667, 75.16666666666667, 1.0, 2.0, 0.5216521628573577, 1.0, 2.0, 0.5216521628573577, 1.0, 1.0, 0.9012379087006248, 6.911199999999999, 6.9112, 170.5573041426782, 2188295.64890857, 2188295.648908571, 429552.2053688477], 
processed observation next is [1.0, 0.5217391304347826, 0.5837282780410744, 0.7516666666666667, 1.0, 1.0, 0.42367730464741893, 1.0, 1.0, 0.42367730464741893, 1.0, 0.5, 0.8795584252446644, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6078599024746028, 0.607859902474603, 0.6411226945803697], 
reward next is 0.3589, 
noisyNet noise sample is [array([-2.2578497], dtype=float32), 1.1463802]. 
=============================================
[2019-04-10 12:35:53,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.0108510e-24 5.2170419e-25 4.5012545e-23 2.1025787e-14], sum to 1.0000
[2019-04-10 12:35:53,037] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8482
[2019-04-10 12:35:53,041] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.83333333333334, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8719154109795437, 6.9112, 6.9112, 168.912956510431, 713122.1261653658, 713122.1261653658, 216747.8425897574], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2569200.0000, 
sim time next is 2569800.0000, 
raw observation next is [28.76666666666667, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8824535897854136, 6.9112, 6.9112, 168.912956510431, 721689.8410657011, 721689.8410657011, 219123.6124944435], 
processed observation next is [1.0, 0.7391304347826086, 0.5624012638230649, 0.785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8566507192505043, 0.0, 0.0, 0.8294399451523027, 0.20046940029602808, 0.20046940029602808, 0.3270501679021545], 
reward next is 0.6729, 
noisyNet noise sample is [array([-1.0941942], dtype=float32), -0.9150744]. 
=============================================
[2019-04-10 12:35:53,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.7837422e-30 5.7055712e-31 3.0218157e-30 4.0370517e-24], sum to 1.0000
[2019-04-10 12:35:53,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3041
[2019-04-10 12:35:53,131] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.51666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9148246123092001, 6.9112, 6.9112, 168.912956510431, 749909.2179798466, 749909.2179798466, 226690.6550122459], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2578200.0000, 
sim time next is 2578800.0000, 
raw observation next is [27.43333333333334, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9140447046775484, 6.9112, 6.9112, 168.912956510431, 749649.5647398575, 749649.5647398575, 226522.9082677458], 
processed observation next is [1.0, 0.8695652173913043, 0.49921011058451853, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8951764691189614, 0.0, 0.0, 0.8294399451523027, 0.20823599020551595, 0.20823599020551595, 0.33809389293693404], 
reward next is 0.6619, 
noisyNet noise sample is [array([-1.4462366], dtype=float32), -0.4510328]. 
=============================================
[2019-04-10 12:35:57,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.0805116e-35 2.2355131e-36 9.2143418e-36 3.0821343e-29], sum to 1.0000
[2019-04-10 12:35:57,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-10 12:35:57,532] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 99.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7896696343880868, 6.911200000000001, 6.9112, 168.912956510431, 662802.794067383, 662802.7940673825, 199740.1397100937], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2699400.0000, 
sim time next is 2700000.0000, 
raw observation next is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7920592405487444, 6.911199999999999, 6.9112, 168.912956510431, 664432.5644943232, 664432.5644943238, 200218.5177620002], 
processed observation next is [0.0, 0.2608695652173913, 0.3364928909952607, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7464137079862736, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1845646012484231, 0.18456460124842328, 0.2988336086000003], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.39492065], dtype=float32), 0.38268387]. 
=============================================
[2019-04-10 12:35:57,541] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.54547 ]
 [73.492775]
 [73.42942 ]
 [73.37613 ]
 [73.31184 ]], R is [[73.61856079]
 [73.58425903]
 [73.55110931]
 [73.51925659]
 [73.48867035]].
[2019-04-10 12:36:01,314] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.7279398e-32 1.7118410e-33 1.2296632e-32 2.8590396e-26], sum to 1.0000
[2019-04-10 12:36:01,320] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-10 12:36:01,324] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.83333333333334, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6247078363722228, 6.911199999999999, 6.9112, 168.912956510431, 542214.1043638098, 542214.1043638104, 169949.2895128434], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6197591954724252, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169170.2589030449], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5362917017956405, 0.0, 0.0, 0.8294399451523027, 0.14934759423072505, 0.14934759423072505, 0.2524929237358879], 
reward next is 0.7475, 
noisyNet noise sample is [array([-2.3621404], dtype=float32), -0.23816203]. 
=============================================
[2019-04-10 12:36:02,209] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.00000000e+00 1.00205467e-22 8.70351625e-22 1.38465067e-21
 1.08322612e-13], sum to 1.0000
[2019-04-10 12:36:02,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0099
[2019-04-10 12:36:02,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 891482.0995995684 W.
[2019-04-10 12:36:02,227] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.16666666666667, 83.0, 1.0, 2.0, 0.1943981314428076, 1.0, 1.0, 0.1943981314428076, 1.0, 2.0, 0.3437149926965632, 6.9112, 6.9112, 170.5573041426782, 891482.0995995684, 891482.0995995684, 277316.9398154459], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2805000.0000, 
sim time next is 2805600.0000, 
raw observation next is [23.33333333333334, 83.0, 1.0, 2.0, 0.2981202837812944, 1.0, 2.0, 0.2981202837812944, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 909629.4302615505, 909629.4302615505, 259201.1022302014], 
processed observation next is [1.0, 0.4782608695652174, 0.3048973143759877, 0.83, 1.0, 1.0, 0.1543617876883065, 1.0, 1.0, 0.1543617876883065, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2526748417393196, 0.2526748417393196, 0.3868673167614946], 
reward next is 0.6131, 
noisyNet noise sample is [array([1.186402], dtype=float32), -0.6868933]. 
=============================================
[2019-04-10 12:36:04,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 5.5541416e-27 5.2579139e-27 1.6960198e-26 1.2883683e-14], sum to 1.0000
[2019-04-10 12:36:04,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2786
[2019-04-10 12:36:04,144] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6395626580612356, 6.911199999999999, 6.9112, 168.912956510431, 553177.8879240341, 553177.8879240347, 172362.604723088], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2849400.0000, 
sim time next is 2850000.0000, 
raw observation next is [22.33333333333334, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6338133564874113, 6.911200000000001, 6.9112, 168.912956510431, 548574.3854320905, 548574.3854320899, 171426.4961380645], 
processed observation next is [1.0, 1.0, 0.2575039494470777, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5534309225456235, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15238177373113626, 0.1523817737311361, 0.2558604419971112], 
reward next is 0.7441, 
noisyNet noise sample is [array([0.9467394], dtype=float32), -0.34138855]. 
=============================================
[2019-04-10 12:36:04,157] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.98535 ]
 [74.969734]
 [74.95007 ]
 [74.92305 ]
 [74.87677 ]], R is [[75.01283264]
 [75.00544739]
 [74.99698639]
 [74.98783875]
 [74.9776001 ]].
[2019-04-10 12:36:14,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.7540242e-28 4.0974259e-29 3.9305815e-27 4.1716746e-16], sum to 1.0000
[2019-04-10 12:36:14,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4225
[2019-04-10 12:36:14,836] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6858040097602042, 6.9112, 6.9112, 168.912956510431, 589003.3438173933, 589003.3438173933, 180211.1700451381], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3098400.0000, 
sim time next is 3099000.0000, 
raw observation next is [22.16666666666667, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6841016620970881, 6.9112, 6.9112, 168.912956510431, 587864.5402136683, 587864.5402136683, 179912.0776286971], 
processed observation next is [1.0, 0.8695652173913043, 0.24960505529225935, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6147581245086441, 0.0, 0.0, 0.8294399451523027, 0.16329570561490786, 0.16329570561490786, 0.26852548899805534], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.3148427], dtype=float32), -0.25121838]. 
=============================================
[2019-04-10 12:36:14,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.027016]
 [76.006096]
 [75.952324]
 [76.09278 ]
 [76.40225 ]], R is [[76.05709076]
 [76.02754974]
 [75.99746704]
 [75.96659851]
 [75.93478394]].
[2019-04-10 12:36:18,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.5583332e-26 5.0783713e-26 8.8023639e-26 3.6185681e-17], sum to 1.0000
[2019-04-10 12:36:18,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6865
[2019-04-10 12:36:18,957] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8285666183601225, 6.9112, 6.9112, 168.912956510431, 691139.5004237609, 691139.5004237609, 207744.1321511632], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3193200.0000, 
sim time next is 3193800.0000, 
raw observation next is [25.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8230015566668616, 6.911200000000001, 6.9112, 168.912956510431, 687413.9141745166, 687413.914174516, 206584.3154195187], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7841482398376362, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19094830949292127, 0.1909483094929211, 0.30833479913361], 
reward next is 0.6917, 
noisyNet noise sample is [array([-1.143198], dtype=float32), 0.3714679]. 
=============================================
[2019-04-10 12:36:19,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.7980298e-25 6.8794415e-26 8.3101646e-26 2.3246948e-17], sum to 1.0000
[2019-04-10 12:36:19,510] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-10 12:36:19,516] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8107016139834392, 6.9112, 6.9112, 168.912956510431, 677677.9107701418, 677677.9107701418, 204009.753382583], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3201000.0000, 
sim time next is 3201600.0000, 
raw observation next is [25.0, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8085849223101343, 6.9112, 6.9112, 168.912956510431, 676251.3459021929, 676251.3459021929, 203576.8878600531], 
processed observation next is [0.0, 0.043478260869565216, 0.38388625592417064, 0.9333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.766566978426993, 0.0, 0.0, 0.8294399451523027, 0.18784759608394247, 0.18784759608394247, 0.30384610128366135], 
reward next is 0.6962, 
noisyNet noise sample is [array([0.4939843], dtype=float32), -0.94181347]. 
=============================================
[2019-04-10 12:36:20,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.00000000e+00 1.77992436e-33 1.27123965e-33 3.31647984e-34
 1.69524918e-27], sum to 1.0000
[2019-04-10 12:36:20,249] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-10 12:36:20,252] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.780960847360557, 6.9112, 6.9112, 168.912956510431, 656322.6936613321, 656322.6936613321, 197997.3647991244], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3222000.0000, 
sim time next is 3222600.0000, 
raw observation next is [26.16666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7838270871496524, 6.911199999999999, 6.9112, 168.912956510431, 658335.4629072178, 658335.4629072185, 198566.3620750686], 
processed observation next is [0.0, 0.30434782608695654, 0.4391785150078992, 0.8316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7363744965239664, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1828709619186716, 0.1828709619186718, 0.29636770458965467], 
reward next is 0.7036, 
noisyNet noise sample is [array([-1.050423], dtype=float32), -0.8772012]. 
=============================================
[2019-04-10 12:36:23,999] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 12:36:24,000] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:36:24,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:36:24,001] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:36:24,002] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:36:24,003] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:36:24,003] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:36:24,004] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:36:24,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:36:24,007] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:36:24,010] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:36:24,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-04-10 12:36:24,036] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-04-10 12:36:24,037] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-04-10 12:36:24,050] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-04-10 12:36:24,084] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-04-10 12:36:28,791] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:36:28,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.6, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4886918814884069, 6.911200000000001, 6.9112, 168.912956510431, 433505.2629856116, 433505.2629856109, 150569.3465300599]
[2019-04-10 12:36:28,795] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:36:28,797] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.3862776e-32 6.3373711e-34 2.7692380e-33 1.7876268e-26], sampled 0.49201679569428414
[2019-04-10 12:36:39,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:36:39,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.2, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6498234550256617, 6.9112, 6.9112, 168.912956510431, 560048.3335572467, 560048.3335572467, 174064.6005434142]
[2019-04-10 12:36:39,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:36:39,918] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 8.9975063e-33 2.0384904e-34 6.7925257e-34 4.0654193e-27], sampled 0.7835444259351908
[2019-04-10 12:36:52,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:36:52,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.93727235, 67.82569931666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9777216409870999, 6.9112, 6.9112, 168.912956510431, 820151.450236403, 820151.450236403, 242723.5081750956]
[2019-04-10 12:36:52,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:36:52,890] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 9.3770047e-26 6.7709004e-26 4.2936974e-25 6.0551180e-14], sampled 0.27133701940683674
[2019-04-10 12:36:58,680] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:36:58,681] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.46666666666667, 78.66666666666667, 1.0, 2.0, 0.6337178408555285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885600.117208707, 885600.117208707, 207186.400665587]
[2019-04-10 12:36:58,682] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:36:58,683] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999976e-01 5.4907898e-21 1.1354820e-20 6.4322075e-20 2.4585447e-07], sampled 0.6353648993324444
[2019-04-10 12:36:58,684] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 885600.117208707 W.
[2019-04-10 12:37:29,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:37:29,439] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.81666666666667, 82.83333333333334, 1.0, 2.0, 0.5926530263949041, 0.0, 2.0, 0.0, 1.0, 2.0, 1.029242313497166, 6.911200000000001, 6.9112, 168.9128692832348, 1657028.272974255, 1657028.272974254, 363031.1091589942]
[2019-04-10 12:37:29,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:37:29,441] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999869e-01 3.5509192e-23 5.5321970e-23 1.6974718e-21 1.3297763e-06], sampled 0.6347677666192634
[2019-04-10 12:37:29,442] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1657028.272974255 W.
[2019-04-10 12:37:35,977] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:37:35,978] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.3604887, 80.76474555, 1.0, 2.0, 0.6619477549353793, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005973296302273, 6.9112, 168.9116239154572, 1821881.579551456, 1754646.671746058, 376625.5863265907]
[2019-04-10 12:37:35,978] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:37:35,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9999845e-01 2.0461260e-23 3.2506222e-23 1.1182171e-21 1.5623156e-06], sampled 0.16746426660535096
[2019-04-10 12:37:35,983] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1821881.579551456 W.
[2019-04-10 12:37:41,832] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.02974022], dtype=float32), 0.051434282]
[2019-04-10 12:37:41,833] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.33333333333333, 72.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9071997892281033, 6.911199999999999, 6.9112, 168.912956510431, 746850.2571553164, 746850.257155317, 225030.4866943185]
[2019-04-10 12:37:41,834] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:37:41,836] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.8725683e-34 7.6068540e-36 5.2977160e-35 2.6345928e-27], sampled 0.8971492215909023
[2019-04-10 12:37:56,558] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.9852 2938097123.7550 1366.0000
[2019-04-10 12:37:56,732] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7359.4981 3106185338.2727 1988.0000
[2019-04-10 12:37:56,848] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7910.8921 2989575676.6066 1592.0000
[2019-04-10 12:37:56,944] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.0449 3185250436.5365 2459.0000
[2019-04-10 12:37:56,949] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7246.1145 3320155039.6425 2243.0000
[2019-04-10 12:37:57,962] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 300000, evaluation results [300000.0, 7246.1145181439, 3320155039.6424537, 2243.0, 7359.498126076471, 3106185338.2726536, 1988.0, 8062.985220010779, 2938097123.7549777, 1366.0, 7031.044946462472, 3185250436.5364575, 2459.0, 7910.892107387553, 2989575676.606571, 1592.0]
[2019-04-10 12:38:00,391] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.7701892e-30 7.1983898e-31 3.5741773e-30 1.6478405e-23], sum to 1.0000
[2019-04-10 12:38:00,396] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4543
[2019-04-10 12:38:00,401] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9147903679163981, 6.9112, 6.9112, 168.912956510431, 749994.1639529181, 749994.1639529181, 226687.4330919687], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3367200.0000, 
sim time next is 3367800.0000, 
raw observation next is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9161286421193702, 6.9112, 6.9112, 168.912956510431, 751091.7178992864, 751091.7178992864, 227003.3906249487], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8977178562431344, 0.0, 0.0, 0.8294399451523027, 0.20863658830535733, 0.20863658830535733, 0.3388110307835055], 
reward next is 0.6612, 
noisyNet noise sample is [array([0.26251787], dtype=float32), -0.33443072]. 
=============================================
[2019-04-10 12:38:04,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.00000000e+00 1.08420628e-16 1.38139295e-17 1.42405434e-15
 1.01174686e-08], sum to 1.0000
[2019-04-10 12:38:04,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6533
[2019-04-10 12:38:04,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1602296.134053981 W.
[2019-04-10 12:38:04,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.33333333333334, 85.66666666666667, 1.0, 1.0, 0.3820643019407422, 1.0, 1.0, 0.3820643019407422, 1.0, 2.0, 0.647873953560449, 6.9112, 6.9112, 170.5573041426782, 1602296.134053981, 1602296.134053981, 341075.0782701312], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3464400.0000, 
sim time next is 3465000.0000, 
raw observation next is [26.5, 84.0, 1.0, 2.0, 0.399385546846986, 1.0, 2.0, 0.399385546846986, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1116372.261301683, 1116372.261301683, 271964.4160177546], 
processed observation next is [1.0, 0.08695652173913043, 0.4549763033175356, 0.84, 1.0, 1.0, 0.27636812873130845, 1.0, 1.0, 0.27636812873130845, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3101034059171342, 0.3101034059171342, 0.40591703883246955], 
reward next is 0.5941, 
noisyNet noise sample is [array([-0.10621247], dtype=float32), -0.7208025]. 
=============================================
[2019-04-10 12:38:04,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[48.57424 ]
 [49.719902]
 [51.7893  ]
 [51.478073]
 [51.48346 ]], R is [[48.73100662]
 [48.73463058]
 [48.24728394]
 [48.45015335]
 [48.65044022]].
[2019-04-10 12:38:07,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9937695e-01 1.1780408e-12 2.5799466e-13 6.9568973e-12 6.2308746e-04], sum to 1.0000
[2019-04-10 12:38:07,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6344
[2019-04-10 12:38:07,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2469312.616759449 W.
[2019-04-10 12:38:07,530] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.588574716947438, 1.0, 2.0, 0.588574716947438, 1.0, 1.0, 1.022159638704461, 6.9112, 6.9112, 170.5573041426782, 2469312.616759449, 2469312.616759449, 481799.4623482412], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3516000.0000, 
sim time next is 3516600.0000, 
raw observation next is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.752182569953335, 6.9112, 168.9083580009419, 2880833.934409848, 2284228.197417282, 474178.3748274128], 
processed observation next is [1.0, 0.6956521739130435, 0.7235387045813582, 0.6566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.08409825699533346, 0.0, 0.8294173643655044, 0.80023164844718, 0.6345078326159117, 0.7077289176528548], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.72756547], dtype=float32), -2.3382027]. 
=============================================
[2019-04-10 12:38:16,477] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 6.1175491e-14 1.2290642e-15 8.1713031e-14 2.1091633e-09], sum to 1.0000
[2019-04-10 12:38:16,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3837
[2019-04-10 12:38:16,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 930089.0920647621 W.
[2019-04-10 12:38:16,494] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.6541069650247273, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565103631, 930089.0920647621, 930089.0920647614, 213354.1890540401], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3724800.0000, 
sim time next is 3725400.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.6314894431146257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 902391.7907275679, 902391.7907275684, 209346.3249308233], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.74, 1.0, 1.0, 0.556011377246537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2506643863132133, 0.25066438631321347, 0.3124572013892885], 
reward next is 0.6875, 
noisyNet noise sample is [array([0.7553181], dtype=float32), 0.7783225]. 
=============================================
[2019-04-10 12:38:16,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9191977e-01 1.1967057e-07 3.0181752e-08 4.6264651e-07 2.0807973e-01], sum to 1.0000
[2019-04-10 12:38:16,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6596
[2019-04-10 12:38:16,961] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1100881.178728486 W.
[2019-04-10 12:38:16,965] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.787688883315475, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1100881.178728486, 1100881.178728487, 240908.6217254631], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3742200.0000, 
sim time next is 3742800.0000, 
raw observation next is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.2557752458289083, 1.0, 1.0, 0.2557752458289083, 1.0, 1.0, 0.4322187858684086, 6.9112, 6.9112, 170.5573041426782, 1072401.868499782, 1072401.868499782, 286466.344694391], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.7133333333333333, 1.0, 1.0, 0.1033436696733835, 1.0, 0.5, 0.1033436696733835, 1.0, 0.5, 0.30758388520537633, 0.0, 0.0, 0.8375144448122397, 0.2978894079166061, 0.2978894079166061, 0.4275617084990911], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2039651], dtype=float32), 1.3611299]. 
=============================================
[2019-04-10 12:38:17,677] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4828436e-02 2.4773946e-08 2.1124491e-09 5.1457283e-07 9.8517102e-01], sum to 1.0000
[2019-04-10 12:38:17,686] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3728
[2019-04-10 12:38:17,690] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666667, 61.00000000000001, 1.0, 2.0, 0.6882911135563211, 1.0, 2.0, 0.6647355962924233, 1.0, 2.0, 1.03, 7.005096808939343, 6.9112, 170.5573041426782, 2789195.441894475, 2721933.338173239, 517656.4229010763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [33.5, 61.5, 1.0, 2.0, 0.7217320963426511, 1.0, 2.0, 0.6814560876855881, 1.0, 2.0, 1.03, 7.005099445774121, 6.9112, 170.5573041426782, 2859434.037502669, 2792170.044909421, 528281.1051040326], 
processed observation next is [1.0, 0.6521739130434783, 0.7867298578199052, 0.615, 1.0, 1.0, 0.6647374654730737, 1.0, 1.0, 0.6162121538380579, 1.0, 1.0, 1.0365853658536586, 0.009389944577412112, 0.0, 0.8375144448122397, 0.7942872326396302, 0.7756027902526169, 0.7884792613493024], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07253191], dtype=float32), 0.092218734]. 
=============================================
[2019-04-10 12:38:17,699] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[12.754538]
 [12.767618]
 [12.429175]
 [11.986186]
 [11.68156 ]], R is [[12.66568947]
 [12.53903294]
 [12.57576656]
 [12.51832485]
 [12.39314175]].
[2019-04-10 12:38:28,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6928995e-11 2.4303384e-16 2.1374783e-18 6.5419222e-15 1.0000000e+00], sum to 1.0000
[2019-04-10 12:38:28,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1374
[2019-04-10 12:38:28,952] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.5, 61.5, 1.0, 2.0, 0.5065538680879403, 1.0, 2.0, 0.5065538680879403, 1.0, 2.0, 0.8797165489447211, 6.911199999999999, 6.9112, 170.5573041426782, 2124896.499089248, 2124896.499089249, 419708.5666274981], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4019400.0000, 
sim time next is 4020000.0000, 
raw observation next is [33.66666666666666, 61.0, 1.0, 2.0, 0.5785432450466217, 1.0, 2.0, 0.5785432450466217, 1.0, 2.0, 1.004738289471194, 6.911200000000001, 6.9112, 170.5573041426782, 2427185.59855634, 2427185.598556339, 473709.2098612482], 
processed observation next is [1.0, 0.5217391304347826, 0.7946287519747232, 0.61, 1.0, 1.0, 0.49222077716460444, 1.0, 1.0, 0.49222077716460444, 1.0, 1.0, 1.0057784017941391, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6742182218212056, 0.6742182218212053, 0.7070286714346988], 
reward next is 0.2930, 
noisyNet noise sample is [array([0.55897063], dtype=float32), 0.44747263]. 
=============================================
[2019-04-10 12:38:28,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[28.321003]
 [27.99113 ]
 [27.412804]
 [27.40786 ]
 [27.443592]], R is [[28.06188774]
 [28.15483856]
 [28.20284271]
 [27.92081451]
 [27.64160728]].
[2019-04-10 12:38:32,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5981407e-01 5.5063952e-14 2.3630987e-15 6.3118884e-13 4.0185899e-02], sum to 1.0000
[2019-04-10 12:38:32,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0394
[2019-04-10 12:38:32,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1210731.378197536 W.
[2019-04-10 12:38:32,094] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 91.5, 1.0, 2.0, 0.433121376821651, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7440498784600766, 6.911199999999999, 6.9112, 168.912956510431, 1210731.378197536, 1210731.378197536, 273176.3392082527], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4084200.0000, 
sim time next is 4084800.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.263342106577217, 1.0, 1.0, 0.263342106577217, 1.0, 2.0, 0.4551085369951813, 6.911199999999999, 6.9112, 170.5573041426782, 1104144.165196748, 1104144.165196749, 289970.2619529894], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.11246036937014098, 1.0, 0.5, 0.11246036937014098, 1.0, 1.0, 0.33549821584778206, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3067067125546522, 0.3067067125546525, 0.4327914357507304], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.649796], dtype=float32), 1.02769]. 
=============================================
[2019-04-10 12:38:37,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9999976e-01 1.2255870e-12 4.5420820e-15 2.6889183e-12 1.8759123e-07], sum to 1.0000
[2019-04-10 12:38:37,575] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-10 12:38:37,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 886411.2818292192 W.
[2019-04-10 12:38:37,585] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.3171503014267096, 1.0, 1.0, 0.3171503014267096, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 886411.2818292192, 886411.2818292192, 253293.8076458115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4221000.0000, 
sim time next is 4221600.0000, 
raw observation next is [31.66666666666666, 76.33333333333333, 1.0, 2.0, 0.2106613037683169, 1.0, 2.0, 0.2106613037683169, 1.0, 1.0, 0.3658490178878397, 6.911199999999999, 6.9112, 170.5573041426782, 883172.459279184, 883172.4592791847, 272873.4163562864], 
processed observation next is [1.0, 0.8695652173913043, 0.6998420221169034, 0.7633333333333333, 1.0, 1.0, 0.048989522612429996, 1.0, 1.0, 0.048989522612429996, 1.0, 0.5, 0.22664514376565814, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.24532568313310665, 0.24532568313310685, 0.40727375575565133], 
reward next is 0.5927, 
noisyNet noise sample is [array([0.51611865], dtype=float32), -0.23690979]. 
=============================================
[2019-04-10 12:38:38,487] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 12:38:38,488] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:38:38,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:38,488] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:38:38,489] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:38,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:38:38,490] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:38:38,490] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:38,492] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:38:38,492] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:38,495] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:38:38,510] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-04-10 12:38:38,525] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-04-10 12:38:38,526] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-04-10 12:38:38,540] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-04-10 12:38:38,577] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-04-10 12:38:57,195] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01941285], dtype=float32), 0.025869254]
[2019-04-10 12:38:57,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.54623345, 88.23327342833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7710566488465, 6.9112, 6.9112, 168.912956510431, 654956.4598430749, 654956.4598430749, 196124.7726423978]
[2019-04-10 12:38:57,198] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:38:57,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 9.1019459e-23 7.1652073e-25 7.9692267e-24 1.7874880e-21], sampled 0.32822991714822314
[2019-04-10 12:38:57,583] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01941285], dtype=float32), 0.025869254]
[2019-04-10 12:38:57,584] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.075169265, 89.84000962500002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7821661355080073, 6.911199999999999, 6.9112, 168.912956510431, 655444.5525334318, 655444.5525334324, 198199.3511292502]
[2019-04-10 12:38:57,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:38:57,587] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.1027817e-22 2.0658214e-24 2.0442411e-23 3.8669569e-20], sampled 0.7721242208353288
[2019-04-10 12:39:03,341] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.01941285], dtype=float32), 0.025869254]
[2019-04-10 12:39:03,341] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.46766337333334, 67.44390907833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7793066429064103, 6.9112, 6.9112, 168.912956510431, 660279.9192317748, 660279.9192317748, 197751.9471169431]
[2019-04-10 12:39:03,342] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:39:03,344] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 7.1276955e-25 3.0032150e-27 3.3774704e-26 1.4767992e-23], sampled 0.701679394610454
[2019-04-10 12:39:11,007] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.01941285], dtype=float32), 0.025869254]
[2019-04-10 12:39:11,008] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.0, 88.66666666666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9636606973954271, 6.911200000000001, 6.9112, 168.9129565103917, 802268.4731992483, 802268.4731992476, 239004.7206998976]
[2019-04-10 12:39:11,008] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:39:11,010] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.4373878e-14 2.3355769e-15 3.1753049e-14 4.1918406e-11], sampled 0.4638316750862701
[2019-04-10 12:39:43,939] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.01941285], dtype=float32), 0.025869254]
[2019-04-10 12:39:43,939] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.5201184, 94.24759639, 1.0, 2.0, 0.984873883532741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1376647.40107787, 1376647.401077869, 294357.2176934966]
[2019-04-10 12:39:43,939] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:39:43,944] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.1747240e-01 7.9487625e-09 2.8610703e-10 4.7629669e-08 1.8252751e-01], sampled 0.14228775233489233
[2019-04-10 12:39:43,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1376647.40107787 W.
[2019-04-10 12:40:09,862] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7948.8558 2994657504.1331 1422.0000
[2019-04-10 12:40:09,969] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7331.5025 3323962497.2231 1950.0000
[2019-04-10 12:40:09,978] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7424.4370 3109518091.3047 1803.0000
[2019-04-10 12:40:10,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7066.8870 3190186781.3899 2277.0000
[2019-04-10 12:40:10,114] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8119.9765 2941989934.5422 1172.0000
[2019-04-10 12:40:11,125] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 325000, evaluation results [325000.0, 7331.502549231988, 3323962497.223089, 1950.0, 7424.436985411088, 3109518091.304725, 1803.0, 8119.976480523402, 2941989934.542185, 1172.0, 7066.88698556133, 3190186781.3899336, 2277.0, 7948.855820643337, 2994657504.1330857, 1422.0]
[2019-04-10 12:40:11,639] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6542690e-02 7.9588756e-07 8.4114234e-08 7.2315612e-05 9.2338413e-01], sum to 1.0000
[2019-04-10 12:40:11,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-10 12:40:11,654] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 78.33333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 14.3150145532553, 6.9112, 169.634718075015, 6105562.765578374, 830597.9890095189, 253970.1414896371], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4241400.0000, 
sim time next is 4242000.0000, 
raw observation next is [30.0, 77.66666666666667, 1.0, 1.0, 1.04, 1.0, 1.0, 0.9292517039954756, 1.0, 2.0, 1.03, 7.35151478392501, 6.9112, 170.5573041426782, 3900824.70077201, 3585409.310186265, 672032.7464258973], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.7766666666666667, 1.0, 0.5, 1.0481927710843375, 1.0, 0.5, 0.9147610891511755, 1.0, 1.0, 1.0365853658536586, 0.04403147839250101, 0.0, 0.8375144448122397, 1.0835624168811138, 0.9959470306072958, 1.003033949889399], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5912129], dtype=float32), 0.628833]. 
=============================================
[2019-04-10 12:40:11,661] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[11.877669]
 [15.436517]
 [15.856149]
 [15.525705]
 [16.143341]], R is [[12.56505966]
 [12.43940926]
 [12.31501484]
 [12.19186497]
 [12.06994629]].
[2019-04-10 12:40:12,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.8805839e-01 2.5774349e-09 2.4095545e-10 9.2933426e-08 1.1941491e-02], sum to 1.0000
[2019-04-10 12:40:12,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-10 12:40:12,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1282471.803291743 W.
[2019-04-10 12:40:12,634] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 79.0, 1.0, 2.0, 0.4587699700313549, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7937574813301543, 6.911199999999999, 6.9112, 168.912956510431, 1282471.803291743, 1282471.803291743, 286303.7810408429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4256400.0000, 
sim time next is 4257000.0000, 
raw observation next is [29.5, 79.0, 1.0, 2.0, 0.9834525942225946, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1374659.450049712, 1374659.450049713, 293927.2651142317], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.79, 1.0, 1.0, 0.9800633665332464, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38184984723603116, 0.3818498472360314, 0.43869741061825623], 
reward next is 0.5613, 
noisyNet noise sample is [array([0.04524764], dtype=float32), -0.53480005]. 
=============================================
[2019-04-10 12:40:12,641] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[8.037839]
 [8.695806]
 [8.728418]
 [9.054793]
 [8.698889]], R is [[ 8.37869549]
 [ 8.86758995]
 [ 9.30195427]
 [ 9.76452351]
 [10.21993542]].
[2019-04-10 12:40:19,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.4199327e-22 5.6786635e-25 5.7278818e-23 1.3120966e-21], sum to 1.0000
[2019-04-10 12:40:19,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0800
[2019-04-10 12:40:19,066] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.926008399472666, 6.9112, 168.9126953733497, 839310.5875837207, 828805.0155735288, 254811.9307529027], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4428600.0000, 
sim time next is 4429200.0000, 
raw observation next is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.94860714982977, 6.9112, 168.9125388902037, 855349.1250375095, 828811.2711058602, 254811.9300740233], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0037407149829769893, 0.0, 0.8294378944457993, 0.23759697917708597, 0.23022535308496117, 0.38031631354331835], 
reward next is 0.4326, 
noisyNet noise sample is [array([-2.184064], dtype=float32), 0.5940324]. 
=============================================
[2019-04-10 12:40:23,066] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.2147282e-35 0.0000000e+00 1.9619598e-35 1.8634932e-28], sum to 1.0000
[2019-04-10 12:40:23,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1948
[2019-04-10 12:40:23,084] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8555589054159891, 6.9112, 6.9112, 168.912956510431, 710670.4678615045, 710670.4678615045, 213518.5311568516], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4516200.0000, 
sim time next is 4516800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8579044081106286, 6.9112, 6.9112, 168.912956510431, 712151.6148201709, 712151.6148201709, 214022.1081356924], 
processed observation next is [0.0, 0.2608695652173913, 0.4628751974723541, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8267126928178398, 0.0, 0.0, 0.8294399451523027, 0.19781989300560304, 0.19781989300560304, 0.3194359822920782], 
reward next is 0.6806, 
noisyNet noise sample is [array([-1.3096148], dtype=float32), -1.4695314]. 
=============================================
[2019-04-10 12:40:25,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.2397965e-29 8.3998172e-34 2.4071070e-30 3.1065898e-23], sum to 1.0000
[2019-04-10 12:40:25,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8444
[2019-04-10 12:40:25,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8832046114126404, 6.9112, 6.9112, 168.912956510431, 729450.423422317, 729450.423422317, 219578.7957112248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4568400.0000, 
sim time next is 4569000.0000, 
raw observation next is [28.0, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8891760124415102, 6.9112, 6.9112, 168.912956510431, 733794.9561776905, 733794.9561776905, 220922.4972501996], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8648487956603783, 0.0, 0.0, 0.8294399451523027, 0.2038319322715807, 0.2038319322715807, 0.32973507052268597], 
reward next is 0.6703, 
noisyNet noise sample is [array([2.3515384], dtype=float32), -0.79063135]. 
=============================================
[2019-04-10 12:40:25,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.253704]
 [59.331657]
 [59.46629 ]
 [59.585304]
 [59.68616 ]], R is [[59.21232224]
 [59.29247284]
 [59.37176514]
 [59.4502182 ]
 [59.52785492]].
[2019-04-10 12:40:34,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2542688e-02 5.1245080e-14 4.0766894e-16 3.7155873e-12 9.8745733e-01], sum to 1.0000
[2019-04-10 12:40:34,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0905
[2019-04-10 12:40:34,948] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.5065811707529645, 1.0, 2.0, 0.5065811707529645, 1.0, 1.0, 0.8752089135278318, 6.9112, 6.9112, 170.5573041426782, 2125011.142142995, 2125011.142142995, 418892.2512681382], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4786200.0000, 
sim time next is 4786800.0000, 
raw observation next is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.4984749051464439, 1.0, 2.0, 0.4984749051464439, 1.0, 2.0, 0.8621593117537087, 6.911200000000001, 6.9112, 170.5573041426782, 2090973.720231628, 2090973.720231627, 413458.7535335182], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.6733333333333335, 1.0, 1.0, 0.39575289776679984, 1.0, 1.0, 0.39575289776679984, 1.0, 1.0, 0.8319015996996448, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5808260333976745, 0.5808260333976742, 0.6171026172142062], 
reward next is 0.3829, 
noisyNet noise sample is [array([1.2940434], dtype=float32), 1.2833743]. 
=============================================
[2019-04-10 12:40:43,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 3.7423885e-32 2.8818567e-35 2.0441080e-31 2.5664574e-25], sum to 1.0000
[2019-04-10 12:40:43,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-10 12:40:43,618] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8679779241256883, 6.9112, 6.9112, 168.912956510431, 717516.8109010708, 717516.8109010708, 216163.7904997292], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4997400.0000, 
sim time next is 4998000.0000, 
raw observation next is [27.66666666666667, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8598907496316532, 6.9112, 6.9112, 168.912956510431, 712274.3428589283, 712274.3428589283, 214412.253683799], 
processed observation next is [1.0, 0.8695652173913043, 0.5102685624012641, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8291350605264064, 0.0, 0.0, 0.8294399451523027, 0.19785398412748006, 0.19785398412748006, 0.320018289080297], 
reward next is 0.6800, 
noisyNet noise sample is [array([0.53176683], dtype=float32), 0.10834326]. 
=============================================
[2019-04-10 12:40:43,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[51.894234]
 [51.834496]
 [51.88523 ]
 [51.907322]
 [52.09413 ]], R is [[52.13176727]
 [52.28781891]
 [52.44001007]
 [52.5893898 ]
 [52.73623657]].
[2019-04-10 12:40:45,903] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7344355e-37], sum to 1.0000
[2019-04-10 12:40:45,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4158
[2019-04-10 12:40:45,916] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8778413088662462, 6.911200000000001, 6.9112, 168.912956510431, 723421.6011666107, 723421.60116661, 218301.7795052799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [31.16666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.887288437372514, 6.9112, 6.9112, 168.912956510431, 730504.2237892565, 730504.2237892565, 220424.4170569278], 
processed observation next is [0.0, 0.5217391304347826, 0.6761453396524489, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8625468748445291, 0.0, 0.0, 0.8294399451523027, 0.20291783994146015, 0.20291783994146015, 0.32899166724914597], 
reward next is 0.6710, 
noisyNet noise sample is [array([-0.84162915], dtype=float32), -0.25543222]. 
=============================================
[2019-04-10 12:40:45,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.22873 ]
 [70.01709 ]
 [69.886665]
 [69.75239 ]
 [69.62188 ]], R is [[70.34605408]
 [70.31677246]
 [70.28753662]
 [70.25849152]
 [70.22987366]].
[2019-04-10 12:40:46,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1687318e-38], sum to 1.0000
[2019-04-10 12:40:46,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8279
[2019-04-10 12:40:46,262] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.83333333333334, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9303389416926302, 6.9112, 6.9112, 168.912956510431, 757263.4901138083, 757263.4901138083, 230137.3795165907], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5058600.0000, 
sim time next is 5059200.0000, 
raw observation next is [31.66666666666667, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9259155446369461, 6.9112, 6.9112, 168.912956510431, 754865.843032267, 754865.843032267, 229137.163291667], 
processed observation next is [0.0, 0.5652173913043478, 0.6998420221169038, 0.6300000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9096531032157879, 0.0, 0.0, 0.8294399451523027, 0.20968495639785192, 0.20968495639785192, 0.3419957661069657], 
reward next is 0.6580, 
noisyNet noise sample is [array([1.953381], dtype=float32), -1.5011116]. 
=============================================
[2019-04-10 12:40:46,504] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6261294e-38], sum to 1.0000
[2019-04-10 12:40:46,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1934
[2019-04-10 12:40:46,518] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8420738965693321, 6.9112, 6.9112, 168.912956510431, 700326.0326932248, 700326.0326932248, 210593.6280745526], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [26.33333333333334, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8458440656168653, 6.911199999999999, 6.9112, 168.912956510431, 703054.358707691, 703054.3587076915, 211401.6319219463], 
processed observation next is [0.0, 0.2608695652173913, 0.44707740916271754, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8120049580693479, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19529287741880305, 0.19529287741880322, 0.31552482376409896], 
reward next is 0.6845, 
noisyNet noise sample is [array([0.32011765], dtype=float32), -0.91538584]. 
=============================================
[2019-04-10 12:40:46,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.41274 ]
 [75.366425]
 [75.26986 ]
 [75.253654]
 [75.23728 ]], R is [[75.36679077]
 [75.29880524]
 [75.23233032]
 [75.16659546]
 [75.10173798]].
[2019-04-10 12:40:47,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:40:47,788] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2835
[2019-04-10 12:40:47,793] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.66666666666667, 60.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8882868276407437, 6.9112, 6.9112, 168.912956510431, 730647.117663384, 730647.117663384, 220626.2667932896], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5066400.0000, 
sim time next is 5067000.0000, 
raw observation next is [31.5, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8866222869409646, 6.9112, 6.9112, 168.912956510431, 729520.7607885478, 729520.7607885478, 220255.0436527824], 
processed observation next is [0.0, 0.6521739130434783, 0.6919431279620853, 0.61, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.861734496269469, 0.0, 0.0, 0.8294399451523027, 0.20264465577459662, 0.20264465577459662, 0.3287388711235558], 
reward next is 0.6713, 
noisyNet noise sample is [array([-0.3619971], dtype=float32), 0.40151423]. 
=============================================
[2019-04-10 12:40:47,802] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.4055 ]
 [73.34294]
 [73.26895]
 [73.1629 ]
 [73.10835]], R is [[73.41819   ]
 [73.35471344]
 [73.29186249]
 [73.22951508]
 [73.16844177]].
[2019-04-10 12:40:50,128] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5933992e-38], sum to 1.0000
[2019-04-10 12:40:50,136] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3441
[2019-04-10 12:40:50,141] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9517863698481125, 6.9112, 6.9112, 168.912956510431, 773503.1762393838, 773503.1762393838, 235277.8533695055], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5152800.0000, 
sim time next is 5153400.0000, 
raw observation next is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9457116359965053, 6.9112, 6.9112, 168.912956510431, 768564.5408492141, 768564.5408492141, 233792.860340024], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9337946780445185, 0.0, 0.0, 0.8294399451523027, 0.2134901502358928, 0.2134901502358928, 0.3489445676716776], 
reward next is 0.6511, 
noisyNet noise sample is [array([-0.6085057], dtype=float32), 0.22368078]. 
=============================================
[2019-04-10 12:40:50,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.759541e-38], sum to 1.0000
[2019-04-10 12:40:50,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7913
[2019-04-10 12:40:50,234] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333333, 82.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8637714311416086, 6.9112, 6.9112, 168.912956510431, 715780.6082808468, 715780.6082808468, 215284.4896111546], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5123400.0000, 
sim time next is 5124000.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8697083488618673, 6.9112, 6.9112, 168.912956510431, 719738.71351649, 719738.71351649, 216579.0932432204], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012641, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8411077425144724, 0.0, 0.0, 0.8294399451523027, 0.1999274204212472, 0.1999274204212472, 0.3232523779749558], 
reward next is 0.6767, 
noisyNet noise sample is [array([0.30896023], dtype=float32), -1.4768174]. 
=============================================
[2019-04-10 12:40:50,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[66.63498 ]
 [66.662735]
 [66.64671 ]
 [66.68973 ]
 [66.71785 ]], R is [[66.60061646]
 [66.61328888]
 [66.62858582]
 [66.6472702 ]
 [66.66917419]].
[2019-04-10 12:40:50,869] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5102906e-38], sum to 1.0000
[2019-04-10 12:40:50,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1597
[2019-04-10 12:40:50,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8845808417621907, 6.9112, 6.9112, 168.912956510431, 730587.4611039674, 730587.4611039674, 219892.5941210624], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5173200.0000, 
sim time next is 5173800.0000, 
raw observation next is [28.0, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8856960282972067, 6.9112, 6.9112, 168.912956510431, 731508.9379428031, 731508.9379428031, 220147.2492878961], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.7900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8606049125575691, 0.0, 0.0, 0.8294399451523027, 0.2031969272063342, 0.2031969272063342, 0.32857798401178523], 
reward next is 0.6714, 
noisyNet noise sample is [array([0.6722132], dtype=float32), 0.40900582]. 
=============================================
[2019-04-10 12:40:51,842] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 12:40:51,843] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:40:51,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:40:51,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:51,846] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:51,848] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:40:51,849] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:51,849] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:40:51,850] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:40:51,850] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:51,851] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:40:51,862] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-04-10 12:40:51,863] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-04-10 12:40:51,898] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-04-10 12:40:51,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-04-10 12:40:51,899] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-04-10 12:41:04,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:04,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 60.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5846688060478331, 6.911199999999999, 6.9112, 168.912956510431, 507300.3232283452, 507300.3232283458, 163807.8515014174]
[2019-04-10 12:41:04,558] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:41:04,561] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1981799e-33], sampled 0.25890312158533413
[2019-04-10 12:41:16,189] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:16,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.05, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7729200428468682, 6.9112, 6.9112, 168.912956510431, 650710.9722657675, 650710.9722657675, 196412.3435587643]
[2019-04-10 12:41:16,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:41:16,196] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.2648755e-38 0.0000000e+00 2.4990353e-38 5.4352770e-35], sampled 0.1180118252023411
[2019-04-10 12:41:25,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:25,357] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.23333333333333, 51.00000000000001, 1.0, 2.0, 0.5617751455942209, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9756176460176001, 6.9112, 6.9112, 168.9128813227426, 1570631.380285196, 1570631.380285196, 343704.0713635649]
[2019-04-10 12:41:25,358] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:41:25,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 7.0841637e-32 3.6482096e-35 2.4271260e-29 1.0597968e-18], sampled 0.400757774954535
[2019-04-10 12:41:25,361] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1570631.380285196 W.
[2019-04-10 12:41:30,103] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:30,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.60714227333333, 69.10586783333333, 1.0, 2.0, 1.030588204784777, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005994998805394, 6.9112, 168.9123159204833, 2337816.524415869, 2270565.944715095, 472902.9122966655]
[2019-04-10 12:41:30,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:41:30,106] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.4187232e-31 3.7514180e-34 1.3301539e-28 4.1201602e-18], sampled 0.9214898211647409
[2019-04-10 12:41:30,108] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2337816.524415869 W.
[2019-04-10 12:41:32,772] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:32,773] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 71.0, 1.0, 1.0, 0.6224984545777923, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128372109253, 869914.9683172797, 869914.9683172797, 205011.6254898584]
[2019-04-10 12:41:32,776] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:41:32,777] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.1108053e-30 7.4166347e-33 7.7687879e-29 3.1596534e-24], sampled 0.9121744675087701
[2019-04-10 12:41:32,778] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 869914.9683172797 W.
[2019-04-10 12:41:35,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:35,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.8792167, 83.28922328, 1.0, 2.0, 0.5682010112336698, 0.0, 2.0, 0.0, 1.0, 2.0, 0.986777249567083, 6.9112, 6.9112, 168.912792613261, 1588610.49461843, 1588610.49461843, 347743.0789330748]
[2019-04-10 12:41:35,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:41:35,954] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.4383916e-29 6.8585105e-32 3.3163161e-26 1.0134950e-15], sampled 0.9733629159810706
[2019-04-10 12:41:35,955] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1588610.49461843 W.
[2019-04-10 12:41:35,982] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:35,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.937401545, 85.27178549499999, 1.0, 2.0, 0.6581572844371701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 919768.2352286527, 919768.2352286527, 212082.0334995684]
[2019-04-10 12:41:35,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:41:35,985] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.6171802e-32 2.1915481e-35 1.1565826e-30 4.9307972e-23], sampled 0.22652348855463333
[2019-04-10 12:41:35,986] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 919768.2352286527 W.
[2019-04-10 12:41:53,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:41:53,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.08446771, 83.9370967, 1.0, 2.0, 0.6824462107849834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953727.0083735688, 953727.0083735682, 217110.9643970407]
[2019-04-10 12:41:53,202] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:41:53,214] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.5159110e-28 3.4613169e-31 9.7500213e-26 1.7092735e-14], sampled 0.39942492587597156
[2019-04-10 12:41:53,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 953727.0083735688 W.
[2019-04-10 12:42:02,780] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:42:02,780] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.10828652333333, 87.11720624, 1.0, 2.0, 0.6392111580782327, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.966096697715727, 6.9112, 168.9125480844777, 1787312.105089774, 1748366.59250091, 373235.6346769983]
[2019-04-10 12:42:02,780] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:42:02,805] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.4065323e-27 1.6379836e-29 8.5898393e-25 3.2945963e-15], sampled 0.08946877060719682
[2019-04-10 12:42:02,805] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1787312.105089774 W.
[2019-04-10 12:42:19,378] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.01018158], dtype=float32), 0.036960207]
[2019-04-10 12:42:19,379] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.6, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9063047332803769, 6.911200000000001, 6.9112, 168.912956510431, 746388.6189448849, 746388.6189448842, 224831.8552102841]
[2019-04-10 12:42:19,379] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:42:19,388] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2630638e-36], sampled 0.7425951169340924
[2019-04-10 12:42:37,487] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4826 3319658082.8562 2143.0000
[2019-04-10 12:42:37,703] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8068 3185274442.1803 2464.0000
[2019-04-10 12:42:37,865] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.3528 3105883788.3095 2014.0000
[2019-04-10 12:42:37,891] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.4111 2937935202.6096 1383.0000
[2019-04-10 12:42:38,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8870 2989446328.4271 1566.0000
[2019-04-10 12:42:39,091] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 350000, evaluation results [350000.0, 7287.48261098085, 3319658082.856155, 2143.0, 7346.352778638047, 3105883788.309514, 2014.0, 8060.411084446975, 2937935202.6096106, 1383.0, 7029.80681676529, 3185274442.1802917, 2464.0, 7923.886968186372, 2989446328.4270563, 1566.0]
[2019-04-10 12:42:40,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9128039e-01 5.4715811e-14 1.2151313e-15 4.3904283e-11 2.0871960e-01], sum to 1.0000
[2019-04-10 12:42:41,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4058
[2019-04-10 12:42:41,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2489282.609906631 W.
[2019-04-10 12:42:41,016] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8899949128265006, 1.0, 1.0, 0.8899949128265006, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2489282.609906631, 2489282.609906631, 466065.665022964], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5232000.0000, 
sim time next is 5232600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9313770012227757, 1.0, 2.0, 0.9313770012227757, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2605147.432748445, 2605147.432748445, 488901.0309902747], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9173216882202117, 1.0, 1.0, 0.9173216882202117, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7236520646523458, 0.7236520646523458, 0.7297030313287681], 
reward next is 0.2703, 
noisyNet noise sample is [array([-0.43746203], dtype=float32), -0.0056816577]. 
=============================================
[2019-04-10 12:42:41,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9394208e-01 1.3669905e-14 2.0767901e-16 4.1481753e-12 6.0579209e-03], sum to 1.0000
[2019-04-10 12:42:41,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2536
[2019-04-10 12:42:41,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2178682.59453074 W.
[2019-04-10 12:42:41,930] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.51936290369431, 1.0, 2.0, 0.51936290369431, 1.0, 2.0, 0.9019616077801803, 6.911199999999999, 6.9112, 170.5573041426782, 2178682.59453074, 2178682.594530741, 428794.6639855106], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5229600.0000, 
sim time next is 5230200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.922060125659637, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005989109479366, 6.9112, 168.9123931538525, 2185915.022529555, 2118668.59015581, 440235.4748552502], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9060965369393217, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00947891094793656, 0.0, 0.8294371788135965, 0.6071986173693208, 0.5885190528210583, 0.657067872918284], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8030564], dtype=float32), -1.0618476]. 
=============================================
[2019-04-10 12:42:42,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9999917e-01 4.4792663e-19 5.0719912e-21 9.7985714e-16 8.1133976e-07], sum to 1.0000
[2019-04-10 12:42:42,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2308
[2019-04-10 12:42:42,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2697686.788879597 W.
[2019-04-10 12:42:42,761] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.964425448152566, 1.0, 2.0, 0.964425448152566, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2697686.788879597, 2697686.788879598, 507857.3190824719], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5245200.0000, 
sim time next is 5245800.0000, 
raw observation next is [30.83333333333334, 70.83333333333334, 1.0, 2.0, 0.2780649003698515, 1.0, 2.0, 0.2780649003698515, 1.0, 1.0, 0.4829067744746873, 6.9112, 6.9112, 170.5573041426782, 1165907.647133074, 1165907.647133074, 295667.7168077492], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7083333333333335, 1.0, 1.0, 0.13019867514439937, 1.0, 1.0, 0.13019867514439937, 1.0, 0.5, 0.3693985054569357, 0.0, 0.0, 0.8375144448122397, 0.32386323531474276, 0.32386323531474276, 0.44129509971305847], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27162686], dtype=float32), -0.3693954]. 
=============================================
[2019-04-10 12:42:48,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3581196e-09 2.1219734e-14 6.6144136e-16 5.2953964e-11 1.0000000e+00], sum to 1.0000
[2019-04-10 12:42:48,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9364
[2019-04-10 12:42:48,020] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.1, 64.0, 1.0, 2.0, 0.685050526351737, 1.0, 2.0, 0.6631153026901311, 1.0, 2.0, 1.03, 7.005096553433505, 6.9112, 170.5573041426782, 2782389.204269019, 2715127.283577008, 516649.9949255717], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5392800.0000, 
sim time next is 5393400.0000, 
raw observation next is [34.28333333333333, 63.33333333333334, 1.0, 2.0, 0.8584800627772151, 1.0, 2.0, 0.7498300709028701, 1.0, 2.0, 1.03, 7.005110231490343, 6.9112, 170.5573041426782, 3146697.117674165, 3079425.398833652, 576059.583586891], 
processed observation next is [1.0, 0.43478260869565216, 0.8238546603475513, 0.6333333333333334, 1.0, 1.0, 0.8294940515388134, 1.0, 1.0, 0.6985904468709279, 1.0, 1.0, 1.0365853658536586, 0.00939102314903426, 0.0, 0.8375144448122397, 0.874082532687268, 0.8553959441204589, 0.8597904232640163], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2051654], dtype=float32), -0.008995801]. 
=============================================
[2019-04-10 12:42:48,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.9861529e-09 2.5459580e-12 3.8923629e-15 4.6163334e-10 1.0000000e+00], sum to 1.0000
[2019-04-10 12:42:48,573] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4899
[2019-04-10 12:42:48,575] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.45, 54.0, 1.0, 2.0, 0.705896600804439, 1.0, 2.0, 0.6735383399164822, 1.0, 2.0, 1.03, 7.005098197102066, 6.9112, 170.5573041426782, 2826173.070356596, 2758909.972237933, 523200.2838953881], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5409000.0000, 
sim time next is 5409600.0000, 
raw observation next is [37.63333333333333, 54.0, 1.0, 2.0, 0.8439196534514832, 1.0, 2.0, 0.7425498662400042, 1.0, 2.0, 1.03, 7.005109082831942, 6.9112, 170.5573041426782, 3116107.281820813, 3048836.385811037, 570642.0507668824], 
processed observation next is [1.0, 0.6086956521739131, 0.9826224328593997, 0.54, 1.0, 1.0, 0.8119513897005822, 1.0, 1.0, 0.6898191159518122, 1.0, 1.0, 1.0365853658536586, 0.009390908283194221, 0.0, 0.8375144448122397, 0.865585356061337, 0.8468989960586214, 0.8517045533834066], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3155832], dtype=float32), 2.063045]. 
=============================================
[2019-04-10 12:43:01,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.3768233e-35 0.0000000e+00 2.5966031e-31 1.4905235e-31], sum to 1.0000
[2019-04-10 12:43:01,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5158
[2019-04-10 12:43:01,792] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8739200056937145, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 217578.1457620178], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5705400.0000, 
sim time next is 5706000.0000, 
raw observation next is [26.5, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8709078800237916, 6.911199999999999, 6.9112, 168.912956510431, 722274.8581099614, 722274.8581099621, 216900.0821913087], 
processed observation next is [0.0, 0.043478260869565216, 0.4549763033175356, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8425705853948677, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20063190503054482, 0.200631905030545, 0.32373146595717717], 
reward next is 0.6763, 
noisyNet noise sample is [array([-1.1173813], dtype=float32), -0.009116313]. 
=============================================
[2019-04-10 12:43:01,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.53387 ]
 [62.86215 ]
 [63.280838]
 [63.699345]
 [64.37088 ]], R is [[62.30452728]
 [62.35673904]
 [62.40799713]
 [62.45986938]
 [62.51267624]].
[2019-04-10 12:43:02,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.8059024e-38 0.0000000e+00 9.5536924e-37 5.1498042e-36], sum to 1.0000
[2019-04-10 12:43:02,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4823
[2019-04-10 12:43:02,946] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.8, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8767685731416991, 6.911199999999999, 6.9112, 168.912956510431, 724759.3202641666, 724759.3202641673, 218140.4747634862], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5727600.0000, 
sim time next is 5728200.0000, 
raw observation next is [27.96666666666667, 79.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8799335825403639, 6.9112, 6.9112, 168.912956510431, 727048.3299479038, 727048.3299479038, 218845.8504966047], 
processed observation next is [0.0, 0.30434782608695654, 0.524486571879937, 0.7916666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8535775396833705, 0.0, 0.0, 0.8294399451523027, 0.20195786942997326, 0.20195786942997326, 0.3266355977561264], 
reward next is 0.6734, 
noisyNet noise sample is [array([0.08167037], dtype=float32), -0.44330755]. 
=============================================
[2019-04-10 12:43:18,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4615441e-03 1.9434381e-09 1.2863681e-13 4.3649743e-07 9.9253803e-01], sum to 1.0000
[2019-04-10 12:43:18,649] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0791
[2019-04-10 12:43:18,652] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5386790202055644, 1.0, 2.0, 0.5386790202055644, 1.0, 2.0, 0.9313415045547762, 6.9112, 6.9112, 170.5573041426782, 2259790.408706541, 2259790.408706541, 442106.9156632934], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6090600.0000, 
sim time next is 6091200.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.5614451103431051, 1.0, 2.0, 0.9709127984406495, 6.9112, 6.9112, 170.5573041426782, 2355385.50732503, 2355385.50732503, 459388.6723088129], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.65, 1.0, 1.0, 0.4716206148712109, 1.0, 1.0, 0.4716206148712109, 1.0, 1.0, 0.9645278029764017, 0.0, 0.0, 0.8375144448122397, 0.6542737520347305, 0.6542737520347305, 0.685654734789273], 
reward next is 0.3143, 
noisyNet noise sample is [array([1.2254101], dtype=float32), 1.7012223]. 
=============================================
[2019-04-10 12:43:18,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 8.0608037e-30 8.1386143e-35 2.0159710e-27 4.8077678e-29], sum to 1.0000
[2019-04-10 12:43:18,894] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3529
[2019-04-10 12:43:18,898] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.3, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8912558454034935, 6.9112, 6.9112, 168.912956510431, 730833.3393894724, 730833.3393894724, 221215.5985820083], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6112800.0000, 
sim time next is 6113400.0000, 
raw observation next is [29.11666666666667, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8944800963274712, 6.9112, 6.9112, 168.912956510431, 733702.6034015524, 733702.6034015524, 221966.6552278954], 
processed observation next is [1.0, 0.782608695652174, 0.5789889415481835, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8713171906432574, 0.0, 0.0, 0.8294399451523027, 0.20380627872265344, 0.20380627872265344, 0.3312935152655155], 
reward next is 0.6687, 
noisyNet noise sample is [array([-0.97991973], dtype=float32), -0.70890605]. 
=============================================
[2019-04-10 12:43:18,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9901032e-01 9.4228063e-12 2.3400232e-14 2.2030053e-09 9.8960462e-04], sum to 1.0000
[2019-04-10 12:43:18,961] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2656
[2019-04-10 12:43:18,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2479760.277881295 W.
[2019-04-10 12:43:18,970] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.171686749800065, 6.9112, 168.9115781017699, 2479760.277881295, 2294963.52035131, 476074.2704365465], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6091200.0000, 
sim time next is 6091800.0000, 
raw observation next is [31.01666666666667, 65.0, 1.0, 2.0, 0.7125898871186456, 1.0, 1.0, 0.7125898871186456, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1992662.565068079, 1992662.565068079, 379559.5326594123], 
processed observation next is [1.0, 0.5217391304347826, 0.6690363349131123, 0.65, 1.0, 1.0, 0.6537227555646332, 1.0, 0.5, 0.6537227555646332, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5535173791855775, 0.5535173791855775, 0.5665067651633019], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09182876], dtype=float32), 0.4042167]. 
=============================================
[2019-04-10 12:43:18,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.1781387e-30 1.3944637e-34 6.9814816e-28 1.4850943e-28], sum to 1.0000
[2019-04-10 12:43:18,985] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-10 12:43:18,993] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.75, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8943524383718803, 6.9112, 6.9112, 168.912956510431, 733954.1797674065, 733954.1797674065, 221951.9989551223], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6114600.0000, 
sim time next is 6115200.0000, 
raw observation next is [28.56666666666666, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8949476386820686, 6.911199999999999, 6.9112, 168.912956510431, 734641.2827991112, 734641.2827991118, 222097.4006345918], 
processed observation next is [1.0, 0.782608695652174, 0.5529225908372825, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.871887364246425, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20406702299975313, 0.20406702299975327, 0.33148865766356983], 
reward next is 0.6685, 
noisyNet noise sample is [array([-0.15200604], dtype=float32), 1.3218962]. 
=============================================
[2019-04-10 12:43:19,643] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 12:43:19,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:43:19,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:43:19,646] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:43:19,647] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:43:19,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:43:19,649] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:43:19,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:43:19,649] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:43:19,649] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:43:19,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:43:19,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-04-10 12:43:19,662] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-04-10 12:43:19,676] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-04-10 12:43:19,707] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-04-10 12:43:19,727] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-04-10 12:44:02,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:02,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.6, 75.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 1.0, 1.03, 9.317605986753234, 6.9112, 178.6582176852504, 5548147.153288831, 3742465.493882944, 693062.3568042114]
[2019-04-10 12:44:02,111] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:44:02,115] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6122882e-03 5.2228089e-10 3.4981809e-12 1.5011024e-06 9.9838626e-01], sampled 0.5156817514963785
[2019-04-10 12:44:08,481] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:08,481] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.93333333333334, 71.33333333333334, 1.0, 2.0, 0.8808582629831581, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005604229936997, 6.9112, 168.9123203501256, 2128243.70999876, 2061270.352374756, 428802.8933250838]
[2019-04-10 12:44:08,482] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:44:08,483] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.7257187e-19 3.9585775e-23 1.5217904e-15 3.1268117e-09], sampled 0.8161474703340998
[2019-04-10 12:44:08,484] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2128243.70999876 W.
[2019-04-10 12:44:26,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:26,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.93333333333333, 61.0, 1.0, 2.0, 0.8298744675633002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1159872.33785061, 1159872.33785061, 251376.4137043772]
[2019-04-10 12:44:26,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:44:26,164] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999988e-01 2.3083277e-18 1.5073187e-21 3.7185839e-15 8.0138605e-08], sampled 0.632508765264952
[2019-04-10 12:44:26,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1159872.33785061 W.
[2019-04-10 12:44:52,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:52,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.73333333333333, 78.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7096616286760403, 6.9112, 6.9112, 168.912956510431, 604483.2407646085, 604483.2407646085, 184462.5415068839]
[2019-04-10 12:44:52,590] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:44:52,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 7.7063494e-32 2.2433726e-36 5.4928161e-31 2.9375366e-31], sampled 0.7303285087917252
[2019-04-10 12:44:54,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:54,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9784569405814196, 6.9112, 6.9112, 168.912956510431, 790746.279113856, 790746.279113856, 241679.3329436692]
[2019-04-10 12:44:54,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:44:54,075] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.0999742e-31 2.6887783e-36 1.0033931e-30 5.7458911e-31], sampled 0.7580990229816922
[2019-04-10 12:44:56,206] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.02511594], dtype=float32), 0.0315832]
[2019-04-10 12:44:56,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.43333333333333, 54.0, 1.0, 2.0, 0.928678141370351, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989541517446, 6.9112, 168.9123159518504, 2195178.1493583, 2127931.441218252, 442132.3406778591]
[2019-04-10 12:44:56,209] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:44:56,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.2074229e-22 5.0659854e-27 1.3876978e-19 2.9307625e-14], sampled 0.44709444305368384
[2019-04-10 12:44:56,212] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2195178.1493583 W.
[2019-04-10 12:44:59,005] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2698 3319764682.9529 2139.0000
[2019-04-10 12:44:59,208] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7345.5623 3106562859.4982 2022.0000
[2019-04-10 12:44:59,351] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.9346 2937965618.5153 1372.0000
[2019-04-10 12:44:59,416] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7028.8030 3185286082.0506 2464.0000
[2019-04-10 12:44:59,457] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.0351 2989502389.7838 1567.0000
[2019-04-10 12:45:00,471] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 375000, evaluation results [375000.0, 7288.269753812669, 3319764682.952862, 2139.0, 7345.56227574409, 3106562859.498194, 2022.0, 8062.934589671188, 2937965618.515286, 1372.0, 7028.802980371256, 3185286082.050608, 2464.0, 7923.0351300122975, 2989502389.7838407, 1567.0]
[2019-04-10 12:45:07,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:45:07,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4068
[2019-04-10 12:45:07,720] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.55, 77.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8935919171138564, 6.911199999999999, 6.9112, 168.912956510431, 735718.1373429636, 735718.1373429642, 221872.560134357], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6289800.0000, 
sim time next is 6290400.0000, 
raw observation next is [28.4, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8970901368396405, 6.9112, 6.9112, 168.912956510431, 738134.5754765758, 738134.5754765758, 222662.156632082], 
processed observation next is [0.0, 0.8260869565217391, 0.5450236966824644, 0.7833333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8745001668776103, 0.0, 0.0, 0.8294399451523027, 0.2050373820768266, 0.2050373820768266, 0.33233157706280897], 
reward next is 0.6677, 
noisyNet noise sample is [array([1.1777872], dtype=float32), -0.6160846]. 
=============================================
[2019-04-10 12:45:08,360] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:45:08,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3409
[2019-04-10 12:45:08,373] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 85.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8980368857573098, 6.911199999999999, 6.9112, 168.912956510431, 739244.3939393072, 739244.3939393079, 222894.2766268394], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6304200.0000, 
sim time next is 6304800.0000, 
raw observation next is [27.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8965749596026933, 6.911199999999999, 6.9112, 168.912956510431, 738100.1267763721, 738100.1267763727, 222558.2541824558], 
processed observation next is [0.0, 1.0, 0.4928909952606636, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.873871901954504, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2050278129934367, 0.20502781299343686, 0.3321764987797848], 
reward next is 0.6678, 
noisyNet noise sample is [array([0.301771], dtype=float32), -0.026318034]. 
=============================================
[2019-04-10 12:45:12,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 3.5325747e-24 4.9647608e-27 2.3328440e-22 5.9174255e-21], sum to 1.0000
[2019-04-10 12:45:12,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5457
[2019-04-10 12:45:12,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3883979.733742341 W.
[2019-04-10 12:45:12,460] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.98333333333333, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 11.17948081911176, 6.9112, 168.8956379228805, 3883979.733742341, 856224.8842752615, 256096.8385583784], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6401400.0000, 
sim time next is 6402000.0000, 
raw observation next is [26.96666666666667, 83.66666666666667, 1.0, 1.0, 0.5359527192344932, 1.0, 1.0, 0.5359527192344932, 1.0, 2.0, 0.9160996991302729, 6.9112, 6.9112, 170.5573041426782, 2248343.126916047, 2248343.126916047, 438010.7825970702], 
processed observation next is [1.0, 0.08695652173913043, 0.47709320695102697, 0.8366666666666667, 1.0, 0.5, 0.44090689064396765, 1.0, 0.5, 0.44090689064396765, 1.0, 1.0, 0.8976825599149668, 0.0, 0.0, 0.8375144448122397, 0.6245397574766798, 0.6245397574766798, 0.653747436712045], 
reward next is 0.3463, 
noisyNet noise sample is [array([-1.4950886], dtype=float32), -0.5459835]. 
=============================================
[2019-04-10 12:45:12,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[44.959953]
 [49.84609 ]
 [49.361767]
 [49.47573 ]
 [49.718014]], R is [[42.68038559]
 [42.253582  ]
 [42.51183701]
 [42.76699448]
 [43.01902008]].
[2019-04-10 12:45:17,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6372855e-02 6.3229456e-13 1.7883608e-16 4.9802884e-10 9.4362712e-01], sum to 1.0000
[2019-04-10 12:45:17,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6404
[2019-04-10 12:45:17,646] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.06666666666667, 87.66666666666667, 1.0, 2.0, 0.2228511819503315, 1.0, 2.0, 0.2228511819503315, 1.0, 2.0, 0.3816880364907743, 6.9112, 6.9112, 170.5573041426782, 934299.3352785045, 934299.3352785045, 276188.4264439442], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6507600.0000, 
sim time next is 6508200.0000, 
raw observation next is [27.13333333333333, 87.33333333333333, 1.0, 2.0, 0.2197880173748822, 1.0, 2.0, 0.2197880173748822, 1.0, 2.0, 0.3764522835134113, 6.9112, 6.9112, 170.5573041426782, 921451.5587714934, 921451.5587714934, 275263.2184291761], 
processed observation next is [1.0, 0.30434782608695654, 0.484992101105845, 0.8733333333333333, 1.0, 1.0, 0.059985563102267714, 1.0, 1.0, 0.059985563102267714, 1.0, 1.0, 0.23957595550416014, 0.0, 0.0, 0.8375144448122397, 0.2559587663254148, 0.2559587663254148, 0.4108406245211583], 
reward next is 0.5892, 
noisyNet noise sample is [array([0.4253858], dtype=float32), 0.18611676]. 
=============================================
[2019-04-10 12:45:24,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.0896137e-25 2.9646379e-28 5.8504623e-24 1.0890836e-19], sum to 1.0000
[2019-04-10 12:45:24,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0957
[2019-04-10 12:45:24,958] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.85, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8528665062713122, 6.911199999999999, 6.9112, 168.912956510431, 714060.8127822543, 714060.812782255, 213082.0306635495], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6672600.0000, 
sim time next is 6673200.0000, 
raw observation next is [24.93333333333334, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8571697052479603, 6.911199999999999, 6.9112, 168.912956510431, 717065.2541081898, 717065.2541081905, 214017.2777033264], 
processed observation next is [1.0, 0.21739130434782608, 0.3807266982622437, 0.9433333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8258167137170248, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19918479280783052, 0.19918479280783072, 0.3194287726915319], 
reward next is 0.6806, 
noisyNet noise sample is [array([1.2284687], dtype=float32), -0.24455307]. 
=============================================
[2019-04-10 12:45:26,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.8044145e-31 2.6803807e-36 2.2525453e-31 1.8719922e-29], sum to 1.0000
[2019-04-10 12:45:26,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-10 12:45:26,825] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.46666666666667, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6454556727977668, 6.9112, 6.9112, 168.912956510431, 558275.86428137, 558275.86428137, 173327.3971759118], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6733200.0000, 
sim time next is 6733800.0000, 
raw observation next is [25.35, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6429688991822017, 6.911199999999999, 6.9112, 168.912956510431, 556491.2114429169, 556491.2114429175, 172915.2456829403], 
processed observation next is [1.0, 0.9565217391304348, 0.4004739336492892, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5645962185148801, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1545808920674769, 0.15458089206747708, 0.2580824562431945], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.79836977], dtype=float32), -0.17804405]. 
=============================================
[2019-04-10 12:45:32,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:45:32,056] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9633
[2019-04-10 12:45:32,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.15, 80.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5945164849477896, 6.911199999999999, 6.9112, 168.912956510431, 518834.3089583835, 518834.3089583841, 165226.0381371815], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6834600.0000, 
sim time next is 6835200.0000, 
raw observation next is [23.13333333333333, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5958623451397956, 6.911199999999999, 6.9112, 168.912956510431, 519975.3937631316, 519975.3937631322, 165429.6599355778], 
processed observation next is [0.0, 0.08695652173913043, 0.29541864139020524, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5071492013899946, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14443760937864766, 0.14443760937864783, 0.24690994020235493], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.3435278], dtype=float32), -0.05939912]. 
=============================================
[2019-04-10 12:45:35,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 3.3928267e-32 4.3102295e-37 2.5987348e-30 7.0810907e-30], sum to 1.0000
[2019-04-10 12:45:35,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8982
[2019-04-10 12:45:35,689] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.8, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7278270818046931, 6.9112, 6.9112, 168.912956510431, 619883.8812531757, 619883.8812531757, 187811.0365333011], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6916800.0000, 
sim time next is 6917400.0000, 
raw observation next is [24.75, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7282389352668995, 6.911200000000001, 6.9112, 168.912956510431, 620249.6057836277, 620249.6057836271, 187888.1033059874], 
processed observation next is [0.0, 0.043478260869565216, 0.3720379146919432, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6685840673986578, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1722915571621188, 0.17229155716211864, 0.28043000493430953], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.9704475], dtype=float32), -0.31266984]. 
=============================================
[2019-04-10 12:45:36,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:45:36,322] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8671
[2019-04-10 12:45:36,326] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.23333333333333, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7304282488134751, 6.911200000000001, 6.9112, 168.912956510431, 622474.592731903, 622474.5927319024, 188300.3847245593], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6923400.0000, 
sim time next is 6924000.0000, 
raw observation next is [24.16666666666667, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7261920364755584, 6.9112, 6.9112, 168.912956510431, 619006.1099668499, 619006.1099668499, 187509.290852095], 
processed observation next is [0.0, 0.13043478260869565, 0.34439178515007923, 0.8866666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.666087849360437, 0.0, 0.0, 0.8294399451523027, 0.1719461416574583, 0.1719461416574583, 0.2798646132120821], 
reward next is 0.7201, 
noisyNet noise sample is [array([0.12000647], dtype=float32), 1.7063428]. 
=============================================
[2019-04-10 12:45:36,335] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.01615 ]
 [82.80496 ]
 [82.431755]
 [82.09231 ]
 [81.67374 ]], R is [[82.98088837]
 [82.87003326]
 [82.75962067]
 [82.65078735]
 [82.54360962]].
[2019-04-10 12:45:40,367] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.6824333e-18 6.2416871e-20 1.4023776e-15 2.5100916e-10], sum to 1.0000
[2019-04-10 12:45:40,372] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3772
[2019-04-10 12:45:40,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1010574.219327909 W.
[2019-04-10 12:45:40,380] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.4, 71.0, 1.0, 2.0, 0.708394220149053, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010574.219327909, 1010574.219327909, 225428.6799455514], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7026000.0000, 
sim time next is 7026600.0000, 
raw observation next is [27.55, 70.0, 1.0, 2.0, 0.3473360789396653, 0.0, 2.0, 0.0, 1.0, 1.0, 0.587112917898942, 6.9112, 6.9112, 168.912956510431, 992878.2295833045, 992878.2295833045, 238021.5216029903], 
processed observation next is [1.0, 0.30434782608695654, 0.504739336492891, 0.7, 1.0, 1.0, 0.21365792643333167, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.4964791681694415, 0.0, 0.0, 0.8294399451523027, 0.2757995082175846, 0.2757995082175846, 0.35525600239252286], 
reward next is 0.6447, 
noisyNet noise sample is [array([1.7820312], dtype=float32), -1.4149538]. 
=============================================
[2019-04-10 12:45:40,901] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-10 12:45:40,902] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:45:40,903] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:40,903] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:45:40,903] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:40,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:45:40,904] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:45:40,905] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:45:40,906] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:40,907] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:40,905] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:45:40,924] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-04-10 12:45:40,924] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-04-10 12:45:40,958] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-04-10 12:45:40,958] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-04-10 12:45:40,974] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-04-10 12:45:53,939] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:45:53,940] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [18.0, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4187378620927768, 6.9112, 6.9112, 168.912956510431, 377077.0301126835, 377077.0301126835, 142357.4953600869]
[2019-04-10 12:45:53,943] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:45:53,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.3305223e-31 1.0098379e-35 1.1586051e-30 4.0551444e-30], sampled 0.3651421575576278
[2019-04-10 12:45:55,479] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:45:55,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.06666666666667, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5859452804119522, 6.9112, 6.9112, 168.912956510431, 512586.8352017128, 512586.8352017128, 163916.5992184605]
[2019-04-10 12:45:55,481] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:45:55,484] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.6093300e-33 6.9800616e-38 9.6473196e-33 1.0376946e-32], sampled 0.6099877462726919
[2019-04-10 12:46:05,174] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:05,175] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.76666666666667, 76.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7425696677218399, 6.9112, 6.9112, 168.912956510431, 634200.4541225189, 634200.4541225189, 190604.4569895752]
[2019-04-10 12:46:05,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:05,179] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.5489997e-33 2.2007278e-37 4.0257323e-32 3.6296556e-32], sampled 0.21113107933370123
[2019-04-10 12:46:09,144] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:09,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9382771602397164, 6.911200000000001, 6.9112, 168.912956510431, 766802.4021754176, 766802.4021754169, 232193.0368855122]
[2019-04-10 12:46:09,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:09,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.0239852e-34 0.0000000e+00 9.6377882e-34 7.2029110e-34], sampled 0.6207254686084036
[2019-04-10 12:46:13,307] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:13,309] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.73333333333333, 67.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.92016817144116, 6.9112, 168.9073361717607, 2170018.201534691, 1454245.271829772, 311353.5064469181]
[2019-04-10 12:46:13,310] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:46:13,312] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999893e-01 2.0171155e-19 2.3939096e-22 8.2963563e-16 1.0480866e-06], sampled 0.43569420018096316
[2019-04-10 12:46:13,313] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2170018.201534691 W.
[2019-04-10 12:46:16,320] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:16,321] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.70838416, 80.38058999, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.929348079808348, 6.9112, 168.9126319426653, 846161.3610613694, 833286.513375808, 255072.0400989006]
[2019-04-10 12:46:16,322] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:46:16,324] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.1774983e-28 2.6866103e-32 3.5941877e-26 3.2828078e-20], sampled 0.8698562515145628
[2019-04-10 12:46:42,296] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:42,298] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.25679292333334, 87.46754509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8675253437661, 6.911200000000001, 6.9112, 168.912956510431, 721973.5762004462, 721973.5762004456, 216219.9482476321]
[2019-04-10 12:46:42,299] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:46:42,300] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.1101771e-34 0.0000000e+00 1.8037107e-33 2.9705054e-33], sampled 0.6862766227804379
[2019-04-10 12:46:47,050] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:47,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.46666666666667, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8567254084016774, 6.911199999999999, 6.9112, 168.912956510431, 714765.9578114221, 714765.9578114228, 213867.8993797268]
[2019-04-10 12:46:47,053] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:47,057] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 6.5005033e-30 9.9293200e-34 1.4433617e-28 7.3679252e-28], sampled 0.8786188328147627
[2019-04-10 12:46:48,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:48,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.85, 91.0, 1.0, 2.0, 0.6413929851651113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 896330.408269062, 896330.4082690626, 208704.1366917441]
[2019-04-10 12:46:48,385] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:46:48,388] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.3163144e-26 2.9903497e-30 5.3201237e-24 1.5015141e-17], sampled 0.6600844073137746
[2019-04-10 12:46:48,388] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 896330.408269062 W.
[2019-04-10 12:46:50,585] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:50,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5986035692717472, 6.9112, 6.9112, 168.912956510431, 526178.344084324, 526178.344084324, 165740.7290392398]
[2019-04-10 12:46:50,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:50,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.5119433e-34 0.0000000e+00 7.9132048e-34 6.3051552e-34], sampled 0.21850289300374992
[2019-04-10 12:46:55,770] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:55,771] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7925367139886652, 6.9112, 6.9112, 168.912956510431, 665093.9799112281, 665093.9799112281, 200321.4606684884]
[2019-04-10 12:46:55,772] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:55,792] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.0269855e-35 0.0000000e+00 7.8902016e-35 1.5402367e-34], sampled 0.1658299062640961
[2019-04-10 12:46:59,215] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:46:59,215] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.23333333333333, 93.0, 1.0, 2.0, 0.5461693732398734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860964.6774027739, 860964.6774027739, 202432.8398471308]
[2019-04-10 12:46:59,216] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:46:59,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9999988e-01 8.6898932e-19 2.9369910e-21 9.5786446e-16 1.2708708e-07], sampled 0.16617724931719247
[2019-04-10 12:47:02,050] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.05038893], dtype=float32), 0.034038577]
[2019-04-10 12:47:02,050] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.36666666666667, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7333061792024501, 6.9112, 6.9112, 168.912956510431, 620454.5489172856, 620454.5489172856, 188796.9464663018]
[2019-04-10 12:47:02,051] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:47:02,058] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 3.6036989e-35 0.0000000e+00 1.2902783e-34 9.1148848e-35], sampled 0.9269721748889418
[2019-04-10 12:47:07,100] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7570 3319693054.9283 2137.0000
[2019-04-10 12:47:07,222] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6821 3106555801.7760 2022.0000
[2019-04-10 12:47:07,420] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.1253 2938228486.2535 1377.0000
[2019-04-10 12:47:07,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.9076 3185208863.4193 2464.0000
[2019-04-10 12:47:07,488] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.3116 2989523801.5447 1566.0000
[2019-04-10 12:47:08,510] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 400000, evaluation results [400000.0, 7289.757003441335, 3319693054.928314, 2137.0, 7347.68214751401, 3106555801.7759657, 2022.0, 8059.125256075674, 2938228486.2534943, 1377.0, 7029.90758490667, 3185208863.4192615, 2464.0, 7923.311578635915, 2989523801.5447392, 1566.0]
[2019-04-10 12:47:09,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.6447206e-21 2.5700248e-24 9.0794628e-18 9.9802847e-11], sum to 1.0000
[2019-04-10 12:47:09,281] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1449
[2019-04-10 12:47:09,281] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1907749.620675823 W.
[2019-04-10 12:47:09,312] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.95, 45.5, 1.0, 2.0, 0.6930211731112509, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.93883494688354, 6.9112, 168.912099769759, 1907749.620675823, 1888144.5376743, 390185.4474326221], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7047000.0000, 
sim time next is 7047600.0000, 
raw observation next is [32.03333333333334, 45.0, 1.0, 2.0, 0.4297473124056067, 1.0, 1.0, 0.4297473124056067, 1.0, 2.0, 0.7228888843299063, 6.911200000000001, 6.9112, 170.5573041426782, 1830551.358257268, 1830551.358257268, 368682.6941615707], 
processed observation next is [1.0, 0.5652173913043478, 0.7172195892575042, 0.45, 1.0, 1.0, 0.31294856916338154, 1.0, 0.5, 0.31294856916338154, 1.0, 1.0, 0.6620596150364709, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5084864884047967, 0.5084864884047967, 0.5502726778530905], 
reward next is 0.4497, 
noisyNet noise sample is [array([0.60843265], dtype=float32), -0.79961413]. 
=============================================
[2019-04-10 12:47:14,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00000000e+00 5.60589645e-24 2.88752462e-27 4.58303982e-21
 1.20306745e-17], sum to 1.0000
[2019-04-10 12:47:14,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9473
[2019-04-10 12:47:14,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1232320.711268435 W.
[2019-04-10 12:47:14,029] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.08333333333334, 76.83333333333334, 1.0, 2.0, 0.440842629441957, 1.0, 2.0, 0.440842629441957, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1232320.711268435, 1232320.711268435, 282893.5703069628], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7132200.0000, 
sim time next is 7132800.0000, 
raw observation next is [26.96666666666667, 77.66666666666667, 1.0, 2.0, 0.3355691440062022, 1.0, 2.0, 0.3355691440062022, 1.0, 1.0, 0.5632906654742957, 6.9112, 6.9112, 170.5573041426782, 1407177.36416082, 1407177.36416082, 317818.7461422158], 
processed observation next is [1.0, 0.5652173913043478, 0.47709320695102697, 0.7766666666666667, 1.0, 1.0, 0.1994808963930147, 1.0, 1.0, 0.1994808963930147, 1.0, 0.5, 0.46742764082231186, 0.0, 0.0, 0.8375144448122397, 0.3908826011557833, 0.3908826011557833, 0.4743563375256952], 
reward next is 0.5256, 
noisyNet noise sample is [array([-0.48586276], dtype=float32), 0.59770244]. 
=============================================
[2019-04-10 12:47:19,430] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.1262909e-17 1.4901234e-19 1.0072209e-14 5.5413707e-08], sum to 1.0000
[2019-04-10 12:47:19,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-10 12:47:19,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1935876.854577789 W.
[2019-04-10 12:47:19,453] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 82.33333333333334, 1.0, 2.0, 0.6923012553698327, 1.0, 2.0, 0.6923012553698327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1935876.854577789, 1935876.854577789, 370851.5964498004], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7204800.0000, 
sim time next is 7205400.0000, 
raw observation next is [29.0, 81.5, 1.0, 2.0, 0.6994222410216188, 1.0, 2.0, 0.6994222410216188, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1955807.389049034, 1955807.389049034, 373881.7191302744], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.815, 1.0, 1.0, 0.6378581217127937, 1.0, 1.0, 0.6378581217127937, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5432798302913984, 0.5432798302913984, 0.5580324166123498], 
reward next is 0.4420, 
noisyNet noise sample is [array([-0.37904826], dtype=float32), 0.6544589]. 
=============================================
[2019-04-10 12:47:35,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:47:35,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-10 12:47:35,456] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 59.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7196826200057029, 6.911200000000001, 6.9112, 168.912956510431, 611936.4929325556, 611936.492932555, 186290.5826763104], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7566600.0000, 
sim time next is 7567200.0000, 
raw observation next is [29.0, 59.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7114029606513368, 6.9112, 6.9112, 168.912956510431, 605774.6591884283, 605774.6591884283, 184778.2791433435], 
processed observation next is [0.0, 0.6086956521739131, 0.5734597156398105, 0.59, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6480523910382157, 0.0, 0.0, 0.8294399451523027, 0.1682707386634523, 0.1682707386634523, 0.2757884763333485], 
reward next is 0.7242, 
noisyNet noise sample is [array([1.7855728], dtype=float32), -0.44696438]. 
=============================================
[2019-04-10 12:47:37,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.0765266e-32 1.9893534e-36 4.5293119e-31 2.7433154e-29], sum to 1.0000
[2019-04-10 12:47:37,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1823
[2019-04-10 12:47:37,541] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.9, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7541971167068927, 6.9112, 6.9112, 168.912956510431, 638235.8368343628, 638235.8368343628, 192789.0903789737], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7610400.0000, 
sim time next is 7611000.0000, 
raw observation next is [23.86666666666667, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.332058850261001, 6.9112, 168.9110849819868, 1170873.940971971, 872305.4958801704, 256545.7381168591], 
processed observation next is [1.0, 0.08695652173913043, 0.33017377567140627, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.04208588502610011, 0.0, 0.8294307550908522, 0.3252427613811031, 0.2423070821889362, 0.3829040867415808], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9361341], dtype=float32), 0.68626374]. 
=============================================
[2019-04-10 12:47:37,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[61.822754]
 [61.230705]
 [61.317013]
 [61.500305]
 [61.598766]], R is [[61.08573151]
 [61.18712997]
 [61.28636169]
 [61.38331985]
 [61.47808075]].
[2019-04-10 12:47:39,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5241876e-01 3.1634106e-13 4.8562995e-15 1.2793897e-09 7.4758118e-01], sum to 1.0000
[2019-04-10 12:47:39,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1025
[2019-04-10 12:47:39,738] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1693666.793724296 W.
[2019-04-10 12:47:39,748] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.73333333333333, 73.5, 1.0, 2.0, 0.6057467173659951, 0.0, 2.0, 0.0, 1.0, 1.0, 1.011172293394155, 6.911199999999999, 6.9112, 168.9125022843931, 1693666.793724296, 1693666.793724297, 362097.0269195148], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7638600.0000, 
sim time next is 7639200.0000, 
raw observation next is [28.0, 72.0, 1.0, 2.0, 0.5643777189635185, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9426319946903166, 6.911199999999999, 6.9112, 168.912956397008, 1577913.157717127, 1577913.157717128, 337246.7409850735], 
processed observation next is [1.0, 0.43478260869565216, 0.5260663507109005, 0.72, 1.0, 1.0, 0.4751538782692994, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9300390179150202, -8.881784197001253e-17, 0.0, 0.8294399445953439, 0.4383092104769797, 0.43830921047698, 0.5033533447538411], 
reward next is 0.4966, 
noisyNet noise sample is [array([-0.01001492], dtype=float32), 0.74726856]. 
=============================================
[2019-04-10 12:47:45,063] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.7474602e-36 0.0000000e+00 2.6018415e-36 1.7614404e-37], sum to 1.0000
[2019-04-10 12:47:45,069] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5128
[2019-04-10 12:47:45,076] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 945156.597415517 W.
[2019-04-10 12:47:45,081] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.2, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.035809102197572, 6.9112, 168.9121965384024, 945156.597415517, 856755.0093398626, 256149.1522251907], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7797600.0000, 
sim time next is 7798200.0000, 
raw observation next is [26.31666666666666, 87.50000000000001, 1.0, 1.0, 0.3342833127795929, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5631021502175548, 6.911200000000001, 6.9112, 168.9128550484229, 934321.5845715808, 934321.5845715802, 231557.1770790352], 
processed observation next is [1.0, 0.2608695652173913, 0.4462875197472351, 0.8750000000000001, 1.0, 0.5, 0.1979317021440878, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4671977441677498, 8.881784197001253e-17, 0.0, 0.8294394469273934, 0.25953377349210577, 0.2595337734921056, 0.3456077269836346], 
reward next is 0.6544, 
noisyNet noise sample is [array([-0.43079683], dtype=float32), -0.27636495]. 
=============================================
[2019-04-10 12:47:51,135] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2077899e-04 5.1424629e-09 1.0495131e-11 1.0541898e-07 9.9947912e-01], sum to 1.0000
[2019-04-10 12:47:51,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-10 12:47:51,145] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.21666666666667, 78.50000000000001, 1.0, 2.0, 0.4603344468557539, 1.0, 2.0, 0.4603344468557539, 1.0, 2.0, 0.7898836531303526, 6.9112, 6.9112, 170.5573041426782, 1930840.153700464, 1930840.153700464, 387218.9299537302], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7895400.0000, 
sim time next is 7896000.0000, 
raw observation next is [28.33333333333334, 78.0, 1.0, 2.0, 0.4586815297022192, 1.0, 2.0, 0.4586815297022192, 1.0, 2.0, 0.7875252624801663, 6.9112, 6.9112, 170.5573041426782, 1923900.885189601, 1923900.885189601, 386259.7949685018], 
processed observation next is [1.0, 0.391304347826087, 0.5418641390205374, 0.78, 1.0, 1.0, 0.3478090719303846, 1.0, 1.0, 0.3478090719303846, 1.0, 1.0, 0.7408844664392272, 0.0, 0.0, 0.8375144448122397, 0.534416912552667, 0.534416912552667, 0.5765071566694057], 
reward next is 0.4235, 
noisyNet noise sample is [array([0.04354095], dtype=float32), -0.14697915]. 
=============================================
[2019-04-10 12:47:51,153] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[21.604652]
 [21.664385]
 [20.762415]
 [21.012127]
 [20.222515]], R is [[22.78112984]
 [22.97537994]
 [23.18305779]
 [23.37528992]
 [23.14153671]].
[2019-04-10 12:47:52,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.9655608e-16 4.8526935e-20 1.6862180e-14 3.6349100e-08], sum to 1.0000
[2019-04-10 12:47:52,226] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9961
[2019-04-10 12:47:52,229] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2175803.090657657 W.
[2019-04-10 12:47:52,235] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 63.0, 1.0, 2.0, 0.77801575909208, 1.0, 2.0, 0.77801575909208, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2175803.090657657, 2175803.090657657, 409303.1797435319], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7921800.0000, 
sim time next is 7922400.0000, 
raw observation next is [30.73333333333333, 62.66666666666666, 1.0, 2.0, 0.7417943694600224, 1.0, 2.0, 0.7417943694600224, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2074408.072918759, 2074408.072918758, 392522.9427691576], 
processed observation next is [1.0, 0.6956521739130435, 0.6556082148499209, 0.6266666666666666, 1.0, 1.0, 0.6889088788674969, 1.0, 1.0, 0.6889088788674969, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5762244646996553, 0.576224464699655, 0.5858551384614292], 
reward next is 0.4141, 
noisyNet noise sample is [array([0.81647986], dtype=float32), 0.93913805]. 
=============================================
[2019-04-10 12:47:52,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:52,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:52,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-04-10 12:47:52,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:52,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:52,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-04-10 12:47:52,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:52,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:52,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-04-10 12:47:52,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:52,737] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:52,739] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-04-10 12:47:52,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:52,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:52,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-04-10 12:47:53,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,018] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-04-10 12:47:53,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,196] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-04-10 12:47:53,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-04-10 12:47:53,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,243] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-04-10 12:47:53,262] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,263] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-04-10 12:47:53,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,480] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-04-10 12:47:53,510] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,512] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-04-10 12:47:53,538] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,540] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-04-10 12:47:53,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-04-10 12:47:53,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 12:47:53,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-04-10 12:47:53,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:53,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-04-10 12:47:55,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.25436885e-02 4.65994322e-04 2.45555770e-04 5.02864877e-03
 9.81716096e-01], sum to 1.0000
[2019-04-10 12:47:55,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-10 12:47:55,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.25, 86.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1873738368633301, 6.911199999999999, 6.9112, 170.5573041426782, 492964.1386797937, 492964.1386797944, 228876.6430720757], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1800.0000, 
sim time next is 2400.0000, 
raw observation next is [20.9, 85.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 475538.0943752091, 475538.0943752091, 226191.3589920628], 
processed observation next is [1.0, 0.0, 0.1895734597156398, 0.8566666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.13209391510422475, 0.13209391510422475, 0.3375990432717355], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5431706], dtype=float32), -1.2740219]. 
=============================================
[2019-04-10 12:47:56,724] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 12:47:56,726] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:47:56,726] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:47:56,727] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:47:56,727] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:47:56,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:56,729] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:56,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:47:56,730] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:56,728] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:56,731] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:47:56,745] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-04-10 12:47:56,746] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-04-10 12:47:56,763] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-04-10 12:47:56,779] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-04-10 12:47:56,813] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-04-10 12:48:07,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:48:07,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.71825227333333, 70.77137106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6073824322397398, 6.911200000000001, 6.9112, 168.912956510431, 528752.1375594273, 528752.1375594266, 167211.84289229]
[2019-04-10 12:48:07,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:48:07,129] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6129253015829077
[2019-04-10 12:48:12,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:48:12,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.73333333333333, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5138867937285349, 6.911200000000001, 6.9112, 168.912956510431, 456114.2524257403, 456114.2524257397, 153701.0270772523]
[2019-04-10 12:48:12,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:48:12,814] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3814335425775539
[2019-04-10 12:48:29,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:48:29,242] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.02409567666667, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.765444717824891, 6.911200000000001, 6.9112, 168.912956510431, 648763.8086696576, 648763.8086696569, 194999.3679132777]
[2019-04-10 12:48:29,243] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:48:29,246] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.6462588e-38 0.0000000e+00 1.0198835e-36 4.7613325e-38], sampled 0.46724798038942794
[2019-04-10 12:48:33,414] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:48:33,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.66666666666666, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8712663436781247, 6.9112, 6.9112, 168.912956510431, 720128.7562293817, 720128.7562293817, 216897.4315668838]
[2019-04-10 12:48:33,417] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:48:33,419] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 8.5882144e-37 0.0000000e+00 1.4858718e-34 9.9603956e-33], sampled 0.8178936363857725
[2019-04-10 12:49:01,452] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:49:01,454] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.07999483166667, 81.20615876166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9328963707783091, 6.911199999999999, 6.9112, 168.912956510431, 763631.1654103891, 763631.1654103897, 230949.599454468]
[2019-04-10 12:49:01,454] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:49:01,456] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 8.2480237e-38 0.0000000e+00 7.4367344e-37 1.7824271e-38], sampled 0.13456723929380265
[2019-04-10 12:49:06,887] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.06797605], dtype=float32), 0.042402007]
[2019-04-10 12:49:06,887] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.655307534228726, 6.9112, 6.9112, 168.912956510431, 565620.250718045, 565620.250718045, 174971.8625850313]
[2019-04-10 12:49:06,888] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:49:06,889] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.29273768755669716
[2019-04-10 12:49:18,825] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.7295 3319695974.6727 2143.0000
[2019-04-10 12:49:19,815] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.0429 2989458966.8815 1566.0000
[2019-04-10 12:49:20,156] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.0765 3185336382.6682 2464.0000
[2019-04-10 12:49:20,332] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8058.6535 2938153319.2928 1380.0000
[2019-04-10 12:49:20,377] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.0945 3106197392.3757 2018.0000
[2019-04-10 12:49:21,405] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 425000, evaluation results [425000.0, 7288.729518164765, 3319695974.6727147, 2143.0, 7349.094528728491, 3106197392.3756804, 2018.0, 8058.653476605709, 2938153319.292826, 1380.0, 7029.0764652394655, 3185336382.6681676, 2464.0, 7924.042862822708, 2989458966.8815045, 1566.0]
[2019-04-10 12:49:30,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9106211e-02 4.8918934e-19 7.2600846e-22 1.2268450e-13 9.8089385e-01], sum to 1.0000
[2019-04-10 12:49:30,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3374
[2019-04-10 12:49:30,373] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.8, 95.0, 1.0, 2.0, 0.2497413515537491, 1.0, 2.0, 0.2497413515537491, 1.0, 2.0, 0.428802415585162, 6.911199999999999, 6.9112, 170.5573041426782, 1095578.911971674, 1095578.911971674, 289884.7441159282], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 126000.0000, 
sim time next is 126600.0000, 
raw observation next is [22.8, 95.16666666666667, 1.0, 2.0, 0.3121632947432413, 1.0, 2.0, 0.3121632947432413, 1.0, 2.0, 0.534478441108517, 6.9112, 6.9112, 170.5573041426782, 1363949.198844394, 1363949.198844394, 313829.8148473499], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9516666666666667, 1.0, 1.0, 0.17128107800390516, 1.0, 1.0, 0.17128107800390516, 1.0, 1.0, 0.4322907818396549, 0.0, 0.0, 0.8375144448122397, 0.3788747774567761, 0.3788747774567761, 0.4684027087273879], 
reward next is 0.5316, 
noisyNet noise sample is [array([0.39945284], dtype=float32), -0.45333895]. 
=============================================
[2019-04-10 12:49:34,356] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.6541742e-32 3.9059459e-36 1.3318351e-27 2.6313754e-22], sum to 1.0000
[2019-04-10 12:49:34,356] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4677
[2019-04-10 12:49:34,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.4, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5518931352991465, 6.911199999999999, 6.9112, 168.912956510431, 485130.2792367901, 485130.2792367908, 158960.6512692172], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 172800.0000, 
sim time next is 173400.0000, 
raw observation next is [20.36666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5499310796440474, 6.9112, 6.9112, 168.912956510431, 483559.4951141838, 483559.4951141838, 158683.7569106496], 
processed observation next is [0.0, 0.0, 0.1642969984202214, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4511354629805456, 0.0, 0.0, 0.8294399451523027, 0.13432208197616216, 0.13432208197616216, 0.23684142822485013], 
reward next is 0.7632, 
noisyNet noise sample is [array([-0.45233846], dtype=float32), 1.0503877]. 
=============================================
[2019-04-10 12:49:43,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 7.9562642e-37 0.0000000e+00 1.5597502e-32 4.7267454e-32], sum to 1.0000
[2019-04-10 12:49:43,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1713
[2019-04-10 12:49:43,614] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5195163530727073, 6.9112, 6.9112, 168.912956510431, 458364.2539521999, 458364.2539521999, 154547.689182507], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5197626676427656, 6.9112, 6.9112, 168.912956510431, 458581.6140000548, 458581.6140000548, 154579.7086593983], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.414344716637519, 0.0, 0.0, 0.8294399451523027, 0.12738378166668188, 0.12738378166668188, 0.23071598307372881], 
reward next is 0.7693, 
noisyNet noise sample is [array([0.26803473], dtype=float32), 1.7552503]. 
=============================================
[2019-04-10 12:49:43,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.57964 ]
 [72.71223 ]
 [72.817665]
 [72.90334 ]
 [72.99046 ]], R is [[72.50121307]
 [72.54553223]
 [72.5894165 ]
 [72.63278961]
 [72.67557526]].
[2019-04-10 12:49:45,545] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3782268e-36], sum to 1.0000
[2019-04-10 12:49:45,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3516
[2019-04-10 12:49:45,590] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.1, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4946508383038775, 6.9112, 6.9112, 168.912956510431, 438197.227455016, 438197.227455016, 151325.6306696258], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 280800.0000, 
sim time next is 281400.0000, 
raw observation next is [20.25, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4979225551252273, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 151742.0601607622], 
processed observation next is [0.0, 0.2608695652173913, 0.1587677725118484, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3877104330795455, 0.0, 0.0, 0.8294399451523027, 0.12245079231082459, 0.12245079231082459, 0.22648068680710776], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.17326957], dtype=float32), -0.74361026]. 
=============================================
[2019-04-10 12:49:57,164] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.2998133e-31 1.3187617e-34 6.2998162e-27 6.1108538e-22], sum to 1.0000
[2019-04-10 12:49:57,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3870
[2019-04-10 12:49:57,177] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.85, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4590225643547137, 6.9112, 6.9112, 168.912956510431, 410050.3107986607, 410050.3107986607, 146925.0217591359], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 426600.0000, 
sim time next is 427200.0000, 
raw observation next is [19.8, 85.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4570278152561281, 6.9112, 6.9112, 168.912956510431, 408293.7364843456, 408293.7364843456, 146698.408922395], 
processed observation next is [1.0, 0.9565217391304348, 0.13744075829383895, 0.8533333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.33783879909283915, 0.0, 0.0, 0.8294399451523027, 0.1134149268012071, 0.1134149268012071, 0.21895284913790297], 
reward next is 0.7810, 
noisyNet noise sample is [array([-0.4294461], dtype=float32), -0.69625026]. 
=============================================
[2019-04-10 12:50:03,089] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0501283e-05 8.9786313e-20 5.0079446e-23 5.9421990e-14 9.9994946e-01], sum to 1.0000
[2019-04-10 12:50:03,094] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-10 12:50:03,097] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.16666666666666, 69.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 419098.7068111025, 419098.7068111018, 215472.0378534894], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 585600.0000, 
sim time next is 586200.0000, 
raw observation next is [22.03333333333333, 69.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 416127.9363305318, 416127.9363305318, 214914.4107615433], 
processed observation next is [1.0, 0.782608695652174, 0.2432859399684044, 0.695, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.11559109342514771, 0.11559109342514771, 0.3207677772560348], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5206399], dtype=float32), -0.8260975]. 
=============================================
[2019-04-10 12:50:08,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.8928991e-33 0.0000000e+00 4.6389507e-31 3.7596602e-24], sum to 1.0000
[2019-04-10 12:50:08,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4999
[2019-04-10 12:50:08,847] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.6, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3983844539923, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 140280.5688195635], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [17.6, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3970219876606945, 6.9112, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073982, 140145.6533955668], 
processed observation next is [1.0, 0.17391304347826086, 0.0331753554502371, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2646609605618226, 0.0, 0.0, 0.8294399451523027, 0.09950699402983283, 0.09950699402983283, 0.20917261700830866], 
reward next is 0.7908, 
noisyNet noise sample is [array([-0.54049575], dtype=float32), -1.6062716]. 
=============================================
[2019-04-10 12:50:19,938] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-10 12:50:19,940] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:50:19,940] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:50:19,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:50:19,941] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:50:19,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:50:19,942] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:50:19,941] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:50:19,945] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:50:19,945] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:50:19,944] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:50:19,961] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-04-10 12:50:19,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-04-10 12:50:19,999] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-04-10 12:50:20,015] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-04-10 12:50:20,016] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-04-10 12:50:42,044] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:50:42,045] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.16666666666667, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8302685100597448, 6.911200000000001, 6.9112, 168.912956510431, 691724.8263710734, 691724.8263710728, 208084.9722220474]
[2019-04-10 12:50:42,046] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:50:42,048] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.6923589e-37 0.0000000e+00 1.4718088e-33 6.1766816e-31], sampled 0.6871111769698932
[2019-04-10 12:50:46,793] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:50:46,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.22623566, 94.5039015, 1.0, 2.0, 0.6209359107220953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867730.4878601795, 867730.4878601789, 204710.6082099987]
[2019-04-10 12:50:46,796] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:50:46,797] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.99998450e-01 4.12058553e-22 5.40582774e-26 1.05477896e-16
 1.51819279e-06], sampled 0.7214518274373133
[2019-04-10 12:50:48,248] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:50:48,249] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.66666666666666, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7117951321115529, 6.9112, 6.9112, 168.912956510431, 608200.7687486969, 608200.7687486969, 184861.0006985416]
[2019-04-10 12:50:48,250] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:50:48,252] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.3551672e-37 0.0000000e+00 6.8713880e-34 2.8176828e-31], sampled 0.38416844188149457
[2019-04-10 12:50:51,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:50:51,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.80554950666667, 98.80392999666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5356428184451256, 6.911199999999999, 6.9112, 168.912956510431, 471995.3083588227, 471995.3083588234, 156700.8909947878]
[2019-04-10 12:50:51,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:50:51,937] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.00000000e+00 1.37371275e-36 0.00000000e+00 6.80380979e-33
 8.14394523e-30], sampled 0.9031739723860743
[2019-04-10 12:51:03,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:51:03,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.4, 81.0, 1.0, 2.0, 0.7330457175377347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1024474.552118535, 1024474.552118535, 228140.4901071558]
[2019-04-10 12:51:03,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:51:03,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9230075e-01 8.6628681e-22 8.8567232e-26 2.3539929e-15 7.6992456e-03], sampled 0.8664772701723527
[2019-04-10 12:51:03,253] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1024474.552118535 W.
[2019-04-10 12:51:09,737] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:51:09,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.61666666666667, 48.66666666666666, 1.0, 2.0, 1.000426227506163, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564383076, 1398400.638571234, 1398400.638571234, 299060.3863136169]
[2019-04-10 12:51:09,740] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:51:09,743] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.99973416e-01 1.63552079e-25 1.41904935e-30 1.35400223e-18
 2.65282342e-05], sampled 0.2042863293439312
[2019-04-10 12:51:09,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1398400.638571234 W.
[2019-04-10 12:51:14,296] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:51:14,297] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.9, 62.0, 1.0, 2.0, 0.5620137472765362, 1.0, 2.0, 0.5620137472765362, 1.0, 1.0, 0.9760320182329998, 6.9112, 6.9112, 169.0403247858759, 2357793.275433435, 2357793.275433435, 460377.1411772172]
[2019-04-10 12:51:14,298] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:51:14,300] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1958041e-02 1.3964591e-24 3.6851804e-30 8.2711298e-17 9.5804197e-01], sampled 0.8785958942423837
[2019-04-10 12:51:26,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.09994116], dtype=float32), 0.053642828]
[2019-04-10 12:51:26,228] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.93561443666667, 85.02936230666667, 1.0, 2.0, 0.6904775411909634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964955.9909039498, 964955.9909039498, 218801.7540230252]
[2019-04-10 12:51:26,231] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:51:26,234] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.5711953e-28 9.4675426e-34 2.3315628e-22 4.4907685e-11], sampled 0.36221240097907026
[2019-04-10 12:51:26,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 964955.9909039498 W.
[2019-04-10 12:51:41,704] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7411.1685 3113231474.5702 1851.0000
[2019-04-10 12:51:42,536] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7983.3903 2998750869.7041 1365.0000
[2019-04-10 12:51:42,561] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7086.8567 3193889016.1694 2285.0000
[2019-04-10 12:51:42,587] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7355.2441 3329977546.0936 1909.0000
[2019-04-10 12:51:42,703] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8134.5967 2947507045.1683 1164.0000
[2019-04-10 12:51:43,717] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 450000, evaluation results [450000.0, 7355.244069050347, 3329977546.093621, 1909.0, 7411.1684617044275, 3113231474.5701766, 1851.0, 8134.5967026720655, 2947507045.1683292, 1164.0, 7086.856744684188, 3193889016.169418, 2285.0, 7983.3903408106125, 2998750869.70411, 1365.0]
[2019-04-10 12:51:46,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.5744849e-26 3.1669563e-30 1.0375574e-22 6.8881159e-16], sum to 1.0000
[2019-04-10 12:51:46,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0471
[2019-04-10 12:51:46,350] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.03333333333333, 97.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6533095838336579, 6.911199999999999, 6.9112, 168.912956510431, 563523.7318817053, 563523.731881706, 174641.1256372174], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1033800.0000, 
sim time next is 1034400.0000, 
raw observation next is [22.06666666666667, 97.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6543514988466458, 6.911200000000001, 6.9112, 168.912956510431, 564311.300097655, 564311.3000976543, 174815.6791246153], 
processed observation next is [1.0, 1.0, 0.2448657187993683, 0.9766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5784774376178606, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15675313891601528, 0.15675313891601508, 0.26091892406659], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.38213003], dtype=float32), 0.49840504]. 
=============================================
[2019-04-10 12:51:48,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0477488e-06 1.6082958e-18 7.4916270e-21 1.0996942e-12 9.9999893e-01], sum to 1.0000
[2019-04-10 12:51:48,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6339
[2019-04-10 12:51:48,381] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.43333333333334, 72.66666666666666, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2405220320194925, 6.911199999999999, 6.9112, 170.5573041426782, 625724.7734712441, 625724.7734712447, 248132.6549456642], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1083000.0000, 
sim time next is 1083600.0000, 
raw observation next is [24.6, 72.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2677254668819533, 6.911200000000001, 6.9112, 170.5573041426782, 695962.4417770369, 695962.4417770363, 258301.8556326687], 
processed observation next is [1.0, 0.5652173913043478, 0.36492890995260674, 0.72, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.10698227668530892, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19332290049362136, 0.1933229004936212, 0.38552515766069956], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4522241], dtype=float32), -0.22251216]. 
=============================================
[2019-04-10 12:51:48,890] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4962227e-07 2.9157119e-22 3.9801299e-27 7.6296882e-15 9.9999964e-01], sum to 1.0000
[2019-04-10 12:51:48,898] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3421
[2019-04-10 12:51:48,903] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.76666666666667, 67.0, 1.0, 2.0, 0.2606251750552035, 1.0, 2.0, 0.2606251750552035, 1.0, 2.0, 0.4570953064872145, 6.9112, 6.9112, 170.5573041426782, 1180430.873148065, 1180430.873148065, 297969.0941328895], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1096800.0000, 
sim time next is 1097400.0000, 
raw observation next is [25.78333333333333, 67.0, 1.0, 2.0, 0.26275213879139, 1.0, 2.0, 0.26275213879139, 1.0, 2.0, 0.4606125214076955, 6.9112, 6.9112, 170.5573041426782, 1189224.717444076, 1189224.717444076, 298688.1917654376], 
processed observation next is [1.0, 0.6956521739130435, 0.4210110584518167, 0.67, 1.0, 1.0, 0.11174956480890358, 1.0, 1.0, 0.11174956480890358, 1.0, 1.0, 0.3422103919606042, 0.0, 0.0, 0.8375144448122397, 0.3303401992900211, 0.3303401992900211, 0.4458032712916979], 
reward next is 0.5542, 
noisyNet noise sample is [array([0.01896709], dtype=float32), -1.0391866]. 
=============================================
[2019-04-10 12:51:56,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.1003343e-26 1.4793772e-29 5.1874831e-21 8.6985723e-14], sum to 1.0000
[2019-04-10 12:51:56,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6640
[2019-04-10 12:51:56,518] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.73333333333333, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6325144337161253, 6.911199999999999, 6.9112, 168.912956510431, 548524.3494125624, 548524.3494125631, 171204.0894456537], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1201200.0000, 
sim time next is 1201800.0000, 
raw observation next is [23.61666666666667, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6319649442395094, 6.9112, 6.9112, 168.912956510431, 548170.3150592564, 548170.3150592564, 171114.0114868382], 
processed observation next is [1.0, 0.9130434782608695, 0.31832543443917877, 0.8133333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5511767612676944, 0.0, 0.0, 0.8294399451523027, 0.15226953196090456, 0.15226953196090456, 0.2553940469952809], 
reward next is 0.7446, 
noisyNet noise sample is [array([-1.9175462], dtype=float32), 0.21963356]. 
=============================================
[2019-04-10 12:52:01,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9024315e-05 5.6613922e-17 6.5078344e-22 1.4893958e-11 9.9997103e-01], sum to 1.0000
[2019-04-10 12:52:01,753] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5437
[2019-04-10 12:52:01,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.438860197222972, 1.0, 2.0, 0.438860197222972, 1.0, 2.0, 0.7449564242862698, 6.911200000000001, 6.9112, 170.5573041426782, 1840690.567280397, 1840690.567280397, 372722.2414998437], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [27.93333333333333, 75.33333333333334, 1.0, 2.0, 0.4620624323189023, 1.0, 2.0, 0.4620624323189023, 1.0, 2.0, 0.7843352127814865, 6.911199999999999, 6.9112, 170.5573041426782, 1938094.621666146, 1938094.621666146, 386864.0036691828], 
processed observation next is [1.0, 0.6956521739130435, 0.522906793048973, 0.7533333333333334, 1.0, 1.0, 0.3518824485769907, 1.0, 1.0, 0.3518824485769907, 1.0, 1.0, 0.736994161928642, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.538359617129485, 0.538359617129485, 0.5774089607002729], 
reward next is 0.4226, 
noisyNet noise sample is [array([-0.6131317], dtype=float32), 0.07212341]. 
=============================================
[2019-04-10 12:52:01,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7466077e-03 2.3925324e-17 6.2146750e-21 6.3787517e-12 9.9425334e-01], sum to 1.0000
[2019-04-10 12:52:01,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7568
[2019-04-10 12:52:01,882] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.86666666666667, 75.66666666666667, 1.0, 2.0, 0.4024973494309971, 1.0, 2.0, 0.4024973494309971, 1.0, 2.0, 0.6826530827754057, 6.9112, 6.9112, 170.5573041426782, 1688055.463324888, 1688055.463324888, 351933.5655661951], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1269600.0000, 
sim time next is 1270200.0000, 
raw observation next is [27.83333333333334, 75.83333333333333, 1.0, 2.0, 0.3866513572241822, 1.0, 2.0, 0.3866513572241822, 1.0, 2.0, 0.655265932005742, 6.9112, 6.9112, 170.5573041426782, 1621547.818285939, 1621547.818285939, 343405.0255730646], 
processed observation next is [1.0, 0.6956521739130435, 0.5181674565560824, 0.7583333333333333, 1.0, 1.0, 0.26102573159540027, 1.0, 1.0, 0.26102573159540027, 1.0, 1.0, 0.5795926000070024, 0.0, 0.0, 0.8375144448122397, 0.45042994952387194, 0.45042994952387194, 0.5125448142881561], 
reward next is 0.4875, 
noisyNet noise sample is [array([1.4666739], dtype=float32), -0.22331685]. 
=============================================
[2019-04-10 12:52:04,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.2906511e-21 1.3964184e-25 8.1500245e-18 3.5928167e-11], sum to 1.0000
[2019-04-10 12:52:04,341] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-10 12:52:04,344] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7727789012303343, 6.9112, 6.9112, 168.912956510431, 651966.8511016708, 651966.8511016708, 196408.0638710615], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1299000.0000, 
sim time next is 1299600.0000, 
raw observation next is [24.3, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.772804136172042, 6.911199999999999, 6.9112, 168.912956510431, 651988.9023952944, 651988.902395295, 196413.0935782188], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.722931873380539, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18110802844313734, 0.1811080284431375, 0.29315387101226686], 
reward next is 0.7068, 
noisyNet noise sample is [array([-0.86548066], dtype=float32), -0.87631965]. 
=============================================
[2019-04-10 12:52:06,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 5.461190e-29 3.934427e-32 6.467203e-25 4.304530e-21], sum to 1.0000
[2019-04-10 12:52:06,206] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6509
[2019-04-10 12:52:06,219] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.43333333333333, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8208317498965761, 6.9112, 6.9112, 168.912956510431, 693367.7802891523, 693367.7802891523, 206287.9214281889], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1309200.0000, 
sim time next is 1309800.0000, 
raw observation next is [24.46666666666667, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8139956723770522, 6.911199999999998, 6.9112, 168.912956510431, 687491.0278524919, 687491.0278524932, 204844.5241828187], 
processed observation next is [1.0, 0.13043478260869565, 0.3586097946287521, 0.9216666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7731654541183561, -1.7763568394002506e-16, 0.0, 0.8294399451523027, 0.19096972995902553, 0.1909697299590259, 0.3057380957952518], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.5821639], dtype=float32), 0.37877327]. 
=============================================
[2019-04-10 12:52:07,342] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.0525928e-31 1.8011785e-35 3.4980658e-27 3.4499794e-24], sum to 1.0000
[2019-04-10 12:52:07,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6211
[2019-04-10 12:52:07,349] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.13333333333333, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5824318185903626, 6.911200000000001, 6.9112, 168.912956510431, 509385.1160155722, 509385.1160155715, 163400.7815298756], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1369200.0000, 
sim time next is 1369800.0000, 
raw observation next is [21.1, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.583881861471859, 6.9112, 6.9112, 168.912956510431, 510714.784000715, 510714.784000715, 163613.0619115974], 
processed observation next is [1.0, 0.8695652173913043, 0.1990521327014219, 0.945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49253885545348663, 0.0, 0.0, 0.8294399451523027, 0.1418652177779764, 0.1418652177779764, 0.24419859986805584], 
reward next is 0.7558, 
noisyNet noise sample is [array([2.060986], dtype=float32), 0.39144632]. 
=============================================
[2019-04-10 12:52:08,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 6.0601459e-32 4.4373916e-37 1.8831593e-28 1.7464230e-26], sum to 1.0000
[2019-04-10 12:52:08,725] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4904
[2019-04-10 12:52:08,729] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.06666666666667, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5859452804119778, 6.9112, 6.9112, 168.912956510431, 512586.8352017128, 512586.8352017128, 163916.5992184649], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1370400.0000, 
sim time next is 1371000.0000, 
raw observation next is [21.03333333333333, 94.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.584795743707789, 6.9112, 6.9112, 168.912956510431, 511649.2636062391, 511649.2636062391, 163744.708841372], 
processed observation next is [1.0, 0.8695652173913043, 0.19589257503949445, 0.9483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49365334598510846, 0.0, 0.0, 0.8294399451523027, 0.14212479544617754, 0.14212479544617754, 0.24439508782294328], 
reward next is 0.7556, 
noisyNet noise sample is [array([-0.04701838], dtype=float32), -0.06728289]. 
=============================================
[2019-04-10 12:52:08,737] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.802177]
 [59.746746]
 [59.93301 ]
 [59.888813]
 [60.043438]], R is [[60.14395905]
 [60.29786682]
 [60.45069122]
 [60.60230255]
 [60.75232697]].
[2019-04-10 12:52:20,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.188459e-38 2.594657e-35], sum to 1.0000
[2019-04-10 12:52:20,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8586
[2019-04-10 12:52:20,774] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.53333333333333, 52.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6123333406499577, 6.911200000000001, 6.9112, 168.912956510431, 530933.4005359894, 530933.4005359889, 168011.5573273553], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1507800.0000, 
sim time next is 1508400.0000, 
raw observation next is [28.8, 51.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6063988144700648, 6.9112, 6.9112, 168.912956510431, 526195.087827559, 526195.087827559, 167087.8519621293], 
processed observation next is [0.0, 0.4782608695652174, 0.5639810426540285, 0.51, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5199985542317863, 0.0, 0.0, 0.8294399451523027, 0.14616530217432194, 0.14616530217432194, 0.24938485367481983], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.4128152], dtype=float32), 1.3840419]. 
=============================================
[2019-04-10 12:52:29,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.1015640e-34 0.0000000e+00 2.1280256e-32 1.2300238e-33], sum to 1.0000
[2019-04-10 12:52:29,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1882
[2019-04-10 12:52:29,857] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.86666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8082307299080013, 6.9112, 6.9112, 168.912956510431, 676195.2105974685, 676195.2105974685, 203508.9832805711], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1732800.0000, 
sim time next is 1733400.0000, 
raw observation next is [24.8, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8039456313694041, 6.9112, 6.9112, 168.912956510431, 673221.3053508436, 673221.3053508436, 202634.0316599727], 
processed observation next is [1.0, 0.043478260869565216, 0.3744075829383887, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7609093065480537, 0.0, 0.0, 0.8294399451523027, 0.1870059181530121, 0.1870059181530121, 0.30243885322383984], 
reward next is 0.6976, 
noisyNet noise sample is [array([-1.1429417], dtype=float32), -0.5591501]. 
=============================================
[2019-04-10 12:52:33,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7976338e-38 0.0000000e+00], sum to 1.0000
[2019-04-10 12:52:33,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5757
[2019-04-10 12:52:33,555] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.83333333333334, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6166551201955369, 6.9112, 6.9112, 168.912956510431, 535856.5860672527, 535856.5860672527, 168669.237394442], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1818600.0000, 
sim time next is 1819200.0000, 
raw observation next is [21.86666666666667, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6166208964354097, 6.9112, 6.9112, 168.912956510431, 535768.6817740325, 535768.6817740325, 168664.8023007285], 
processed observation next is [1.0, 0.043478260869565216, 0.23538704581358633, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5324645078480607, 0.0, 0.0, 0.8294399451523027, 0.14882463382612016, 0.14882463382612016, 0.2517385108966097], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.44957793], dtype=float32), -2.1846344]. 
=============================================
[2019-04-10 12:52:35,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.3396684e-24 1.4133512e-28 2.4159215e-21 2.9103184e-18], sum to 1.0000
[2019-04-10 12:52:35,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6279
[2019-04-10 12:52:35,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1540272.657290823 W.
[2019-04-10 12:52:35,555] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 85.83333333333334, 1.0, 2.0, 0.5509244709105343, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9233349451979693, 6.911199999999999, 6.9112, 168.912956510431, 1540272.657290823, 1540272.657290823, 330192.7219453446], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1858200.0000, 
sim time next is 1858800.0000, 
raw observation next is [26.1, 85.66666666666667, 1.0, 2.0, 0.4371647118089922, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7318936214812585, 6.9112, 6.9112, 168.912956510431, 1222040.470805055, 1222040.470805055, 271932.4972901945], 
processed observation next is [1.0, 0.5217391304347826, 0.4360189573459717, 0.8566666666666667, 1.0, 1.0, 0.321885194950593, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6730410018064127, 0.0, 0.0, 0.8294399451523027, 0.3394556863347375, 0.3394556863347375, 0.4058693989405888], 
reward next is 0.5941, 
noisyNet noise sample is [array([0.17962481], dtype=float32), 1.0189122]. 
=============================================
[2019-04-10 12:52:38,114] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-10 12:52:38,115] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:52:38,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:52:38,117] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:52:38,118] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:52:38,119] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:52:38,121] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:52:38,121] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:52:38,121] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:52:38,123] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:52:38,124] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:52:38,134] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-04-10 12:52:38,148] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-04-10 12:52:38,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-04-10 12:52:38,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-04-10 12:52:38,203] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-04-10 12:52:51,961] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:52:51,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.31666666666667, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4246097200093344, 6.9112, 6.9112, 168.912956510431, 381925.5692079452, 381925.5692079452, 142997.9336969895]
[2019-04-10 12:52:51,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:52:51,967] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4389587393527489
[2019-04-10 12:52:57,371] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:52:57,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8304610687082302, 6.911199999999999, 6.9112, 168.912956510431, 701216.7262283522, 701216.7262283528, 208335.6171844241]
[2019-04-10 12:52:57,373] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:52:57,377] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8032121915956977
[2019-04-10 12:53:01,711] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:01,713] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.94803293666667, 93.48650423166667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9358744570572938, 6.911200000000001, 6.9112, 168.9129564921892, 767366.5804283485, 767366.5804283479, 231724.2017286533]
[2019-04-10 12:53:01,713] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:53:01,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.8166255e-38 0.0000000e+00 1.4209632e-35 5.2030233e-35], sampled 0.5004111543504216
[2019-04-10 12:53:06,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:06,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7667673135457259, 6.911200000000001, 6.9112, 168.912956510431, 646834.9573899846, 646834.957389984, 195216.7984239709]
[2019-04-10 12:53:06,924] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:06,926] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08354007666914476
[2019-04-10 12:53:12,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:12,128] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7688349146555111, 6.9112, 6.9112, 168.912956510431, 648162.2974202308, 648162.2974202308, 195618.0595199043]
[2019-04-10 12:53:12,129] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:12,131] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5309793430046809
[2019-04-10 12:53:36,341] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:36,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.46666666666666, 52.5, 1.0, 2.0, 0.8994939942097862, 1.0, 1.0, 0.8994939942097862, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2515877.934324751, 2515877.934324751, 471208.4536475837]
[2019-04-10 12:53:36,343] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:36,345] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 3.4859783e-30 2.0750902e-35 6.3682923e-27 2.5394740e-23], sampled 0.5405950286062462
[2019-04-10 12:53:36,347] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2515877.934324751 W.
[2019-04-10 12:53:41,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:41,419] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.66666666666667, 91.33333333333334, 1.0, 2.0, 0.709057940377014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 990934.5800332058, 990934.5800332064, 222812.6658313916]
[2019-04-10 12:53:41,420] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:41,423] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.2974205e-32 1.3283230e-36 5.7699706e-29 6.2358815e-27], sampled 0.9985391601610948
[2019-04-10 12:53:41,425] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 990934.5800332058 W.
[2019-04-10 12:53:48,451] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:48,451] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.76077622333333, 55.65614342333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6472144647419273, 6.9112, 6.9112, 168.912956510431, 561577.480877024, 561577.480877024, 173595.3497853519]
[2019-04-10 12:53:48,452] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:53:48,454] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.19642455590433605
[2019-04-10 12:53:52,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:52,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.41242916, 94.43598888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.121085495022472, 6.9112, 168.9117516844696, 977757.9487505292, 828859.0169574167, 254814.2420759963]
[2019-04-10 12:53:52,157] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:53:52,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.45374668889209724
[2019-04-10 12:53:52,159] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 977757.9487505292 W.
[2019-04-10 12:53:52,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.09463646], dtype=float32), 0.05057807]
[2019-04-10 12:53:52,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.35, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6288377758385743, 6.9112, 6.9112, 168.912956510431, 545368.6149059576, 545368.6149059576, 170613.6155157124]
[2019-04-10 12:53:52,961] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:53:52,964] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5160966166901013
[2019-04-10 12:53:59,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8870 2989446328.4271 1566.0000
[2019-04-10 12:53:59,915] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.0711 2937954485.8673 1382.0000
[2019-04-10 12:53:59,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8919 3185219386.8708 2464.0000
[2019-04-10 12:53:59,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4826 3319658082.8562 2143.0000
[2019-04-10 12:54:00,052] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.9107 3105802677.4938 2010.0000
[2019-04-10 12:54:01,068] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 475000, evaluation results [475000.0, 7287.48261098085, 3319658082.856155, 2143.0, 7346.910748810438, 3105802677.493814, 2010.0, 8060.071082549855, 2937954485.8672786, 1382.0, 7029.891904325195, 3185219386.870774, 2464.0, 7923.886968186372, 2989446328.4270563, 1566.0]
[2019-04-10 12:54:01,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:54:01,813] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-10 12:54:01,818] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.15, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7844525923130297, 6.911200000000001, 6.9112, 168.912956510431, 665063.6750821503, 665063.6750821496, 198793.8127924638], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1927800.0000, 
sim time next is 1928400.0000, 
raw observation next is [25.26666666666667, 84.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8028582112894114, 6.911200000000001, 6.9112, 168.912956510431, 680478.1975876169, 680478.1975876162, 202556.4724396286], 
processed observation next is [1.0, 0.30434782608695654, 0.3965244865718801, 0.8433333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7595831844992822, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1890217215521158, 0.1890217215521156, 0.3023230931934755], 
reward next is 0.6977, 
noisyNet noise sample is [array([-0.64330065], dtype=float32), -1.0700788]. 
=============================================
[2019-04-10 12:54:06,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 12:54:06,591] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-10 12:54:06,595] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.05, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8251340090331213, 6.9112, 6.9112, 168.912956510431, 688195.661996717, 688195.661996717, 207010.6675600641], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [26.0, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8219565503731913, 6.911199999999999, 6.9112, 168.912956510431, 685820.6490539908, 685820.6490539914, 206343.89389378], 
processed observation next is [0.0, 0.782608695652174, 0.4312796208530806, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7828738419185258, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19050573584833078, 0.19050573584833094, 0.3079759610354926], 
reward next is 0.6920, 
noisyNet noise sample is [array([-0.14030507], dtype=float32), 0.89514524]. 
=============================================
[2019-04-10 12:54:12,594] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.0475386e-24 1.4811630e-27 8.1762588e-22 2.7506961e-19], sum to 1.0000
[2019-04-10 12:54:12,602] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4251
[2019-04-10 12:54:12,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 898999.142080164 W.
[2019-04-10 12:54:12,613] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.93333333333333, 95.33333333333334, 1.0, 2.0, 0.3216522421507868, 1.0, 2.0, 0.3216522421507868, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 898999.142080164, 898999.142080164, 254186.5099167694], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2172000.0000, 
sim time next is 2172600.0000, 
raw observation next is [24.9, 95.5, 1.0, 2.0, 0.3126696412380687, 1.0, 2.0, 0.3126696412380687, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 873883.0707118298, 873883.0707118298, 252360.8293652848], 
processed observation next is [1.0, 0.13043478260869565, 0.3791469194312796, 0.955, 1.0, 1.0, 0.17189113402176948, 1.0, 1.0, 0.17189113402176948, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.24274529741995274, 0.24274529741995274, 0.37665795427654447], 
reward next is 0.6233, 
noisyNet noise sample is [array([0.32371414], dtype=float32), -0.62017274]. 
=============================================
[2019-04-10 12:54:20,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.2859593e-38 0.0000000e+00 2.2295250e-37 1.7100470e-32], sum to 1.0000
[2019-04-10 12:54:20,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6212
[2019-04-10 12:54:20,513] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.7, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9582002557537869, 6.9112, 6.9112, 168.912956510431, 771005.4875571156, 771005.4875571156, 236443.3106659405], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2311200.0000, 
sim time next is 2311800.0000, 
raw observation next is [31.56666666666667, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9759984343184669, 6.9112, 6.9112, 168.912956510431, 785331.8938997262, 785331.8938997262, 240868.1598965976], 
processed observation next is [1.0, 0.782608695652174, 0.6951026856240128, 0.6866666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9707297979493499, 0.0, 0.0, 0.8294399451523027, 0.2181477483054795, 0.2181477483054795, 0.3595047162635785], 
reward next is 0.6405, 
noisyNet noise sample is [array([-0.41972142], dtype=float32), 0.5798726]. 
=============================================
[2019-04-10 12:54:23,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.2650705e-19 6.1210457e-23 3.2879151e-16 5.0431436e-13], sum to 1.0000
[2019-04-10 12:54:23,602] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3743
[2019-04-10 12:54:23,620] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 994088.2771141735 W.
[2019-04-10 12:54:23,641] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.16666666666667, 77.66666666666667, 1.0, 2.0, 0.2371055679950369, 1.0, 2.0, 0.2371055679950369, 1.0, 2.0, 0.4036933339525228, 6.911199999999999, 6.9112, 170.5573041426782, 994088.2771141735, 994088.2771141742, 280490.0904044441], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2355600.0000, 
sim time next is 2356200.0000, 
raw observation next is [28.3, 77.0, 1.0, 2.0, 0.3584434036948381, 1.0, 2.0, 0.3584434036948381, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1001876.314685142, 1001876.314685142, 262162.9869852557], 
processed observation next is [1.0, 0.2608695652173913, 0.5402843601895735, 0.77, 1.0, 1.0, 0.2270402454154676, 1.0, 1.0, 0.2270402454154676, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.27829897630142836, 0.27829897630142836, 0.391288040276501], 
reward next is 0.6087, 
noisyNet noise sample is [array([-0.9207979], dtype=float32), -1.3126973]. 
=============================================
[2019-04-10 12:54:28,081] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9999988e-01 1.0297891e-15 1.2631234e-18 2.2602371e-11 1.2722217e-07], sum to 1.0000
[2019-04-10 12:54:28,096] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0621
[2019-04-10 12:54:28,096] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1190010.285219657 W.
[2019-04-10 12:54:28,100] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.2838101069186851, 1.0, 1.0, 0.2838101069186851, 1.0, 1.0, 0.4865282838567159, 6.911199999999999, 6.9112, 170.5573041426782, 1190010.285219657, 1190010.285219658, 297322.9661364643], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2440200.0000, 
sim time next is 2440800.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.412339201357021, 1.0, 2.0, 0.412339201357021, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1152600.095722371, 1152600.095722371, 275279.1466962722], 
processed observation next is [1.0, 0.2608695652173913, 0.5071090047393366, 0.84, 1.0, 1.0, 0.2919749413940012, 1.0, 1.0, 0.2919749413940012, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.32016669325621416, 0.32016669325621416, 0.4108643980541376], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8839204], dtype=float32), 1.2185433]. 
=============================================
[2019-04-10 12:54:28,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.1255097e-32 4.4324665e-38 1.1295664e-30 1.7677750e-29], sum to 1.0000
[2019-04-10 12:54:28,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1996
[2019-04-10 12:54:28,898] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.9, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9538424388965504, 6.911199999999999, 6.9112, 168.912956510431, 775149.2410533043, 775149.2410533049, 235781.3860497617], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2421600.0000, 
sim time next is 2422200.0000, 
raw observation next is [28.85, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9503345718521339, 6.9112, 6.9112, 168.912956510431, 772890.7641782783, 772890.7641782783, 234950.276568302], 
processed observation next is [1.0, 0.0, 0.5663507109004741, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9394324046977243, 0.0, 0.0, 0.8294399451523027, 0.21469187893841063, 0.21469187893841063, 0.35067205457955525], 
reward next is 0.6493, 
noisyNet noise sample is [array([-0.4629721], dtype=float32), 1.4691845]. 
=============================================
[2019-04-10 12:54:32,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3438185e-04 5.8916323e-17 7.4315050e-20 2.6753502e-12 9.9966562e-01], sum to 1.0000
[2019-04-10 12:54:32,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9537
[2019-04-10 12:54:32,792] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.86666666666667, 87.0, 1.0, 2.0, 0.4655307497570049, 1.0, 2.0, 0.4655307497570049, 1.0, 2.0, 0.7978873376657355, 6.911199999999999, 6.9112, 170.5573041426782, 1952655.539249133, 1952655.539249133, 390353.6707378598], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2472000.0000, 
sim time next is 2472600.0000, 
raw observation next is [26.98333333333333, 86.5, 1.0, 2.0, 0.4776911978809369, 1.0, 2.0, 0.4776911978809369, 1.0, 2.0, 0.8195800976265268, 6.911200000000001, 6.9112, 170.5573041426782, 2003709.892955753, 2003709.892955753, 398335.0579010755], 
processed observation next is [1.0, 0.6086956521739131, 0.4778830963665086, 0.865, 1.0, 1.0, 0.37071228660353844, 1.0, 1.0, 0.37071228660353844, 1.0, 1.0, 0.7799757288128376, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.556586081376598, 0.556586081376598, 0.5945299371657844], 
reward next is 0.4055, 
noisyNet noise sample is [array([-1.2828912], dtype=float32), -0.97306985]. 
=============================================
[2019-04-10 12:54:38,621] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3250974e-13 3.4288249e-19 2.6748609e-24 7.4099194e-14 1.0000000e+00], sum to 1.0000
[2019-04-10 12:54:38,624] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-10 12:54:38,626] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.21666666666667, 75.16666666666667, 1.0, 2.0, 0.5216520735283988, 1.0, 2.0, 0.5216520735283988, 1.0, 2.0, 0.9017620374454131, 6.9112, 6.9112, 170.5573041426782, 2188295.273796828, 2188295.273796828, 429651.6753243255], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2549400.0000, 
sim time next is 2550000.0000, 
raw observation next is [29.33333333333334, 74.33333333333334, 1.0, 2.0, 0.437897128332088, 1.0, 2.0, 0.437897128332088, 1.0, 2.0, 0.7560908172762636, 6.9112, 6.9112, 170.5573041426782, 1836647.75099809, 1836647.75099809, 374176.7232957653], 
processed observation next is [1.0, 0.5217391304347826, 0.5892575039494474, 0.7433333333333334, 1.0, 1.0, 0.3227676244964916, 1.0, 1.0, 0.3227676244964916, 1.0, 1.0, 0.702549777166175, 0.0, 0.0, 0.8375144448122397, 0.5101799308328028, 0.5101799308328028, 0.5584727213369631], 
reward next is 0.4415, 
noisyNet noise sample is [array([-0.3908261], dtype=float32), 0.59806526]. 
=============================================
[2019-04-10 12:54:38,729] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[51.144917]
 [51.386234]
 [51.524246]
 [51.56056 ]
 [51.52207 ]], R is [[51.32677841]
 [51.17224121]
 [51.06266403]
 [50.97042084]
 [50.89043045]].
[2019-04-10 12:54:49,194] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6418653e-37 1.9429801e-37], sum to 1.0000
[2019-04-10 12:54:49,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4850
[2019-04-10 12:54:49,209] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.66666666666666, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6814317492730755, 6.911200000000001, 6.9112, 168.912956510431, 585125.0563520129, 585125.0563520123, 179446.8119918662], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2731200.0000, 
sim time next is 2731800.0000, 
raw observation next is [22.83333333333334, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6813707006108729, 6.911200000000001, 6.9112, 168.912956510431, 584766.6491672527, 584766.649167252, 179436.6278951587], 
processed observation next is [0.0, 0.6086956521739131, 0.2812006319115327, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6114276836717962, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16243518032423687, 0.16243518032423668, 0.2678158625300876], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.19537574], dtype=float32), -3.4383254]. 
=============================================
[2019-04-10 12:54:51,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.4537616e-35 0.0000000e+00 1.3697118e-33 3.2145576e-31], sum to 1.0000
[2019-04-10 12:54:51,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8393
[2019-04-10 12:54:51,779] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6018167309485615, 6.911199999999999, 6.9112, 168.912956510431, 521876.3759363483, 521876.375936349, 166390.1627408241], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2785800.0000, 
sim time next is 2786400.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6032439062004334, 6.9112, 6.9112, 168.912956510431, 523113.9895834465, 523113.9895834465, 166608.3389486985], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5161511051224797, 0.0, 0.0, 0.8294399451523027, 0.14530944155095737, 0.14530944155095737, 0.24866916260999777], 
reward next is 0.7513, 
noisyNet noise sample is [array([-1.4454389], dtype=float32), -1.382112]. 
=============================================
[2019-04-10 12:54:53,248] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7811965e-07 9.0421383e-23 5.0444056e-26 6.9903631e-17 9.9999928e-01], sum to 1.0000
[2019-04-10 12:54:53,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1947
[2019-04-10 12:54:53,261] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.33333333333333, 87.0, 1.0, 2.0, 0.2220758279099373, 1.0, 2.0, 0.2220758279099373, 1.0, 2.0, 0.3774379347382431, 6.9112, 6.9112, 170.5573041426782, 959756.3820311913, 959756.3820311913, 278926.8987005146], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [24.5, 86.0, 1.0, 2.0, 0.2379201414743593, 1.0, 2.0, 0.2379201414743593, 1.0, 2.0, 0.4035092164077868, 6.911199999999999, 6.9112, 170.5573041426782, 1025103.821606108, 1025103.821606108, 283556.5596266095], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.86, 1.0, 1.0, 0.08183149575224011, 1.0, 1.0, 0.08183149575224011, 1.0, 1.0, 0.2725722151314473, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2847510615572522, 0.2847510615572522, 0.4232187457113575], 
reward next is 0.5768, 
noisyNet noise sample is [array([0.51928604], dtype=float32), -1.3361202]. 
=============================================
[2019-04-10 12:54:54,782] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-10 12:54:54,784] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:54:54,784] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:54:54,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:54,785] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:54,786] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:54:54,785] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:54:54,787] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:54,788] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:54,789] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:54:54,789] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:54:54,795] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-04-10 12:54:54,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-04-10 12:54:54,810] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-04-10 12:54:54,843] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-04-10 12:54:54,863] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-04-10 12:54:57,325] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1118761], dtype=float32), 0.055649646]
[2019-04-10 12:54:57,325] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6514439601385617, 6.9112, 6.9112, 168.912956510431, 565428.0070533092, 565428.0070533092, 174292.8479972801]
[2019-04-10 12:54:57,327] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:54:57,330] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.064585e-38], sampled 0.5880236696194682
[2019-04-10 12:55:16,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1118761], dtype=float32), 0.055649646]
[2019-04-10 12:55:16,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.88983038333333, 98.32009657166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7500365298745618, 6.9112, 6.9112, 168.912956510431, 638717.6083788516, 638717.6083788516, 192026.1089977987]
[2019-04-10 12:55:16,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:55:16,761] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.026773e-37], sampled 0.1599900831337292
[2019-04-10 12:55:40,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1118761], dtype=float32), 0.055649646]
[2019-04-10 12:55:40,829] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 81.5, 1.0, 2.0, 0.2073648551398417, 1.0, 1.0, 0.2073648551398417, 1.0, 1.0, 0.3601241767723984, 6.9112, 6.9112, 170.5573041426782, 869346.8890800207, 869346.8890800207, 271919.8106865023]
[2019-04-10 12:55:40,830] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 12:55:40,833] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.8322010e-03 1.3680411e-21 2.2042736e-25 1.1404532e-15 9.9616772e-01], sampled 0.5713103718975544
[2019-04-10 12:55:48,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.1118761], dtype=float32), 0.055649646]
[2019-04-10 12:55:48,521] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.54642138, 82.58748430666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.932096662702479, 6.9112, 168.9126551460049, 843631.4861244215, 828806.7008452032, 254812.2958326443]
[2019-04-10 12:55:48,522] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:55:48,524] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2004101e-37 4.5210511e-35], sampled 0.7110547096008537
[2019-04-10 12:55:57,670] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1118761], dtype=float32), 0.055649646]
[2019-04-10 12:55:57,671] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 85.0, 1.0, 2.0, 0.6601396598041455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 922539.788239753, 922539.7882397537, 212479.3137929204]
[2019-04-10 12:55:57,671] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:55:57,673] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.7895426e-01 1.7383390e-21 1.5791659e-25 4.9696794e-16 2.1045806e-02], sampled 0.7610641412066174
[2019-04-10 12:55:57,674] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 922539.788239753 W.
[2019-04-10 12:56:16,918] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8151.4953 2946726034.5073 1118.0000
[2019-04-10 12:56:16,930] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7178.1068 3190933789.6191 2060.0000
[2019-04-10 12:56:17,122] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8049.8758 2994890202.2763 1224.0000
[2019-04-10 12:56:17,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7442.7293 3118290602.3822 1781.0000
[2019-04-10 12:56:17,218] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7399.8272 3324572058.0354 1830.0000
[2019-04-10 12:56:18,233] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 500000, evaluation results [500000.0, 7399.827208607327, 3324572058.0353513, 1830.0, 7442.729279192662, 3118290602.382175, 1781.0, 8151.495263519451, 2946726034.5073037, 1118.0, 7178.106762089127, 3190933789.6190915, 2060.0, 8049.875789019542, 2994890202.2763047, 1224.0]
[2019-04-10 12:56:26,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7810044e-37 7.6353854e-33], sum to 1.0000
[2019-04-10 12:56:26,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1457
[2019-04-10 12:56:26,657] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5452446368230801, 6.9112, 6.9112, 168.912956510431, 478834.0511005365, 478834.0511005365, 158057.7576267705], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [20.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5504754434133705, 6.911200000000001, 6.9112, 168.912956510431, 483192.4815268922, 483192.4815268916, 158785.6279242479], 
processed observation next is [1.0, 0.13043478260869565, 0.1706161137440759, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4517993212358177, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13422013375747005, 0.13422013375746988, 0.23699347451380284], 
reward next is 0.7630, 
noisyNet noise sample is [array([-1.2927374], dtype=float32), 1.270165]. 
=============================================
[2019-04-10 12:56:30,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.2582040e-33 1.0598109e-37 2.9601598e-30 3.4784707e-24], sum to 1.0000
[2019-04-10 12:56:30,251] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5394
[2019-04-10 12:56:30,254] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6758338493941138, 6.9112, 6.9112, 168.912956510431, 580626.1696701146, 580626.1696701146, 178474.7021710013], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3108600.0000, 
sim time next is 3109200.0000, 
raw observation next is [22.66666666666666, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.678451158571361, 6.9112, 6.9112, 168.912956510431, 582565.0106566799, 582565.0106566799, 178928.5000519396], 
processed observation next is [1.0, 1.0, 0.27330173775671385, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6078672665504401, 0.0, 0.0, 0.8294399451523027, 0.16182361407129997, 0.16182361407129997, 0.2670574627640889], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.23192148], dtype=float32), 0.6812261]. 
=============================================
[2019-04-10 12:56:33,465] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.5210845e-32 5.7505472e-36 4.9205416e-29 6.0062045e-23], sum to 1.0000
[2019-04-10 12:56:33,471] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4215
[2019-04-10 12:56:33,475] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8749645846836195, 6.9112, 6.9112, 168.912956510431, 722148.2976788848, 722148.2976788848, 217692.8596885588], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3189600.0000, 
sim time next is 3190200.0000, 
raw observation next is [25.83333333333334, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.873213949566429, 6.9112, 6.9112, 168.912956510431, 720445.442047145, 720445.442047145, 217288.7284988279], 
processed observation next is [1.0, 0.9565217391304348, 0.42338072669826254, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8453828653249132, 0.0, 0.0, 0.8294399451523027, 0.2001237339019847, 0.2001237339019847, 0.32431153507287747], 
reward next is 0.6757, 
noisyNet noise sample is [array([-0.6926522], dtype=float32), 1.0266744]. 
=============================================
[2019-04-10 12:56:34,350] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.6543809e-35 0.0000000e+00 7.3492922e-34 4.0209744e-32], sum to 1.0000
[2019-04-10 12:56:34,360] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5635
[2019-04-10 12:56:34,363] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7693347459848642, 6.9112, 6.9112, 168.912956510431, 648725.3603336428, 648725.3603336428, 195719.3407142202], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3212400.0000, 
sim time next is 3213000.0000, 
raw observation next is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7681722897560229, 6.911199999999999, 6.9112, 168.912956510431, 647753.9435523852, 647753.9435523859, 195489.6534212456], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7172832801902718, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17993165098677366, 0.17993165098677386, 0.2917756021212621], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.5462346], dtype=float32), -0.7150446]. 
=============================================
[2019-04-10 12:56:34,373] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.61643 ]
 [67.697205]
 [67.74446 ]
 [67.75252 ]
 [67.68381 ]], R is [[67.65281677]
 [67.68416595]
 [67.71542358]
 [67.74629974]
 [67.77622986]].
[2019-04-10 12:56:34,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.4695506e-37 0.0000000e+00 1.4444967e-35 2.3099828e-34], sum to 1.0000
[2019-04-10 12:56:34,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-10 12:56:34,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8557277586982885, 6.911199999999999, 6.9112, 168.912956510431, 708150.4100429899, 708150.4100429906, 213469.3646941433], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [28.66666666666666, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8744324096814978, 6.9112, 6.9112, 168.912956510431, 720589.8685582536, 720589.8685582536, 217531.4697955292], 
processed observation next is [0.0, 0.391304347826087, 0.5576619273301735, 0.7733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8468687922945094, 0.0, 0.0, 0.8294399451523027, 0.20016385237729267, 0.20016385237729267, 0.3246738355157152], 
reward next is 0.6753, 
noisyNet noise sample is [array([0.16889556], dtype=float32), 0.6129237]. 
=============================================
[2019-04-10 12:56:36,900] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 8.4786160e-36 0.0000000e+00 3.4323340e-33 1.3445535e-31], sum to 1.0000
[2019-04-10 12:56:36,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5073
[2019-04-10 12:56:36,915] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9027043003820545, 6.911200000000001, 6.9112, 168.912956510431, 741632.6264427049, 741632.6264427042, 223920.0202041417], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [28.0, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.894144808583442, 6.9112, 6.9112, 168.912956510431, 735988.7379402298, 735988.7379402298, 221992.653872874], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.870908303150539, 0.0, 0.0, 0.8294399451523027, 0.20444131609450827, 0.20444131609450827, 0.3313323192132448], 
reward next is 0.6687, 
noisyNet noise sample is [array([1.1170074], dtype=float32), 0.08968557]. 
=============================================
[2019-04-10 12:56:38,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 7.8215417e-36 0.0000000e+00 3.4701828e-34 1.8555244e-31], sum to 1.0000
[2019-04-10 12:56:38,617] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1353
[2019-04-10 12:56:38,621] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7234746534004523, 6.9112, 6.9112, 168.912956510431, 616474.9472071078, 616474.9472071078, 187002.4861310799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3303600.0000, 
sim time next is 3304200.0000, 
raw observation next is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.723316501977832, 6.9112, 6.9112, 168.912956510431, 616340.1499663205, 616340.1499663205, 186973.1293935562], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6625810999729659, 0.0, 0.0, 0.8294399451523027, 0.1712055972128668, 0.1712055972128668, 0.2790643722291884], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.12323729], dtype=float32), -0.43199757]. 
=============================================
[2019-04-10 12:56:40,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 5.5995755e-22 1.3388406e-25 9.5158386e-20 2.1218440e-14], sum to 1.0000
[2019-04-10 12:56:40,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2590
[2019-04-10 12:56:40,284] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 79.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.944301907929013, 6.9112, 168.9125950249289, 852293.6618274337, 828810.0793194569, 254812.1579139678], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3347400.0000, 
sim time next is 3348000.0000, 
raw observation next is [30.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.983956875238296, 6.9112, 168.9123293028015, 880437.0960730648, 828821.0563420303, 254812.1583202387], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00727568752382961, 0.0, 0.829436865275705, 0.2445658600202958, 0.23022807120611952, 0.3803166542093115], 
reward next is 0.2559, 
noisyNet noise sample is [array([0.66024154], dtype=float32), 1.2999068]. 
=============================================
[2019-04-10 12:56:40,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[47.82368 ]
 [47.2592  ]
 [47.04934 ]
 [46.08562 ]
 [45.388172]], R is [[49.61382294]
 [49.11768341]
 [49.29604721]
 [48.99607086]
 [48.50611115]].
[2019-04-10 12:56:42,531] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4021087e-17 2.3481196e-30 1.7905873e-35 4.9893031e-22 1.0000000e+00], sum to 1.0000
[2019-04-10 12:56:42,540] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3317
[2019-04-10 12:56:42,546] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.3697705159349981, 1.0, 2.0, 0.3697705159349981, 1.0, 2.0, 0.6421691012008479, 6.9112, 6.9112, 170.5573041426782, 1550701.320187195, 1550701.320187195, 336786.1913197812], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.374979929701692, 1.0, 2.0, 0.374979929701692, 1.0, 2.0, 0.6512161301341369, 6.9112, 6.9112, 170.5573041426782, 1572563.991730908, 1572563.991730908, 339471.1458881281], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.84, 1.0, 1.0, 0.24696377072493014, 1.0, 1.0, 0.24696377072493014, 1.0, 1.0, 0.5746538172367524, 0.0, 0.0, 0.8375144448122397, 0.43682333103636334, 0.43682333103636334, 0.506673352071833], 
reward next is 0.4933, 
noisyNet noise sample is [array([-0.10996955], dtype=float32), 1.5815823]. 
=============================================
[2019-04-10 12:56:46,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 9.6558367e-23 6.5754150e-25 1.4919540e-20 2.2521844e-16], sum to 1.0000
[2019-04-10 12:56:46,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4394
[2019-04-10 12:56:46,490] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8579928584015939, 6.9112, 6.9112, 168.912956510431, 710959.2798855753, 710959.2798855753, 214000.6618672444], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8575951577770233, 6.9112, 6.9112, 168.912956510431, 710629.622705914, 710629.622705914, 213912.7129741632], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8263355582646627, 0.0, 0.0, 0.8294399451523027, 0.19739711741830943, 0.19739711741830943, 0.31927270593158685], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.17059927], dtype=float32), 1.1962991]. 
=============================================
[2019-04-10 12:56:59,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.3132480e-29 1.9035644e-34 2.6023994e-27 3.0649157e-26], sum to 1.0000
[2019-04-10 12:56:59,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2109
[2019-04-10 12:56:59,988] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8559264539161655, 6.9112, 6.9112, 168.912956510431, 709844.1747993702, 709844.1747993702, 213563.5925473469], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3627000.0000, 
sim time next is 3627600.0000, 
raw observation next is [28.0, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8469899778485553, 6.9112, 6.9112, 168.912956510431, 703714.157063147, 703714.157063147, 211642.5986865934], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8134024120104332, 0.0, 0.0, 0.8294399451523027, 0.19547615473976307, 0.19547615473976307, 0.31588447565163197], 
reward next is 0.6841, 
noisyNet noise sample is [array([-2.1958258], dtype=float32), -1.4469475]. 
=============================================
[2019-04-10 12:57:07,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4303280e-05 1.1586880e-10 1.2012360e-13 8.7223934e-06 9.9994695e-01], sum to 1.0000
[2019-04-10 12:57:07,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3578
[2019-04-10 12:57:07,564] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 65.5, 1.0, 2.0, 0.5961427041080182, 1.0, 2.0, 0.5961427041080182, 1.0, 2.0, 1.03, 6.917161032385855, 6.9112, 170.5573041426782, 2501095.207092407, 2496825.077115125, 486582.9414146326], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3755400.0000, 
sim time next is 3756000.0000, 
raw observation next is [31.66666666666667, 65.0, 1.0, 2.0, 0.6080699234449983, 1.0, 2.0, 0.6080699234449983, 1.0, 2.0, 1.03, 6.940446729890614, 6.9112, 170.5573041426782, 2551186.511114962, 2530235.888813407, 490922.5021284837], 
processed observation next is [1.0, 0.4782608695652174, 0.6998420221169038, 0.65, 1.0, 1.0, 0.5277950884879498, 1.0, 1.0, 0.5277950884879498, 1.0, 1.0, 1.0365853658536586, 0.0029246729890614275, 0.0, 0.8375144448122397, 0.7086629197541561, 0.7028433024481686, 0.7327201524305728], 
reward next is 0.1210, 
noisyNet noise sample is [array([-0.43366048], dtype=float32), 0.1370429]. 
=============================================
[2019-04-10 12:57:07,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[20.432898]
 [20.843925]
 [20.594463]
 [20.648788]
 [20.61953 ]], R is [[20.3772316 ]
 [20.4174118 ]
 [20.52345276]
 [20.63607788]
 [20.7481575 ]].
[2019-04-10 12:57:10,672] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 12:57:10,672] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:57:10,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:57:10,674] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:57:10,674] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:57:10,676] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:57:10,676] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:57:10,680] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:57:10,681] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:57:10,691] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:57:10,694] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-04-10 12:57:10,695] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-04-10 12:57:10,694] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-04-10 12:57:10,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:57:10,696] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-04-10 12:57:10,806] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-04-10 12:57:20,454] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:57:20,455] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.53333333333333, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6807375314058622, 6.911199999999999, 6.9112, 168.912956510431, 585429.9025257614, 585429.902525762, 179323.4885955943]
[2019-04-10 12:57:20,456] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 12:57:20,458] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 4.4253356e-31 8.5027419e-37 5.6256308e-30 8.9862258e-32], sampled 0.35849308370167443
[2019-04-10 12:57:24,564] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:57:24,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.96076785666667, 92.81722507333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7352581996722335, 6.9112, 6.9112, 168.912956510431, 624255.0816121883, 624255.0816121883, 189190.994090833]
[2019-04-10 12:57:24,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 12:57:24,567] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.9452840e-30 1.6131747e-35 7.5343072e-29 9.8976831e-31], sampled 0.22494752297570764
[2019-04-10 12:58:02,915] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:58:02,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.38333333333333, 60.16666666666666, 1.0, 2.0, 0.6645947853592983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928768.51221045, 928768.51221045, 213394.7556208804]
[2019-04-10 12:58:02,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:58:02,920] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999857e-01 1.4684946e-15 7.6514323e-20 7.5253137e-12 1.4645824e-06], sampled 0.86157343628908
[2019-04-10 12:58:02,920] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 928768.51221045 W.
[2019-04-10 12:58:09,273] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:58:09,275] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.95862551666666, 89.93476511333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7794251648696663, 6.911200000000001, 6.9112, 168.912956510431, 659001.0572012535, 659001.0572012528, 197756.849854604]
[2019-04-10 12:58:09,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:58:09,278] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.3022284e-30 2.6093160e-36 9.8257222e-30 3.2626007e-32], sampled 0.9608214869840347
[2019-04-10 12:58:20,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:58:20,487] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.12010342, 85.186394035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7071067955517868, 6.9112, 6.9112, 168.912956510431, 604622.2025908723, 604622.2025908723, 184009.4720151071]
[2019-04-10 12:58:20,488] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:58:20,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.0453080e-30 1.9860325e-36 9.1063242e-30 6.0715770e-32], sampled 0.5496636366061415
[2019-04-10 12:58:26,058] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.11666197], dtype=float32), 0.04579803]
[2019-04-10 12:58:26,059] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.43433465, 96.21839702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7339090493381392, 6.911199999999999, 6.9112, 168.912956510431, 626619.1935234156, 626619.1935234162, 188961.5161518855]
[2019-04-10 12:58:26,061] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 12:58:26,064] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.4668593e-29 1.7186516e-34 3.5102418e-28 5.2328739e-30], sampled 0.7950368544740302
[2019-04-10 12:58:31,627] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8151.3580 2949944419.5897 1119.0000
[2019-04-10 12:58:31,811] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7414.3774 3119299451.4084 1814.0000
[2019-04-10 12:58:31,928] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7118.6718 3198678880.3448 2182.0000
[2019-04-10 12:58:32,060] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7366.5702 3333404400.2326 1868.0000
[2019-04-10 12:58:32,182] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8010.4407 3000405515.8033 1297.0000
[2019-04-10 12:58:33,196] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 525000, evaluation results [525000.0, 7366.570189538483, 3333404400.232606, 1868.0, 7414.377416027639, 3119299451.40838, 1814.0, 8151.357999219951, 2949944419.5896564, 1119.0, 7118.671849237945, 3198678880.344761, 2182.0, 8010.440684300067, 3000405515.803343, 1297.0]
[2019-04-10 12:58:38,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.1155774e-24 4.7308835e-29 5.4544399e-23 5.3278291e-21], sum to 1.0000
[2019-04-10 12:58:38,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3025
[2019-04-10 12:58:38,209] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.942559791740264, 6.9112, 168.9125869451285, 851057.2640284188, 828809.5971230034, 254811.9313694233], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [28.83333333333334, 84.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99036704690589, 6.9112, 168.9123025363919, 884986.4411062562, 828822.8307591585, 254811.8951763205], 
processed observation next is [0.0, 0.0, 0.5655608214849924, 0.8483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007916704690588982, 0.0, 0.8294367338403801, 0.24582956697396005, 0.23022856409976625, 0.38031626145719477], 
reward next is 0.2238, 
noisyNet noise sample is [array([-1.1292505], dtype=float32), -1.8114705]. 
=============================================
[2019-04-10 12:58:41,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.5132502e-34 2.6643154e-28 5.2726806e-33 2.6486116e-18 1.0000000e+00], sum to 1.0000
[2019-04-10 12:58:41,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6003
[2019-04-10 12:58:41,032] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5095622606601069, 1.0, 2.0, 0.5095622606601069, 1.0, 2.0, 0.8849411319519481, 6.9112, 6.9112, 170.5573041426782, 2137528.721115626, 2137528.721115626, 421822.056985679], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3984000.0000, 
sim time next is 3984600.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4778774214644984, 1.0, 2.0, 0.4778774214644984, 1.0, 2.0, 0.8299150446056164, 6.9112, 6.9112, 170.5573041426782, 2004491.751865993, 2004491.751865993, 400185.4122898398], 
processed observation next is [1.0, 0.08695652173913043, 0.6129541864139019, 0.7983333333333335, 1.0, 1.0, 0.3709366523668655, 1.0, 1.0, 0.3709366523668655, 1.0, 1.0, 0.7925793226897759, 0.0, 0.0, 0.8375144448122397, 0.5568032644072203, 0.5568032644072203, 0.5972916601340893], 
reward next is 0.4027, 
noisyNet noise sample is [array([0.81417644], dtype=float32), 0.0037401025]. 
=============================================
[2019-04-10 12:58:47,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4343271e-28 5.9837700e-23 2.2791195e-27 2.0468656e-13 1.0000000e+00], sum to 1.0000
[2019-04-10 12:58:47,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9498
[2019-04-10 12:58:47,240] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 67.0, 1.0, 2.0, 0.8324923104256878, 1.0, 2.0, 0.7368361947271066, 1.0, 2.0, 1.03, 7.005108181377881, 6.9112, 170.5573041426782, 3092100.202146167, 3024829.951884605, 566444.570413025], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [34.33333333333333, 67.0, 1.0, 2.0, 0.8020944362514022, 1.0, 2.0, 0.7216372576399638, 1.0, 2.0, 1.03, 7.005105783588605, 6.9112, 170.5573041426782, 3028241.217992676, 2960972.685365108, 555516.1816019949], 
processed observation next is [1.0, 0.6956521739130435, 0.8262243285939966, 0.67, 1.0, 1.0, 0.7615595617486773, 1.0, 1.0, 0.66462320197586, 1.0, 1.0, 1.0365853658536586, 0.009390578358860502, 0.0, 0.8375144448122397, 0.8411781161090767, 0.8224924126014188, 0.8291286292567088], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16147842], dtype=float32), 0.7759278]. 
=============================================
[2019-04-10 12:58:49,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4155963e-05 5.9367875e-13 2.5042143e-16 1.8955141e-06 9.9998391e-01], sum to 1.0000
[2019-04-10 12:58:49,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-10 12:58:49,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.2586451468905316, 1.0, 2.0, 0.2586451468905316, 1.0, 2.0, 0.4491810848917213, 6.9112, 6.9112, 170.5573041426782, 1084440.73336288, 1084440.73336288, 288454.1635513834], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4163400.0000, 
sim time next is 4164000.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.254354074480106, 1.0, 2.0, 0.254354074480106, 1.0, 2.0, 0.4417289111941429, 6.9112, 6.9112, 170.5573041426782, 1066440.288000643, 1066440.288000643, 286930.7068365482], 
processed observation next is [1.0, 0.17391304347826086, 0.5260663507109005, 0.89, 1.0, 1.0, 0.10163141503627228, 1.0, 1.0, 0.10163141503627228, 1.0, 1.0, 0.31918159901724746, 0.0, 0.0, 0.8375144448122397, 0.29623341333351194, 0.29623341333351194, 0.4282547863232063], 
reward next is 0.5717, 
noisyNet noise sample is [array([-0.23161726], dtype=float32), -1.7132756]. 
=============================================
[2019-04-10 12:58:49,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[34.53478 ]
 [34.631927]
 [34.75131 ]
 [34.741714]
 [34.966087]], R is [[34.65391541]
 [34.87685013]
 [35.09581757]
 [35.30880737]
 [35.51284027]].
[2019-04-10 12:58:50,160] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0535301e-08 1.1721787e-13 8.7033628e-19 2.3566619e-08 1.0000000e+00], sum to 1.0000
[2019-04-10 12:58:50,166] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3323
[2019-04-10 12:58:50,172] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.285719890338855, 1.0, 2.0, 0.285719890338855, 1.0, 2.0, 0.4962009605069793, 6.9112, 6.9112, 170.5573041426782, 1198022.444159977, 1198022.444159977, 298656.1427739173], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [30.33333333333333, 83.16666666666667, 1.0, 2.0, 0.3193271672493168, 1.0, 2.0, 0.3193271672493168, 1.0, 2.0, 0.5545656864041438, 6.9112, 6.9112, 170.5573041426782, 1339025.636184163, 1339025.636184163, 312730.1396747531], 
processed observation next is [1.0, 0.30434782608695654, 0.6366508688783569, 0.8316666666666667, 1.0, 1.0, 0.17991224969797207, 1.0, 1.0, 0.17991224969797207, 1.0, 1.0, 0.45678742244407783, 0.0, 0.0, 0.8375144448122397, 0.37195156560671194, 0.37195156560671194, 0.46676140249963155], 
reward next is 0.5332, 
noisyNet noise sample is [array([1.0775942], dtype=float32), -0.9546155]. 
=============================================
[2019-04-10 12:58:50,190] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[41.724197]
 [41.717873]
 [41.52474 ]
 [41.42296 ]
 [41.220108]], R is [[41.55516815]
 [41.69386292]
 [41.83312225]
 [41.97603226]
 [42.1227417 ]].
[2019-04-10 12:58:53,473] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1389790e-18 1.3483331e-17 5.9065295e-22 3.9162305e-11 1.0000000e+00], sum to 1.0000
[2019-04-10 12:58:53,473] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7157
[2019-04-10 12:58:53,483] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.2816461314307009, 1.0, 2.0, 0.2816461314307009, 1.0, 2.0, 0.4874494101591092, 6.9112, 6.9112, 170.5573041426782, 1180931.780726772, 1180931.780726772, 296905.6565678613], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.2832792077641537, 1.0, 2.0, 0.2832792077641537, 1.0, 2.0, 0.4902292092985314, 6.9112, 6.9112, 170.5573041426782, 1187783.002246089, 1187783.002246089, 297537.9158100576], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.79, 1.0, 1.0, 0.13648097320982375, 1.0, 1.0, 0.13648097320982375, 1.0, 1.0, 0.37832830402259926, 0.0, 0.0, 0.8375144448122397, 0.3299397228461358, 0.3299397228461358, 0.44408644150754867], 
reward next is 0.5559, 
noisyNet noise sample is [array([2.0679033], dtype=float32), 0.23386042]. 
=============================================
[2019-04-10 12:58:54,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9734040e-15 4.0699133e-15 1.2096561e-19 4.4531937e-08 1.0000000e+00], sum to 1.0000
[2019-04-10 12:58:54,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8330
[2019-04-10 12:58:54,176] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.3037666104070517, 1.0, 2.0, 0.3037666104070517, 1.0, 2.0, 0.5273987962179993, 6.9112, 6.9112, 170.5573041426782, 1273737.226376287, 1273737.226376287, 306003.3982811733], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.3061495092087979, 1.0, 2.0, 0.3061495092087979, 1.0, 2.0, 0.5310849492045052, 6.9112, 6.9112, 170.5573041426782, 1283735.047350472, 1283735.047350472, 306964.2628657355], 
processed observation next is [1.0, 0.17391304347826086, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.16403555326361194, 1.0, 1.0, 0.16403555326361194, 1.0, 1.0, 0.4281523770786649, 0.0, 0.0, 0.8375144448122397, 0.35659306870846447, 0.35659306870846447, 0.45815561621751566], 
reward next is 0.5418, 
noisyNet noise sample is [array([0.16347618], dtype=float32), -0.05032427]. 
=============================================
[2019-04-10 12:58:58,113] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.9180924e-21 9.3587578e-15 3.6077301e-19 2.3525577e-07 9.9999976e-01], sum to 1.0000
[2019-04-10 12:58:58,120] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7306
[2019-04-10 12:58:58,124] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 56.66666666666667, 1.0, 2.0, 0.8171726955112709, 1.0, 2.0, 0.7291763872698982, 1.0, 2.0, 1.03, 7.005106972933527, 6.9112, 170.5573041426782, 3059916.785431352, 2992647.400827637, 560892.5841612078], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [35.66666666666666, 56.83333333333333, 1.0, 2.0, 0.8648817275290757, 1.0, 2.0, 0.7530309032788006, 1.0, 2.0, 1.03, 7.00511073653007, 6.9112, 170.5573041426782, 3160146.555963955, 3092874.475342942, 578466.925672148], 
processed observation next is [1.0, 0.6521739130434783, 0.889415481832543, 0.5683333333333332, 1.0, 1.0, 0.8372069006374405, 1.0, 1.0, 0.7024468714202416, 1.0, 1.0, 1.0365853658536586, 0.009391073653006999, 0.0, 0.8375144448122397, 0.8778184877677653, 0.8591317987063728, 0.8633834711524597], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33476207], dtype=float32), 1.3826481]. 
=============================================
[2019-04-10 12:58:58,132] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[16.563393]
 [16.560835]
 [16.56284 ]
 [16.554672]
 [16.60205 ]], R is [[16.58666229]
 [16.42079544]
 [16.25658798]
 [16.09402275]
 [15.93308258]].
[2019-04-10 12:59:02,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0246770e-01 1.3879304e-12 5.4885614e-17 5.8599131e-08 6.9753218e-01], sum to 1.0000
[2019-04-10 12:59:02,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6478
[2019-04-10 12:59:02,931] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.2012538494412505, 1.0, 2.0, 0.2012538494412505, 1.0, 2.0, 0.3495113808144117, 6.9112, 6.9112, 170.5573041426782, 843717.3120231851, 843717.3120231851, 270192.013551907], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4465800.0000, 
sim time next is 4466400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.2019115534780793, 1.0, 2.0, 0.2019115534780793, 1.0, 2.0, 0.3506535952203347, 6.9112, 6.9112, 170.5573041426782, 846475.6954028577, 846475.6954028577, 270375.4381834318], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.038447654792866603, 1.0, 1.0, 0.038447654792866603, 1.0, 1.0, 0.20811414051260327, 0.0, 0.0, 0.8375144448122397, 0.23513213761190493, 0.23513213761190493, 0.4035454301245251], 
reward next is 0.5965, 
noisyNet noise sample is [array([0.8105658], dtype=float32), -0.11239511]. 
=============================================
[2019-04-10 12:59:03,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3011594e-05 7.6415816e-14 5.0607158e-19 3.2675626e-11 9.9994695e-01], sum to 1.0000
[2019-04-10 12:59:03,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7840
[2019-04-10 12:59:03,302] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 79.0, 1.0, 2.0, 0.1950022536460697, 1.0, 2.0, 0.1950022536460697, 1.0, 2.0, 0.3386544263525045, 6.9112, 6.9112, 170.5573041426782, 817498.7334310771, 817498.7334310771, 268477.4766754621], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4472400.0000, 
sim time next is 4473000.0000, 
raw observation next is [29.5, 79.0, 1.0, 2.0, 0.1931698906643375, 1.0, 2.0, 0.1931698906643375, 1.0, 2.0, 0.3354722178249334, 6.911200000000001, 6.9112, 170.5573041426782, 809814.1045644126, 809814.104564412, 267985.3287400922], 
processed observation next is [0.0, 0.782608695652174, 0.5971563981042655, 0.79, 1.0, 1.0, 0.027915530920888557, 1.0, 1.0, 0.027915530920888557, 1.0, 1.0, 0.1896002656401627, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2249483623790035, 0.22494836237900334, 0.39997810259715255], 
reward next is 0.6000, 
noisyNet noise sample is [array([-0.720656], dtype=float32), -0.37257227]. 
=============================================
[2019-04-10 12:59:03,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[59.56215 ]
 [59.087437]
 [58.802414]
 [58.378204]
 [57.57527 ]], R is [[59.59384155]
 [59.59719086]
 [59.59929276]
 [59.60181427]
 [59.60400772]].
[2019-04-10 12:59:12,201] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9123469e-08 1.5656714e-09 3.0185303e-14 8.6251202e-06 9.9999130e-01], sum to 1.0000
[2019-04-10 12:59:12,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0500
[2019-04-10 12:59:12,219] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.33333333333333, 65.66666666666667, 1.0, 2.0, 0.5643756048786552, 1.0, 2.0, 0.5643756048786552, 1.0, 2.0, 0.9801337838096359, 6.911199999999999, 6.9112, 170.5573041426782, 2367691.221953602, 2367691.221953603, 462518.7279945372], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4624800.0000, 
sim time next is 4625400.0000, 
raw observation next is [33.66666666666667, 64.33333333333333, 1.0, 2.0, 0.5892697969612173, 1.0, 2.0, 0.5892697969612173, 1.0, 2.0, 1.023366762821922, 6.9112, 6.9112, 170.5573041426782, 2472231.647633838, 2472231.647633838, 482366.5180220704], 
processed observation next is [1.0, 0.5217391304347826, 0.7946287519747238, 0.6433333333333333, 1.0, 1.0, 0.5051443336882135, 1.0, 1.0, 0.5051443336882135, 1.0, 1.0, 1.0284960522218562, 0.0, 0.0, 0.8375144448122397, 0.6867310132316217, 0.6867310132316217, 0.7199500268986125], 
reward next is 0.2800, 
noisyNet noise sample is [array([-0.8515439], dtype=float32), 1.2139132]. 
=============================================
[2019-04-10 12:59:12,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1055846e-11 2.9930233e-10 8.5929554e-14 3.5037137e-06 9.9999654e-01], sum to 1.0000
[2019-04-10 12:59:12,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-10 12:59:12,625] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 81.0, 1.0, 2.0, 0.5830857439292609, 1.0, 2.0, 0.5830857439292609, 1.0, 2.0, 1.012627107803003, 6.911199999999999, 6.9112, 170.5573041426782, 2446261.579626556, 2446261.579626557, 477356.5546808612], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4609200.0000, 
sim time next is 4609800.0000, 
raw observation next is [31.0, 79.5, 1.0, 2.0, 0.6788176436366342, 1.0, 2.0, 0.6599988613325796, 1.0, 2.0, 1.03, 7.005096062006342, 6.9112, 170.5573041426782, 2769298.322791982, 2702036.754129237, 514722.8951704506], 
processed observation next is [1.0, 0.34782608695652173, 0.6682464454976303, 0.795, 1.0, 1.0, 0.6130333055863063, 1.0, 1.0, 0.5903600738946742, 1.0, 1.0, 1.0365853658536586, 0.009389606200634226, 0.0, 0.8375144448122397, 0.7692495341088839, 0.7505657650358992, 0.7682431271200756], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06045614], dtype=float32), -1.3851242]. 
=============================================
[2019-04-10 12:59:16,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.6387857e-19 9.6200542e-25 4.2473011e-18 6.9504640e-15], sum to 1.0000
[2019-04-10 12:59:16,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2146
[2019-04-10 12:59:16,444] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7657777367609742, 6.911199999999999, 6.9112, 168.912956510431, 647699.9483840843, 647699.9483840849, 195047.5053244033], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4654800.0000, 
sim time next is 4655400.0000, 
raw observation next is [24.08333333333333, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.766536276343667, 6.9112, 6.9112, 168.912956510431, 648668.7607734681, 648668.7607734681, 195201.6260003956], 
processed observation next is [1.0, 0.9130434782608695, 0.34044233807266966, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7152881418825207, 0.0, 0.0, 0.8294399451523027, 0.1801857668815189, 0.1801857668815189, 0.2913457104483516], 
reward next is 0.7087, 
noisyNet noise sample is [array([1.2291235], dtype=float32), -0.31335667]. 
=============================================
[2019-04-10 12:59:23,351] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-10 12:59:23,353] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 12:59:23,361] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 12:59:23,361] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:59:23,362] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-04-10 12:59:23,477] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:59:23,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-04-10 12:59:23,518] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 12:59:23,518] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:59:23,520] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-04-10 12:59:23,597] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 12:59:23,597] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:59:23,599] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-04-10 12:59:23,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 12:59:23,685] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 12:59:23,687] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-04-10 12:59:27,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12993667], dtype=float32), 0.044271052]
[2019-04-10 12:59:27,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.03333333333333, 60.5, 1.0, 2.0, 0.2455672956595272, 1.0, 2.0, 0.2455672956595272, 1.0, 2.0, 0.4117169578374898, 6.9112, 6.9112, 178.6582176852504, 1029559.517673776, 1029559.517673776, 284761.8032674546]
[2019-04-10 12:59:27,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 12:59:27,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7533107e-06 2.5423123e-13 2.9113176e-18 2.0514113e-09 9.9999619e-01], sampled 0.11997567109098872
[2019-04-10 13:00:08,364] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12993667], dtype=float32), 0.044271052]
[2019-04-10 13:00:08,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.85, 70.33333333333333, 1.0, 2.0, 0.2023964314990567, 1.0, 2.0, 0.2023964314990567, 1.0, 2.0, 0.3440858363980751, 6.9112, 6.9112, 169.0403247858759, 875025.6006813428, 875025.6006813428, 272924.5630159288]
[2019-04-10 13:00:08,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:00:08,368] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4680743e-02 1.5796130e-11 2.6582599e-16 3.5362493e-08 9.7531927e-01], sampled 0.5659488151702128
[2019-04-10 13:00:34,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12993667], dtype=float32), 0.044271052]
[2019-04-10 13:00:34,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.3, 45.0, 1.0, 2.0, 0.3659351235586475, 1.0, 2.0, 0.3659351235586475, 1.0, 2.0, 0.6355082930267679, 6.911200000000001, 6.9112, 178.6582176852504, 1534555.562167674, 1534555.562167673, 336859.5294502098]
[2019-04-10 13:00:34,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:00:34,116] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9438988e-02 1.9201082e-12 1.6290630e-18 5.9925838e-09 9.8056096e-01], sampled 0.48465503796897114
[2019-04-10 13:01:00,485] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12993667], dtype=float32), 0.044271052]
[2019-04-10 13:01:00,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.2, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.702389349805226, 6.911199999999999, 6.9112, 168.912956510431, 602118.3093779559, 602118.3093779564, 183159.9870090131]
[2019-04-10 13:01:00,487] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:01:00,489] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 4.3700511e-20 5.4760540e-25 5.4548048e-19 1.9314874e-16], sampled 0.4864294452141491
[2019-04-10 13:01:04,526] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8067.6705 3093531142.3079 462.0000
[2019-04-10 13:01:04,616] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7871.5868 3231941212.2060 663.0000
[2019-04-10 13:01:04,747] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7752.3592 3298100891.2357 620.0000
[2019-04-10 13:01:04,796] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7962.3375 3127997919.5834 503.0000
[2019-04-10 13:01:04,815] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7658.9142 3417363704.6295 878.0000
[2019-04-10 13:01:05,828] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 550000, evaluation results [550000.0, 7658.914221930242, 3417363704.6294675, 878.0, 7871.586829230386, 3231941212.205959, 663.0, 8067.67053354104, 3093531142.3078823, 462.0, 7752.359218801391, 3298100891.2356954, 620.0, 7962.337521437824, 3127997919.5834074, 503.0]
[2019-04-10 13:01:06,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.3625188e-20 1.9599584e-24 3.1702982e-19 6.0035717e-15], sum to 1.0000
[2019-04-10 13:01:06,328] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5860
[2019-04-10 13:01:06,331] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.875119161379458, 6.9112, 6.9112, 168.912956510431, 722768.5569846709, 722768.5569846709, 217745.4457024728], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4740000.0000, 
sim time next is 4740600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8736939099589192, 6.9112, 6.9112, 168.912956510431, 721592.3580844647, 721592.3580844647, 217424.1584760058], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8459681828767307, 0.0, 0.0, 0.8294399451523027, 0.20044232169012907, 0.20044232169012907, 0.32451366936717285], 
reward next is 0.6755, 
noisyNet noise sample is [array([-0.84681374], dtype=float32), -0.49227005]. 
=============================================
[2019-04-10 13:01:08,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7344232e-14 2.5091013e-17 1.0584170e-22 1.0015711e-11 1.0000000e+00], sum to 1.0000
[2019-04-10 13:01:08,267] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-10 13:01:08,271] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.2904027194931788, 1.0, 2.0, 0.2904027194931788, 1.0, 2.0, 0.504333486112774, 6.9112, 6.9112, 170.5573041426782, 1217668.679982653, 1217668.679982653, 300521.7950630576], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4780800.0000, 
sim time next is 4781400.0000, 
raw observation next is [30.0, 74.16666666666667, 1.0, 2.0, 0.3442861884439256, 1.0, 2.0, 0.3442861884439256, 1.0, 2.0, 0.5979112521447405, 6.9112, 6.9112, 170.5573041426782, 1443756.080714908, 1443756.080714908, 324191.7530321715], 
processed observation next is [1.0, 0.34782608695652173, 0.6208530805687204, 0.7416666666666667, 1.0, 1.0, 0.2099833595709947, 1.0, 1.0, 0.2099833595709947, 1.0, 1.0, 0.5096478684691957, 0.0, 0.0, 0.8375144448122397, 0.4010433557541411, 0.4010433557541411, 0.4838682881077187], 
reward next is 0.5161, 
noisyNet noise sample is [array([-0.4679719], dtype=float32), -1.745723]. 
=============================================
[2019-04-10 13:01:09,367] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.3765492e-01 5.0776402e-13 1.3816666e-18 6.1220011e-09 2.6234508e-01], sum to 1.0000
[2019-04-10 13:01:09,373] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1434
[2019-04-10 13:01:09,381] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2796542036055842, 6.911199999999999, 6.9112, 170.5573041426782, 683242.3189225656, 683242.3189225661, 257637.9643271045], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4815000.0000, 
sim time next is 4815600.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8510784620971236, 6.9112, 6.9112, 168.912956510431, 695001.8702513613, 695001.8702513613, 212087.4984791635], 
processed observation next is [1.0, 0.7391304347826086, 0.6366508688783573, 0.6866666666666668, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.8183883684111262, 0.0, 0.0, 0.8294399451523027, 0.19305607506982259, 0.19305607506982259, 0.3165485051927813], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.5766949], dtype=float32), 0.77714443]. 
=============================================
[2019-04-10 13:01:09,774] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.5607540e-26 1.5181418e-32 6.3557097e-23 4.6261242e-22], sum to 1.0000
[2019-04-10 13:01:09,784] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8109
[2019-04-10 13:01:09,789] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.83333333333333, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8689332135602241, 6.9112, 6.9112, 168.912956510431, 713395.7618843023, 713395.7618843023, 216192.624281598], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4817400.0000, 
sim time next is 4818000.0000, 
raw observation next is [29.66666666666667, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8660126433349072, 6.911199999999999, 6.9112, 168.912956510431, 712498.7054627738, 712498.7054627744, 215599.5239376744], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8366007845547648, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1979163070729927, 0.19791630707299288, 0.32179033423533493], 
reward next is 0.6782, 
noisyNet noise sample is [array([-1.2163324], dtype=float32), -0.1441543]. 
=============================================
[2019-04-10 13:01:09,796] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[57.738728]
 [57.95992 ]
 [57.735226]
 [56.803665]
 [56.490475]], R is [[57.43516159]
 [57.53813553]
 [57.64087677]
 [57.74510574]
 [57.85110855]].
[2019-04-10 13:01:12,265] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9206662e-17 6.7331385e-19 4.1993590e-24 1.6921696e-13 1.0000000e+00], sum to 1.0000
[2019-04-10 13:01:12,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5519
[2019-04-10 13:01:12,273] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5545405331691152, 1.0, 2.0, 0.5545405331691152, 1.0, 2.0, 0.9612965231102003, 6.911199999999999, 6.9112, 170.5573041426782, 2326392.328412047, 2326392.328412048, 454555.3139457667], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4881000.0000, 
sim time next is 4881600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5584972270875336, 1.0, 2.0, 0.5584972270875336, 1.0, 2.0, 0.9682658068898007, 6.9112, 6.9112, 170.5573041426782, 2343006.89634714, 2343006.89634714, 457616.5478237942], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4680689482982332, 1.0, 1.0, 0.4680689482982332, 1.0, 1.0, 0.961299764499757, 0.0, 0.0, 0.8375144448122397, 0.6508352489853166, 0.6508352489853166, 0.6830097728713346], 
reward next is 0.3170, 
noisyNet noise sample is [array([0.6871875], dtype=float32), 0.783401]. 
=============================================
[2019-04-10 13:01:15,698] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.612562e-12 3.732264e-18 4.304115e-23 6.800934e-13 1.000000e+00], sum to 1.0000
[2019-04-10 13:01:15,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-10 13:01:15,713] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.66666666666667, 1.0, 2.0, 0.4668737241829436, 1.0, 2.0, 0.4668737241829436, 1.0, 2.0, 0.8022723368945709, 6.9112, 6.9112, 170.5573041426782, 1958293.755653212, 1958293.755653212, 391562.5322494326], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4962000.0000, 
sim time next is 4962600.0000, 
raw observation next is [30.0, 68.0, 1.0, 2.0, 0.479102814556834, 1.0, 2.0, 0.479102814556834, 1.0, 2.0, 0.8224444570632268, 6.9112, 6.9112, 170.5573041426782, 2009636.573695854, 2009636.573695854, 399335.6193897742], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.68, 1.0, 1.0, 0.372413029586547, 1.0, 1.0, 0.372413029586547, 1.0, 1.0, 0.7834688500771059, 0.0, 0.0, 0.8375144448122397, 0.5582323815821817, 0.5582323815821817, 0.5960233125220511], 
reward next is 0.4040, 
noisyNet noise sample is [array([0.04707029], dtype=float32), 0.50841886]. 
=============================================
[2019-04-10 13:01:16,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6447302e-10 4.4838782e-15 6.1890519e-20 3.9266215e-10 1.0000000e+00], sum to 1.0000
[2019-04-10 13:01:16,456] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7907
[2019-04-10 13:01:16,460] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 65.0, 1.0, 2.0, 0.4348830063807924, 1.0, 2.0, 0.4348830063807924, 1.0, 2.0, 0.7431751623814666, 6.911199999999999, 6.9112, 170.5573041426782, 1823995.025444165, 1823995.025444165, 371161.591293623], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4970400.0000, 
sim time next is 4971000.0000, 
raw observation next is [30.36666666666666, 65.0, 1.0, 2.0, 0.4441325821051925, 1.0, 2.0, 0.4441325821051925, 1.0, 2.0, 0.7595638041290014, 6.911199999999999, 6.9112, 170.5573041426782, 1862823.505033348, 1862823.505033349, 376793.4510731976], 
processed observation next is [1.0, 0.5217391304347826, 0.6382306477093204, 0.65, 1.0, 1.0, 0.33028021940384644, 1.0, 1.0, 0.33028021940384644, 1.0, 1.0, 0.706785126986587, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5174509736203744, 0.5174509736203747, 0.562378285183877], 
reward next is 0.4376, 
noisyNet noise sample is [array([-1.3522991], dtype=float32), 1.0089788]. 
=============================================
[2019-04-10 13:01:16,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[53.172462]
 [53.21988 ]
 [52.850117]
 [52.180756]
 [52.86641 ]], R is [[53.0851326 ]
 [53.00030899]
 [52.94138718]
 [52.85300827]
 [52.67107391]].
[2019-04-10 13:01:17,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.2869821e-23 2.3500474e-29 1.3116662e-21 2.7788347e-19], sum to 1.0000
[2019-04-10 13:01:17,321] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-10 13:01:17,326] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8509374893745504, 6.911200000000001, 6.9112, 168.912956510431, 706426.2629180701, 706426.2629180695, 212488.9150699368], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [27.33333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8411505944609561, 6.911200000000001, 6.9112, 168.912956510431, 699879.4402785505, 699879.4402785498, 210402.689150696], 
processed observation next is [1.0, 0.8695652173913043, 0.4944707740916275, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8062812127572635, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1944109556329307, 0.1944109556329305, 0.3140338644040239], 
reward next is 0.6860, 
noisyNet noise sample is [array([0.57813126], dtype=float32), 0.3613307]. 
=============================================
[2019-04-10 13:01:21,237] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.1539694e-24 3.3428438e-29 7.9409175e-23 1.4645162e-19], sum to 1.0000
[2019-04-10 13:01:21,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9508
[2019-04-10 13:01:21,245] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8838121638803167, 6.9112, 6.9112, 168.912956510431, 729951.9943890817, 729951.9943890817, 219717.2486178718], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5086200.0000, 
sim time next is 5086800.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8837780698969964, 6.9112, 6.9112, 168.912956510431, 729924.2131311885, 729924.2131311885, 219709.4897232964], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8582659388987761, 0.0, 0.0, 0.8294399451523027, 0.20275672586977458, 0.20275672586977458, 0.32792461152730806], 
reward next is 0.6721, 
noisyNet noise sample is [array([0.5362731], dtype=float32), -0.3446074]. 
=============================================
[2019-04-10 13:01:22,114] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 9.1526009e-24 3.6130975e-29 1.4228221e-22 9.7549888e-22], sum to 1.0000
[2019-04-10 13:01:22,119] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2843
[2019-04-10 13:01:22,123] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.16666666666666, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8742111886755942, 6.9112, 6.9112, 168.912956510431, 723638.5322214165, 723638.5322214165, 217597.6019297928], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8720029599328271, 6.9112, 6.9112, 168.912956510431, 722230.4299762567, 722230.4299762567, 217114.5698362642], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8439060486985694, 0.0, 0.0, 0.8294399451523027, 0.20061956388229352, 0.20061956388229352, 0.3240515967705436], 
reward next is 0.6759, 
noisyNet noise sample is [array([0.3189455], dtype=float32), 0.9490332]. 
=============================================
[2019-04-10 13:01:23,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 6.0327428e-25 9.7101276e-32 3.7149235e-24 4.9197148e-22], sum to 1.0000
[2019-04-10 13:01:23,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7483
[2019-04-10 13:01:23,986] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.947176330864754, 6.9112, 6.9112, 168.912956510431, 769755.305596947, 769755.305596947, 234150.0055582089], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5151600.0000, 
sim time next is 5152200.0000, 
raw observation next is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9864135167873378, 6.9112, 6.9112, 168.912956510431, 801654.7828365151, 801654.7828365151, 243931.5575322771], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9834311180333388, 0.0, 0.0, 0.8294399451523027, 0.2226818841212542, 0.2226818841212542, 0.36407695154071207], 
reward next is 0.6359, 
noisyNet noise sample is [array([-1.8264073], dtype=float32), -0.50877875]. 
=============================================
[2019-04-10 13:01:25,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 8.7239896e-15 3.4563240e-19 5.2788725e-13 7.3325657e-10], sum to 1.0000
[2019-04-10 13:01:25,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0570
[2019-04-10 13:01:25,848] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8715967474451762, 6.911199999999999, 6.9112, 168.912956510431, 722237.7369859159, 722237.7369859166, 217034.8034612199], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5187000.0000, 
sim time next is 5187600.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8704203361586752, 6.911200000000001, 6.9112, 168.912956510431, 721261.1485467441, 721261.1485467434, 216770.4105383161], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8419760197057014, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20035031904076225, 0.20035031904076206, 0.32353792617659116], 
reward next is 0.6765, 
noisyNet noise sample is [array([0.8161376], dtype=float32), 0.78108144]. 
=============================================
[2019-04-10 13:01:26,683] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6503601e-06 2.6397311e-09 2.2647857e-14 1.1208689e-05 9.9998617e-01], sum to 1.0000
[2019-04-10 13:01:26,689] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1749
[2019-04-10 13:01:26,691] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 67.5, 1.0, 2.0, 0.6005488769896321, 1.0, 2.0, 0.6005488769896321, 1.0, 2.0, 1.03, 6.925763164234365, 6.9112, 170.5573041426782, 2519599.784683873, 2509167.597751152, 488177.4632647521], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5226600.0000, 
sim time next is 5227200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6241021871604564, 1.0, 2.0, 0.6241021871604564, 1.0, 2.0, 1.03, 6.971748239342964, 6.9112, 170.5573041426782, 2618521.127108811, 2575147.96032254, 496883.3071814512], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5471110688680197, 1.0, 1.0, 0.5471110688680197, 1.0, 1.0, 1.0365853658536586, 0.006054823934296394, 0.0, 0.8375144448122397, 0.7273669797524475, 0.7153188778673721, 0.7416168763902257], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21717227], dtype=float32), -0.091421366]. 
=============================================
[2019-04-10 13:01:27,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.68816185e-01 1.40229793e-06 1.50073357e-11 1.00404555e-04
 4.31082040e-01], sum to 1.0000
[2019-04-10 13:01:27,885] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5061
[2019-04-10 13:01:27,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2611680.702271762 W.
[2019-04-10 13:01:27,893] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 67.5, 1.0, 2.0, 0.933710298664042, 1.0, 2.0, 0.933710298664042, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2611680.702271762, 2611680.702271762, 490218.060621975], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5242200.0000, 
sim time next is 5242800.0000, 
raw observation next is [31.66666666666667, 68.0, 1.0, 2.0, 0.9947208982549801, 1.0, 2.0, 0.9947208982549801, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2782523.467541115, 2782523.467541116, 525802.5289185032], 
processed observation next is [1.0, 0.6956521739130435, 0.6998420221169038, 0.68, 1.0, 1.0, 0.9936396364517832, 1.0, 1.0, 0.9936396364517832, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7729231854280875, 0.7729231854280877, 0.7847798939082138], 
reward next is 0.2152, 
noisyNet noise sample is [array([0.31805825], dtype=float32), 0.056537706]. 
=============================================
[2019-04-10 13:01:28,360] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9818110e-01 1.7968370e-08 4.3404126e-14 4.0742229e-06 1.8148901e-03], sum to 1.0000
[2019-04-10 13:01:28,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8058
[2019-04-10 13:01:28,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2514160.079256404 W.
[2019-04-10 13:01:28,376] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.8988804321265228, 1.0, 2.0, 0.8988804321265228, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2514160.079256404, 2514160.079256405, 470881.7318597112], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5244600.0000, 
sim time next is 5245200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.9633698809143928, 1.0, 2.0, 0.9633698809143928, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2694730.976313011, 2694730.976313011, 507241.8799293115], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.9558673264028829, 1.0, 1.0, 0.9558673264028829, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7485363823091696, 0.7485363823091696, 0.7570774327303157], 
reward next is 0.2429, 
noisyNet noise sample is [array([0.31032822], dtype=float32), -0.50102305]. 
=============================================
[2019-04-10 13:01:31,941] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.5926555e-11 2.1050094e-15 3.3405292e-11 1.0224724e-09], sum to 1.0000
[2019-04-10 13:01:31,946] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3079
[2019-04-10 13:01:31,949] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.7, 80.0, 1.0, 2.0, 0.2066459786465838, 1.0, 2.0, 0.2066459786465838, 1.0, 1.0, 0.3588757260396984, 6.9112, 6.9112, 170.5573041426782, 866331.8870255651, 866331.8870255651, 271713.836523533], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5347800.0000, 
sim time next is 5348400.0000, 
raw observation next is [30.6, 80.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.137137577332086, 6.9112, 168.9115903320223, 989150.0580925975, 828863.4609803043, 254813.1278559396], 
processed observation next is [1.0, 0.9130434782608695, 0.6492890995260664, 0.8033333333333335, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.022593757733208618, 0.0, 0.8294332365908769, 0.2747639050257215, 0.23023985027230676, 0.38031810127752175], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5466106], dtype=float32), -1.0195067]. 
=============================================
[2019-04-10 13:01:33,840] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.3411421e-08 1.7081975e-11 1.2897122e-08 1.1207864e-08], sum to 1.0000
[2019-04-10 13:01:33,844] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2158
[2019-04-10 13:01:33,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 924026.130775157 W.
[2019-04-10 13:01:33,852] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 86.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.045375305973227, 6.9112, 168.9120562494249, 924026.130775157, 828838.0580898562, 254812.6509087823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5364000.0000, 
sim time next is 5364600.0000, 
raw observation next is [29.25, 86.5, 1.0, 1.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 11.54400727271086, 6.9112, 170.378410824454, 4770938.761195303, 1455751.329917418, 300535.4861216547], 
processed observation next is [1.0, 0.08695652173913043, 0.5853080568720379, 0.865, 1.0, 0.5, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.46328072727108605, 0.0, 0.8366359967220435, 1.3252607669986953, 0.40437536942150504, 0.4485604270472458], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21956751], dtype=float32), 0.07347024]. 
=============================================
[2019-04-10 13:01:38,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7046215e-17 1.6506618e-12 2.1024619e-18 6.8177826e-08 9.9999988e-01], sum to 1.0000
[2019-04-10 13:01:38,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3549
[2019-04-10 13:01:38,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.65, 74.0, 1.0, 2.0, 0.5809507147265496, 1.0, 2.0, 0.5809507147265496, 1.0, 2.0, 1.00891926814284, 6.911199999999999, 6.9112, 170.5573041426782, 2437295.602426161, 2437295.602426162, 475638.8818696329], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5473800.0000, 
sim time next is 5474400.0000, 
raw observation next is [31.86666666666667, 73.33333333333334, 1.0, 2.0, 0.5655887182931774, 1.0, 2.0, 0.5655887182931774, 1.0, 2.0, 0.9822405606279242, 6.9112, 6.9112, 170.5573041426782, 2372785.355806811, 2372785.355806811, 463466.1958745296], 
processed observation next is [1.0, 0.34782608695652173, 0.7093206951026858, 0.7333333333333334, 1.0, 1.0, 0.47661291360623775, 1.0, 1.0, 0.47661291360623775, 1.0, 1.0, 0.9783421471072244, 0.0, 0.0, 0.8375144448122397, 0.6591070432796696, 0.6591070432796696, 0.6917405908575068], 
reward next is 0.3083, 
noisyNet noise sample is [array([-1.7393355], dtype=float32), -0.09339924]. 
=============================================
[2019-04-10 13:01:39,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1090091e-03 6.5817083e-05 1.7894272e-09 7.6860808e-02 9.2196435e-01], sum to 1.0000
[2019-04-10 13:01:39,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9017
[2019-04-10 13:01:39,304] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.7, 86.0, 1.0, 2.0, 0.2674667738163223, 1.0, 2.0, 0.2674667738163223, 1.0, 2.0, 0.4645013335052157, 6.9112, 6.9112, 170.5573041426782, 1121447.167074492, 1121447.167074492, 291666.532083413], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5464800.0000, 
sim time next is 5465400.0000, 
raw observation next is [28.9, 85.16666666666667, 1.0, 2.0, 0.3345503822884557, 1.0, 2.0, 0.3345503822884557, 1.0, 2.0, 0.5810033765329847, 6.911199999999999, 6.9112, 170.5573041426782, 1402902.487941335, 1402902.487941336, 319618.3521961416], 
processed observation next is [1.0, 0.2608695652173913, 0.5687203791469194, 0.8516666666666667, 1.0, 1.0, 0.1982534726366936, 1.0, 1.0, 0.1982534726366936, 1.0, 1.0, 0.4890285079670545, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3896951355392597, 0.38969513553926, 0.4770423167106591], 
reward next is 0.5230, 
noisyNet noise sample is [array([0.4145516], dtype=float32), 1.7732402]. 
=============================================
[2019-04-10 13:01:42,731] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9980754e-01 6.6524881e-06 4.6698329e-10 1.8495503e-04 8.9242701e-07], sum to 1.0000
[2019-04-10 13:01:42,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6928
[2019-04-10 13:01:42,749] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2597909.523654192 W.
[2019-04-10 13:01:42,752] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.3, 68.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.351617586405919, 6.9112, 168.9103629519203, 2597909.523654192, 2285466.927299215, 475204.6362511718], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5565600.0000, 
sim time next is 5566200.0000, 
raw observation next is [31.5, 67.0, 1.0, 2.0, 0.7861091271095049, 1.0, 1.0, 0.7861091271095049, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2198460.275330243, 2198460.275330243, 413168.1460612272], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.67, 1.0, 1.0, 0.7423001531439818, 1.0, 0.5, 0.7423001531439818, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6106834098139564, 0.6106834098139564, 0.6166688747182495], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04474326], dtype=float32), 0.4339473]. 
=============================================
[2019-04-10 13:01:47,049] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 13:01:47,050] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:01:47,050] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:01:47,050] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:47,050] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:01:47,051] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:01:47,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:47,052] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:01:47,053] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:47,056] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:47,054] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:01:47,071] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-04-10 13:01:47,072] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-04-10 13:01:47,104] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-04-10 13:01:47,124] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-04-10 13:01:47,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-04-10 13:01:57,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:01:57,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.85, 84.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5724265991920503, 6.911199999999999, 6.9112, 168.912956510431, 503198.6155590275, 503198.6155590282, 161867.8198028852]
[2019-04-10 13:01:57,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:01:57,357] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 6.1189898e-31 2.7961829e-38 5.2089000e-28 1.3395146e-33], sampled 0.6059790129468161
[2019-04-10 13:02:30,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:02:30,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.32086577166667, 99.78599633500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4985222036609475, 6.9112, 6.9112, 168.912956510431, 443375.6716998272, 443375.6716998272, 151716.5021285128]
[2019-04-10 13:02:30,396] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:02:30,399] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 7.2866882e-31 4.5549218e-38 5.9074579e-28 1.3137664e-33], sampled 0.970495223295099
[2019-04-10 13:02:37,437] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:02:37,439] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.05, 53.0, 1.0, 2.0, 0.8541274358021017, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984909058321, 6.9112, 168.9123159833777, 2090829.5562582, 2023586.134519078, 421641.8168864069]
[2019-04-10 13:02:37,440] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:02:37,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.1277784e-17 1.1030871e-25 1.0747641e-12 1.7827480e-14], sampled 0.19962068193017435
[2019-04-10 13:02:37,443] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2090829.5562582 W.
[2019-04-10 13:02:41,113] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:02:41,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.75, 46.5, 1.0, 1.0, 0.6006705756491385, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9128920558001, 839399.3872541481, 839399.3872541476, 200869.7393140345]
[2019-04-10 13:02:41,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:02:41,122] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.8139619e-19 4.7122517e-25 1.1296137e-15 7.9180094e-18], sampled 0.9690711932124983
[2019-04-10 13:02:42,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:02:42,862] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666666, 71.0, 1.0, 2.0, 0.9874439415293157, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992688264394, 6.9112, 168.9123159524836, 2277427.668918387, 2210178.728375965, 459498.6392717263]
[2019-04-10 13:02:42,865] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:02:42,867] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.0923838e-16 2.5050089e-24 3.1384018e-10 1.1407177e-13], sampled 0.6458146031868082
[2019-04-10 13:02:42,868] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2277427.668918387 W.
[2019-04-10 13:03:01,731] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:03:01,732] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.49246258, 88.044990435, 1.0, 2.0, 0.8108628747481433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104288, 1133286.644406481, 1133286.64440648, 246601.0982737603]
[2019-04-10 13:03:01,733] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:03:01,734] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.5160698e-20 1.4828654e-26 7.7625554e-17 1.2900961e-19], sampled 0.515328754414404
[2019-04-10 13:03:01,735] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1133286.644406481 W.
[2019-04-10 13:03:20,445] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12368371], dtype=float32), 0.04922161]
[2019-04-10 13:03:20,447] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.37353691, 99.74715261, 1.0, 2.0, 0.7241077046366496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1011977.201018241, 1011977.201018242, 226140.0870019262]
[2019-04-10 13:03:20,448] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:03:20,449] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 4.065460e-15 1.739865e-20 4.116412e-11 5.686465e-11], sampled 0.6428583580718429
[2019-04-10 13:03:20,451] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1011977.201018241 W.
[2019-04-10 13:03:21,012] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7359.6464 3109146373.1404 1977.0000
[2019-04-10 13:03:21,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7971.2410 2991630705.5659 1431.0000
[2019-04-10 13:03:21,224] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8094.5642 2941431448.8102 1279.0000
[2019-04-10 13:03:21,265] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7058.7433 3186732979.3513 2384.0000
[2019-04-10 13:03:21,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7321.5979 3322331655.6202 2029.0000
[2019-04-10 13:03:22,338] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 575000, evaluation results [575000.0, 7321.597949741598, 3322331655.620249, 2029.0, 7359.646422848807, 3109146373.1404424, 1977.0, 8094.564184632133, 2941431448.810234, 1279.0, 7058.74332470249, 3186732979.351256, 2384.0, 7971.240991506905, 2991630705.5658793, 1431.0]
[2019-04-10 13:03:24,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.1579637e-30 7.7403837e-38 3.8350046e-28 5.0288259e-34], sum to 1.0000
[2019-04-10 13:03:24,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1364
[2019-04-10 13:03:24,539] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.36666666666667, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8612276124683232, 6.9112, 6.9112, 168.912956510431, 714372.9387784209, 714372.9387784209, 214741.5970265408], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5722800.0000, 
sim time next is 5723400.0000, 
raw observation next is [26.53333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.862524305602647, 6.911199999999999, 6.9112, 168.912956510431, 715186.3914972004, 715186.3914972009, 215021.2988723148], 
processed observation next is [0.0, 0.21739130434782608, 0.45655608214849913, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8323467141495696, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1986628865270001, 0.19866288652700026, 0.3209273117497236], 
reward next is 0.6791, 
noisyNet noise sample is [array([-0.43993232], dtype=float32), -0.05286876]. 
=============================================
[2019-04-10 13:03:24,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 7.9116068e-26 1.8005423e-32 7.2992927e-23 2.8338332e-29], sum to 1.0000
[2019-04-10 13:03:24,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0403
[2019-04-10 13:03:24,938] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.33333333333334, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8578583014689005, 6.9112, 6.9112, 168.912956510431, 712268.7299873306, 712268.7299873306, 214016.7900787225], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5710800.0000, 
sim time next is 5711400.0000, 
raw observation next is [26.3, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8584471188003896, 6.9112, 6.9112, 168.912956510431, 712631.6026108376, 712631.6026108376, 214143.1280262335], 
processed observation next is [0.0, 0.08695652173913043, 0.4454976303317536, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8273745351224262, 0.0, 0.0, 0.8294399451523027, 0.19795322294745488, 0.19795322294745488, 0.3196166089943783], 
reward next is 0.6804, 
noisyNet noise sample is [array([-0.91804427], dtype=float32), 0.4492526]. 
=============================================
[2019-04-10 13:03:25,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.3144913e-33 0.0000000e+00 2.1010153e-29 1.0879571e-36], sum to 1.0000
[2019-04-10 13:03:25,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0072
[2019-04-10 13:03:25,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.36666666666667, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782294730007763, 6.911199999999999, 6.9112, 168.912956510431, 724818.5745895375, 724818.574589538, 218429.6948492588], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5737200.0000, 
sim time next is 5737800.0000, 
raw observation next is [30.53333333333333, 64.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8787528296431407, 6.9112, 6.9112, 168.912956510431, 725168.7741283949, 725168.7741283949, 218545.2397416385], 
processed observation next is [0.0, 0.391304347826087, 0.646129541864139, 0.6483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8521375971257812, 0.0, 0.0, 0.8294399451523027, 0.2014357705912208, 0.2014357705912208, 0.32618692498752017], 
reward next is 0.6738, 
noisyNet noise sample is [array([0.6457436], dtype=float32), 0.9061586]. 
=============================================
[2019-04-10 13:03:27,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 5.5582295e-28 1.8161089e-35 8.8708225e-27 2.7483314e-30], sum to 1.0000
[2019-04-10 13:03:27,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8770
[2019-04-10 13:03:27,442] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9331640532414811, 6.911200000000001, 6.9112, 168.912956510431, 763110.3419035353, 763110.3419035347, 230981.1236944488], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5776800.0000, 
sim time next is 5777400.0000, 
raw observation next is [27.95, 83.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9328506758324578, 6.9112, 6.9112, 168.912956510431, 762846.3371070576, 762846.3371070576, 230905.3647945168], 
processed observation next is [0.0, 0.8695652173913043, 0.523696682464455, 0.8366666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.918110580283485, 0.0, 0.0, 0.8294399451523027, 0.21190176030751598, 0.21190176030751598, 0.344634872827637], 
reward next is 0.6554, 
noisyNet noise sample is [array([-0.6986408], dtype=float32), 0.96551716]. 
=============================================
[2019-04-10 13:03:29,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8208147e-01 5.4339847e-07 2.4888713e-12 1.7887173e-02 3.0748488e-05], sum to 1.0000
[2019-04-10 13:03:29,699] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4324
[2019-04-10 13:03:29,704] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2460854.040657257 W.
[2019-04-10 13:03:29,707] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 66.33333333333334, 1.0, 2.0, 0.5865605505292871, 1.0, 1.0, 0.5865605505292871, 1.0, 2.0, 1.018661697731146, 6.911200000000001, 6.9112, 170.5573041426782, 2460854.040657257, 2460854.040657257, 480163.9445887925], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5829600.0000, 
sim time next is 5830200.0000, 
raw observation next is [32.05, 66.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.626943052938643, 6.9112, 168.9089859160568, 2791907.002315704, 2284146.182672723, 474487.8765597259], 
processed observation next is [1.0, 0.4782608695652174, 0.7180094786729857, 0.66, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.07157430529386426, 0.0, 0.8294204477161794, 0.7755297228654733, 0.6344850507424231, 0.7081908605369044], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4496647], dtype=float32), -2.0805435]. 
=============================================
[2019-04-10 13:03:33,814] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4434450e-01 1.0552105e-07 7.7271663e-13 1.5560596e-01 4.9431426e-05], sum to 1.0000
[2019-04-10 13:03:33,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-10 13:03:33,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2389713.854935987 W.
[2019-04-10 13:03:33,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 78.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.0605289683031, 6.9112, 168.9120241083603, 2389713.854935987, 2283775.33010081, 475746.2236277924], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5934600.0000, 
sim time next is 5935200.0000, 
raw observation next is [30.33333333333334, 78.66666666666667, 1.0, 2.0, 0.5425542902455399, 1.0, 1.0, 0.5425542902455399, 1.0, 2.0, 0.9422373767109371, 6.9112, 6.9112, 170.5573041426782, 2276062.19989665, 2276062.19989665, 445821.5714508581], 
processed observation next is [1.0, 0.6956521739130435, 0.6366508688783573, 0.7866666666666667, 1.0, 1.0, 0.44886059065727696, 1.0, 0.5, 0.44886059065727696, 1.0, 1.0, 0.9295577764767525, 0.0, 0.0, 0.8375144448122397, 0.6322394999712917, 0.6322394999712917, 0.6654053305236688], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94877505], dtype=float32), -0.267589]. 
=============================================
[2019-04-10 13:03:34,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.000000e+00 8.194190e-35 0.000000e+00 1.921763e-32 0.000000e+00], sum to 1.0000
[2019-04-10 13:03:34,578] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7058
[2019-04-10 13:03:34,582] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.68333333333334, 80.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.00458408551019, 6.911200000000001, 6.9112, 168.9128459324027, 808341.9369974235, 808341.936997423, 248152.3113164387], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5940600.0000, 
sim time next is 5941200.0000, 
raw observation next is [29.56666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9927889991457135, 6.9112, 6.9112, 168.912956510431, 798847.4121429699, 798847.4121429699, 245120.2535251363], 
processed observation next is [1.0, 0.782608695652174, 0.6003159557661929, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9912060965191628, 0.0, 0.0, 0.8294399451523027, 0.22190205892860276, 0.22190205892860276, 0.3658511246643825], 
reward next is 0.6341, 
noisyNet noise sample is [array([-1.1684265], dtype=float32), -1.0815184]. 
=============================================
[2019-04-10 13:03:34,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 3.203708e-33 0.000000e+00 1.414786e-28 5.123894e-38], sum to 1.0000
[2019-04-10 13:03:34,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5001
[2019-04-10 13:03:34,686] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.9, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9799674398364687, 6.9112, 6.9112, 168.912956510431, 788526.7182802961, 788526.7182802961, 241866.790330534], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5939400.0000, 
sim time next is 5940000.0000, 
raw observation next is [29.8, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9866032835252004, 6.9112, 6.9112, 168.9129484946429, 793868.2176437862, 793868.2176437862, 243545.1767982653], 
processed observation next is [1.0, 0.782608695652174, 0.6113744075829385, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9836625408843906, 0.0, 0.0, 0.8294399057911135, 0.22051894934549618, 0.22051894934549618, 0.36350026387800793], 
reward next is 0.6365, 
noisyNet noise sample is [array([-0.14401358], dtype=float32), 1.3617183]. 
=============================================
[2019-04-10 13:03:34,696] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[57.747524]
 [56.471596]
 [53.32065 ]
 [49.915997]
 [46.629402]], R is [[58.36085892]
 [58.41625595]
 [58.47291565]
 [58.53390121]
 [58.6266861 ]].
[2019-04-10 13:03:43,768] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.1227626e-18 2.3964106e-26 2.0770378e-13 9.4447615e-23], sum to 1.0000
[2019-04-10 13:03:43,774] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4475
[2019-04-10 13:03:43,780] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 948028.0435974965 W.
[2019-04-10 13:03:43,790] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 88.66666666666667, 1.0, 2.0, 0.2261243375716568, 1.0, 1.0, 0.2261243375716568, 1.0, 2.0, 0.3902458616042275, 6.911199999999999, 6.9112, 170.5573041426782, 948028.0435974965, 948028.043597497, 277386.7579206688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6160200.0000, 
sim time next is 6160800.0000, 
raw observation next is [27.53333333333333, 88.33333333333334, 1.0, 2.0, 0.7092096421368757, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991146.687762812, 991146.6877628126, 222847.015023906], 
processed observation next is [1.0, 0.30434782608695654, 0.5039494470774091, 0.8833333333333334, 1.0, 1.0, 0.6496501712492477, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2753185243785589, 0.2753185243785591, 0.33260748511030747], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5083779], dtype=float32), -0.6330323]. 
=============================================
[2019-04-10 13:03:47,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.7724558e-35 0.0000000e+00 2.4464129e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:03:47,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3345
[2019-04-10 13:03:47,900] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.73333333333334, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8944886974673109, 6.9112, 6.9112, 168.912956510431, 735579.0213199387, 735579.0213199387, 222044.7849633827], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6240000.0000, 
sim time next is 6240600.0000, 
raw observation next is [26.8, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8965609742815377, 6.9112, 6.9112, 168.912956510431, 736831.1469815206, 736831.1469815206, 222505.1583019599], 
processed observation next is [0.0, 0.21739130434782608, 0.4691943127962086, 0.895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8738548466848021, 0.0, 0.0, 0.8294399451523027, 0.20467531860597793, 0.20467531860597793, 0.33209725119695505], 
reward next is 0.6679, 
noisyNet noise sample is [array([0.31513172], dtype=float32), -0.5890881]. 
=============================================
[2019-04-10 13:03:49,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.4469832e-37 0.0000000e+00 7.8107127e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:03:49,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0437
[2019-04-10 13:03:49,130] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.71666666666667, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9103129102862564, 6.911200000000001, 6.9112, 168.912956510431, 745416.4125799548, 745416.4125799541, 225594.9012960809], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6264600.0000, 
sim time next is 6265200.0000, 
raw observation next is [30.73333333333333, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9056497929145314, 6.9112, 6.9112, 168.912956510431, 742362.8257202517, 742362.8257202517, 224536.4912100736], 
processed observation next is [0.0, 0.5217391304347826, 0.6556082148499209, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8849387718469894, 0.0, 0.0, 0.8294399451523027, 0.20621189603340323, 0.20621189603340323, 0.3351290913583188], 
reward next is 0.6649, 
noisyNet noise sample is [array([0.983234], dtype=float32), -1.7206712]. 
=============================================
[2019-04-10 13:03:49,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.4507431e-34 0.0000000e+00 2.9932356e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:03:49,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1946
[2019-04-10 13:03:49,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.5, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.851623511344032, 6.9112, 6.9112, 168.912956510431, 706175.9430876522, 706175.9430876522, 212613.4142790608], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6280200.0000, 
sim time next is 6280800.0000, 
raw observation next is [30.46666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8496487901623365, 6.911200000000001, 6.9112, 168.912956510431, 704838.0802898043, 704838.0802898037, 212190.1092865178], 
processed observation next is [0.0, 0.6956521739130435, 0.6429699842022119, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8166448660516298, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19578835563605673, 0.19578835563605657, 0.3167016556515191], 
reward next is 0.6833, 
noisyNet noise sample is [array([-0.11485197], dtype=float32), -1.5583045]. 
=============================================
[2019-04-10 13:04:01,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 5.7793986e-29 5.1204821e-35 2.1422961e-24 3.1882596e-34], sum to 1.0000
[2019-04-10 13:04:01,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8184
[2019-04-10 13:04:01,159] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8817020717915709, 6.911200000000001, 6.9112, 168.912956510431, 727006.3449525684, 727006.3449525678, 219192.4896657901], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6565200.0000, 
sim time next is 6565800.0000, 
raw observation next is [26.95, 86.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8814979758796159, 6.9112, 6.9112, 168.912956510431, 726850.8241783377, 726850.8241783377, 219146.5761873945], 
processed observation next is [1.0, 1.0, 0.476303317535545, 0.8666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8554853364385558, 0.0, 0.0, 0.8294399451523027, 0.20190300671620492, 0.20190300671620492, 0.32708444207073806], 
reward next is 0.6729, 
noisyNet noise sample is [array([-0.5621765], dtype=float32), 0.5174281]. 
=============================================
[2019-04-10 13:04:03,076] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 13:04:03,077] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:04:03,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:04:03,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:04:03,078] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:04:03,078] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:04:03,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:04:03,079] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:04:03,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:04:03,082] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:04:03,082] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:04:03,094] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-04-10 13:04:03,094] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-04-10 13:04:03,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-04-10 13:04:03,127] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-04-10 13:04:03,141] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-04-10 13:04:17,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:04:17,163] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.79679386, 88.33553897000002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7124518679065672, 6.9112, 6.9112, 168.912956510431, 609024.4863179438, 609024.4863179438, 184981.6545073718]
[2019-04-10 13:04:17,165] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:04:17,167] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.7473514e-31 3.7767393e-38 2.6978598e-29 0.0000000e+00], sampled 0.17999825139580927
[2019-04-10 13:04:18,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:04:18,866] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [17.83333333333334, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4104381914183515, 6.9112, 6.9112, 168.912956510431, 370855.0687774135, 370855.0687774135, 141400.7557053817]
[2019-04-10 13:04:18,867] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:04:18,869] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.5951322e-35 0.0000000e+00 1.2340751e-33 0.0000000e+00], sampled 0.48579693819756375
[2019-04-10 13:05:05,648] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:05:05,649] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.90309271, 68.29370063, 1.0, 2.0, 0.747100203330404, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597822456106, 6.9112, 168.9123160230903, 1941040.699046305, 1873802.019485811, 395188.2964473356]
[2019-04-10 13:05:05,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:05:05,652] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.2589192e-18 1.9689274e-25 6.5413674e-13 1.6989500e-22], sampled 0.19260205891811377
[2019-04-10 13:05:05,654] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1941040.699046305 W.
[2019-04-10 13:05:09,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:05:09,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.26537881333333, 84.22196485, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.150488213272896, 6.9112, 168.9116566685513, 1623629.175629037, 1453871.18782327, 311362.6686097248]
[2019-04-10 13:05:09,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:05:09,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.9037838e-19 1.2378231e-26 1.7400538e-13 4.6592760e-23], sampled 0.20433681855541175
[2019-04-10 13:05:09,773] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1623629.175629037 W.
[2019-04-10 13:05:15,255] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:05:15,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.01666666666667, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.964971125824165, 6.9112, 6.9112, 168.912956510431, 783101.9786781631, 783101.9786781631, 238477.7414427558]
[2019-04-10 13:05:15,256] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:05:15,258] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 3.2164711e-31 3.9438514e-38 8.0348660e-29 0.0000000e+00], sampled 0.5870610867391369
[2019-04-10 13:05:34,204] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.12887742], dtype=float32), 0.04744233]
[2019-04-10 13:05:34,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.53259599, 68.29638735500001, 1.0, 2.0, 0.9229062676998131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1289977.029613731, 1289977.029613731, 276314.4281709095]
[2019-04-10 13:05:34,206] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:05:34,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 7.2042098e-17 9.7800284e-22 3.3177362e-13 2.5175730e-20], sampled 0.590224694797377
[2019-04-10 13:05:34,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1289977.029613731 W.
[2019-04-10 13:05:35,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.9073 3319525877.9852 2140.0000
[2019-04-10 13:05:35,259] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.4565 3185257463.4433 2464.0000
[2019-04-10 13:05:35,355] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7350.0201 3105715863.9998 2008.0000
[2019-04-10 13:05:35,373] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.6167 2989382995.4001 1566.0000
[2019-04-10 13:05:35,378] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.6842 2937783971.1587 1377.0000
[2019-04-10 13:05:36,390] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 600000, evaluation results [600000.0, 7287.907278157287, 3319525877.9851575, 2140.0, 7350.020138991863, 3105715863.9997993, 2008.0, 8060.68421724455, 2937783971.1586657, 1377.0, 7030.456536062478, 3185257463.4433084, 2464.0, 7924.616693874975, 2989382995.400146, 1566.0]
[2019-04-10 13:05:38,285] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 9.2020236e-26 4.1928934e-31 7.6124641e-19 1.1330620e-30], sum to 1.0000
[2019-04-10 13:05:38,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9521
[2019-04-10 13:05:38,290] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.53333333333334, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8576364847003449, 6.9112, 6.9112, 168.912956510431, 710700.5257481782, 710700.5257481782, 213923.0362577043], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6646800.0000, 
sim time next is 6647400.0000, 
raw observation next is [26.5, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8549352692578159, 6.911199999999999, 6.9112, 168.912956510431, 708787.1439663708, 708787.1439663714, 213337.4451809022], 
processed observation next is [1.0, 0.9565217391304348, 0.4549763033175356, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8230917917778242, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19688531776843635, 0.1968853177684365, 0.31841409728492864], 
reward next is 0.6816, 
noisyNet noise sample is [array([-1.8641057], dtype=float32), 0.32538787]. 
=============================================
[2019-04-10 13:05:42,365] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.0137480e-27 2.1928694e-35 1.5792393e-23 1.2092039e-34], sum to 1.0000
[2019-04-10 13:05:42,372] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2720
[2019-04-10 13:05:42,375] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.35, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.897231584998766, 6.9112, 6.9112, 168.912956510431, 786147.8112437947, 786147.8112437947, 223088.775587473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6747000.0000, 
sim time next is 6747600.0000, 
raw observation next is [22.3, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7326227733146478, 6.9112, 6.9112, 168.912956510431, 642075.9602737251, 642075.9602737251, 188523.3992421797], 
processed observation next is [1.0, 0.08695652173913043, 0.25592417061611383, 0.8333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6739302113593265, 0.0, 0.0, 0.8294399451523027, 0.17835443340936807, 0.17835443340936807, 0.2813782078241488], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.09721101], dtype=float32), -1.1555254]. 
=============================================
[2019-04-10 13:05:50,911] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:05:50,918] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5323
[2019-04-10 13:05:50,924] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7352651520041561, 6.9112, 6.9112, 168.912956510431, 624812.9897712702, 624812.9897712702, 189196.6632975504], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6938400.0000, 
sim time next is 6939000.0000, 
raw observation next is [26.7, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7363711496449032, 6.911200000000001, 6.9112, 168.912956510431, 625638.6753910359, 625638.6753910353, 189404.5757766604], 
processed observation next is [0.0, 0.30434782608695654, 0.46445497630331756, 0.735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6785014020059796, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1737885209419544, 0.17378852094195424, 0.2826933966815827], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.20903], dtype=float32), -0.6068799]. 
=============================================
[2019-04-10 13:05:50,937] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.79357 ]
 [71.843445]
 [71.891975]
 [71.8958  ]
 [71.95668 ]], R is [[71.74375153]
 [71.743927  ]
 [71.74479675]
 [71.74619293]
 [71.7479248 ]].
[2019-04-10 13:05:51,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:05:51,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-10 13:05:51,646] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.05, 66.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7438291974585731, 6.911199999999999, 6.9112, 168.912956510431, 630702.9727467664, 630702.972746767, 190809.1174546317], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6943800.0000, 
sim time next is 6944400.0000, 
raw observation next is [28.2, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7456237537866844, 6.911199999999999, 6.9112, 168.912956510431, 631996.9600349625, 631996.9600349631, 191149.8249772356], 
processed observation next is [0.0, 0.391304347826087, 0.5355450236966824, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6897850655935175, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1755547111208229, 0.17555471112082308, 0.28529824623468003], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.34651288], dtype=float32), -1.181246]. 
=============================================
[2019-04-10 13:05:51,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:05:51,854] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2134
[2019-04-10 13:05:51,861] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.95, 62.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.754178335423783, 6.911199999999999, 6.9112, 168.912956510431, 638240.6781133377, 638240.6781133383, 192785.7802860272], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6947400.0000, 
sim time next is 6948000.0000, 
raw observation next is [29.1, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7589005693913768, 6.911199999999999, 6.9112, 168.912956510431, 642065.4255987656, 642065.4255987662, 193701.8863109713], 
processed observation next is [0.0, 0.43478260869565216, 0.5781990521327015, 0.62, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7059763041358255, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17835150711076822, 0.1783515071107684, 0.2891072930014497], 
reward next is 0.7109, 
noisyNet noise sample is [array([-1.8141862], dtype=float32), -0.20191906]. 
=============================================
[2019-04-10 13:05:51,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.2101  ]
 [74.15244 ]
 [74.086815]
 [74.036   ]
 [73.987625]], R is [[74.27313995]
 [74.24266815]
 [74.21359253]
 [74.18516541]
 [74.15748596]].
[2019-04-10 13:05:53,297] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 8.2352952e-37 0.0000000e+00 2.1735385e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:05:53,302] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6180
[2019-04-10 13:05:53,308] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.03333333333334, 56.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6981093249977864, 6.911199999999999, 6.9112, 168.912956510431, 597208.9294324246, 597208.9294324252, 182390.604694785], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6981000.0000, 
sim time next is 6981600.0000, 
raw observation next is [28.96666666666667, 56.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.690081434455713, 6.911199999999999, 6.9112, 168.912956510431, 591149.2877517611, 591149.2877517617, 180965.1120507776], 
processed observation next is [0.0, 0.8260869565217391, 0.5718799368088469, 0.5633333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6220505298240402, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16420813548660032, 0.1642081354866005, 0.2700971821653397], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.17188464], dtype=float32), 1.0419139]. 
=============================================
[2019-04-10 13:05:53,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.7693924e-31 4.4767840e-38 9.1512162e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:05:53,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0180
[2019-04-10 13:05:53,858] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.13333333333334, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7649173616023915, 6.9112, 6.9112, 168.912956510431, 646801.0828001644, 646801.0828001644, 194875.833214466], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7001400.0000, 
sim time next is 7002000.0000, 
raw observation next is [26.1, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7659235397479236, 6.9112, 6.9112, 168.912956510431, 647514.3464406335, 647514.3464406335, 195071.9516672552], 
processed observation next is [1.0, 0.043478260869565216, 0.4360189573459717, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7145409021316141, 0.0, 0.0, 0.8294399451523027, 0.17986509623350933, 0.17986509623350933, 0.29115216666754506], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.05298534], dtype=float32), -0.9905422]. 
=============================================
[2019-04-10 13:05:53,865] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[55.176666]
 [55.27271 ]
 [55.369976]
 [55.62867 ]
 [56.21519 ]], R is [[55.19086838]
 [55.34810257]
 [55.50380325]
 [55.65770721]
 [55.81005478]].
[2019-04-10 13:05:55,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 7.9598790e-14 1.3882319e-20 2.4165967e-08 3.6648416e-17], sum to 1.0000
[2019-04-10 13:05:55,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6141
[2019-04-10 13:05:55,396] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1806603.434785504 W.
[2019-04-10 13:05:55,401] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.76666666666667, 58.16666666666667, 1.0, 2.0, 0.4307399377684368, 1.0, 2.0, 0.4307399377684368, 1.0, 2.0, 0.7195856394566033, 6.9112, 6.9112, 170.5573041426782, 1806603.434785504, 1806603.434785504, 366070.2958941245], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7035000.0000, 
sim time next is 7035600.0000, 
raw observation next is [29.93333333333334, 57.33333333333334, 1.0, 2.0, 0.4427596619766563, 1.0, 2.0, 0.4427596619766563, 1.0, 2.0, 0.739745552103229, 6.911199999999999, 6.9112, 170.5573041426782, 1857060.081355933, 1857060.081355934, 373075.5699764538], 
processed observation next is [1.0, 0.43478260869565216, 0.6176935229067935, 0.5733333333333335, 1.0, 1.0, 0.32862609876705573, 1.0, 1.0, 0.32862609876705573, 1.0, 1.0, 0.6826165269551574, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5158500225988703, 0.5158500225988706, 0.5568292089200803], 
reward next is 0.4432, 
noisyNet noise sample is [array([-0.38171384], dtype=float32), 1.6673083]. 
=============================================
[2019-04-10 13:06:00,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.00000000e+00 1.38737636e-17 9.12905930e-25 8.39053978e-13
 1.75294557e-24], sum to 1.0000
[2019-04-10 13:06:00,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9150
[2019-04-10 13:06:00,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1407177.364067553 W.
[2019-04-10 13:06:00,045] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.96666666666667, 77.66666666666667, 1.0, 2.0, 0.503353715975963, 1.0, 2.0, 0.503353715975963, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1407177.364067553, 1407177.364067553, 301286.39893317], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7132800.0000, 
sim time next is 7133400.0000, 
raw observation next is [26.85, 78.5, 1.0, 2.0, 0.5317772644978408, 1.0, 2.0, 0.5317772644978408, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1486693.43985001, 1486693.43985001, 310408.678692024], 
processed observation next is [1.0, 0.5652173913043478, 0.4715639810426541, 0.785, 1.0, 1.0, 0.43587622228655515, 1.0, 1.0, 0.43587622228655515, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4129703999583361, 0.4129703999583361, 0.4632965353612299], 
reward next is 0.5367, 
noisyNet noise sample is [array([-0.06376722], dtype=float32), -0.39760646]. 
=============================================
[2019-04-10 13:06:01,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.000000e+00 5.613033e-35 0.000000e+00 8.175348e-31 0.000000e+00], sum to 1.0000
[2019-04-10 13:06:01,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1894
[2019-04-10 13:06:01,457] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.8, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7928524427896403, 6.911199999999999, 6.9112, 168.912956510431, 665343.2341960453, 665343.2341960459, 200385.5829382162], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7174800.0000, 
sim time next is 7175400.0000, 
raw observation next is [25.8, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7937637936474538, 6.911200000000001, 6.9112, 168.912956510431, 665934.3699529376, 665934.369952937, 200567.965784817], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7484924312773826, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18498176943137154, 0.1849817694313714, 0.2993551728131597], 
reward next is 0.7006, 
noisyNet noise sample is [array([-0.37578267], dtype=float32), 0.7388786]. 
=============================================
[2019-04-10 13:06:04,162] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.8007774e-35 0.0000000e+00 6.1099599e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:06:04,171] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-10 13:06:04,174] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.45, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6399710379508032, 6.9112, 6.9112, 168.912956510431, 553953.6647652133, 553953.6647652133, 172424.692801355], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7248600.0000, 
sim time next is 7249200.0000, 
raw observation next is [22.43333333333333, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6373550698411226, 6.9112, 6.9112, 168.912956510431, 551913.1091430961, 551913.1091430961, 171996.6175203531], 
processed observation next is [1.0, 0.9130434782608695, 0.2622432859399683, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5577500851721007, 0.0, 0.0, 0.8294399451523027, 0.15330919698419337, 0.15330919698419337, 0.2567113694333628], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.9287591], dtype=float32), -0.25749534]. 
=============================================
[2019-04-10 13:06:04,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 6.275401e-34 0.000000e+00], sum to 1.0000
[2019-04-10 13:06:05,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4372
[2019-04-10 13:06:05,008] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5855910249701821, 6.911199999999999, 6.9112, 168.912956510431, 512407.4271966888, 512407.4271966895, 163860.8163526829], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7266600.0000, 
sim time next is 7267200.0000, 
raw observation next is [21.76666666666667, 88.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5761447912065499, 6.9112, 6.9112, 168.912956510431, 504201.5056263976, 504201.5056263976, 162472.4468379781], 
processed observation next is [1.0, 0.08695652173913043, 0.23064770932069528, 0.8866666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4831034039104267, 0.0, 0.0, 0.8294399451523027, 0.14005597378511045, 0.14005597378511045, 0.24249618931041506], 
reward next is 0.7575, 
noisyNet noise sample is [array([0.8074161], dtype=float32), -0.3801476]. 
=============================================
[2019-04-10 13:06:14,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.461049e-34 0.000000e+00], sum to 1.0000
[2019-04-10 13:06:14,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1273
[2019-04-10 13:06:14,995] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.66666666666666, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7074714291496843, 6.9112, 6.9112, 168.912956510431, 602733.0706916737, 602733.0706916737, 184065.5471991888], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7483200.0000, 
sim time next is 7483800.0000, 
raw observation next is [25.73333333333333, 78.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7096616286760403, 6.9112, 6.9112, 168.912956510431, 604483.2407646085, 604483.2407646085, 184462.5415068839], 
processed observation next is [0.0, 0.6086956521739131, 0.41864139020537117, 0.7833333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6459288154585856, 0.0, 0.0, 0.8294399451523027, 0.16791201132350236, 0.16791201132350236, 0.27531722612967746], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.01053433], dtype=float32), 0.43137008]. 
=============================================
[2019-04-10 13:06:17,087] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-10 13:06:17,089] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:06:17,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:06:17,090] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:06:17,090] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:06:17,092] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:06:17,091] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:06:17,093] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:06:17,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:06:17,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:06:17,096] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:06:17,105] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-04-10 13:06:17,105] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-04-10 13:06:17,140] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-04-10 13:06:17,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-04-10 13:06:17,141] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-04-10 13:06:27,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:06:27,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5332130436903474, 6.9112, 6.9112, 168.912956510431, 469072.5909474737, 469072.5909474737, 156402.3381152526]
[2019-04-10 13:06:27,323] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:06:27,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 7.1174669e-36 0.0000000e+00 3.9795425e-33 0.0000000e+00], sampled 0.4638758330820625
[2019-04-10 13:06:42,854] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:06:42,854] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.798279635, 67.964938935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7541369844288028, 6.911200000000002, 6.9112, 168.912956510431, 642893.3077397867, 642893.3077397854, 192824.1867718177]
[2019-04-10 13:06:42,856] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:06:42,858] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.9042546e-35 0.0000000e+00 2.4984437e-31 0.0000000e+00], sampled 0.8417779988580824
[2019-04-10 13:06:52,376] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:06:52,376] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.15, 77.5, 1.0, 2.0, 0.9787506239654554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1368082.851720897, 1368082.851720897, 292516.8274916714]
[2019-04-10 13:06:52,376] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:06:52,385] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.7140562e-18 2.6782159e-27 2.6809876e-11 4.1264477e-20], sampled 0.7391315167295176
[2019-04-10 13:06:52,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1368082.851720897 W.
[2019-04-10 13:07:26,411] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:26,412] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.34999999999999, 60.5, 1.0, 2.0, 0.6638168663407523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 927680.8987400802, 927680.8987400808, 213235.187393352]
[2019-04-10 13:07:26,413] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:07:26,415] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00000000e+00 7.09105638e-25 1.02176254e-32 6.54378085e-19
 2.29992101e-29], sampled 0.318646012474475
[2019-04-10 13:07:26,416] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 927680.8987400802 W.
[2019-04-10 13:07:35,011] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:35,011] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.99835893666667, 81.53822055666666, 1.0, 2.0, 0.8224108517014852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1149435.18436802, 1149435.184368019, 249483.5550428448]
[2019-04-10 13:07:35,011] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:07:35,012] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 8.8703622e-24 5.4975483e-33 7.5085878e-17 4.2892708e-28], sampled 0.459702003559491
[2019-04-10 13:07:35,014] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1149435.18436802 W.
[2019-04-10 13:07:36,626] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:36,627] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.596541055, 99.25927282166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 13.37978317959698, 6.9112, 168.8770838263329, 5447827.122660506, 859766.1744551983, 257108.377575427]
[2019-04-10 13:07:36,627] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:07:36,629] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 5.5363233e-22 8.6317070e-28 5.4557260e-17 5.7962979e-26], sampled 0.8354081579428184
[2019-04-10 13:07:36,630] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5447827.122660506 W.
[2019-04-10 13:07:42,177] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:42,178] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.05973997333333, 88.73758000999999, 1.0, 2.0, 0.819301969863071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956438686, 1145087.740500839, 1145087.740500839, 248712.0725938317]
[2019-04-10 13:07:42,179] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:07:42,181] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.6861250e-25 1.9443272e-32 1.1231951e-21 6.5898301e-32], sampled 0.6866665356180977
[2019-04-10 13:07:42,182] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1145087.740500839 W.
[2019-04-10 13:07:45,713] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:45,716] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.6, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8606795104264021, 6.911199999999999, 6.9112, 168.912956510431, 717369.6417502972, 717369.6417502977, 214724.7751741761]
[2019-04-10 13:07:45,716] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:07:45,719] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 3.043741e-36 0.000000e+00 2.636094e-32 0.000000e+00], sampled 0.539458580126515
[2019-04-10 13:07:54,904] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13662812], dtype=float32), 0.048295375]
[2019-04-10 13:07:54,904] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.13333333333333, 70.33333333333333, 1.0, 2.0, 0.7637182819743867, 1.0, 2.0, 0.7637182819743867, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2135778.826268747, 2135778.826268747, 402593.0125487307]
[2019-04-10 13:07:54,904] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:07:54,906] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9999964e-01 3.0590097e-15 8.3720254e-24 4.0676233e-07 7.4268357e-15], sampled 0.6661484514088483
[2019-04-10 13:07:54,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2135778.826268747 W.
[2019-04-10 13:07:55,265] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2422 3319559800.3494 2141.0000
[2019-04-10 13:07:55,502] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.3732 3105707721.0972 2009.0000
[2019-04-10 13:07:55,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.8131 2989431545.6547 1566.0000
[2019-04-10 13:07:55,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8122 2937703935.7172 1377.0000
[2019-04-10 13:07:55,663] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:07:56,679] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 625000, evaluation results [625000.0, 7288.242226404031, 3319559800.349445, 2141.0, 7348.373212887662, 3105707721.0971675, 2009.0, 8061.812245130806, 2937703935.717156, 1377.0, 7030.534901418457, 3185211196.137981, 2464.0, 7925.813122660512, 2989431545.654693, 1566.0]
[2019-04-10 13:07:59,123] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.5199679e-31 0.0000000e+00 1.8326499e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:07:59,129] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1688
[2019-04-10 13:07:59,132] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.93333333333334, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8019077474803196, 6.911199999999999, 6.9112, 168.912956510431, 671630.1126504984, 671630.1126504991, 202215.3950593901], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7600200.0000, 
sim time next is 7600800.0000, 
raw observation next is [24.86666666666667, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7985040156437883, 6.9112, 6.9112, 168.912956510431, 669289.3830200105, 669289.3830200105, 201525.7808804005], 
processed observation next is [0.0, 1.0, 0.3775671406003162, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7542731898094978, 0.0, 0.0, 0.8294399451523027, 0.1859137175055585, 0.1859137175055585, 0.3007847475826873], 
reward next is 0.6992, 
noisyNet noise sample is [array([-0.02867148], dtype=float32), -1.0468591]. 
=============================================
[2019-04-10 13:08:05,256] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.6556885e-22 3.3953678e-30 1.2967033e-18 8.1550044e-32], sum to 1.0000
[2019-04-10 13:08:05,259] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0087
[2019-04-10 13:08:05,265] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1989765.044949769 W.
[2019-04-10 13:08:05,269] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.00000000000001, 59.33333333333334, 1.0, 2.0, 0.7115546752723553, 1.0, 2.0, 0.7115546752723553, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1989765.044949769, 1989765.044949769, 379107.758461625], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7741200.0000, 
sim time next is 7741800.0000, 
raw observation next is [31.9, 59.5, 1.0, 2.0, 0.7045611597724378, 1.0, 2.0, 0.7045611597724378, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1970190.655989725, 1970190.655989725, 376080.4477159259], 
processed observation next is [1.0, 0.6086956521739131, 0.7109004739336492, 0.595, 1.0, 1.0, 0.6440495900872745, 1.0, 1.0, 0.6440495900872745, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.547275182219368, 0.547275182219368, 0.5613141010685462], 
reward next is 0.4387, 
noisyNet noise sample is [array([-0.716859], dtype=float32), -0.9862763]. 
=============================================
[2019-04-10 13:08:06,779] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.4986883e-26 3.6688123e-33 2.7489715e-22 1.4985558e-32], sum to 1.0000
[2019-04-10 13:08:06,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8742
[2019-04-10 13:08:06,794] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1022652.08859295 W.
[2019-04-10 13:08:06,799] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.2, 86.33333333333334, 1.0, 2.0, 0.3658728534525512, 1.0, 1.0, 0.3658728534525512, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1022652.08859295, 1022652.08859295, 263865.0316814873], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7785600.0000, 
sim time next is 7786200.0000, 
raw observation next is [26.15, 86.66666666666666, 1.0, 2.0, 0.3502762468933112, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5883174399978899, 6.911199999999999, 6.9112, 168.9129565104309, 979042.4260727138, 979042.4260727145, 236990.1140144685], 
processed observation next is [1.0, 0.08695652173913043, 0.43838862559241704, 0.8666666666666666, 1.0, 1.0, 0.21720029746182073, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.49794809755840225, -8.881784197001253e-17, 0.0, 0.8294399451523021, 0.2719562294646427, 0.27195622946464293, 0.3537165880812963], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09235153], dtype=float32), 0.2616756]. 
=============================================
[2019-04-10 13:08:09,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.9661899e-34 0.0000000e+00 1.9967782e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:08:09,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3964
[2019-04-10 13:08:09,238] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.3, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8394823924886723, 6.9112, 6.9112, 168.912956510431, 688671.3538058647, 688671.3538058647, 209700.9417210493], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7839600.0000, 
sim time next is 7840200.0000, 
raw observation next is [29.15, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8605734499899688, 6.9112, 6.9112, 168.912956510431, 705791.2117622736, 705791.2117622736, 214302.616557647], 
processed observation next is [1.0, 0.7391304347826086, 0.5805687203791469, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8299676219389862, 0.0, 0.0, 0.8294399451523027, 0.19605311437840933, 0.19605311437840933, 0.3198546515785776], 
reward next is 0.6801, 
noisyNet noise sample is [array([0.80405897], dtype=float32), 0.58620566]. 
=============================================
[2019-04-10 13:08:10,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.3178689e-31 0.0000000e+00 9.1142173e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:08:10,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8769
[2019-04-10 13:08:10,579] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.06666666666667, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8628775166201805, 6.911200000000001, 6.9112, 168.912956510431, 713580.5100328624, 713580.5100328617, 215035.8912275115], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7867200.0000, 
sim time next is 7867800.0000, 
raw observation next is [26.05, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8664408880597836, 6.9112, 6.9112, 168.912956510431, 716234.1123669059, 716234.1123669059, 215819.8736585978], 
processed observation next is [1.0, 0.043478260869565216, 0.43364928909952616, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8371230342192483, 0.0, 0.0, 0.8294399451523027, 0.1989539201019183, 0.1989539201019183, 0.3221192144158176], 
reward next is 0.6779, 
noisyNet noise sample is [array([-0.9641861], dtype=float32), 1.1003379]. 
=============================================
[2019-04-10 13:08:11,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.7810205e-18 3.0767737e-24 1.2619249e-17 2.3209603e-28], sum to 1.0000
[2019-04-10 13:08:11,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9966
[2019-04-10 13:08:12,004] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 902183.0309202938 W.
[2019-04-10 13:08:12,006] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.05, 83.66666666666667, 1.0, 2.0, 0.6455792064431739, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 902183.0309202938, 902183.0309202938, 209535.4462175242], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7888200.0000, 
sim time next is 7888800.0000, 
raw observation next is [27.1, 83.33333333333334, 1.0, 2.0, 0.3308080418791701, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5586316641124918, 6.911200000000001, 6.9112, 168.912956510431, 924603.9768086708, 924603.9768086702, 230529.4677678912], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.8333333333333335, 1.0, 1.0, 0.19374462877008447, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.46174593184450213, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25683443800240857, 0.2568344380024084, 0.3440738324893899], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5598916], dtype=float32), 1.437298]. 
=============================================
[2019-04-10 13:08:14,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:14,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:14,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-04-10 13:08:14,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:14,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:14,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-04-10 13:08:15,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-04-10 13:08:15,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,106] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-04-10 13:08:15,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-04-10 13:08:15,158] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,161] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-04-10 13:08:15,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,242] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-04-10 13:08:15,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,286] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-04-10 13:08:15,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-04-10 13:08:15,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-04-10 13:08:15,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-04-10 13:08:15,497] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-04-10 13:08:15,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-04-10 13:08:15,594] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-04-10 13:08:15,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:08:15,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:15,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-04-10 13:08:15,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-04-10 13:08:21,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 4.2839225e-36 0.0000000e+00 7.6829510e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:08:21,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2835
[2019-04-10 13:08:21,532] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6913297758176278, 6.9112, 6.9112, 168.912956510431, 599712.4730124623, 599712.4730124623, 181137.2146912563], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 97200.0000, 
sim time next is 97800.0000, 
raw observation next is [22.5, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7438878655067143, 6.9112, 6.9112, 168.912956510431, 645170.0220620674, 645170.0220620674, 190808.8490790097], 
processed observation next is [1.0, 0.13043478260869565, 0.2654028436018958, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6876681286667248, 0.0, 0.0, 0.8294399451523027, 0.17921389501724094, 0.17921389501724094, 0.28478932698359655], 
reward next is 0.7152, 
noisyNet noise sample is [array([-2.4051285], dtype=float32), 0.17658834]. 
=============================================
[2019-04-10 13:08:32,260] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 3.176287e-36 0.000000e+00], sum to 1.0000
[2019-04-10 13:08:32,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9965
[2019-04-10 13:08:32,272] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.93333333333333, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5016250304336499, 6.9112, 6.9112, 168.912956510431, 443667.5745969001, 443667.5745969001, 152221.783123717], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 337800.0000, 
sim time next is 338400.0000, 
raw observation next is [20.9, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4975122463831543, 6.9112, 6.9112, 168.912956510431, 440156.2608427943, 440156.2608427943, 151704.3987132985], 
processed observation next is [0.0, 0.9565217391304348, 0.1895734597156398, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.38721005656482227, 0.0, 0.0, 0.8294399451523027, 0.12226562801188731, 0.12226562801188731, 0.2264244756914903], 
reward next is 0.7736, 
noisyNet noise sample is [array([-2.7245822], dtype=float32), -2.2668731]. 
=============================================
[2019-04-10 13:08:37,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 8.6140978e-38 0.0000000e+00 3.7102187e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:08:37,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-10 13:08:37,250] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.86666666666666, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4229365413555983, 6.911200000000001, 6.9112, 168.912956510431, 378904.9235685278, 378904.9235685272, 142945.4472049272], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [19.9, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4247098885216627, 6.9112, 6.9112, 168.912956510431, 380402.9795722726, 380402.9795722726, 143135.425170276], 
processed observation next is [1.0, 0.2608695652173913, 0.14218009478672985, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2984266933191008, 0.0, 0.0, 0.8294399451523027, 0.10566749432563129, 0.10566749432563129, 0.21363496294071044], 
reward next is 0.7864, 
noisyNet noise sample is [array([0.39122736], dtype=float32), 1.8054903]. 
=============================================
[2019-04-10 13:08:39,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 8.0345613e-36 0.0000000e+00 4.8249183e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:08:39,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4400
[2019-04-10 13:08:39,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4420687485030141, 6.9112, 6.9112, 168.912956510431, 396475.7470193428, 396475.7470193428, 144936.6352286497], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 507600.0000, 
sim time next is 508200.0000, 
raw observation next is [19.73333333333333, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4411814946288094, 6.911200000000001, 6.9112, 168.912956510431, 395713.5954245076, 395713.5954245069, 144838.14898198], 
processed observation next is [1.0, 0.9130434782608695, 0.13428120063191146, 0.815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.31851401784001143, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10992044317347434, 0.10992044317347413, 0.21617634176414924], 
reward next is 0.7838, 
noisyNet noise sample is [array([0.7672504], dtype=float32), -0.053255823]. 
=============================================
[2019-04-10 13:08:40,193] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-10 13:08:40,194] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:08:40,195] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:40,196] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:08:40,196] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:40,196] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:08:40,197] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:08:40,197] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:40,198] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:40,199] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:08:40,200] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:08:40,212] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-04-10 13:08:40,226] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-04-10 13:08:40,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-04-10 13:08:40,246] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-04-10 13:08:40,278] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-04-10 13:08:57,557] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:08:57,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.05330437333333, 85.33775633166667, 1.0, 2.0, 0.5298751311649471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565071136, 818599.3996336729, 818599.3996336729, 197733.1232034082]
[2019-04-10 13:08:57,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:08:57,561] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.2194784e-32 0.0000000e+00 8.5023269e-28 0.0000000e+00], sampled 0.8311704365213486
[2019-04-10 13:08:57,704] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:08:57,705] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.68505129, 88.10614124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7420292522081814, 6.9112, 6.9112, 168.912956510431, 633366.0474232567, 633366.0474232567, 190499.2764361351]
[2019-04-10 13:08:57,707] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:08:57,709] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.6444538e-33 0.0000000e+00 1.8041961e-29 0.0000000e+00], sampled 0.2815156064482711
[2019-04-10 13:09:34,781] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:34,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666667, 52.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9056029835919747, 6.9112, 6.9112, 168.912956510431, 742041.7401686095, 742041.7401686095, 224513.4469598643]
[2019-04-10 13:09:34,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:09:34,796] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.4720846e-34 0.0000000e+00 4.6364228e-30 0.0000000e+00], sampled 0.38150163166001116
[2019-04-10 13:09:37,021] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:37,021] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.18333333333334, 47.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.986703182043608, 6.911200000000001, 6.9112, 168.9129565084642, 796328.7888904996, 796328.7888904989, 243705.9975527812]
[2019-04-10 13:09:37,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:09:37,024] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0479558e-36 0.0000000e+00], sampled 0.7694743061248265
[2019-04-10 13:09:49,122] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:49,122] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.11666666666667, 78.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9690545696728458, 6.9112, 6.9112, 168.912956510431, 787743.0181577519, 787743.0181577519, 239563.3722424048]
[2019-04-10 13:09:49,123] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:09:49,125] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.9602365e-30 0.0000000e+00 4.8573087e-26 0.0000000e+00], sampled 0.8629563165131033
[2019-04-10 13:09:53,895] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:53,896] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.9, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9799674398454253, 6.9112, 6.9112, 168.912956510431, 788526.7182875057, 788526.7182875057, 241866.7903327917]
[2019-04-10 13:09:53,899] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:09:53,901] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.7325188e-37 0.0000000e+00], sampled 0.4740288658062668
[2019-04-10 13:09:58,874] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:58,874] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.13696703, 89.54543537, 1.0, 2.0, 0.984689951474402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1376390.135917336, 1376390.135917336, 294307.2599357951]
[2019-04-10 13:09:58,875] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:09:58,877] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.4887316e-16 2.2230502e-25 1.2404661e-10 7.4600483e-18], sampled 0.16106994649889628
[2019-04-10 13:09:58,879] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1376390.135917336 W.
[2019-04-10 13:09:59,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:09:59,528] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.15, 56.83333333333333, 1.0, 2.0, 0.8055284195111636, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.963662391753353, 6.9112, 168.9126013559451, 2022810.680764101, 1985592.132693565, 410560.4047901795]
[2019-04-10 13:09:59,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:09:59,532] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.00000000e+00 3.57857119e-22 3.36625135e-35 5.36475940e-15
 1.05801924e-26], sampled 0.2902739101278974
[2019-04-10 13:09:59,532] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2022810.680764101 W.
[2019-04-10 13:10:02,226] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:10:02,226] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 70.5, 1.0, 2.0, 0.713258225599343, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002632588021792, 6.9112, 168.9123780048363, 1893681.908368297, 1828816.705714728, 387673.9720721609]
[2019-04-10 13:10:02,228] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:10:02,229] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999988e-01 4.9719356e-15 5.2332632e-26 9.8269368e-08 3.7946322e-15], sampled 0.26825437089768733
[2019-04-10 13:10:02,230] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1893681.908368297 W.
[2019-04-10 13:10:11,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13108656], dtype=float32), 0.040090345]
[2019-04-10 13:10:11,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.53333333333333, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7155441443239537, 6.911199999999999, 6.9112, 168.912956510431, 609649.1494993184, 609649.1494993191, 185538.3474482617]
[2019-04-10 13:10:11,580] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:10:11,583] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.9896334e-36 0.0000000e+00 1.9103314e-32 0.0000000e+00], sampled 0.16558668557839673
[2019-04-10 13:10:13,642] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.2982 3105472580.9823 2004.0000
[2019-04-10 13:10:14,027] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.5460 2989420346.8201 1566.0000
[2019-04-10 13:10:14,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.9593 3319464209.0367 2141.0000
[2019-04-10 13:10:14,426] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:10:14,457] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.8991 2937622518.4735 1375.0000
[2019-04-10 13:10:15,471] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 650000, evaluation results [650000.0, 7289.959287598338, 3319464209.036736, 2141.0, 7349.298248636515, 3105472580.9823003, 2004.0, 8062.899129726193, 2937622518.473538, 1375.0, 7030.534901418457, 3185211196.137981, 2464.0, 7923.546035955112, 2989420346.820067, 1566.0]
[2019-04-10 13:10:19,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 6.8619647e-32 0.0000000e+00 1.8465586e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:19,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3596
[2019-04-10 13:10:19,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.18333333333334, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3999063464665933, 6.911200000000001, 6.9112, 168.912956510431, 360915.5059647441, 360915.5059647434, 140414.1970301977], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 604200.0000, 
sim time next is 604800.0000, 
raw observation next is [18.1, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3988915971725611, 6.911200000000001, 6.9112, 168.912956510431, 360056.391797975, 360056.3917979744, 140310.7563918634], 
processed observation next is [1.0, 0.0, 0.05687203791469207, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2669409721616599, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10001566438832639, 0.10001566438832622, 0.2094190393908409], 
reward next is 0.7906, 
noisyNet noise sample is [array([-0.72976846], dtype=float32), -0.6614323]. 
=============================================
[2019-04-10 13:10:20,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.6345170e-31 0.0000000e+00 1.9162776e-27 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:20,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1787
[2019-04-10 13:10:20,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8883947160377544, 6.9112, 6.9112, 168.912956510431, 792847.8674232693, 792847.8674232693, 220232.6465351305], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 640800.0000, 
sim time next is 641400.0000, 
raw observation next is [23.16666666666667, 63.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004198049705221, 7.08275325386464, 6.9112, 168.9121030230327, 1017789.263930026, 896084.0961890002, 248477.1015372465], 
processed observation next is [1.0, 0.43478260869565216, 0.2969984202211693, 0.6316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0051195728112452, 0.01715532538646398, 0.0, 0.8294357541384488, 0.28271923998056275, 0.24891224894138894, 0.37086134557797984], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9083058], dtype=float32), 1.0471339]. 
=============================================
[2019-04-10 13:10:23,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.000000e+00 7.563279e-36 0.000000e+00 6.000397e-35 0.000000e+00], sum to 1.0000
[2019-04-10 13:10:23,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5319
[2019-04-10 13:10:23,097] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.6, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4002275072795648, 6.911199999999999, 6.9112, 168.912956510431, 361031.0721932916, 361031.0721932922, 140462.3823995392], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 704400.0000, 
sim time next is 705000.0000, 
raw observation next is [17.6, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3983844539923, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 140280.5688195635], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9216666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2663225048686585, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09983652937703905, 0.09983652937703892, 0.20937398331278131], 
reward next is 0.7906, 
noisyNet noise sample is [array([1.3763902], dtype=float32), -0.2698157]. 
=============================================
[2019-04-10 13:10:23,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.729744]
 [73.8546  ]
 [74.03825 ]
 [74.35244 ]
 [74.72725 ]], R is [[73.65262604]
 [73.70645142]
 [73.75932312]
 [73.81200409]
 [73.86310577]].
[2019-04-10 13:10:27,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.4228237e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:27,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2435
[2019-04-10 13:10:27,726] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.15, 88.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4893024626557924, 6.9112, 6.9112, 168.912956510431, 434605.4772025342, 434605.4772025342, 150615.7808538453], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 798600.0000, 
sim time next is 799200.0000, 
raw observation next is [20.3, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4912077425363247, 6.911199999999999, 6.9112, 168.912956510431, 436119.7882263062, 436119.7882263068, 150856.821658147], 
processed observation next is [0.0, 0.2608695652173913, 0.16113744075829392, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.37952163723942034, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1211443856184184, 0.12114438561841855, 0.22515943531066718], 
reward next is 0.7748, 
noisyNet noise sample is [array([1.4666538], dtype=float32), 0.27537957]. 
=============================================
[2019-04-10 13:10:29,013] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.5335928e-35 0.0000000e+00 6.3852794e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:29,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5462
[2019-04-10 13:10:29,024] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.93333333333333, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5594104570357795, 6.9112, 6.9112, 168.912956510431, 490571.6716944638, 490571.6716944638, 160047.4965250248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 843000.0000, 
sim time next is 843600.0000, 
raw observation next is [22.86666666666667, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5558472858421594, 6.9112, 6.9112, 168.912956510431, 487630.0184247644, 487630.0184247644, 159541.7028129832], 
processed observation next is [0.0, 0.782608695652174, 0.28278041074249627, 0.7833333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4583503485879992, 0.0, 0.0, 0.8294399451523027, 0.1354527828957679, 0.1354527828957679, 0.23812194449698984], 
reward next is 0.7619, 
noisyNet noise sample is [array([0.5070706], dtype=float32), -0.61396706]. 
=============================================
[2019-04-10 13:10:33,057] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.7260857e-38 0.0000000e+00 6.8301350e-38 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:33,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7657
[2019-04-10 13:10:33,066] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.86666666666667, 75.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5767204453737864, 6.9112, 6.9112, 168.912956510431, 503540.6873273705, 503540.6873273705, 162584.0264821013], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 926400.0000, 
sim time next is 927000.0000, 
raw observation next is [23.8, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5784418954467205, 6.9112, 6.9112, 168.912956510431, 504887.1789225037, 504887.1789225037, 162838.7657800714], 
processed observation next is [0.0, 0.7391304347826086, 0.3270142180094788, 0.76, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4859047505447811, 0.0, 0.0, 0.8294399451523027, 0.14024643858958435, 0.14024643858958435, 0.24304293400010657], 
reward next is 0.7570, 
noisyNet noise sample is [array([-0.11599521], dtype=float32), -0.7799043]. 
=============================================
[2019-04-10 13:10:33,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.775276]
 [80.78728 ]
 [80.751045]
 [80.66456 ]
 [80.63833 ]], R is [[80.7145462 ]
 [80.66474152]
 [80.61689758]
 [80.56944275]
 [80.52272797]].
[2019-04-10 13:10:33,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.6842253e-38 0.0000000e+00 1.5272148e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:33,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7155
[2019-04-10 13:10:33,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.65, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.601269034446243, 6.911200000000001, 6.9112, 168.912956510431, 522494.233093371, 522494.2330933705, 166290.8005172335], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [22.6, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6032235827776063, 6.911199999999999, 6.9112, 168.912956510431, 524183.5858627975, 524183.585862798, 166589.5136523051], 
processed observation next is [0.0, 0.8695652173913043, 0.27014218009478685, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5161263204604956, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14560655162855488, 0.145606551628555, 0.24864106515269416], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.31356415], dtype=float32), 0.19792755]. 
=============================================
[2019-04-10 13:10:34,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.7091074e-37 0.0000000e+00 2.4231244e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:34,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9484
[2019-04-10 13:10:34,629] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.88333333333333, 93.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.591394273073277, 6.911200000000001, 6.9112, 168.912956510431, 514014.6031773683, 514014.6031773677, 164796.5030991657], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5912673566631912, 6.911200000000001, 6.9112, 168.912956510431, 513924.8772747177, 513924.8772747171, 164777.1608605492], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5015455569063307, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14275691035408825, 0.14275691035408808, 0.24593606098589432], 
reward next is 0.7541, 
noisyNet noise sample is [array([-0.06559841], dtype=float32), -1.1117203]. 
=============================================
[2019-04-10 13:10:40,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9999928e-01 5.8339683e-12 6.6363518e-22 7.0297489e-07 1.9963000e-14], sum to 1.0000
[2019-04-10 13:10:40,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-10 13:10:40,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1230453.318136182 W.
[2019-04-10 13:10:40,599] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.7, 67.16666666666667, 1.0, 2.0, 0.4018762644644798, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7111532424848577, 6.911199999999999, 6.9112, 168.912956510431, 1230453.318136182, 1230453.318136182, 269667.3110097257], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1093800.0000, 
sim time next is 1094400.0000, 
raw observation next is [25.7, 67.0, 1.0, 2.0, 0.3996357440946913, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7092845450025638, 6.9112, 6.9112, 168.912956510431, 1229328.00018864, 1229328.00018864, 269282.2061833302], 
processed observation next is [1.0, 0.6956521739130435, 0.4170616113744076, 0.67, 1.0, 1.0, 0.2766695711984233, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6454689573201997, 0.0, 0.0, 0.8294399451523027, 0.34148000005240003, 0.34148000005240003, 0.40191374057213464], 
reward next is 0.5981, 
noisyNet noise sample is [array([1.2972842], dtype=float32), -0.6204073]. 
=============================================
[2019-04-10 13:10:45,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.3958914e-32 0.0000000e+00 1.2019047e-28 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:45,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5812
[2019-04-10 13:10:45,418] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.2, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6303353185850955, 6.9112, 6.9112, 168.912956510431, 546798.935816469, 546798.935816469, 170851.7304193206], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1204200.0000, 
sim time next is 1204800.0000, 
raw observation next is [23.1, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6315170212686476, 6.911199999999999, 6.9112, 168.912956510431, 547751.2515474142, 547751.2515474149, 171042.4446746424], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5506305137422531, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15215312542983728, 0.15215312542983747, 0.25528723085767524], 
reward next is 0.7447, 
noisyNet noise sample is [array([0.3812929], dtype=float32), -1.5837163]. 
=============================================
[2019-04-10 13:10:48,066] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.0156038e-31 0.0000000e+00 9.2734298e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:10:48,073] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3298
[2019-04-10 13:10:48,076] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.06666666666667, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.786624179822923, 6.911199999999999, 6.9112, 168.912956510431, 662346.3595153056, 662346.3595153063, 199163.8660497214], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1282200.0000, 
sim time next is 1282800.0000, 
raw observation next is [25.03333333333334, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7879834124845546, 6.911200000000001, 6.9112, 168.912956510431, 663453.2282169106, 663453.22821691, 199438.6876755385], 
processed observation next is [1.0, 0.8695652173913043, 0.3854660347551346, 0.8966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7414431859567739, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1842925633935863, 0.18429256339358613, 0.29766968309781866], 
reward next is 0.7023, 
noisyNet noise sample is [array([1.3623657], dtype=float32), -0.26919243]. 
=============================================
[2019-04-10 13:10:56,180] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-10 13:10:56,181] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:10:56,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:56,184] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:10:56,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:10:56,185] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:56,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:56,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:10:56,189] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:10:56,190] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:56,191] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:10:56,206] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-04-10 13:10:56,222] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-04-10 13:10:56,239] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-04-10 13:10:56,241] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-04-10 13:10:56,241] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-04-10 13:11:13,545] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:11:13,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.88689343, 80.33649422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5420396633341448, 6.9112, 6.9112, 168.912956510431, 476848.2431780769, 476848.2431780769, 157592.962779542]
[2019-04-10 13:11:13,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:11:13,549] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7384475e-37 0.0000000e+00], sampled 0.9677176672543394
[2019-04-10 13:11:27,023] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:11:27,025] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 61.33333333333334, 1.0, 2.0, 0.7687310527277295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1074372.137845701, 1074372.137845702, 236382.2462575464]
[2019-04-10 13:11:27,026] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:11:27,027] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.5905842e-33 0.0000000e+00 2.1093029e-29 0.0000000e+00], sampled 0.6037250155943262
[2019-04-10 13:11:27,028] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1074372.137845701 W.
[2019-04-10 13:11:40,138] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:11:40,139] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.872146533703869, 6.9112, 170.5573041426782, 3598496.727554839, 2910131.636656949, 548140.9127098132]
[2019-04-10 13:11:40,141] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:11:40,144] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.3134094e-27 0.0000000e+00 3.6712287e-21 2.2015747e-38], sampled 0.8289685512395758
[2019-04-10 13:11:40,144] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3598496.727554839 W.
[2019-04-10 13:11:43,253] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:11:43,254] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9424706007549744, 6.911200000000001, 6.9112, 168.912956510431, 769578.751089806, 769578.7510898054, 233180.1597332289]
[2019-04-10 13:11:43,255] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:11:43,256] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.7744643e-38 0.0000000e+00 2.2611843e-35 0.0000000e+00], sampled 0.9068627269323004
[2019-04-10 13:12:04,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:12:04,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.02914139, 58.78765249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9554356523139351, 6.9112, 6.9112, 168.912956510431, 776982.8676158215, 776982.8676158215, 236200.5425706718]
[2019-04-10 13:12:04,064] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:12:04,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 5.4730733e-38 0.0000000e+00 1.9475978e-35 0.0000000e+00], sampled 0.6490174838179902
[2019-04-10 13:12:08,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:12:08,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.66666666666666, 91.33333333333334, 1.0, 2.0, 0.6728749826631878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940345.1771390627, 940345.1771390627, 215099.4457474809]
[2019-04-10 13:12:08,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:12:08,247] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.2769537e-28 0.0000000e+00 8.7424346e-25 0.0000000e+00], sampled 0.5929306713696333
[2019-04-10 13:12:08,248] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 940345.1771390627 W.
[2019-04-10 13:12:13,779] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:12:13,780] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.53333333333333, 63.83333333333334, 1.0, 2.0, 0.9753542663116854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1363332.42590237, 1363332.42590237, 291509.8636834967]
[2019-04-10 13:12:13,780] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:12:13,782] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00000000e+00 9.48335240e-31 0.00000000e+00 1.05560035e-26
 0.00000000e+00], sampled 0.5253670381896455
[2019-04-10 13:12:13,782] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1363332.42590237 W.
[2019-04-10 13:12:20,817] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:12:20,817] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.82059898333333, 81.50291444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7564082744404328, 6.911200000000001, 6.9112, 168.912956510431, 642952.5049065605, 642952.5049065598, 193250.0040358249]
[2019-04-10 13:12:20,817] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:12:20,819] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.1267137e-37 0.0000000e+00 6.9940123e-35 0.0000000e+00], sampled 0.6929807500383764
[2019-04-10 13:12:24,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13644055], dtype=float32), 0.03943718]
[2019-04-10 13:12:24,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.86007196, 83.0675747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5387980272929802, 6.9112, 6.9112, 168.912956510431, 476014.464487378, 476014.464487378, 157077.9055277966]
[2019-04-10 13:12:24,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:12:24,251] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.1652057e-37 0.0000000e+00], sampled 0.40090855842596806
[2019-04-10 13:12:28,357] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.8696 3105762425.4286 2010.0000
[2019-04-10 13:12:28,569] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2435 3319589399.8756 2143.0000
[2019-04-10 13:12:28,814] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-04-10 13:12:28,847] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-04-10 13:12:28,881] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-04-10 13:12:29,895] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 675000, evaluation results [675000.0, 7288.243514976095, 3319589399.87561, 2143.0, 7348.869599821661, 3105762425.4286246, 2010.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-04-10 13:12:45,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.7011945e-38 0.0000000e+00 9.0558884e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:45,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5369
[2019-04-10 13:12:45,304] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.48333333333333, 95.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6116916802972266, 6.911200000000001, 6.9112, 168.912956510431, 532482.1838473212, 532482.1838473206, 167879.0139118687], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [21.5, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6142713039658578, 6.911200000000001, 6.9112, 168.912956510431, 534410.3794460463, 534410.3794460457, 168286.1564982283], 
processed observation next is [1.0, 1.0, 0.21800947867298584, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5295991511778754, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14844732762390175, 0.1484473276239016, 0.2511733679078034], 
reward next is 0.7488, 
noisyNet noise sample is [array([1.7058278], dtype=float32), -0.060634945]. 
=============================================
[2019-04-10 13:12:48,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.7584900e-33 0.0000000e+00 5.5505113e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:48,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5882
[2019-04-10 13:12:48,364] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7726627563629956, 6.9112, 6.9112, 168.912956510431, 651413.0798743238, 651413.0798743238, 196377.3863322957], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1903800.0000, 
sim time next is 1904400.0000, 
raw observation next is [24.3, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7745661909315058, 6.9112, 6.9112, 168.912956510431, 652647.5764196299, 652647.5764196299, 196749.8064759495], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7250807206481779, 0.0, 0.0, 0.8294399451523027, 0.1812909934498972, 0.1812909934498972, 0.29365642757604404], 
reward next is 0.7063, 
noisyNet noise sample is [array([0.39667118], dtype=float32), -0.2121673]. 
=============================================
[2019-04-10 13:12:52,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.7913591e-35 0.0000000e+00 1.2790084e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:52,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4861
[2019-04-10 13:12:52,682] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.16666666666666, 96.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7219108586488291, 6.9112, 6.9112, 168.912956510431, 615447.9821304696, 615447.9821304696, 186714.4414468113], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1979400.0000, 
sim time next is 1980000.0000, 
raw observation next is [23.2, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7240356144162533, 6.911199999999999, 6.9112, 168.912956510431, 616846.4755540118, 616846.4755540125, 187106.0150724886], 
processed observation next is [1.0, 0.9565217391304348, 0.29857819905213273, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6634580663612845, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17134624320944772, 0.1713462432094479, 0.2792627090634158], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.58352196], dtype=float32), -0.8811586]. 
=============================================
[2019-04-10 13:12:52,690] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[68.564835]
 [68.67916 ]
 [68.666214]
 [68.66211 ]
 [68.609085]], R is [[68.53385925]
 [68.56983948]
 [68.60625458]
 [68.64346313]
 [68.68125916]].
[2019-04-10 13:12:53,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 7.835988e-38 0.000000e+00 5.207034e-35 0.000000e+00], sum to 1.0000
[2019-04-10 13:12:53,681] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3019
[2019-04-10 13:12:53,687] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.76666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8538194728831964, 6.9112, 6.9112, 168.912956510431, 707557.854038222, 707557.854038222, 213081.7444970296], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2046000.0000, 
sim time next is 2046600.0000, 
raw observation next is [26.7, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.850673364969116, 6.9112, 6.9112, 168.912956510431, 705354.5522846185, 705354.5522846185, 212403.8816028467], 
processed observation next is [0.0, 0.6956521739130435, 0.46445497630331756, 0.855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8178943475233122, 0.0, 0.0, 0.8294399451523027, 0.19593182007906068, 0.19593182007906068, 0.3170207188102189], 
reward next is 0.6830, 
noisyNet noise sample is [array([0.1128132], dtype=float32), -0.010599167]. 
=============================================
[2019-04-10 13:12:56,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 4.610126e-38 0.000000e+00], sum to 1.0000
[2019-04-10 13:12:56,304] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7830
[2019-04-10 13:12:56,306] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.23333333333333, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7772792278870181, 6.9112, 6.9112, 168.912956510431, 653890.3115176935, 653890.3115176935, 197272.2613547311], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2082000.0000, 
sim time next is 2082600.0000, 
raw observation next is [24.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7753935435009786, 6.911199999999999, 6.9112, 168.912956510431, 652580.5756895425, 652580.575689543, 196900.8750962402], 
processed observation next is [0.0, 0.08695652173913043, 0.3459715639810427, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7260896871963152, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.181272382135984, 0.18127238213598418, 0.2938819031287167], 
reward next is 0.7061, 
noisyNet noise sample is [array([0.12931737], dtype=float32), 1.1354884]. 
=============================================
[2019-04-10 13:12:57,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.3407592e-37 0.0000000e+00 3.2373526e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:57,560] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6887
[2019-04-10 13:12:57,566] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.4, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9308939265138554, 6.911200000000001, 6.9112, 168.912956510431, 758314.5437898856, 758314.5437898849, 230299.2503313582], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [29.6, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9346668497062429, 6.911200000000001, 6.9112, 168.912956510431, 760684.4562303604, 760684.4562303597, 231172.0961538021], 
processed observation next is [0.0, 0.43478260869565216, 0.6018957345971565, 0.7566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.920325426471028, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2113012378417668, 0.2113012378417666, 0.345032979334033], 
reward next is 0.6550, 
noisyNet noise sample is [array([-1.9424766], dtype=float32), 1.1690693]. 
=============================================
[2019-04-10 13:12:57,585] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.8024 ]
 [68.79711]
 [68.78886]
 [68.78687]
 [68.74446]], R is [[68.79441833]
 [68.76274872]
 [68.73262787]
 [68.70401001]
 [68.67708588]].
[2019-04-10 13:12:57,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.3390374e-36 0.0000000e+00 3.8406146e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:57,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6976
[2019-04-10 13:12:57,996] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9237697062124418, 6.911200000000001, 6.9112, 168.912956510431, 753992.2469766083, 753992.2469766077, 228666.8075620852], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2110200.0000, 
sim time next is 2110800.0000, 
raw observation next is [29.2, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9272875872055905, 6.9112, 6.9112, 168.912956510431, 756099.9466820168, 756099.9466820168, 229470.2906000125], 
processed observation next is [0.0, 0.43478260869565216, 0.5829383886255924, 0.7733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.911326325860476, 0.0, 0.0, 0.8294399451523027, 0.2100277629672269, 0.2100277629672269, 0.34249297104479476], 
reward next is 0.6575, 
noisyNet noise sample is [array([-0.26754043], dtype=float32), 0.9342922]. 
=============================================
[2019-04-10 13:12:58,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.4220580e-33 0.0000000e+00 2.3205559e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:12:58,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4770
[2019-04-10 13:12:58,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.1, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9901313810673317, 6.9112, 6.9112, 168.912956510431, 797168.9587472043, 797168.9587472043, 244468.840059775], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2125200.0000, 
sim time next is 2125800.0000, 
raw observation next is [30.15, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.003844203611888, 6.911200000000001, 6.9112, 168.912848057333, 807930.0579188232, 807930.0579188226, 247971.6355150129], 
processed observation next is [0.0, 0.6086956521739131, 0.6279620853080569, 0.765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0046880531852291, 8.881784197001253e-17, 0.0, 0.8294394125979415, 0.224425016088562, 0.22442501608856183, 0.3701069186791237], 
reward next is 0.6299, 
noisyNet noise sample is [array([0.72123206], dtype=float32), -1.0054916]. 
=============================================
[2019-04-10 13:13:00,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 6.2299931e-32 0.0000000e+00 2.6326599e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:13:00,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8957
[2019-04-10 13:13:00,073] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.3, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8511672961874677, 6.911199999999999, 6.9112, 168.912956510431, 707918.0295500153, 707918.029550016, 212579.3906103706], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2167200.0000, 
sim time next is 2167800.0000, 
raw observation next is [25.25, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 10.18121207040254, 6.9112, 168.894149332389, 3177691.85266885, 858091.0435322666, 256166.9365635275], 
processed observation next is [1.0, 0.08695652173913043, 0.39573459715639814, 0.9416666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.3270012070402539, 0.0, 0.8293475932981531, 0.8826921812969029, 0.2383586232034074, 0.382338711288847], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5933664], dtype=float32), -0.6038426]. 
=============================================
[2019-04-10 13:13:02,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.6280301e-33 0.0000000e+00 2.1088303e-27 0.0000000e+00], sum to 1.0000
[2019-04-10 13:13:02,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4155
[2019-04-10 13:13:02,878] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.13333333333333, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9360697265751561, 6.9112, 6.9112, 168.912956510431, 753255.8699807245, 753255.8699807245, 231062.981349128], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2223600.0000, 
sim time next is 2224200.0000, 
raw observation next is [30.96666666666667, 71.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9515356139487609, 6.9112, 6.9112, 168.912956510431, 766297.371055169, 766297.371055169, 234845.0105384243], 
processed observation next is [1.0, 0.7391304347826086, 0.6666666666666667, 0.7133333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9408970901814155, 0.0, 0.0, 0.8294399451523027, 0.21286038084865808, 0.21286038084865808, 0.35051494110212583], 
reward next is 0.6495, 
noisyNet noise sample is [array([0.8976421], dtype=float32), -1.4991782]. 
=============================================
[2019-04-10 13:13:03,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999964e-01 1.2841641e-12 2.7757204e-18 3.4799695e-07 1.0868561e-16], sum to 1.0000
[2019-04-10 13:13:03,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7149
[2019-04-10 13:13:03,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1007995.218854224 W.
[2019-04-10 13:13:03,636] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.3, 86.0, 1.0, 2.0, 0.7212597984028324, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103366, 1007995.218854224, 1007995.218854225, 225498.4019541146], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2256000.0000, 
sim time next is 2256600.0000, 
raw observation next is [26.25, 86.0, 1.0, 2.0, 0.2290872154139184, 1.0, 1.0, 0.2290872154139184, 1.0, 1.0, 0.3864641542759419, 6.911200000000001, 6.9112, 170.5573041426782, 960455.4947978961, 960455.4947978954, 277695.0562985223], 
processed observation next is [1.0, 0.08695652173913043, 0.4431279620853081, 0.86, 1.0, 1.0, 0.07118941616134747, 1.0, 0.5, 0.07118941616134747, 1.0, 0.5, 0.2517855539950511, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2667931929994156, 0.2667931929994154, 0.41447023328137655], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.329099], dtype=float32), 1.7388151]. 
=============================================
[2019-04-10 13:13:07,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.0416442e-18 4.5278596e-24 3.6033646e-15 4.7672704e-25], sum to 1.0000
[2019-04-10 13:13:07,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6702
[2019-04-10 13:13:07,933] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.05, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9059176410604778, 6.911199999999999, 6.9112, 168.912956510431, 743941.6393067554, 743941.6393067561, 224655.9002370943], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [28.0, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9013764486295589, 6.9112, 6.9112, 168.912956510431, 740724.5742801601, 740724.5742801601, 223618.6632260314], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8797273763775108, 0.0, 0.0, 0.8294399451523027, 0.20575682618893337, 0.20575682618893337, 0.33375919884482297], 
reward next is 0.6662, 
noisyNet noise sample is [array([2.304893], dtype=float32), -0.97231865]. 
=============================================
[2019-04-10 13:13:10,407] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 13:13:10,409] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:13:10,410] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:13:10,411] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:13:10,411] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:13:10,412] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:13:10,413] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:13:10,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:13:10,413] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:13:10,414] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:13:10,413] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:13:10,436] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-04-10 13:13:10,452] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-04-10 13:13:10,452] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-04-10 13:13:10,453] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-04-10 13:13:10,483] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-04-10 13:13:14,318] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:14,319] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.61666666666667, 49.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3972892984371815, 6.9112, 6.9112, 168.912956510431, 358967.106817412, 358967.106817412, 140120.3861491749]
[2019-04-10 13:13:14,320] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:13:14,322] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.7422982e-25 3.4043246e-33 8.1905206e-23 4.9846855e-35], sampled 0.03904536279339654
[2019-04-10 13:13:17,714] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:17,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.0, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5225777645037526, 6.911200000000001, 6.9112, 168.912956510431, 463014.8448492708, 463014.8448492702, 154864.3575872287]
[2019-04-10 13:13:17,718] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:13:17,720] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.4492672e-25 7.5034875e-33 1.6097367e-22 1.8379793e-35], sampled 0.3887061700857297
[2019-04-10 13:13:26,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:26,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.76415705833333, 85.32298431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7739237754123296, 6.9112, 6.9112, 168.912956510431, 655301.344669215, 655301.344669215, 196670.4134093971]
[2019-04-10 13:13:26,151] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:13:26,154] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.3396372e-18 9.3928287e-26 8.7316476e-15 5.4019637e-25], sampled 0.29978357750332385
[2019-04-10 13:13:27,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:27,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.73333333333333, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8904597606165422, 6.9112, 6.9112, 168.912956510431, 757860.276293091, 757860.276293091, 221765.7079019736]
[2019-04-10 13:13:27,918] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:13:27,920] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.9337007e-20 1.1018543e-26 9.8299484e-17 6.0187152e-27], sampled 0.6172160517983252
[2019-04-10 13:13:33,080] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:33,081] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.69071756, 78.55179852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.973529270551266, 6.9112, 168.9124764245853, 873036.57372495, 828818.1696662302, 254812.8553157788]
[2019-04-10 13:13:33,082] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:13:33,084] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.8907815e-19 6.7624078e-26 1.2005471e-15 6.1332845e-26], sampled 0.4019158055546732
[2019-04-10 13:13:33,085] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 873036.57372495 W.
[2019-04-10 13:13:47,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:13:47,339] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.78304684666666, 71.31390719999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8623628174792195, 6.9112, 6.9112, 168.912956510431, 715271.5121206936, 715271.5121206936, 214992.3177683012]
[2019-04-10 13:13:47,339] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:13:47,341] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 7.8679562e-21 2.2738697e-31 7.2831313e-16 1.2351084e-30], sampled 0.9506907422892347
[2019-04-10 13:14:03,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:14:03,100] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.37370703666667, 86.23209127333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8013276853902669, 6.911199999999999, 6.9112, 168.912956510431, 675445.9749923223, 675445.974992323, 202183.6518756303]
[2019-04-10 13:14:03,100] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:14:03,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.9913873e-22 1.3748132e-29 3.0602534e-19 6.0992744e-31], sampled 0.9757369957799927
[2019-04-10 13:14:13,288] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:14:13,289] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.38254612666667, 65.51372209, 1.0, 2.0, 0.4851993940514349, 1.0, 2.0, 0.4851993940514349, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1356381.526632424, 1356381.526632424, 296201.6689227648]
[2019-04-10 13:14:13,294] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:14:13,296] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6150454e-02 3.7693200e-09 4.6371839e-20 9.8384953e-01 1.6548535e-11], sampled 0.914406901054409
[2019-04-10 13:14:37,631] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15200348], dtype=float32), 0.04236664]
[2019-04-10 13:14:37,821] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.28333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5837141668842638, 6.9112, 6.9112, 168.912956510431, 509433.2232813542, 509433.2232813542, 163614.6784100307]
[2019-04-10 13:14:37,821] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:14:37,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.3618163e-25 7.7180836e-33 1.4998559e-22 2.5685794e-35], sampled 0.45601920073083424
[2019-04-10 13:14:43,602] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7947.8209 3117806290.9486 747.0000
[2019-04-10 13:14:44,109] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7914.3051 3314948227.5402 597.0000
[2019-04-10 13:14:44,114] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8360.8244 2992700489.8637 501.0000
[2019-04-10 13:14:44,185] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7849.5336 3176359368.4242 687.0000
[2019-04-10 13:14:44,206] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8421.9439 2950308489.5896 542.0000
[2019-04-10 13:14:45,221] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 700000, evaluation results [700000.0, 7914.305072300443, 3314948227.540162, 597.0, 7947.8208924275705, 3117806290.9485655, 747.0, 8421.94391467325, 2950308489.5895786, 542.0, 7849.533614630041, 3176359368.4242353, 687.0, 8360.824400933787, 2992700489.863685, 501.0]
[2019-04-10 13:14:47,513] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0649175e-10 1.9353319e-14 1.4212949e-26 1.0000000e+00 2.5582282e-13], sum to 1.0000
[2019-04-10 13:14:47,521] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3510
[2019-04-10 13:14:47,525] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.95, 89.0, 1.0, 2.0, 0.6878904468084625, 1.0, 2.0, 0.6878904468084625, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1923531.871420328, 1923531.871420329, 368974.627586976], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2463000.0000, 
sim time next is 2463600.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6034818255576697, 1.0, 2.0, 0.6034818255576697, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1687316.190583003, 1687316.190583003, 335533.8421400451], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5222672597080358, 1.0, 1.0, 0.5222672597080358, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.46869894182861194, 0.46869894182861194, 0.5007967793135001], 
reward next is 0.4992, 
noisyNet noise sample is [array([0.81577384], dtype=float32), 0.8608047]. 
=============================================
[2019-04-10 13:14:56,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.2877026e-25 2.0648160e-36 1.8142720e-22 8.7698849e-34], sum to 1.0000
[2019-04-10 13:14:56,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1189
[2019-04-10 13:14:56,033] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8277300748438478, 6.9112, 6.9112, 168.912956510431, 689813.7229251548, 689813.7229251548, 207548.5366601994], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8345990306783748, 6.911200000000001, 6.9112, 168.912956510431, 694856.4561947358, 694856.4561947352, 209000.5152251858], 
processed observation next is [0.0, 0.6956521739130435, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7982915008272864, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19301568227631552, 0.19301568227631535, 0.31194106750027734], 
reward next is 0.6881, 
noisyNet noise sample is [array([-0.2625], dtype=float32), -1.3316435]. 
=============================================
[2019-04-10 13:15:00,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.00000000e+00 1.44154732e-26 1.05419114e-35 1.60865472e-24
 1.29791575e-33], sum to 1.0000
[2019-04-10 13:15:00,082] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-10 13:15:00,085] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6630724072552058, 6.911199999999999, 6.9112, 168.912956510431, 570630.7573937544, 570630.757393755, 176288.4399205728], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2751000.0000, 
sim time next is 2751600.0000, 
raw observation next is [22.0, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6577407589980202, 6.911200000000001, 6.9112, 168.912956510431, 566741.8131268523, 566741.8131268517, 175386.0194536074], 
processed observation next is [0.0, 0.8695652173913043, 0.2417061611374408, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5826106817049026, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15742828142412563, 0.15742828142412546, 0.26177017828896626], 
reward next is 0.7382, 
noisyNet noise sample is [array([0.92626536], dtype=float32), -0.064373955]. 
=============================================
[2019-04-10 13:15:22,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 3.7785788e-33 0.0000000e+00 1.5748283e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:15:22,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6397
[2019-04-10 13:15:22,078] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.83333333333333, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9915815892075764, 6.911200000000001, 6.9112, 168.9129319072702, 797875.5051917321, 797875.5051917314, 244812.015897322], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3246600.0000, 
sim time next is 3247200.0000, 
raw observation next is [33.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025281062916493, 6.911200000000001, 6.9112, 168.9127319699621, 825002.2969351917, 825002.2969351911, 253562.6227285167], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0308305645323086, 8.881784197001253e-17, 0.0, 0.8294388425558052, 0.22916730470421992, 0.22916730470421975, 0.378451675714204], 
reward next is 0.6215, 
noisyNet noise sample is [array([0.0862001], dtype=float32), -0.37805676]. 
=============================================
[2019-04-10 13:15:22,595] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.5807216e-26 6.4770033e-33 1.5117332e-24 2.6561557e-36], sum to 1.0000
[2019-04-10 13:15:22,603] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7739
[2019-04-10 13:15:22,608] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 77.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.020628159845012, 6.9112, 6.9112, 168.9128177915731, 821256.8459900392, 821256.8459900392, 252335.982691122], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3263400.0000, 
sim time next is 3264000.0000, 
raw observation next is [29.66666666666666, 77.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9924291149189162, 6.9112, 6.9112, 168.9129565104242, 799073.7981858366, 799073.7981858366, 245057.8977765338], 
processed observation next is [0.0, 0.782608695652174, 0.6050552922590835, 0.7766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9907672133157515, 0.0, 0.0, 0.8294399451522694, 0.22196494394051017, 0.22196494394051017, 0.3657580563828863], 
reward next is 0.6342, 
noisyNet noise sample is [array([-0.5131753], dtype=float32), -2.1310263]. 
=============================================
[2019-04-10 13:15:22,617] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[50.386124]
 [49.10502 ]
 [49.77821 ]
 [48.954453]
 [50.798313]], R is [[51.39640808]
 [50.88244629]
 [50.37362289]
 [49.86988831]
 [49.37118912]].
[2019-04-10 13:15:23,399] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.000000e+00 5.556299e-36 0.000000e+00 2.025360e-34 0.000000e+00], sum to 1.0000
[2019-04-10 13:15:23,406] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4129
[2019-04-10 13:15:23,411] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8813168188230998, 6.9112, 6.9112, 168.912956510431, 727576.1151952758, 727576.1151952758, 219137.7112213139], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3273000.0000, 
sim time next is 3273600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8798539236883292, 6.9112, 6.9112, 168.912956510431, 726591.7946593971, 726591.7946593971, 218813.6489491631], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8534803947418649, 0.0, 0.0, 0.8294399451523027, 0.20183105407205476, 0.20183105407205476, 0.32658753574501953], 
reward next is 0.6734, 
noisyNet noise sample is [array([0.8764359], dtype=float32), 0.7796204]. 
=============================================
[2019-04-10 13:15:23,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 4.7486655e-33 0.0000000e+00 5.5810425e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:15:23,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-10 13:15:23,968] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.33333333333334, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.80321576783027, 6.911199999999999, 6.9112, 168.912956510431, 673621.6583218668, 673621.6583218673, 202506.0378809292], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3285600.0000, 
sim time next is 3286200.0000, 
raw observation next is [26.16666666666667, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8009480907368747, 6.911199999999999, 6.9112, 168.912956510431, 672093.0460930732, 672093.0460930738, 202045.7483042865], 
processed observation next is [0.0, 0.0, 0.4391785150078992, 0.8316666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7572537691913106, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18669251280363144, 0.1866925128036316, 0.30156081836460674], 
reward next is 0.6984, 
noisyNet noise sample is [array([0.04546377], dtype=float32), 0.1087313]. 
=============================================
[2019-04-10 13:15:26,054] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 13:15:26,057] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:15:26,057] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:15:26,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:15:26,059] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:15:26,058] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:15:26,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:15:26,060] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:15:26,060] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:15:26,061] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:15:26,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:15:26,077] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-04-10 13:15:26,077] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-04-10 13:15:26,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-04-10 13:15:26,138] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-04-10 13:15:26,156] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-04-10 13:15:27,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14605781], dtype=float32), 0.04074599]
[2019-04-10 13:15:27,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6586311326942376, 6.9112, 6.9112, 168.912956510431, 574962.5313030612, 574962.5313030612, 175431.3091167479]
[2019-04-10 13:15:27,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:15:27,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.1021416e-31 0.0000000e+00 1.5451176e-31 0.0000000e+00], sampled 0.23720049186482794
[2019-04-10 13:15:37,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14605781], dtype=float32), 0.04074599]
[2019-04-10 13:15:37,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.14148272, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6214702990919112, 6.9112, 6.9112, 168.912956510431, 544866.6016402992, 544866.6016402992, 169318.4657672175]
[2019-04-10 13:15:37,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:15:37,602] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.2439078e-32 0.0000000e+00 3.5681523e-32 0.0000000e+00], sampled 0.21293064728172661
[2019-04-10 13:16:21,071] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14605781], dtype=float32), 0.04074599]
[2019-04-10 13:16:21,072] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.15, 86.0, 1.0, 2.0, 0.9083826145249925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269664.694248074, 1269664.694248074, 272258.5698155818]
[2019-04-10 13:16:21,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:16:21,075] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 9.8839459e-24 7.5995199e-35 1.0183790e-21 1.3574717e-34], sampled 0.005285577972293343
[2019-04-10 13:16:21,076] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1269664.694248074 W.
[2019-04-10 13:16:30,384] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14605781], dtype=float32), 0.04074599]
[2019-04-10 13:16:30,387] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.81515552, 78.26179840500001, 1.0, 2.0, 0.7434476657100311, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977996468599, 6.9112, 168.9123160292712, 1935929.200908528, 1868690.683161602, 394359.3845196033]
[2019-04-10 13:16:30,387] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:16:30,389] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.00000000e+00 1.16392689e-20 1.09054495e-30 2.91398442e-18
 2.13900791e-29], sampled 0.2856216203092632
[2019-04-10 13:16:30,390] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1935929.200908528 W.
[2019-04-10 13:17:03,813] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.4657 3105842961.3824 2010.0000
[2019-04-10 13:17:03,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:17:03,844] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.7172 3319665676.7007 2143.0000
[2019-04-10 13:17:04,181] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.7983 2989441646.9794 1566.0000
[2019-04-10 13:17:04,248] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0040 2937894402.2503 1381.0000
[2019-04-10 13:17:05,262] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 725000, evaluation results [725000.0, 7286.717165170319, 3319665676.700661, 2143.0, 7347.465737110903, 3105842961.3823595, 2010.0, 8061.003965267002, 2937894402.250323, 1381.0, 7030.534901418457, 3185211196.137981, 2464.0, 7923.798294513853, 2989441646.979391, 1566.0]
[2019-04-10 13:17:05,592] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.5924218e-22 1.4570972e-29 5.1174336e-21 2.7391717e-31], sum to 1.0000
[2019-04-10 13:17:05,592] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8438
[2019-04-10 13:17:05,639] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 79.0, 1.0, 1.0, 0.5958316864258776, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128923346858, 832634.6931279714, 832634.6931279714, 199971.4635850973], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3346800.0000, 
sim time next is 3347400.0000, 
raw observation next is [30.0, 79.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.943749671061995, 6.9112, 168.9125985377956, 851901.7344420122, 828809.9264553577, 254812.157874511], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.79, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00325496710619948, 0.0, 0.8294381873427822, 0.2366393706783367, 0.2302249795709327, 0.38031665354404626], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36875743], dtype=float32), 0.5209763]. 
=============================================
[2019-04-10 13:17:15,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:17:15,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-10 13:17:15,684] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333334, 71.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9349618247111083, 6.9112, 6.9112, 168.912956510431, 758870.2912250569, 758870.2912250569, 231141.5165865746], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [30.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9248986645314845, 6.911200000000001, 6.9112, 168.912956510431, 752977.7837121148, 752977.7837121141, 228844.4983939008], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.72, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9084130055262006, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20916049547558743, 0.20916049547558724, 0.3415589528267176], 
reward next is 0.6584, 
noisyNet noise sample is [array([-1.1663892], dtype=float32), 0.27331528]. 
=============================================
[2019-04-10 13:17:28,523] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 2.8419315e-21 3.2640141e-28 1.0525351e-21 3.4550148e-29], sum to 1.0000
[2019-04-10 13:17:28,526] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2670
[2019-04-10 13:17:28,529] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1038493.206810993 W.
[2019-04-10 13:17:28,532] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7430716303239664, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1038493.206810993, 1038493.206810993, 230413.3660954282], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3553800.0000, 
sim time next is 3554400.0000, 
raw observation next is [26.83333333333334, 79.66666666666667, 1.0, 2.0, 0.3499273321580229, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5844496634605878, 6.911199999999999, 6.9112, 168.912956510431, 978066.7400652252, 978066.7400652259, 236434.2809105011], 
processed observation next is [1.0, 0.13043478260869565, 0.4707740916271725, 0.7966666666666667, 1.0, 1.0, 0.2167799182626782, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.4932312969031558, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27168520557367365, 0.27168520557367387, 0.35288698643358374], 
reward next is 0.6471, 
noisyNet noise sample is [array([-1.2546357], dtype=float32), -0.5880893]. 
=============================================
[2019-04-10 13:17:29,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 6.5950925e-15 1.4922087e-22 1.3655436e-14 1.9297865e-23], sum to 1.0000
[2019-04-10 13:17:29,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1560
[2019-04-10 13:17:29,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2266041.71706076 W.
[2019-04-10 13:17:29,952] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 69.0, 1.0, 2.0, 0.8102517452764578, 1.0, 1.0, 0.8102517452764578, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2266041.71706076, 2266041.717060759, 424896.5253107327], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [31.5, 68.5, 1.0, 2.0, 0.8089110906412955, 1.0, 2.0, 0.8089110906412955, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2262288.897015209, 2262288.89701521, 424236.6570648809], 
processed observation next is [1.0, 0.4782608695652174, 0.6919431279620853, 0.685, 1.0, 1.0, 0.7697723983630066, 1.0, 1.0, 0.7697723983630066, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6284135825042246, 0.628413582504225, 0.6331890403953446], 
reward next is 0.3668, 
noisyNet noise sample is [array([-2.2508671], dtype=float32), -0.02916918]. 
=============================================
[2019-04-10 13:17:35,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.0265210e-32 0.0000000e+00 1.0100117e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:17:35,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3925
[2019-04-10 13:17:35,015] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8882312222529962, 6.9112, 6.9112, 168.912956510431, 732988.9704993886, 732988.9704993886, 220704.8133579982], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8890227391041103, 6.9112, 6.9112, 168.912956510431, 733981.0445219798, 733981.0445219798, 220898.752145597], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8646618769562321, 0.0, 0.0, 0.8294399451523027, 0.2038836234783277, 0.2038836234783277, 0.32969963006805525], 
reward next is 0.6703, 
noisyNet noise sample is [array([-0.28380585], dtype=float32), 0.16184646]. 
=============================================
[2019-04-10 13:17:35,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.2505772e-32 0.0000000e+00 2.1882495e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:17:35,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-10 13:17:35,168] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8217821244985165, 6.9112, 6.9112, 168.912956510431, 696764.1148111348, 696764.1148111348, 206525.9317414345], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3735000.0000, 
sim time next is 3735600.0000, 
raw observation next is [26.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8177749726319399, 6.911200000000001, 6.9112, 168.912956510431, 693402.7587438179, 693402.7587438173, 205678.3803432867], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7777743568682194, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1926118774288383, 0.19261187742883815, 0.3069826572287861], 
reward next is 0.6930, 
noisyNet noise sample is [array([1.309833], dtype=float32), -0.86225283]. 
=============================================
[2019-04-10 13:17:35,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.0747003e-20 1.3213352e-26 3.8254478e-21 6.9222013e-31], sum to 1.0000
[2019-04-10 13:17:35,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9673
[2019-04-10 13:17:35,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1300650.604443732 W.
[2019-04-10 13:17:35,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 1.0, 0.4574695399142304, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7702950426192273, 6.9112, 6.9112, 168.9116765553204, 1300650.604443732, 1300650.604443732, 283615.1416379001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3723600.0000, 
sim time next is 3724200.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.7250424296276333, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956190807, 1033896.047596772, 1033896.047596771, 229131.4720411295], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.74, 1.0, 1.0, 0.6687258188284738, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399435828025, 0.2871933465546589, 0.2871933465546586, 0.3419872717031784], 
reward next is 0.6580, 
noisyNet noise sample is [array([0.23433521], dtype=float32), 0.42836687]. 
=============================================
[2019-04-10 13:17:39,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 9.904212e-32 0.000000e+00 1.173679e-33 0.000000e+00], sum to 1.0000
[2019-04-10 13:17:39,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4351
[2019-04-10 13:17:39,737] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8975378582381511, 6.9112, 6.9112, 168.912956510431, 738223.4157592438, 738223.4157592438, 222754.7274288026], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3811200.0000, 
sim time next is 3811800.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9036996060589394, 6.911199999999999, 6.9112, 168.912956510431, 742411.409173976, 742411.4091739766, 224150.4356460368], 
processed observation next is [0.0, 0.08695652173913043, 0.4865718799368086, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8825604951938284, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20622539143721558, 0.20622539143721574, 0.33455288902393554], 
reward next is 0.6654, 
noisyNet noise sample is [array([-0.02976346], dtype=float32), 0.6166848]. 
=============================================
[2019-04-10 13:17:45,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.00000000e+00 3.34590091e-17 1.05443945e-22 1.35044287e-18
 1.62163560e-25], sum to 1.0000
[2019-04-10 13:17:45,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-10 13:17:45,524] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [35.0, 56.0, 1.0, 1.0, 0.6087333860743608, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128583112549, 850671.1703515152, 850671.1703515158, 202384.0992571681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3940200.0000, 
sim time next is 3940800.0000, 
raw observation next is [35.0, 56.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.003663569083582, 6.9112, 168.912278662557, 894423.0863875833, 828826.5113694251, 254812.6753018188], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009246356908358155, 0.0, 0.8294366166089212, 0.24845085732988426, 0.23022958649150696, 0.3803174258236101], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28360397], dtype=float32), -1.9481757]. 
=============================================
[2019-04-10 13:17:47,254] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 9.5799894e-14 1.9459501e-19 4.6679718e-17 4.5453211e-24], sum to 1.0000
[2019-04-10 13:17:47,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8806
[2019-04-10 13:17:47,260] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 1.0, 1.0, 0.2994555297573184, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5200552237314818, 6.911199999999999, 6.9112, 168.9128992764216, 836939.6052464907, 836939.6052464913, 221777.2542520231], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3979200.0000, 
sim time next is 3979800.0000, 
raw observation next is [29.0, 84.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.962853879159915, 6.9112, 168.9124960840266, 865460.1544447071, 828815.2146671463, 254811.9397091513], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005165387915991459, 0.0, 0.829437684247873, 0.24040559845686307, 0.23022644851865176, 0.3803163279241064], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2359963], dtype=float32), -0.8845104]. 
=============================================
[2019-04-10 13:17:49,988] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 6.2384371e-24 1.0025701e-31 2.2572689e-30 2.9792377e-37], sum to 1.0000
[2019-04-10 13:17:49,995] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9432
[2019-04-10 13:17:50,000] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 84.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9293513951191131, 6.911199999999999, 6.9112, 168.912956510431, 759067.7852456463, 759067.785245647, 230023.4574259048], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4054200.0000, 
sim time next is 4054800.0000, 
raw observation next is [27.66666666666667, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9263060938442126, 6.9112, 6.9112, 168.912956510431, 756980.8991221943, 756980.8991221943, 229313.3354899045], 
processed observation next is [1.0, 0.9565217391304348, 0.5102685624012641, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9101293827368445, 0.0, 0.0, 0.8294399451523027, 0.2102724719783873, 0.2102724719783873, 0.34225870968642463], 
reward next is 0.6577, 
noisyNet noise sample is [array([1.4820279], dtype=float32), -0.9818138]. 
=============================================
[2019-04-10 13:17:52,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.00000000e+00 2.94242504e-17 1.03200525e-26 1.89661238e-20
 3.21537044e-29], sum to 1.0000
[2019-04-10 13:17:52,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2116
[2019-04-10 13:17:52,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2729126.882301104 W.
[2019-04-10 13:17:52,449] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 79.0, 1.0, 2.0, 0.659690649387246, 1.0, 2.0, 0.6504353642078857, 1.0, 2.0, 1.03, 7.005094554016475, 6.9112, 170.5573041426782, 2729126.882301104, 2661866.393872842, 508899.460538434], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [30.83333333333334, 79.0, 1.0, 2.0, 0.6856675592628466, 1.0, 2.0, 0.6634238191456859, 1.0, 2.0, 1.03, 7.005096602083582, 6.9112, 170.5573041426782, 2783685.161787943, 2716423.206245902, 516841.0369173633], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.79, 1.0, 1.0, 0.6212862159793332, 1.0, 1.0, 0.5944865290911878, 1.0, 1.0, 1.0365853658536586, 0.009389660208358208, 0.0, 0.8375144448122397, 0.7732458782744286, 0.7545620017349728, 0.7714045327124825], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.58707047], dtype=float32), -1.9432523]. 
=============================================
[2019-04-10 13:17:57,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 4.0887412e-23 2.2436390e-35 1.1142366e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:17:57,019] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0523
[2019-04-10 13:17:57,029] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2737263.303410614 W.
[2019-04-10 13:17:57,033] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 51.0, 1.0, 2.0, 0.9785586044625465, 1.0, 2.0, 0.9785586044625465, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2737263.303410614, 2737263.303410614, 516166.4634611598], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4207200.0000, 
sim time next is 4207800.0000, 
raw observation next is [36.0, 50.5, 1.0, 2.0, 0.9614763727946645, 1.0, 2.0, 0.9614763727946645, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2689428.768746078, 2689428.768746078, 506144.1891693415], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.505, 1.0, 1.0, 0.9535859913188729, 1.0, 1.0, 0.9535859913188729, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7470635468739105, 0.7470635468739105, 0.75543908831245], 
reward next is 0.2446, 
noisyNet noise sample is [array([-1.6932172], dtype=float32), 1.4273854]. 
=============================================
[2019-04-10 13:17:59,923] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-10 13:17:59,924] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:17:59,925] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:17:59,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:17:59,926] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:17:59,926] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:17:59,926] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:17:59,927] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:17:59,927] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:17:59,928] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:17:59,927] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:17:59,948] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-04-10 13:17:59,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-04-10 13:17:59,963] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-04-10 13:17:59,996] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-04-10 13:17:59,997] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-04-10 13:18:09,282] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:09,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.23333333333333, 95.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7281431119754903, 6.9112, 6.9112, 168.912956510431, 621183.4742768897, 621183.4742768897, 187876.3558849163]
[2019-04-10 13:18:09,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:18:09,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 3.7461458e-24 9.1988670e-33 1.4541369e-30 0.0000000e+00], sampled 0.05529307563172903
[2019-04-10 13:18:21,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:21,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.1, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8575067038317015, 6.911199999999999, 6.9112, 168.912956510431, 710020.1507385321, 710020.1507385327, 213875.2806098301]
[2019-04-10 13:18:21,629] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:18:21,631] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.5111697e-22 9.8074052e-31 1.4284846e-28 9.4797065e-37], sampled 0.823897983983223
[2019-04-10 13:18:30,914] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:30,915] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.58839391, 88.78940779999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5807752766380688, 6.9112, 6.9112, 168.912956510431, 507546.5778463861, 507546.5778463861, 163166.3074241201]
[2019-04-10 13:18:30,916] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:18:30,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.2224268e-24 6.6229159e-33 1.2475431e-30 0.0000000e+00], sampled 0.2840280700562119
[2019-04-10 13:18:36,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:36,928] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.23333333333333, 72.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.972792091111116, 6.9112, 6.9112, 168.912956510431, 789984.3844087707, 789984.3844087707, 240458.5090462087]
[2019-04-10 13:18:36,930] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:18:36,933] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.2637234e-22 4.0624036e-31 1.5686201e-28 5.4871449e-37], sampled 0.06798986241175453
[2019-04-10 13:18:40,644] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:40,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.23333333333333, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9051195095090615, 6.9112, 6.9112, 168.912956510431, 744825.8087578397, 744825.8087578397, 224531.7990949829]
[2019-04-10 13:18:40,648] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:18:40,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 4.7011604e-23 1.6856631e-31 2.3468759e-29 6.4198438e-38], sampled 0.2712366515658696
[2019-04-10 13:18:55,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:18:55,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.36666666666667, 68.33333333333333, 1.0, 1.0, 0.6304672687964725, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128413725908, 881055.6643397032, 881055.6643397032, 206555.3220984124]
[2019-04-10 13:18:55,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:18:55,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 2.7040192e-23 1.0182606e-32 7.9646090e-31 0.0000000e+00], sampled 0.2326365871999364
[2019-04-10 13:18:55,561] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 881055.6643397032 W.
[2019-04-10 13:19:12,895] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:19:12,897] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.8, 92.0, 1.0, 2.0, 0.7018723219510584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980887.7760285179, 980887.7760285179, 221253.3644597218]
[2019-04-10 13:19:12,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:19:12,900] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.1713109e-14 1.1519690e-21 1.1076655e-19 2.5999398e-26], sampled 0.22805934991002175
[2019-04-10 13:19:12,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 980887.7760285179 W.
[2019-04-10 13:19:15,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14844516], dtype=float32), 0.057178617]
[2019-04-10 13:19:15,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.5, 82.83333333333333, 1.0, 2.0, 0.6982294400595539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975794.3996037547, 975794.3996037547, 220467.8953317531]
[2019-04-10 13:19:15,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:19:15,878] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.4194176e-22 4.6521150e-32 1.7008441e-29 0.0000000e+00], sampled 0.4990550024935049
[2019-04-10 13:19:15,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 975794.3996037547 W.
[2019-04-10 13:19:21,745] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-04-10 13:19:21,770] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5443 3185198689.2667 2464.0000
[2019-04-10 13:19:21,780] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2435 3319594020.0677 2143.0000
[2019-04-10 13:19:21,945] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0822 2937707417.1805 1381.0000
[2019-04-10 13:19:21,998] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8961 2989371039.2274 1566.0000
[2019-04-10 13:19:23,018] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 750000, evaluation results [750000.0, 7288.243513177262, 3319594020.0676656, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8061.082249279071, 2937707417.180471, 1381.0, 7030.544258714058, 3185198689.26675, 2464.0, 7923.896129508693, 2989371039.2273927, 1566.0]
[2019-04-10 13:19:23,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.3652608e-20 9.4953394e-30 9.6563009e-26 9.7585627e-37], sum to 1.0000
[2019-04-10 13:19:23,113] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9479
[2019-04-10 13:19:23,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 3164323.409108892 W.
[2019-04-10 13:19:23,131] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.5, 58.5, 1.0, 2.0, 0.8668698124052916, 1.0, 2.0, 0.7540249457169083, 1.0, 2.0, 1.03, 7.005110893376146, 6.9112, 170.5573041426782, 3164323.409108892, 3097051.216132655, 579218.8883861448], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4275000.0000, 
sim time next is 4275600.0000, 
raw observation next is [36.66666666666666, 58.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.033692485727427, 6.9112, 170.5573041426782, 2997178.310301176, 2909431.960098873, 553023.1314815619], 
processed observation next is [1.0, 0.4782608695652174, 0.9368088467614529, 0.58, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.012249248572742654, 0.0, 0.8375144448122397, 0.8325495306392156, 0.8081755444719091, 0.8254076589277044], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6505686], dtype=float32), -0.47185704]. 
=============================================
[2019-04-10 13:19:53,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 5.6833607e-27 0.0000000e+00 1.8313980e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:19:53,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-10 13:19:53,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1824889.804566741 W.
[2019-04-10 13:19:53,473] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4350961611023625, 1.0, 1.0, 0.4350961611023625, 1.0, 2.0, 0.7515937934099807, 6.9112, 6.9112, 170.5573041426782, 1824889.804566741, 1824889.804566741, 372547.0905782569], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4711800.0000, 
sim time next is 4712400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4153802938354446, 1.0, 2.0, 0.4153802938354446, 1.0, 2.0, 0.7181124250376729, 6.911199999999999, 6.9112, 170.5573041426782, 1742129.862008953, 1742129.862008953, 361089.577382706], 
processed observation next is [1.0, 0.5652173913043478, 0.6682464454976303, 0.66, 1.0, 1.0, 0.29563890823547545, 1.0, 1.0, 0.29563890823547545, 1.0, 1.0, 0.656234664680089, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4839249616691536, 0.4839249616691536, 0.538939667735382], 
reward next is 0.4611, 
noisyNet noise sample is [array([-0.756448], dtype=float32), -1.6051781]. 
=============================================
[2019-04-10 13:20:09,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 5.3687695e-31 0.0000000e+00 2.2056791e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:20:09,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7420
[2019-04-10 13:20:09,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8601750490486575, 6.911199999999999, 6.9112, 168.912956510431, 714707.5848065187, 714707.5848065193, 214546.150666547], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5100600.0000, 
sim time next is 5101200.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8568016392164056, 6.911200000000001, 6.9112, 168.912956510431, 712394.8416375022, 712394.8416375016, 213814.1485409866], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8253678527029336, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19788745601041727, 0.1978874560104171, 0.31912559483729347], 
reward next is 0.6809, 
noisyNet noise sample is [array([0.5028379], dtype=float32), 1.0268382]. 
=============================================
[2019-04-10 13:20:14,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 8.0918176e-25 1.7031891e-35 9.6485172e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:20:14,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2194
[2019-04-10 13:20:14,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1072457.871129557 W.
[2019-04-10 13:20:14,055] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.2557885961660502, 1.0, 1.0, 0.2557885961660502, 1.0, 2.0, 0.4332247433063788, 6.9112, 6.9112, 170.5573041426782, 1072457.871129557, 1072457.871129557, 286554.0169289491], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5203200.0000, 
sim time next is 5203800.0000, 
raw observation next is [26.5, 86.5, 1.0, 2.0, 0.2667738383899438, 1.0, 2.0, 0.2667738383899438, 1.0, 2.0, 0.4524441317816232, 6.9112, 6.9112, 170.5573041426782, 1118540.279613928, 1118540.279613928, 290486.9904095962], 
processed observation next is [1.0, 0.21739130434782608, 0.4549763033175356, 0.865, 1.0, 1.0, 0.11659498601198046, 1.0, 1.0, 0.11659498601198046, 1.0, 1.0, 0.3322489411971014, 0.0, 0.0, 0.8375144448122397, 0.31070563322609107, 0.31070563322609107, 0.4335626722531286], 
reward next is 0.5664, 
noisyNet noise sample is [array([2.1276724], dtype=float32), -0.24068667]. 
=============================================
[2019-04-10 13:20:14,891] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 13:20:14,892] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:20:14,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:20:14,893] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:20:14,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:20:14,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:20:14,896] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:20:14,896] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:20:14,897] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:20:14,897] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:20:14,898] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:20:14,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-04-10 13:20:14,928] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-04-10 13:20:14,929] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-04-10 13:20:14,929] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-04-10 13:20:14,945] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-04-10 13:20:22,822] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15087432], dtype=float32), 0.046302963]
[2019-04-10 13:20:22,822] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.16666666666667, 63.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004198049705221, 7.08275325386464, 6.9112, 168.9121030230327, 1017789.263930026, 896084.0961890002, 248477.1015372465]
[2019-04-10 13:20:22,824] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:20:22,827] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.3166624e-27 2.4202294e-36 6.1679905e-32 0.0000000e+00], sampled 0.7467562303727545
[2019-04-10 13:20:22,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1017789.263930026 W.
[2019-04-10 13:20:46,575] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15087432], dtype=float32), 0.046302963]
[2019-04-10 13:20:46,576] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.5, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.042386698465649, 6.9112, 168.9120090298161, 928959.0704726536, 835891.2335302031, 255214.9149034759]
[2019-04-10 13:20:46,576] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:20:46,579] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5885022895789356
[2019-04-10 13:20:46,580] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 928959.0704726536 W.
[2019-04-10 13:21:02,903] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15087432], dtype=float32), 0.046302963]
[2019-04-10 13:21:02,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.90694570333334, 68.77437166666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8745400532159645, 6.9112, 6.9112, 168.912956510431, 723534.2779739765, 723534.2779739765, 217659.1168080516]
[2019-04-10 13:21:02,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:21:02,907] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 1.26217755e-36 0.00000000e+00 0.00000000e+00
 0.00000000e+00], sampled 0.9003709780564261
[2019-04-10 13:21:07,030] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.15087432], dtype=float32), 0.046302963]
[2019-04-10 13:21:07,030] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.18452122, 76.49228676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9846492531047452, 6.9112, 6.9112, 168.912956510431, 793262.540079367, 793262.540079367, 243105.2084119692]
[2019-04-10 13:21:07,030] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:21:07,033] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7431428278738988
[2019-04-10 13:21:33,231] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15087432], dtype=float32), 0.046302963]
[2019-04-10 13:21:33,233] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 79.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.339754917429541, 6.9112, 168.9103851488996, 1757990.139496304, 1453963.159622293, 311353.9043393582]
[2019-04-10 13:21:33,233] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:21:33,235] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.5115457e-28 0.0000000e+00 4.5836411e-33 0.0000000e+00], sampled 0.7232044393644201
[2019-04-10 13:21:33,235] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1757990.139496304 W.
[2019-04-10 13:21:35,508] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:21:36,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.5628 2937699989.4820 1381.0000
[2019-04-10 13:21:36,251] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2307 3319593021.6301 2143.0000
[2019-04-10 13:21:36,488] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-04-10 13:21:36,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.9536 3105706129.0052 2010.0000
[2019-04-10 13:21:37,516] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 775000, evaluation results [775000.0, 7288.230700372886, 3319593021.630112, 2143.0, 7348.953624334263, 3105706129.005181, 2010.0, 8062.5628493041395, 2937699989.481995, 1381.0, 7030.534901418457, 3185211196.137981, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-04-10 13:21:42,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:21:42,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4350
[2019-04-10 13:21:42,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.65, 59.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.956421474725009, 6.9112, 168.9125172465283, 860895.021282849, 828813.4341393624, 254813.212673637], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5334600.0000, 
sim time next is 5335200.0000, 
raw observation next is [34.4, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.042016258397137, 6.9112, 168.9120190480266, 921642.1693123836, 828837.1283256749, 254813.2350539516], 
processed observation next is [1.0, 0.782608695652174, 0.8293838862559241, 0.61, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.013081625839713684, 0.0, 0.8294353417827249, 0.25601171369788434, 0.2302325356460208, 0.3803182612745546], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.68203616], dtype=float32), -0.4463607]. 
=============================================
[2019-04-10 13:21:43,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.2339532e-24 5.6054830e-34 6.9116010e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:21:43,869] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2120
[2019-04-10 13:21:43,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 897121.4281216526 W.
[2019-04-10 13:21:43,880] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 69.0, 1.0, 1.0, 0.213987131427796, 1.0, 1.0, 0.213987131427796, 1.0, 2.0, 0.3716248806643411, 6.9112, 6.9112, 170.5573041426782, 897121.4281216526, 897121.4281216526, 273850.607627803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5338800.0000, 
sim time next is 5339400.0000, 
raw observation next is [32.75, 70.5, 1.0, 2.0, 0.6227893333410596, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564703005, 870321.625244257, 870321.6252442576, 205069.4550244614], 
processed observation next is [1.0, 0.8260869565217391, 0.7511848341232228, 0.705, 1.0, 1.0, 0.545529317278385, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399449552435, 0.24175600701229363, 0.24175600701229377, 0.30607381346934537], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1442983], dtype=float32), -0.4410304]. 
=============================================
[2019-04-10 13:21:44,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.7353566e-10 1.1759120e-16 2.6476803e-12 9.7129012e-18], sum to 1.0000
[2019-04-10 13:21:44,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-10 13:21:44,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1579644.524165142 W.
[2019-04-10 13:21:44,116] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.05, 88.5, 1.0, 2.0, 0.564996522381033, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9812121121707104, 6.9112, 6.9112, 168.9129565103799, 1579644.524165142, 1579644.524165142, 345673.1300063251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5367000.0000, 
sim time next is 5367600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.756296612468937, 6.9112, 168.9023667230433, 2763488.790487564, 1454595.814837136, 309889.4157691195], 
processed observation next is [1.0, 0.13043478260869565, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.18450966124689366, 0.0, 0.8293879444482034, 0.7676357751354345, 0.4040543930103156, 0.4625215160733127], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6638985], dtype=float32), -2.2443993]. 
=============================================
[2019-04-10 13:21:47,762] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 4.8999887e-16 2.8418528e-23 4.9038307e-20 1.8574200e-26], sum to 1.0000
[2019-04-10 13:21:47,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7862
[2019-04-10 13:21:47,771] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.73333333333333, 81.33333333333334, 1.0, 1.0, 0.3023861090090357, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5251446707343469, 6.911200000000001, 6.9112, 168.912873558327, 845133.4551759368, 845133.4551759361, 222727.3131601041], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5438400.0000, 
sim time next is 5439000.0000, 
raw observation next is [29.66666666666667, 81.16666666666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.971342032053584, 6.9112, 168.9124457168682, 871484.262736124, 828817.5642930008, 254812.1926993316], 
processed observation next is [1.0, 0.9565217391304348, 0.6050552922590839, 0.8116666666666665, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00601420320535837, 0.0, 0.8294374369220671, 0.24207896187114555, 0.23022710119250023, 0.3803167055213904], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4916948], dtype=float32), 0.68841773]. 
=============================================
[2019-04-10 13:21:47,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[12.59808 ]
 [11.99823 ]
 [12.935041]
 [12.464132]
 [13.144897]], R is [[12.01374531]
 [11.89360809]
 [11.77467251]
 [11.65692616]
 [11.54035664]].
[2019-04-10 13:21:48,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.2219166e-22 2.0046525e-34 1.4991242e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:21:48,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7174
[2019-04-10 13:21:48,682] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2271992.072277157 W.
[2019-04-10 13:21:48,688] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.41666666666666, 51.0, 1.0, 2.0, 0.5415849583237532, 1.0, 2.0, 0.5415849583237532, 1.0, 2.0, 0.9405539677257585, 6.9112, 6.9112, 170.5573041426782, 2271992.072277157, 2271992.072277157, 445095.4496214498], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5490600.0000, 
sim time next is 5491200.0000, 
raw observation next is [36.53333333333333, 50.0, 1.0, 2.0, 0.6434639960562625, 1.0, 2.0, 0.6423220375423939, 1.0, 2.0, 1.03, 7.005093274768152, 6.9112, 170.5573041426782, 2695047.891144939, 2627788.319094287, 504065.6890609131], 
processed observation next is [1.0, 0.5652173913043478, 0.9304897314375986, 0.5, 1.0, 1.0, 0.5704385494653764, 1.0, 1.0, 0.5690626958342095, 1.0, 1.0, 1.0365853658536586, 0.009389327476815179, 0.0, 0.8375144448122397, 0.7486244142069276, 0.7299411997484131, 0.7523368493446464], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16405512], dtype=float32), -0.8915628]. 
=============================================
[2019-04-10 13:21:51,462] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 8.8136044e-22 1.3979209e-33 6.3095470e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:21:51,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7388
[2019-04-10 13:21:51,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3613456.364316919 W.
[2019-04-10 13:21:51,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.53333333333333, 50.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.784008767806863, 6.9112, 168.9024338804248, 3613456.364316919, 2284904.136201169, 471168.6987591065], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5491200.0000, 
sim time next is 5491800.0000, 
raw observation next is [36.65, 49.0, 1.0, 2.0, 0.8483868242786401, 1.0, 1.0, 0.7447834516535825, 1.0, 2.0, 1.03, 7.005109435237297, 6.9112, 170.5573041426782, 3125492.242036615, 3058221.093584549, 572293.9868216295], 
processed observation next is [1.0, 0.5652173913043478, 0.9360189573459715, 0.49, 1.0, 1.0, 0.8173335232272773, 1.0, 0.5, 0.6925101827151596, 1.0, 1.0, 1.0365853658536586, 0.009390943523729689, 0.0, 0.8375144448122397, 0.8681922894546152, 0.8495058593290414, 0.8541701295845215], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0342517], dtype=float32), -0.44227904]. 
=============================================
[2019-04-10 13:21:53,652] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.8965087e-30 0.0000000e+00 2.8358887e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:21:53,652] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-10 13:21:53,653] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.7, 79.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.957651604091549, 6.9112, 168.9124612868667, 861768.0448121476, 828813.7747452761, 254811.9148177416], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5517000.0000, 
sim time next is 5517600.0000, 
raw observation next is [29.56666666666667, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.965490322824303, 6.9112, 168.9124009535178, 867331.2414806935, 828815.9446181561, 254811.8558605078], 
processed observation next is [1.0, 0.8695652173913043, 0.6003159557661929, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005429032282430324, 0.0, 0.8294372171135238, 0.24092534485574818, 0.23022665128282113, 0.38031620277687733], 
reward next is 0.3482, 
noisyNet noise sample is [array([0.6462733], dtype=float32), 0.8530423]. 
=============================================
[2019-04-10 13:21:55,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 5.1864662e-24 1.8110819e-32 3.4331004e-29 6.1457798e-37], sum to 1.0000
[2019-04-10 13:21:55,244] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-10 13:21:55,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.7, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9280320977213011, 6.9112, 6.9112, 168.912956510431, 759076.5392937136, 759076.5392937136, 229756.776801685], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5533200.0000, 
sim time next is 5533800.0000, 
raw observation next is [26.63333333333333, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.926363864871843, 6.9112, 6.9112, 168.912956510431, 757951.0718281247, 757951.0718281247, 229368.603155699], 
processed observation next is [1.0, 0.043478260869565216, 0.46129541864139006, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9101998352095645, 0.0, 0.0, 0.8294399451523027, 0.2105419643967013, 0.2105419643967013, 0.34234119873984925], 
reward next is 0.6577, 
noisyNet noise sample is [array([0.15904899], dtype=float32), 0.0017390157]. 
=============================================
[2019-04-10 13:22:01,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 4.0737726e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:01,386] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-10 13:22:01,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.6, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.963907949616932, 6.9112, 6.9112, 168.912956510431, 782320.7138654083, 782320.7138654083, 238217.6442956249], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5605200.0000, 
sim time next is 5605800.0000, 
raw observation next is [27.51666666666667, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9602543480694462, 6.9112, 6.9112, 168.912956510431, 780089.4646153904, 780089.4646153904, 237349.2775687427], 
processed observation next is [1.0, 0.9130434782608695, 0.5031595576619274, 0.8916666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9515296927676172, 0.0, 0.0, 0.8294399451523027, 0.21669151794871955, 0.21669151794871955, 0.3542526530876757], 
reward next is 0.6457, 
noisyNet noise sample is [array([2.1373694], dtype=float32), -0.41530338]. 
=============================================
[2019-04-10 13:22:06,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.7174776e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:06,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-10 13:22:06,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.76666666666667, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9175338273102767, 6.911200000000001, 6.9112, 168.912956510431, 752259.380780187, 752259.3807801863, 227336.3022295547], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5689200.0000, 
sim time next is 5689800.0000, 
raw observation next is [27.7, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9160893218494537, 6.9112, 6.9112, 168.912956510431, 751239.8670519596, 751239.8670519596, 227001.7720687261], 
processed observation next is [0.0, 0.8695652173913043, 0.5118483412322274, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8976699046944557, 0.0, 0.0, 0.8294399451523027, 0.20867774084776655, 0.20867774084776655, 0.3388086150279494], 
reward next is 0.6612, 
noisyNet noise sample is [array([-0.04418749], dtype=float32), 1.2697783]. 
=============================================
[2019-04-10 13:22:07,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.8749014e-31 0.0000000e+00 2.4882695e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:07,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7311
[2019-04-10 13:22:07,843] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.06666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8685576208519183, 6.911200000000001, 6.9112, 168.912956510431, 719288.0111071462, 719288.0111071455, 216338.233822428], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5725200.0000, 
sim time next is 5725800.0000, 
raw observation next is [27.25, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8712888597871639, 6.9112, 6.9112, 168.912956510431, 721195.3291722643, 721195.3291722643, 216938.974568183], 
processed observation next is [0.0, 0.2608695652173913, 0.490521327014218, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8430351948623949, 0.0, 0.0, 0.8294399451523027, 0.20033203588118453, 0.20033203588118453, 0.32378951428087016], 
reward next is 0.6762, 
noisyNet noise sample is [array([1.6644176], dtype=float32), 0.22931291]. 
=============================================
[2019-04-10 13:22:11,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.6164428e-28 5.6886458e-37 1.1490402e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:11,105] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5395
[2019-04-10 13:22:11,133] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.9, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9143921824454323, 6.9112, 6.9112, 168.912956510431, 750684.140932508, 750684.140932508, 226636.3523774522], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5792400.0000, 
sim time next is 5793000.0000, 
raw observation next is [26.88333333333333, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9136352701208419, 6.9112, 6.9112, 168.912956510431, 750206.0448565994, 750206.0448565994, 226463.8439294353], 
processed observation next is [1.0, 0.043478260869565216, 0.47314375987361756, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8946771586839536, 0.0, 0.0, 0.8294399451523027, 0.20839056801572206, 0.20839056801572206, 0.3380057372081124], 
reward next is 0.6620, 
noisyNet noise sample is [array([1.3391851], dtype=float32), 1.0189036]. 
=============================================
[2019-04-10 13:22:11,147] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.08757 ]
 [60.338394]
 [60.71792 ]
 [61.070198]
 [61.62173 ]], R is [[59.66420746]
 [59.72930145]
 [59.79328156]
 [59.85620499]
 [59.91825867]].
[2019-04-10 13:22:11,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 6.3376888e-33 0.0000000e+00 4.0172517e-38 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:11,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6304
[2019-04-10 13:22:11,570] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.5, 66.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9693647967179332, 6.9112, 6.9112, 168.912956510431, 784796.3819184309, 784796.3819184309, 239474.6512510632], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5764800.0000, 
sim time next is 5765400.0000, 
raw observation next is [31.3, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9655962004460013, 6.911200000000001, 6.9112, 168.912956510431, 782136.294792859, 782136.2947928584, 238556.309298288], 
processed observation next is [0.0, 0.7391304347826086, 0.6824644549763034, 0.675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9580441468853674, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21726008188690527, 0.21726008188690513, 0.3560541929825194], 
reward next is 0.6439, 
noisyNet noise sample is [array([0.48127863], dtype=float32), -0.5375367]. 
=============================================
[2019-04-10 13:22:11,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 7.3939547e-32 0.0000000e+00 3.7838629e-38 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:11,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-10 13:22:11,887] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.35, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.921608492346384, 6.911200000000001, 6.9112, 168.912956510431, 755434.8485502922, 755434.8485502916, 228295.6571639823], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5783400.0000, 
sim time next is 5784000.0000, 
raw observation next is [27.3, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9197641190571993, 6.911200000000001, 6.9112, 168.912956510431, 754212.5270855913, 754212.5270855908, 227870.0618582219], 
processed observation next is [0.0, 0.9565217391304348, 0.4928909952606636, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9021513647039016, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2095034797459976, 0.20950347974599742, 0.3401045699376446], 
reward next is 0.6599, 
noisyNet noise sample is [array([0.0017051], dtype=float32), -0.1309592]. 
=============================================
[2019-04-10 13:22:11,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.599476]
 [62.645504]
 [62.700893]
 [62.742638]
 [62.738026]], R is [[62.5925827 ]
 [62.62591553]
 [62.65823364]
 [62.68899155]
 [62.71881866]].
[2019-04-10 13:22:14,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.3975714e-15 5.4121554e-24 1.5111835e-19 1.9356302e-26], sum to 1.0000
[2019-04-10 13:22:14,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5747
[2019-04-10 13:22:14,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2379183.695814688 W.
[2019-04-10 13:22:15,034] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.8, 75.33333333333333, 1.0, 2.0, 0.8506686111417712, 1.0, 2.0, 0.8506686111417712, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2379183.695814688, 2379183.695814688, 445292.319229394], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5820000.0000, 
sim time next is 5820600.0000, 
raw observation next is [29.95, 74.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.728408376953415, 6.9112, 168.907420871492, 2867408.382396368, 2287671.633480095, 474418.8320335218], 
processed observation next is [1.0, 0.34782608695652173, 0.6184834123222749, 0.7466666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.08172083769534151, 0.0, 0.8294127626308857, 0.7965023284434355, 0.6354643426333597, 0.7080878090052564], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7040114], dtype=float32), 0.463958]. 
=============================================
[2019-04-10 13:22:16,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 8.8689746e-24 8.3220780e-37 8.8702625e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:16,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3500
[2019-04-10 13:22:16,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2013742.953682844 W.
[2019-04-10 13:22:16,301] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.53333333333333, 63.33333333333333, 1.0, 2.0, 0.4800808674633352, 1.0, 2.0, 0.4800808674633352, 1.0, 1.0, 0.833741701614869, 6.9112, 6.9112, 170.5573041426782, 2013742.953682844, 2013742.953682844, 401645.0211966502], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5835000.0000, 
sim time next is 5835600.0000, 
raw observation next is [32.6, 63.0, 1.0, 2.0, 0.7817825012464509, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005980347116656, 6.9112, 168.9123932059834, 1989577.633772132, 1922337.417678585, 403375.5499501511], 
processed observation next is [1.0, 0.5652173913043478, 0.7440758293838864, 0.63, 1.0, 1.0, 0.7370873508993384, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00947803471166564, 0.0, 0.829437179069583, 0.5526604538255923, 0.5339826160218292, 0.6020530596270912], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39724022], dtype=float32), 0.3234155]. 
=============================================
[2019-04-10 13:22:31,787] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.0510806e-26 3.1306477e-36 9.6105982e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:22:31,791] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9915
[2019-04-10 13:22:31,794] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.2, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8949855613296432, 6.9112, 6.9112, 168.912956510431, 736296.1956722644, 736296.1956722644, 222171.6070202073], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6123600.0000, 
sim time next is 6124200.0000, 
raw observation next is [27.21666666666667, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8972753401290819, 6.911199999999999, 6.9112, 168.912956510431, 737988.600777555, 737988.6007775556, 222693.1773171126], 
processed observation next is [1.0, 0.9130434782608695, 0.48894154818325447, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8747260245476608, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20499683354932083, 0.204996833549321, 0.33237787659270535], 
reward next is 0.6676, 
noisyNet noise sample is [array([-0.7761173], dtype=float32), 1.6041914]. 
=============================================
[2019-04-10 13:22:33,399] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.00000000e+00 4.47304101e-25 8.45701088e-35 1.20901795e-29
 4.54686684e-38], sum to 1.0000
[2019-04-10 13:22:33,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5212
[2019-04-10 13:22:33,408] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.28333333333333, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9161808029251907, 6.911200000000001, 6.9112, 168.912956510431, 751806.918324652, 751806.9183246514, 227044.2446153227], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6126600.0000, 
sim time next is 6127200.0000, 
raw observation next is [27.3, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.914192319538559, 6.9112, 6.9112, 168.912956510431, 749713.0219583681, 749713.0219583681, 226555.3328829794], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8953564872421449, 0.0, 0.0, 0.8294399451523027, 0.20825361721065783, 0.20825361721065783, 0.33814228788504386], 
reward next is 0.6619, 
noisyNet noise sample is [array([0.2591912], dtype=float32), -2.7826295]. 
=============================================
[2019-04-10 13:22:34,321] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-10 13:22:34,322] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:22:34,323] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:22:34,323] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:22:34,324] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:22:34,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:22:34,327] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:22:34,328] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:22:34,328] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:22:34,329] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:22:34,331] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:22:34,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-04-10 13:22:34,365] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-04-10 13:22:34,366] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-04-10 13:22:34,405] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-04-10 13:22:34,424] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-04-10 13:22:43,236] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:22:43,237] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.86666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5297596909406661, 6.9112, 6.9112, 168.912956510431, 466458.0325844144, 466458.0325844144, 155927.6002240407]
[2019-04-10 13:22:43,238] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:22:43,240] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.3099843e-31 0.0000000e+00 6.3838261e-37 0.0000000e+00], sampled 0.5242029253505485
[2019-04-10 13:22:50,292] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:22:50,293] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.38694765, 87.3136055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5924118766532985, 6.911200000000001, 6.9112, 168.912956510431, 519356.4717273489, 519356.4717273483, 164850.3042807263]
[2019-04-10 13:22:50,293] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:22:50,295] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.4680222e-31 0.0000000e+00 5.4584834e-36 0.0000000e+00], sampled 0.9301224514607725
[2019-04-10 13:23:26,549] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:23:26,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.85, 79.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.945466982537553, 6.9112, 168.9125883371947, 853120.5256239991, 828810.4018213201, 254811.9955713678]
[2019-04-10 13:23:26,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:23:26,552] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.5798787e-20 2.8519344e-28 9.2794426e-24 1.8300711e-30], sampled 0.03373436955930231
[2019-04-10 13:23:36,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:23:36,216] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.03333333333333, 94.33333333333334, 1.0, 1.0, 0.6291317325560546, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128699118628, 879188.5272310189, 879188.5272310196, 206291.6454353286]
[2019-04-10 13:23:36,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:23:36,220] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.6265918e-22 4.4172064e-31 6.3604776e-26 8.8878283e-34], sampled 0.7742489471939406
[2019-04-10 13:23:36,221] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 879188.5272310189 W.
[2019-04-10 13:23:41,498] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:23:41,499] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9689397414165353, 6.9112, 6.9112, 168.912956510431, 803177.384302047, 803177.384302047, 240209.1505302045]
[2019-04-10 13:23:41,501] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:23:41,502] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 9.7089001e-33 0.0000000e+00 1.2392241e-37 0.0000000e+00], sampled 0.7301351061069842
[2019-04-10 13:23:49,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14800456], dtype=float32), 0.0436761]
[2019-04-10 13:23:49,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 58.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.426989870012385, 6.9112, 168.9101972634822, 1819918.660433827, 1454005.551934577, 311357.6864750476]
[2019-04-10 13:23:49,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:23:49,525] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.8931518e-29 0.0000000e+00 7.7234699e-34 0.0000000e+00], sampled 0.13169382974860044
[2019-04-10 13:23:49,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1819918.660433827 W.
[2019-04-10 13:23:55,201] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2435 3319589399.8756 2143.0000
[2019-04-10 13:23:55,742] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8963 2989371222.9206 1566.0000
[2019-04-10 13:23:55,754] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.2610 3185088381.1007 2464.0000
[2019-04-10 13:23:55,803] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-04-10 13:23:55,895] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-04-10 13:23:56,910] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 800000, evaluation results [800000.0, 7288.243514976095, 3319589399.87561, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7031.260974738479, 3185088381.1007032, 2464.0, 7923.896304398911, 2989371222.920572, 1566.0]
[2019-04-10 13:23:57,844] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.6842577e-21 3.7714813e-32 5.7582378e-26 1.7910363e-37], sum to 1.0000
[2019-04-10 13:23:57,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3489
[2019-04-10 13:23:57,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 951749.7373764407 W.
[2019-04-10 13:23:57,859] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.15, 89.5, 1.0, 2.0, 0.6810319940139475, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 951749.7373764407, 951749.7373764407, 216805.061911065], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6157800.0000, 
sim time next is 6158400.0000, 
raw observation next is [27.23333333333333, 89.33333333333333, 1.0, 2.0, 0.3418744796532316, 1.0, 1.0, 0.3418744796532316, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 955544.2901312156, 955544.2901312156, 258480.4745668344], 
processed observation next is [1.0, 0.2608695652173913, 0.4897314375987361, 0.8933333333333333, 1.0, 1.0, 0.20707768632919468, 1.0, 0.5, 0.20707768632919468, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.26542896948089323, 0.26542896948089323, 0.38579175308482744], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4037472], dtype=float32), 0.17205228]. 
=============================================
[2019-04-10 13:23:58,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.3142844e-24 3.3504683e-35 1.1477446e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:23:58,417] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6089
[2019-04-10 13:23:58,425] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2283099.467735519 W.
[2019-04-10 13:23:58,430] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.40000000000001, 74.0, 1.0, 2.0, 0.5442302617831516, 1.0, 1.0, 0.5442302617831516, 1.0, 2.0, 0.9403417860808917, 6.911200000000001, 6.9112, 170.5573041426782, 2283099.467735519, 2283099.467735518, 446126.0499142748], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6193200.0000, 
sim time next is 6193800.0000, 
raw observation next is [29.3, 74.5, 1.0, 2.0, 1.001981133512252, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990977396570975, 6.9112, 168.9124818390243, 2297774.988083628, 2241178.316275898, 464475.1464863653], 
processed observation next is [1.0, 0.6956521739130435, 0.5876777251184835, 0.745, 1.0, 1.0, 1.0023869078460867, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007977739657097516, 0.0, 0.8294376142983904, 0.63827083002323, 0.6225495322988606, 0.6932464872930826], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2421265], dtype=float32), -0.09341552]. 
=============================================
[2019-04-10 13:24:00,715] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.0987581e-31 0.0000000e+00 1.6140611e-35 0.0000000e+00], sum to 1.0000
[2019-04-10 13:24:00,721] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3290
[2019-04-10 13:24:00,727] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333334, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8849702354388745, 6.911200000000001, 6.9112, 168.912956510431, 729058.7304384433, 729058.7304384427, 219912.5381884481], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6216600.0000, 
sim time next is 6217200.0000, 
raw observation next is [26.8, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8839805834300477, 6.911199999999999, 6.9112, 168.912956510431, 728345.3659425989, 728345.3659425996, 219690.7558090727], 
processed observation next is [1.0, 1.0, 0.4691943127962086, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8585129066220093, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20231815720627747, 0.20231815720627766, 0.3278966504613025], 
reward next is 0.6721, 
noisyNet noise sample is [array([0.35277796], dtype=float32), 0.7741019]. 
=============================================
[2019-04-10 13:24:03,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 4.856596e-37 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:24:03,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-10 13:24:03,488] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8965749596026933, 6.911199999999999, 6.9112, 168.912956510431, 738100.1267763721, 738100.1267763727, 222558.2541824558], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6304800.0000, 
sim time next is 6305400.0000, 
raw observation next is [27.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8957713269698361, 6.911200000000001, 6.9112, 168.912956510431, 737455.4527830102, 737455.4527830096, 222373.1684001036], 
processed observation next is [0.0, 1.0, 0.4928909952606636, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8728918621583366, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2048487368841695, 0.20484873688416935, 0.3319002513434382], 
reward next is 0.6681, 
noisyNet noise sample is [array([-0.60021406], dtype=float32), -0.8489195]. 
=============================================
[2019-04-10 13:24:14,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:24:14,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6679
[2019-04-10 13:24:14,559] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.95, 68.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8102875628446036, 6.9112, 6.9112, 168.912956510431, 676499.9847174002, 676499.9847174002, 203902.1259790778], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6546600.0000, 
sim time next is 6547200.0000, 
raw observation next is [28.83333333333334, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8075480656545319, 6.911199999999999, 6.9112, 168.912956510431, 674444.7068110128, 674444.7068110134, 203337.5622626776], 
processed observation next is [1.0, 0.782608695652174, 0.5655608214849924, 0.69, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7653025190908924, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.187345751891948, 0.18734575189194816, 0.3034888988995188], 
reward next is 0.6965, 
noisyNet noise sample is [array([0.38623527], dtype=float32), 0.09663634]. 
=============================================
[2019-04-10 13:24:21,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.2231326e-24 3.0647261e-35 7.4000325e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:24:21,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7337
[2019-04-10 13:24:21,189] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.36666666666667, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8298046780017991, 6.9112, 6.9112, 168.912956510431, 691455.1192242103, 691455.1192242103, 207989.0563729081], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6657600.0000, 
sim time next is 6658200.0000, 
raw observation next is [25.3, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8300642200246461, 6.9112, 6.9112, 168.912956510431, 691884.3983671967, 691884.3983671967, 208050.497805108], 
processed observation next is [1.0, 0.043478260869565216, 0.39810426540284366, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7927612439324951, 0.0, 0.0, 0.8294399451523027, 0.19219011065755462, 0.19219011065755462, 0.3105231310524], 
reward next is 0.6895, 
noisyNet noise sample is [array([-0.39375985], dtype=float32), -0.2970725]. 
=============================================
[2019-04-10 13:24:26,519] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.4567937e-30 0.0000000e+00 1.8414662e-36 0.0000000e+00], sum to 1.0000
[2019-04-10 13:24:26,519] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-10 13:24:26,519] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2039318.980211928 W.
[2019-04-10 13:24:26,522] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.0933741e-30 0.0000000e+00 2.8031518e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:24:26,522] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7402
[2019-04-10 13:24:26,527] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.8, 65.0, 1.0, 2.0, 0.8173237722817984, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.97082897847609, 6.9112, 168.9126010271733, 2039318.980211928, 1997016.21963777, 413340.742820462], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6710400.0000, 
sim time next is 6711000.0000, 
raw observation next is [29.76666666666667, 65.33333333333334, 1.0, 2.0, 0.4391340492874411, 1.0, 1.0, 0.4391340492874411, 1.0, 2.0, 0.7454018032225543, 6.9112, 6.9112, 170.5573041426782, 1841840.159254832, 1841840.159254832, 372881.9046529568], 
processed observation next is [1.0, 0.6956521739130435, 0.6097946287519749, 0.6533333333333334, 1.0, 1.0, 0.3242578907077604, 1.0, 0.5, 0.3242578907077604, 1.0, 1.0, 0.6895143941738466, 0.0, 0.0, 0.8375144448122397, 0.5116222664596756, 0.5116222664596756, 0.556540156198443], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4825189], dtype=float32), 2.1668115]. 
=============================================
[2019-04-10 13:24:26,534] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333333, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5979699405094321, 6.911199999999999, 6.9112, 168.912956510431, 521575.4064605988, 521575.4064605994, 165753.4187776241], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6742200.0000, 
sim time next is 6742800.0000, 
raw observation next is [23.3, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5943068605694317, 6.911199999999999, 6.9112, 168.912956510431, 518690.2853947809, 518690.2853947815, 165193.5832464868], 
processed observation next is [1.0, 0.043478260869565216, 0.3033175355450238, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5052522689871118, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14408063483188358, 0.14408063483188374, 0.24655758693505495], 
reward next is 0.7534, 
noisyNet noise sample is [array([-1.3368138], dtype=float32), 1.544524]. 
=============================================
[2019-04-10 13:24:26,573] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[45.462475]
 [45.008945]
 [44.770954]
 [44.941456]
 [44.570812]], R is [[45.58095932]
 [45.21007919]
 [45.15949249]
 [45.13351822]
 [45.10958481]].
[2019-04-10 13:24:46,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:24:46,101] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4597
[2019-04-10 13:24:46,153] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.45, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7562278889395421, 6.9112, 6.9112, 168.912956510431, 640099.13477655, 640099.13477655, 193185.290702272], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6996600.0000, 
sim time next is 6997200.0000, 
raw observation next is [26.4, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7580865398746621, 6.911200000000001, 6.9112, 168.912956510431, 641649.3312446676, 641649.3312446671, 193546.7912840645], 
processed observation next is [0.0, 1.0, 0.45023696682464454, 0.7733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7049835852130026, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.178235925345741, 0.17823592534574084, 0.28887580788666345], 
reward next is 0.7111, 
noisyNet noise sample is [array([-2.0086849], dtype=float32), -0.21120848]. 
=============================================
[2019-04-10 13:24:46,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:24:46,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7971
[2019-04-10 13:24:46,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.46666666666667, 57.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6746275682169073, 6.9112, 6.9112, 168.912956510431, 580746.5785393581, 580746.5785393581, 178262.4602853753], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6985200.0000, 
sim time next is 6985800.0000, 
raw observation next is [28.35, 58.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6800420111339242, 6.9112, 6.9112, 168.912956510431, 584786.8259990885, 584786.8259990885, 179202.6578028886], 
processed observation next is [0.0, 0.8695652173913043, 0.5426540284360191, 0.585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6098073306511269, 0.0, 0.0, 0.8294399451523027, 0.1624407849997468, 0.1624407849997468, 0.26746665343714715], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.10027124], dtype=float32), -0.7077002]. 
=============================================
[2019-04-10 13:24:51,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.1181980e-20 2.1285940e-32 7.4859064e-25 3.6805433e-34], sum to 1.0000
[2019-04-10 13:24:51,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0287
[2019-04-10 13:24:51,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1088432.679396078 W.
[2019-04-10 13:24:51,295] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 73.0, 1.0, 2.0, 0.2595967652239002, 1.0, 2.0, 0.2595967652239002, 1.0, 2.0, 0.4323135999886724, 6.9112, 6.9112, 170.5573041426782, 1088432.679396078, 1088432.679396078, 287225.1810889223], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7024800.0000, 
sim time next is 7025400.0000, 
raw observation next is [27.25, 72.0, 1.0, 2.0, 0.7620264302552291, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104208, 1083668.716935538, 1083668.716935537, 237375.1280425258], 
processed observation next is [1.0, 0.30434782608695654, 0.490521327014218, 0.72, 1.0, 1.0, 0.7132848557291916, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451522526, 0.30101908803764943, 0.3010190880376492, 0.35429123588436684], 
reward next is 0.6457, 
noisyNet noise sample is [array([-0.904455], dtype=float32), 1.170742]. 
=============================================
[2019-04-10 13:24:52,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.7818960e-24 1.3586572e-35 7.3534328e-29 0.0000000e+00], sum to 1.0000
[2019-04-10 13:24:52,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3745
[2019-04-10 13:24:52,630] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2161267.932732284 W.
[2019-04-10 13:24:52,636] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 53.5, 1.0, 2.0, 0.8858747946677489, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.947304713326085, 6.9112, 168.9127121030305, 2161267.932732284, 2135654.043532315, 435563.1970079722], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7053000.0000, 
sim time next is 7053600.0000, 
raw observation next is [30.5, 55.0, 1.0, 2.0, 0.4716760823904344, 1.0, 1.0, 0.4716760823904344, 1.0, 2.0, 0.7883461194376161, 6.911199999999999, 6.9112, 170.5573041426782, 1978455.763456544, 1978455.763456545, 390735.5959263042], 
processed observation next is [1.0, 0.6521739130434783, 0.6445497630331753, 0.55, 1.0, 1.0, 0.3634651595065475, 1.0, 0.5, 0.3634651595065475, 1.0, 1.0, 0.7418855115092878, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5495710454045956, 0.5495710454045959, 0.5831874566064241], 
reward next is 0.4168, 
noisyNet noise sample is [array([1.5616093], dtype=float32), -0.44505206]. 
=============================================
[2019-04-10 13:24:53,723] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 13:24:53,724] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:24:53,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:24:53,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:24:53,725] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:24:53,725] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:24:53,726] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:24:53,727] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:24:53,727] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:24:53,729] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:24:53,731] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:24:53,741] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-04-10 13:24:53,742] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-04-10 13:24:53,768] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-04-10 13:24:53,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-04-10 13:24:53,802] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-04-10 13:25:00,075] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:00,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.31721108, 79.54767185666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3445942911341126, 6.911200000000001, 6.9112, 168.912956510431, 312415.2574823718, 312415.2574823712, 115199.5590077736]
[2019-04-10 13:25:00,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:25:00,079] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.6140507e-34 0.0000000e+00 2.6125203e-38 0.0000000e+00], sampled 0.3713643186943354
[2019-04-10 13:25:05,674] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:05,675] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.76666666666667, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.548683644750011, 6.911200000000001, 6.9112, 168.912956510431, 482757.1376222793, 482757.1376222787, 158501.6792727783]
[2019-04-10 13:25:05,675] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:25:05,677] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 7.9165599e-33 0.0000000e+00 5.3958547e-37 0.0000000e+00], sampled 0.5443868990083762
[2019-04-10 13:25:06,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:06,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.0, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5425830659359578, 6.911200000000001, 6.9112, 168.912956510431, 478630.0161734464, 478630.0161734458, 157620.590415271]
[2019-04-10 13:25:06,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:25:06,372] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.7803192e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.0017080260713335793
[2019-04-10 13:25:09,949] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:09,949] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.45, 47.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5366937424875577, 6.911199999999999, 6.9112, 168.912956510431, 473848.9353130712, 473848.9353130719, 156807.0344851788]
[2019-04-10 13:25:09,951] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:25:09,953] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.5120756e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.3128507460774307
[2019-04-10 13:25:11,681] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:11,683] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 71.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8089331897624815, 6.9112, 6.9112, 168.912956510431, 682608.8810793108, 682608.8810793108, 203775.0997633647]
[2019-04-10 13:25:11,684] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:25:11,687] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.5043595e-33 0.0000000e+00 3.6562307e-37 0.0000000e+00], sampled 0.23188054620620446
[2019-04-10 13:25:15,541] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:15,543] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.25, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8143789418957247, 6.911199999999999, 6.9112, 168.912956510431, 683352.8397871889, 683352.8397871896, 204840.4748082918]
[2019-04-10 13:25:15,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:25:15,546] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 9.2501322e-34 0.0000000e+00 4.3233516e-38 0.0000000e+00], sampled 0.8503081599209233
[2019-04-10 13:25:18,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:18,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.97524189, 70.82189620666666, 1.0, 2.0, 0.6290872799373776, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.980512578672231, 6.9112, 168.9121262921526, 1758981.054043626, 1709808.566865888, 370120.8829210664]
[2019-04-10 13:25:18,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:25:18,277] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.7163788e-19 3.4281800e-29 1.3978389e-21 8.7831981e-29], sampled 0.2577452427002421
[2019-04-10 13:25:18,277] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1758981.054043626 W.
[2019-04-10 13:25:40,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:40,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9258759891511741, 6.9112, 6.9112, 168.912956510431, 754670.1993389437, 754670.1993389437, 229120.1620971024]
[2019-04-10 13:25:40,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:25:40,752] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.4205580e-34 0.0000000e+00 2.4617182e-38 0.0000000e+00], sampled 0.16491618386013773
[2019-04-10 13:25:42,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14591074], dtype=float32), 0.040827144]
[2019-04-10 13:25:42,526] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.35, 85.00000000000001, 1.0, 2.0, 0.6441603522030828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 900199.3723800521, 900199.3723800527, 209261.5875289467]
[2019-04-10 13:25:42,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:25:42,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.1325423e-32 0.0000000e+00 3.0443785e-37 0.0000000e+00], sampled 0.032517557862023816
[2019-04-10 13:25:42,531] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 900199.3723800521 W.
[2019-04-10 13:26:13,844] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.3625 2937768147.0469 1381.0000
[2019-04-10 13:26:14,315] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.3118 3105651465.6512 2010.0000
[2019-04-10 13:26:14,351] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4826 3319666567.8661 2143.0000
[2019-04-10 13:26:14,520] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.2588 2989377746.6630 1566.0000
[2019-04-10 13:26:14,699] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.9992 3185078101.3840 2464.0000
[2019-04-10 13:26:15,712] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 825000, evaluation results [825000.0, 7287.4826105148595, 3319666567.8661056, 2143.0, 7348.311810265577, 3105651465.6511517, 2010.0, 8060.362462993507, 2937768147.046921, 1381.0, 7031.999221275206, 3185078101.383992, 2464.0, 7925.2587706716795, 2989377746.662952, 1566.0]
[2019-04-10 13:26:16,910] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:26:16,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4194
[2019-04-10 13:26:16,920] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7973477477493549, 6.911200000000001, 6.9112, 168.912956510431, 667400.635851529, 667400.6358515283, 201267.0637869861], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7149600.0000, 
sim time next is 7150200.0000, 
raw observation next is [26.1, 84.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7967094649346607, 6.911200000000001, 6.9112, 168.912956510431, 667089.4214282708, 667089.4214282702, 201141.3625457892], 
processed observation next is [1.0, 0.782608695652174, 0.4360189573459717, 0.8483333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7520847133349521, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18530261706340853, 0.1853026170634084, 0.30021098887431225], 
reward next is 0.6998, 
noisyNet noise sample is [array([0.00843799], dtype=float32), -0.061529357]. 
=============================================
[2019-04-10 13:26:18,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.6183415e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:18,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1270
[2019-04-10 13:26:18,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1654747.034333455 W.
[2019-04-10 13:26:18,403] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.33333333333334, 83.33333333333334, 1.0, 2.0, 0.5918377539835668, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9901119114863886, 6.9112, 6.9112, 168.912956510431, 1654747.034333455, 1654747.034333455, 354008.5291804129], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7140000.0000, 
sim time next is 7140600.0000, 
raw observation next is [26.3, 83.5, 1.0, 2.0, 0.5651099199971386, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9452415476342649, 6.911199999999999, 6.9112, 168.912956510431, 1579961.802660733, 1579961.802660734, 337982.4222605206], 
processed observation next is [1.0, 0.6521739130434783, 0.4454976303317536, 0.835, 1.0, 1.0, 0.47603604818932355, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9332213995539815, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4388782785168703, 0.43887827851687056, 0.5044513765082397], 
reward next is 0.4955, 
noisyNet noise sample is [array([0.31613097], dtype=float32), -1.7560933]. 
=============================================
[2019-04-10 13:26:18,956] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 9.097299e-37 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:26:18,963] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9447
[2019-04-10 13:26:18,965] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.7, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7856821493962802, 6.911200000000001, 6.9112, 168.912956510431, 660854.158530974, 660854.1585309732, 198960.0806449133], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7169400.0000, 
sim time next is 7170000.0000, 
raw observation next is [25.7, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7863855220278685, 6.9112, 6.9112, 168.912956510431, 661242.8875761909, 661242.8875761909, 199098.4467671564], 
processed observation next is [1.0, 1.0, 0.4170616113744076, 0.8566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7394945390583761, 0.0, 0.0, 0.8294399451523027, 0.18367857988227523, 0.18367857988227523, 0.2971618608465021], 
reward next is 0.7028, 
noisyNet noise sample is [array([-0.767026], dtype=float32), -0.7946397]. 
=============================================
[2019-04-10 13:26:18,977] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[66.938354]
 [66.97585 ]
 [67.010605]
 [67.02894 ]
 [66.97921 ]], R is [[66.95333099]
 [66.98683929]
 [67.0200882 ]
 [67.05302429]
 [67.08577728]].
[2019-04-10 13:26:21,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.2446493e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:21,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9482
[2019-04-10 13:26:21,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1522654.688794156 W.
[2019-04-10 13:26:21,539] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.5204813127672481, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8937218284558744, 6.9112, 6.9112, 168.9128256785326, 1522654.688794156, 1522654.688794156, 322411.1750774824], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7225200.0000, 
sim time next is 7225800.0000, 
raw observation next is [24.03333333333333, 88.5, 1.0, 2.0, 0.431547508097203, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7419010372705176, 6.9112, 6.9112, 168.9129564777619, 1264559.569873334, 1264559.569873334, 276681.4136457958], 
processed observation next is [1.0, 0.6521739130434783, 0.3380726698262243, 0.885, 1.0, 1.0, 0.31511747963518427, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6852451674030702, 0.0, 0.0, 0.8294399449918824, 0.3512665471870372, 0.3512665471870372, 0.4129573337996952], 
reward next is 0.5870, 
noisyNet noise sample is [array([-0.29930234], dtype=float32), -0.95371586]. 
=============================================
[2019-04-10 13:26:22,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:26:22,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1870
[2019-04-10 13:26:22,455] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.7, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.661975763854901, 6.9112, 6.9112, 168.912956510431, 570926.7520654096, 570926.7520654096, 176096.1347101835], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7236600.0000, 
sim time next is 7237200.0000, 
raw observation next is [23.6, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6591102408386745, 6.911199999999999, 6.9112, 168.912956510431, 568530.9732790663, 568530.9732790668, 175612.4155769394], 
processed observation next is [1.0, 0.782608695652174, 0.3175355450236968, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5842807815105787, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1579252703552962, 0.15792527035529635, 0.26210808295065585], 
reward next is 0.7379, 
noisyNet noise sample is [array([0.5108806], dtype=float32), 0.8764943]. 
=============================================
[2019-04-10 13:26:22,805] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 4.909652e-35 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:26:22,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3604
[2019-04-10 13:26:22,813] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 81.5, 1.0, 2.0, 0.2352093413258358, 1.0, 1.0, 0.2352093413258358, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 701836.6832874912, 701836.6832874906, 245206.0177403184], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7233000.0000, 
sim time next is 7233600.0000, 
raw observation next is [24.2, 82.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6521488181117409, 6.911200000000001, 6.9112, 168.912956510431, 559998.9494482015, 559998.9494482008, 174458.4636946995], 
processed observation next is [1.0, 0.7391304347826086, 0.3459715639810427, 0.82, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.575791241599684, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15555526373561152, 0.15555526373561132, 0.2603857667085067], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.236522], dtype=float32), -1.1171232]. 
=============================================
[2019-04-10 13:26:25,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.4859729e-29 0.0000000e+00 1.6391249e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:25,037] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2035
[2019-04-10 13:26:25,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 962043.3203813718 W.
[2019-04-10 13:26:25,049] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.91666666666666, 77.0, 1.0, 2.0, 0.6215648112769274, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 962043.3203813718, 962043.3203813725, 216142.7761432228], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7290600.0000, 
sim time next is 7291200.0000, 
raw observation next is [24.13333333333333, 76.0, 1.0, 2.0, 0.3297986447922498, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5875646156497324, 6.911200000000001, 6.9112, 168.912956510431, 1020580.789435076, 1020580.789435075, 240604.8149991954], 
processed observation next is [1.0, 0.391304347826087, 0.3428120063191152, 0.76, 1.0, 1.0, 0.19252848770150577, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.49703001908503947, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2834946637319655, 0.2834946637319653, 0.3591116641779036], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6828017], dtype=float32), -0.12538947]. 
=============================================
[2019-04-10 13:26:26,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:26:26,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6572
[2019-04-10 13:26:26,370] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.15, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6399455800425301, 6.911199999999999, 6.9112, 168.912956510431, 548563.5966493398, 548563.5966493404, 172449.2516678293], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7320600.0000, 
sim time next is 7321200.0000, 
raw observation next is [27.06666666666666, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6514458247116401, 6.9112, 6.9112, 168.912956510431, 558884.8203863238, 558884.8203863238, 174342.4758832775], 
processed observation next is [1.0, 0.7391304347826086, 0.4818325434439175, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5749339325751709, 0.0, 0.0, 0.8294399451523027, 0.15524578344064552, 0.15524578344064552, 0.260212650572056], 
reward next is 0.7398, 
noisyNet noise sample is [array([-1.6238885], dtype=float32), 1.0214598]. 
=============================================
[2019-04-10 13:26:27,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 6.888475e-26 3.817270e-36 2.810141e-28 0.000000e+00], sum to 1.0000
[2019-04-10 13:26:27,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1639
[2019-04-10 13:26:27,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1103318.787937988 W.
[2019-04-10 13:26:28,000] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 93.0, 1.0, 2.0, 0.6995273293891078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1103318.787937988, 1103318.787937988, 235908.9922133402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7383000.0000, 
sim time next is 7383600.0000, 
raw observation next is [21.3, 93.0, 1.0, 2.0, 0.7074703751776842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114112.989064285, 1114112.989064286, 237692.1747551566], 
processed observation next is [1.0, 0.4782608695652174, 0.2085308056872039, 0.93, 1.0, 1.0, 0.6475546688887761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30947583029563475, 0.309475830295635, 0.35476443993306955], 
reward next is 0.6452, 
noisyNet noise sample is [array([-0.7028151], dtype=float32), -0.71518207]. 
=============================================
[2019-04-10 13:26:35,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 9.176028e-38 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:26:35,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-10 13:26:35,306] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7415831540752641, 6.911199999999999, 6.9112, 168.912956510431, 627111.2829354965, 627111.2829354971, 190359.1089449015], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7574400.0000, 
sim time next is 7575000.0000, 
raw observation next is [28.91666666666667, 62.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7384692695810098, 6.9112, 6.9112, 168.912956510431, 625125.2680326798, 625125.2680326798, 189776.1893037372], 
processed observation next is [0.0, 0.6956521739130435, 0.5695102685624015, 0.625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.68106008485489, 0.0, 0.0, 0.8294399451523027, 0.1736459077868555, 0.1736459077868555, 0.2832480437369212], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.6993383], dtype=float32), -0.10120171]. 
=============================================
[2019-04-10 13:26:35,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.10874 ]
 [68.91702 ]
 [68.81229 ]
 [68.71402 ]
 [68.658966]], R is [[69.22498322]
 [69.24861908]
 [69.26958466]
 [69.28804016]
 [69.30417633]].
[2019-04-10 13:26:36,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 5.454107e-38 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:26:36,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5686
[2019-04-10 13:26:36,702] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7405397027109467, 6.911199999999999, 6.9112, 168.912956510431, 626773.4796244135, 626773.4796244142, 190168.0198179049], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7578000.0000, 
sim time next is 7578600.0000, 
raw observation next is [28.36666666666667, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7333061792024501, 6.9112, 6.9112, 168.912956510431, 620454.5489172856, 620454.5489172856, 188796.9464663018], 
processed observation next is [0.0, 0.7391304347826086, 0.543443917851501, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6747636331737196, 0.0, 0.0, 0.8294399451523027, 0.1723484858103571, 0.1723484858103571, 0.28178648726313704], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.20092429], dtype=float32), -1.9653491]. 
=============================================
[2019-04-10 13:26:40,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.6249126e-31 0.0000000e+00 4.1680117e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:40,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0578
[2019-04-10 13:26:40,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2104501.254091893 W.
[2019-04-10 13:26:40,511] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333333, 63.0, 1.0, 2.0, 0.8638953958784911, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.970312710383743, 6.9112, 168.9125550037477, 2104501.254091893, 2062564.762526612, 425433.1255876095], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7656600.0000, 
sim time next is 7657200.0000, 
raw observation next is [30.2, 64.0, 1.0, 2.0, 0.6254737767275511, 1.0, 1.0, 0.6254737767275511, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1748855.169021436, 1748855.169021436, 343844.6301143585], 
processed observation next is [1.0, 0.6521739130434783, 0.6303317535545023, 0.64, 1.0, 1.0, 0.5487635864187362, 1.0, 0.5, 0.5487635864187362, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4857931025059544, 0.4857931025059544, 0.5132009404691918], 
reward next is 0.4868, 
noisyNet noise sample is [array([-2.94995], dtype=float32), -0.63951975]. 
=============================================
[2019-04-10 13:26:43,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 8.6779771e-32 0.0000000e+00 1.1373468e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:43,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4495
[2019-04-10 13:26:43,729] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2310743.102308484 W.
[2019-04-10 13:26:43,732] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 63.0, 1.0, 2.0, 0.8262205252460751, 1.0, 2.0, 0.8262205252460751, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2310743.102308484, 2310743.102308484, 432834.9039885576], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7732800.0000, 
sim time next is 7733400.0000, 
raw observation next is [31.55, 62.66666666666666, 1.0, 2.0, 0.7748460609775364, 1.0, 2.0, 0.7748460609775364, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2166929.729360317, 2166929.729360317, 407809.0093073687], 
processed observation next is [1.0, 0.5217391304347826, 0.6943127962085308, 0.6266666666666666, 1.0, 1.0, 0.728730193948839, 1.0, 1.0, 0.728730193948839, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6019249248223102, 0.6019249248223102, 0.6086701631453264], 
reward next is 0.3913, 
noisyNet noise sample is [array([-0.3937575], dtype=float32), -0.8490751]. 
=============================================
[2019-04-10 13:26:45,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 8.0712655e-30 0.0000000e+00 5.9833515e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:45,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4422
[2019-04-10 13:26:45,821] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.65, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8907229688211529, 6.911200000000001, 6.9112, 168.912956510431, 733574.0720935225, 733574.0720935218, 221220.9258690765], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7769400.0000, 
sim time next is 7770000.0000, 
raw observation next is [26.6, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8908571630364404, 6.911200000000001, 6.9112, 168.912956510431, 733688.0086441473, 733688.0086441467, 221251.8800340783], 
processed observation next is [1.0, 0.9565217391304348, 0.4597156398104266, 0.8966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8668989793127322, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20380222462337425, 0.20380222462337408, 0.3302266866180273], 
reward next is 0.6698, 
noisyNet noise sample is [array([0.2845899], dtype=float32), -1.272605]. 
=============================================
[2019-04-10 13:26:45,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.06685 ]
 [60.271294]
 [60.31423 ]
 [60.40222 ]
 [60.408833]], R is [[60.09783173]
 [60.16667175]
 [60.23451996]
 [60.30200577]
 [60.3692894 ]].
[2019-04-10 13:26:50,836] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 2.0269651e-29 0.0000000e+00 3.6793641e-34 0.0000000e+00], sum to 1.0000
[2019-04-10 13:26:50,836] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1425
[2019-04-10 13:26:50,854] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.75, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9216516882541552, 6.911200000000001, 6.9112, 168.912956510431, 753035.8593542437, 753035.8593542431, 228198.4648907451], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7846200.0000, 
sim time next is 7846800.0000, 
raw observation next is [27.6, 86.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9253668503960489, 6.911199999999999, 6.9112, 168.912956510431, 755795.6329067571, 755795.6329067578, 229070.2247878206], 
processed observation next is [1.0, 0.8260869565217391, 0.5071090047393366, 0.8666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9089839638976205, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20994323136298806, 0.20994323136298826, 0.34189585789226956], 
reward next is 0.6581, 
noisyNet noise sample is [array([-0.27914527], dtype=float32), -1.7205191]. 
=============================================
[2019-04-10 13:26:57,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:26:57,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:57,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-04-10 13:26:57,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:26:57,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:57,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-04-10 13:26:59,050] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:26:59,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:59,054] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-04-10 13:26:59,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:26:59,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:59,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-04-10 13:26:59,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:26:59,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:26:59,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-04-10 13:27:00,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-04-10 13:27:00,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-04-10 13:27:00,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-04-10 13:27:00,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-04-10 13:27:00,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-04-10 13:27:00,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-04-10 13:27:00,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:00,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:00,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-04-10 13:27:01,209] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:01,209] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:01,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-04-10 13:27:01,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:01,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:01,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-04-10 13:27:01,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:01,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:01,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-04-10 13:27:02,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-10 13:27:02,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:02,540] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-04-10 13:27:03,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 4.1934563e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:27:03,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4784
[2019-04-10 13:27:03,699] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5354257111882773, 6.911199999999999, 6.9112, 168.912956510431, 474150.5971306083, 474150.597130609, 156575.9440123043], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 10800.0000, 
sim time next is 11400.0000, 
raw observation next is [21.03333333333333, 85.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.629743208629933, 6.9112, 6.9112, 168.912956510431, 557446.6730675341, 557446.6730675341, 170429.5429091712], 
processed observation next is [1.0, 0.13043478260869565, 0.19589257503949445, 0.8500000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5484673275974793, 0.0, 0.0, 0.8294399451523027, 0.15484629807431502, 0.15484629807431502, 0.2543724521032406], 
reward next is 0.7456, 
noisyNet noise sample is [array([1.1094613], dtype=float32), -0.19227587]. 
=============================================
[2019-04-10 13:27:07,986] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-10 13:27:07,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:27:07,995] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:27:07,995] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:07,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:07,997] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:27:07,998] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:07,998] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:27:07,998] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-04-10 13:27:07,998] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:27:08,041] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:08,042] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-04-10 13:27:08,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-04-10 13:27:08,085] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:27:08,086] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-04-10 13:27:08,108] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-04-10 13:27:26,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:27:26,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.378309245, 93.492485235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5849210871512438, 6.9112, 6.9112, 168.912956510431, 517985.1897093322, 517985.1897093322, 163541.879754148]
[2019-04-10 13:27:26,925] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:27:26,930] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.9049881e-34 0.0000000e+00 3.1423700e-38 0.0000000e+00], sampled 0.7223555712246913
[2019-04-10 13:27:27,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:27:27,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.5, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6082127094542195, 6.911200000000001, 6.9112, 168.912956510431, 530117.5610728323, 530117.5610728317, 167327.6940508335]
[2019-04-10 13:27:27,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:27:27,881] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00000e+00 6.86148e-36 0.00000e+00 0.00000e+00 0.00000e+00], sampled 0.3969949596937066
[2019-04-10 13:27:41,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:27:41,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7078825017427443, 6.9112, 6.9112, 168.912956510431, 604419.0189070036, 604419.0189070036, 184147.3307217877]
[2019-04-10 13:27:41,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:27:41,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3800141993102567
[2019-04-10 13:27:57,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:27:57,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.66666666666667, 56.33333333333334, 1.0, 2.0, 0.9876025770175668, 1.0, 2.0, 0.9876025770175668, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2762589.43729482, 2762589.43729482, 521541.446610105]
[2019-04-10 13:27:57,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:27:57,199] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6669588559109179
[2019-04-10 13:27:57,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2762589.43729482 W.
[2019-04-10 13:28:03,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:28:03,419] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.86666666666667, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9562684552177619, 6.911200000000001, 6.9112, 168.912956510431, 779089.8973850841, 779089.8973850834, 236475.8727466249]
[2019-04-10 13:28:03,420] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:28:03,423] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.4721446e-31 0.0000000e+00 4.0749361e-35 0.0000000e+00], sampled 0.1657226248062984
[2019-04-10 13:28:29,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14134513], dtype=float32), 0.03246076]
[2019-04-10 13:28:29,484] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.27294796666667, 89.74035337333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7556159231682127, 6.9112, 6.9112, 168.912956510431, 641362.427328227, 641362.427328227, 193087.4027823934]
[2019-04-10 13:28:29,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:28:29,488] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.2890460e-30 0.0000000e+00 1.5868461e-33 0.0000000e+00], sampled 0.08074530579466921
[2019-04-10 13:28:32,853] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-04-10 13:28:32,967] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.5290 2989441077.2496 1566.0000
[2019-04-10 13:28:33,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-04-10 13:28:33,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.7256 2937840706.0387 1381.0000
[2019-04-10 13:28:33,339] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:28:34,354] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 850000, evaluation results [850000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8061.7256276379185, 2937840706.0387306, 1381.0, 7030.534901418457, 3185211196.137981, 2464.0, 7924.529048264964, 2989441077.24962, 1566.0]
[2019-04-10 13:28:37,415] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 7.4401505e-19 4.5135519e-29 2.3970226e-19 3.5908985e-26], sum to 1.0000
[2019-04-10 13:28:37,420] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4676
[2019-04-10 13:28:37,421] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1427817.730455525 W.
[2019-04-10 13:28:37,427] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.8, 95.5, 1.0, 2.0, 0.3273534923155298, 1.0, 1.0, 0.3273534923155298, 1.0, 2.0, 0.5598060078754842, 6.911199999999999, 6.9112, 170.5573041426782, 1427817.730455525, 1427817.730455526, 320310.0125894989], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 127800.0000, 
sim time next is 128400.0000, 
raw observation next is [22.8, 95.66666666666667, 1.0, 2.0, 0.4641001557330734, 1.0, 2.0, 0.4641001557330734, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1355793.516067238, 1355793.516067238, 296346.9402304588], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9566666666666667, 1.0, 1.0, 0.35433753702779924, 1.0, 1.0, 0.35433753702779924, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.37660931001867726, 0.37660931001867726, 0.44230886601561015], 
reward next is 0.5577, 
noisyNet noise sample is [array([0.558582], dtype=float32), -1.0157107]. 
=============================================
[2019-04-10 13:28:55,046] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.00000000e+00 3.30714850e-31 0.00000000e+00 1.30931355e-33
 0.00000000e+00], sum to 1.0000
[2019-04-10 13:28:55,055] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5244
[2019-04-10 13:28:55,059] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.2, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8703386233169781, 6.911200000000001, 6.9112, 168.912956510431, 777850.2612768414, 777850.2612768408, 216061.0733089947], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [22.4, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8832214925216487, 6.911200000000001, 6.9112, 168.912956510431, 789525.9277171761, 789525.9277171755, 218944.7682286639], 
processed observation next is [1.0, 0.43478260869565216, 0.2606635071090047, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8575871860020106, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2193127576992156, 0.2193127576992154, 0.3267832361621849], 
reward next is 0.6732, 
noisyNet noise sample is [array([0.23172137], dtype=float32), 0.14241765]. 
=============================================
[2019-04-10 13:28:56,498] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.1774815e-26 0.0000000e+00 1.2962713e-28 7.0507728e-36], sum to 1.0000
[2019-04-10 13:28:56,502] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6224
[2019-04-10 13:28:56,506] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.23333333333333, 65.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4742865986833651, 6.911200000000001, 6.9112, 168.912956510431, 420806.0190036932, 420806.0190036926, 148844.1270608998], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 580800.0000, 
sim time next is 581400.0000, 
raw observation next is [23.1, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4684222679960891, 6.911200000000001, 6.9112, 168.912956510431, 416457.3798551781, 416457.3798551775, 148116.3483035545], 
processed observation next is [1.0, 0.7391304347826086, 0.2938388625592418, 0.655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3517344731659623, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11568260551532726, 0.11568260551532708, 0.22106917657246938], 
reward next is 0.7789, 
noisyNet noise sample is [array([-1.5342776], dtype=float32), 1.1303228]. 
=============================================
[2019-04-10 13:28:57,155] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.2426378e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:28:57,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7275
[2019-04-10 13:28:57,166] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.03333333333333, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4658806449427563, 6.9112, 6.9112, 168.912956510431, 415820.7852035937, 415820.7852035937, 147729.2700842364], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 586200.0000, 
sim time next is 586800.0000, 
raw observation next is [21.9, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4618805560823718, 6.9112, 6.9112, 168.912956510431, 412422.9783335476, 412422.9783335476, 147260.7637651088], 
processed observation next is [1.0, 0.8260869565217391, 0.23696682464454974, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.34375677571020946, 0.0, 0.0, 0.8294399451523027, 0.11456193842598544, 0.11456193842598544, 0.219792184724043], 
reward next is 0.7802, 
noisyNet noise sample is [array([-0.72002727], dtype=float32), 0.7554427]. 
=============================================
[2019-04-10 13:28:58,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.5193983e-33 0.0000000e+00 1.2119815e-38 0.0000000e+00], sum to 1.0000
[2019-04-10 13:28:58,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7874
[2019-04-10 13:28:58,774] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4430855671391422, 6.9112, 6.9112, 168.912956510431, 397035.174288188, 397035.174288188, 145073.2281093803], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 682200.0000, 
sim time next is 682800.0000, 
raw observation next is [19.7, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4416033141140762, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 144909.4261430125], 
processed observation next is [1.0, 0.9130434782608695, 0.1327014218009479, 0.8266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.31902843184643437, 0.0, 0.0, 0.8294399451523027, 0.10993054694236297, 0.10993054694236297, 0.21628272558658582], 
reward next is 0.7837, 
noisyNet noise sample is [array([1.2201846], dtype=float32), 0.8575232]. 
=============================================
[2019-04-10 13:29:03,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 8.8717928e-22 1.3714940e-34 1.2609490e-23 4.6714633e-32], sum to 1.0000
[2019-04-10 13:29:03,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-10 13:29:03,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 937043.7466295102 W.
[2019-04-10 13:29:03,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.58333333333333, 48.83333333333334, 1.0, 2.0, 0.5719359761049867, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937043.7466295102, 937043.7466295096, 209691.3502838626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 744600.0000, 
sim time next is 745200.0000, 
raw observation next is [25.5, 49.0, 1.0, 2.0, 0.601907676441616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 989904.1619881522, 989904.1619881516, 216022.4586874016], 
processed observation next is [1.0, 0.6521739130434783, 0.40758293838862564, 0.49, 1.0, 1.0, 0.5203706945079711, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27497337833004226, 0.2749733783300421, 0.32242158013045014], 
reward next is 0.6776, 
noisyNet noise sample is [array([2.3219218], dtype=float32), -1.0519913]. 
=============================================
[2019-04-10 13:29:03,880] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 7.576124e-37 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:29:03,888] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5549
[2019-04-10 13:29:03,892] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.9, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4512945280053381, 6.911200000000001, 6.9112, 168.912956510431, 403930.1500833399, 403930.1500833393, 146006.6487400517], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [22.7, 63.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4496989613564539, 6.911200000000001, 6.9112, 168.912956510431, 402372.2842304306, 402372.28423043, 145839.1610151394], 
processed observation next is [1.0, 0.782608695652174, 0.27488151658767773, 0.635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3289011723859193, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1117700789528974, 0.11177007895289723, 0.21767038957483492], 
reward next is 0.7823, 
noisyNet noise sample is [array([-0.73416984], dtype=float32), 1.569509]. 
=============================================
[2019-04-10 13:29:03,922] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.0631602e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:29:03,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-10 13:29:03,936] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4514670208809035, 6.911200000000001, 6.9112, 168.912956510431, 403835.3042693184, 403835.3042693177, 146043.0292932987], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 758400.0000, 
sim time next is 759000.0000, 
raw observation next is [22.3, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.453730496182385, 6.9112, 6.9112, 168.912956510431, 405752.3284923905, 405752.3284923905, 146302.1350488255], 
processed observation next is [1.0, 0.782608695652174, 0.25592417061611383, 0.665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3338176782712012, 0.0, 0.0, 0.8294399451523027, 0.11270898013677515, 0.11270898013677515, 0.21836139559526194], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.73416984], dtype=float32), 1.569509]. 
=============================================
[2019-04-10 13:29:03,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.67798 ]
 [75.74121 ]
 [75.823715]
 [75.63446 ]
 [75.48816 ]], R is [[75.80493927]
 [75.82891846]
 [75.85296631]
 [75.87651825]
 [75.90019226]].
[2019-04-10 13:29:04,636] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.3503391e-19 1.0233975e-31 8.6567004e-21 1.6505605e-28], sum to 1.0000
[2019-04-10 13:29:04,641] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6160
[2019-04-10 13:29:04,648] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 952258.2389934995 W.
[2019-04-10 13:29:04,653] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.85, 50.0, 1.0, 2.0, 0.2889801435613993, 0.0, 2.0, 0.0, 1.0, 2.0, 0.532606600234153, 6.9112, 6.9112, 168.912956510431, 952258.2389934995, 952258.2389934995, 229397.3437172142], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 750600.0000, 
sim time next is 751200.0000, 
raw observation next is [24.76666666666667, 50.0, 1.0, 2.0, 0.1971863128080033, 1.0, 1.0, 0.1971863128080033, 1.0, 2.0, 0.3626353788150852, 6.9112, 6.9112, 170.5573041426782, 969351.3243032551, 969351.3243032551, 282535.2311996974], 
processed observation next is [1.0, 0.6956521739130435, 0.3728278041074251, 0.5, 1.0, 1.0, 0.03275459374458228, 1.0, 0.5, 0.03275459374458228, 1.0, 1.0, 0.22272607172571363, 0.0, 0.0, 0.8375144448122397, 0.2692642567509042, 0.2692642567509042, 0.4216943749249215], 
reward next is 0.5783, 
noisyNet noise sample is [array([-0.31686133], dtype=float32), -0.36714917]. 
=============================================
[2019-04-10 13:29:06,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:29:06,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7633
[2019-04-10 13:29:06,832] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.45, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4934729678595544, 6.9112, 6.9112, 168.912956510431, 437923.1044617129, 437923.1044617129, 151144.3445096104], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 799800.0000, 
sim time next is 800400.0000, 
raw observation next is [20.6, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4957588152689543, 6.9112, 6.9112, 168.912956510431, 439714.3510270267, 439714.3510270267, 151437.0546698155], 
processed observation next is [0.0, 0.2608695652173913, 0.17535545023696694, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.38507172593774913, 0.0, 0.0, 0.8294399451523027, 0.1221428752852852, 0.1221428752852852, 0.22602545473106792], 
reward next is 0.7740, 
noisyNet noise sample is [array([0.28707957], dtype=float32), 0.66297066]. 
=============================================
[2019-04-10 13:29:11,082] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.2230924e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:29:11,089] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-10 13:29:11,093] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5704203678157002, 6.9112, 6.9112, 168.912956510431, 498329.8766802831, 498329.8766802831, 161664.6028694742], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 925200.0000, 
sim time next is 925800.0000, 
raw observation next is [23.93333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5699661657385768, 6.911199999999999, 6.9112, 168.912956510431, 497794.5797429986, 497794.5797429992, 161602.3491272733], 
processed observation next is [0.0, 0.7391304347826086, 0.3333333333333332, 0.7466666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47556849480314245, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13827627215083294, 0.13827627215083313, 0.2411975360108557], 
reward next is 0.7588, 
noisyNet noise sample is [array([-1.3406307], dtype=float32), -0.35577416]. 
=============================================
[2019-04-10 13:29:16,317] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-10 13:29:16,317] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:29:16,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:29:16,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:16,318] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:16,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-04-10 13:29:16,369] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:29:16,379] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:16,380] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-04-10 13:29:16,409] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:29:16,410] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-04-10 13:29:16,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:29:16,442] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:16,462] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:29:16,469] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-04-10 13:29:16,486] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-04-10 13:29:27,063] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:29:27,064] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.83333333333333, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6832645916287609, 6.9112, 6.9112, 168.912956510431, 594398.2192844866, 594398.2192844866, 179692.0410586677]
[2019-04-10 13:29:27,065] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:29:27,067] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00000e+00 5.23421e-38 0.00000e+00 0.00000e+00 0.00000e+00], sampled 0.2391076382499452
[2019-04-10 13:29:48,194] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:29:48,194] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.729299162298285, 6.911200000000001, 6.9112, 168.912956510431, 622066.7417673621, 622066.7417673614, 188092.050133617]
[2019-04-10 13:29:48,195] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:29:48,197] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 5.503142e-36 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.4273366202198092
[2019-04-10 13:30:09,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:09,614] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 75.0, 1.0, 2.0, 0.6333670517561034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 885109.6965596054, 885109.696559606, 207127.5277510032]
[2019-04-10 13:30:09,615] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:30:09,618] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.5859795e-27 0.0000000e+00 7.3214608e-31 0.0000000e+00], sampled 0.31391141625090957
[2019-04-10 13:30:09,620] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 885109.6965596054 W.
[2019-04-10 13:30:15,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:15,831] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.02373203333333, 82.95798546, 1.0, 2.0, 0.56576077643529, 0.0, 2.0, 0.0, 1.0, 2.0, 0.941820022598712, 6.9112, 6.9112, 168.9129563708736, 1581782.854696583, 1581782.854696583, 337352.1236445233]
[2019-04-10 13:30:15,835] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:30:15,837] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.7742436e-30 0.0000000e+00 2.6265581e-32 0.0000000e+00], sampled 0.5460863803120997
[2019-04-10 13:30:15,840] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1581782.854696583 W.
[2019-04-10 13:30:17,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:17,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.05, 59.0, 1.0, 2.0, 0.3679872920063859, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6390722309580132, 6.911200000000001, 6.9112, 168.9128793171472, 1028569.798015116, 1028569.798015116, 246334.5507554449]
[2019-04-10 13:30:17,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:30:17,615] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.7509643e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.544217996903297
[2019-04-10 13:30:17,616] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1028569.798015116 W.
[2019-04-10 13:30:18,638] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:18,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.86666666666667, 58.33333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.168700807305429, 6.9112, 168.9114255017916, 1011550.478520585, 828872.1991153316, 254812.1165497574]
[2019-04-10 13:30:18,640] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:30:18,642] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.6427480e-26 1.1558469e-36 5.2049846e-29 3.3751091e-36], sampled 0.3745754297199513
[2019-04-10 13:30:18,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1011550.478520585 W.
[2019-04-10 13:30:36,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:36,548] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.9, 59.0, 1.0, 1.0, 0.6444623304961665, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9127776783754, 900621.5594774926, 900621.5594774933, 209320.8953223929]
[2019-04-10 13:30:36,548] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:30:36,549] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.0360805e-25 3.2750385e-36 2.8566707e-28 2.0606389e-35], sampled 0.2373068485382438
[2019-04-10 13:30:36,550] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 900621.5594774926 W.
[2019-04-10 13:30:38,519] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:38,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.98333333333333, 67.33333333333333, 1.0, 2.0, 0.5949139199359105, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.914795489595569, 6.9112, 168.9128050812315, 1663354.572563939, 1660803.810784382, 363652.8389907578]
[2019-04-10 13:30:38,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:30:38,523] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.6612649e-31 0.0000000e+00 1.7554127e-34 0.0000000e+00], sampled 0.8021332847815468
[2019-04-10 13:30:38,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1663354.572563939 W.
[2019-04-10 13:30:46,525] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13510041], dtype=float32), 0.0268789]
[2019-04-10 13:30:46,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 58.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5284783401649469, 6.911200000000001, 6.9112, 168.912956510431, 467577.9160834845, 467577.9160834838, 155669.7209921745]
[2019-04-10 13:30:46,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:30:46,529] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 8.295418e-38 0.000000e+00 0.000000e+00 0.000000e+00], sampled 0.9957610705753261
[2019-04-10 13:30:46,867] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6828 3105706253.8067 2010.0000
[2019-04-10 13:30:47,106] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4827 3319593200.0117 2143.0000
[2019-04-10 13:30:47,294] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8074 3185204826.7980 2464.0000
[2019-04-10 13:30:47,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.9009 2989372718.0820 1566.0000
[2019-04-10 13:30:47,350] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.3540 2937917468.4979 1381.0000
[2019-04-10 13:30:48,365] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 875000, evaluation results [875000.0, 7287.482731418222, 3319593200.01172, 2143.0, 7347.682841450599, 3105706253.806708, 2010.0, 8060.354003801793, 2937917468.497896, 1381.0, 7029.8073902314445, 3185204826.797956, 2464.0, 7925.900925007857, 2989372718.0819726, 1566.0]
[2019-04-10 13:30:51,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 7.7037038e-17 5.6923001e-27 4.3963401e-18 2.9254571e-25], sum to 1.0000
[2019-04-10 13:30:51,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8050
[2019-04-10 13:30:51,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 988845.7697090202 W.
[2019-04-10 13:30:51,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 75.33333333333334, 1.0, 2.0, 0.6302074577255982, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 988845.7697090202, 988845.7697090202, 219345.9940444175], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1080600.0000, 
sim time next is 1081200.0000, 
raw observation next is [23.93333333333334, 74.66666666666667, 1.0, 2.0, 0.5078489376868105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796602.1151457469, 796602.1151457469, 194949.7056171697], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333337, 0.7466666666666667, 1.0, 1.0, 0.4070469128756753, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.221278365318263, 0.221278365318263, 0.2909697098763727], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.60492975], dtype=float32), 1.3751765]. 
=============================================
[2019-04-10 13:30:54,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.1887396e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:30:54,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5123
[2019-04-10 13:30:54,516] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.05, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5276450937429181, 6.9112, 6.9112, 168.912956510431, 466575.7557576234, 466575.7557576234, 155571.0008851911], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [20.0, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5181631606873247, 6.9112, 6.9112, 168.912956510431, 458312.5188377349, 458312.5188377349, 154325.782612651], 
processed observation next is [1.0, 0.13043478260869565, 0.1469194312796209, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41239409839917646, 0.0, 0.0, 0.8294399451523027, 0.1273090330104819, 0.1273090330104819, 0.23033698897410598], 
reward next is 0.7697, 
noisyNet noise sample is [array([-0.21264742], dtype=float32), 0.8192258]. 
=============================================
[2019-04-10 13:30:57,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 2.6892939e-34 0.0000000e+00 4.6121862e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:30:57,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7154
[2019-04-10 13:30:57,167] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.5, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6301939849261565, 6.9112, 6.9112, 168.912956510431, 546761.3378697922, 546761.3378697922, 170827.8564216586], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1202400.0000, 
sim time next is 1203000.0000, 
raw observation next is [23.4, 82.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6295080446466883, 6.9112, 6.9112, 168.912956510431, 546202.1469404835, 546202.1469404835, 170717.4503172049], 
processed observation next is [1.0, 0.9565217391304348, 0.30805687203791465, 0.8283333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5481805422520589, 0.0, 0.0, 0.8294399451523027, 0.15172281859457876, 0.15172281859457876, 0.2548021646525446], 
reward next is 0.7452, 
noisyNet noise sample is [array([-0.43003824], dtype=float32), 0.98932165]. 
=============================================
[2019-04-10 13:30:57,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.3962 ]
 [68.54653]
 [68.5823 ]
 [68.84752]
 [68.55312]], R is [[68.41074371]
 [68.47167206]
 [68.53156281]
 [68.5907135 ]
 [68.64940643]].
[2019-04-10 13:30:57,734] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 3.4430164e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:30:57,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4328
[2019-04-10 13:30:57,746] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.93333333333333, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6174167286836709, 6.9112, 6.9112, 168.912956510431, 537458.4168164312, 537458.4168164312, 168772.509248388], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [21.9, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6111872311453501, 6.9112, 6.9112, 168.912956510431, 531989.4497294846, 531989.4497294846, 167801.6570629858], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5258380867626221, 0.0, 0.0, 0.8294399451523027, 0.14777484714707906, 0.14777484714707906, 0.25045023442236686], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.28953642], dtype=float32), -0.4493852]. 
=============================================
[2019-04-10 13:30:57,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.5495481e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:30:57,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5574
[2019-04-10 13:30:57,897] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.93333333333333, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6174167286836709, 6.9112, 6.9112, 168.912956510431, 537458.4168164312, 537458.4168164312, 168772.509248388], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [21.9, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6111872311453501, 6.9112, 6.9112, 168.912956510431, 531989.4497294846, 531989.4497294846, 167801.6570629858], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5258380867626221, 0.0, 0.0, 0.8294399451523027, 0.14777484714707906, 0.14777484714707906, 0.25045023442236686], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.0145134], dtype=float32), -0.043374237]. 
=============================================
[2019-04-10 13:30:58,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:30:58,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8515
[2019-04-10 13:30:58,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.6, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8009646108919374, 6.9112, 6.9112, 168.912956510431, 670656.2848252932, 670656.2848252932, 202016.4867300711], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1275600.0000, 
sim time next is 1276200.0000, 
raw observation next is [26.45, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7984140029116321, 6.9112, 6.9112, 168.912956510431, 668955.3979834877, 668955.3979834877, 201501.4253260752], 
processed observation next is [1.0, 0.782608695652174, 0.45260663507109006, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7541634181849171, 0.0, 0.0, 0.8294399451523027, 0.18582094388430215, 0.18582094388430215, 0.30074839600906744], 
reward next is 0.6993, 
noisyNet noise sample is [array([1.2435656], dtype=float32), 0.5499532]. 
=============================================
[2019-04-10 13:31:00,185] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.4836270e-29 0.0000000e+00 1.0585846e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:31:00,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-10 13:31:00,201] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.56666666666667, 93.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7849640319431955, 6.911199999999999, 6.9112, 168.912956510431, 660787.5824006919, 660787.5824006925, 198825.0673410316], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [24.53333333333333, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7842272138936848, 6.911199999999999, 6.9112, 168.912956510431, 660260.844352134, 660260.8443521347, 198677.9909071999], 
processed observation next is [1.0, 0.9565217391304348, 0.36176935229067925, 0.9333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7368624559679082, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18340579009781502, 0.1834057900978152, 0.29653431478686554], 
reward next is 0.7035, 
noisyNet noise sample is [array([0.17480816], dtype=float32), -2.0965698]. 
=============================================
[2019-04-10 13:31:00,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[60.77966 ]
 [60.870285]
 [60.982895]
 [61.065945]
 [61.158752]], R is [[60.79815674]
 [60.89342117]
 [60.98772049]
 [61.08123398]
 [61.17365265]].
[2019-04-10 13:31:02,888] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.1489013e-24 7.8010408e-36 4.6279812e-28 4.1086649e-36], sum to 1.0000
[2019-04-10 13:31:02,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-10 13:31:02,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1090956.643485524 W.
[2019-04-10 13:31:02,905] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.7229716532611119, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1090956.643485524, 1090956.643485524, 236339.5698711538], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [22.4, 93.0, 1.0, 2.0, 0.3866406734550176, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6808592615354498, 6.911199999999999, 6.9112, 168.912956510431, 1174780.105268134, 1174780.105268135, 261771.4422719924], 
processed observation next is [1.0, 0.5217391304347826, 0.2606635071090047, 0.93, 1.0, 1.0, 0.2610128595843586, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.6108039774822559, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3263278070189261, 0.3263278070189264, 0.3907036451820782], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8014248], dtype=float32), -1.1119709]. 
=============================================
[2019-04-10 13:31:05,253] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:31:05,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7699
[2019-04-10 13:31:05,265] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.35, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5867858050339962, 6.9112, 6.9112, 168.912956510431, 512502.2542531451, 512502.2542531451, 164060.9111132969], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1405800.0000, 
sim time next is 1406400.0000, 
raw observation next is [21.43333333333334, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5873522551079331, 6.9112, 6.9112, 168.912956510431, 512773.129944919, 512773.129944919, 164150.0711777089], 
processed observation next is [0.0, 0.2608695652173913, 0.21484992101105885, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49677104281455253, 0.0, 0.0, 0.8294399451523027, 0.14243698054025528, 0.14243698054025528, 0.2450001062353864], 
reward next is 0.7550, 
noisyNet noise sample is [array([-1.7242635], dtype=float32), -1.4486829]. 
=============================================
[2019-04-10 13:31:06,174] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:31:06,186] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-10 13:31:06,189] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.2, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6498234550256617, 6.9112, 6.9112, 168.912956510431, 560048.3335572467, 560048.3335572467, 174064.6005434142], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1422000.0000, 
sim time next is 1422600.0000, 
raw observation next is [23.33333333333333, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6548489188065995, 6.9112, 6.9112, 168.912956510431, 564521.001018214, 564521.001018214, 174900.2124610197], 
processed observation next is [0.0, 0.4782608695652174, 0.30489731437598716, 0.8816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5790840473251212, 0.0, 0.0, 0.8294399451523027, 0.15681138917172613, 0.15681138917172613, 0.26104509322540254], 
reward next is 0.7390, 
noisyNet noise sample is [array([0.90091884], dtype=float32), -0.33146268]. 
=============================================
[2019-04-10 13:31:17,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.000000e+00 4.889429e-35 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:31:17,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6359
[2019-04-10 13:31:17,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.66666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7583313524569667, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 193582.0797893784], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1669200.0000, 
sim time next is 1669800.0000, 
raw observation next is [23.68333333333333, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7820836090745655, 6.911199999999999, 6.9112, 168.912956510431, 660898.7693431445, 660898.7693431451, 198286.77733787], 
processed observation next is [1.0, 0.30434782608695654, 0.32148499210110576, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7342483037494701, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18358299148420681, 0.18358299148420695, 0.2959504139371194], 
reward next is 0.7040, 
noisyNet noise sample is [array([-0.37720746], dtype=float32), -0.11302058]. 
=============================================
[2019-04-10 13:31:19,483] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.7115004e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:31:19,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4664
[2019-04-10 13:31:19,494] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.65, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8572498837734893, 6.911199999999999, 6.9112, 168.912956510431, 710266.2688861516, 710266.2688861522, 213833.8479584907], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1714200.0000, 
sim time next is 1714800.0000, 
raw observation next is [26.6, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8598561233672996, 6.9112, 6.9112, 168.912956510431, 712269.9366325306, 712269.9366325306, 214405.5211096463], 
processed observation next is [1.0, 0.8695652173913043, 0.4597156398104266, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8290928333747555, 0.0, 0.0, 0.8294399451523027, 0.19785276017570297, 0.19785276017570297, 0.32000824046215864], 
reward next is 0.6800, 
noisyNet noise sample is [array([0.47307512], dtype=float32), 0.6502126]. 
=============================================
[2019-04-10 13:31:20,814] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.1684435e-22 4.5480405e-33 2.0637410e-25 8.2194498e-36], sum to 1.0000
[2019-04-10 13:31:20,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-10 13:31:20,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1143405.665464765 W.
[2019-04-10 13:31:20,832] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.8, 82.0, 1.0, 2.0, 0.3807155765844438, 1.0, 2.0, 0.3807155765844438, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1143405.665464765, 1143405.665464765, 276757.0168863861], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [23.71666666666667, 82.66666666666667, 1.0, 2.0, 0.391912545057354, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6897698849479733, 6.9112, 6.9112, 168.912956510431, 1189809.465273914, 1189809.465273914, 263985.0858524844], 
processed observation next is [1.0, 0.4782608695652174, 0.32306477093206964, 0.8266666666666667, 1.0, 1.0, 0.267364512117294, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6216705913999674, 0.0, 0.0, 0.8294399451523027, 0.33050262924275386, 0.33050262924275386, 0.39400759082460357], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4675813], dtype=float32), -0.6316033]. 
=============================================
[2019-04-10 13:31:27,483] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.7194495e-29 0.0000000e+00 2.2340026e-30 0.0000000e+00], sum to 1.0000
[2019-04-10 13:31:27,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4678
[2019-04-10 13:31:27,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 968463.6278082344 W.
[2019-04-10 13:31:27,505] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.1, 95.66666666666666, 1.0, 1.0, 0.2309964479179991, 1.0, 1.0, 0.2309964479179991, 1.0, 2.0, 0.3860366067804403, 6.911199999999999, 6.9112, 170.5573041426782, 968463.6278082344, 968463.627808235, 278008.5780946507], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1909200.0000, 
sim time next is 1909800.0000, 
raw observation next is [24.05, 95.5, 1.0, 2.0, 0.2860337396461163, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4779665440524792, 6.911199999999999, 6.9112, 168.9129563751907, 804232.5907865919, 804232.5907865925, 216096.3776452497], 
processed observation next is [1.0, 0.08695652173913043, 0.3388625592417062, 0.955, 1.0, 1.0, 0.139799686320622, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.3633738342103404, -8.881784197001253e-17, 0.0, 0.8294399444882109, 0.2233979418851644, 0.22339794188516457, 0.32253190693320855], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.4048917], dtype=float32), -0.9161864]. 
=============================================
[2019-04-10 13:31:29,118] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-10 13:31:29,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:31:29,121] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:31:29,122] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:31:29,122] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:31:29,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:31:29,124] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:31:29,127] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:31:29,126] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:31:29,127] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:31:29,130] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:31:29,150] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-04-10 13:31:29,150] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-04-10 13:31:29,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-04-10 13:31:29,150] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-04-10 13:31:29,220] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-04-10 13:31:34,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:31:34,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.7, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4712318838911153, 6.911200000000001, 6.9112, 168.912956510431, 418597.1364213601, 418597.1364213594, 148461.6371833973]
[2019-04-10 13:31:34,148] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:34,150] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7358395487728355
[2019-04-10 13:31:34,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:31:34,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.84987897, 92.36242122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5690074321714427, 6.9112, 6.9112, 168.912956510431, 499864.5289029965, 499864.5289029965, 161386.2238163313]
[2019-04-10 13:31:34,831] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:31:34,834] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.9941804e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.29087928192593426
[2019-04-10 13:31:39,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:31:39,491] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.43333333333333, 66.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.533027849837988, 6.9112, 6.9112, 168.912956510431, 469094.101236043, 469094.101236043, 156371.421795004]
[2019-04-10 13:31:39,492] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:31:39,494] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.9216235e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9973065826555659
[2019-04-10 13:32:22,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:32:22,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.280651165, 74.36922237499999, 1.0, 2.0, 0.6331334419088046, 1.0, 2.0, 0.6331334419088046, 1.0, 2.0, 1.03, 6.957651899416735, 6.9112, 184.5923449428631, 2656238.800327139, 2620225.207157522, 505540.7244018977]
[2019-04-10 13:32:22,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:32:22,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 5.69521036e-12 5.89215846e-19 5.88287828e-13
 1.00279575e-16], sampled 0.1703347972258792
[2019-04-10 13:32:22,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2656238.800327139 W.
[2019-04-10 13:32:26,450] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:32:26,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.6869576680131998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 960034.6752812007, 960034.6752812007, 218055.4227884784]
[2019-04-10 13:32:26,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:32:26,455] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.5491659e-34 0.0000000e+00 3.5345983e-38 0.0000000e+00], sampled 0.9534101734755894
[2019-04-10 13:32:26,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 960034.6752812007 W.
[2019-04-10 13:32:49,705] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13564068], dtype=float32), 0.027857758]
[2019-04-10 13:32:49,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.95481257333333, 69.41176079166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9614716942567387, 6.9112, 6.9112, 168.912956510431, 781961.7366316419, 781961.7366316419, 237695.555202354]
[2019-04-10 13:32:49,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:32:49,711] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.1625387e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.9659384235278734
[2019-04-10 13:33:08,127] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-04-10 13:33:08,238] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.6167 2989382995.4001 1566.0000
[2019-04-10 13:33:08,484] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-04-10 13:33:08,497] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5349 3185211196.1380 2464.0000
[2019-04-10 13:33:08,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-04-10 13:33:09,534] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 900000, evaluation results [900000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7030.534901418457, 3185211196.137981, 2464.0, 7924.616693874975, 2989382995.400146, 1566.0]
[2019-04-10 13:33:11,332] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.7768793e-32 0.0000000e+00 1.8052214e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:33:11,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9880
[2019-04-10 13:33:11,343] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.1, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8575067038317015, 6.911199999999999, 6.9112, 168.912956510431, 710020.1507385321, 710020.1507385327, 213875.2806098301], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2042400.0000, 
sim time next is 2043000.0000, 
raw observation next is [27.05, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8570777032596085, 6.911200000000001, 6.9112, 168.912956510431, 709686.56521086, 709686.5652108593, 213781.2155877407], 
processed observation next is [0.0, 0.6521739130434783, 0.4810426540284361, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8257045161702542, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19713515700301668, 0.19713515700301648, 0.3190764411757324], 
reward next is 0.6809, 
noisyNet noise sample is [array([-0.35335895], dtype=float32), -0.09862446]. 
=============================================
[2019-04-10 13:33:11,352] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.877182]
 [63.849667]
 [63.80445 ]
 [63.66957 ]
 [63.611   ]], R is [[63.94139862]
 [63.98276901]
 [64.02324677]
 [64.06290436]
 [64.10135651]].
[2019-04-10 13:33:13,008] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 4.0219389e-33 0.0000000e+00 1.0449573e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:33:13,015] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-10 13:33:13,022] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.4, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8694440902921632, 6.9112, 6.9112, 168.912956510431, 718328.0743165503, 718328.0743165503, 216478.0319856968], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2039400.0000, 
sim time next is 2040000.0000, 
raw observation next is [27.33333333333334, 82.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8687568840859526, 6.9112, 6.9112, 168.912956510431, 718177.7489100752, 718177.7489100752, 216338.8384583104], 
processed observation next is [0.0, 0.6086956521739131, 0.4944707740916275, 0.8266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8399474196170152, 0.0, 0.0, 0.8294399451523027, 0.19949381914168754, 0.19949381914168754, 0.32289378874374686], 
reward next is 0.6771, 
noisyNet noise sample is [array([-1.106536], dtype=float32), -0.17323522]. 
=============================================
[2019-04-10 13:33:13,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.1546  ]
 [70.13142 ]
 [70.116264]
 [70.1096  ]
 [70.04638 ]], R is [[70.17236328]
 [70.14753723]
 [70.12255096]
 [70.09700775]
 [70.07113647]].
[2019-04-10 13:33:24,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.3095476e-19 8.8366336e-29 2.1801318e-21 6.1854592e-29], sum to 1.0000
[2019-04-10 13:33:24,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5381
[2019-04-10 13:33:24,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 995521.7627157079 W.
[2019-04-10 13:33:24,855] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.43333333333334, 74.66666666666667, 1.0, 2.0, 0.2374473180450466, 1.0, 1.0, 0.2374473180450466, 1.0, 1.0, 0.4027804473374504, 6.9112, 6.9112, 170.5573041426782, 995521.7627157079, 995521.7627157079, 280488.7006934957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2274600.0000, 
sim time next is 2275200.0000, 
raw observation next is [28.6, 74.0, 1.0, 2.0, 0.66763181452778, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 933014.6124460528, 933014.6124460523, 214012.0828689311], 
processed observation next is [1.0, 0.34782608695652173, 0.5545023696682465, 0.74, 1.0, 1.0, 0.599556403045518, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25917072567945915, 0.259170725679459, 0.3194210192073598], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.747115], dtype=float32), 0.21834692]. 
=============================================
[2019-04-10 13:33:25,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.3959723e-25 4.9786640e-34 4.0082756e-26 2.2420209e-36], sum to 1.0000
[2019-04-10 13:33:25,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4238
[2019-04-10 13:33:25,338] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.11666666666667, 77.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.018855048831457, 6.911200000000001, 6.9112, 168.9128269666244, 819829.5469813384, 819829.5469813378, 251870.3683465077], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2319000.0000, 
sim time next is 2319600.0000, 
raw observation next is [30.03333333333333, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.013007500885908, 6.911200000000001, 6.9112, 168.9128637725132, 815122.4670293775, 815122.4670293768, 250340.2049805204], 
processed observation next is [1.0, 0.8695652173913043, 0.622432859399684, 0.7733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0158628059584245, 8.881784197001253e-17, 0.0, 0.8294394897666706, 0.2264229075081604, 0.22642290750816021, 0.3736420969858513], 
reward next is 0.6264, 
noisyNet noise sample is [array([-1.1864557], dtype=float32), -0.79626364]. 
=============================================
[2019-04-10 13:33:25,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.6400603e-16 4.8765216e-27 6.5593793e-18 3.8317455e-27], sum to 1.0000
[2019-04-10 13:33:25,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4932
[2019-04-10 13:33:25,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2277034.933832186 W.
[2019-04-10 13:33:25,813] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.26666666666667, 64.0, 1.0, 2.0, 0.8141789306710036, 1.0, 2.0, 0.8141789306710036, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2277034.933832186, 2277034.933832187, 426836.0465497144], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2305200.0000, 
sim time next is 2305800.0000, 
raw observation next is [32.3, 64.0, 1.0, 2.0, 0.5259336681565234, 1.0, 2.0, 0.5259336681565234, 1.0, 1.0, 0.9133728526660322, 6.911199999999999, 6.9112, 170.5573041426782, 2206274.772481533, 2206274.772481534, 433543.2281285917], 
processed observation next is [1.0, 0.6956521739130435, 0.7298578199052131, 0.64, 1.0, 1.0, 0.4288357447668957, 1.0, 1.0, 0.4288357447668957, 1.0, 0.5, 0.8943571373976001, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6128541034670925, 0.6128541034670928, 0.6470794449680474], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46317267], dtype=float32), 0.66343343]. 
=============================================
[2019-04-10 13:33:29,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 9.2302989e-24 6.6329567e-32 3.3481072e-26 9.1942272e-35], sum to 1.0000
[2019-04-10 13:33:29,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8608
[2019-04-10 13:33:29,057] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.3, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9922881588076701, 6.9112, 6.9112, 168.912956510431, 801582.9489451122, 801582.9489451122, 245171.1033122356], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2415600.0000, 
sim time next is 2416200.0000, 
raw observation next is [29.26666666666667, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9988818050708923, 6.911200000000001, 6.9112, 168.9128690704717, 807306.1126854201, 807306.1126854195, 246884.7165852053], 
processed observation next is [1.0, 1.0, 0.5860979462875199, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9986363476474296, 8.881784197001253e-17, 0.0, 0.8294395157820723, 0.22425169796817224, 0.2242516979681721, 0.3684846516197094], 
reward next is 0.6315, 
noisyNet noise sample is [array([-0.42831802], dtype=float32), -0.38891214]. 
=============================================
[2019-04-10 13:33:30,453] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.00000000e+00 1.02554537e-19 4.25469992e-31 1.00293756e-23
 2.97640386e-33], sum to 1.0000
[2019-04-10 13:33:30,458] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5432
[2019-04-10 13:33:30,464] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2465749.088008152 W.
[2019-04-10 13:33:30,467] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.2, 60.0, 1.0, 2.0, 0.8815892502206525, 1.0, 2.0, 0.8815892502206525, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2465749.088008152, 2465749.088008152, 461548.4924218413], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2394000.0000, 
sim time next is 2394600.0000, 
raw observation next is [33.06666666666667, 60.83333333333333, 1.0, 2.0, 0.3964716291315175, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6885401046301348, 6.9112, 6.9112, 168.9129565104192, 1108228.57326242, 1108228.57326242, 258047.2163368912], 
processed observation next is [1.0, 0.7391304347826086, 0.7661927330173778, 0.6083333333333333, 1.0, 1.0, 0.2728573844958042, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6201708593050423, 0.0, 0.0, 0.8294399451522447, 0.3078412703506722, 0.3078412703506722, 0.3851450990102854], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49907297], dtype=float32), 1.2957963]. 
=============================================
[2019-04-10 13:33:35,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.7467796e-22 5.8491057e-30 1.8848511e-25 5.0893128e-32], sum to 1.0000
[2019-04-10 13:33:35,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9079
[2019-04-10 13:33:35,589] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.71666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9436263965539362, 6.911200000000001, 6.9112, 168.912956510431, 768550.7971480859, 768550.7971480853, 233367.8133829183], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2505000.0000, 
sim time next is 2505600.0000, 
raw observation next is [26.7, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9418686152786663, 6.9112, 6.9112, 168.912956510431, 767321.5878844807, 767321.5878844807, 232950.4457128838], 
processed observation next is [1.0, 0.0, 0.46445497630331756, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9291080674130076, 0.0, 0.0, 0.8294399451523027, 0.21314488552346686, 0.21314488552346686, 0.34768723240728927], 
reward next is 0.6523, 
noisyNet noise sample is [array([0.16513093], dtype=float32), -2.4471805]. 
=============================================
[2019-04-10 13:33:37,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 5.8907202e-19 2.0576065e-28 2.3584091e-21 8.3283643e-30], sum to 1.0000
[2019-04-10 13:33:37,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9064
[2019-04-10 13:33:37,113] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2053289.287821768 W.
[2019-04-10 13:33:37,121] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.4894997763680573, 1.0, 2.0, 0.4894997763680573, 1.0, 2.0, 0.8457944805998944, 6.911199999999999, 6.9112, 170.5573041426782, 2053289.287821768, 2053289.287821769, 407201.8720730019], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [27.68333333333333, 84.83333333333333, 1.0, 2.0, 0.4953097902799816, 1.0, 2.0, 0.4953097902799816, 1.0, 2.0, 0.8559366005948531, 6.9112, 6.9112, 170.5573041426782, 2077684.003915816, 2077684.003915816, 411158.0326125309], 
processed observation next is [1.0, 0.391304347826087, 0.5110584518167456, 0.8483333333333333, 1.0, 1.0, 0.39193950636142366, 1.0, 1.0, 0.39193950636142366, 1.0, 1.0, 0.8243129275546988, 0.0, 0.0, 0.8375144448122397, 0.5771344455321711, 0.5771344455321711, 0.6136687053918372], 
reward next is 0.3863, 
noisyNet noise sample is [array([-2.1450543], dtype=float32), -0.44700134]. 
=============================================
[2019-04-10 13:33:37,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[32.60675 ]
 [32.716766]
 [32.414955]
 [32.48015 ]
 [32.730957]], R is [[32.54253769]
 [32.6093483 ]
 [32.69428635]
 [32.79115677]
 [32.87201309]].
[2019-04-10 13:33:38,021] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 4.1211410e-30 3.7138201e-38 5.6025997e-33 0.0000000e+00], sum to 1.0000
[2019-04-10 13:33:38,027] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2651
[2019-04-10 13:33:38,030] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.63333333333333, 88.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8882509026506338, 6.911200000000001, 6.9112, 168.912956510431, 732666.8116229931, 732666.8116229925, 220696.699171866], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2583600.0000, 
sim time next is 2584200.0000, 
raw observation next is [26.51666666666667, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8844482368931946, 6.911200000000001, 6.9112, 168.912956510431, 730268.1667612691, 730268.1667612685, 219854.6395722587], 
processed observation next is [1.0, 0.9130434782608695, 0.45576619273301755, 0.8866666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.859083215723408, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20285226854479696, 0.20285226854479682, 0.3281412530929234], 
reward next is 0.6719, 
noisyNet noise sample is [array([-0.14638829], dtype=float32), -0.6259519]. 
=============================================
[2019-04-10 13:33:49,385] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:33:49,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1410
[2019-04-10 13:33:49,399] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5967279402471628, 6.911199999999999, 6.9112, 168.912956510431, 517463.466596218, 517463.4665962186, 165616.7081806799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6018167309485615, 6.911199999999999, 6.9112, 168.912956510431, 521876.3759363483, 521876.375936349, 166390.1627408241], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5144106474982457, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14496565998231897, 0.14496565998231917, 0.24834352647884195], 
reward next is 0.7517, 
noisyNet noise sample is [array([-0.23270382], dtype=float32), -1.4973925]. 
=============================================
[2019-04-10 13:33:54,065] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-10 13:33:54,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:33:54,067] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:33:54,067] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:33:54,069] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:33:54,070] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:33:54,070] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:33:54,070] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:33:54,071] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:33:54,071] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:33:54,072] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:33:54,097] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-04-10 13:33:54,114] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-04-10 13:33:54,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-04-10 13:33:54,114] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-04-10 13:33:54,159] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-04-10 13:34:57,588] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.14162317], dtype=float32), 0.02467027]
[2019-04-10 13:34:57,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.91281162, 71.70620697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8395495015363802, 6.9112, 6.9112, 168.912956510431, 701026.8362937011, 701026.8362937011, 210125.7927309395]
[2019-04-10 13:34:57,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:34:57,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3172638408578884
[2019-04-10 13:35:23,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14162317], dtype=float32), 0.02467027]
[2019-04-10 13:35:23,770] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.43333333333333, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9446978148825819, 6.9112, 6.9112, 168.912956510431, 775215.8698422785, 775215.8698422785, 233893.2477460018]
[2019-04-10 13:35:23,771] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:35:23,773] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6906492798428481
[2019-04-10 13:35:43,789] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.2308 3319599876.4447 2143.0000
[2019-04-10 13:35:44,232] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-04-10 13:35:44,532] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.6175 3185157732.2717 2464.0000
[2019-04-10 13:35:44,582] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.7984 2989436396.8986 1566.0000
[2019-04-10 13:35:44,631] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.9947 2937841177.1021 1381.0000
[2019-04-10 13:35:45,648] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 925000, evaluation results [925000.0, 7288.230812648848, 3319599876.4446845, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8060.994687308518, 2937841177.10208, 1381.0, 7030.617469616518, 3185157732.2716994, 2464.0, 7923.798383958106, 2989436396.898591, 1566.0]
[2019-04-10 13:35:49,287] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:35:49,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5143
[2019-04-10 13:35:49,298] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6017126588162008, 6.911200000000001, 6.9112, 168.912956510431, 527203.1610932074, 527203.1610932067, 166264.9192650405], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2956200.0000, 
sim time next is 2956800.0000, 
raw observation next is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5618293892576323, 6.911200000000001, 6.9112, 168.912956510431, 492250.5029396632, 492250.5029396626, 160401.5888617043], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4656455966556491, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13673625081657312, 0.13673625081657295, 0.23940535651000644], 
reward next is 0.7606, 
noisyNet noise sample is [array([0.4482823], dtype=float32), 1.4050757]. 
=============================================
[2019-04-10 13:35:55,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 3.8384979e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:35:55,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3332
[2019-04-10 13:35:55,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.16666666666667, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.684101662097061, 6.9112, 6.9112, 168.912956510431, 587864.5402136683, 587864.5402136683, 179912.0776286923], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3099000.0000, 
sim time next is 3099600.0000, 
raw observation next is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.682637249262442, 6.9112, 6.9112, 168.912956510431, 586936.0112060449, 586936.0112060449, 179655.0908198253], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6129722551981, 0.0, 0.0, 0.8294399451523027, 0.16303778089056803, 0.16303778089056803, 0.2681419265967542], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.9270746], dtype=float32), -1.840287]. 
=============================================
[2019-04-10 13:35:57,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:35:57,450] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2110
[2019-04-10 13:35:57,453] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.611202816012495, 6.9112, 6.9112, 168.912956510431, 531439.1029385976, 531439.1029385976, 167813.8538654131], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3128400.0000, 
sim time next is 3129000.0000, 
raw observation next is [21.16666666666667, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6124466756505899, 6.9112, 6.9112, 168.912956510431, 532561.5974290697, 532561.5974290697, 168006.4442634695], 
processed observation next is [1.0, 0.21739130434782608, 0.2022116903633494, 0.9900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5273739946958412, 0.0, 0.0, 0.8294399451523027, 0.14793377706363048, 0.14793377706363048, 0.25075588696040224], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.20785667], dtype=float32), 1.2603215]. 
=============================================
[2019-04-10 13:35:57,470] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.20068 ]
 [70.10688 ]
 [70.03939 ]
 [69.98175 ]
 [69.966324]], R is [[70.35294342]
 [70.39894867]
 [70.44424438]
 [70.48760223]
 [70.5289917 ]].
[2019-04-10 13:35:58,850] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.0829500e-23 4.4643244e-35 2.3226910e-25 8.5724470e-35], sum to 1.0000
[2019-04-10 13:35:58,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5738
[2019-04-10 13:35:58,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1476781.720856115 W.
[2019-04-10 13:35:58,869] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.3521562465736996, 1.0, 1.0, 0.3521562465736996, 1.0, 1.0, 0.5914442598747712, 6.9112, 6.9112, 170.5573041426782, 1476781.720856115, 1476781.720856115, 325496.9547480794], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3164400.0000, 
sim time next is 3165000.0000, 
raw observation next is [26.16666666666667, 84.0, 1.0, 2.0, 0.5263845235694057, 1.0, 2.0, 0.5263845235694057, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1471606.566227769, 1471606.566227769, 308642.297957498], 
processed observation next is [1.0, 0.6521739130434783, 0.4391785150078992, 0.84, 1.0, 1.0, 0.42937894405952487, 1.0, 1.0, 0.42937894405952487, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4087796017299358, 0.4087796017299358, 0.4606601462052209], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4264071], dtype=float32), 0.2828051]. 
=============================================
[2019-04-10 13:35:58,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.71907 ]
 [59.345917]
 [58.938656]
 [58.68722 ]
 [58.736153]], R is [[59.4213295 ]
 [58.82711792]
 [58.23884583]
 [58.17852783]
 [57.59674454]].
[2019-04-10 13:35:58,896] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.5717928e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:35:58,901] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4786
[2019-04-10 13:35:58,905] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7844736241420228, 6.9112, 6.9112, 168.912956510431, 659226.4918818943, 659226.4918818943, 198703.8083153794], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3206400.0000, 
sim time next is 3207000.0000, 
raw observation next is [25.0, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7816797315269906, 6.9112, 6.9112, 168.912956510431, 657270.9418419404, 657270.9418419404, 198148.5973182116], 
processed observation next is [0.0, 0.08695652173913043, 0.38388625592417064, 0.9033333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7337557701548666, 0.0, 0.0, 0.8294399451523027, 0.18257526162276122, 0.18257526162276122, 0.29574417510180834], 
reward next is 0.7043, 
noisyNet noise sample is [array([-0.9974734], dtype=float32), 2.9239025]. 
=============================================
[2019-04-10 13:35:58,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.992355]
 [69.20651 ]
 [69.43133 ]
 [69.76152 ]
 [69.74014 ]], R is [[68.91056824]
 [68.92489624]
 [68.93837738]
 [68.95098877]
 [68.96260834]].
[2019-04-10 13:36:00,388] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.3523987e-30 0.0000000e+00 1.7543031e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:36:00,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0906
[2019-04-10 13:36:00,405] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 912215.3782122113 W.
[2019-04-10 13:36:00,408] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 74.33333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.028733521318475, 6.9112, 168.9121497799014, 912215.3782122113, 828833.4512323686, 254812.5842178981], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3261000.0000, 
sim time next is 3261600.0000, 
raw observation next is [31.0, 75.0, 1.0, 1.0, 0.3061136572841469, 1.0, 1.0, 0.3061136572841469, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 855552.3859649258, 855552.3859649258, 251080.0706900471], 
processed observation next is [0.0, 0.782608695652174, 0.6682464454976303, 0.75, 1.0, 0.5, 0.16399235817367097, 1.0, 0.5, 0.16399235817367097, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23765344054581272, 0.23765344054581272, 0.37474637416424944], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00257723], dtype=float32), 0.7921915]. 
=============================================
[2019-04-10 13:36:00,706] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:36:00,713] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2925
[2019-04-10 13:36:00,716] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7668027587411408, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 195219.3994030775], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7667725735148422, 6.911200000000001, 6.9112, 168.912956510431, 646576.9607043526, 646576.960704352, 195213.4472166886], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7155763091644416, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17960471130676461, 0.17960471130676445, 0.2913633540547591], 
reward next is 0.7086, 
noisyNet noise sample is [array([1.8886833], dtype=float32), 0.5403162]. 
=============================================
[2019-04-10 13:36:01,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.3811333e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:36:01,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3284
[2019-04-10 13:36:01,299] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.33333333333334, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7865832341281436, 6.911199999999999, 6.9112, 168.912956510431, 660264.1175018304, 660264.1175018311, 199115.2262756518], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [26.5, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7885675623169377, 6.9112, 6.9112, 168.912956510431, 661553.9149877997, 661553.9149877997, 199509.393194493], 
processed observation next is [0.0, 0.30434782608695654, 0.4549763033175356, 0.815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7421555638011436, 0.0, 0.0, 0.8294399451523027, 0.1837649763854999, 0.1837649763854999, 0.2977752137231239], 
reward next is 0.7022, 
noisyNet noise sample is [array([1.4624932], dtype=float32), 0.6903817]. 
=============================================
[2019-04-10 13:36:02,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.6892646e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:36:02,958] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1303
[2019-04-10 13:36:02,963] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.16666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7452493770422063, 6.9112, 6.9112, 168.912956510431, 632396.6356465198, 632396.6356465198, 191086.0165868445], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7338571928123899, 6.9112, 6.9112, 168.912956510431, 624174.6248391629, 624174.6248391629, 188935.8480736673], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6754356009907193, 0.0, 0.0, 0.8294399451523027, 0.1733818402331008, 0.1733818402331008, 0.2819938030950258], 
reward next is 0.7180, 
noisyNet noise sample is [array([1.7791791], dtype=float32), 0.29619262]. 
=============================================
[2019-04-10 13:36:12,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 3.1740074e-15 2.1910037e-21 2.4730972e-16 1.1562873e-22], sum to 1.0000
[2019-04-10 13:36:12,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3985
[2019-04-10 13:36:12,241] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1031227.563142882 W.
[2019-04-10 13:36:12,245] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.737875377207314, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 1031227.563142882, 1031227.563142881, 229229.7553503346], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3466200.0000, 
sim time next is 3466800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.3624829590071501, 1.0, 1.0, 0.3624829590071501, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1013172.510181531, 1013172.510181531, 263080.8152546934], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.23190717952668688, 1.0, 0.5, 0.23190717952668688, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2814368083837586, 0.2814368083837586, 0.39265793321596026], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21324027], dtype=float32), -0.08622778]. 
=============================================
[2019-04-10 13:36:13,135] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.5687750e-14 1.5556715e-20 1.3914908e-15 1.3695060e-21], sum to 1.0000
[2019-04-10 13:36:13,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2189
[2019-04-10 13:36:13,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1064208.759012632 W.
[2019-04-10 13:36:13,152] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.7614626290215535, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565103226, 1064208.759012632, 1064208.759012632, 234668.146273025], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3465600.0000, 
sim time next is 3466200.0000, 
raw observation next is [26.83333333333333, 80.66666666666667, 1.0, 2.0, 0.3698932792310288, 1.0, 1.0, 0.3698932792310288, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1033895.00789171, 1033895.00789171, 264800.7912585587], 
processed observation next is [1.0, 0.08695652173913043, 0.470774091627172, 0.8066666666666668, 1.0, 1.0, 0.24083527618196243, 1.0, 0.5, 0.24083527618196243, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2871930577476972, 0.2871930577476972, 0.39522506157993836], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.755308], dtype=float32), -0.2502435]. 
=============================================
[2019-04-10 13:36:13,432] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.1782475e-15 1.3826565e-27 2.8505785e-18 1.5384519e-26], sum to 1.0000
[2019-04-10 13:36:13,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4408
[2019-04-10 13:36:13,445] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2417180.978668953 W.
[2019-04-10 13:36:13,452] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.83333333333333, 66.83333333333333, 1.0, 2.0, 0.576160851013581, 1.0, 2.0, 0.576160851013581, 1.0, 2.0, 1.000600858905549, 6.9112, 6.9112, 170.5573041426782, 2417180.978668953, 2417180.978668953, 471807.3987628666], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 1.022721051951651, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005994590514376, 6.9112, 168.9122927088433, 2326804.682188772, 2259554.401384019, 470364.7866766102], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 1.0273747613875313, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479459051437633, 0.0, 0.8294366855826174, 0.6463346339413256, 0.6276540003844496, 0.7020369950397167], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2783065], dtype=float32), -1.4885188]. 
=============================================
[2019-04-10 13:36:13,745] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.2780074e-19 3.1972303e-30 2.0783361e-22 5.1128804e-31], sum to 1.0000
[2019-04-10 13:36:13,750] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-10 13:36:13,755] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 894796.0072499972 W.
[2019-04-10 13:36:13,758] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.3201477330847989, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5351289919811727, 6.911200000000001, 6.9112, 168.912956510427, 894796.0072499972, 894796.0072499966, 226289.6018306132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3475800.0000, 
sim time next is 3476400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6365150963753605, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889510.827981483, 889510.827981483, 207735.5065535982], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5620663811751332, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2470863411059675, 0.2470863411059675, 0.31005299485611676], 
reward next is 0.6899, 
noisyNet noise sample is [array([0.00653487], dtype=float32), 0.7948823]. 
=============================================
[2019-04-10 13:36:17,553] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.7601563e-12 5.9757433e-22 8.8850707e-14 5.2839284e-21], sum to 1.0000
[2019-04-10 13:36:17,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8645
[2019-04-10 13:36:17,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1327016.298331054 W.
[2019-04-10 13:36:17,572] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.474694618723361, 0.0, 1.0, 0.0, 1.0, 1.0, 0.806305101810807, 6.9112, 6.9112, 168.912956510431, 1327016.298331054, 1327016.298331054, 291676.9695744252], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3569400.0000, 
sim time next is 3570000.0000, 
raw observation next is [28.66666666666666, 75.66666666666666, 1.0, 2.0, 0.4105423873931922, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6969691688041011, 6.911200000000001, 6.9112, 168.912956510431, 1147580.803269, 1147580.803269, 261797.5041980293], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.7566666666666666, 1.0, 1.0, 0.28981010529300266, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6304502058586599, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3187724453525, 0.3187724453525, 0.39074254357914817], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.9420571], dtype=float32), 0.36921132]. 
=============================================
[2019-04-10 13:36:17,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[34.178932]
 [33.95719 ]
 [33.83013 ]
 [33.837116]
 [34.85407 ]], R is [[34.4053154 ]
 [34.06126404]
 [33.72065353]
 [33.38344574]
 [33.64437485]].
[2019-04-10 13:36:18,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.4131705e-13 4.2532871e-24 2.0690330e-15 2.8695112e-23], sum to 1.0000
[2019-04-10 13:36:18,503] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6806
[2019-04-10 13:36:18,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2217016.365485 W.
[2019-04-10 13:36:18,516] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 68.0, 1.0, 2.0, 0.7927376480101345, 1.0, 1.0, 0.7927376480101345, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2217016.365485, 2217016.365485, 416351.6391955765], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [31.0, 68.66666666666667, 1.0, 2.0, 0.9798943879575781, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999983996251079, 6.9112, 168.9124284486321, 2266860.890016245, 2203874.66427684, 457406.439770391], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6866666666666668, 1.0, 1.0, 0.9757763710332267, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008878399625107924, 0.0, 0.8294373521271224, 0.6296835805600681, 0.6121874067435668, 0.6826961787617776], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2254943], dtype=float32), -0.18163691]. 
=============================================
[2019-04-10 13:36:18,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00000000e+00 1.14143945e-22 1.53854202e-31 1.26792666e-26
 5.00143000e-34], sum to 1.0000
[2019-04-10 13:36:18,810] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3383
[2019-04-10 13:36:18,815] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8898372136693607, 6.9112, 6.9112, 168.912956510431, 733054.007059291, 733054.007059291, 221025.6314635153], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [29.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8930307318146893, 6.9112, 6.9112, 168.912956510431, 735700.2414383213, 735700.2414383213, 221760.4063364363], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8695496729447431, 0.0, 0.0, 0.8294399451523027, 0.20436117817731148, 0.20436117817731148, 0.33098568109915866], 
reward next is 0.6690, 
noisyNet noise sample is [array([0.05445727], dtype=float32), -0.65193236]. 
=============================================
[2019-04-10 13:36:24,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 4.5649384e-13 3.0662933e-20 4.7082148e-15 2.2572705e-21], sum to 1.0000
[2019-04-10 13:36:24,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6960
[2019-04-10 13:36:24,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2037145.998450572 W.
[2019-04-10 13:36:24,667] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.485654906433422, 1.0, 1.0, 0.485654906433422, 1.0, 2.0, 0.825545538148841, 6.911199999999999, 6.9112, 170.5573041426782, 2037145.998450572, 2037145.998450573, 402199.6692984536], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3745800.0000, 
sim time next is 3746400.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.8413126478896739, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.972734327678475, 6.9112, 168.9125898301579, 2072893.634452303, 2029239.159315462, 419428.8469903948], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.8088104191441854, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006153432767847455, 0.0, 0.8294381445842949, 0.575803787347862, 0.563677544254295, 0.6260132044632758], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7261575], dtype=float32), -0.085575975]. 
=============================================
[2019-04-10 13:36:30,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 4.017894e-31 0.000000e+00 9.620890e-34 0.000000e+00], sum to 1.0000
[2019-04-10 13:36:30,784] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0315
[2019-04-10 13:36:30,794] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8772467121052163, 6.911199999999999, 6.9112, 168.912956510431, 724528.1909779891, 724528.1909779897, 218226.2056723965], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3801000.0000, 
sim time next is 3801600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8784714821752678, 6.911199999999999, 6.9112, 168.912956510431, 725540.089293811, 725540.0892938116, 218503.4794100129], 
processed observation next is [0.0, 0.0, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8517944904576435, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20153891369272528, 0.20153891369272545, 0.3261245961343476], 
reward next is 0.6739, 
noisyNet noise sample is [array([-0.6137971], dtype=float32), -1.2671643]. 
=============================================
[2019-04-10 13:36:31,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 5.5831226e-25 8.0435589e-33 2.2827218e-27 2.6712404e-32], sum to 1.0000
[2019-04-10 13:36:31,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8358
[2019-04-10 13:36:31,899] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 63.00000000000001, 1.0, 2.0, 0.3095269939159799, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5375460262909839, 6.911200000000001, 6.9112, 168.9129564739309, 865099.5127568803, 865099.5127568797, 225082.8697117409], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [34.0, 63.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.128461669879709, 6.9112, 168.9116283817094, 982992.7534075341, 828861.059141428, 254813.2915350237], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.021726166987970873, 0.0, 0.8294334234322597, 0.2730535426132039, 0.2302391830948411, 0.3803183455746622], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7902743], dtype=float32), -0.006565158]. 
=============================================
[2019-04-10 13:36:31,932] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.273167]
 [57.524334]
 [57.3287  ]
 [57.474865]
 [57.214973]], R is [[57.82003021]
 [57.24182892]
 [56.66941071]
 [56.10271835]
 [56.1369133 ]].
[2019-04-10 13:36:32,163] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-10 13:36:32,164] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:36:32,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:36:32,175] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:36:32,175] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:36:32,176] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:36:32,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:36:32,177] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:36:32,178] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:36:32,178] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:36:32,188] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-04-10 13:36:32,188] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-04-10 13:36:32,180] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-04-10 13:36:32,242] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-04-10 13:36:32,305] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:36:32,307] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-04-10 13:36:40,734] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:36:40,735] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.5, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4077364064106045, 6.9112, 6.9112, 168.912956510431, 367878.0752335408, 367878.0752335408, 141188.1098985275]
[2019-04-10 13:36:40,735] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:36:40,736] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 4.3961322e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.6440970259632133
[2019-04-10 13:36:45,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:36:45,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.97083894333333, 74.34656502333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6707772024360853, 6.911199999999999, 6.9112, 168.912956510431, 576357.7816009446, 576357.7816009453, 177604.6148177224]
[2019-04-10 13:36:45,876] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-10 13:36:45,880] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.7239444e-33 0.0000000e+00 6.8774125e-36 0.0000000e+00], sampled 0.3794810987615931
[2019-04-10 13:37:00,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:37:00,090] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.482932135, 76.56120813000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8788696608049501, 6.911199999999999, 6.9112, 168.912956510431, 728516.6079055374, 728516.607905538, 218686.0692321937]
[2019-04-10 13:37:00,091] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:37:00,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 5.5231383e-30 6.5066789e-38 2.2787920e-32 0.0000000e+00], sampled 0.42633127581815533
[2019-04-10 13:37:01,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:37:01,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.58333333333334, 81.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9501960408789074, 6.9112, 6.9112, 168.912956510431, 795989.8942661001, 795989.8942661001, 235806.8253939015]
[2019-04-10 13:37:01,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:37:01,901] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.9006576e-31 0.0000000e+00 7.0082505e-34 0.0000000e+00], sampled 0.50885668460897
[2019-04-10 13:37:13,570] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:37:13,572] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.46710767666667, 64.05761580166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004040570484515, 6.9112, 6.9112, 168.9129269447596, 807904.4293443821, 807904.4293443821, 248011.610165035]
[2019-04-10 13:37:13,573] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:37:13,575] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.7261888e-31 0.0000000e+00 1.1749691e-33 0.0000000e+00], sampled 0.32155841926243833
[2019-04-10 13:37:20,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:37:20,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 62.5, 1.0, 1.0, 0.6484491134985657, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9127910389482, 906195.3755901298, 906195.3755901298, 210117.8933960886]
[2019-04-10 13:37:20,848] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:37:20,851] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 8.0099267e-21 1.3797625e-29 4.3794064e-23 1.4939356e-28], sampled 0.18212170583155052
[2019-04-10 13:37:20,852] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 906195.3755901298 W.
[2019-04-10 13:38:00,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.13715494], dtype=float32), 0.030637728]
[2019-04-10 13:38:00,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.73217886333333, 92.20917051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6658249899028739, 6.9112, 6.9112, 168.912956510431, 575476.0153164268, 575476.0153164268, 176738.9629525101]
[2019-04-10 13:38:00,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:38:00,890] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.5078122e-33 0.0000000e+00 1.8860353e-35 0.0000000e+00], sampled 0.1754953360480357
[2019-04-10 13:38:03,611] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.9116 2989365571.8530 1566.0000
[2019-04-10 13:38:03,932] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-04-10 13:38:03,951] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.3661 2937831822.1909 1381.0000
[2019-04-10 13:38:03,955] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8072 3185213016.4429 2464.0000
[2019-04-10 13:38:03,983] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.9449 3105789227.8021 2010.0000
[2019-04-10 13:38:04,995] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 950000, evaluation results [950000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7346.9449437439125, 3105789227.8021092, 2010.0, 8062.366084066912, 2937831822.1909375, 1381.0, 7029.807235593465, 3185213016.4429317, 2464.0, 7925.911591021176, 2989365571.853049, 1566.0]
[2019-04-10 13:38:15,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:38:15,549] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9053
[2019-04-10 13:38:15,552] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.66666666666667, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9913396479019314, 6.911200000000001, 6.9112, 168.912956510431, 797680.7541271225, 797680.7541271218, 244750.0589657251], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [29.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9851231945757932, 6.9112, 6.9112, 168.9129362947223, 793930.7021628711, 793930.7021628711, 243241.3937315949], 
processed observation next is [1.0, 0.782608695652174, 0.5971563981042655, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9818575543607234, 0.0, 0.0, 0.8294398458839182, 0.22053630615635308, 0.22053630615635308, 0.3630468563158133], 
reward next is 0.6370, 
noisyNet noise sample is [array([-0.6629022], dtype=float32), -0.29709965]. 
=============================================
[2019-04-10 13:38:15,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.6889  ]
 [57.176884]
 [57.70627 ]
 [58.084297]
 [57.40342 ]], R is [[56.52223587]
 [56.59171295]
 [56.6578064 ]
 [56.72945023]
 [56.80495071]].
[2019-04-10 13:38:19,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.6453089e-31 0.0000000e+00 1.1210585e-37 0.0000000e+00], sum to 1.0000
[2019-04-10 13:38:19,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0222
[2019-04-10 13:38:19,158] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 76.33333333333334, 1.0, 2.0, 0.6062289039067252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564687603, 847169.8993356868, 847169.8993356862, 201912.3869288426], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4131600.0000, 
sim time next is 4132200.0000, 
raw observation next is [31.0, 75.66666666666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.039191977335894, 6.9112, 168.9120974499957, 919637.7984372952, 828836.3463732671, 254812.6614990559], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.7566666666666666, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.012799197733589373, 0.0, 0.8294357267722858, 0.2554549440103598, 0.23023231843701863, 0.3803174052224715], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.93407416], dtype=float32), -0.788447]. 
=============================================
[2019-04-10 13:38:25,188] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.8490553e-20 5.3283711e-31 4.0773961e-26 8.1885212e-38], sum to 1.0000
[2019-04-10 13:38:25,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4856
[2019-04-10 13:38:25,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1404191.570918122 W.
[2019-04-10 13:38:25,203] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.16666666666667, 79.00000000000001, 1.0, 2.0, 0.5022863833425041, 1.0, 1.0, 0.5022863833425041, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1404191.570918122, 1404191.570918122, 300969.1364479632], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4255800.0000, 
sim time next is 4256400.0000, 
raw observation next is [29.33333333333334, 79.0, 1.0, 2.0, 0.4587700885818097, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7932926093253138, 6.9112, 6.9112, 168.912956510431, 1282472.134894622, 1282472.134894622, 286227.5991093137], 
processed observation next is [1.0, 0.2608695652173913, 0.5892575039494474, 0.79, 1.0, 1.0, 0.34791576937567426, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.7479178162503826, 0.0, 0.0, 0.8294399451523027, 0.35624225969295054, 0.35624225969295054, 0.4272053718049458], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.85184467], dtype=float32), 0.7177229]. 
=============================================
[2019-04-10 13:38:27,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9943417e-01 5.5571715e-04 1.0084591e-11 1.0027971e-05 2.0949412e-11], sum to 1.0000
[2019-04-10 13:38:27,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1173
[2019-04-10 13:38:27,965] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 84.0, 1.0, 1.0, 1.04, 1.0, 1.0, 1.04, 1.0, 2.0, 1.03, 9.80178993257022, 6.9112, 170.5573041426782, 5813877.828547591, 3743230.678166792, 686745.9880927256], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4328400.0000, 
sim time next is 4329000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.7043473858596048, 1.0, 2.0, 0.6727637324440648, 1.0, 2.0, 1.03, 7.005098074945807, 6.9112, 170.5573041426782, 2822919.137813124, 2755656.127199959, 522706.516459177], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.84, 1.0, 1.0, 0.6437920311561504, 1.0, 1.0, 0.6057394366795962, 1.0, 1.0, 1.0365853658536586, 0.009389807494580715, 0.0, 0.8375144448122397, 0.78414420494809, 0.765460035333322, 0.7801589797898164], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2707417], dtype=float32), 2.2591097]. 
=============================================
[2019-04-10 13:38:27,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[12.391352]
 [ 9.026918]
 [10.48793 ]
 [10.423037]
 [10.373509]], R is [[11.90838623]
 [11.78930283]
 [11.67140961]
 [12.17875481]
 [12.05696774]].
[2019-04-10 13:38:31,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 6.6681857e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:38:31,734] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6921
[2019-04-10 13:38:31,738] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9702149386570234, 6.9112, 6.9112, 168.912956510431, 780676.5250469598, 780676.5250469598, 239421.0102528523], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4385400.0000, 
sim time next is 4386000.0000, 
raw observation next is [32.0, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9678206105329066, 6.9112, 6.9112, 168.912956510431, 779160.2719094914, 779160.2719094914, 238847.8486058726], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9607568421133007, 0.0, 0.0, 0.8294399451523027, 0.2164334088637476, 0.2164334088637476, 0.3564893262774218], 
reward next is 0.6435, 
noisyNet noise sample is [array([0.59326553], dtype=float32), -1.9476433]. 
=============================================
[2019-04-10 13:38:31,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[21.145851]
 [21.831753]
 [22.246155]
 [22.33715 ]
 [20.371788]], R is [[21.04155731]
 [21.47379684]
 [21.90225601]
 [22.32850456]
 [22.75751305]].
[2019-04-10 13:38:36,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.3078120e-15 1.3142498e-22 1.1013294e-18 3.7601748e-25], sum to 1.0000
[2019-04-10 13:38:36,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-10 13:38:36,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 878317.4726574152 W.
[2019-04-10 13:38:36,014] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666666, 76.33333333333334, 1.0, 2.0, 0.3142555917819282, 1.0, 1.0, 0.3142555917819282, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 878317.4726574152, 878317.4726574152, 252707.1810026189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4441200.0000, 
sim time next is 4441800.0000, 
raw observation next is [31.83333333333333, 75.66666666666666, 1.0, 2.0, 0.6287313265793519, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 878628.7429249336, 878628.742924933, 206221.4469450176], 
processed observation next is [0.0, 0.391304347826087, 0.7077409162717218, 0.7566666666666666, 1.0, 1.0, 0.5526883452763276, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24406353970137043, 0.24406353970137026, 0.3077932043955487], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9375549], dtype=float32), 1.180103]. 
=============================================
[2019-04-10 13:38:39,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.000000e+00 4.117577e-36 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-10 13:38:39,551] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5131
[2019-04-10 13:38:39,584] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8550660566910293, 6.9112, 6.9112, 168.912956510431, 710950.5380309998, 710950.5380309998, 213430.9201677161], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4492800.0000, 
sim time next is 4493400.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8533758678463992, 6.911199999999999, 6.9112, 168.912956510431, 709827.6556061554, 709827.655606156, 213067.0690418287], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8211900827395112, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1971743487794876, 0.19717434877948778, 0.31801055080869955], 
reward next is 0.6820, 
noisyNet noise sample is [array([0.4175477], dtype=float32), 2.3018498]. 
=============================================
[2019-04-10 13:38:45,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 2.561118e-32 0.000000e+00 5.931641e-36 0.000000e+00], sum to 1.0000
[2019-04-10 13:38:45,332] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-10 13:38:45,341] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.895534541365902, 6.911199999999999, 6.9112, 168.912956510431, 738384.2427864469, 738384.2427864476, 222361.6566921381], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8966678241278508, 6.911200000000001, 6.9112, 168.912956510431, 739630.018258668, 739630.0182586674, 222635.4289973694], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8739851513754278, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20545278284963, 0.20545278284962984, 0.3322916850707006], 
reward next is 0.6677, 
noisyNet noise sample is [array([-2.5033495], dtype=float32), -1.720699]. 
=============================================
[2019-04-10 13:38:59,852] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-10 13:38:59,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:38:59,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:38:59,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-04-10 13:38:59,952] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:38:59,953] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:38:59,954] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-04-10 13:39:00,062] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:39:00,062] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:39:00,064] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-04-10 13:39:00,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:39:00,125] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:39:00,127] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-04-10 13:39:00,182] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:39:00,183] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:39:00,184] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-04-10 13:39:17,360] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.11885935], dtype=float32), 0.033460863]
[2019-04-10 13:39:17,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.38333333333334, 61.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5040738611338629, 6.9112, 6.9112, 168.912956510431, 449633.2796965032, 449633.2796965032, 152332.1404652748]
[2019-04-10 13:39:17,361] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:39:17,364] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.3497805e-30 0.0000000e+00 1.7210715e-35 0.0000000e+00], sampled 0.43948345973651726
[2019-04-10 13:39:20,516] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.11885935], dtype=float32), 0.033460863]
[2019-04-10 13:39:20,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.76666666666667, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5034527762105482, 6.911199999999999, 6.9112, 168.912956510431, 447064.2469335308, 447064.2469335314, 152367.7821315341]
[2019-04-10 13:39:20,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:39:20,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.0914563e-32 0.0000000e+00 2.4181331e-37 0.0000000e+00], sampled 0.6611907124171774
[2019-04-10 13:39:24,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.11885935], dtype=float32), 0.033460863]
[2019-04-10 13:39:24,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.2935136, 83.22477437500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6480461434522282, 6.9112, 6.9112, 168.912956510431, 566661.5076732046, 566661.5076732046, 173647.4149627673]
[2019-04-10 13:39:24,925] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:39:24,927] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.6152543e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.7837851722399617
[2019-04-10 13:40:24,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.11885935], dtype=float32), 0.033460863]
[2019-04-10 13:40:24,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.46232761333333, 93.32213709666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6584755388366071, 6.9112, 6.9112, 168.912956510431, 568982.9182902041, 568982.9182902041, 175497.7582617593]
[2019-04-10 13:40:24,370] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:40:24,372] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.6842306e-30 0.0000000e+00 4.2166239e-35 0.0000000e+00], sampled 0.8253836604357299
[2019-04-10 13:40:26,192] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.11885935], dtype=float32), 0.033460863]
[2019-04-10 13:40:26,192] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.109406495, 90.09012929833335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5001860531162862, 6.9112, 6.9112, 168.912956510431, 444129.4514016839, 444129.4514016839, 151961.5652244679]
[2019-04-10 13:40:26,194] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-10 13:40:26,197] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.4886089e-32 0.0000000e+00 4.9279977e-37 0.0000000e+00], sampled 0.4157207193813536
[2019-04-10 13:40:31,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-04-10 13:40:31,685] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.7256 2937840706.0387 1381.0000
[2019-04-10 13:40:31,752] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.8131 2989431545.6547 1566.0000
[2019-04-10 13:40:31,779] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.3240 3105698253.2214 2010.0000
[2019-04-10 13:40:31,795] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.6128 3185090319.5018 2464.0000
[2019-04-10 13:40:32,811] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 975000, evaluation results [975000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7348.32396249469, 3105698253.2213993, 2010.0, 8061.7256276379185, 2937840706.0387306, 1381.0, 7030.612784376717, 3185090319.501788, 2464.0, 7925.813122660512, 2989431545.654693, 1566.0]
[2019-04-10 13:40:32,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.1954467e-28 9.7660425e-38 4.4250657e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:40:32,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-10 13:40:32,968] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8107513109575795, 6.911200000000001, 6.9112, 168.912956510431, 678785.2714381646, 678785.2714381639, 204046.106276189], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4756800.0000, 
sim time next is 4757400.0000, 
raw observation next is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8096245474758068, 6.9112, 6.9112, 168.912956510431, 677856.7945549953, 677856.7945549953, 203811.2978450501], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7678348139948864, 0.0, 0.0, 0.8294399451523027, 0.18829355404305423, 0.18829355404305423, 0.3041959669329106], 
reward next is 0.6958, 
noisyNet noise sample is [array([0.23357634], dtype=float32), -0.04545433]. 
=============================================
[2019-04-10 13:40:42,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 3.3302102e-26 2.0453253e-38 1.5721633e-32 0.0000000e+00], sum to 1.0000
[2019-04-10 13:40:42,439] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8999
[2019-04-10 13:40:42,446] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.021780971514833, 6.911200000000001, 6.9112, 168.9128321657967, 849312.6790699731, 849312.6790699725, 253937.8710993385], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4942800.0000, 
sim time next is 4943400.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.768967675331613, 6.9112, 168.9082952872524, 1465469.659751416, 856956.5414217736, 256148.9929512611], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.08577676753316128, 0.0, 0.8294170564125786, 0.40707490548650443, 0.23804348372827044, 0.38231192977800166], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05031656], dtype=float32), 1.7213734]. 
=============================================
[2019-04-10 13:40:51,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-04-10 13:40:51,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0383
[2019-04-10 13:40:51,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.33333333333334, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8802517230874716, 6.9112, 6.9112, 168.912956510431, 726903.3764013159, 726903.3764013159, 218903.3914011771], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5125200.0000, 
sim time next is 5125800.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8842013386406585, 6.9112, 6.9112, 168.912956510431, 729495.5083918261, 729495.5083918261, 219777.5145121851], 
processed observation next is [0.0, 0.30434782608695654, 0.5576619273301741, 0.7566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8587821202934859, 0.0, 0.0, 0.8294399451523027, 0.20263764121995168, 0.20263764121995168, 0.32802614106296285], 
reward next is 0.6720, 
noisyNet noise sample is [array([-0.58414406], dtype=float32), 1.5149925]. 
=============================================
[2019-04-10 13:40:58,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.1240192e-17 3.8084674e-30 2.2423693e-23 4.1100528e-32], sum to 1.0000
[2019-04-10 13:40:58,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4036
[2019-04-10 13:40:58,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2405206.124426011 W.
[2019-04-10 13:40:58,778] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.8599638917945764, 1.0, 1.0, 0.8599638917945764, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2405206.124426011, 2405206.124426011, 450113.496748348], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5214600.0000, 
sim time next is 5215200.0000, 
raw observation next is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.766329414954057, 1.0, 2.0, 0.766329414954057, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2143088.303316475, 2143088.303316475, 403808.4518970348], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590839, 0.7133333333333333, 1.0, 1.0, 0.7184691746434422, 1.0, 1.0, 0.7184691746434422, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5953023064767986, 0.5953023064767986, 0.6026991819358729], 
reward next is 0.3973, 
noisyNet noise sample is [array([0.41683334], dtype=float32), -0.28384084]. 
=============================================
[2019-04-10 13:41:02,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 6.8608745e-22 1.9248572e-34 5.4298460e-28 1.7147451e-38], sum to 1.0000
[2019-04-10 13:41:02,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1953
[2019-04-10 13:41:02,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2704608.176883997 W.
[2019-04-10 13:41:02,595] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.45, 65.0, 1.0, 2.0, 0.966897175088812, 1.0, 2.0, 0.966897175088812, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2704608.176883997, 2704608.176883997, 509310.7112384333], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5304600.0000, 
sim time next is 5305200.0000, 
raw observation next is [33.76666666666667, 63.33333333333334, 1.0, 2.0, 0.6714210168635087, 1.0, 2.0, 0.656300547946017, 1.0, 1.0, 1.03, 7.005095478837973, 6.9112, 170.5573041426782, 2753763.415516179, 2686502.264600658, 512454.7566534814], 
processed observation next is [1.0, 0.391304347826087, 0.7993680884676149, 0.6333333333333334, 1.0, 1.0, 0.6041217070644683, 1.0, 1.0, 0.5859042746337554, 1.0, 0.5, 1.0365853658536586, 0.009389547883797266, 0.0, 0.8375144448122397, 0.7649342820878275, 0.7462506290557382, 0.7648578457514648], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7289382], dtype=float32), -1.2846334]. 
=============================================
[2019-04-10 13:41:06,691] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.7617142e-24 2.3307383e-35 2.3668508e-31 0.0000000e+00], sum to 1.0000
[2019-04-10 13:41:06,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9273
[2019-04-10 13:41:06,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1464802.820257361 W.
[2019-04-10 13:41:06,703] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.96666666666667, 83.0, 1.0, 2.0, 0.5239525277194099, 1.0, 1.0, 0.5239525277194099, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1464802.820257361, 1464802.820257361, 307876.0562215254], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5296800.0000, 
sim time next is 5297400.0000, 
raw observation next is [30.1, 82.5, 1.0, 2.0, 0.3514311253095834, 1.0, 2.0, 0.3514311253095834, 1.0, 1.0, 0.61031964461366, 6.9112, 6.9112, 170.5573041426782, 1473738.804466259, 1473738.804466259, 327633.6710511292], 
processed observation next is [1.0, 0.30434782608695654, 0.6255924170616115, 0.825, 1.0, 1.0, 0.21859171724046195, 1.0, 1.0, 0.21859171724046195, 1.0, 0.5, 0.5247800544069023, 0.0, 0.0, 0.8375144448122397, 0.40937189012951636, 0.40937189012951636, 0.4890054791807898], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.33897537], dtype=float32), 0.43334043]. 
=============================================
[2019-04-10 13:41:21,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 4.5915317e-19 4.6804473e-27 8.6054274e-23 5.4275211e-30], sum to 1.0000
[2019-04-10 13:41:21,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-10 13:41:21,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1283344.573616814 W.
[2019-04-10 13:41:21,558] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.13333333333334, 95.0, 1.0, 2.0, 0.9181639833553138, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 1283344.573616814, 1283344.573616814, 274982.9429026053], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5539800.0000, 
sim time next is 5540400.0000, 
raw observation next is [26.1, 95.0, 1.0, 2.0, 0.4558814839256226, 1.0, 1.0, 0.4558814839256226, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1274384.94403826, 1274384.94403826, 287119.9917640667], 
processed observation next is [1.0, 0.13043478260869565, 0.4360189573459717, 0.95, 1.0, 1.0, 0.344435522801955, 1.0, 0.5, 0.344435522801955, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.35399581778840555, 0.35399581778840555, 0.428537301140398], 
reward next is 0.5715, 
noisyNet noise sample is [array([0.35817778], dtype=float32), -0.22454825]. 
=============================================
[2019-04-10 13:41:24,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.1995992e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-10 13:41:24,465] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3189
[2019-04-10 13:41:24,472] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.68333333333333, 91.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8528267697486961, 6.911200000000001, 6.9112, 168.912956510431, 708456.9348052742, 708456.9348052735, 212918.3837357661], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5627400.0000, 
sim time next is 5628000.0000, 
raw observation next is [25.66666666666667, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8511840965991883, 6.911199999999999, 6.9112, 168.912956510431, 707460.1998255189, 707460.1998255195, 212568.8109650466], 
processed observation next is [0.0, 0.13043478260869565, 0.4154818325434442, 0.9166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8185171909746197, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19651672217375524, 0.1965167221737554, 0.317266882037383], 
reward next is 0.6827, 
noisyNet noise sample is [array([1.4371097], dtype=float32), -0.5569498]. 
=============================================
[2019-04-10 13:41:24,481] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.10675 ]
 [72.8831  ]
 [72.57034 ]
 [72.424706]
 [72.3978  ]], R is [[73.02868652]
 [72.98061371]
 [72.93235016]
 [72.88360596]
 [72.83434296]].
[2019-04-10 13:41:29,414] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-10 13:41:29,416] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-10 13:41:29,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:41:29,417] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-10 13:41:29,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-10 13:41:29,419] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:41:29,420] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:41:29,420] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-10 13:41:29,419] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-10 13:41:29,421] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:41:29,422] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-10 13:41:29,885] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-04-10 13:41:29,998] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-04-10 13:41:30,009] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-04-10 13:41:30,009] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-04-10 13:41:30,498] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/9/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-04-10 13:41:52,550] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.11780936], dtype=float32), 0.030031297]
[2019-04-10 13:41:52,551] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.8, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6938466520856341, 6.911199999999999, 6.9112, 168.912956510431, 595420.8676647046, 595420.8676647051, 181632.0127204149]
[2019-04-10 13:41:52,551] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:41:52,553] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.00000e+00 2.24577e-36 0.00000e+00 0.00000e+00 0.00000e+00], sampled 0.20847657979872758
[2019-04-10 13:41:54,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.11780936], dtype=float32), 0.030031297]
[2019-04-10 13:41:54,640] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8429182618514572, 6.9112, 6.9112, 168.912956510431, 702106.7860821306, 702106.7860821306, 210808.5303965013]
[2019-04-10 13:41:54,641] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-10 13:41:54,641] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.3942084e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sampled 0.19982114575759857
[2019-04-10 13:42:13,625] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.11780936], dtype=float32), 0.030031297]
[2019-04-10 13:42:13,625] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.1, 79.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9789985059451112, 6.9112, 6.9112, 168.912956510431, 794407.0883997601, 794407.0883997601, 241988.3935973843]
[2019-04-10 13:42:13,626] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-10 13:42:13,627] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.5935743e-32 0.0000000e+00 9.5484780e-38 0.0000000e+00], sampled 0.2695447943421283
[2019-04-10 13:42:29,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.11780936], dtype=float32), 0.030031297]
[2019-04-10 13:42:29,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.73333333333333, 62.66666666666667, 1.0, 2.0, 0.9406089068300666, 1.0, 2.0, 0.7908944929292959, 1.0, 2.0, 1.03, 7.005116711618896, 6.9112, 170.5573041426782, 3319254.917998809, 3251978.557178619, 608107.8891687932]
[2019-04-10 13:42:29,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-10 13:42:29,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.7004474e-30 0.0000000e+00 3.5265982e-38 0.0000000e+00], sampled 0.5883366051043134
[2019-04-10 13:42:29,947] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3319254.917998809 W.
[2019-04-10 13:42:58,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0039 2937825084.7326 1381.0000
[2019-04-10 13:42:58,673] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4827 3319593200.0117 2143.0000
[2019-04-10 13:42:58,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.3455 2989319888.0117 1566.0000
[2019-04-10 13:42:58,834] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.5353 3185150361.5388 2464.0000
[2019-04-10 13:42:59,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6660 3105588197.8702 2010.0000
[2019-04-10 13:43:00,171] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1000000, evaluation results [1000000.0, 7287.482731418222, 3319593200.01172, 2143.0, 7347.665987012129, 3105588197.8702025, 2010.0, 8061.003877585687, 2937825084.732598, 1381.0, 7030.535304348862, 3185150361.5388017, 2464.0, 7925.34552522051, 2989319888.0117445, 1566.0]
