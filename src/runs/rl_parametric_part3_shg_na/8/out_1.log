Using TensorFlow backend.
[2019-04-23 09:31:55,720] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_add_time_to_state=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_r_term_zero=True, is_warm_start=True, job_mode='Train', learning_rate=0.0005, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='Part3-NA-Shg-Train-v1-res1/model_data/model.ckpt-2500000', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res2', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-04-23 09:31:55,720] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-23 09:31:55.758602: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-23 09:32:11,784] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-23 09:32:11,784] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-04-23 09:32:11,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,806] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,817] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,820] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-04-23 09:32:11,820] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:11,820] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-23 09:32:11,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:11,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-04-23 09:32:12,821] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:12,823] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-23 09:32:12,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:12,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-04-23 09:32:13,709] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 09:32:13,710] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:32:13,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:32:13,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,711] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:32:13,712] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,712] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:32:13,713] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:32:13,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,713] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,714] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,718] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,728] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,748] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,749] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run1
[2019-04-23 09:32:13,823] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:13,824] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-23 09:32:13,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:13,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-04-23 09:32:14,825] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:14,826] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-23 09:32:14,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:14,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-04-23 09:32:15,827] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:15,828] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-23 09:32:15,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:15,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-04-23 09:32:16,829] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:16,830] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-23 09:32:16,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:16,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-04-23 09:32:17,831] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:17,832] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-23 09:32:17,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:17,917] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-04-23 09:32:18,833] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:18,834] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-23 09:32:18,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:18,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-04-23 09:32:19,835] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:19,841] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-23 09:32:19,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:19,948] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-04-23 09:32:20,842] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:20,843] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-23 09:32:20,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:20,930] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-04-23 09:32:21,844] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:21,845] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-23 09:32:21,936] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:21,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-04-23 09:32:22,845] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:22,848] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-23 09:32:22,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:22,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-04-23 09:32:23,849] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:23,850] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-23 09:32:23,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:23,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-04-23 09:32:24,851] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:24,853] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-23 09:32:24,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:24,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-04-23 09:32:25,853] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:25,854] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-23 09:32:25,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:25,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-04-23 09:32:26,855] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-23 09:32:26,856] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-23 09:32:26,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:32:26,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run1
[2019-04-23 09:32:28,989] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:32:28,992] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.02409662, 58.63889941666667, 1.0, 2.0, 0.2665326129188206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 444911.7061856221, 444911.7061856221, 161793.5920467583]
[2019-04-23 09:32:28,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:32:28,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2414902947097438
[2019-04-23 09:32:55,691] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:32:55,692] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.8, 70.66666666666667, 1.0, 2.0, 0.526767664461989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773855.2089769847, 773855.2089769847, 192635.6912338451]
[2019-04-23 09:32:55,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:32:55,696] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.21173565e-32
 0.00000000e+00], sampled 0.9547263456179831
[2019-04-23 09:33:03,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:33:03,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3977676784717663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591824.6310228049, 591824.6310228043, 173734.0849522283]
[2019-04-23 09:33:03,066] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:33:03,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6748005e-31 0.0000000e+00], sampled 0.9014384822302065
[2019-04-23 09:33:41,867] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.10276632]
[2019-04-23 09:33:41,868] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.55, 80.5, 1.0, 2.0, 0.5446991173061282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761154.7657538848, 761154.7657538854, 190895.9316898881]
[2019-04-23 09:33:41,870] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:33:41,876] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4718773e-26 0.0000000e+00], sampled 0.7871940042631167
[2019-04-23 09:34:23,945] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8346.4557 2972003103.9871 861.0000
[2019-04-23 09:34:24,113] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8389.3026 2914565027.0313 1009.0000
[2019-04-23 09:34:24,141] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8677.6168 2824168262.5245 658.0000
[2019-04-23 09:34:24,172] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8209.1487 3130966285.8621 920.0000
[2019-04-23 09:34:24,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8744.4570 2770794475.5613 713.0000
[2019-04-23 09:34:25,482] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 8209.148728965183, 3130966285.86209, 920.0, 8389.302619220314, 2914565027.0313025, 1009.0, 8744.456997317853, 2770794475.561282, 713.0, 8346.455683757804, 2972003103.987141, 861.0, 8677.61684504349, 2824168262.5245023, 658.0]
[2019-04-23 09:34:30,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.3936308e-11 7.6080286e-03 6.5650731e-17 4.2846400e-02 9.4954550e-01], sum to 1.0000
[2019-04-23 09:34:30,530] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3473
[2019-04-23 09:34:30,628] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.83333333333334, 63.0, 1.0, 2.0, 0.4990372728351632, 1.0, 1.0, 0.4990372728351632, 1.0, 1.0, 0.8357239692492493, 6.9112, 6.9112, 170.5573041426782, 2112533.298487618, 2112533.298487618, 410342.9520670532], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [27.75, 63.5, 1.0, 2.0, 0.4407515185565417, 1.0, 2.0, 0.4407515185565417, 1.0, 2.0, 0.7435445528367174, 6.9112, 6.9112, 170.5573041426782, 1885272.804369802, 1885272.804369802, 376167.7621229931], 
processed observation next is [1.0, 0.6086956521739131, 0.514218009478673, 0.635, 1.0, 1.0, 0.32620664886330325, 1.0, 1.0, 0.32620664886330325, 1.0, 1.0, 0.6872494546789235, 0.0, 0.0, 0.8375144448122397, 0.5236868901027227, 0.5236868901027227, 0.5614444210790942], 
reward next is 0.4386, 
noisyNet noise sample is [array([0.2075265], dtype=float32), 1.0416876]. 
=============================================
[2019-04-23 09:34:38,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5845407e-19 0.0000000e+00], sum to 1.0000
[2019-04-23 09:34:38,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9677
[2019-04-23 09:34:38,294] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 96.0, 1.0, 2.0, 0.3303072886808421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520174.9095118733, 520174.9095118727, 168524.7694924881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [20.8, 96.0, 1.0, 2.0, 0.3263997564430149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515268.4718229907, 515268.4718229913, 168168.4146442934], 
processed observation next is [1.0, 0.9565217391304348, 0.1848341232227489, 0.96, 1.0, 1.0, 0.1884334414976083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14313013106194186, 0.14313013106194203, 0.2509976337974528], 
reward next is 0.7490, 
noisyNet noise sample is [array([0.43457466], dtype=float32), -0.9041791]. 
=============================================
[2019-04-23 09:34:38,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[99.55236 ]
 [99.37462 ]
 [99.19079 ]
 [98.99852 ]
 [98.651375]], R is [[99.48912811]
 [99.24271393]
 [98.99836731]
 [98.75645447]
 [98.51647186]].
[2019-04-23 09:34:38,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.832436e-22 0.000000e+00], sum to 1.0000
[2019-04-23 09:34:38,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-23 09:34:38,707] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 96.0, 1.0, 2.0, 0.3043500634891076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485130.2792367901, 485130.2792367901, 165997.3618849702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 172800.0000, 
sim time next is 173400.0000, 
raw observation next is [20.36666666666667, 96.0, 1.0, 2.0, 0.3031448632281573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483559.4951141838, 483559.4951141845, 165888.6869817479], 
processed observation next is [0.0, 0.0, 0.1642969984202214, 0.96, 1.0, 1.0, 0.16041549786524978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13432208197616216, 0.13432208197616236, 0.24759505519663863], 
reward next is 0.7524, 
noisyNet noise sample is [array([-0.17055087], dtype=float32), 1.5119392]. 
=============================================
[2019-04-23 09:34:43,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:43,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1883
[2019-04-23 09:34:43,978] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 91.0, 1.0, 2.0, 0.2845031306979766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457763.0814410559, 457763.0814410559, 164120.3348613558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 255000.0000, 
sim time next is 255600.0000, 
raw observation next is [20.5, 91.0, 1.0, 2.0, 0.2833729599606403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456234.4313794903, 456234.4313794903, 164017.6541141958], 
processed observation next is [0.0, 1.0, 0.1706161137440759, 0.91, 1.0, 1.0, 0.13659392766342207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12673178649430286, 0.12673178649430286, 0.2448024688271579], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.37735736], dtype=float32), -0.8226924]. 
=============================================
[2019-04-23 09:34:46,049] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:46,057] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2602
[2019-04-23 09:34:46,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:46,117] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9963
[2019-04-23 09:34:46,165] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 79.0, 1.0, 2.0, 0.3031729866589544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480976.1166820957, 480976.1166820964, 165660.2373612459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 295200.0000, 
sim time next is 295800.0000, 
raw observation next is [22.86666666666667, 78.66666666666667, 1.0, 2.0, 0.303940100671897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482007.6237752045, 482007.6237752045, 165730.9130084243], 
processed observation next is [0.0, 0.43478260869565216, 0.28278041074249627, 0.7866666666666667, 1.0, 1.0, 0.16137361526734575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13389100660422348, 0.13389100660422348, 0.24735957165436465], 
reward next is 0.7526, 
noisyNet noise sample is [array([1.7452663], dtype=float32), -0.06511482]. 
=============================================
[2019-04-23 09:34:46,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 82.66666666666667, 1.0, 2.0, 0.2970708622658521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 472652.8191173339, 472652.8191173346, 165092.4803245918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 290400.0000, 
sim time next is 291000.0000, 
raw observation next is [22.23333333333333, 82.33333333333334, 1.0, 2.0, 0.2981145243526388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474091.621026023, 474091.6210260236, 165190.2972076315], 
processed observation next is [0.0, 0.34782608695652173, 0.25276461295418634, 0.8233333333333335, 1.0, 1.0, 0.1543548486176371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13169211695167304, 0.1316921169516732, 0.24655268239945], 
reward next is 0.7534, 
noisyNet noise sample is [array([-0.6829605], dtype=float32), -0.5423762]. 
=============================================
[2019-04-23 09:34:46,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.60206 ]
 [75.580246]
 [75.56973 ]
 [75.55209 ]
 [75.554924]], R is [[75.59590912]
 [75.59354401]
 [75.59124756]
 [75.58899689]
 [75.58678436]].
[2019-04-23 09:34:46,489] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7920: loss 0.0819
[2019-04-23 09:34:46,536] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7920: learning rate 0.0005
[2019-04-23 09:34:46,552] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7928: loss 0.0200
[2019-04-23 09:34:46,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7930: learning rate 0.0005
[2019-04-23 09:34:46,605] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7950: loss 0.0098
[2019-04-23 09:34:46,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7953: learning rate 0.0005
[2019-04-23 09:34:46,620] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7955: loss 0.0085
[2019-04-23 09:34:46,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7956: learning rate 0.0005
[2019-04-23 09:34:46,663] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7979: loss 0.0088
[2019-04-23 09:34:46,668] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7980: learning rate 0.0005
[2019-04-23 09:34:46,676] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7981: loss 0.0060
[2019-04-23 09:34:46,678] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7981: loss 0.0038
[2019-04-23 09:34:46,680] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7982: learning rate 0.0005
[2019-04-23 09:34:46,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7982: learning rate 0.0005
[2019-04-23 09:34:46,698] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7991: loss 0.0041
[2019-04-23 09:34:46,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7992: learning rate 0.0005
[2019-04-23 09:34:46,719] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8000: loss 0.0036
[2019-04-23 09:34:46,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8003: learning rate 0.0005
[2019-04-23 09:34:46,728] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8008: loss 0.0235
[2019-04-23 09:34:46,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8008: learning rate 0.0005
[2019-04-23 09:34:46,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8013: loss 0.0446
[2019-04-23 09:34:46,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8014: learning rate 0.0005
[2019-04-23 09:34:46,745] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8014: loss 0.0081
[2019-04-23 09:34:46,751] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8015: loss 0.0363
[2019-04-23 09:34:46,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8015: learning rate 0.0005
[2019-04-23 09:34:46,754] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8015: learning rate 0.0005
[2019-04-23 09:34:46,757] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8017: loss 0.0346
[2019-04-23 09:34:46,758] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8017: learning rate 0.0005
[2019-04-23 09:34:46,769] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8022: loss 0.0234
[2019-04-23 09:34:46,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8022: learning rate 0.0005
[2019-04-23 09:34:46,828] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8044: loss 0.0386
[2019-04-23 09:34:46,830] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8044: learning rate 0.0005
[2019-04-23 09:34:47,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:47,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6997
[2019-04-23 09:34:47,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 78.0, 1.0, 2.0, 0.3066500209044473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485678.6350541453, 485678.6350541453, 165984.1924993817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 316800.0000, 
sim time next is 317400.0000, 
raw observation next is [22.93333333333333, 78.16666666666667, 1.0, 2.0, 0.3057860900630685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484764.9369811529, 484764.9369811529, 165926.998357975], 
processed observation next is [0.0, 0.6956521739130435, 0.28593996840442326, 0.7816666666666667, 1.0, 1.0, 0.16359769887116687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13465692693920914, 0.13465692693920914, 0.24765223635518657], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.3849098], dtype=float32), -1.5702524]. 
=============================================
[2019-04-23 09:34:47,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:47,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2605
[2019-04-23 09:34:47,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3013051075397071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479424.4883262644, 479424.4883262644, 165574.4798935821], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [22.6, 79.0, 1.0, 2.0, 0.2993775856819734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476788.9494197393, 476788.9494197399, 165393.446471702], 
processed observation next is [0.0, 0.7391304347826086, 0.27014218009478685, 0.79, 1.0, 1.0, 0.15587660925538965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13244137483881646, 0.13244137483881663, 0.2468558902562716], 
reward next is 0.7531, 
noisyNet noise sample is [array([0.63266504], dtype=float32), 0.36631307]. 
=============================================
[2019-04-23 09:34:48,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.541891e-31 0.000000e+00], sum to 1.0000
[2019-04-23 09:34:48,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5268
[2019-04-23 09:34:48,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 85.33333333333333, 1.0, 2.0, 0.2878063677158854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461361.9366500977, 461361.9366500982, 164355.0262740652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 330000.0000, 
sim time next is 330600.0000, 
raw observation next is [21.36666666666667, 85.66666666666667, 1.0, 2.0, 0.2869434534910081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460192.6131244791, 460192.6131244785, 164276.8727312944], 
processed observation next is [0.0, 0.8260869565217391, 0.21169036334913136, 0.8566666666666667, 1.0, 1.0, 0.1408957270976001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12783128142346642, 0.12783128142346625, 0.24518936228551402], 
reward next is 0.7548, 
noisyNet noise sample is [array([-1.0469725], dtype=float32), -1.433855]. 
=============================================
[2019-04-23 09:34:49,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:34:49,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0724
[2019-04-23 09:34:49,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 86.0, 1.0, 2.0, 0.2764787152356477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 447421.9574306399, 447421.9574306393, 163423.2076608312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [20.8, 86.0, 1.0, 2.0, 0.2733448314107534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442603.5072770095, 442603.5072770101, 163104.8118955987], 
processed observation next is [0.0, 0.9565217391304348, 0.1848341232227489, 0.86, 1.0, 1.0, 0.12451184507319685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1229454186880582, 0.12294541868805837, 0.2434400177546249], 
reward next is 0.7566, 
noisyNet noise sample is [array([-0.03372161], dtype=float32), 1.522575]. 
=============================================
[2019-04-23 09:34:59,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9352108e-32 0.0000000e+00], sum to 1.0000
[2019-04-23 09:34:59,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-23 09:34:59,818] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.83333333333333, 87.0, 1.0, 2.0, 0.2321318764183155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 385011.7829843339, 385011.7829843339, 159002.7278875467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
processed observation next is [1.0, 1.0, 0.09083728278041096, 0.87, 1.0, 1.0, 0.07366329259156455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10650673415106725, 0.10650673415106725, 0.23717051313799883], 
reward next is 0.7628, 
noisyNet noise sample is [array([1.1752498], dtype=float32), 0.13002636]. 
=============================================
[2019-04-23 09:35:04,553] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15938: loss 0.0476
[2019-04-23 09:35:04,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15938: learning rate 0.0005
[2019-04-23 09:35:04,567] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15943: loss 0.0410
[2019-04-23 09:35:04,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15944: learning rate 0.0005
[2019-04-23 09:35:04,587] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15954: loss 0.0212
[2019-04-23 09:35:04,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15955: learning rate 0.0005
[2019-04-23 09:35:04,604] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15958: loss 0.0128
[2019-04-23 09:35:04,606] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15960: learning rate 0.0005
[2019-04-23 09:35:04,621] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15966: loss 0.0213
[2019-04-23 09:35:04,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15967: learning rate 0.0005
[2019-04-23 09:35:04,635] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15972: loss 0.0054
[2019-04-23 09:35:04,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15972: learning rate 0.0005
[2019-04-23 09:35:04,644] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15974: loss 0.0096
[2019-04-23 09:35:04,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15975: learning rate 0.0005
[2019-04-23 09:35:04,655] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15978: loss 0.0058
[2019-04-23 09:35:04,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15978: learning rate 0.0005
[2019-04-23 09:35:04,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15992: loss 0.0039
[2019-04-23 09:35:04,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15995: learning rate 0.0005
[2019-04-23 09:35:04,705] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15996: loss 0.0033
[2019-04-23 09:35:04,708] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15996: learning rate 0.0005
[2019-04-23 09:35:04,733] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16012: loss 0.0035
[2019-04-23 09:35:04,736] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16014: learning rate 0.0005
[2019-04-23 09:35:04,742] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16015: loss 0.0034
[2019-04-23 09:35:04,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16016: learning rate 0.0005
[2019-04-23 09:35:04,753] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16020: loss 0.0045
[2019-04-23 09:35:04,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16021: learning rate 0.0005
[2019-04-23 09:35:04,775] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16032: loss 0.0081
[2019-04-23 09:35:04,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16032: learning rate 0.0005
[2019-04-23 09:35:04,783] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16034: loss 0.0157
[2019-04-23 09:35:04,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16035: learning rate 0.0005
[2019-04-23 09:35:04,819] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16053: loss 0.0348
[2019-04-23 09:35:04,822] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16053: learning rate 0.0005
[2019-04-23 09:35:06,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.551411e-34 0.000000e+00], sum to 1.0000
[2019-04-23 09:35:06,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4360
[2019-04-23 09:35:06,307] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.06666666666667, 87.0, 1.0, 2.0, 0.211391117839434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 352652.3475898298, 352652.3475898298, 156682.8825518083], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [18.3, 86.0, 1.0, 2.0, 0.2135148031529875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 355961.443750149, 355961.4437501484, 156949.4386614617], 
processed observation next is [1.0, 0.2608695652173913, 0.06635071090047404, 0.86, 1.0, 1.0, 0.05242747367829816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09887817881948582, 0.09887817881948567, 0.23425289352456968], 
reward next is 0.7657, 
noisyNet noise sample is [array([-0.3664112], dtype=float32), 1.0957714]. 
=============================================
[2019-04-23 09:35:08,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2590164e-31 0.0000000e+00], sum to 1.0000
[2019-04-23 09:35:08,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-23 09:35:08,818] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 60.0, 1.0, 2.0, 0.246842079243286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 406347.7311477184, 406347.7311477184, 160589.7111319035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [23.18333333333333, 61.33333333333333, 1.0, 2.0, 0.2504187298447559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412273.4695163005, 412273.4695163011, 160939.509386387], 
processed observation next is [1.0, 0.782608695652174, 0.29778830963665076, 0.6133333333333333, 1.0, 1.0, 0.0968900359575372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11452040819897236, 0.11452040819897254, 0.24020822296475675], 
reward next is 0.7598, 
noisyNet noise sample is [array([-0.8678966], dtype=float32), 1.0657357]. 
=============================================
[2019-04-23 09:35:22,204] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23909: loss 0.1117
[2019-04-23 09:35:22,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23909: learning rate 0.0005
[2019-04-23 09:35:22,219] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23912: loss 0.0676
[2019-04-23 09:35:22,222] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23912: learning rate 0.0005
[2019-04-23 09:35:22,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23925: loss 0.0857
[2019-04-23 09:35:22,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23925: learning rate 0.0005
[2019-04-23 09:35:22,284] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23940: loss 0.0571
[2019-04-23 09:35:22,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23941: learning rate 0.0005
[2019-04-23 09:35:22,323] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23960: loss 0.0559
[2019-04-23 09:35:22,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23961: learning rate 0.0005
[2019-04-23 09:35:22,337] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23966: loss 0.0443
[2019-04-23 09:35:22,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23966: learning rate 0.0005
[2019-04-23 09:35:22,354] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23971: loss 0.0433
[2019-04-23 09:35:22,356] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23972: learning rate 0.0005
[2019-04-23 09:35:22,373] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23980: loss 0.0442
[2019-04-23 09:35:22,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23982: learning rate 0.0005
[2019-04-23 09:35:22,386] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23986: loss 0.0257
[2019-04-23 09:35:22,389] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23986: learning rate 0.0005
[2019-04-23 09:35:22,392] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23989: loss 0.0191
[2019-04-23 09:35:22,396] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23989: learning rate 0.0005
[2019-04-23 09:35:22,402] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 23992: loss 0.0085
[2019-04-23 09:35:22,410] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 23993: learning rate 0.0005
[2019-04-23 09:35:22,466] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24022: loss 0.0059
[2019-04-23 09:35:22,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24023: learning rate 0.0005
[2019-04-23 09:35:22,518] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24045: loss 0.0261
[2019-04-23 09:35:22,519] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24046: loss 0.0267
[2019-04-23 09:35:22,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24046: learning rate 0.0005
[2019-04-23 09:35:22,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24047: learning rate 0.0005
[2019-04-23 09:35:22,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24075: loss 0.0542
[2019-04-23 09:35:22,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24079: learning rate 0.0005
[2019-04-23 09:35:22,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24088: loss 0.0424
[2019-04-23 09:35:22,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24088: learning rate 0.0005
[2019-04-23 09:35:24,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:35:24,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-23 09:35:24,556] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 87.66666666666667, 1.0, 2.0, 0.3384132657816531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524183.5858627975, 524183.585862798, 168622.8294463988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [22.55, 88.0, 1.0, 2.0, 0.3387289176552802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524766.1754814181, 524766.1754814174, 168672.0485317231], 
processed observation next is [0.0, 0.8695652173913043, 0.26777251184834133, 0.88, 1.0, 1.0, 0.2032878525967231, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14576838207817172, 0.14576838207817153, 0.2517493261667509], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.28329897], dtype=float32), -0.599047]. 
=============================================
[2019-04-23 09:35:24,732] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 09:35:24,738] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:35:24,740] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:35:24,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,743] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:35:24,744] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:35:24,746] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,745] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:35:24,746] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,748] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,749] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:35:24,766] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,767] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,782] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,821] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run2
[2019-04-23 09:35:24,822] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run2
[2019-04-23 09:35:28,798] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:28,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.93333333333333, 89.83333333333333, 1.0, 2.0, 0.2929882873432826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469089.0690366681, 469089.0690366681, 164883.7680691813]
[2019-04-23 09:35:28,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:35:28,804] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1474123916898269
[2019-04-23 09:35:57,913] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:57,914] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.77149693, 70.66947462, 1.0, 2.0, 0.4303315642890675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104055, 606943.6261604563, 606943.6261604563, 174137.3055505385]
[2019-04-23 09:35:57,916] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:35:57,919] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.254142e-38 0.000000e+00], sampled 0.16709336821516285
[2019-04-23 09:35:59,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:35:59,056] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 85.5, 1.0, 2.0, 0.7473744893047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044509.702720558, 1044509.702720558, 231403.9013687684]
[2019-04-23 09:35:59,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:35:59,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6120957e-38 0.0000000e+00], sampled 0.9713376384883396
[2019-04-23 09:36:10,864] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:10,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.6, 57.66666666666667, 1.0, 2.0, 0.5130342743368855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716891.8351393198, 716891.8351393198, 185663.1108818094]
[2019-04-23 09:36:10,866] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:10,869] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.44918777299403645
[2019-04-23 09:36:12,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:12,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 81.16666666666666, 1.0, 2.0, 0.5106250491987964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713524.1577941555, 713524.1577941562, 185276.5599778855]
[2019-04-23 09:36:12,151] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:12,152] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6337055e-37 0.0000000e+00], sampled 0.8838392040616314
[2019-04-23 09:36:47,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:47,775] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.01666666666667, 94.16666666666667, 1.0, 2.0, 0.6116502410487251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 854748.953485946, 854748.9534859468, 202930.3302393857]
[2019-04-23 09:36:47,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:36:47,783] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5372494125553657
[2019-04-23 09:36:48,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:36:48,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066]
[2019-04-23 09:36:48,303] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:36:48,306] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43320294242967716
[2019-04-23 09:37:06,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:37:06,114] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.25796125833333, 90.86245502, 1.0, 2.0, 0.4017387458499152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660750.6507338983, 660750.6507338983, 179727.6665769322]
[2019-04-23 09:37:06,115] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:37:06,117] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22021562064898048
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.15980987]
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.36666666666667, 61.0, 1.0, 2.0, 0.403386701501198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 629678.8521892619, 629678.8521892613, 177793.5041055562]
[2019-04-23 09:37:11,938] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:37:11,941] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7109363684468294
[2019-04-23 09:37:13,596] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6985 2927265673.6025 1340.0000
[2019-04-23 09:37:13,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.5478 3163641139.4299 1775.0000
[2019-04-23 09:37:13,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2097 2779160217.1565 933.0000
[2019-04-23 09:37:13,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7839 3007412927.2304 1766.0000
[2019-04-23 09:37:13,921] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3815 2842403427.4721 1132.0000
[2019-04-23 09:37:14,932] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 25000, evaluation results [25000.0, 7885.547773540918, 3163641139.4298544, 1775.0, 8254.698514502707, 2927265673.6025105, 1340.0, 8661.209722185526, 2779160217.1564946, 933.0, 7997.78389971888, 3007412927.2304306, 1766.0, 8497.381504661924, 2842403427.4721303, 1132.0]
[2019-04-23 09:37:18,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.7866345e-25 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:18,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8851
[2019-04-23 09:37:18,191] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 95.0, 1.0, 2.0, 0.4035655307059746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622761.6777443201, 622761.6777443201, 177079.374243855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [21.76666666666667, 95.0, 1.0, 2.0, 0.4490403868515986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693652.4545397687, 693652.4545397687, 183952.0066599745], 
processed observation next is [1.0, 0.5217391304347826, 0.23064770932069528, 0.95, 1.0, 1.0, 0.33619323717060073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19268123737215798, 0.19268123737215798, 0.27455523382085745], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.35741326], dtype=float32), -0.53296876]. 
=============================================
[2019-04-23 09:37:18,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.41277 ]
 [64.984505]
 [63.840626]
 [64.24222 ]
 [64.07577 ]], R is [[65.21655273]
 [65.30008698]
 [65.36699677]
 [65.36919403]
 [65.40205383]].
[2019-04-23 09:37:18,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1001671e-27 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:18,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9729
[2019-04-23 09:37:18,814] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 96.0, 1.0, 2.0, 0.5330327924032255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824628.4409084235, 824628.4409084235, 198432.8941335611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1003200.0000, 
sim time next is 1003800.0000, 
raw observation next is [21.6, 96.0, 1.0, 2.0, 0.5202789958299762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804925.5168352509, 804925.5168352509, 196103.687232592], 
processed observation next is [1.0, 0.6086956521739131, 0.22274881516587688, 0.96, 1.0, 1.0, 0.42202288654213993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22359042134312523, 0.22359042134312523, 0.29269207049640594], 
reward next is 0.7073, 
noisyNet noise sample is [array([0.7404093], dtype=float32), -0.8224834]. 
=============================================
[2019-04-23 09:37:19,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7637566e-36 1.0000000e+00 0.0000000e+00 3.4303653e-21 0.0000000e+00], sum to 1.0000
[2019-04-23 09:37:19,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8900
[2019-04-23 09:37:19,147] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 96.83333333333334, 1.0, 2.0, 0.5879670029979287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 905193.3262040943, 905193.3262040943, 208626.7930460681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1007400.0000, 
sim time next is 1008000.0000, 
raw observation next is [21.7, 97.0, 1.0, 2.0, 0.6386368761396531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 982137.0935775214, 982137.0935775208, 219174.3486969676], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.97, 1.0, 1.0, 0.5646227423369314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2728158593270893, 0.2728158593270891, 0.32712589357756355], 
reward next is 0.6729, 
noisyNet noise sample is [array([0.5377404], dtype=float32), -0.5845203]. 
=============================================
[2019-04-23 09:37:19,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.63264]
 [70.60004]
 [70.69384]
 [70.59424]
 [70.08832]], R is [[70.49804688]
 [70.48168182]
 [70.46871948]
 [70.46642303]
 [70.45937347]].
[2019-04-23 09:37:30,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31860: loss 0.3683
[2019-04-23 09:37:30,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31860: learning rate 0.0005
[2019-04-23 09:37:30,244] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31902: loss 0.5176
[2019-04-23 09:37:30,248] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31903: learning rate 0.0005
[2019-04-23 09:37:30,287] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31920: loss 0.4676
[2019-04-23 09:37:30,288] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31920: learning rate 0.0005
[2019-04-23 09:37:30,317] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31929: loss 0.4099
[2019-04-23 09:37:30,323] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31931: learning rate 0.0005
[2019-04-23 09:37:30,344] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31943: loss 0.3643
[2019-04-23 09:37:30,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31944: learning rate 0.0005
[2019-04-23 09:37:30,355] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31946: loss 0.3508
[2019-04-23 09:37:30,359] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31946: learning rate 0.0005
[2019-04-23 09:37:30,406] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31969: loss 0.3205
[2019-04-23 09:37:30,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31971: learning rate 0.0005
[2019-04-23 09:37:30,437] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31985: loss 0.1858
[2019-04-23 09:37:30,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31985: learning rate 0.0005
[2019-04-23 09:37:30,442] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31986: loss 0.1610
[2019-04-23 09:37:30,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31986: learning rate 0.0005
[2019-04-23 09:37:30,496] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32004: loss 0.1227
[2019-04-23 09:37:30,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32005: learning rate 0.0005
[2019-04-23 09:37:30,591] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32045: loss 0.0604
[2019-04-23 09:37:30,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32046: learning rate 0.0005
[2019-04-23 09:37:30,610] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32056: loss 0.0222
[2019-04-23 09:37:30,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32057: learning rate 0.0005
[2019-04-23 09:37:30,622] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32059: loss 0.0362
[2019-04-23 09:37:30,624] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32059: loss 0.0198
[2019-04-23 09:37:30,625] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32059: learning rate 0.0005
[2019-04-23 09:37:30,629] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32061: loss 0.0069
[2019-04-23 09:37:30,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32061: learning rate 0.0005
[2019-04-23 09:37:30,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32062: learning rate 0.0005
[2019-04-23 09:37:30,666] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32075: loss 0.0072
[2019-04-23 09:37:30,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32075: learning rate 0.0005
[2019-04-23 09:37:30,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:30,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3187
[2019-04-23 09:37:30,872] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 85.33333333333334, 1.0, 2.0, 0.3554413071659422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547751.2515474142, 547751.2515474149, 170464.3484644801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [23.0, 86.16666666666666, 1.0, 2.0, 0.3560965178943104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548579.436239281, 548579.436239281, 170528.3612299317], 
processed observation next is [1.0, 0.9565217391304348, 0.28909952606635075, 0.8616666666666666, 1.0, 1.0, 0.22421267216181975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1523831767331336, 0.1523831767331336, 0.2545199421342264], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.8303618], dtype=float32), -0.46187267]. 
=============================================
[2019-04-23 09:37:41,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:42,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6370
[2019-04-23 09:37:42,110] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.4, 98.0, 1.0, 2.0, 0.3099197336711298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 491720.4763980514, 491720.4763980507, 166442.265502585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1395000.0000, 
sim time next is 1395600.0000, 
raw observation next is [20.43333333333333, 98.0, 1.0, 2.0, 0.3124547762544969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495360.7909442527, 495360.7909442527, 166703.9298310911], 
processed observation next is [0.0, 0.13043478260869565, 0.1674565560821484, 0.98, 1.0, 1.0, 0.17163226054758662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13760021970673686, 0.13760021970673686, 0.24881183556879272], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.6505396], dtype=float32), 0.969189]. 
=============================================
[2019-04-23 09:37:48,341] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39860: loss 0.0812
[2019-04-23 09:37:48,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39860: learning rate 0.0005
[2019-04-23 09:37:48,426] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39891: loss 0.1391
[2019-04-23 09:37:48,430] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39891: learning rate 0.0005
[2019-04-23 09:37:48,465] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39907: loss 0.1145
[2019-04-23 09:37:48,469] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39908: learning rate 0.0005
[2019-04-23 09:37:48,492] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39919: loss 0.1187
[2019-04-23 09:37:48,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39919: learning rate 0.0005
[2019-04-23 09:37:48,549] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39935: loss 0.0877
[2019-04-23 09:37:48,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39937: learning rate 0.0005
[2019-04-23 09:37:48,575] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39944: loss 0.0571
[2019-04-23 09:37:48,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39944: learning rate 0.0005
[2019-04-23 09:37:48,660] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39975: loss 0.0401
[2019-04-23 09:37:48,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39976: learning rate 0.0005
[2019-04-23 09:37:48,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39985: loss 0.0094
[2019-04-23 09:37:48,689] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39988: learning rate 0.0005
[2019-04-23 09:37:48,717] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39998: loss 0.0052
[2019-04-23 09:37:48,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39998: learning rate 0.0005
[2019-04-23 09:37:48,750] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40004: loss 0.0028
[2019-04-23 09:37:48,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40004: learning rate 0.0005
[2019-04-23 09:37:48,763] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40005: loss 0.0018
[2019-04-23 09:37:48,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40007: learning rate 0.0005
[2019-04-23 09:37:48,821] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40031: loss 0.0012
[2019-04-23 09:37:48,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40034: learning rate 0.0005
[2019-04-23 09:37:48,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40041: loss 0.0010
[2019-04-23 09:37:48,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40041: learning rate 0.0005
[2019-04-23 09:37:48,855] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40045: loss 0.0018
[2019-04-23 09:37:48,859] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40045: learning rate 0.0005
[2019-04-23 09:37:48,936] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40071: loss 0.0027
[2019-04-23 09:37:48,938] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40071: learning rate 0.0005
[2019-04-23 09:37:49,273] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40195: loss 0.0387
[2019-04-23 09:37:49,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40195: learning rate 0.0005
[2019-04-23 09:37:51,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:51,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-23 09:37:51,153] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 74.0, 1.0, 2.0, 0.3532127284353498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543324.9260934021, 543324.9260934027, 170067.8775380704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1537200.0000, 
sim time next is 1537800.0000, 
raw observation next is [24.6, 75.5, 1.0, 2.0, 0.3542998769845044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544581.0258525242, 544581.0258525242, 170159.7827732003], 
processed observation next is [0.0, 0.8260869565217391, 0.36492890995260674, 0.755, 1.0, 1.0, 0.22204804455964383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15127250718125673, 0.15127250718125673, 0.25396982503462734], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.7240056], dtype=float32), 1.1018342]. 
=============================================
[2019-04-23 09:37:54,514] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:54,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8885
[2019-04-23 09:37:54,532] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 85.16666666666667, 1.0, 2.0, 0.3847513131915307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590258.5987439593, 590258.5987439593, 174078.2045986252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [23.4, 85.0, 1.0, 2.0, 0.3990236293460754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610780.2515295085, 610780.2515295091, 175897.4022419333], 
processed observation next is [1.0, 0.34782608695652173, 0.30805687203791465, 0.85, 1.0, 1.0, 0.2759320835494884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16966118098041902, 0.16966118098041919, 0.26253343618199], 
reward next is 0.7375, 
noisyNet noise sample is [array([0.9413071], dtype=float32), 2.3995712]. 
=============================================
[2019-04-23 09:37:54,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.18559 ]
 [71.096115]
 [70.8802  ]
 [70.98984 ]
 [71.29495 ]], R is [[71.21181488]
 [71.23987579]
 [71.2660141 ]
 [71.28653717]
 [71.30900574]].
[2019-04-23 09:37:56,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:37:56,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-23 09:37:56,454] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 86.33333333333334, 1.0, 2.0, 0.7236314791354455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080989.417543761, 1080989.417543761, 235162.7322399884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1606800.0000, 
sim time next is 1607400.0000, 
raw observation next is [23.9, 87.0, 1.0, 2.0, 0.7590619797247687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1133001.807641249, 1133001.807641248, 243730.3944333877], 
processed observation next is [1.0, 0.6086956521739131, 0.33175355450236965, 0.87, 1.0, 1.0, 0.7097132285840587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31472272434479137, 0.3147227243447911, 0.3637767081095339], 
reward next is 0.6362, 
noisyNet noise sample is [array([-1.1490645], dtype=float32), -0.038624994]. 
=============================================
[2019-04-23 09:38:02,841] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5564364e-25 1.0000000e+00 8.5805126e-31 7.8984128e-15 2.6762476e-25], sum to 1.0000
[2019-04-23 09:38:02,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0769
[2019-04-23 09:38:02,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1928698.098685594 W.
[2019-04-23 09:38:02,968] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.56666666666667, 76.5, 1.0, 2.0, 0.459824215619476, 1.0, 2.0, 0.459824215619476, 1.0, 1.0, 0.7888773306295616, 6.9112, 6.9112, 170.5573041426782, 1928698.098685594, 1928698.098685594, 386876.1187746875], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [28.63333333333334, 76.0, 1.0, 2.0, 0.7449230836696593, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.983022695440308, 6.9112, 168.9125286380091, 1937993.950945889, 1887040.587924125, 395520.2954888534], 
processed observation next is [1.0, 0.6956521739130435, 0.5560821484992104, 0.76, 1.0, 1.0, 0.6926784140598304, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007182269544030806, 0.0, 0.8294378441028303, 0.5383316530405248, 0.5241779410900347, 0.5903287992370947], 
reward next is 0.0506, 
noisyNet noise sample is [array([-1.004074], dtype=float32), -1.3016319]. 
=============================================
[2019-04-23 09:38:09,637] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47814: loss 0.0977
[2019-04-23 09:38:09,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47816: learning rate 0.0005
[2019-04-23 09:38:09,725] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47847: loss 0.1200
[2019-04-23 09:38:09,732] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47847: learning rate 0.0005
[2019-04-23 09:38:09,893] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47912: loss 0.1215
[2019-04-23 09:38:09,895] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47912: learning rate 0.0005
[2019-04-23 09:38:09,937] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47929: loss 0.1373
[2019-04-23 09:38:09,941] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47931: learning rate 0.0005
[2019-04-23 09:38:10,030] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47966: loss 0.1394
[2019-04-23 09:38:10,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47966: learning rate 0.0005
[2019-04-23 09:38:10,049] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47971: loss 0.1554
[2019-04-23 09:38:10,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47971: learning rate 0.0005
[2019-04-23 09:38:10,078] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47982: loss 0.1532
[2019-04-23 09:38:10,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47982: learning rate 0.0005
[2019-04-23 09:38:10,085] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47983: loss 0.2120
[2019-04-23 09:38:10,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47985: learning rate 0.0005
[2019-04-23 09:38:10,123] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47996: loss 0.1747
[2019-04-23 09:38:10,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47998: learning rate 0.0005
[2019-04-23 09:38:10,153] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48007: loss 0.1301
[2019-04-23 09:38:10,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48007: learning rate 0.0005
[2019-04-23 09:38:10,192] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48022: loss 0.2027
[2019-04-23 09:38:10,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48022: learning rate 0.0005
[2019-04-23 09:38:10,219] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48030: loss 0.1877
[2019-04-23 09:38:10,222] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48030: learning rate 0.0005
[2019-04-23 09:38:10,240] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48039: loss 0.2124
[2019-04-23 09:38:10,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48040: learning rate 0.0005
[2019-04-23 09:38:10,253] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48043: loss 0.1684
[2019-04-23 09:38:10,261] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48044: learning rate 0.0005
[2019-04-23 09:38:10,492] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48129: loss 0.0415
[2019-04-23 09:38:10,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48129: learning rate 0.0005
[2019-04-23 09:38:10,516] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48137: loss 0.0298
[2019-04-23 09:38:10,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48138: learning rate 0.0005
[2019-04-23 09:38:10,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:38:10,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5380
[2019-04-23 09:38:10,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333333, 94.66666666666667, 1.0, 2.0, 0.3361926734803173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525136.4967936483, 525136.4967936483, 168819.4305102034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1808400.0000, 
sim time next is 1809000.0000, 
raw observation next is [21.45, 95.0, 1.0, 2.0, 0.3386862032060498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528242.5919976203, 528242.5919976203, 169046.6448126051], 
processed observation next is [1.0, 0.9565217391304348, 0.2156398104265403, 0.95, 1.0, 1.0, 0.2032363894048793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1467340533326723, 0.1467340533326723, 0.2523084250934405], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.8225947], dtype=float32), -0.59887433]. 
=============================================
[2019-04-23 09:38:10,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.96179 ]
 [85.91781 ]
 [85.873215]
 [85.69507 ]
 [85.68802 ]], R is [[85.89198303]
 [85.78109741]
 [85.67167664]
 [85.56351471]
 [85.45627594]].
[2019-04-23 09:38:13,703] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2858607e-30 0.0000000e+00], sum to 1.0000
[2019-04-23 09:38:13,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-23 09:38:13,721] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [24.71666666666667, 89.66666666666667, 1.0, 2.0, 0.8951863576699861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1278924.57930032, 1278924.57930032, 272699.238558078], 
processed observation next is [1.0, 0.43478260869565216, 0.3704581358609796, 0.8966666666666667, 1.0, 1.0, 0.8737185032168507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35525682758342225, 0.35525682758342225, 0.40701378889265377], 
reward next is 0.5930, 
noisyNet noise sample is [array([-0.6357273], dtype=float32), 0.5777712]. 
=============================================
[2019-04-23 09:38:13,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.30991 ]
 [70.9194  ]
 [70.822975]
 [70.733116]
 [70.786285]], R is [[71.23170471]
 [71.12430573]
 [71.00756073]
 [70.89545441]
 [70.78931427]].
[2019-04-23 09:38:15,517] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 09:38:15,518] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:38:15,519] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:38:15,519] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,520] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:38:15,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,520] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:38:15,521] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,522] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:38:15,523] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,526] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:38:15,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,558] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,580] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,580] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run3
[2019-04-23 09:38:15,619] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run3
[2019-04-23 09:38:37,558] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:38:37,561] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.46666666666667, 52.66666666666667, 1.0, 2.0, 0.2038880258608933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 340797.1709729611, 340797.1709729611, 155688.7188106712]
[2019-04-23 09:38:37,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:38:37,564] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5686517220505963
[2019-04-23 09:38:49,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:38:49,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 93.5, 1.0, 2.0, 0.3901065997402537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585834.8545793705, 585834.8545793712, 173349.9853749929]
[2019-04-23 09:38:49,378] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:38:49,380] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4225549312590958
[2019-04-23 09:39:03,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:39:03,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.58839391, 88.78940779999999, 1.0, 2.0, 0.3302651360059471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521193.7786214587, 521193.778621458, 168624.0941074877]
[2019-04-23 09:39:03,652] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:39:03,655] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8813291109176173
[2019-04-23 09:39:59,218] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.19780229]
[2019-04-23 09:39:59,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.88845082333334, 91.69522763, 1.0, 2.0, 0.532954605354385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744737.399278292, 744737.3992782925, 188918.8279295965]
[2019-04-23 09:39:59,220] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:39:59,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24528811499198677
[2019-04-23 09:40:23,965] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:40:25,294] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:40:25,458] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:40:25,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:40:25,709] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:40:26,723] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 50000, evaluation results [50000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:40:27,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:27,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-23 09:40:27,914] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333333, 88.33333333333334, 1.0, 2.0, 0.4619212663082412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655440.5529497884, 655440.5529497884, 179131.7906215914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1890600.0000, 
sim time next is 1891200.0000, 
raw observation next is [24.96666666666667, 88.66666666666667, 1.0, 2.0, 0.4590128919104424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651900.7031326913, 651900.703132692, 178778.1598062251], 
processed observation next is [1.0, 0.9130434782608695, 0.3823064770932071, 0.8866666666666667, 1.0, 1.0, 0.3482083035065572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1810835286479698, 0.18108352864797, 0.2668330743376494], 
reward next is 0.7332, 
noisyNet noise sample is [array([-1.6581613], dtype=float32), 1.5497767]. 
=============================================
[2019-04-23 09:40:33,671] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:33,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2358
[2019-04-23 09:40:33,693] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 91.5, 1.0, 2.0, 0.4119402021999823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607025.5068812162, 607025.5068812162, 174971.4271783108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1967400.0000, 
sim time next is 1968000.0000, 
raw observation next is [23.53333333333333, 92.0, 1.0, 2.0, 0.4073195822012597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601361.8269960075, 601361.8269960075, 174476.4283479802], 
processed observation next is [1.0, 0.782608695652174, 0.3143759873617693, 0.92, 1.0, 1.0, 0.2859272074713972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1670449519433354, 0.1670449519433354, 0.26041257962385106], 
reward next is 0.7396, 
noisyNet noise sample is [array([-2.1825202], dtype=float32), -0.68494695]. 
=============================================
[2019-04-23 09:40:33,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.10615 ]
 [72.605225]
 [72.71682 ]
 [72.96151 ]
 [72.82404 ]], R is [[72.71128082]
 [72.72301483]
 [72.73377228]
 [72.74394989]
 [72.75439453]].
[2019-04-23 09:40:38,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:38,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8394
[2019-04-23 09:40:38,189] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 84.0, 1.0, 2.0, 0.5168110363721877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722171.1124931996, 722171.1124931996, 186272.0921017974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2035800.0000, 
sim time next is 2036400.0000, 
raw observation next is [27.36666666666667, 83.33333333333334, 1.0, 2.0, 0.5168558437300147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722233.745791788, 722233.7457917886, 186279.3772949345], 
processed observation next is [0.0, 0.5652173913043478, 0.49605055292259104, 0.8333333333333335, 1.0, 1.0, 0.4178986069036321, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20062048494216334, 0.2006204849421635, 0.27802892133572316], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.5673664], dtype=float32), 0.31211057]. 
=============================================
[2019-04-23 09:40:39,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:39,570] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6679
[2019-04-23 09:40:39,574] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.0, 1.0, 2.0, 0.4859981027887919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679100.5861074259, 679100.5861074264, 181431.3260307892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2055600.0000, 
sim time next is 2056200.0000, 
raw observation next is [25.73333333333333, 88.16666666666667, 1.0, 2.0, 0.484744198171802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677347.9067545087, 677347.9067545087, 181240.2720779809], 
processed observation next is [0.0, 0.8260869565217391, 0.41864139020537117, 0.8816666666666667, 1.0, 1.0, 0.3792098773154241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18815219632069688, 0.18815219632069688, 0.2705078687731058], 
reward next is 0.7295, 
noisyNet noise sample is [array([-1.2554067], dtype=float32), -0.39216426]. 
=============================================
[2019-04-23 09:40:42,189] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55823: loss 0.0331
[2019-04-23 09:40:42,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55824: learning rate 0.0005
[2019-04-23 09:40:42,339] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55877: loss 0.0053
[2019-04-23 09:40:42,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55877: learning rate 0.0005
[2019-04-23 09:40:42,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55904: loss 0.0014
[2019-04-23 09:40:42,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55904: learning rate 0.0005
[2019-04-23 09:40:42,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55959: loss 0.0131
[2019-04-23 09:40:42,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55960: learning rate 0.0005
[2019-04-23 09:40:42,586] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55968: loss 0.0035
[2019-04-23 09:40:42,589] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55968: learning rate 0.0005
[2019-04-23 09:40:42,626] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55981: loss 0.0019
[2019-04-23 09:40:42,629] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55983: learning rate 0.0005
[2019-04-23 09:40:42,642] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 55988: loss 0.0022
[2019-04-23 09:40:42,644] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 55988: learning rate 0.0005
[2019-04-23 09:40:42,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55989: loss 0.0031
[2019-04-23 09:40:42,649] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55990: learning rate 0.0005
[2019-04-23 09:40:42,677] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56000: loss 0.0018
[2019-04-23 09:40:42,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56000: learning rate 0.0005
[2019-04-23 09:40:42,688] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56000: loss 0.0014
[2019-04-23 09:40:42,691] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56002: learning rate 0.0005
[2019-04-23 09:40:42,714] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56010: loss 0.0014
[2019-04-23 09:40:42,719] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56011: learning rate 0.0005
[2019-04-23 09:40:42,721] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56011: loss 0.0015
[2019-04-23 09:40:42,724] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56011: learning rate 0.0005
[2019-04-23 09:40:42,737] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56019: loss 0.0018
[2019-04-23 09:40:42,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56019: learning rate 0.0005
[2019-04-23 09:40:42,771] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56032: loss 0.0046
[2019-04-23 09:40:42,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56033: learning rate 0.0005
[2019-04-23 09:40:42,974] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56107: loss 0.0039
[2019-04-23 09:40:42,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56107: learning rate 0.0005
[2019-04-23 09:40:43,061] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56143: loss 0.0015
[2019-04-23 09:40:43,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56143: learning rate 0.0005
[2019-04-23 09:40:47,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:47,234] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2554
[2019-04-23 09:40:47,240] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5079720506626362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709815.7400262054, 709815.7400262054, 184854.4702663285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2164800.0000, 
sim time next is 2165400.0000, 
raw observation next is [25.45, 94.0, 1.0, 2.0, 0.5063128050034075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707496.4177415373, 707496.4177415373, 184590.9725924404], 
processed observation next is [1.0, 0.043478260869565216, 0.4052132701421801, 0.94, 1.0, 1.0, 0.405196150606515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1965267827059826, 0.1965267827059826, 0.27550891431707525], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.06705478], dtype=float32), -0.051217925]. 
=============================================
[2019-04-23 09:40:52,065] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9353507e-32 0.0000000e+00], sum to 1.0000
[2019-04-23 09:40:52,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-23 09:40:52,083] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.06666666666666, 80.16666666666666, 1.0, 2.0, 0.5576038116014002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779194.2153856759, 779194.2153856759, 193113.7596043959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2231400.0000, 
sim time next is 2232000.0000, 
raw observation next is [28.9, 81.0, 1.0, 2.0, 0.5560766237647707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777059.3461913638, 777059.3461913643, 192848.7764723962], 
processed observation next is [1.0, 0.8695652173913043, 0.5687203791469194, 0.81, 1.0, 1.0, 0.4651525587527358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21584981838648992, 0.2158498183864901, 0.28783399473491966], 
reward next is 0.7122, 
noisyNet noise sample is [array([-1.3722591], dtype=float32), 0.09608543]. 
=============================================
[2019-04-23 09:40:52,102] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[46.876053]
 [47.031845]
 [47.418293]
 [47.549435]
 [47.847973]], R is [[47.18795395]
 [47.427845  ]
 [47.66516495]
 [47.90035629]
 [48.13343048]].
[2019-04-23 09:40:52,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.475846e-30 0.000000e+00], sum to 1.0000
[2019-04-23 09:40:52,755] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7856
[2019-04-23 09:40:52,761] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.3, 82.33333333333334, 1.0, 2.0, 0.5445081688488133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760887.8415388272, 760887.8415388279, 190863.5972684024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [28.2, 82.66666666666667, 1.0, 2.0, 0.5433030048505412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759203.1609540338, 759203.1609540345, 190659.0467544961], 
processed observation next is [1.0, 0.9130434782608695, 0.5355450236966824, 0.8266666666666667, 1.0, 1.0, 0.4497626564464351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21088976693167608, 0.21088976693167627, 0.28456574142462104], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.27895296], dtype=float32), -0.047800645]. 
=============================================
[2019-04-23 09:40:55,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9239902e-36 1.0000000e+00 0.0000000e+00 1.3428681e-22 0.0000000e+00], sum to 1.0000
[2019-04-23 09:40:55,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2686
[2019-04-23 09:40:55,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2017103.533751438 W.
[2019-04-23 09:40:55,941] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.8014505591823877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999001516798011, 6.9112, 168.9124350512162, 2017103.533751438, 1954814.30813656, 408461.6664497059], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [31.91666666666666, 63.83333333333334, 1.0, 2.0, 0.4914297843066927, 1.0, 1.0, 0.4914297843066927, 1.0, 2.0, 0.8534510170276295, 6.9112, 6.9112, 170.5573041426782, 2061392.820521601, 2061392.820521601, 409269.7810848841], 
processed observation next is [1.0, 0.5217391304347826, 0.7116903633491308, 0.6383333333333334, 1.0, 1.0, 0.3872648003695093, 1.0, 0.5, 0.3872648003695093, 1.0, 1.0, 0.821281728082475, 0.0, 0.0, 0.8375144448122397, 0.5726091168115558, 0.5726091168115558, 0.6108504195296777], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49060172], dtype=float32), 0.5159018]. 
=============================================
[2019-04-23 09:40:57,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.22775895e-35 1.00000000e+00 0.00000000e+00 1.27055085e-26
 0.00000000e+00], sum to 1.0000
[2019-04-23 09:40:57,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6050
[2019-04-23 09:40:57,589] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.28333333333333, 64.66666666666667, 1.0, 2.0, 0.2601591305303317, 1.0, 2.0, 0.2601591305303317, 1.0, 2.0, 0.4518103737920136, 6.911199999999999, 6.9112, 170.5573041426782, 1090791.753957912, 1090791.753957913, 288997.9938681801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2308200.0000, 
sim time next is 2308800.0000, 
raw observation next is [32.16666666666667, 65.33333333333334, 1.0, 2.0, 0.5320381759676455, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104249, 743456.355688527, 743456.3556885277, 188771.4594889885], 
processed observation next is [1.0, 0.7391304347826086, 0.7235387045813588, 0.6533333333333334, 1.0, 1.0, 0.4361905734549945, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522727, 0.20651565435792416, 0.20651565435792435, 0.28174844699849033], 
reward next is 0.7183, 
noisyNet noise sample is [array([1.6826578], dtype=float32), 0.6405528]. 
=============================================
[2019-04-23 09:40:58,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:40:58,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6488
[2019-04-23 09:40:58,223] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 77.66666666666667, 1.0, 2.0, 0.5745063385012651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802822.6916874194, 802822.6916874194, 196092.6855707108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2320800.0000, 
sim time next is 2321400.0000, 
raw observation next is [29.78333333333333, 77.83333333333333, 1.0, 2.0, 0.572678841892785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800267.9619169089, 800267.9619169089, 195766.490518366], 
processed observation next is [1.0, 0.8695652173913043, 0.6105845181674565, 0.7783333333333333, 1.0, 1.0, 0.4851552311961264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22229665608803023, 0.22229665608803023, 0.2921887918184567], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.51159537], dtype=float32), -0.4811942]. 
=============================================
[2019-04-23 09:41:03,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1026521e-19 1.0000000e+00 6.2999669e-25 2.3977091e-11 7.0697098e-20], sum to 1.0000
[2019-04-23 09:41:03,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-23 09:41:03,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2729489.594059058 W.
[2019-04-23 09:41:03,255] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.06666666666667, 61.16666666666666, 1.0, 2.0, 0.6598633515895871, 1.0, 2.0, 0.6505217153090561, 1.0, 2.0, 1.03, 7.005094567632042, 6.9112, 170.5573041426782, 2729489.594059058, 2662229.095877411, 508950.4596790414], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [33.1, 61.0, 1.0, 2.0, 0.9550466305432843, 1.0, 2.0, 0.9550466305432843, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2671424.351617706, 2671424.351617706, 502413.6521837124], 
processed observation next is [1.0, 0.6956521739130435, 0.7677725118483413, 0.61, 1.0, 1.0, 0.9458393139075715, 1.0, 1.0, 0.9458393139075715, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7420623198938072, 0.7420623198938072, 0.7498711226622573], 
reward next is 0.2501, 
noisyNet noise sample is [array([0.8275662], dtype=float32), 0.6242577]. 
=============================================
[2019-04-23 09:41:03,378] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63775: loss 0.0706
[2019-04-23 09:41:03,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63776: learning rate 0.0005
[2019-04-23 09:41:03,606] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63861: loss 0.0439
[2019-04-23 09:41:03,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63861: learning rate 0.0005
[2019-04-23 09:41:03,644] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63871: loss 0.3303
[2019-04-23 09:41:03,647] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63873: learning rate 0.0005
[2019-04-23 09:41:03,802] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63933: loss 0.1098
[2019-04-23 09:41:03,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63933: learning rate 0.0005
[2019-04-23 09:41:03,900] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63969: loss 0.3040
[2019-04-23 09:41:03,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63969: learning rate 0.0005
[2019-04-23 09:41:03,932] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63979: loss 0.2609
[2019-04-23 09:41:03,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63979: learning rate 0.0005
[2019-04-23 09:41:03,949] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63985: loss 0.0279
[2019-04-23 09:41:03,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63990: learning rate 0.0005
[2019-04-23 09:41:03,983] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64000: loss 0.0456
[2019-04-23 09:41:03,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64000: learning rate 0.0005
[2019-04-23 09:41:03,999] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64005: loss 0.5877
[2019-04-23 09:41:04,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64006: learning rate 0.0005
[2019-04-23 09:41:04,018] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64011: loss 0.4100
[2019-04-23 09:41:04,022] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64011: learning rate 0.0005
[2019-04-23 09:41:04,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64025: loss 0.1572
[2019-04-23 09:41:04,070] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64026: learning rate 0.0005
[2019-04-23 09:41:04,083] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64031: loss 0.2660
[2019-04-23 09:41:04,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64031: learning rate 0.0005
[2019-04-23 09:41:04,195] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64072: loss 0.2738
[2019-04-23 09:41:04,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64074: learning rate 0.0005
[2019-04-23 09:41:04,229] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64089: loss 0.1642
[2019-04-23 09:41:04,235] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64089: learning rate 0.0005
[2019-04-23 09:41:04,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64099: loss 0.0398
[2019-04-23 09:41:04,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64099: learning rate 0.0005
[2019-04-23 09:41:04,352] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64130: loss 0.0168
[2019-04-23 09:41:04,358] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64132: learning rate 0.0005
[2019-04-23 09:41:08,466] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:08,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-23 09:41:08,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1698504.129978935 W.
[2019-04-23 09:41:08,485] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.404986740620096, 1.0, 1.0, 0.404986740620096, 1.0, 2.0, 0.6852729263455949, 6.9112, 6.9112, 170.5573041426782, 1698504.129978935, 1698504.129978935, 353055.3729815353], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [26.05, 89.0, 1.0, 2.0, 0.5770410844151131, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9741224474753358, 6.911200000000001, 6.9112, 168.9129564997117, 1613344.885351168, 1613344.885351167, 347033.1204681488], 
processed observation next is [1.0, 0.5217391304347826, 0.43364928909952616, 0.89, 1.0, 1.0, 0.49041094507844957, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.9684420091162632, 8.881784197001253e-17, 0.0, 0.829439945099666, 0.4481513570419911, 0.4481513570419908, 0.5179598812957444], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5155593], dtype=float32), 0.8650728]. 
=============================================
[2019-04-23 09:41:12,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:12,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6081
[2019-04-23 09:41:12,115] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 95.0, 1.0, 2.0, 0.5464744912966144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763636.5397815894, 763636.53978159, 191197.9855881855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [26.43333333333333, 95.0, 1.0, 2.0, 0.5453855525236866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762114.3246316728, 762114.3246316728, 191012.578711956], 
processed observation next is [1.0, 0.043478260869565216, 0.4518167456556081, 0.95, 1.0, 1.0, 0.45227175002853803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.211698423508798, 0.211698423508798, 0.28509340106262093], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.89268357], dtype=float32), 1.3439715]. 
=============================================
[2019-04-23 09:41:24,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:41:24,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2399
[2019-04-23 09:41:24,429] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686800.0000, 
sim time next is 2687400.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4420703068049817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 634967.3739069798, 634967.3739069805, 177237.9213649489], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32779555036744784, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17637982608527217, 0.17637982608527236, 0.26453421099246105], 
reward next is 0.7355, 
noisyNet noise sample is [array([-1.5812435], dtype=float32), -1.274261]. 
=============================================
[2019-04-23 09:41:24,504] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71704: loss 0.0081
[2019-04-23 09:41:24,505] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71704: learning rate 0.0005
[2019-04-23 09:41:24,829] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71831: loss 0.0106
[2019-04-23 09:41:24,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71832: learning rate 0.0005
[2019-04-23 09:41:24,958] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71880: loss 0.0344
[2019-04-23 09:41:24,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71880: learning rate 0.0005
[2019-04-23 09:41:25,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71906: loss 0.0391
[2019-04-23 09:41:25,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71906: learning rate 0.0005
[2019-04-23 09:41:25,044] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71911: loss 0.0362
[2019-04-23 09:41:25,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71914: learning rate 0.0005
[2019-04-23 09:41:25,237] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71980: loss 0.0892
[2019-04-23 09:41:25,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71982: learning rate 0.0005
[2019-04-23 09:41:25,303] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72009: loss 0.1123
[2019-04-23 09:41:25,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72011: learning rate 0.0005
[2019-04-23 09:41:25,315] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72014: loss 0.0367
[2019-04-23 09:41:25,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72014: learning rate 0.0005
[2019-04-23 09:41:25,328] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72017: loss 0.0789
[2019-04-23 09:41:25,329] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72018: loss 0.0540
[2019-04-23 09:41:25,331] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72018: learning rate 0.0005
[2019-04-23 09:41:25,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72018: learning rate 0.0005
[2019-04-23 09:41:25,386] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72037: loss 0.1086
[2019-04-23 09:41:25,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72037: learning rate 0.0005
[2019-04-23 09:41:25,435] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72055: loss 0.0352
[2019-04-23 09:41:25,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72056: learning rate 0.0005
[2019-04-23 09:41:25,511] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72087: loss 0.0092
[2019-04-23 09:41:25,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72087: learning rate 0.0005
[2019-04-23 09:41:25,563] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 72104: loss 0.0035
[2019-04-23 09:41:25,565] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 72104: learning rate 0.0005
[2019-04-23 09:41:25,580] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72109: loss 0.0043
[2019-04-23 09:41:25,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72109: learning rate 0.0005
[2019-04-23 09:41:25,607] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72116: loss 0.0047
[2019-04-23 09:41:25,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72117: learning rate 0.0005
[2019-04-23 09:41:33,264] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-23 09:41:33,266] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:41:33,266] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:41:33,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:41:33,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,270] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,270] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:41:33,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:41:33,275] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,276] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,293] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,311] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run4
[2019-04-23 09:41:33,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run4
[2019-04-23 09:41:42,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:42,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 74.33333333333333, 1.0, 2.0, 0.3197170662374055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506777.1611194508, 506777.1611194514, 167556.1229432241]
[2019-04-23 09:41:42,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:41:42,199] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1587251912027856
[2019-04-23 09:41:50,325] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:50,326] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.68614035666667, 98.58740946333333, 1.0, 2.0, 0.3452096814513745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551808.4233626828, 551808.4233626828, 171115.3912764342]
[2019-04-23 09:41:50,327] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:41:50,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.738325315836886
[2019-04-23 09:41:50,458] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:41:50,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.65, 96.5, 1.0, 2.0, 0.5395014637421485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832332.804350214, 832332.804350214, 199398.5118316869]
[2019-04-23 09:41:50,462] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:41:50,467] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.18373290179087765
[2019-04-23 09:42:24,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23417704]
[2019-04-23 09:42:24,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.65449890833333, 98.47447721666666, 1.0, 2.0, 0.4484980531466788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700949.915453486, 700949.9154534866, 184682.2181351544]
[2019-04-23 09:42:24,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:42:24,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1429759035276238
[2019-04-23 09:43:41,401] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:43:41,660] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:43:41,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:43:41,803] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:43:41,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:43:42,864] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:43:55,010] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79706: loss 2.4005
[2019-04-23 09:43:55,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79706: learning rate 0.0005
[2019-04-23 09:43:55,407] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79829: loss 3.1662
[2019-04-23 09:43:55,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79830: learning rate 0.0005
[2019-04-23 09:43:55,533] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79852: loss 2.7859
[2019-04-23 09:43:55,536] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79852: learning rate 0.0005
[2019-04-23 09:43:55,704] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79890: loss 3.2891
[2019-04-23 09:43:55,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79892: learning rate 0.0005
[2019-04-23 09:43:55,875] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79939: loss 3.4709
[2019-04-23 09:43:55,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79939: learning rate 0.0005
[2019-04-23 09:43:55,984] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79950: loss 3.2159
[2019-04-23 09:43:55,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79950: learning rate 0.0005
[2019-04-23 09:43:56,194] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80011: loss 3.0790
[2019-04-23 09:43:56,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80012: learning rate 0.0005
[2019-04-23 09:43:56,294] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80022: loss 2.8649
[2019-04-23 09:43:56,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80023: learning rate 0.0005
[2019-04-23 09:43:56,386] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80033: loss 2.5416
[2019-04-23 09:43:56,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80033: learning rate 0.0005
[2019-04-23 09:43:56,473] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80040: loss 2.6604
[2019-04-23 09:43:56,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80040: learning rate 0.0005
[2019-04-23 09:43:56,581] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80058: loss 2.4846
[2019-04-23 09:43:56,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80059: loss 2.2331
[2019-04-23 09:43:56,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80058: learning rate 0.0005
[2019-04-23 09:43:56,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80059: learning rate 0.0005
[2019-04-23 09:43:56,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80077: loss 2.2046
[2019-04-23 09:43:56,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80079: learning rate 0.0005
[2019-04-23 09:43:56,841] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80084: loss 2.2280
[2019-04-23 09:43:56,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80085: learning rate 0.0005
[2019-04-23 09:43:56,937] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80094: loss 2.1308
[2019-04-23 09:43:56,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80094: learning rate 0.0005
[2019-04-23 09:43:57,030] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80107: loss 2.4267
[2019-04-23 09:43:57,035] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80107: learning rate 0.0005
[2019-04-23 09:44:00,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:00,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-23 09:44:00,496] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.5764305647285458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868067.4489496684, 868067.4489496684, 204174.1835280488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5250409983552279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2534094491690596, 0.2534094491690596, 0.31345196911197226], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.8129751], dtype=float32), -1.8449723]. 
=============================================
[2019-04-23 09:44:00,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:00,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9972
[2019-04-23 09:44:00,859] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.5290479725213855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 810072.181546793, 810072.1815467936, 196816.9398456459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.5855272744419193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893171.9750684722, 893171.9750684722, 207240.9124076013], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.97, 1.0, 1.0, 0.500635270411951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24810332640790894, 0.24810332640790894, 0.30931479463821093], 
reward next is 0.6907, 
noisyNet noise sample is [array([-0.7251426], dtype=float32), -0.36925295]. 
=============================================
[2019-04-23 09:44:14,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:14,822] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1264
[2019-04-23 09:44:14,829] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.578124993567868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807881.3708210465, 807881.3708210465, 196741.2898296856], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.77, 1.0, 1.0, 0.4917168597203228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22441149189473514, 0.22441149189473514, 0.2936437161637098], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.31103012], dtype=float32), -1.3185787]. 
=============================================
[2019-04-23 09:44:17,331] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87698: loss 0.0229
[2019-04-23 09:44:17,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87698: learning rate 0.0005
[2019-04-23 09:44:17,600] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87795: loss 0.0149
[2019-04-23 09:44:17,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87796: learning rate 0.0005
[2019-04-23 09:44:17,769] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87861: loss 0.0359
[2019-04-23 09:44:17,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87862: learning rate 0.0005
[2019-04-23 09:44:17,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87926: loss 0.0287
[2019-04-23 09:44:17,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87928: learning rate 0.0005
[2019-04-23 09:44:18,034] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87958: loss 0.0212
[2019-04-23 09:44:18,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87959: learning rate 0.0005
[2019-04-23 09:44:18,048] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87962: loss 0.0224
[2019-04-23 09:44:18,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87962: learning rate 0.0005
[2019-04-23 09:44:18,088] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87976: loss 0.0145
[2019-04-23 09:44:18,096] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87979: learning rate 0.0005
[2019-04-23 09:44:18,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88008: loss 0.0171
[2019-04-23 09:44:18,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88008: learning rate 0.0005
[2019-04-23 09:44:18,218] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88025: loss 0.0213
[2019-04-23 09:44:18,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88025: learning rate 0.0005
[2019-04-23 09:44:18,260] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88039: loss 0.0366
[2019-04-23 09:44:18,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88040: learning rate 0.0005
[2019-04-23 09:44:18,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88056: loss 0.0315
[2019-04-23 09:44:18,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88057: learning rate 0.0005
[2019-04-23 09:44:18,334] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88067: loss 0.0574
[2019-04-23 09:44:18,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88068: learning rate 0.0005
[2019-04-23 09:44:18,345] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88070: loss 0.0648
[2019-04-23 09:44:18,347] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88070: learning rate 0.0005
[2019-04-23 09:44:18,421] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88099: loss 0.0701
[2019-04-23 09:44:18,423] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88099: learning rate 0.0005
[2019-04-23 09:44:18,501] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88129: loss 0.0499
[2019-04-23 09:44:18,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88130: learning rate 0.0005
[2019-04-23 09:44:18,549] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88145: loss 0.0327
[2019-04-23 09:44:18,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88146: learning rate 0.0005
[2019-04-23 09:44:20,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:20,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-23 09:44:20,391] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 77.0, 1.0, 2.0, 0.5900030291869829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824486.3753322603, 824486.3753322603, 198898.6271320656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [30.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5886233678286997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822557.652259809, 822557.652259809, 198645.9138852008], 
processed observation next is [0.0, 0.6956521739130435, 0.6366508688783573, 0.7766666666666667, 1.0, 1.0, 0.504365503408072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22848823673883584, 0.22848823673883584, 0.2964864386346281], 
reward next is 0.7035, 
noisyNet noise sample is [array([0.15837975], dtype=float32), -0.369176]. 
=============================================
[2019-04-23 09:44:22,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:22,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5701
[2019-04-23 09:44:22,166] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5375003237314502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751091.7178992851, 751091.7178992851, 189680.1092237539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3367800.0000, 
sim time next is 3368400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5387947595640994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752901.1765240583, 752901.176524059, 189897.467371552], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44433103561939685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091392157011273, 0.2091392157011275, 0.2834290557784358], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.6094721], dtype=float32), -0.8732687]. 
=============================================
[2019-04-23 09:44:31,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:31,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8146
[2019-04-23 09:44:31,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.7490208551332743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046811.75168703, 1046811.75168703, 231779.4291621183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.7951577808035653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1111325.240513955, 1111325.240513955, 242723.3068325832], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.79, 1.0, 1.0, 0.7532021455464641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30870145569832086, 0.30870145569832086, 0.3622735922874376], 
reward next is 0.6377, 
noisyNet noise sample is [array([0.4042055], dtype=float32), 0.13292319]. 
=============================================
[2019-04-23 09:44:36,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:36,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3711
[2019-04-23 09:44:36,394] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7268589847660603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015824.093397069, 1015824.093397069, 226746.1007951709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3553200.0000, 
sim time next is 3553800.0000, 
raw observation next is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7438852858019134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039630.902383026, 1039630.902383027, 230599.472486983], 
processed observation next is [1.0, 0.13043478260869565, 0.4747235387045811, 0.7933333333333334, 1.0, 1.0, 0.6914280551830282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28878636177306277, 0.28878636177306305, 0.3441783171447507], 
reward next is 0.6558, 
noisyNet noise sample is [array([-0.14580765], dtype=float32), 0.079631045]. 
=============================================
[2019-04-23 09:44:38,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:38,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1340
[2019-04-23 09:44:38,353] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2252596.248552679 W.
[2019-04-23 09:44:38,363] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 0.5369656516698038, 1.0, 1.0, 0.5369656516698038, 1.0, 2.0, 0.9325317596959024, 6.911199999999999, 6.9112, 170.5573041426782, 2252596.248552679, 2252596.248552679, 441649.8140312906], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.7854771491153139, 1.0, 2.0, 0.7854771491153139, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2196691.051231064, 2196691.051231063, 412865.8897629573], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.7415387338738721, 1.0, 1.0, 0.7415387338738721, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6101919586752955, 0.6101919586752953, 0.6162177459148617], 
reward next is 0.3838, 
noisyNet noise sample is [array([0.20610292], dtype=float32), -1.2093532]. 
=============================================
[2019-04-23 09:44:38,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[45.928867]
 [47.219994]
 [47.6591  ]
 [47.28787 ]
 [48.542656]], R is [[46.90467072]
 [46.43562317]
 [45.9712677 ]
 [45.8938446 ]
 [45.83606339]].
[2019-04-23 09:44:38,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:38,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6481
[2019-04-23 09:44:38,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2299437.998989636 W.
[2019-04-23 09:44:38,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 1.003169263889139, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000558170183028, 6.9112, 168.9123545011351, 2299437.998989636, 2236044.463527616, 464487.5258107574], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5274655429183829, 1.0, 1.0, 0.5274655429183829, 1.0, 2.0, 0.9160332125286569, 6.911200000000001, 6.9112, 170.5573041426782, 2212707.564343199, 2212707.564343198, 434658.7969856866], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.43068137701009984, 1.0, 0.5, 0.43068137701009984, 1.0, 1.0, 0.8976014786934838, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6146409900953331, 0.6146409900953328, 0.648744473112965], 
reward next is 0.3513, 
noisyNet noise sample is [array([-0.08388752], dtype=float32), -0.8226827]. 
=============================================
[2019-04-23 09:44:38,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[48.41023 ]
 [48.12054 ]
 [47.940205]
 [47.54949 ]
 [47.458   ]], R is [[47.31481552]
 [46.84166718]
 [46.37324905]
 [45.90951538]
 [45.45042038]].
[2019-04-23 09:44:38,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95743: loss -49.1243
[2019-04-23 09:44:38,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95743: learning rate 0.0005
[2019-04-23 09:44:39,061] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95814: loss 53.6367
[2019-04-23 09:44:39,062] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95814: learning rate 0.0005
[2019-04-23 09:44:39,113] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95832: loss -248.7453
[2019-04-23 09:44:39,114] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95832: learning rate 0.0005
[2019-04-23 09:44:39,338] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95915: loss -172.8149
[2019-04-23 09:44:39,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95916: learning rate 0.0005
[2019-04-23 09:44:39,386] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95933: loss -271.7799
[2019-04-23 09:44:39,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95933: learning rate 0.0005
[2019-04-23 09:44:39,432] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95951: loss -208.0046
[2019-04-23 09:44:39,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95951: learning rate 0.0005
[2019-04-23 09:44:39,523] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95982: loss -179.4750
[2019-04-23 09:44:39,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95982: learning rate 0.0005
[2019-04-23 09:44:39,630] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96027: loss -58.7132
[2019-04-23 09:44:39,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96027: learning rate 0.0005
[2019-04-23 09:44:39,650] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96036: loss 16.8237
[2019-04-23 09:44:39,652] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96036: learning rate 0.0005
[2019-04-23 09:44:39,661] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96037: loss -28.1817
[2019-04-23 09:44:39,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96038: learning rate 0.0005
[2019-04-23 09:44:39,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96059: loss 27.8904
[2019-04-23 09:44:39,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96059: learning rate 0.0005
[2019-04-23 09:44:39,729] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96066: loss -3.1959
[2019-04-23 09:44:39,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96066: learning rate 0.0005
[2019-04-23 09:44:39,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96071: loss -31.8060
[2019-04-23 09:44:39,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96072: learning rate 0.0005
[2019-04-23 09:44:39,822] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96102: loss -49.0251
[2019-04-23 09:44:39,825] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96103: learning rate 0.0005
[2019-04-23 09:44:39,910] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96130: loss -61.7807
[2019-04-23 09:44:39,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96131: learning rate 0.0005
[2019-04-23 09:44:40,052] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96184: loss -56.1033
[2019-04-23 09:44:40,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96185: learning rate 0.0005
[2019-04-23 09:44:43,480] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:43,493] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-23 09:44:43,503] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1986822.479053878 W.
[2019-04-23 09:44:43,508] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.4736689112166798, 1.0, 1.0, 0.4736689112166798, 1.0, 2.0, 0.8150314036204213, 6.9112, 6.9112, 170.5573041426782, 1986822.479053878, 1986822.479053878, 396124.4567329639], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [29.66666666666667, 72.66666666666667, 1.0, 2.0, 0.8249122871374834, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990288177907447, 6.9112, 168.9124860303226, 2049939.696320287, 1993831.977187275, 414671.0099807826], 
processed observation next is [1.0, 0.391304347826087, 0.6050552922590839, 0.7266666666666667, 1.0, 1.0, 0.7890509483584137, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00790881779074466, 0.0, 0.8294376348795838, 0.5694276934223019, 0.5538422158853542, 0.6189119551951979], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.50519514], dtype=float32), -0.7972966]. 
=============================================
[2019-04-23 09:44:44,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:44:44,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5647
[2019-04-23 09:44:44,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2178067.943854704 W.
[2019-04-23 09:44:44,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.7788247945120497, 1.0, 2.0, 0.7788247945120497, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2178067.943854704, 2178067.943854704, 409688.6080548358], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3661200.0000, 
sim time next is 3661800.0000, 
raw observation next is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.7057778701679969, 1.0, 2.0, 0.7057778701679969, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1973596.122498524, 1973596.122498524, 376605.3471016664], 
processed observation next is [1.0, 0.391304347826087, 0.5892575039494474, 0.7333333333333334, 1.0, 1.0, 0.6455155062265022, 1.0, 1.0, 0.6455155062265022, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5482211451384789, 0.5482211451384789, 0.5620975329875617], 
reward next is 0.4379, 
noisyNet noise sample is [array([-1.4440963], dtype=float32), -0.27285486]. 
=============================================
[2019-04-23 09:44:50,200] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 09:44:50,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:44:50,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,203] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:44:50,206] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:44:50,207] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:44:50,207] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:44:50,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,209] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,212] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:44:50,230] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,231] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,271] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,288] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run5
[2019-04-23 09:44:50,311] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run5
[2019-04-23 09:44:52,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:44:52,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 89.5, 1.0, 2.0, 0.4135951609476725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626516.2791282908, 626516.2791282908, 177227.6711084742]
[2019-04-23 09:44:52,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:44:52,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3641793054958158
[2019-04-23 09:45:05,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:05,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 63.0, 1.0, 2.0, 0.339616266163696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529191.2975675158, 529191.2975675158, 169109.6313013383]
[2019-04-23 09:45:05,807] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:45:05,814] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.22239858031408244
[2019-04-23 09:45:08,802] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:08,802] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.60527901666667, 89.04442147833333, 1.0, 2.0, 0.2772225403852945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 454207.8395548508, 454207.8395548508, 163713.7701505831]
[2019-04-23 09:45:08,804] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:45:08,808] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6908207535117291
[2019-04-23 09:45:32,647] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:45:32,647] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.68222579333333, 93.26226953666668, 1.0, 2.0, 0.8205785663779307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1146872.924139572, 1146872.924139572, 249031.0281789241]
[2019-04-23 09:45:32,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:45:32,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7200378210256436
[2019-04-23 09:46:57,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.20027252]
[2019-04-23 09:46:57,381] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.8069539, 72.064530515, 1.0, 2.0, 0.7473084481801283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044417.360102242, 1044417.360102242, 231388.5616718912]
[2019-04-23 09:46:57,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:46:57,383] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6720242013393194
[2019-04-23 09:46:59,128] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:47:00,295] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:47:00,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:47:00,455] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:47:00,749] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:47:01,767] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 100000, evaluation results [100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:47:01,912] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7918226e-37 0.0000000e+00], sum to 1.0000
[2019-04-23 09:47:01,921] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9914
[2019-04-23 09:47:01,927] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1776223.590963016 W.
[2019-04-23 09:47:01,937] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 68.83333333333334, 1.0, 2.0, 0.6352487977747312, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.96240978740479, 6.9112, 168.9126303600648, 1776223.590963016, 1739893.67542198, 372353.1063976838], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3748200.0000, 
sim time next is 3748800.0000, 
raw observation next is [29.66666666666667, 67.66666666666667, 1.0, 2.0, 0.4630493226714034, 1.0, 1.0, 0.4630493226714034, 1.0, 2.0, 0.7865663872897917, 6.911199999999999, 6.9112, 170.5573041426782, 1942237.829281321, 1942237.829281322, 387577.6294630373], 
processed observation next is [1.0, 0.391304347826087, 0.6050552922590839, 0.6766666666666667, 1.0, 1.0, 0.35307147309807635, 1.0, 0.5, 0.35307147309807635, 1.0, 1.0, 0.7397151064509655, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5395105081337003, 0.5395105081337005, 0.5784740738254288], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48943597], dtype=float32), 1.7666098]. 
=============================================
[2019-04-23 09:47:05,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:05,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9708
[2019-04-23 09:47:05,092] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5199642814043567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726578.8381689326, 726578.8381689332, 186783.0717770881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5197819404245921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726323.954523323, 726323.9545233224, 186753.4403347651], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4214240246079423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20175665403425638, 0.20175665403425622, 0.2787364781115897], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.02207906], dtype=float32), 0.25232762]. 
=============================================
[2019-04-23 09:47:05,928] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:05,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-23 09:47:05,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5184873768171205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724514.3625625372, 724514.3625625379, 186543.3496807762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3800400.0000, 
sim time next is 3801000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.518497269532667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724528.1909779884, 724528.1909779878, 186544.9532383817], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41987622835261074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20125783082721901, 0.20125783082721885, 0.27842530334086824], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.80104715], dtype=float32), 0.005227282]. 
=============================================
[2019-04-23 09:47:05,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.65469 ]
 [75.6099  ]
 [75.58675 ]
 [75.58776 ]
 [75.602905]], R is [[75.63366699]
 [75.5989151 ]
 [75.56423187]
 [75.52966309]
 [75.49568176]].
[2019-04-23 09:47:07,568] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:07,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-23 09:47:07,585] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5358341797314213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748762.6625055039, 748762.6625055044, 189401.0652399772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3822600.0000, 
sim time next is 3823200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5357262742093543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748611.8246158515, 748611.8246158515, 189383.0216842303], 
processed observation next is [0.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.440634065312475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20794772905995876, 0.20794772905995876, 0.2826612263943736], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.52428716], dtype=float32), -0.40398607]. 
=============================================
[2019-04-23 09:47:08,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:08,877] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3215
[2019-04-23 09:47:08,885] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 61.0, 1.0, 2.0, 0.616858181621116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862029.7291866334, 862029.7291866334, 203929.5574139173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3848400.0000, 
sim time next is 3849000.0000, 
raw observation next is [34.58333333333334, 60.83333333333334, 1.0, 2.0, 0.6432020043697867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898859.5364505397, 898859.5364505397, 209070.0666631784], 
processed observation next is [0.0, 0.5652173913043478, 0.8380726698262247, 0.6083333333333334, 1.0, 1.0, 0.5701228968310683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24968320456959436, 0.24968320456959436, 0.3120448756166842], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.79209435], dtype=float32), -0.8459704]. 
=============================================
[2019-04-23 09:47:08,895] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.916695]
 [72.81635 ]
 [72.81455 ]
 [72.79055 ]
 [72.76064 ]], R is [[72.64749146]
 [72.61664581]
 [72.58614349]
 [72.55599213]
 [72.52616119]].
[2019-04-23 09:47:11,559] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103671: loss 0.6940
[2019-04-23 09:47:11,562] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103671: learning rate 0.0005
[2019-04-23 09:47:11,842] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103777: loss 0.3664
[2019-04-23 09:47:11,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103777: learning rate 0.0005
[2019-04-23 09:47:11,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103792: loss 0.4071
[2019-04-23 09:47:11,889] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103792: learning rate 0.0005
[2019-04-23 09:47:12,278] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103945: loss 0.6708
[2019-04-23 09:47:12,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103947: learning rate 0.0005
[2019-04-23 09:47:12,294] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103951: loss 0.6271
[2019-04-23 09:47:12,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103951: learning rate 0.0005
[2019-04-23 09:47:12,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103954: loss 0.6396
[2019-04-23 09:47:12,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103954: learning rate 0.0005
[2019-04-23 09:47:12,342] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103964: loss 0.7573
[2019-04-23 09:47:12,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103964: learning rate 0.0005
[2019-04-23 09:47:12,473] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104016: loss 0.9235
[2019-04-23 09:47:12,477] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104016: learning rate 0.0005
[2019-04-23 09:47:12,519] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104031: loss 0.9624
[2019-04-23 09:47:12,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104033: learning rate 0.0005
[2019-04-23 09:47:12,603] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104059: loss 0.8820
[2019-04-23 09:47:12,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104060: learning rate 0.0005
[2019-04-23 09:47:12,630] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104071: loss 1.0815
[2019-04-23 09:47:12,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104071: learning rate 0.0005
[2019-04-23 09:47:12,666] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104086: loss 1.1332
[2019-04-23 09:47:12,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104086: learning rate 0.0005
[2019-04-23 09:47:12,684] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104092: loss 1.0705
[2019-04-23 09:47:12,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104093: learning rate 0.0005
[2019-04-23 09:47:12,700] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104096: loss 1.1379
[2019-04-23 09:47:12,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104097: learning rate 0.0005
[2019-04-23 09:47:12,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104106: loss 1.0261
[2019-04-23 09:47:12,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104106: learning rate 0.0005
[2019-04-23 09:47:13,007] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104208: loss 0.5020
[2019-04-23 09:47:13,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104208: learning rate 0.0005
[2019-04-23 09:47:17,239] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:17,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-23 09:47:17,259] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.610584911719432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853259.6139355367, 853259.6139355374, 202733.9930727872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6046563939403528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844971.5333202941, 844971.5333202948, 201616.6447879491], 
processed observation next is [0.0, 0.9130434782608695, 0.6840442338072673, 0.7366666666666667, 1.0, 1.0, 0.5236824023377744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23471431481119281, 0.234714314811193, 0.30092036535514793], 
reward next is 0.6991, 
noisyNet noise sample is [array([-1.6957014], dtype=float32), -0.118513666]. 
=============================================
[2019-04-23 09:47:24,319] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:24,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7392
[2019-04-23 09:47:24,448] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.5, 1.0, 2.0, 0.5400614900624227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754671.9095572466, 754671.909557246, 190111.1631032673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [27.66666666666666, 85.66666666666666, 1.0, 2.0, 0.5398106426584461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754321.2555846643, 754321.255584665, 190068.885851162], 
processed observation next is [1.0, 0.043478260869565216, 0.5102685624012636, 0.8566666666666666, 1.0, 1.0, 0.44555499115475433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2095336821068512, 0.2095336821068514, 0.28368490425546566], 
reward next is 0.7163, 
noisyNet noise sample is [array([1.6167109], dtype=float32), 0.9208033]. 
=============================================
[2019-04-23 09:47:26,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:26,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4715
[2019-04-23 09:47:26,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.735177323856685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027455.038609861, 1027455.038609861, 228622.2823998151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4089600.0000, 
sim time next is 4090200.0000, 
raw observation next is [28.33333333333334, 83.16666666666667, 1.0, 2.0, 0.7867123272419863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1099515.62842214, 1099515.62842214, 240676.6542889805], 
processed observation next is [1.0, 0.34782608695652173, 0.5418641390205374, 0.8316666666666667, 1.0, 1.0, 0.7430269002915497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30542100789503884, 0.30542100789503884, 0.35921888699847837], 
reward next is 0.6408, 
noisyNet noise sample is [array([1.0439641], dtype=float32), -1.1602951]. 
=============================================
[2019-04-23 09:47:30,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:30,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4057
[2019-04-23 09:47:30,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 85.66666666666667, 1.0, 2.0, 0.9084117563382776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1269705.450704207, 1269705.450704207, 272265.2237851233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [28.5, 86.5, 1.0, 2.0, 0.9235018350140526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1290809.980066686, 1290809.980066686, 276487.330545371], 
processed observation next is [1.0, 0.13043478260869565, 0.5497630331753555, 0.865, 1.0, 1.0, 0.9078335361615091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35855832779630165, 0.35855832779630165, 0.41266765753040446], 
reward next is 0.5873, 
noisyNet noise sample is [array([-0.40806472], dtype=float32), -1.3773901]. 
=============================================
[2019-04-23 09:47:33,095] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111705: loss -145.7855
[2019-04-23 09:47:33,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111705: learning rate 0.0005
[2019-04-23 09:47:33,139] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111722: loss -108.6601
[2019-04-23 09:47:33,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111722: learning rate 0.0005
[2019-04-23 09:47:33,403] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111817: loss -93.5915
[2019-04-23 09:47:33,405] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111817: learning rate 0.0005
[2019-04-23 09:47:33,712] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111935: loss -39.5687
[2019-04-23 09:47:33,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111935: learning rate 0.0005
[2019-04-23 09:47:33,743] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111947: loss -62.8649
[2019-04-23 09:47:33,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111947: learning rate 0.0005
[2019-04-23 09:47:33,765] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111955: loss -75.0018
[2019-04-23 09:47:33,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111955: learning rate 0.0005
[2019-04-23 09:47:33,793] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111965: loss -101.8634
[2019-04-23 09:47:33,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111965: learning rate 0.0005
[2019-04-23 09:47:33,811] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111971: loss -11.7853
[2019-04-23 09:47:33,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111972: learning rate 0.0005
[2019-04-23 09:47:33,835] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111978: loss -65.0363
[2019-04-23 09:47:33,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111978: learning rate 0.0005
[2019-04-23 09:47:34,142] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112092: loss -6.3219
[2019-04-23 09:47:34,143] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112092: learning rate 0.0005
[2019-04-23 09:47:34,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112092: loss -44.9352
[2019-04-23 09:47:34,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112092: learning rate 0.0005
[2019-04-23 09:47:34,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112112: loss -14.6313
[2019-04-23 09:47:34,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112112: learning rate 0.0005
[2019-04-23 09:47:34,219] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112119: loss -74.7644
[2019-04-23 09:47:34,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112119: learning rate 0.0005
[2019-04-23 09:47:34,270] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112135: loss -56.2893
[2019-04-23 09:47:34,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112136: learning rate 0.0005
[2019-04-23 09:47:34,307] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112149: loss -37.4421
[2019-04-23 09:47:34,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112149: learning rate 0.0005
[2019-04-23 09:47:34,323] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112155: loss -18.5634
[2019-04-23 09:47:34,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112155: learning rate 0.0005
[2019-04-23 09:47:40,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.46369662e-32 1.00000000e+00 0.00000000e+00 1.25472666e-29
 0.00000000e+00], sum to 1.0000
[2019-04-23 09:47:40,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3600
[2019-04-23 09:47:40,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3437362.376763312 W.
[2019-04-23 09:47:40,754] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 47.0, 1.0, 2.0, 0.9968155816987803, 1.0, 2.0, 0.8189978303636527, 1.0, 1.0, 1.03, 7.005121147468668, 6.9112, 170.5573041426782, 3437362.376763312, 3370082.838363491, 631491.6616453897], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4290000.0000, 
sim time next is 4290600.0000, 
raw observation next is [38.0, 46.0, 1.0, 2.0, 0.8571112443067193, 1.0, 2.0, 0.7491456616676221, 1.0, 2.0, 1.03, 7.005110123503014, 6.9112, 170.5573041426782, 3143821.34708386, 3076549.705599064, 575546.2923005044], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.46, 1.0, 1.0, 0.8278448726586979, 1.0, 1.0, 0.6977658574308699, 1.0, 1.0, 1.0365853658536586, 0.00939101235030142, 0.0, 0.8375144448122397, 0.8732837075232944, 0.8545971404441844, 0.8590243168664244], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2125844], dtype=float32), 0.69218796]. 
=============================================
[2019-04-23 09:47:42,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:42,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1443
[2019-04-23 09:47:42,826] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6182377847733076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863958.4425649181, 863958.4425649188, 204193.0585556088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4326000.0000, 
sim time next is 4326600.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6179569735819913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863565.8625013917, 863565.8625013924, 204139.2569754155], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5397071970867365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2398794062503866, 0.23987940625038678, 0.30468545817226195], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.2984685], dtype=float32), 2.4166045]. 
=============================================
[2019-04-23 09:47:43,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:43,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8978
[2019-04-23 09:47:43,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6211749877591769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868064.7239830779, 868064.7239830779, 204757.4684565242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6198830505090315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866258.5622388503, 866258.5622388503, 204508.9283890536], 
processed observation next is [1.0, 0.0, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5420277716976283, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24062737839968062, 0.24062737839968062, 0.30523720655082626], 
reward next is 0.6948, 
noisyNet noise sample is [array([-0.825131], dtype=float32), 1.8142822]. 
=============================================
[2019-04-23 09:47:46,163] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:46,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2192
[2019-04-23 09:47:46,182] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5578264390727593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779505.4286276018, 779505.4286276018, 193153.9909183258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [32.0, 66.33333333333334, 1.0, 2.0, 0.5586634548170861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780675.5023987315, 780675.5023987315, 193299.1968412533], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6633333333333334, 1.0, 1.0, 0.4682692226711881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21685430622186988, 0.21685430622186988, 0.2885062639421691], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.2459883], dtype=float32), 2.6337128]. 
=============================================
[2019-04-23 09:47:54,064] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:54,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1215
[2019-04-23 09:47:54,084] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.04236376], dtype=float32), 0.20512027]. 
=============================================
[2019-04-23 09:47:54,113] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119579: loss 0.0468
[2019-04-23 09:47:54,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119579: learning rate 0.0005
[2019-04-23 09:47:54,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:54,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9340
[2019-04-23 09:47:54,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.1175146], dtype=float32), 0.8102009]. 
=============================================
[2019-04-23 09:47:54,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119703: loss 0.0939
[2019-04-23 09:47:54,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119703: learning rate 0.0005
[2019-04-23 09:47:54,500] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119724: loss 0.0481
[2019-04-23 09:47:54,501] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119724: learning rate 0.0005
[2019-04-23 09:47:54,911] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119888: loss 0.1079
[2019-04-23 09:47:54,916] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119889: learning rate 0.0005
[2019-04-23 09:47:54,990] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119916: loss 0.0859
[2019-04-23 09:47:54,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119917: learning rate 0.0005
[2019-04-23 09:47:55,051] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119938: loss 0.0427
[2019-04-23 09:47:55,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119938: learning rate 0.0005
[2019-04-23 09:47:55,096] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119953: loss 0.0373
[2019-04-23 09:47:55,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119955: learning rate 0.0005
[2019-04-23 09:47:55,117] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119959: loss 0.0393
[2019-04-23 09:47:55,124] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119961: learning rate 0.0005
[2019-04-23 09:47:55,189] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119989: loss 0.0324
[2019-04-23 09:47:55,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119989: learning rate 0.0005
[2019-04-23 09:47:55,451] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120089: loss 0.0337
[2019-04-23 09:47:55,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120089: learning rate 0.0005
[2019-04-23 09:47:55,604] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120143: loss 0.0617
[2019-04-23 09:47:55,606] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120144: learning rate 0.0005
[2019-04-23 09:47:55,622] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120154: loss 0.0617
[2019-04-23 09:47:55,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120154: learning rate 0.0005
[2019-04-23 09:47:55,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120164: loss 0.0504
[2019-04-23 09:47:55,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120164: learning rate 0.0005
[2019-04-23 09:47:55,696] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120180: loss 0.0502
[2019-04-23 09:47:55,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120181: learning rate 0.0005
[2019-04-23 09:47:55,707] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120183: loss 0.0526
[2019-04-23 09:47:55,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120184: learning rate 0.0005
[2019-04-23 09:47:55,859] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120240: loss 0.0608
[2019-04-23 09:47:55,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120241: learning rate 0.0005
[2019-04-23 09:47:57,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:57,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1692
[2019-04-23 09:47:57,289] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.5013865127069486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700610.3909894257, 700610.3909894257, 183813.1069169299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.497565174219769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695268.9118359083, 695268.9118359083, 183214.9644337025], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.74, 1.0, 1.0, 0.39465683640936017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1931302532877523, 0.1931302532877523, 0.2734551707965709], 
reward next is 0.7265, 
noisyNet noise sample is [array([-2.1518946], dtype=float32), 0.5277869]. 
=============================================
[2019-04-23 09:47:57,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:47:57,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5687
[2019-04-23 09:47:57,658] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.83333333333334, 1.0, 2.0, 0.498187785142159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696139.1975420427, 696139.1975420427, 183312.3870787389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [28.0, 75.66666666666667, 1.0, 2.0, 0.5004519591862049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 699304.0666094151, 699304.0666094158, 183666.8470487834], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7566666666666667, 1.0, 1.0, 0.398134890585789, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19425112961372643, 0.19425112961372662, 0.27412962246087075], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.22683795], dtype=float32), -0.6749651]. 
=============================================
[2019-04-23 09:48:08,616] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-23 09:48:08,617] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:48:08,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:48:08,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:48:08,620] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:48:08,619] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:48:08,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,623] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,624] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,623] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,625] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:48:08,647] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,647] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,649] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,693] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run6
[2019-04-23 09:48:08,694] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-04-23 09:48:11,740] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:11,742] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.840409835, 93.94808416000001, 1.0, 2.0, 0.3000175735058938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486221.2797052862, 486221.2797052862, 166072.9488457477]
[2019-04-23 09:48:11,744] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:48:11,750] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8890620599138706
[2019-04-23 09:48:54,344] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:54,345] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.40598191, 92.25725217, 1.0, 2.0, 0.4857461252780294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678748.3774420738, 678748.3774420738, 181393.3493274871]
[2019-04-23 09:48:54,346] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:48:54,349] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14799916367403576
[2019-04-23 09:48:55,225] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:55,227] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.3395761230537678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523113.9895834465, 523113.9895834465, 168447.6071190568]
[2019-04-23 09:48:55,228] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:48:55,230] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3332244327286761
[2019-04-23 09:48:55,453] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:55,455] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.440947, 80.47096381, 1.0, 2.0, 0.6378728900277771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891409.1003228223, 891409.1003228223, 208004.8176528347]
[2019-04-23 09:48:55,456] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:48:55,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.02748286723757054
[2019-04-23 09:48:57,476] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:48:57,478] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.73780723333333, 99.80042975, 1.0, 2.0, 0.2395584700995803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396004.5478011331, 396004.5478011325, 159818.6823886287]
[2019-04-23 09:48:57,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:48:57,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6695715393943125
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.3, 51.16666666666667, 1.0, 2.0, 0.6493881457780336, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129565103979, 907508.2157245894, 907508.2157245894, 210303.2241905436]
[2019-04-23 09:49:23,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:49:23,321] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10082124073241483
[2019-04-23 09:49:53,160] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.1600933]
[2019-04-23 09:49:53,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.4, 87.0, 1.0, 2.0, 0.6770387758926479, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988562930211798, 6.9112, 168.9124328949123, 1842998.402226777, 1788114.647043369, 380400.3056830283]
[2019-04-23 09:49:53,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:49:53,164] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3409199713974237
[2019-04-23 09:49:53,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1842998.402226777 W.
[2019-04-23 09:50:15,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:50:17,096] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:50:17,131] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:50:17,217] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:50:17,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:50:18,342] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 125000, evaluation results [125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:50:23,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:23,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9500
[2019-04-23 09:50:23,835] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4857725200252103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678785.2714381652, 678785.2714381646, 181396.6560870234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4756800.0000, 
sim time next is 4757400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.485108267694878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677856.794554996, 677856.794554996, 181295.4846495466], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37964851529503374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18829355404305442, 0.18829355404305442, 0.2705902755963382], 
reward next is 0.7294, 
noisyNet noise sample is [array([1.1118612], dtype=float32), -0.8333189]. 
=============================================
[2019-04-23 09:50:25,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127659: loss -396.1098
[2019-04-23 09:50:25,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127661: learning rate 0.0005
[2019-04-23 09:50:25,575] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127713: loss -497.2686
[2019-04-23 09:50:25,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127713: learning rate 0.0005
[2019-04-23 09:50:25,895] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127836: loss -627.3439
[2019-04-23 09:50:25,899] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127836: learning rate 0.0005
[2019-04-23 09:50:25,907] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127837: loss -517.9501
[2019-04-23 09:50:25,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127840: learning rate 0.0005
[2019-04-23 09:50:26,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:26,165] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127936: loss -498.7962
[2019-04-23 09:50:26,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127936: learning rate 0.0005
[2019-04-23 09:50:26,169] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-23 09:50:26,179] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127941: loss -346.2711
[2019-04-23 09:50:26,180] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2914717.182409708 W.
[2019-04-23 09:50:26,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127941: learning rate 0.0005
[2019-04-23 09:50:26,185] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.793961163515005, 6.9112, 168.9081247747239, 2914717.182409708, 2288473.944845961, 474290.7591136899], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4809600.0000, 
sim time next is 4810200.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.9594760898215843, 1.0, 1.0, 0.9594760898215843, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2683827.594331028, 2683827.594331028, 504974.6078630974], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.9511760118332341, 1.0, 0.5, 0.9511760118332341, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7455076650919522, 0.7455076650919522, 0.7536934445717871], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52137387], dtype=float32), 0.17942068]. 
=============================================
[2019-04-23 09:50:26,214] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127954: loss -411.7746
[2019-04-23 09:50:26,215] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127954: learning rate 0.0005
[2019-04-23 09:50:26,255] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127969: loss -450.5604
[2019-04-23 09:50:26,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127969: learning rate 0.0005
[2019-04-23 09:50:26,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127985: loss -482.9890
[2019-04-23 09:50:26,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127985: learning rate 0.0005
[2019-04-23 09:50:26,499] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128058: loss -568.6868
[2019-04-23 09:50:26,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128058: learning rate 0.0005
[2019-04-23 09:50:26,632] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128103: loss -442.2797
[2019-04-23 09:50:26,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128103: learning rate 0.0005
[2019-04-23 09:50:26,687] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128127: loss -472.6254
[2019-04-23 09:50:26,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128127: learning rate 0.0005
[2019-04-23 09:50:26,740] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128146: loss -448.4095
[2019-04-23 09:50:26,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128146: learning rate 0.0005
[2019-04-23 09:50:26,796] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128167: loss -479.4893
[2019-04-23 09:50:26,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128168: learning rate 0.0005
[2019-04-23 09:50:26,800] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128168: loss -497.6752
[2019-04-23 09:50:26,803] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128169: learning rate 0.0005
[2019-04-23 09:50:26,869] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128191: loss -338.1742
[2019-04-23 09:50:26,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128191: learning rate 0.0005
[2019-04-23 09:50:30,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:30,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-23 09:50:30,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1833375.87399357 W.
[2019-04-23 09:50:30,880] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6701621294473366, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982901788839088, 6.9112, 168.912529332979, 1833375.87399357, 1782508.285849429, 379148.2280836794], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.4334938045408064, 1.0, 1.0, 0.4334938045408064, 1.0, 2.0, 0.7431128817906862, 6.9112, 6.9112, 170.5573041426782, 1818163.463784503, 1818163.463784503, 370702.6137315314], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.3174624151094053, 1.0, 0.5, 0.3174624151094053, 1.0, 1.0, 0.6867230265740076, 0.0, 0.0, 0.8375144448122397, 0.5050454066068064, 0.5050454066068064, 0.5532874831813902], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73219526], dtype=float32), 1.7546736]. 
=============================================
[2019-04-23 09:50:34,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:34,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6211
[2019-04-23 09:50:34,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5100935444767921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712781.2081546375, 712781.2081546369, 185192.638322905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4922400.0000, 
sim time next is 4923000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5097186140793517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712257.1220534906, 712257.1220534906, 185132.8167083495], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4092995350353635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19784920057041405, 0.19784920057041405, 0.2763176368781336], 
reward next is 0.7237, 
noisyNet noise sample is [array([1.5545969], dtype=float32), 0.010216483]. 
=============================================
[2019-04-23 09:50:34,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.40461]
 [74.37405]
 [74.3565 ]
 [74.26869]
 [74.24147]], R is [[74.42498779]
 [74.40433502]
 [74.3838501 ]
 [74.36361694]
 [74.34370422]].
[2019-04-23 09:50:46,577] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135575: loss 0.8814
[2019-04-23 09:50:46,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135575: learning rate 0.0005
[2019-04-23 09:50:46,785] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135651: loss 0.8791
[2019-04-23 09:50:46,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135653: learning rate 0.0005
[2019-04-23 09:50:47,216] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135816: loss 0.8989
[2019-04-23 09:50:47,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135816: learning rate 0.0005
[2019-04-23 09:50:47,228] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135820: loss 0.8978
[2019-04-23 09:50:47,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135820: learning rate 0.0005
[2019-04-23 09:50:47,525] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135934: loss 0.8638
[2019-04-23 09:50:47,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135934: loss 0.8633
[2019-04-23 09:50:47,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135935: learning rate 0.0005
[2019-04-23 09:50:47,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135935: learning rate 0.0005
[2019-04-23 09:50:47,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135935: loss 0.8613
[2019-04-23 09:50:47,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135936: loss 0.8634
[2019-04-23 09:50:47,538] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135936: learning rate 0.0005
[2019-04-23 09:50:47,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135936: learning rate 0.0005
[2019-04-23 09:50:47,552] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135940: loss 0.8599
[2019-04-23 09:50:47,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135941: learning rate 0.0005
[2019-04-23 09:50:48,095] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136145: loss 0.8344
[2019-04-23 09:50:48,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136145: learning rate 0.0005
[2019-04-23 09:50:48,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136157: loss 0.8432
[2019-04-23 09:50:48,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136160: learning rate 0.0005
[2019-04-23 09:50:48,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136164: loss 0.8373
[2019-04-23 09:50:48,139] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136164: loss 0.8313
[2019-04-23 09:50:48,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136164: learning rate 0.0005
[2019-04-23 09:50:48,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136164: learning rate 0.0005
[2019-04-23 09:50:48,219] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136190: loss 0.8380
[2019-04-23 09:50:48,223] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136190: learning rate 0.0005
[2019-04-23 09:50:48,240] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136196: loss 0.8509
[2019-04-23 09:50:48,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136197: learning rate 0.0005
[2019-04-23 09:50:48,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 136260: loss 0.8301
[2019-04-23 09:50:48,412] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 136261: learning rate 0.0005
[2019-04-23 09:50:51,654] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:51,664] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9202
[2019-04-23 09:50:51,670] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 64.0, 1.0, 2.0, 0.5497218410204334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768175.986051796, 768175.9860517967, 191753.2879752476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5156400.0000, 
sim time next is 5157000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.5469941449358731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764362.9584741134, 764362.9584741141, 191286.8474243076], 
processed observation next is [0.0, 0.6956521739130435, 0.6919431279620853, 0.645, 1.0, 1.0, 0.45420981317575065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21232304402058705, 0.21232304402058724, 0.28550275734971287], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.6607234], dtype=float32), -0.16415522]. 
=============================================
[2019-04-23 09:50:51,688] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.4238 ]
 [73.37617]
 [73.3928 ]
 [73.33786]
 [73.33259]], R is [[73.47109985]
 [73.45018768]
 [73.42795563]
 [73.40699768]
 [73.38644409]].
[2019-04-23 09:50:56,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:50:56,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5375
[2019-04-23 09:50:56,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2755920.275750069 W.
[2019-04-23 09:50:56,427] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.574118423542581, 6.9112, 168.9092531528229, 2755920.275750069, 2285633.441298408, 474692.8131645269], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5223000.0000, 
sim time next is 5223600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.9123623699653448, 1.0, 1.0, 0.9123623699653448, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2551907.439784277, 2551907.439784277, 478280.8072696804], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.8944124939341503, 1.0, 0.5, 0.8944124939341503, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7088631777178548, 0.7088631777178548, 0.7138519511487766], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5878052], dtype=float32), -0.07863935]. 
=============================================
[2019-04-23 09:51:03,139] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:03,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8205
[2019-04-23 09:51:03,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2048326.953944854 W.
[2019-04-23 09:51:03,159] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [36.21666666666667, 52.0, 1.0, 2.0, 0.7324768447454107, 1.0, 1.0, 0.7324768447454107, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2048326.953944854, 2048326.953944854, 388350.3427033167], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5316600.0000, 
sim time next is 5317200.0000, 
raw observation next is [36.2, 52.0, 1.0, 2.0, 0.9864511959997594, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00599259176818, 6.9112, 168.9123930989401, 2276038.160631194, 2208789.257832014, 459168.7210722109], 
processed observation next is [1.0, 0.5652173913043478, 0.9146919431279622, 0.52, 1.0, 1.0, 0.9836761397587462, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009479259176818022, 0.0, 0.8294371785439515, 0.6322328223975539, 0.6135525716200039, 0.6853264493615088], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8792398], dtype=float32), 1.0501719]. 
=============================================
[2019-04-23 09:51:07,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:07,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8371
[2019-04-23 09:51:07,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3154081.112698884 W.
[2019-04-23 09:51:07,783] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.01666666666667, 60.66666666666666, 1.0, 2.0, 0.8619947058759556, 1.0, 2.0, 0.7515873924522406, 1.0, 2.0, 1.03, 7.005110508765996, 6.9112, 170.5573041426782, 3154081.112698884, 3086809.195234542, 577379.5853603571], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5395800.0000, 
sim time next is 5396400.0000, 
raw observation next is [35.2, 60.0, 1.0, 2.0, 0.8731737009514918, 1.0, 2.0, 0.7571768899900087, 1.0, 2.0, 1.03, 7.005111390716066, 6.9112, 170.5573041426782, 3177567.612445308, 3110295.063204255, 581608.2601424089], 
processed observation next is [1.0, 0.4782608695652174, 0.8672985781990523, 0.6, 1.0, 1.0, 0.8471972300620383, 1.0, 1.0, 0.7074420361325405, 1.0, 1.0, 1.0365853658536586, 0.009391139071606602, 0.0, 0.8375144448122397, 0.8826576701236967, 0.8639708508900709, 0.8680720300632968], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29023135], dtype=float32), 0.4321464]. 
=============================================
[2019-04-23 09:51:08,072] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143628: loss -54.6212
[2019-04-23 09:51:08,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143629: learning rate 0.0005
[2019-04-23 09:51:08,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:08,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5586
[2019-04-23 09:51:08,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2345593.846060887 W.
[2019-04-23 09:51:08,104] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.3, 66.66666666666667, 1.0, 2.0, 0.5591132942403283, 1.0, 2.0, 0.5591132942403283, 1.0, 1.0, 0.970994890503583, 6.9112, 6.9112, 170.5573041426782, 2345593.846060887, 2345593.846060887, 458432.5332861157], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5390400.0000, 
sim time next is 5391000.0000, 
raw observation next is [33.5, 66.0, 1.0, 2.0, 0.829842654425213, 1.0, 2.0, 0.829842654425213, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2320882.746340751, 2320882.746340751, 434674.4882491853], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.66, 1.0, 1.0, 0.7949911499098952, 1.0, 1.0, 0.7949911499098952, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6446896517613198, 0.6446896517613198, 0.6487678929092319], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6670949], dtype=float32), 0.9026759]. 
=============================================
[2019-04-23 09:51:08,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[32.0251  ]
 [31.30671 ]
 [28.123363]
 [26.003176]
 [27.44597 ]], R is [[32.26314163]
 [32.25628281]
 [32.27173615]
 [31.94901848]
 [31.62952805]].
[2019-04-23 09:51:08,218] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143681: loss -12.4814
[2019-04-23 09:51:08,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143681: learning rate 0.0005
[2019-04-23 09:51:08,652] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143842: loss -26.5266
[2019-04-23 09:51:08,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143842: learning rate 0.0005
[2019-04-23 09:51:08,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143892: loss -20.5130
[2019-04-23 09:51:08,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143892: learning rate 0.0005
[2019-04-23 09:51:08,842] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143911: loss -87.6924
[2019-04-23 09:51:08,843] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143911: learning rate 0.0005
[2019-04-23 09:51:08,897] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143931: loss -31.2438
[2019-04-23 09:51:08,901] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143932: learning rate 0.0005
[2019-04-23 09:51:08,917] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 143937: loss -30.2159
[2019-04-23 09:51:08,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 143938: learning rate 0.0005
[2019-04-23 09:51:08,987] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143962: loss -49.0695
[2019-04-23 09:51:08,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143963: learning rate 0.0005
[2019-04-23 09:51:09,062] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 143988: loss -33.5299
[2019-04-23 09:51:09,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 143989: learning rate 0.0005
[2019-04-23 09:51:09,349] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144093: loss -39.4555
[2019-04-23 09:51:09,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144093: learning rate 0.0005
[2019-04-23 09:51:09,398] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144110: loss -45.0792
[2019-04-23 09:51:09,402] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144111: learning rate 0.0005
[2019-04-23 09:51:09,420] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144119: loss -64.6569
[2019-04-23 09:51:09,422] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144119: learning rate 0.0005
[2019-04-23 09:51:09,473] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144138: loss -50.5932
[2019-04-23 09:51:09,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144140: learning rate 0.0005
[2019-04-23 09:51:09,484] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144142: loss -41.0866
[2019-04-23 09:51:09,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144142: learning rate 0.0005
[2019-04-23 09:51:09,568] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144173: loss -62.4672
[2019-04-23 09:51:09,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144174: learning rate 0.0005
[2019-04-23 09:51:09,809] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 144259: loss -37.7257
[2019-04-23 09:51:09,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 144259: learning rate 0.0005
[2019-04-23 09:51:16,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9790066e-32 1.0000000e+00 8.5946384e-37 5.1830958e-27 0.0000000e+00], sum to 1.0000
[2019-04-23 09:51:16,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5067
[2019-04-23 09:51:16,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2837836.848022943 W.
[2019-04-23 09:51:16,208] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.96666666666667, 48.33333333333333, 1.0, 2.0, 1.014472375895918, 1.0, 2.0, 1.014472375895918, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2837836.848022943, 2837836.848022943, 537790.568746211], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5498400.0000, 
sim time next is 5499000.0000, 
raw observation next is [35.85, 48.5, 1.0, 2.0, 0.7228090117642364, 1.0, 2.0, 0.6819945453963807, 1.0, 1.0, 1.03, 7.005099530694256, 6.9112, 170.5573041426782, 2861696.02909468, 2794431.975669685, 528629.5078243153], 
processed observation next is [1.0, 0.6521739130434783, 0.8981042654028437, 0.485, 1.0, 1.0, 0.6660349539328149, 1.0, 1.0, 0.6168608980679285, 1.0, 0.5, 1.0365853658536586, 0.009389953069425605, 0.0, 0.8375144448122397, 0.7949155636374111, 0.7762311043526903, 0.7889992654094259], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1236567], dtype=float32), -0.54269516]. 
=============================================
[2019-04-23 09:51:16,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[23.8885  ]
 [23.16751 ]
 [23.201496]
 [23.543066]
 [23.082035]], R is [[23.19743919]
 [23.16279221]
 [22.93116379]
 [22.7018528 ]
 [22.47483444]].
[2019-04-23 09:51:19,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:51:19,673] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6513
[2019-04-23 09:51:19,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 92.5, 1.0, 2.0, 0.701682999488077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980623.0700098822, 980623.0700098815, 221208.1142939565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5549400.0000, 
sim time next is 5550000.0000, 
raw observation next is [26.33333333333333, 91.66666666666667, 1.0, 2.0, 0.6955827965759316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972093.9514108831, 972093.9514108831, 219894.0566086252], 
processed observation next is [1.0, 0.21739130434782608, 0.44707740916271704, 0.9166666666666667, 1.0, 1.0, 0.6332322850312428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2700260976141342, 0.2700260976141342, 0.3282000844904854], 
reward next is 0.6718, 
noisyNet noise sample is [array([-1.1355404], dtype=float32), 0.6231233]. 
=============================================
[2019-04-23 09:51:19,698] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[53.471413]
 [53.438675]
 [53.479248]
 [53.611897]
 [53.680355]], R is [[53.81160355]
 [53.94332504]
 [54.07694244]
 [54.19968414]
 [54.34266281]].
[2019-04-23 09:51:25,218] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-23 09:51:25,219] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:51:25,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,223] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:51:25,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:51:25,225] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,226] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,228] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:51:25,227] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:51:25,233] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,234] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:51:25,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,276] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,298] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,299] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run7
[2019-04-23 09:51:25,317] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run7
[2019-04-23 09:51:38,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:51:38,506] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.973782195, 77.02405769333333, 1.0, 2.0, 0.2760897921279168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 448471.9042329647, 448471.9042329641, 163470.7127273229]
[2019-04-23 09:51:38,506] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:51:38,508] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7973601657671212
[2019-04-23 09:52:04,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:04,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.26666666666667, 96.33333333333334, 1.0, 2.0, 0.7307054591673569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021202.330666356, 1021202.330666356, 227613.4451615379]
[2019-04-23 09:52:04,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:04,944] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.45377882558486504
[2019-04-23 09:52:06,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:06,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.56666666666666, 90.33333333333333, 1.0, 2.0, 0.444090748929569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636703.5922903854, 636703.5922903861, 177381.4598196742]
[2019-04-23 09:52:06,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:06,438] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6291313579806148
[2019-04-23 09:52:21,020] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:21,021] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.46064661166667, 64.69382938333332, 1.0, 2.0, 0.5779327012082538, 0.0, 2.0, 0.0, 1.0, 1.0, 1.003677976733882, 6.911200000000001, 6.9112, 168.9127216761939, 1615839.649447756, 1615839.649447755, 353693.210234365]
[2019-04-23 09:52:21,022] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:52:21,023] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6198113105662647
[2019-04-23 09:52:22,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:22,279] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.2, 69.0, 1.0, 2.0, 0.5765971281113492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805745.4966921896, 805745.4966921896, 196466.4421906566]
[2019-04-23 09:52:22,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:52:22,282] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09287748269267637
[2019-04-23 09:52:44,755] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:44,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.56666666666666, 53.0, 1.0, 2.0, 0.5373747842663013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750916.2296672772, 750916.2296672772, 189659.0769790997]
[2019-04-23 09:52:44,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:44,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11682841138233069
[2019-04-23 09:52:49,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:52:49,477] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.08333333333333, 77.5, 1.0, 2.0, 0.5459009997419011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762834.8621751659, 762834.8621751665, 191100.422018578]
[2019-04-23 09:52:49,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:52:49,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7654681722152197
[2019-04-23 09:53:14,606] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.23527083]
[2019-04-23 09:53:14,607] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.26871077666667, 49.76786628333334, 1.0, 2.0, 0.4215926951721732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609687.5767481984, 609687.5767481984, 174882.2940384457]
[2019-04-23 09:53:14,608] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:53:14,614] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3619530385632047
[2019-04-23 09:53:32,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:53:32,943] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:53:33,034] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:53:33,367] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:53:33,415] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:53:34,430] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 150000, evaluation results [150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:53:38,552] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151550: loss 0.0222
[2019-04-23 09:53:38,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151552: learning rate 0.0005
[2019-04-23 09:53:38,707] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151610: loss 0.0921
[2019-04-23 09:53:38,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151611: learning rate 0.0005
[2019-04-23 09:53:39,175] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151780: loss 0.0250
[2019-04-23 09:53:39,177] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151780: learning rate 0.0005
[2019-04-23 09:53:39,250] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 151806: loss 0.0493
[2019-04-23 09:53:39,256] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 151807: learning rate 0.0005
[2019-04-23 09:53:39,433] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151878: loss 0.0013
[2019-04-23 09:53:39,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151878: learning rate 0.0005
[2019-04-23 09:53:39,572] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151926: loss 0.0155
[2019-04-23 09:53:39,578] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151928: learning rate 0.0005
[2019-04-23 09:53:39,590] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151935: loss 0.0180
[2019-04-23 09:53:39,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151935: learning rate 0.0005
[2019-04-23 09:53:39,723] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151982: loss 0.0022
[2019-04-23 09:53:39,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151982: learning rate 0.0005
[2019-04-23 09:53:39,902] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152050: loss 0.0146
[2019-04-23 09:53:39,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152051: learning rate 0.0005
[2019-04-23 09:53:40,072] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152112: loss 0.0450
[2019-04-23 09:53:40,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152112: learning rate 0.0005
[2019-04-23 09:53:40,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152129: loss 0.0537
[2019-04-23 09:53:40,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152130: learning rate 0.0005
[2019-04-23 09:53:40,163] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 152144: loss 0.0245
[2019-04-23 09:53:40,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 152144: learning rate 0.0005
[2019-04-23 09:53:40,177] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 152147: loss 0.0322
[2019-04-23 09:53:40,179] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 152147: learning rate 0.0005
[2019-04-23 09:53:40,281] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152189: loss 0.0152
[2019-04-23 09:53:40,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152189: learning rate 0.0005
[2019-04-23 09:53:40,362] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152221: loss 0.0182
[2019-04-23 09:53:40,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152221: learning rate 0.0005
[2019-04-23 09:53:40,793] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152380: loss 0.0561
[2019-04-23 09:53:40,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152380: learning rate 0.0005
[2019-04-23 09:53:47,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:53:47,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-23 09:53:47,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1670926.500087064 W.
[2019-04-23 09:53:47,503] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.5976199588551261, 0.0, 2.0, 0.0, 1.0, 2.0, 1.017765769310614, 6.911200000000001, 6.9112, 168.9128980057304, 1670926.500087064, 1670926.500087064, 361728.8488257495], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5797800.0000, 
sim time next is 5798400.0000, 
raw observation next is [26.6, 90.33333333333334, 1.0, 2.0, 0.5636158608221066, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9596911912790405, 6.9112, 6.9112, 168.9129564958222, 1575781.538551021, 1575781.538551021, 340797.9052790949], 
processed observation next is [1.0, 0.08695652173913043, 0.4597156398104266, 0.9033333333333334, 1.0, 1.0, 0.4742359768941043, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9508429161939518, 0.0, 0.0, 0.8294399450805667, 0.43771709404195025, 0.43771709404195025, 0.5086535899687983], 
reward next is 0.4913, 
noisyNet noise sample is [array([0.71629417], dtype=float32), -1.2518607]. 
=============================================
[2019-04-23 09:54:00,163] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159601: loss 53.3639
[2019-04-23 09:54:00,165] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159602: learning rate 0.0005
[2019-04-23 09:54:00,383] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159682: loss 131.9834
[2019-04-23 09:54:00,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159682: learning rate 0.0005
[2019-04-23 09:54:00,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159796: loss 78.5443
[2019-04-23 09:54:00,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159796: learning rate 0.0005
[2019-04-23 09:54:00,774] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159827: loss -82.1083
[2019-04-23 09:54:00,775] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159827: learning rate 0.0005
[2019-04-23 09:54:00,881] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159866: loss 108.5683
[2019-04-23 09:54:00,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159866: learning rate 0.0005
[2019-04-23 09:54:00,998] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159908: loss -17.3382
[2019-04-23 09:54:01,000] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159908: learning rate 0.0005
[2019-04-23 09:54:01,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159929: loss 167.7493
[2019-04-23 09:54:01,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159929: learning rate 0.0005
[2019-04-23 09:54:01,234] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159995: loss 4.6929
[2019-04-23 09:54:01,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159995: learning rate 0.0005
[2019-04-23 09:54:01,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160000: loss 81.7639
[2019-04-23 09:54:01,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160000: learning rate 0.0005
[2019-04-23 09:54:01,417] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160061: loss 83.5877
[2019-04-23 09:54:01,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160062: learning rate 0.0005
[2019-04-23 09:54:01,492] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160089: loss 15.6302
[2019-04-23 09:54:01,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160089: learning rate 0.0005
[2019-04-23 09:54:01,496] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 160090: loss -39.4087
[2019-04-23 09:54:01,499] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 160090: learning rate 0.0005
[2019-04-23 09:54:01,517] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 160096: loss -25.4194
[2019-04-23 09:54:01,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 160099: learning rate 0.0005
[2019-04-23 09:54:01,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160202: loss -53.9671
[2019-04-23 09:54:01,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160202: learning rate 0.0005
[2019-04-23 09:54:02,003] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160273: loss 7.5500
[2019-04-23 09:54:02,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160273: learning rate 0.0005
[2019-04-23 09:54:02,048] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160289: loss 26.4820
[2019-04-23 09:54:02,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160292: learning rate 0.0005
[2019-04-23 09:54:03,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:03,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8567
[2019-04-23 09:54:03,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 74.0, 1.0, 2.0, 0.5351201013424776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747764.4741624974, 747764.4741624967, 189282.4901317209], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6027600.0000, 
sim time next is 6028200.0000, 
raw observation next is [29.45, 75.0, 1.0, 2.0, 0.533764301461443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745869.2455511234, 745869.2455511227, 189056.2838746074], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.75, 1.0, 1.0, 0.43827024272463017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20718590154197872, 0.20718590154197852, 0.2821735580218021], 
reward next is 0.7178, 
noisyNet noise sample is [array([-2.4476666], dtype=float32), 1.6665088]. 
=============================================
[2019-04-23 09:54:18,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:18,442] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0048
[2019-04-23 09:54:18,449] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 89.5, 1.0, 2.0, 0.5272986350211235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736831.1469815219, 736831.1469815212, 187984.0942567497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6240600.0000, 
sim time next is 6241200.0000, 
raw observation next is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.52786251708802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737619.3722879834, 737619.3722879834, 188077.1312553247], 
processed observation next is [0.0, 0.21739130434782608, 0.4723538704581361, 0.8933333333333333, 1.0, 1.0, 0.43115965914219273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2048942700799954, 0.2048942700799954, 0.28071213620197716], 
reward next is 0.7193, 
noisyNet noise sample is [array([-2.0602086], dtype=float32), -0.83169895]. 
=============================================
[2019-04-23 09:54:21,412] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167452: loss 0.0105
[2019-04-23 09:54:21,417] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167452: learning rate 0.0005
[2019-04-23 09:54:22,007] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167675: loss 0.0278
[2019-04-23 09:54:22,012] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167676: learning rate 0.0005
[2019-04-23 09:54:22,171] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167739: loss 0.0027
[2019-04-23 09:54:22,175] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167739: learning rate 0.0005
[2019-04-23 09:54:22,220] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 167758: loss 0.0016
[2019-04-23 09:54:22,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 167759: learning rate 0.0005
[2019-04-23 09:54:22,451] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167844: loss 0.0209
[2019-04-23 09:54:22,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167844: learning rate 0.0005
[2019-04-23 09:54:22,632] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167913: loss 0.0156
[2019-04-23 09:54:22,634] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167914: learning rate 0.0005
[2019-04-23 09:54:22,653] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167924: loss 0.0100
[2019-04-23 09:54:22,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167924: learning rate 0.0005
[2019-04-23 09:54:22,931] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168022: loss 0.0019
[2019-04-23 09:54:22,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168022: learning rate 0.0005
[2019-04-23 09:54:22,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168031: loss 0.0078
[2019-04-23 09:54:22,955] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168031: learning rate 0.0005
[2019-04-23 09:54:23,059] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168071: loss 0.0049
[2019-04-23 09:54:23,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168072: learning rate 0.0005
[2019-04-23 09:54:23,130] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 168100: loss 0.0253
[2019-04-23 09:54:23,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 168100: learning rate 0.0005
[2019-04-23 09:54:23,190] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168118: loss 0.0455
[2019-04-23 09:54:23,193] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168118: learning rate 0.0005
[2019-04-23 09:54:23,248] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168139: loss 0.0401
[2019-04-23 09:54:23,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168139: learning rate 0.0005
[2019-04-23 09:54:23,538] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168250: loss 0.0194
[2019-04-23 09:54:23,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168250: learning rate 0.0005
[2019-04-23 09:54:23,829] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 168357: loss 0.0041
[2019-04-23 09:54:23,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 168357: learning rate 0.0005
[2019-04-23 09:54:23,897] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168382: loss 0.0081
[2019-04-23 09:54:23,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168382: learning rate 0.0005
[2019-04-23 09:54:27,884] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:27,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-23 09:54:27,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 70.5, 1.0, 2.0, 0.5158433814178596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720818.4913110913, 720818.4913110913, 186115.4075364385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6367800.0000, 
sim time next is 6368400.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5143807941019103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718774.0400198154, 718774.0400198147, 185879.6017012107], 
processed observation next is [0.0, 0.7391304347826086, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4149166193998919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19965945556105985, 0.19965945556105966, 0.2774322413450906], 
reward next is 0.7226, 
noisyNet noise sample is [array([-0.00224469], dtype=float32), -0.96323943]. 
=============================================
[2019-04-23 09:54:32,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:54:32,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4565
[2019-04-23 09:54:32,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2123125.736693108 W.
[2019-04-23 09:54:32,462] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 67.16666666666667, 1.0, 2.0, 0.8772017456978204, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979495320398028, 6.9112, 168.91255064797, 2123125.736693108, 2074674.802276952, 428746.9819633072], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6450600.0000, 
sim time next is 6451200.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.9147550710907213, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977333334459289, 6.9112, 168.9125074257809, 2175689.718422257, 2128772.579560882, 439181.1518022362], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.897295266374363, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006613333445928937, 0.0, 0.8294377399410788, 0.6043582551172936, 0.5913257165446895, 0.655494256421248], 
reward next is 0.0138, 
noisyNet noise sample is [array([0.8398278], dtype=float32), -0.27890012]. 
=============================================
[2019-04-23 09:54:41,596] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-23 09:54:41,598] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:54:41,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:54:41,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:54:41,604] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,605] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:54:41,606] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,607] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,607] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:54:41,612] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:54:41,624] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,666] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run8
[2019-04-23 09:54:41,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run8
[2019-04-23 09:54:49,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:54:49,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.56666666666667, 85.66666666666667, 1.0, 2.0, 0.3063295940111303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505308.0982090575, 505308.0982090575, 166984.6376327895]
[2019-04-23 09:54:49,246] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:54:49,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2614907172212252
[2019-04-23 09:55:17,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:55:17,416] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.50985306666666, 91.24886720666667, 1.0, 2.0, 0.4897413125986473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684332.7768400114, 684332.7768400114, 182004.5005769795]
[2019-04-23 09:55:17,418] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:55:17,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3718510350623593
[2019-04-23 09:55:34,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:55:34,437] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.95374612666667, 91.75319116, 1.0, 2.0, 0.3040831278845786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487455.1501065402, 487455.1501065395, 166190.6948996339]
[2019-04-23 09:55:34,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:55:34,440] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0896850357655139
[2019-04-23 09:56:18,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:18,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.18654572, 68.188914475, 1.0, 2.0, 0.464722003493342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661300.4903722984, 661300.4903722984, 179787.6388867173]
[2019-04-23 09:56:18,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:56:18,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5740378231212584
[2019-04-23 09:56:22,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:22,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.944618385, 88.15753422, 1.0, 2.0, 0.5999678798302436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838417.0263012078, 838417.0263012078, 200740.2178655962]
[2019-04-23 09:56:22,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:56:22,482] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.17381507174897437
[2019-04-23 09:56:42,436] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2584156]
[2019-04-23 09:56:42,439] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.21848618, 83.94704945, 1.0, 2.0, 0.3340499116203408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 527452.1393759404, 527452.1393759411, 169119.0018721139]
[2019-04-23 09:56:42,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:56:42,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15717445715836953
[2019-04-23 09:56:53,598] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 09:56:53,822] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 09:56:53,861] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 09:56:53,947] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 09:56:53,961] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 09:56:54,974] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 175000, evaluation results [175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 09:56:55,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:56:55,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7334
[2019-04-23 09:56:55,216] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 77.0, 1.0, 2.0, 0.4946960288855848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 691258.4289693049, 691258.4289693049, 182769.3002074526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6555600.0000, 
sim time next is 6556200.0000, 
raw observation next is [27.66666666666666, 77.66666666666667, 1.0, 2.0, 0.4962764284963369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693467.5050002947, 693467.5050002947, 183014.8726285548], 
processed observation next is [1.0, 0.9130434782608695, 0.5102685624012636, 0.7766666666666667, 1.0, 1.0, 0.39310413071847816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19262986250008185, 0.19262986250008185, 0.2731565263112758], 
reward next is 0.7268, 
noisyNet noise sample is [array([-1.0499811], dtype=float32), 1.1432102]. 
=============================================
[2019-04-23 09:56:56,139] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175438: loss -3.5714
[2019-04-23 09:56:56,141] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175438: learning rate 0.0005
[2019-04-23 09:56:56,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175719: loss -164.8328
[2019-04-23 09:56:56,894] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175721: learning rate 0.0005
[2019-04-23 09:56:56,918] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175730: loss 19.9320
[2019-04-23 09:56:56,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175730: learning rate 0.0005
[2019-04-23 09:56:57,032] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 175773: loss -146.9077
[2019-04-23 09:56:57,038] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 175773: learning rate 0.0005
[2019-04-23 09:56:57,259] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175859: loss -197.3134
[2019-04-23 09:56:57,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175861: learning rate 0.0005
[2019-04-23 09:56:57,286] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175867: loss -173.8707
[2019-04-23 09:56:57,287] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175867: learning rate 0.0005
[2019-04-23 09:56:57,361] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175891: loss -98.9881
[2019-04-23 09:56:57,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175891: learning rate 0.0005
[2019-04-23 09:56:57,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175981: loss -97.6831
[2019-04-23 09:56:57,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175981: learning rate 0.0005
[2019-04-23 09:56:57,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176015: loss -30.8638
[2019-04-23 09:56:57,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176015: learning rate 0.0005
[2019-04-23 09:56:57,694] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176023: loss -136.7369
[2019-04-23 09:56:57,703] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176028: learning rate 0.0005
[2019-04-23 09:56:57,838] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 176080: loss -105.4004
[2019-04-23 09:56:57,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 176080: learning rate 0.0005
[2019-04-23 09:56:57,912] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176109: loss -181.8378
[2019-04-23 09:56:57,914] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176109: learning rate 0.0005
[2019-04-23 09:56:58,087] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176170: loss -159.5296
[2019-04-23 09:56:58,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176171: learning rate 0.0005
[2019-04-23 09:56:58,199] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176211: loss -109.5986
[2019-04-23 09:56:58,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176212: learning rate 0.0005
[2019-04-23 09:56:58,405] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176290: loss -64.4436
[2019-04-23 09:56:58,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176291: learning rate 0.0005
[2019-04-23 09:56:58,644] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176378: loss -7.5602
[2019-04-23 09:56:58,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176378: learning rate 0.0005
[2019-04-23 09:57:06,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:06,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5865
[2019-04-23 09:57:06,918] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 69.83333333333333, 1.0, 2.0, 0.3731243859180476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567287.1956160339, 567287.1956160332, 171911.6274366681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6731400.0000, 
sim time next is 6732000.0000, 
raw observation next is [25.7, 70.0, 1.0, 2.0, 0.3701595994005154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564494.007154932, 564494.007154932, 171719.5684334686], 
processed observation next is [1.0, 0.9565217391304348, 0.4170616113744076, 0.7, 1.0, 1.0, 0.24115614385604264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15680389087637, 0.15680389087637, 0.2562978633335352], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.0564003], dtype=float32), 1.9291044]. 
=============================================
[2019-04-23 09:57:06,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.8418  ]
 [67.79381 ]
 [67.799644]
 [67.60704 ]
 [67.528786]], R is [[67.86051941]
 [67.92533112]
 [67.98913574]
 [68.05178833]
 [68.11329651]].
[2019-04-23 09:57:09,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:09,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6682
[2019-04-23 09:57:09,645] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 79.0, 1.0, 2.0, 0.3478090090862956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549331.0909467237, 549331.0909467237, 170876.5057150823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6764400.0000, 
sim time next is 6765000.0000, 
raw observation next is [23.13333333333333, 78.50000000000001, 1.0, 2.0, 0.3724173087015109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587538.6706923113, 587538.6706923113, 174095.6664532488], 
processed observation next is [1.0, 0.30434782608695654, 0.29541864139020524, 0.7850000000000001, 1.0, 1.0, 0.24387627554398905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1632051863034198, 0.1632051863034198, 0.25984427828843104], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.7163119], dtype=float32), 1.1638778]. 
=============================================
[2019-04-23 09:57:09,661] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.532616]
 [65.18942 ]
 [65.27956 ]
 [65.33555 ]
 [65.13953 ]], R is [[65.55167389]
 [65.64111328]
 [65.72491455]
 [65.81033325]
 [65.89789581]].
[2019-04-23 09:57:17,839] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183412: loss 0.1456
[2019-04-23 09:57:17,844] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183414: learning rate 0.0005
[2019-04-23 09:57:18,555] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183679: loss 0.0408
[2019-04-23 09:57:18,560] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183680: learning rate 0.0005
[2019-04-23 09:57:18,595] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 183693: loss 0.0432
[2019-04-23 09:57:18,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 183695: learning rate 0.0005
[2019-04-23 09:57:18,706] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183736: loss 0.0358
[2019-04-23 09:57:18,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183736: learning rate 0.0005
[2019-04-23 09:57:19,048] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183864: loss 0.0022
[2019-04-23 09:57:19,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183865: learning rate 0.0005
[2019-04-23 09:57:19,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183870: loss 0.0036
[2019-04-23 09:57:19,069] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183871: learning rate 0.0005
[2019-04-23 09:57:19,242] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183935: loss 0.0142
[2019-04-23 09:57:19,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183935: learning rate 0.0005
[2019-04-23 09:57:19,381] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 183989: loss 0.0026
[2019-04-23 09:57:19,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 183989: learning rate 0.0005
[2019-04-23 09:57:19,429] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184005: loss 0.0115
[2019-04-23 09:57:19,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184005: learning rate 0.0005
[2019-04-23 09:57:19,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:19,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4036
[2019-04-23 09:57:19,542] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.93333333333333, 50.66666666666667, 1.0, 2.0, 0.3428301808668725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529361.4641478624, 529361.4641478618, 168987.8760007616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6888000.0000, 
sim time next is 6888600.0000, 
raw observation next is [28.8, 51.5, 1.0, 2.0, 0.3473424583430342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535688.6631004182, 535688.6631004182, 169481.8156276213], 
processed observation next is [0.0, 0.7391304347826086, 0.5639810426540285, 0.515, 1.0, 1.0, 0.21366561246148696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14880240641678283, 0.14880240641678283, 0.25295793377256914], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.478378], dtype=float32), 0.4995742]. 
=============================================
[2019-04-23 09:57:19,553] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184049: loss 0.0021
[2019-04-23 09:57:19,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184050: learning rate 0.0005
[2019-04-23 09:57:19,814] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 184144: loss 0.0043
[2019-04-23 09:57:19,817] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184145: learning rate 0.0005
[2019-04-23 09:57:19,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 184160: loss 0.0027
[2019-04-23 09:57:19,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 184161: learning rate 0.0005
[2019-04-23 09:57:19,972] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184200: loss 0.0033
[2019-04-23 09:57:19,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184200: learning rate 0.0005
[2019-04-23 09:57:20,050] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184234: loss 0.0025
[2019-04-23 09:57:20,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184234: learning rate 0.0005
[2019-04-23 09:57:20,324] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184335: loss 0.0021
[2019-04-23 09:57:20,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184335: learning rate 0.0005
[2019-04-23 09:57:20,693] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184473: loss 0.0038
[2019-04-23 09:57:20,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184473: learning rate 0.0005
[2019-04-23 09:57:23,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:23,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8404
[2019-04-23 09:57:23,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.26666666666667, 52.0, 1.0, 2.0, 0.4516574988904564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 643087.5587748671, 643087.5587748665, 177912.960581691], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6961200.0000, 
sim time next is 6961800.0000, 
raw observation next is [31.08333333333333, 52.0, 1.0, 2.0, 0.4441880433620905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636648.3770958986, 636648.3770958979, 177370.571620863], 
processed observation next is [0.0, 0.5652173913043478, 0.6721958925750393, 0.52, 1.0, 1.0, 0.3303470401952897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1768467714155274, 0.1768467714155272, 0.2647321964490492], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.4067079], dtype=float32), 0.5355394]. 
=============================================
[2019-04-23 09:57:27,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:27,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8992
[2019-04-23 09:57:27,241] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.65, 83.5, 1.0, 2.0, 0.8336761607015539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1185683.662403752, 1185683.662403752, 255271.9060994001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7007400.0000, 
sim time next is 7008000.0000, 
raw observation next is [25.6, 83.66666666666667, 1.0, 2.0, 0.7935991486742273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129934.362737885, 1129934.362737885, 245252.8569832499], 
processed observation next is [1.0, 0.08695652173913043, 0.4123222748815167, 0.8366666666666667, 1.0, 1.0, 0.7513242755111172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31387065631607913, 0.31387065631607913, 0.3660490402735073], 
reward next is 0.6340, 
noisyNet noise sample is [array([-1.2389833], dtype=float32), 1.9955999]. 
=============================================
[2019-04-23 09:57:27,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.583664]
 [66.74798 ]
 [66.47123 ]
 [67.05182 ]
 [66.74173 ]], R is [[66.21154785]
 [66.16842651]
 [66.08798981]
 [66.00204468]
 [66.07502747]].
[2019-04-23 09:57:39,175] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191385: loss 0.0812
[2019-04-23 09:57:39,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191386: learning rate 0.0005
[2019-04-23 09:57:39,910] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 191661: loss 0.0760
[2019-04-23 09:57:39,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 191662: learning rate 0.0005
[2019-04-23 09:57:39,987] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191687: loss 0.0790
[2019-04-23 09:57:39,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191689: learning rate 0.0005
[2019-04-23 09:57:40,038] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191711: loss 0.0771
[2019-04-23 09:57:40,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191711: learning rate 0.0005
[2019-04-23 09:57:40,452] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191868: loss 0.0760
[2019-04-23 09:57:40,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191869: learning rate 0.0005
[2019-04-23 09:57:40,506] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191884: loss 0.0773
[2019-04-23 09:57:40,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191884: learning rate 0.0005
[2019-04-23 09:57:40,553] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191897: loss 0.0772
[2019-04-23 09:57:40,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191897: learning rate 0.0005
[2019-04-23 09:57:40,688] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 191953: loss 0.0887
[2019-04-23 09:57:40,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 191954: learning rate 0.0005
[2019-04-23 09:57:40,716] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191961: loss 0.0978
[2019-04-23 09:57:40,719] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191961: learning rate 0.0005
[2019-04-23 09:57:40,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192009: loss 0.0831
[2019-04-23 09:57:40,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192009: learning rate 0.0005
[2019-04-23 09:57:41,094] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 192093: loss 0.0825
[2019-04-23 09:57:41,097] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 192094: learning rate 0.0005
[2019-04-23 09:57:41,212] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 192139: loss 0.1106
[2019-04-23 09:57:41,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 192140: learning rate 0.0005
[2019-04-23 09:57:41,333] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192180: loss 0.0888
[2019-04-23 09:57:41,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192180: learning rate 0.0005
[2019-04-23 09:57:41,448] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192222: loss 0.0955
[2019-04-23 09:57:41,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192224: learning rate 0.0005
[2019-04-23 09:57:41,668] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 192303: loss 0.1013
[2019-04-23 09:57:41,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 192303: learning rate 0.0005
[2019-04-23 09:57:41,972] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192408: loss 0.1358
[2019-04-23 09:57:41,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192409: learning rate 0.0005
[2019-04-23 09:57:56,566] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 09:57:56,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7044
[2019-04-23 09:57:56,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 82.66666666666667, 1.0, 2.0, 0.2898212026336421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465893.8763467673, 465893.8763467673, 164675.0138125497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7416600.0000, 
sim time next is 7417200.0000, 
raw observation next is [21.63333333333333, 83.33333333333334, 1.0, 2.0, 0.2928300893170977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470243.2167977881, 470243.2167977875, 164974.4481674853], 
processed observation next is [1.0, 0.8695652173913043, 0.2243285939968403, 0.8333333333333335, 1.0, 1.0, 0.14798805941819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13062311577716337, 0.1306231157771632, 0.24623051965296314], 
reward next is 0.7538, 
noisyNet noise sample is [array([0.423527], dtype=float32), 0.06962994]. 
=============================================
[2019-04-23 09:58:00,954] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199385: loss 0.0430
[2019-04-23 09:58:00,957] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199385: learning rate 0.0005
[2019-04-23 09:58:01,665] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199651: loss 0.0247
[2019-04-23 09:58:01,670] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199651: learning rate 0.0005
[2019-04-23 09:58:01,777] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 199692: loss 0.0155
[2019-04-23 09:58:01,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 199692: learning rate 0.0005
[2019-04-23 09:58:01,860] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199722: loss 0.0129
[2019-04-23 09:58:01,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199722: learning rate 0.0005
[2019-04-23 09:58:02,251] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199869: loss 0.0207
[2019-04-23 09:58:02,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199869: learning rate 0.0005
[2019-04-23 09:58:02,356] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199908: loss 0.0038
[2019-04-23 09:58:02,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199908: learning rate 0.0005
[2019-04-23 09:58:02,418] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 199931: loss 0.0092
[2019-04-23 09:58:02,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 199931: learning rate 0.0005
[2019-04-23 09:58:02,472] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199951: loss 0.0123
[2019-04-23 09:58:02,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199951: learning rate 0.0005
[2019-04-23 09:58:02,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199974: loss 0.0486
[2019-04-23 09:58:02,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199974: learning rate 0.0005
[2019-04-23 09:58:02,604] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-23 09:58:02,605] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 09:58:02,606] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 09:58:02,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,606] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 09:58:02,607] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 09:58:02,608] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,610] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,609] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,612] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 09:58:02,616] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 09:58:02,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,656] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,657] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,705] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run9
[2019-04-23 09:58:02,723] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run9
[2019-04-23 09:58:20,961] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:20,962] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.05, 78.5, 1.0, 2.0, 0.3171040195629773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500848.2221666477, 500848.2221666477, 167076.9413814815]
[2019-04-23 09:58:20,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:58:20,968] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08410307981498089
[2019-04-23 09:58:24,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:24,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.0, 56.0, 1.0, 2.0, 0.2066179980198703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 345241.1681552971, 345241.1681552971, 155998.3848966024]
[2019-04-23 09:58:24,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:58:24,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2353325573211137
[2019-04-23 09:58:39,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:58:39,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.09368976333334, 81.42426167333333, 1.0, 2.0, 0.62560111552353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874252.5885463987, 874252.5885463987, 205613.9544992502]
[2019-04-23 09:58:39,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 09:58:39,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.27379451976561553
[2019-04-23 09:59:05,635] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:05,636] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 79.33333333333333, 1.0, 2.0, 0.5703613088713821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797028.1997051478, 797028.1997051478, 195353.5753770476]
[2019-04-23 09:59:05,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:05,643] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.814967736313254
[2019-04-23 09:59:15,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:15,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.76666666666667, 63.66666666666667, 1.0, 2.0, 0.5584023508663726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780310.502096027, 780310.502096027, 193251.9119116482]
[2019-04-23 09:59:15,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 09:59:15,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4173267115302882
[2019-04-23 09:59:18,148] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:18,149] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.5, 50.0, 1.0, 2.0, 0.6446017235272711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 900816.4402990958, 900816.4402990965, 209344.9879821384]
[2019-04-23 09:59:18,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:18,153] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15029234255396562
[2019-04-23 09:59:19,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:19,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.5, 70.5, 1.0, 2.0, 0.9660308241915481, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99566853991017, 6.9112, 168.9123871082448, 2247456.906069078, 2187532.218679623, 453421.1834107374]
[2019-04-23 09:59:19,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:19,808] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15293557783858658
[2019-04-23 09:59:19,809] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2247456.906069078 W.
[2019-04-23 09:59:29,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:29,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.29851371666667, 69.427973355, 1.0, 2.0, 0.5038302685724638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704026.2948309104, 704026.2948309104, 184198.5988004312]
[2019-04-23 09:59:29,504] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 09:59:29,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.46352673131625965
[2019-04-23 09:59:59,905] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2075179]
[2019-04-23 09:59:59,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.93333333333334, 76.33333333333334, 1.0, 2.0, 1.002648680476791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129317068804, 1401509.244939603, 1401509.244939602, 299741.7657488313]
[2019-04-23 09:59:59,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 09:59:59,909] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7917811788243619
[2019-04-23 10:00:12,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:00:12,981] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:00:13,106] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:00:13,142] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:00:13,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:00:14,174] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 200000, evaluation results [200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:00:14,263] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200039: loss 0.0174
[2019-04-23 10:00:14,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200040: learning rate 0.0005
[2019-04-23 10:00:14,501] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 200123: loss 0.0116
[2019-04-23 10:00:14,504] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 200123: learning rate 0.0005
[2019-04-23 10:00:14,588] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 200152: loss 0.0348
[2019-04-23 10:00:14,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 200154: learning rate 0.0005
[2019-04-23 10:00:14,845] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200242: loss 0.0155
[2019-04-23 10:00:14,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200243: learning rate 0.0005
[2019-04-23 10:00:15,035] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200311: loss 0.0507
[2019-04-23 10:00:15,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200311: learning rate 0.0005
[2019-04-23 10:00:15,222] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 200379: loss 0.0270
[2019-04-23 10:00:15,225] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 200379: learning rate 0.0005
[2019-04-23 10:00:15,537] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200488: loss 0.0028
[2019-04-23 10:00:15,539] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200489: learning rate 0.0005
[2019-04-23 10:00:17,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:17,432] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4541
[2019-04-23 10:00:17,439] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.0, 1.0, 2.0, 0.3996088689809195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590598.5511635176, 590598.5511635176, 173498.3430802972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7545600.0000, 
sim time next is 7546200.0000, 
raw observation next is [24.03333333333333, 89.0, 1.0, 2.0, 0.4032313478690614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594119.3487265224, 594119.3487265224, 173767.9098471192], 
processed observation next is [0.0, 0.34782608695652173, 0.3380726698262243, 0.89, 1.0, 1.0, 0.2810016239386282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.165033152424034, 0.165033152424034, 0.25935508932405854], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.0224499], dtype=float32), 0.69054884]. 
=============================================
[2019-04-23 10:00:18,778] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:18,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-23 10:00:18,796] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 62.0, 1.0, 2.0, 0.4445139563660606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633156.6034245412, 633156.6034245412, 176913.5538732694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7573800.0000, 
sim time next is 7574400.0000, 
raw observation next is [29.0, 62.0, 1.0, 2.0, 0.43750673349833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 627111.2829354965, 627111.2829354971, 176419.7443471122], 
processed observation next is [0.0, 0.6956521739130435, 0.5734597156398105, 0.62, 1.0, 1.0, 0.3222972692750964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1741975785931935, 0.17419757859319365, 0.26331305126434656], 
reward next is 0.7367, 
noisyNet noise sample is [array([-1.0767745], dtype=float32), -0.2965151]. 
=============================================
[2019-04-23 10:00:25,129] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:25,140] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2556
[2019-04-23 10:00:25,146] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.08333333333334, 64.83333333333334, 1.0, 2.0, 0.5775474897580325, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9754391719738575, 6.911199999999999, 6.9112, 168.9129565104285, 1614761.817344894, 1614761.817344895, 347443.1441871512], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7657800.0000, 
sim time next is 7658400.0000, 
raw observation next is [29.96666666666667, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.89190821441428, 6.9112, 168.9015837761941, 2859740.31682906, 1454652.348294085, 309549.8994474969], 
processed observation next is [1.0, 0.6521739130434783, 0.6192733017377569, 0.6566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.1980708214414279, 0.0, 0.8293840998207391, 0.7943723102302945, 0.4040700967483569, 0.46201477529477153], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12257934], dtype=float32), 0.36658284]. 
=============================================
[2019-04-23 10:00:34,783] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207376: loss 0.5074
[2019-04-23 10:00:34,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207378: learning rate 0.0005
[2019-04-23 10:00:35,492] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207624: loss 0.4816
[2019-04-23 10:00:35,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207625: learning rate 0.0005
[2019-04-23 10:00:35,724] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 207705: loss 0.3602
[2019-04-23 10:00:35,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 207706: learning rate 0.0005
[2019-04-23 10:00:35,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 207728: loss 0.3549
[2019-04-23 10:00:35,793] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 207729: learning rate 0.0005
[2019-04-23 10:00:36,111] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207841: loss 0.2566
[2019-04-23 10:00:36,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207841: learning rate 0.0005
[2019-04-23 10:00:36,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 207896: loss 0.2596
[2019-04-23 10:00:36,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 207896: learning rate 0.0005
[2019-04-23 10:00:36,279] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207901: loss 0.2406
[2019-04-23 10:00:36,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207903: learning rate 0.0005
[2019-04-23 10:00:36,335] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207918: loss 0.2517
[2019-04-23 10:00:36,339] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207919: learning rate 0.0005
[2019-04-23 10:00:36,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207998: loss 0.2416
[2019-04-23 10:00:36,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207998: learning rate 0.0005
[2019-04-23 10:00:36,621] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208017: loss 0.2274
[2019-04-23 10:00:36,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208018: learning rate 0.0005
[2019-04-23 10:00:36,862] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 208102: loss 0.2197
[2019-04-23 10:00:36,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 208105: learning rate 0.0005
[2019-04-23 10:00:36,922] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 208125: loss 0.2215
[2019-04-23 10:00:36,924] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 208125: learning rate 0.0005
[2019-04-23 10:00:37,337] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208268: loss 0.3431
[2019-04-23 10:00:37,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208269: learning rate 0.0005
[2019-04-23 10:00:37,389] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208288: loss 0.3300
[2019-04-23 10:00:37,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208288: learning rate 0.0005
[2019-04-23 10:00:37,499] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 208330: loss 0.3566
[2019-04-23 10:00:37,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 208330: learning rate 0.0005
[2019-04-23 10:00:37,755] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208423: loss 0.3809
[2019-04-23 10:00:37,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208423: learning rate 0.0005
[2019-04-23 10:00:39,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:39,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0134
[2019-04-23 10:00:39,223] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.75, 71.0, 1.0, 2.0, 0.3406889169242498, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5817093134495274, 6.911200000000001, 6.9112, 168.912956510431, 952233.2690440153, 952233.2690440146, 234748.4142841789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7837800.0000, 
sim time next is 7838400.0000, 
raw observation next is [29.6, 72.0, 1.0, 2.0, 0.4860680376062967, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679198.3394897704, 679198.3394897704, 181445.0872116965], 
processed observation next is [1.0, 0.7391304347826086, 0.6018957345971565, 0.72, 1.0, 1.0, 0.38080486458589957, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1886662054138251, 0.1886662054138251, 0.27081356300253207], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2906259], dtype=float32), -0.8065065]. 
=============================================
[2019-04-23 10:00:46,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:46,720] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:46,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-04-23 10:00:47,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,302] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res18/Eplus-env-sub_run2
[2019-04-23 10:00:47,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-04-23 10:00:47,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-04-23 10:00:47,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-04-23 10:00:47,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-04-23 10:00:47,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-04-23 10:00:47,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:47,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:47,976] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-04-23 10:00:48,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,060] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-04-23 10:00:48,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-04-23 10:00:48,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-04-23 10:00:48,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,131] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-04-23 10:00:48,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,183] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-04-23 10:00:48,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-04-23 10:00:48,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-04-23 10:00:48,288] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-23 10:00:48,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:00:48,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-04-23 10:00:56,262] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:00:56,272] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-23 10:00:56,280] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 89.0, 1.0, 2.0, 0.3437129169526595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534126.1433099344, 534126.1433099338, 169468.9904006323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [22.33333333333334, 89.0, 1.0, 2.0, 0.3438353877293951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534082.3449685606, 534082.3449685606, 169459.2737652238], 
processed observation next is [1.0, 0.043478260869565216, 0.2575039494470777, 0.89, 1.0, 1.0, 0.20944022617999408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14835620693571128, 0.14835620693571128, 0.2529242892018266], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.42834514], dtype=float32), -2.8660831]. 
=============================================
[2019-04-23 10:01:02,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:02,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7082
[2019-04-23 10:01:02,594] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.83333333333333, 96.0, 1.0, 2.0, 0.2844410003306096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458693.0151236086, 458693.0151236086, 164184.2491094271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 189600.0000, 
sim time next is 190200.0000, 
raw observation next is [19.81666666666667, 96.0, 1.0, 2.0, 0.2839575869409868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458056.6161098718, 458056.6161098718, 164140.8983375772], 
processed observation next is [0.0, 0.17391304347826086, 0.13823064770932092, 0.96, 1.0, 1.0, 0.13729829751926118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12723794891940882, 0.12723794891940882, 0.2449864154292197], 
reward next is 0.7550, 
noisyNet noise sample is [array([1.6156354], dtype=float32), -0.50289804]. 
=============================================
[2019-04-23 10:01:12,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:12,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7300
[2019-04-23 10:01:12,179] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([-0.73738986], dtype=float32), 0.8929609]. 
=============================================
[2019-04-23 10:01:14,037] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:14,045] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3184
[2019-04-23 10:01:14,053] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.31666666666667, 87.0, 1.0, 2.0, 0.2605029842532295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424335.7549007521, 424335.7549007521, 161896.8102515732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [20.3, 87.0, 1.0, 2.0, 0.2602382537486013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424014.5985252329, 424014.5985252323, 161874.0045438348], 
processed observation next is [1.0, 0.08695652173913043, 0.16113744075829392, 0.87, 1.0, 1.0, 0.10872078764891725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11778183292367582, 0.11778183292367564, 0.24160299185646986], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.46957198], dtype=float32), -0.1327862]. 
=============================================
[2019-04-23 10:01:15,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:15,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7266
[2019-04-23 10:01:15,945] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.41666666666667, 78.33333333333333, 1.0, 2.0, 0.3726745707419122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607374.3786738176, 607374.3786738182, 175496.8364599389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 377400.0000, 
sim time next is 378000.0000, 
raw observation next is [21.5, 78.0, 1.0, 2.0, 0.3832612156722808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624262.9154209254, 624262.915420926, 176952.8392955865], 
processed observation next is [1.0, 0.391304347826087, 0.21800947867298584, 0.78, 1.0, 1.0, 0.2569412237015431, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1734063653947015, 0.17340636539470164, 0.264108715366547], 
reward next is 0.7359, 
noisyNet noise sample is [array([2.7778876], dtype=float32), -2.1855567]. 
=============================================
[2019-04-23 10:01:15,959] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[72.96454 ]
 [73.15325 ]
 [73.44691 ]
 [73.784424]
 [74.18585 ]], R is [[72.90325165]
 [72.91228485]
 [72.92404938]
 [72.94097137]
 [72.9631424 ]].
[2019-04-23 10:01:16,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:16,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0878
[2019-04-23 10:01:16,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 89.5, 1.0, 2.0, 0.2208344335891245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367540.3790585317, 367540.3790585317, 157758.9833277278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 531000.0000, 
sim time next is 531600.0000, 
raw observation next is [18.03333333333333, 89.66666666666667, 1.0, 2.0, 0.2186143000393094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 363966.4045390229, 363966.4045390223, 157534.2777165281], 
processed observation next is [1.0, 0.13043478260869565, 0.05371248025276459, 0.8966666666666667, 1.0, 1.0, 0.05857144583049324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10110177903861747, 0.1011017790386173, 0.2351257876366091], 
reward next is 0.7649, 
noisyNet noise sample is [array([-1.8034661], dtype=float32), -1.5833055]. 
=============================================
[2019-04-23 10:01:16,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:16,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-23 10:01:16,895] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 86.0, 1.0, 2.0, 0.244680711747906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 402682.2676706034, 402682.2676706034, 160382.3105867617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [19.7, 86.0, 1.0, 2.0, 0.2444180700154164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 402250.722132477, 402250.7221324776, 160356.9301463709], 
processed observation next is [1.0, 1.0, 0.1327014218009479, 0.86, 1.0, 1.0, 0.08966032531977879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11173631170346583, 0.111736311703466, 0.23933870171100133], 
reward next is 0.7607, 
noisyNet noise sample is [array([1.2159958], dtype=float32), 0.7595636]. 
=============================================
[2019-04-23 10:01:17,697] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:17,705] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-23 10:01:17,711] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([1.082157], dtype=float32), -2.91153]. 
=============================================
[2019-04-23 10:01:21,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:01:21,307] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-23 10:01:21,313] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 84.0, 1.0, 2.0, 0.2326657747229007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 384388.3960436397, 384388.3960436397, 159187.0100435298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [19.61666666666667, 83.83333333333333, 1.0, 2.0, 0.2367096035335818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 391107.8924866772, 391107.8924866778, 159561.6484643982], 
processed observation next is [1.0, 0.13043478260869565, 0.12875197472353894, 0.8383333333333333, 1.0, 1.0, 0.08037301630552024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10864108124629923, 0.10864108124629938, 0.23815171412596745], 
reward next is 0.7618, 
noisyNet noise sample is [array([-0.8046277], dtype=float32), -1.247049]. 
=============================================
[2019-04-23 10:01:22,643] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-23 10:01:22,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:01:22,644] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:01:22,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:01:22,648] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,647] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:01:22,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:01:22,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,650] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:01:22,669] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,688] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,690] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run10
[2019-04-23 10:01:22,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run10
[2019-04-23 10:01:37,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:01:37,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.0, 92.5, 1.0, 2.0, 0.3544759504115104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548808.8898096017, 548808.8898096017, 170620.6795310446]
[2019-04-23 10:01:37,514] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:01:37,519] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6497306637666496
[2019-04-23 10:01:53,803] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:01:53,803] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.5, 84.5, 1.0, 2.0, 0.5529395100161054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844919.2874635188, 844919.2874635188, 201067.8893506665]
[2019-04-23 10:01:53,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:01:53,808] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.29691998121766117
[2019-04-23 10:02:13,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:13,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444]
[2019-04-23 10:02:13,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:13,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8653014828869345
[2019-04-23 10:02:28,330] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:28,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.23483681166667, 65.16833487666668, 1.0, 2.0, 0.8325328909906673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1163589.913726398, 1163589.913726398, 252069.3145354791]
[2019-04-23 10:02:28,332] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:02:28,337] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8369137254553556
[2019-04-23 10:02:40,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:40,744] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.09999999999999, 49.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.109584983538773, 6.9112, 168.9117187630503, 1594591.418086402, 1453851.314191885, 311356.3024867051]
[2019-04-23 10:02:40,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:40,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.31131253143728144
[2019-04-23 10:02:52,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.25883254]
[2019-04-23 10:02:52,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.18333333333333, 78.16666666666667, 1.0, 2.0, 0.5619285007923895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785239.759397992, 785239.759397992, 193867.0914209615]
[2019-04-23 10:02:52,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:02:52,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33004301485963694
[2019-04-23 10:03:32,978] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:03:33,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:03:33,225] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:03:33,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:03:33,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:03:34,355] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 225000, evaluation results [225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:03:43,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:43,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8709
[2019-04-23 10:03:43,817] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 60.5, 1.0, 2.0, 0.2436747180262885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 402194.8189099152, 402194.8189099146, 160246.7239985138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [22.9, 62.0, 1.0, 2.0, 0.2448529244925271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 403930.1500833399, 403930.1500833393, 160369.7330999935], 
processed observation next is [1.0, 0.782608695652174, 0.2843601895734597, 0.62, 1.0, 1.0, 0.09018424637653867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11220281946759442, 0.11220281946759425, 0.23935781059700523], 
reward next is 0.7606, 
noisyNet noise sample is [array([1.4326662], dtype=float32), 0.5234606]. 
=============================================
[2019-04-23 10:03:44,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:44,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-23 10:03:44,788] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 611400.0000, 
sim time next is 612000.0000, 
raw observation next is [17.2, 91.0, 1.0, 2.0, 0.2048236585511596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 342264.3359998221, 342264.3359998221, 155835.6189262077], 
processed observation next is [1.0, 0.08695652173913043, 0.014218009478673018, 0.91, 1.0, 1.0, 0.04195621512187903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09507342666661726, 0.09507342666661726, 0.23259047600926525], 
reward next is 0.7674, 
noisyNet noise sample is [array([0.95099324], dtype=float32), -0.07311777]. 
=============================================
[2019-04-23 10:03:44,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.550415]
 [71.54889 ]
 [71.59788 ]
 [71.61695 ]
 [71.66792 ]], R is [[71.94636536]
 [71.99416351]
 [72.04131317]
 [72.08778381]
 [72.13359833]].
[2019-04-23 10:03:53,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:53,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1695
[2019-04-23 10:03:53,131] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 89.33333333333334, 1.0, 2.0, 0.2540526982891675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417111.0264477726, 417111.0264477726, 161313.052710072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 778200.0000, 
sim time next is 778800.0000, 
raw observation next is [19.5, 89.66666666666667, 1.0, 2.0, 0.2547393510533554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418024.3709610438, 418024.3709610438, 161381.6543271054], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.8966666666666667, 1.0, 1.0, 0.1020956036787414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11611788082251216, 0.11611788082251216, 0.2408681407867245], 
reward next is 0.7591, 
noisyNet noise sample is [array([1.2591063], dtype=float32), -0.6010082]. 
=============================================
[2019-04-23 10:03:56,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:03:56,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3198
[2019-04-23 10:03:56,832] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 65.0, 1.0, 2.0, 0.2900126695283051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464387.5928239186, 464387.5928239193, 164558.2732376559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 829800.0000, 
sim time next is 830400.0000, 
raw observation next is [24.46666666666667, 65.66666666666667, 1.0, 2.0, 0.2917979729636215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 466567.3884336081, 466567.3884336088, 164701.5387627984], 
processed observation next is [0.0, 0.6086956521739131, 0.3586097946287521, 0.6566666666666667, 1.0, 1.0, 0.14674454573930298, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12960205234266892, 0.1296020523426691, 0.24582319218328116], 
reward next is 0.7542, 
noisyNet noise sample is [array([0.1917116], dtype=float32), 0.61985284]. 
=============================================
[2019-04-23 10:04:11,192] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:11,204] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-23 10:04:11,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.16666666666667, 1.0, 2.0, 0.670047564466006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029646.155231514, 1029646.155231514, 226111.9375705542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1008600.0000, 
sim time next is 1009200.0000, 
raw observation next is [21.7, 97.33333333333334, 1.0, 2.0, 0.6354124623894929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 975815.7821863971, 975815.7821863965, 218323.9132498642], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9733333333333334, 1.0, 1.0, 0.5607379064933649, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2710599394962214, 0.27105993949622126, 0.32585658694009584], 
reward next is 0.6741, 
noisyNet noise sample is [array([-1.0917808], dtype=float32), -0.34124917]. 
=============================================
[2019-04-23 10:04:12,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:12,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1010
[2019-04-23 10:04:12,843] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.66666666666666, 1.0, 2.0, 0.6585724213295752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010208.83483966, 1010208.83483966, 223312.2328489435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1010400.0000, 
sim time next is 1011000.0000, 
raw observation next is [21.7, 97.83333333333334, 1.0, 2.0, 0.6618211589878472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014583.8273416, 1014583.8273416, 223976.1209066036], 
processed observation next is [1.0, 0.6956521739130435, 0.2274881516587678, 0.9783333333333334, 1.0, 1.0, 0.5925556132383701, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2818288409282222, 0.2818288409282222, 0.3342927177710502], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.7240373], dtype=float32), -0.458278]. 
=============================================
[2019-04-23 10:04:12,868] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.37812 ]
 [63.395203]
 [63.35344 ]
 [63.12236 ]
 [63.402718]], R is [[63.33546066]
 [63.36880493]
 [63.40576553]
 [63.44585419]
 [63.47391891]].
[2019-04-23 10:04:21,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:21,528] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2021
[2019-04-23 10:04:21,541] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 94.0, 1.0, 2.0, 0.2792906045621624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451695.6538115449, 451695.6538115443, 163708.5847561406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1137600.0000, 
sim time next is 1138200.0000, 
raw observation next is [19.85, 94.33333333333334, 1.0, 2.0, 0.2743350859392549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443821.2545537705, 443821.2545537712, 163187.4146740149], 
processed observation next is [1.0, 0.17391304347826086, 0.1398104265402845, 0.9433333333333335, 1.0, 1.0, 0.12570492281837936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12328368182049179, 0.12328368182049201, 0.24356330548360433], 
reward next is 0.7564, 
noisyNet noise sample is [array([0.43962803], dtype=float32), 1.2256786]. 
=============================================
[2019-04-23 10:04:28,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:28,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2672
[2019-04-23 10:04:28,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1689902.738290467 W.
[2019-04-23 10:04:28,514] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.6044015601179256, 0.0, 1.0, 0.0, 1.0, 2.0, 1.013076794567798, 6.911200000000001, 6.9112, 168.9129563112766, 1689902.738290467, 1689902.738290466, 362245.4865772667], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [28.18333333333333, 71.83333333333333, 1.0, 2.0, 0.6176865012990023, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.915483503226586, 6.9112, 168.9129001136094, 1727077.570169854, 1724038.70656873, 369278.97148001], 
processed observation next is [1.0, 0.4782608695652174, 0.5347551342812005, 0.7183333333333333, 1.0, 1.0, 0.5393813268662677, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0004283503226585772, 0.0, 0.82943966821809, 0.4797437694916261, 0.47889964071353613, 0.5511626440000149], 
reward next is 0.4274, 
noisyNet noise sample is [array([1.1136059], dtype=float32), 0.21372125]. 
=============================================
[2019-04-23 10:04:29,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:29,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-23 10:04:29,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1981707.509335753 W.
[2019-04-23 10:04:29,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.7086759035549475, 1.0, 1.0, 0.7086759035549475, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1981707.509335753, 1981707.509335753, 377851.397848755], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.6584990297678837, 1.0, 2.0, 0.6584990297678837, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1841274.724014334, 1841274.724014335, 356855.0679642363], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.5885530479131128, 1.0, 1.0, 0.5885530479131128, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5114652011150927, 0.5114652011150931, 0.5326195044242333], 
reward next is 0.4674, 
noisyNet noise sample is [array([0.84896684], dtype=float32), -0.5786036]. 
=============================================
[2019-04-23 10:04:36,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:36,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8722
[2019-04-23 10:04:36,969] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
processed observation next is [1.0, 0.7391304347826086, 0.18878357030015785, 0.9283333333333332, 1.0, 1.0, 0.1640022337309235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13529385465598642, 0.13529385465598642, 0.24794667966507133], 
reward next is 0.7521, 
noisyNet noise sample is [array([1.1500489], dtype=float32), -0.60920155]. 
=============================================
[2019-04-23 10:04:37,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:04:37,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-23 10:04:37,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.86666666666667, 92.66666666666667, 1.0, 2.0, 0.3025808591077417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481811.8164296245, 481811.8164296245, 165751.3338185707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [20.88333333333333, 92.83333333333333, 1.0, 2.0, 0.3061218539966665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487057.8767615511, 487057.8767615511, 166124.2753755978], 
processed observation next is [1.0, 0.7391304347826086, 0.18878357030015785, 0.9283333333333332, 1.0, 1.0, 0.1640022337309235, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13529385465598642, 0.13529385465598642, 0.24794667966507133], 
reward next is 0.7521, 
noisyNet noise sample is [array([-1.5062777], dtype=float32), 1.1836458]. 
=============================================
[2019-04-23 10:04:40,847] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-23 10:04:40,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:04:40,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,849] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:04:40,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:04:40,851] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:04:40,851] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,851] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:04:40,855] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,857] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,855] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:04:40,880] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,880] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,916] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,936] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run11
[2019-04-23 10:04:40,955] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run11
[2019-04-23 10:05:05,050] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:05,051] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.15137839333333, 96.11075226833333, 1.0, 2.0, 0.4438844172172695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633313.8529845127, 633313.8529845127, 176958.7522847053]
[2019-04-23 10:05:05,052] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:05:05,056] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8585482374944623
[2019-04-23 10:05:09,215] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:09,219] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.43333333333333, 88.33333333333334, 1.0, 2.0, 0.438490591079734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638124.4035881577, 638124.4035881584, 177767.3400177261]
[2019-04-23 10:05:09,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:05:09,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.520006417082703
[2019-04-23 10:05:18,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:18,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.73758738, 93.29024662333333, 1.0, 2.0, 0.575155878261168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803730.7104815161, 803730.7104815161, 196209.4930188654]
[2019-04-23 10:05:18,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:05:18,220] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6106063740242511
[2019-04-23 10:05:19,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:19,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.83333333333334, 81.33333333333334, 1.0, 2.0, 0.7222089761594981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1009322.370755592, 1009322.370755592, 225709.0557350525]
[2019-04-23 10:05:19,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:05:19,388] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6843136173478381
[2019-04-23 10:05:44,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:44,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.46639253333333, 60.59586582666667, 1.0, 2.0, 0.635650895989715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888302.628718366, 888302.628718366, 207575.2151924756]
[2019-04-23 10:05:44,798] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:05:44,803] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8485308278506953
[2019-04-23 10:05:50,394] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:05:50,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.96666666666667, 60.0, 1.0, 2.0, 0.7080693390763718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104262, 989552.32962193, 989552.32962193, 222601.5872647566]
[2019-04-23 10:05:50,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:05:50,397] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2869290060617846
[2019-04-23 10:06:02,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:02,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 70.0, 1.0, 2.0, 0.4822587121336495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673873.7594510508, 673873.7594510501, 180864.0671062706]
[2019-04-23 10:06:02,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:06:02,127] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6381020450136674
[2019-04-23 10:06:13,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:13,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 83.0, 1.0, 2.0, 1.009399198420276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128868874903, 1410951.436535815, 1410951.436535816, 301806.3331834676]
[2019-04-23 10:06:13,045] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:06:13,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2877440172040816
[2019-04-23 10:06:27,784] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:27,785] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.87871423, 85.96236060999999, 1.0, 2.0, 0.5764077105633096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805480.701397891, 805480.701397891, 196433.8561203916]
[2019-04-23 10:06:27,786] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:06:27,788] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.08628963344871599
[2019-04-23 10:06:31,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.29470947]
[2019-04-23 10:06:31,702] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.75861793666667, 64.34988939166666, 1.0, 2.0, 0.6745729741398708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 998806.5287862775, 998806.5287862769, 222710.0358102998]
[2019-04-23 10:06:31,702] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:06:31,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03694831903573914
[2019-04-23 10:06:50,870] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:06:51,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:06:51,527] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:06:51,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:06:51,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:06:52,816] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:06:55,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:06:55,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8148
[2019-04-23 10:06:55,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1448400.0000, 
sim time next is 1449000.0000, 
raw observation next is [25.3, 80.0, 1.0, 2.0, 0.4123818573780608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606327.0677499198, 606327.0677499193, 174866.3882194718], 
processed observation next is [0.0, 0.782608695652174, 0.39810426540284366, 0.8, 1.0, 1.0, 0.2920263341904347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16842418548608884, 0.16842418548608867, 0.2609946092827937], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.85932314], dtype=float32), -1.1816754]. 
=============================================
[2019-04-23 10:06:55,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.15084]
 [78.16515]
 [78.15668]
 [78.06439]
 [78.04112]], R is [[78.11395264]
 [78.07159424]
 [78.02990723]
 [77.98831177]
 [77.94687653]].
[2019-04-23 10:06:56,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:06:56,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7382
[2019-04-23 10:06:56,547] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 82.66666666666666, 1.0, 2.0, 0.4058292445935688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599728.062371842, 599728.062371842, 174341.8726408554], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1450200.0000, 
sim time next is 1450800.0000, 
raw observation next is [24.5, 84.0, 1.0, 2.0, 0.4026097145536569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596571.4829347419, 596571.4829347419, 174099.046317623], 
processed observation next is [0.0, 0.8260869565217391, 0.3601895734597157, 0.84, 1.0, 1.0, 0.28025266813693606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16571430081520608, 0.16571430081520608, 0.25984932286212387], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.398358], dtype=float32), -0.7387242]. 
=============================================
[2019-04-23 10:07:04,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:04,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7926
[2019-04-23 10:07:04,263] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.00000000000001, 1.0, 2.0, 0.3264188989787366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511970.4120501378, 511970.4120501378, 167843.0780645584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1559400.0000, 
sim time next is 1560000.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3260876342298332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511458.9051673762, 511458.9051673768, 167803.87279087], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18805739063835328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14207191810204894, 0.1420719181020491, 0.2504535414789104], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.22030732], dtype=float32), 0.35776243]. 
=============================================
[2019-04-23 10:07:04,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.40311 ]
 [69.49996 ]
 [69.312904]
 [69.21328 ]
 [69.14645 ]], R is [[69.54258728]
 [69.59664917]
 [69.65006256]
 [69.70274353]
 [69.75473785]].
[2019-04-23 10:07:07,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:07,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7302
[2019-04-23 10:07:07,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.83333333333334, 1.0, 2.0, 0.4194867575474115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612962.5116811729, 612962.5116811722, 175384.3472161093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1637400.0000, 
sim time next is 1638000.0000, 
raw observation next is [23.1, 98.0, 1.0, 2.0, 0.4200944851691179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613363.8565777503, 613363.8565777509, 175408.5805615911], 
processed observation next is [1.0, 1.0, 0.2938388625592418, 0.98, 1.0, 1.0, 0.30131865683026254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1703788490493751, 0.17037884904937525, 0.26180385158446434], 
reward next is 0.7382, 
noisyNet noise sample is [array([0.7507412], dtype=float32), -0.8389249]. 
=============================================
[2019-04-23 10:07:07,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.4363  ]
 [71.4301  ]
 [71.41347 ]
 [71.40527 ]
 [71.401764]], R is [[71.53502655]
 [71.55791473]
 [71.58065033]
 [71.60327911]
 [71.62571716]].
[2019-04-23 10:07:07,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:07,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5330
[2019-04-23 10:07:07,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 98.83333333333333, 1.0, 2.0, 0.4276244205896276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619928.9886196837, 619928.9886196837, 175914.8479870278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1644600.0000, 
sim time next is 1645200.0000, 
raw observation next is [23.2, 99.0, 1.0, 2.0, 0.4291391812672065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621195.0623200836, 621195.0623200836, 176011.604917373], 
processed observation next is [1.0, 0.043478260869565216, 0.29857819905213273, 0.99, 1.0, 1.0, 0.3122158810448271, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.172554183977801, 0.172554183977801, 0.2627038879363776], 
reward next is 0.7373, 
noisyNet noise sample is [array([0.38487834], dtype=float32), -0.28917244]. 
=============================================
[2019-04-23 10:07:14,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:14,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2124
[2019-04-23 10:07:14,606] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.31666666666667, 77.33333333333333, 1.0, 2.0, 0.4976794542122041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695428.6523850602, 695428.6523850609, 183234.7198775038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1705800.0000, 
sim time next is 1706400.0000, 
raw observation next is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357678387], 
processed observation next is [1.0, 0.782608695652174, 0.5355450236966824, 0.78, 1.0, 1.0, 0.4030883318132741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19584748788950676, 0.19584748788950693, 0.2750972175639384], 
reward next is 0.7249, 
noisyNet noise sample is [array([0.7349468], dtype=float32), -0.38958633]. 
=============================================
[2019-04-23 10:07:16,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:16,651] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6628
[2019-04-23 10:07:16,657] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.16666666666666, 1.0, 2.0, 0.523230978863991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 188570.2929640439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 1.0, 1.0, 0.38207905486057536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.2726361589626489], 
reward next is 0.7274, 
noisyNet noise sample is [array([0.7341562], dtype=float32), 0.07274519]. 
=============================================
[2019-04-23 10:07:20,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:20,047] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4077
[2019-04-23 10:07:20,060] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 97.0, 1.0, 2.0, 0.3606247084474044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549235.8589680403, 549235.8589680396, 170393.7417863672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1833600.0000, 
sim time next is 1834200.0000, 
raw observation next is [22.15, 96.5, 1.0, 2.0, 0.3621832438266025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 550631.5804476335, 550631.5804476335, 170480.1388647433], 
processed observation next is [1.0, 0.21739130434782608, 0.24881516587677724, 0.965, 1.0, 1.0, 0.23154607689952109, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15295321679100932, 0.15295321679100932, 0.25444796845484074], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.14773272], dtype=float32), -0.502697]. 
=============================================
[2019-04-23 10:07:31,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:32,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-23 10:07:32,017] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
processed observation next is [1.0, 0.8695652173913043, 0.2843601895734597, 0.9566666666666667, 1.0, 1.0, 0.2791246345450137, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16599520968387887, 0.16599520968387868, 0.2600946575224543], 
reward next is 0.7399, 
noisyNet noise sample is [array([-0.90151834], dtype=float32), -0.4676022]. 
=============================================
[2019-04-23 10:07:39,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:39,523] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5596
[2019-04-23 10:07:39,532] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 91.5, 1.0, 2.0, 0.4756561449921946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664644.9191639184, 664644.9191639189, 179869.5470382831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064600.0000, 
sim time next is 2065200.0000, 
raw observation next is [24.96666666666667, 91.66666666666666, 1.0, 2.0, 0.4765954530357104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665957.8466101834, 665957.8466101841, 180009.9954166866], 
processed observation next is [0.0, 0.9130434782608695, 0.3823064770932071, 0.9166666666666665, 1.0, 1.0, 0.3693921120912173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18498829072505094, 0.18498829072505113, 0.2686716349502785], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.4851866], dtype=float32), 0.18996802]. 
=============================================
[2019-04-23 10:07:43,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:43,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2089
[2019-04-23 10:07:43,202] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333334, 82.33333333333334, 1.0, 2.0, 0.5247232997634513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733231.209425492, 733231.209425492, 187561.110487983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2107200.0000, 
sim time next is 2107800.0000, 
raw observation next is [28.15, 81.5, 1.0, 2.0, 0.5281077366108463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737962.1537803897, 737962.1537803903, 188117.8881391904], 
processed observation next is [0.0, 0.391304347826087, 0.533175355450237, 0.815, 1.0, 1.0, 0.43145510435041723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20498948716121937, 0.2049894871612195, 0.280772967371926], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.16529123], dtype=float32), 0.64468545]. 
=============================================
[2019-04-23 10:07:45,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:45,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5606
[2019-04-23 10:07:45,063] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.68333333333333, 90.66666666666667, 1.0, 2.0, 0.5344547091604308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746834.3436235257, 746834.3436235257, 189170.4358686113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2152200.0000, 
sim time next is 2152800.0000, 
raw observation next is [26.6, 91.0, 1.0, 2.0, 0.5327190006761872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744408.0558479098, 744408.0558479098, 188881.2250719339], 
processed observation next is [0.0, 0.9565217391304348, 0.4597156398104266, 0.91, 1.0, 1.0, 0.43701084418817726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20678001551330827, 0.20678001551330827, 0.281912276226767], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.2864439], dtype=float32), -0.42539585]. 
=============================================
[2019-04-23 10:07:52,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:52,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-23 10:07:52,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 85.0, 1.0, 2.0, 0.5202554624139636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726985.8629665411, 726985.8629665418, 186830.5868944517], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [27.1, 85.0, 1.0, 2.0, 0.5183004901568588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724253.125288148, 724253.1252881474, 186513.1048810808], 
processed observation next is [1.0, 1.0, 0.4834123222748816, 0.85, 1.0, 1.0, 0.4196391447672997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20118142369115222, 0.20118142369115205, 0.2783777684792251], 
reward next is 0.7216, 
noisyNet noise sample is [array([-1.6560458], dtype=float32), -0.82045245]. 
=============================================
[2019-04-23 10:07:57,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:07:57,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9718
[2019-04-23 10:07:57,778] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.43333333333334, 69.33333333333334, 1.0, 2.0, 0.5688671780349456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794939.5060654316, 794939.5060654316, 195089.9930337572], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2312400.0000, 
sim time next is 2313000.0000, 
raw observation next is [31.3, 70.0, 1.0, 2.0, 0.5691510590202485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 795336.3521258673, 795336.352125868, 195140.2342272678], 
processed observation next is [1.0, 0.782608695652174, 0.6824644549763034, 0.7, 1.0, 1.0, 0.4809048903858416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22092676447940757, 0.22092676447940776, 0.29125408093622057], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.67537564], dtype=float32), -0.1822483]. 
=============================================
[2019-04-23 10:07:57,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.18768 ]
 [64.60584 ]
 [64.51973 ]
 [64.49762 ]
 [64.452385]], R is [[63.93965912]
 [64.00908661]
 [64.07962036]
 [64.15209961]
 [64.22558594]].
[2019-04-23 10:07:59,683] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 10:07:59,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:07:59,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:07:59,689] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:07:59,691] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,691] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:07:59,693] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:07:59,694] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,697] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:07:59,716] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,737] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,738] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,757] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run12
[2019-04-23 10:07:59,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run12
[2019-04-23 10:08:11,445] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:11,446] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 50.33333333333334, 1.0, 2.0, 0.5445268599407027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 872902.984441503, 872902.9844415024, 203283.7117461775]
[2019-04-23 10:08:11,447] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:11,453] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6093949340112617
[2019-04-23 10:08:15,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:15,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 60.0, 1.0, 2.0, 0.3479882098658624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537665.9709052722, 537665.9709052715, 169671.9400954718]
[2019-04-23 10:08:15,505] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:15,509] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8566934088955152
[2019-04-23 10:08:23,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:23,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.95341154333333, 91.05955045166667, 1.0, 2.0, 0.6168149212529731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861969.2503652268, 861969.2503652268, 203920.7520631097]
[2019-04-23 10:08:23,700] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:08:23,704] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9460381904428234
[2019-04-23 10:08:31,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:31,196] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.9, 90.5, 1.0, 2.0, 0.645481610843356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 980669.2851604467, 980669.285160446, 219343.8134598392]
[2019-04-23 10:08:31,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:08:31,199] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03601436036800432
[2019-04-23 10:08:45,306] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:08:45,306] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.2, 53.0, 1.0, 2.0, 0.557469596149129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779006.594262574, 779006.594262574, 193090.0051797389]
[2019-04-23 10:08:45,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:08:45,310] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2960345722198069
[2019-04-23 10:09:12,884] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:09:12,885] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.48333333333333, 75.33333333333334, 1.0, 2.0, 0.5478595057094756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765572.6388595215, 765572.6388595221, 191434.4672676616]
[2019-04-23 10:09:12,885] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:09:12,887] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5513436801389294
[2019-04-23 10:09:42,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:09:42,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.96123649, 89.91220284666667, 1.0, 2.0, 0.5813982477806315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812457.2194660412, 812457.2194660412, 197331.1953135226]
[2019-04-23 10:09:42,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:09:42,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05164548975690941
[2019-04-23 10:10:02,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.28598318]
[2019-04-23 10:10:02,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.73333333333333, 75.33333333333334, 1.0, 2.0, 0.4659669663162758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665474.4254368199, 665474.4254368205, 180278.3304316765]
[2019-04-23 10:10:02,448] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:10:02,450] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6900982730155261
[2019-04-23 10:10:10,031] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:10:10,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:10:10,258] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:10:10,302] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:10:10,425] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:10:11,441] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 275000, evaluation results [275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:10:21,212] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:21,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-23 10:10:21,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1796217.904786802 W.
[2019-04-23 10:10:21,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6423987578056709, 1.0, 2.0, 0.6423987578056709, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1796217.904786802, 1796217.904786802, 350438.2880722096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.6221227319954996, 1.0, 2.0, 0.6221227319954996, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1739477.879708518, 1739477.879708518, 342565.448613683], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.544726183127108, 1.0, 1.0, 0.544726183127108, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4831882999190328, 0.4831882999190328, 0.5112917143487806], 
reward next is 0.4887, 
noisyNet noise sample is [array([0.24830467], dtype=float32), -0.2677218]. 
=============================================
[2019-04-23 10:10:27,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:27,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-23 10:10:27,728] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 89.66666666666667, 1.0, 2.0, 0.5173097320172007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722868.206990218, 722868.206990218, 186352.1543742471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2586000.0000, 
sim time next is 2586600.0000, 
raw observation next is [26.1, 90.0, 1.0, 2.0, 0.5143747832153845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718765.6378186536, 718765.6378186536, 185878.5886857517], 
processed observation next is [1.0, 0.9565217391304348, 0.4360189573459717, 0.9, 1.0, 1.0, 0.41490937736793315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19965712161629268, 0.19965712161629268, 0.2774307293817189], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.9811676], dtype=float32), -0.38159505]. 
=============================================
[2019-04-23 10:10:33,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:33,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8334
[2019-04-23 10:10:33,091] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3949073917860469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589260.5812168231, 589260.5812168224, 173549.8204274147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2676600.0000, 
sim time next is 2677200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3950110177672582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589415.3091307866, 589415.3091307866, 173564.0042775566], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27109761176778097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16372647475855182, 0.16372647475855182, 0.2590507526530696], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.43474853], dtype=float32), -0.7452561]. 
=============================================
[2019-04-23 10:10:38,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:38,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2811
[2019-04-23 10:10:38,049] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3841903961835303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578664.1240266134, 578664.1240266134, 172755.6668044197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3840836943958654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578503.4916854348, 578503.4916854348, 172741.3072146917], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25793216192272944, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16069541435706522, 0.16069541435706522, 0.2578228465890921], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.14040506], dtype=float32), -0.9038085]. 
=============================================
[2019-04-23 10:10:42,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:42,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1613
[2019-04-23 10:10:42,783] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3513653200284982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542214.1043638098, 542214.1043638104, 170026.38428374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3488649461903132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169630.7982640818], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21549993516905205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14934759423072505, 0.14934759423072505, 0.25318029591654], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.8481115], dtype=float32), 0.9150287]. 
=============================================
[2019-04-23 10:10:46,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:46,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2268
[2019-04-23 10:10:46,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 90.66666666666667, 1.0, 2.0, 0.3643887805136739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 556863.0691090797, 556863.0691090804, 171098.9585283331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2848800.0000, 
sim time next is 2849400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.3613194830044288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 553177.8879240341, 553177.8879240347, 170815.9815989881], 
processed observation next is [1.0, 1.0, 0.2654028436018958, 0.915, 1.0, 1.0, 0.23050540121015517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15366052442334283, 0.153660524423343, 0.2549492262671464], 
reward next is 0.7451, 
noisyNet noise sample is [array([1.2474526], dtype=float32), -0.18079893]. 
=============================================
[2019-04-23 10:10:46,373] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:46,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-23 10:10:46,396] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3625740804952334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 558502.2154738517, 558502.2154738524, 171362.3075981955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2860200.0000, 
sim time next is 2860800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3608919693442823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555931.4741309017, 555931.4741309017, 171145.0564798836], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2299903245111835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15442540948080605, 0.15442540948080605, 0.25544038280579645], 
reward next is 0.7446, 
noisyNet noise sample is [array([1.1588892], dtype=float32), 1.2013886]. 
=============================================
[2019-04-23 10:10:48,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:48,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-23 10:10:48,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.6692399577254918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027449.271806899, 1027449.271806899, 225822.4052598489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [22.5, 91.5, 1.0, 2.0, 0.6907842199617018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1058590.6258932, 1058590.625893201, 230597.951246472], 
processed observation next is [1.0, 0.6086956521739131, 0.2654028436018958, 0.915, 1.0, 1.0, 0.6274508674237371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.294052951637, 0.2940529516370003, 0.34417604663652535], 
reward next is 0.6558, 
noisyNet noise sample is [array([0.18706751], dtype=float32), 0.3892721]. 
=============================================
[2019-04-23 10:10:50,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:50,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-23 10:10:50,046] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.08333333333334, 99.5, 1.0, 2.0, 0.3084631333891708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490992.1554524192, 490992.1554524192, 166413.9369656206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.308800462644727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491248.2889387395, 491248.2889387395, 166428.6719962212], 
processed observation next is [1.0, 0.8695652173913043, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16722947306593614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13645785803853874, 0.13645785803853874, 0.24840100297943463], 
reward next is 0.7516, 
noisyNet noise sample is [array([1.5095273], dtype=float32), -0.17214331]. 
=============================================
[2019-04-23 10:10:54,193] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:54,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5172
[2019-04-23 10:10:54,212] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5497304460693864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869077.117597409, 869077.117597409, 203339.2766194669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2991600.0000, 
sim time next is 2992200.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.6323485296520729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 999692.4680557448, 999692.4680557442, 220503.448472093], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.5570464212675577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2776923522377069, 0.2776923522377067, 0.3291096245852134], 
reward next is 0.6709, 
noisyNet noise sample is [array([-0.32147932], dtype=float32), -1.3864592]. 
=============================================
[2019-04-23 10:10:59,681] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:10:59,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9084
[2019-04-23 10:10:59,701] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4293043876365769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623128.9560877865, 623128.9560877858, 176247.5925242753], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3091800.0000, 
sim time next is 3092400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4290735432712089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622794.3850525762, 622794.3850525768, 176215.0042449138], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3121367991219385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1729984402923823, 0.17299844029238243, 0.2630074690222594], 
reward next is 0.7370, 
noisyNet noise sample is [array([-0.49690524], dtype=float32), -0.5324249]. 
=============================================
[2019-04-23 10:11:03,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:03,216] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3296
[2019-04-23 10:11:03,219] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 69.66666666666666, 1.0, 2.0, 0.5884164292016372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822268.358886864, 822268.358886864, 198608.6892002957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3332400.0000, 
sim time next is 3333000.0000, 
raw observation next is [32.0, 70.33333333333334, 1.0, 2.0, 0.5922542599998402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 827633.5325379958, 827633.5325379951, 199312.7587013185], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.7033333333333335, 1.0, 1.0, 0.508740072288964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22989820348277662, 0.22989820348277643, 0.297481729404953], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.9178076], dtype=float32), 0.9819553]. 
=============================================
[2019-04-23 10:11:03,226] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.041176]
 [73.00806 ]
 [72.98877 ]
 [72.971275]
 [73.002655]], R is [[73.02402496]
 [72.9973526 ]
 [72.97180176]
 [72.94740295]
 [72.92346191]].
[2019-04-23 10:11:04,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:04,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5177
[2019-04-23 10:11:04,363] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 93.16666666666667, 1.0, 2.0, 0.4920327516193084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687535.7214298253, 687535.7214298247, 182357.0076422961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3181800.0000, 
sim time next is 3182400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.387190345622479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19072403351124664, 0.19072403351124664, 0.27202148299331014], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.7041428], dtype=float32), 0.19914801]. 
=============================================
[2019-04-23 10:11:08,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:11:08,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-23 10:11:08,584] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.5817994424674762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813018.0712923688, 813018.0712923688, 197404.7590774237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3252000.0000, 
sim time next is 3252600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.5804348998496559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811110.5043199442, 811110.5043199449, 197158.2650330917], 
processed observation next is [0.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.4944998793369348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22530847342220672, 0.22530847342220692, 0.29426606721356974], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.50958186], dtype=float32), -0.31186077]. 
=============================================
[2019-04-23 10:11:09,372] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-23 10:11:09,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:11:09,375] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:11:09,376] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:11:09,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,377] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:11:09,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:11:09,379] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,380] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:11:09,399] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,400] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,448] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run13
[2019-04-23 10:11:09,448] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run13
[2019-04-23 10:11:14,745] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:11:14,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.06666666666667, 89.33333333333334, 1.0, 2.0, 0.2549050236881701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415034.4809460548, 415034.4809460548, 161326.1703894624]
[2019-04-23 10:11:14,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-23 10:11:14,749] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.21328225567161552
[2019-04-23 10:12:29,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:29,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.76666666666667, 77.5, 1.0, 2.0, 0.5792351173049398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809433.2670798957, 809433.2670798957, 196940.6422608315]
[2019-04-23 10:12:29,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:29,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36646481896220506
[2019-04-23 10:12:31,766] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:31,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.13333333333334, 77.33333333333334, 1.0, 2.0, 0.5849246423122585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817386.9679044632, 817386.9679044632, 197970.9101162657]
[2019-04-23 10:12:31,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:31,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1013578059788921
[2019-04-23 10:12:51,641] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2975252]
[2019-04-23 10:12:51,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.25, 88.5, 1.0, 2.0, 0.5898681718824761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824297.8489187291, 824297.8489187291, 198872.7697888904]
[2019-04-23 10:12:51,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-23 10:12:51,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4864260195757609
[2019-04-23 10:13:16,483] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-23 10:13:17,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-23 10:13:17,571] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-23 10:13:17,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-23 10:13:17,837] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-23 10:13:18,854] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 300000, evaluation results [300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-23 10:13:19,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:19,193] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0279
[2019-04-23 10:13:19,201] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5197891354897839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726334.0120772389, 726334.0120772389, 186754.518086806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3445800.0000, 
sim time next is 3446400.0000, 
raw observation next is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5198032636949518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726353.76105566, 726353.76105566, 186756.7224078644], 
processed observation next is [1.0, 0.9130434782608695, 0.5102685624012641, 0.8066666666666668, 1.0, 1.0, 0.4214497152951226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20176493362657222, 0.20176493362657222, 0.27874137672815585], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.58424723], dtype=float32), -0.797]. 
=============================================
[2019-04-23 10:13:23,665] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:23,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9652
[2019-04-23 10:13:23,684] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5968682171159964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834083.7436432667, 834083.7436432667, 200164.2127618514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3337800.0000, 
sim time next is 3338400.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5138012731337371, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2315294868959351, 0.2315294868959351, 0.29863832334685986], 
reward next is 0.7014, 
noisyNet noise sample is [array([-1.4053011], dtype=float32), -1.5955559]. 
=============================================
[2019-04-23 10:13:23,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:23,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-23 10:13:23,773] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5350653751139779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747687.9740799333, 747687.9740799333, 189272.5834599001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3362400.0000, 
sim time next is 3363000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4419344861785306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20836683766327363, 0.20836683766327346, 0.2829308381689719], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.31653222], dtype=float32), -0.16907473]. 
=============================================
[2019-04-23 10:13:23,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.773155]
 [71.72623 ]
 [71.739845]
 [71.73015 ]
 [71.73069 ]], R is [[71.72959137]
 [71.72980499]
 [71.73075867]
 [71.73235321]
 [71.73438263]].
[2019-04-23 10:13:25,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:25,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3580
[2019-04-23 10:13:25,598] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.00000000000001, 1.0, 2.0, 0.5529439566125797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772680.1768895868, 772680.1768895861, 192306.9920530774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.550857715978353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769763.821872252, 769763.821872252, 191948.1791811568], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4588647180462084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21382328385340335, 0.21382328385340335, 0.28648981967336834], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.9914334], dtype=float32), 0.110124424]. 
=============================================
[2019-04-23 10:13:38,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:38,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0143
[2019-04-23 10:13:38,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2028186.858156472 W.
[2019-04-23 10:13:38,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.8093697736766838, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991563694953897, 6.9112, 168.912477560935, 2028186.858156472, 1971174.248709893, 410705.5414852551], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3577200.0000, 
sim time next is 3577800.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.79237666067048, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989364403948253, 6.9112, 168.9124259383805, 2004404.314993207, 1948951.970802914, 406583.9403263928], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.7498513983981686, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007816440394825274, 0.0, 0.8294373398006377, 0.556778976387002, 0.5413755474452538, 0.6068417019796908], 
reward next is 0.0023, 
noisyNet noise sample is [array([2.303847], dtype=float32), -0.16948862]. 
=============================================
[2019-04-23 10:13:50,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:50,113] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-23 10:13:50,121] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 76.5, 1.0, 2.0, 0.7581051865048776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1068852.72550017, 1068852.72550017, 235174.7294630643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3738600.0000, 
sim time next is 3739200.0000, 
raw observation next is [27.33333333333333, 75.66666666666666, 1.0, 2.0, 0.7779722481120304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087510.913913973, 1087510.913913973, 238602.1432485454], 
processed observation next is [1.0, 0.2608695652173913, 0.494470774091627, 0.7566666666666666, 1.0, 1.0, 0.7324966844723257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3020863649761036, 0.3020863649761036, 0.35612260186350064], 
reward next is 0.6439, 
noisyNet noise sample is [array([2.454161], dtype=float32), 0.42475387]. 
=============================================
[2019-04-23 10:13:50,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:50,921] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5648
[2019-04-23 10:13:50,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2528869.450373106 W.
[2019-04-23 10:13:50,932] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666666, 63.5, 1.0, 2.0, 0.6027560765562527, 1.0, 2.0, 0.6027560765562527, 1.0, 1.0, 1.03, 6.930072308021879, 6.9112, 170.5573041426782, 2528869.450373106, 2515350.448460625, 488979.9525018409], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3757800.0000, 
sim time next is 3758400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.6794876499962594, 1.0, 2.0, 0.6603338645123923, 1.0, 2.0, 1.03, 7.005096114832019, 6.9112, 170.5573041426782, 2770705.526044153, 2703443.919540226, 514928.2387295628], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.63, 1.0, 1.0, 0.613840542164168, 1.0, 1.0, 0.5907636921836051, 1.0, 1.0, 1.0365853658536586, 0.009389611483201943, 0.0, 0.8375144448122397, 0.7696404239011536, 0.7509566443167295, 0.7685496100441236], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16749541], dtype=float32), -0.37918893]. 
=============================================
[2019-04-23 10:13:51,193] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:51,203] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1022
[2019-04-23 10:13:51,208] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.0, 1.0, 2.0, 0.4825924020485993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695678.6407714129, 695678.6407714129, 183628.2395879161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3736200.0000, 
sim time next is 3736800.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.486988359873998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702015.3451218415, 702015.3451218408, 184320.2300252992], 
processed observation next is [1.0, 0.2608695652173913, 0.4312796208530806, 0.79, 1.0, 1.0, 0.3819136865951783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19500426253384487, 0.19500426253384467, 0.2751048209332824], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.8937628], dtype=float32), -0.2416629]. 
=============================================
[2019-04-23 10:13:52,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:52,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-23 10:13:52,738] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.66666666666667, 1.0, 2.0, 0.5874176855503325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820872.1493224866, 820872.1493224866, 198425.2997707888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5831125585352475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814853.7495796885, 814853.7495796885, 197641.9999554863], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.67, 1.0, 1.0, 0.49772597413885233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2263482637721357, 0.2263482637721357, 0.2949880596350542], 
reward next is 0.7050, 
noisyNet noise sample is [array([-0.06134652], dtype=float32), 1.5017084]. 
=============================================
[2019-04-23 10:13:52,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.688644]
 [72.607666]
 [72.50015 ]
 [72.35144 ]
 [72.12762 ]], R is [[72.80142212]
 [72.7772522 ]
 [72.75221252]
 [72.72635651]
 [72.69854736]].
[2019-04-23 10:13:58,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:13:58,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4594
[2019-04-23 10:13:58,509] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.66666666666667, 1.0, 2.0, 0.6045622374870508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844839.9029176588, 844839.9029176594, 201599.4660853402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5995248817101846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837797.7203946159, 837797.7203946159, 200657.9150921838], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5174998574821501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23272158899850443, 0.23272158899850443, 0.2994894255107221], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.2527415], dtype=float32), -0.6980992]. 
=============================================
[2019-04-23 10:14:03,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:03,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-23 10:14:03,316] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.66666666666667, 1.0, 2.0, 0.5874176855503325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820872.1493224866, 820872.1493224866, 198425.2997707888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5831125585352475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814853.7495796885, 814853.7495796885, 197641.9999554863], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.67, 1.0, 1.0, 0.49772597413885233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2263482637721357, 0.2263482637721357, 0.2949880596350542], 
reward next is 0.7050, 
noisyNet noise sample is [array([1.4173863], dtype=float32), -0.80398715]. 
=============================================
[2019-04-23 10:14:03,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.535416]
 [68.459404]
 [68.35998 ]
 [68.21987 ]
 [68.00691 ]], R is [[68.68315125]
 [68.70016479]
 [68.71589661]
 [68.73040771]
 [68.74256134]].
[2019-04-23 10:14:10,212] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:10,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4424
[2019-04-23 10:14:10,222] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 85.0, 1.0, 2.0, 0.5417378231236277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757015.2210022269, 757015.2210022275, 190394.0819614282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4064400.0000, 
sim time next is 4065000.0000, 
raw observation next is [27.76666666666667, 85.16666666666667, 1.0, 2.0, 0.5412760465771025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756369.7125669847, 756369.7125669853, 190316.0600203178], 
processed observation next is [1.0, 0.043478260869565216, 0.515007898894155, 0.8516666666666667, 1.0, 1.0, 0.4473205380447018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21010269793527353, 0.2101026979352737, 0.28405382092584747], 
reward next is 0.7159, 
noisyNet noise sample is [array([0.32365757], dtype=float32), 0.3579674]. 
=============================================
[2019-04-23 10:14:10,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[56.704704]
 [56.651108]
 [56.670403]
 [56.730286]
 [56.743942]], R is [[56.74113846]
 [56.88955688]
 [57.03650284]
 [57.18194199]
 [57.32579803]].
[2019-04-23 10:14:18,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-23 10:14:18,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0252
[2019-04-23 10:14:18,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3468371.453389085 W.
[2019-04-23 10:14:18,505] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.66666666666667, 65.0, 1.0, 2.0, 1.011571745565208, 1.0, 2.0, 0.8263759122968666, 1.0, 2.0, 1.03, 7.0051223121691, 6.9112, 170.5573041426782, 3468371.453389085, 3401091.080666969, 637829.0843416115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116000.0000, 
sim time next is 4116600.0000, 
raw observation next is [35.5, 65.5, 1.0, 2.0, 1.004732078630981, 1.0, 2.0, 0.8229560788297532, 1.0, 2.0, 1.03, 7.005121772308661, 6.9112, 170.5573041426782, 3453998.261891277, 3386718.275893153, 634882.3626573604], 
processed observation next is [1.0, 0.6521739130434783, 0.8815165876777251, 0.655, 1.0, 1.0, 1.0057012995553989, 1.0, 1.0, 0.7866940708792207, 1.0, 1.0, 1.0365853658536586, 0.009392177230866139, 0.0, 0.8375144448122397, 0.9594439616364658, 0.940755076636987, 0.947585615906508], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6057465], dtype=float32), 2.6423419]. 
=============================================
[2019-04-23 10:14:25,377] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-23 10:14:25,379] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-23 10:14:25,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,380] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-23 10:14:25,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-23 10:14:25,385] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,386] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-23 10:14:25,389] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,388] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-23 10:14:25,390] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-23 10:14:25,408] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v1-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,444] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v2-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,446] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v3-res2/Eplus-env-sub_run14
[2019-04-23 10:14:25,489] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/8/Eplus-env-Part3-NA-Shg-Test-v4-res2/Eplus-env-sub_run14
[2019-04-23 10:14:33,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:33,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.23484933333333, 42.11325310333334, 1.0, 2.0, 0.3264115394861993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 529246.960260036, 529246.9602600366, 169247.453800797]
[2019-04-23 10:14:33,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-23 10:14:33,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.36922485829218843
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.35, 77.5, 1.0, 2.0, 0.3263948956647002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513931.2196525275, 513931.2196525269, 168039.3934773798]
[2019-04-23 10:14:45,620] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-23 10:14:45,622] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.26469325736990246
[2019-04-23 10:14:46,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.07749905], dtype=float32), 0.2927659]
[2019-04-23 10:14:46,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.26824027666667, 93.71724167333333, 1.0, 2.0, 0.2707529227719555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445327.9625740654, 445327.9625740654, 163025.3367552402]
[2019-04-23 10:14:46,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-23 10:14:46,532] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5350512042021871
